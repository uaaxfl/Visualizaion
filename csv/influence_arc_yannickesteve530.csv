2008.iwslt-evaluation.9,W08-0313,1,0.765935,"Arabic words. A continuous space language model was deployed to improve the modeling of the target language. Both approaches achieved significant improvements in the BLEU score. The system achieves a score of 49.4 on the test set of the 2008 IWSLT evaluation. 1. Introduction This paper describes the system developed by the LIUM laboratory for the 2008 IWSLT evaluation. We only participated in the Arabic/English BTEC task. The architecture of the system is very similar to a large system built for the NIST Arabic/English task [1] or a system built for the translation between French and English [2]. All three are statistical phrase-based machine translation systems based on the freely available Moses decoder [3], with extensions for rescoring nbest lists with a continuous space language model in a second pass. No system combination is used. The training data of the translation model of the IWSLT system is limited to the provided BTEC corpora. Small improvements could be achieved using additional language model training data, namely LDC’s Gigaword corpus. All the models are case sensitive and include punctuation markers. We compare two different tokenization of the Arabic source text: a"
2008.iwslt-evaluation.9,P07-2045,0,0.0103883,"pproaches achieved significant improvements in the BLEU score. The system achieves a score of 49.4 on the test set of the 2008 IWSLT evaluation. 1. Introduction This paper describes the system developed by the LIUM laboratory for the 2008 IWSLT evaluation. We only participated in the Arabic/English BTEC task. The architecture of the system is very similar to a large system built for the NIST Arabic/English task [1] or a system built for the translation between French and English [2]. All three are statistical phrase-based machine translation systems based on the freely available Moses decoder [3], with extensions for rescoring nbest lists with a continuous space language model in a second pass. No system combination is used. The training data of the translation model of the IWSLT system is limited to the provided BTEC corpora. Small improvements could be achieved using additional language model training data, namely LDC’s Gigaword corpus. All the models are case sensitive and include punctuation markers. We compare two different tokenization of the Arabic source text: a full word mode and a morphological decomposition kindly provided by SYSTRAN. The later one achieved improvements in"
2008.iwslt-evaluation.9,2003.mtsummit-tttt.3,0,0.0235751,"mode and a morphological decomposition kindly provided by SYSTRAN. The later one achieved improvements in the BLEU score of several points. This paper is organized as follows. In the next section, the main architecture of the SMT system architecture is presented. In the following section the experimental results are provided and commented. The paper concludes with a discussion of future research issues. 2. System architecture The goal of statistical machine translation is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units [4, 5] and a log linear framework in order to introduce several models - 63 - e∗ = arg max p(e|f ) = arg max p(f , e)P (e) e X λi hi (e, f ))} arg max{exp( = e e (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [6]. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit [3] an"
2008.iwslt-evaluation.9,J03-1002,0,0.00258594,"mode and a morphological decomposition kindly provided by SYSTRAN. The later one achieved improvements in the BLEU score of several points. This paper is organized as follows. In the next section, the main architecture of the SMT system architecture is presented. In the following section the experimental results are provided and commented. The paper concludes with a discussion of future research issues. 2. System architecture The goal of statistical machine translation is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units [4, 5] and a log linear framework in order to introduce several models - 63 - e∗ = arg max p(e|f ) = arg max p(f , e)P (e) e X λi hi (e, f ))} arg max{exp( = e e (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [6]. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit [3] an"
2008.iwslt-evaluation.9,P02-1038,0,0.0394729,"ults are provided and commented. The paper concludes with a discussion of future research issues. 2. System architecture The goal of statistical machine translation is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units [4, 5] and a log linear framework in order to introduce several models - 63 - e∗ = arg max p(e|f ) = arg max p(f , e)P (e) e X λi hi (e, f ))} arg max{exp( = e e (1) i The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [6]. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit [3] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target LM is then constructed as detailed in section 3.2. The translation its"
2008.iwslt-evaluation.9,2006.iwslt-papers.2,1,0.829288,"the resulting probability functions are smooth functions of the word representation, better generalization to unknown n-grams can be expected. A neural network can be used to simultaneously learn the projection of the words onto the continuous space and to estimate the n-gram probabilities. This is still a n-gram approach, but the language model posterior probabilities are “interpolated” for any possible context of length n − 1 instead of backing-off to shorter contexts. This approach was already successfully applied in statistical machine translation systems, ranging from small IWSLT systems [9, 10] to large NIST systems [1]. A standard fully-connected multi-layer perceptron is used. The inputs to the neural network are the indices of the n − 1 previous words in the vocabulary hj = wj−n+1 , . . . , wj−2 , wj−1 and the outputs are the posterior probabilities of all words of the vocabulary: P (wj = i|hj ) ∀i ∈ [1, N ] (2) where N is the size of the vocabulary. The input uses the so-called 1-of-n coding, i.e., the ith word of the vocabulary is - 64 - The value of the output neuron pi corresponds directly to the probability P (wj = i|hj ). Training is performed with the standard backpropagat"
2008.iwslt-evaluation.9,2006.iwslt-evaluation.15,0,0.020417,"those from last year’s evaluation. Once the this year’s Arabic test data was available we build an interpolated language model on the source part of the BTEC corpus and all development corpora. After analyzing the interpolation coefficients, we found evidence that this year’s test data has similar characteristics than Dev4 and Dev5 and to less extent Dev6. Therefore, we decided to add the last two corpora to the training material after optimizing the system and to retrain the full system keeping all settings unmodified. This idea was already successfully proposed in previous IWSLT evaluations [12]. We have envisaged the use of additional sources of bitexts in order to improve the translation model, in particular large amounts of data that are available to build an Arabic/English translations system for the NIST task. However, initial experiments were not very concluding and that data is not used in the final system. We also investigated the possibility to increase the amount of monolingual data. This is detailed in the next section. We have realized that the test data of this year’s evaluation did contain only few punctuation marks. Many sentence had no punctuation at the end. This is"
2008.iwslt-evaluation.9,N06-2013,0,0.0611447,"Missing"
2008.iwslt-evaluation.9,2007.iwslt-1.13,0,0.0373629,"Missing"
2008.iwslt-evaluation.9,W07-0728,0,0.0497444,"Missing"
2008.iwslt-evaluation.9,W07-0732,0,0.0337468,"Missing"
2008.iwslt-evaluation.9,2006.iwslt-evaluation.17,0,\N,Missing
2008.iwslt-evaluation.9,2007.iwslt-1.11,0,\N,Missing
2009.iwslt-evaluation.10,2006.iwslt-evaluation.15,0,0.0321155,"and Dev3 corpora. The target language model was trained on the English side of the those corpora. No additional texts were used (constrained condition). We report results on Dev6 (development data) and Dev7 (internal test set). All BLEU scores are case-sensitive and include punctuations. For some systems, the Dev6 corpus was added to the training material after optimizing the system and the full system was retrained, keeping all settings unmodified. By these means we hope to lower the OOV rate on the official test set. This idea was already successfully proposed in previous IWSLT evaluations [1]. The statistical phrase-based system is based on the Moses SMT toolkit [2] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target language model (LM) is constructed on all available English data. The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence. In our system fourteen features functions were used, namely phrase and lexical translation prob"
2009.iwslt-evaluation.10,P07-2045,0,0.00776614,"e of the those corpora. No additional texts were used (constrained condition). We report results on Dev6 (development data) and Dev7 (internal test set). All BLEU scores are case-sensitive and include punctuations. For some systems, the Dev6 corpus was added to the training material after optimizing the system and the full system was retrained, keeping all settings unmodified. By these means we hope to lower the OOV rate on the official test set. This idea was already successfully proposed in previous IWSLT evaluations [1]. The statistical phrase-based system is based on the Moses SMT toolkit [2] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. A 4-gram back-off target language model (LM) is constructed on all available English data. The translation itself is performed in two passes: first, Moses is run and a 1000-best list is generated for each sentence. In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion"
2009.iwslt-evaluation.10,W07-0732,0,0.0227542,"us 376k words En Btec human translations Dev1−3 Giza++ Phrase extraction SRILM CSLM Phrase table 4g LM 4g CSLM phrase table SMT system 4−gram LM Moses Ar/En Src Moses 1000 bests Trg Src LM rescoring phrase table Trg SYSTRAN decode optimized with MERT λi Moses En’/En Condor BLEU SPE system 2nd pass optimisation Figure 2: Comparison of SMT and SPE systems. Figure 1: Architecture of the SMT system. 3. SPE System In the last years, there is increasing interest in the interaction between rule-based and statistical machine translation. A popular and successful idea is statistical post editing (SPE) [6, 7]. The principle idea is to train an SMT system to correct the outputs of a rule-based translation system. This is shown in figure 2. The operation performed by the rulebased translation system could also be seen as a very good tokenization or preprocessing, that actually performs many of the translation steps. Therefore, the task of the SMT system itself is very simplified. Accordingly, we argue that an SMT and SPE system are only two extreme cases of the interaction between tokenization/preprocessing and translation itself. An interesting question is whether both systems can be combined since"
2009.iwslt-evaluation.10,J07-2003,0,0.131844,"Missing"
2009.iwslt-evaluation.10,W09-0424,0,0.0287796,"Missing"
2009.iwslt-evaluation.10,2008.iwslt-evaluation.6,0,0.0423786,"Missing"
2009.iwslt-evaluation.10,P07-1040,0,0.0322066,"he development set (Dev6) using the provided Z-MERT procedure. The grammar rules extraction tools and Z-MERT are provided in the Joshua toolkit. Figure 3 summarizes the architecture of the Joshua translation system. The decoder is based on the token pass decoding algorithm. The scores used to evaluate the hypotheses are the following: 5. System combination • the system score : this replaces the score of the translation model. Until now, the words given by all systems 1 have the same probability which is . M The system combination approach is based on confusion network decoding as described in [11, 12] and shown in Figure 4. The protocol can be decomposed into three steps : • the language model (LM) probability. The 4-gram LM used for the combination is the same than the one used by each single system. 1. 1-best hypotheses from all M systems are aligned and confusion networks are built. It is obvious that this combination framework is not optimal, but as we can see in the results section, this simple architecture can already achieve improvements when combining only two systems. 2. All confusion networks are connected into a single lattice. 6. Experimental Evaluation 3. A 4-gram language mod"
2009.iwslt-evaluation.10,2006.amta-papers.25,0,0.0173726,"ter optimize our hierarchical systems built with Joshua. Rescoring the n-best lists with the continuous space LM achieved an improvement of 1.2 BLEU on the internal test set for the Arabic/English SMT system, and 0.6 BLEU for the SPE system. Due to time constraints, the continuous space LM was not applied on the hierarchical system. The improvements obtained by the CSLM are generally smaller 5.1. Hypotheses alignment and confusion network generation For each segment, the best hypotheses of M − 1 systems are aligned against the last one used as backbone. The alignment is done with the TER tool [13], without any tuning performed at this step (default edit costs are used). M confusion networks are generated in this way. Then all the confusion networks are connected into a single lattice by adding a first and last node. The probability of the first arcs must reflect how well such system provides a well structured hypothesis (good order). In our experiments, no tuning was done at this step, and we chose equal prior probabilities for all systems. - 67 - Proceedings of IWSLT 2009, Tokyo - Japan Approach: Train bitexts Arabic/English: Btec+Dev123 Btec+Dev1236 LM SMT Moses Dev Test Hierarchical"
2009.iwslt-evaluation.10,W07-0728,0,\N,Missing
2009.iwslt-evaluation.10,2008.iwslt-evaluation.10,0,\N,Missing
2010.iwslt-evaluation.14,P07-2045,0,0.00435578,"each using only a subset of corpora, optimizing them on the development corpus and comparing the resulting BLEU scores. The Table 3 shows the BLEU scores obtained on different corpus combinations. Regarding these experiments, the best combination we could determine was to select the TED corpus, along with the news-commentary and the Europarl ones, for a total amount of data of about forty-seven millions of tokens. corpus combination All corpora TED + NC + Eparl TED + NC + UN200x All except Gigaword Parallel corpus Giza++ Our statistical phrase-based systems are based on the Moses SMT toolkit [4] and constructed as follows. First, Giza++ is used to perform word alignments in both directions. Second, phrases and lexical reorderings are extracted. Both steps use the default settings of the Moses SMT toolkit. In our systems, fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model. The coefficients of these feature functions are tuned on the development corpus with the cMERT tool using 100best lists. The Figure 1 shows the basi"
2010.iwslt-evaluation.14,P08-1115,0,0.0234514,"ench in ASR condition as a full-fledged language, consequently our recaser can be regarded as a “French ASR-to-French” SMT system. This system was then optimized on the CRR development corpus, which is nothing more than the ASR manual transcription, except that it contains case and punctuation. The Figure 2 presents the global architecture of our SMT system for ASR condition. 3. Translating ASR outputs 3.1. Handling ASR word lattices For this evaluation campaign, word lattices, n-best lists and 1-best hypotheses were available. The use of word lattices as input of Moses system is described in [5]. Alternatively, we decided to generate another kind of input, namely confusion networks [6], computed from the word lattices. The word lattices were provided by the organizers under SLF format, which is the file format used by the HTK tools. In practice, word lattices provided by the organizers were very large, too large to be reasonably managed “as is” by the Moses decoder. This large size can be mainly explained because the word lattice topology strictly represents the history constraints applied to words in these word lattices, making their language model scores consistent with their histo"
2010.iwslt-evaluation.14,2005.iwslt-1.19,0,\N,Missing
2010.iwslt-evaluation.14,W09-0424,0,\N,Missing
2010.iwslt-evaluation.14,P05-1056,0,\N,Missing
2010.iwslt-evaluation.14,2009.iwslt-evaluation.5,0,\N,Missing
2010.iwslt-evaluation.14,2005.eamt-1.19,0,\N,Missing
2010.iwslt-evaluation.14,zhang-etal-2004-interpreting,0,\N,Missing
2010.iwslt-evaluation.14,2010.iwslt-papers.5,0,\N,Missing
2011.iwslt-evaluation.10,2011.iwslt-evaluation.1,0,0.0188061,"f Le Mans, France firstname.lastname@lium.univ-lemans.fr Abstract This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign. We participated in three of the proposed tasks, namely the Automatic Speech Recognition task (ASR), the ASR system combination task (ASR_SC) and the Spoken Language Translation task (SLT), since these tasks are all related to speech translation. We present the approaches and specificities we developed on each task. 1. Introduction This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign [1]. This year, new interesting tasks were proposed compared to last year evaluation campaign. As a matter of fact, the three tasks we participated in are all linked together in the same pipeline: speech recognition, ASR system combination and speech translation. Like the last year campaign, all of the considered tasks were related to the TED talks, requiring speech recognition of English, and speech translation from English to French. 2.1. The LIUM’s TED corpus For our system training, we aimed at using audio and transcripts from the TED talks. In order to get the desired data, we developed a sp"
2011.iwslt-evaluation.10,W11-2158,1,0.806845,"tter performance, but with the help of a second level of filtering to discard the out-of-domain data. For this task, we considered the following corpora among those available: the latest versions of News-Commentary and Europarl, the TED corpus provided by the organizers and a subset of the French–English 109 Gigaword. Like the last year’s evaluation campaign, we didn’t took into account the un200x corpus due to our experiments, showing its inappropriate style regarding the TED in-domain data. The Gigaword corpus was filtered with the same techniques used in our WMT 11 systems, as described in [20]. We call this internal subset ccb2. Table 5 summarizes the characteristics of those different corpora. In order to filter our ccb2 corpus, we tried a filtering approach based on LM perplexities, inspired by previous work described in [21]. We first built a 4-gram LM on the English data from the TED corpus. Using this LM, we computed the perplexity of each sentence from the ccb2 English part and sorted them in an ascending order. We then applied different thresholds on the sorted corpus and the resulting sets were integrated in our training data, in order to study the impact of the selection o"
2011.iwslt-evaluation.10,2010.iwslt-evaluation.14,1,0.868227,"y the organizers after the dev2010 and tst2010 run submissions) in a confusion network and extracting the most likely solution from it. We also changed the repartition of the talks between the original development and test set, increasing the size of the dev set by reducing the size of the test set, in order to make the tuning process more robust with more data. We call these sets LIUM dev2010 and LIUM tst2010. We then introduced different input types, after the baseline system had been fixed. Moreover, all of our data was processed by a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in bo"
2011.iwslt-evaluation.10,1983.tc-1.13,0,0.0562963,"and tst2010 run submissions) in a confusion network and extracting the most likely solution from it. We also changed the repartition of the talks between the original development and test set, increasing the size of the dev set by reducing the size of the test set, in order to make the tuning process more robust with more data. We call these sets LIUM dev2010 and LIUM tst2010. We then introduced different input types, after the baseline system had been fixed. Moreover, all of our data was processed by a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We us"
2011.iwslt-evaluation.10,N03-1017,0,0.00415307,"g process more robust with more data. We call these sets LIUM dev2010 and LIUM tst2010. We then introduced different input types, after the baseline system had been fixed. Moreover, all of our data was processed by a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool [18].1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on LIUM dev2010, usi"
2011.iwslt-evaluation.10,J03-1002,0,0.00381576,"g process more robust with more data. We call these sets LIUM dev2010 and LIUM tst2010. We then introduced different input types, after the baseline system had been fixed. Moreover, all of our data was processed by a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool [18].1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on LIUM dev2010, usi"
2011.iwslt-evaluation.10,P02-1038,0,0.0983405,"ase, it was necessary to recover them for the final output. We used the same technique as in last year’s evaluation campaign, namely recasing using a separate SMT system dedicated to this task [12]. This technique is summarized by figure 1. The main differences with last year are: • less but more appropriate training data using perplexity data selection based on a French in-domain LM; (1) • suppression of the lexical reordering (instead of limiting it); The feature functions hi are the system models and the λi weights are typically optimized to maximize a scoring function on a development set [16]. • a better development set for the tuning of the system, coming from a real ASR output. = e i 1 The source is available at http://www.cs.cmu.edu/~qing/ 82 Corpus TED News-Comm. Europarl v6 ccb2 TOTAL LIUM dev2010 LIUM tst2010 #En tokens (millions) Orig. ASR 2.0M 1.8M 2.8M 2.6M 50.6M 46.6M 232.5M 220.0M 287.9M 271.0M N/A 39k N/A 9k #Fr tokens (millions) Orig. ASR 2.2M 2.0M 3.3M 3.1M 56.2M 51.2M 272.6M 258.4M 334.3M 314.7M N/A 39k N/A 9k Table 5: Characteristics of the considered parallel corpora. Orig is the original data while ASR is the processed data. 4.2. Bilingual data selection Data set"
2011.iwslt-evaluation.10,P07-2045,0,0.0108598,"y a newer version of our in-house script first described in [12] and based on previous work by [13]. The goal of this script is to make training, development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool [18].1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on LIUM dev2010, using the MERT tool. 4.1.2. Language modeling The French language models were trained on all the French parts of the allowed parallel corpora, in addition to the proposed News monolingual corpus. 4-gram back-off"
2011.iwslt-evaluation.10,W08-0509,0,0.0733857,"development and test data resemble to ASR outputs. 4.1. Architecture of the LIUM’s SLT System The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . Our system is a phrase-based system [14, 15] which uses a log linear framework in order to introduce several models explaining the translation process: e∗ = arg max p(e|f ) X arg max{exp( λi hi (e, f ))} Our system is based on the Moses SMT toolkit [17] and is constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool [18].1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on LIUM dev2010, using the MERT tool. 4.1.2. Language modeling The French language models were trained on all the French parts of the allowed parallel corpora, in addition to the proposed News monolingual corpus. 4-gram back-off LMs were used. The word list contains all the French words of our phrase table filtered on the 150k words from the ASR decoding vocabulary. Separa"
2014.iwslt-evaluation.14,rousseau-etal-2014-enhancing,1,0.817939,"ing (NLP) systems like the ones we are going to present here can often be enhanced using various methods, which can occur before, during or after the actual system processing. Among these, one of the most efficient pre-processing method is data selection, i.e. the fact to determine which data will be injected into the system we are going to build. For this campaign, many data selection processing was done, both in ASR and SLT tasks. 2.1. Selection for the ASR task 2.1.1. Acoustic models training data selection For our acoustic modeling we used as a primary source the TED-LIUM corpus release 2 [1], removing from it all talks recorded after December 31st, 2010. In order to strengthen this base, we first added data from the Euronews corpora [2] distributed by the campaign organizers and from the 1997 English Broadcast News Speech (HUB4) [3]. Then, from the MediaEval 2014 evaluation campaign Search and Hyperlinking Task data transcripts (BBC recordings from 2008 which were decoded by the LIUM) [4], we applied a threshold on our confidence measures to select the best possible segments for our system within a limit of 50 hours of speech. Table 1 summarizes the characteristics of the data in"
2014.iwslt-evaluation.14,gretter-2014-euronews,0,0.147438,"e actual system processing. Among these, one of the most efficient pre-processing method is data selection, i.e. the fact to determine which data will be injected into the system we are going to build. For this campaign, many data selection processing was done, both in ASR and SLT tasks. 2.1. Selection for the ASR task 2.1.1. Acoustic models training data selection For our acoustic modeling we used as a primary source the TED-LIUM corpus release 2 [1], removing from it all talks recorded after December 31st, 2010. In order to strengthen this base, we first added data from the Euronews corpora [2] distributed by the campaign organizers and from the 1997 English Broadcast News Speech (HUB4) [3]. Then, from the MediaEval 2014 evaluation campaign Search and Hyperlinking Task data transcripts (BBC recordings from 2008 which were decoded by the LIUM) [4], we applied a threshold on our confidence measures to select the best possible segments for our system within a limit of 50 hours of speech. Table 1 summarizes the characteristics of the data included in our ASR system acoustic models. Corpus TED-LIUM Euronews 1997 HUB4 MedialEval 14 Total Duration 130.1h 68.2h 75.0h 50.0h 323.3h Segments 6"
2014.iwslt-evaluation.14,W08-0509,0,0.0139487,"d. 4.1. Architecture of the LIUM SLT system The SMT system is based on the Moses toolkit [11]. The standard 14 feature functions were used (i.e phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, word and phrase penalty and target language model (LM) probability). In addition to these, an Operation Sequence Model (OSM) [12] have been trained and included in the system. 4.1.1. Translation model The translation models have been trained with the standard procedure. First, the bitexts are word aligned in both directions with GIZA++ [13]. Then the phrase pairs are extracted and the lexical and phrase probabilities are computed. The weights have been optimized with MERT using two versions of the development data. For some systems, the provided transcriptions were used, and for others, the outputs of our ASR system was used. This was performed for the sake of comparing the impact of ASR systems improvement (observed during the last few years). 4.1.2. Language modeling Table 3: Interpolation coefficients and perplexities for the bigram, trigram, quadrigram and CSLM language models used in the LIUM ASR system. The language model"
2014.iwslt-evaluation.14,P07-2045,0,0.0036987,"Missing"
2014.iwslt-evaluation.14,P11-1105,0,\N,Missing
2015.iwslt-evaluation.7,P11-1105,0,\N,Missing
2015.iwslt-evaluation.7,P07-2045,0,\N,Missing
2015.iwslt-evaluation.7,gretter-2014-euronews,0,\N,Missing
2015.iwslt-evaluation.7,W08-0509,0,\N,Missing
2015.jeptalnrecital-court.28,P14-1002,0,0.0317617,"Missing"
2015.jeptalnrecital-court.28,D14-1218,0,0.0229708,"Missing"
2015.jeptalnrecital-court.28,D13-1043,0,0.0209622,"Missing"
2015.jeptalnrecital-court.28,D14-1051,1,0.86499,"Missing"
2015.jeptalnrecital-court.33,A00-2004,0,0.172421,"Missing"
2015.jeptalnrecital-court.33,W02-0908,0,0.0427818,"Missing"
2015.jeptalnrecital-court.33,J97-1003,0,0.648522,"Missing"
2015.jeptalnrecital-court.33,P11-1154,0,0.057336,"Missing"
2015.jeptalnrecital-court.33,P06-1004,0,0.0855874,"Missing"
2015.jeptalnrecital-court.33,J02-1002,0,0.115371,"Missing"
2016.jeptalnrecital-jep.38,H92-1073,0,0.828493,"Missing"
2016.jeptalnrecital-jep.72,devillers-etal-2004-french,0,0.0194825,"Missing"
2016.jeptalnrecital-jep.72,H90-1021,0,0.519788,"Missing"
2016.jeptalnrecital-jep.81,J81-4005,0,0.771917,"Missing"
2016.jeptalnrecital-jep.81,gravier-etal-2012-etape,0,0.044814,"Missing"
2016.jeptalnrecital-jep.81,N10-1025,0,0.043427,"Missing"
2016.jeptalnrecital-jep.81,D14-1162,0,0.0742615,"Missing"
2016.jeptalnrecital-jep.81,P06-2093,0,0.0997918,"Missing"
2016.jeptalnrecital-jep.81,P10-1040,0,0.158083,"Missing"
2018.jeptalnrecital-court.25,auer-etal-2010-elan,0,0.0801363,"Missing"
2018.jeptalnrecital-court.25,2015.jeptalnrecital-court.33,1,0.722633,"Missing"
2018.jeptalnrecital-court.25,P03-1071,0,0.096718,"Missing"
2018.jeptalnrecital-court.3,Y15-1023,0,0.0336275,"Missing"
2018.jeptalnrecital-court.3,C16-1228,0,0.0422731,"Missing"
2018.jeptalnrecital-court.3,D14-1181,0,0.00996705,"Missing"
2018.jeptalnrecital-court.3,S16-1077,0,0.0515409,"Missing"
2018.jeptalnrecital-court.3,D17-1056,0,0.0356781,"Missing"
2018.jeptalnrecital-court.3,C16-1329,0,0.0348775,"Missing"
2019.jeptalnrecital-court.2,luzzati-etal-2014-human,0,0.0474015,"Missing"
2019.jeptalnrecital-court.2,2018.jeptalnrecital-court.25,1,0.783425,"Missing"
2019.jeptalnrecital-court.24,Y15-1023,1,0.887555,"Missing"
2019.jeptalnrecital-court.24,C16-1228,0,0.0395983,"Missing"
2019.jeptalnrecital-court.24,L18-1550,0,0.0293447,"Missing"
2019.jeptalnrecital-court.24,D14-1181,0,0.016591,"Missing"
2019.jeptalnrecital-court.24,D14-1162,0,0.0812704,"Missing"
2019.jeptalnrecital-court.24,N18-1202,0,0.105432,"Missing"
2019.jeptalnrecital-court.24,S16-1077,0,0.0568844,"Missing"
2019.jeptalnrecital-court.24,D17-1056,0,0.0375576,"Missing"
2019.jeptalnrecital-long.6,bechet-etal-2012-decoda,0,0.0382693,"Missing"
2019.jeptalnrecital-long.6,devillers-etal-2004-french,0,0.0594883,"Missing"
2019.jeptalnrecital-long.6,esteve-etal-2010-epac,1,0.527742,"Missing"
2019.jeptalnrecital-long.6,giraudel-etal-2012-repere,0,0.0191512,"Missing"
2019.jeptalnrecital-long.6,gravier-etal-2012-etape,0,0.0581911,"Missing"
2019.jeptalnrecital-long.6,W11-0411,0,0.0410345,"Missing"
2019.jeptalnrecital-long.6,N19-1119,0,0.0453567,"Missing"
2020.iwslt-1.2,P19-1126,0,0.076722,"Missing"
2020.iwslt-1.2,2005.mtsummit-papers.11,0,0.172794,".e. reading the full source before writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model that jointly optimizes a large set of wait-k paths. The results demonstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder"
2020.iwslt-1.2,D18-2012,0,0.0233125,"nstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output embeddings. We optimize our models with label-smoothed maximum likelihood (Szegedy et al., 2016) with a smoothing rate  = 0.1. The parameters are updated using 6 27 7 https://github.com/pytorch/fairseq/ blob/simulastsharedtask/examples/ simultaneous_translation http://www.statmt.org/wmt19/ 40 TED-LIUM 3 How2"
2020.iwslt-1.2,N18-2079,0,0.0241677,"eous speech translation track, we build on Transformer-based wait-k models for the text-to-text subtask. For speech-to-text simultaneous translation, we attach a wait-k MT system to a hybrid ASR system. We propose an algorithm to control the latency of the ASR+MT cascade and achieve a good latency-quality trade-off on both subtasks. 1 • IWSLT 2020 offline translation track with end-to-end models for the English-German language pair, • IWSLT 2020 simultaneous translation track with a cascade of an ASR system trained using Kaldi (Povey et al., 2011) and an online MT system with wait-k policies (Dalvi et al., 2018; Ma et al., 2019). Introduction While cascaded speech-to-text translation (AST) systems (combining source language speech recognition (ASR) and source-to-target text translation (MT)) remain state-of-the-art, recent works have attempted to build end-to-end AST with very encouraging results (Bérard et al., 2016; Weiss et al., 2017; Bérard et al., 2018; Jia et al., 2019; Sperber et al., 2019). This year, IWSLT 2020 offline translation track attempts to evaluate if endto-end AST will close the gap with cascaded AST for the English-to-German language pair. Another increasingly popular topic is si"
2020.iwslt-1.2,N19-1202,0,0.0478636,"Missing"
2020.iwslt-1.2,N19-4009,0,0.0278244,"multi-path Ensemble 25 23 21 19 1 3 5 7 9 Average Lagging (AL) in detokenized tokens Figure 2: [Text-to-Text] Latency-quality trade-offs evaluated on MuST-C tst-COMMON with greedy decoding. Offline systems have an AL of 18.55 words. The red vertical bars correspond to the AL evaluation thresholds. Adam (Kingma and Ba, 2015) (β1 , β2 = 0.9, 0.98) with a learning rate that follows an inverse squareroot schedule. We train for a total of 50K updates and evaluate with the check-pointed weights corresponding to the lowest (best) loss on the development set. Our models are implemented with Fairseq (Ott et al., 2019). We generate translation hypotheses with greedy decoding and evaluate the latency-quality trade-off by measuring casesensitive detokenized BLEU (Papineni et al., 2002) and word-level Average Lagging (AL) (Ma et al., 2019). Table 5: Parallel training data for the MT systems. Results. We show in Figure 2 the performance of our systems on the test set (MuST tst-COMMON) measured with the provided evaluation server.7 We denote with ktrain =∞ a unidirectional model trained for wait-until-end decoding i.e. reading the full source before writing the target. We evaluate four wait-k systems each traine"
2020.iwslt-1.2,P02-1040,0,0.106509,"OMMON with greedy decoding. Offline systems have an AL of 18.55 words. The red vertical bars correspond to the AL evaluation thresholds. Adam (Kingma and Ba, 2015) (β1 , β2 = 0.9, 0.98) with a learning rate that follows an inverse squareroot schedule. We train for a total of 50K updates and evaluate with the check-pointed weights corresponding to the lowest (best) loss on the development set. Our models are implemented with Fairseq (Ott et al., 2019). We generate translation hypotheses with greedy decoding and evaluate the latency-quality trade-off by measuring casesensitive detokenized BLEU (Papineni et al., 2002) and word-level Average Lagging (AL) (Ma et al., 2019). Table 5: Parallel training data for the MT systems. Results. We show in Figure 2 the performance of our systems on the test set (MuST tst-COMMON) measured with the provided evaluation server.7 We denote with ktrain =∞ a unidirectional model trained for wait-until-end decoding i.e. reading the full source before writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model t"
2020.iwslt-1.2,P16-1162,0,0.0399967,"t which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output embeddings. We optimize our models with label-smoothed maximum likelihood (Szegedy et al., 2016) with a smoothing rate  = 0.1. The parameters are updated using 6 27 7 https://github.com/pytorch/fairseq/ blob/simulastsharedtask/examples/ simultaneous_translation http://www.statmt.org/wmt19/ 40 TED-LIUM 3 How2 Europarl #hours #words #speakers 452 365 94 5.05M 3.31M 0.75M 2,028 13,"
2020.iwslt-1.2,P13-1135,0,0.0524922,"Missing"
2020.iwslt-1.2,Q19-1020,0,0.0316591,"models for the English-German language pair, • IWSLT 2020 simultaneous translation track with a cascade of an ASR system trained using Kaldi (Povey et al., 2011) and an online MT system with wait-k policies (Dalvi et al., 2018; Ma et al., 2019). Introduction While cascaded speech-to-text translation (AST) systems (combining source language speech recognition (ASR) and source-to-target text translation (MT)) remain state-of-the-art, recent works have attempted to build end-to-end AST with very encouraging results (Bérard et al., 2016; Weiss et al., 2017; Bérard et al., 2018; Jia et al., 2019; Sperber et al., 2019). This year, IWSLT 2020 offline translation track attempts to evaluate if endto-end AST will close the gap with cascaded AST for the English-to-German language pair. Another increasingly popular topic is simultaneous (online) machine translation which consists in generating an output hypothesis before the entire ∗ This paper goes as follows: we review the systems built for the offline speech translation track in §2. Then, we present our approaches to the simultaneous track for both text-to-text and speech-to-text subtasks in §3. We ultimately conclude this work in §4. 2 Offline Speech translat"
2020.iwslt-1.2,tiedemann-2012-parallel,0,0.0284139,"ore writing the target. We evaluate four wait-k systems each trained with a value of ktrain in {5, 7, 9, ∞} and decoded with keval ranging from 2 to 11. We then ensemble the aforementioned wait-k models and evaluate a multipath model that jointly optimizes a large set of wait-k paths. The results demonstrate that multipath is competetive with wait-k without the need to select which path to optimize (some values of k, e.g. 5, underperform in comparison). Ensembling the wait-k models gives a boost of 1.43 BLEU points on average. and WMT’19 data,6 namely, Europarl (Koehn, 2005), News Commentary (Tiedemann, 2012) and Common Crawl (Smith et al., 2013). We remove pairs with a length-ratio exceeding 1.3 from Common Crawl and pairs exceeding a length-ratio of 1.5 from the rest. We develop on MuST-C dev and report results on MuST-C tst-COMMON. For open-vocabulary translation, we use SentencePiece (Kudo and Richardson, 2018) to segment the bitexts with byte pair encoding (Sennrich et al., 2016). This results in a joint vocabulary of 32K types. Details of the training data are provided in Table 5. We train Transformer big architectures and tie the embeddings of the encoder with the decoder’s input and output"
2020.jeptalnrecital-jep.43,L16-1166,1,0.89134,"Missing"
2020.jeptalnrecital-jep.8,galibert-etal-2014-etape,0,0.0354513,"Missing"
2020.jeptalnrecital-jep.8,gravier-etal-2012-etape,0,0.060583,"Missing"
2020.jeptalnrecital-jep.8,W11-0411,1,0.781368,"Missing"
2020.jeptalnrecital-jep.8,J98-4004,0,0.318247,"Missing"
2020.jeptalnrecital-jep.8,N16-1030,0,0.152007,"Missing"
2020.jeptalnrecital-jep.8,P10-1052,0,0.0882307,"Missing"
2020.jeptalnrecital-jep.8,P16-1101,0,0.0527671,"Missing"
2020.lrec-1.197,garnier-rizet-etal-2008-callsurf,0,0.026891,"plored the prediction of continuous dimensions such as activation and valence in SEMAINE multimodal database (McKeown et al., 2012) and SEWA database (Kossaifi et al., 2019). SEMAINE is composed of simulated conversations between a human user and a machine through Sensitive Artificial Listener (SAL) scenarios (Douglas-Cowie et al., 2008) and SEWA consists of discussions on commercials between two persons, talking about the ads they saw. Call center corpora are usually domain-dependant: DECODA (Lailler et al., 2016) (parisian transportation operator) is annotated with named entities, CallSurf (Garnier-Rizet et al., 2008) (French energy operator) is partially annotated with emotion categories (Devillers et al., 2010) or NATURAL (Morrison et al., 2007) (Chinese electricity company) is annotated with two classes: anger and neutral. To our knowledge, no call center corpus gathers different domains with the same annotation scheme. Allo-Media company develops services for cross-domain call centers allowing us to collect data from various domains. The main goals of call center conversations are either to pursue a person to sign a contract, or to solve some technical or financial problems. As a result, the question o"
2020.lrec-1.197,L16-1705,0,0.0178635,"particularly convenient as most of the discrete emotional labels can be translated into these two dimensions, thus allowing multi-corpora approaches (Schuller, 2018). We studied existing and available corpora for SER. Existing corpora are often acted one and usually not related to call center conversations. Even if many efforts are made to move from acted to real-life databases, there are still few available emotional spontaneous speech corpora. Most of these corpora aim at modeling social aspects of real-life or induced interactions such as laughter (Devillers et al., 2015) or disfluencies (Gilmartin and Campbell, 2016). In SER corpora, emotion is mainly represented with discrete categories, for instance anger, neutral and positive in call center conversations (Devillers et al., 2010), probably because “part of the reason for the dominance of discrete emotions is the ease of collecting training data” (Campbell, 2008). In the course of AVEC challenges (Valstar et al., 2013), recent studies explored the prediction of continuous dimensions such as activation and valence in SEMAINE multimodal database (McKeown et al., 2012) and SEWA database (Kossaifi et al., 2019). SEMAINE is composed of simulated conversations"
2020.lrec-1.197,L16-1166,1,0.900955,"Missing"
2020.lrec-1.197,H89-2032,0,0.433253,"Missing"
2020.lrec-1.529,auer-etal-2010-elan,0,0.0931201,"Missing"
2020.lrec-1.529,bazillon-etal-2008-manual,1,0.695665,"iption (Section 3.1.), thematic segmentation (Section 3.2.), and in-domain words annotation (Section 3.3.), established for annotating the PASTEL corpus. 3.1. Manual transcription This manual transcription was carried out in two passes. The first pass consists in automatically transcribing the course through a generic speech recognition system (i.e. not adapted to the targeted courses). The human expert intervenes during the second pass to manually correct the errors made by the automatic transcription. This two-step approach proved to be a faster way than a manual from scratch transcription (Bazillon et al., 2008). The conventions used for the evaluation of transcription campaigns (Gravier et al., 2004) served as a guide for manually transcribing registered lectures. The speech recognition system is based on the Kaldi toolkit (Povey et al., 2011). Acoustic models were trained on about 300 hours of speech from French broadcast news with manual transcriptions, and are based on a chain-TDNN approach (Povey et al., 2016). The generic n-gram language models were trained on these manual transcriptions of speech, but also on newspaper articles, for a total of 1.6 billions of words. The vocabulary of the gener"
2020.lrec-1.529,gravier-etal-2004-ester,0,0.126641,"(Section 3.3.), established for annotating the PASTEL corpus. 3.1. Manual transcription This manual transcription was carried out in two passes. The first pass consists in automatically transcribing the course through a generic speech recognition system (i.e. not adapted to the targeted courses). The human expert intervenes during the second pass to manually correct the errors made by the automatic transcription. This two-step approach proved to be a faster way than a manual from scratch transcription (Bazillon et al., 2008). The conventions used for the evaluation of transcription campaigns (Gravier et al., 2004) served as a guide for manually transcribing registered lectures. The speech recognition system is based on the Kaldi toolkit (Povey et al., 2011). Acoustic models were trained on about 300 hours of speech from French broadcast news with manual transcriptions, and are based on a chain-TDNN approach (Povey et al., 2016). The generic n-gram language models were trained on these manual transcriptions of speech, but also on newspaper articles, for a total of 1.6 billions of words. The vocabulary of the generic language model contains around 160k words. More details about language models can be fou"
2020.lrec-1.529,J97-1003,0,0.803763,"dvances in ASR allow us to imagine new applications for enhancing learning. Lectures can automatically be transcribed in text which, in turn, can be used by learners to read the courses. But, unlike handouts or any written educational resources, transcribed lecture audio can be tedious to browse, making it difficult for learners to retrieve relevant information in the absence of structural information such as topic boundaries. We present in this section our work on the thematic segmentation of oral lectures. Our thematic segmentation baseline system consists in using the TextTiling algorithm (Hearst, 1997) which is based on analysis of lexical distribution between adjacent blocs. The main reasons of choosing TextTiling is related to its simplicity and that it is an unsupervised algorithm that does not require training data (note that our corpus only contains 388 segments). In TextTiling, a block is constituted of k sentences, while a sentence is constituted of w words. Similarity is computed using a sliding window between adjacent blocks. The similarity values allow us to draw a lexical cohesion curve. Topic boundaries are detected based on the valley depth of the lexical cohesion curve. Otherw"
2020.lrec-1.529,P11-4015,0,0.0181479,"Missing"
2020.lrec-1.529,trancoso-etal-2008-lectra,0,0.0544874,"scientific fields from both speech and text processing, with language model adaptation, thematic segmentation and transcription to slide alignment. Keywords: Multimodal corpus, Educational context, Thematic segmentation, Alignment, Language model adaptation 1. Introduction With the increasing number of applications handling spontaneous speech, lecture processing has becoming an active field of research. In this particular educational context, a large number of projects have been developed, coming with different datasets. Among them, we can first quote the LECTRA corpus (Trancoso et al., 2006; Trancoso et al., 2008) dealing with classroom lectures in European Portuguese. This corpus provides the audio of lectures and their manual transcription. In addition to the oral modality, the “Spontaneous Speech Corpus and Processing Technology”(Furui et al., 2000; Furui et al., 2001) and the CHIL projects (Lamel et al., 2005) include, in addition to the audio and the transcription, the video recording of lectures. Finally, the LMELectures corpus (Riedhammer et al., 2013) is the most complete one with various modalities (audio, video and text), including the annotation of speech transcription, a segmentation in spe"
2020.lrec-1.556,I11-1142,1,0.788645,"TAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of sp"
2020.lrec-1.556,E12-1018,1,0.802446,"French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an"
2020.lrec-1.556,galibert-etal-2014-etape,0,0.730244,"seeks to locate and classify named entity mentions in unstructured text into pre-defined categories (such as person names, organizations, locations, ...). Quaero project (Grouin et al., 2011) is at the initiative of an extended definition of named entity for French data. This extended version has a multilevel tree structure, where base entities are combined to define more complex ones. With the extended definition, named entity recognition consists in the detection, the classification and the decomposition of the entities. This new definition was used for the French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011)"
2020.lrec-1.556,giraudel-etal-2012-repere,0,0.0475205,"Missing"
2020.lrec-1.556,goryainova-etal-2014-morpho,1,0.845632,") and test (7 hours). These data have manual transcriptions and are fully manually annotated with named entities concepts. Our training data were augmented with the Quaero corpus (Grouin et al., 2011). This corpus is composed of data recorded from French radio and TV stations between 1998 and 2004. These data are made up of 100 hours of speech manually transcribed and fully annotated with named entities following the Quaero annotation guideline. 5.2. Automatic speech recognition In this study, we used several corpora (ESTER 1&2 (Galliano et al., 2009), REPERE (Giraudel et al., 2012) and VERA (Goryainova et al., 2014)) for a total of around 220 hours of speech. These data are used for the acoustic model training of the kaldi ASR system of the pipeline approach. The LM of this approach was trained using the speech transcripts augmented with several French newspapers (see section 4.2.3 in (Del´eglise et al., 2009)). For ASR parts, our pipeline system and our E2E system use the same dataset except for the speech of ETAPE train dataset which is used only with our E2E approach. 6. Experiments All our experiments are evaluated on the ETAPE test set with the Slot Error Rate (SER) metric (Makhoul et al., 1999) def"
2020.lrec-1.556,gravier-etal-2012-etape,0,0.0460721,"Missing"
2020.lrec-1.556,W11-0411,1,0.682334,"to do structured NER. Finally, we compare the performances of ETAPE’s systems (state-of-the-art systems in 2012) with the performances obtained using current technologies. The results show the interest of the E2E approach, which however remains below an updated pipeline approach. Keywords: Named Entity Recognition, Automatic Speech Recognition, Tree-structured Named Entity, End-to-End 1. Introduction Named entity recognition seeks to locate and classify named entity mentions in unstructured text into pre-defined categories (such as person names, organizations, locations, ...). Quaero project (Grouin et al., 2011) is at the initiative of an extended definition of named entity for French data. This extended version has a multilevel tree structure, where base entities are combined to define more complex ones. With the extended definition, named entity recognition consists in the detection, the classification and the decomposition of the entities. This new definition was used for the French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treest"
2020.lrec-1.556,J98-4004,0,0.294477,"n 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptio"
2020.lrec-1.556,N16-1030,0,0.0999424,"r named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptions has a major impact on NER performances (Ben Jannet et al., 2015). In 2012, HMM-GMM implementations were still the stateof-the-art approaches for ASR technologies. Since this date, the great contribution of neural approaches for NER and ASR tasks were demonstrated. Recent studies (Lample et al., 2016; Ma and Hovy, 2016) improve the NER accuracy by using a combination of bidirectional Long Short-Term Memory (bLSTM) and CRF layers. Other studies (Tomashenko et al., 2016) are based on a combination of HMM and Deep Neural Network (DNN) to reach ASR state-of-the-art performances. Lately, some E2E approaches for Named Entity Recognition from speech have been proposed in (Ghannay et al., 2018). In this work, the E2E systems will learn an alignment between audio and manual transcription enriched with NE without tree-structure. Other works use End-to-End approach to map directly speech to intent i"
2020.lrec-1.556,P10-1052,0,0.0667031,"Missing"
2020.lrec-1.556,P16-1101,0,0.420815,"ition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptions has a major impact on NER performances (Ben Jannet et al., 2015). In 2012, HMM-GMM implementations were still the stateof-the-art approaches for ASR technologies. Since this date, the great contribution of neural approaches for NER and ASR tasks were demonstrated. Recent studies (Lample et al., 2016; Ma and Hovy, 2016) improve the NER accuracy by using a combination of bidirectional Long Short-Term Memory (bLSTM) and CRF layers. Other studies (Tomashenko et al., 2016) are based on a combination of HMM and Deep Neural Network (DNN) to reach ASR state-of-the-art performances. Lately, some E2E approaches for Named Entity Recognition from speech have been proposed in (Ghannay et al., 2018). In this work, the E2E systems will learn an alignment between audio and manual transcription enriched with NE without tree-structure. Other works use End-to-End approach to map directly speech to intent instead of map speech"
2020.lrec-1.556,E99-1023,0,0.156231,"de <loc.adm.town <name paris &gt; &gt; &gt;”. org.adm/loc.adm.town are Named Entities types with subtypes and kind/name are components. With the Quaero definition of named entity, NER consists in entity detection, classification and decomposition. Since this new definition is used for the French evaluation campaign ETAPE, the task in this study consists in Quaero named entity extraction from speech. 3. 3.1. problem by reducing all labels related to a word into a single one. Figure 1 illustrates an example of this concatenation. Pipelines systems 3-pass implementation Our NER systems use standard BIO2 (Sang and Veenstra, 1999) format. This standard consists of writing a column file with first the words column and then the labels column. There is one couple of word/label per line and two different sentences are separated by an empty line. The label of a word corresponds to the named entity concept in which the word is located. This label is prefixed by a ”B-” or an ”I-” depending on the position of the word in the concept. ”B-” (Begin) is used to prefixed the label of the first word and ”I-” (Inside) for all the others. ”O” (Outside) is the label used for words that are not inside a concept. Due to the structure of"
2020.lrec-1.610,W15-3202,0,0.0723262,"Missing"
2020.lrec-1.610,Q18-1008,0,0.0248364,"al male and female singular male plural male and female dual Lemma ÉJ Ôg. (pretty) I. k@ (love) Table 1: Examples showing reducing sparsity between word and lemma levels. In addition to lemmas, we decide to evaluate also words in order to see closely the gain obtained by lemmas. 4956 رواية جميلة استمتعت حقا بقراءتها Positive :) Negative :( n x k representation of review with non static channel Convolutional layer with multiple filters Max-pooling Output layer Figure 1: CNN architecture for an example review 3.3. Embedding sets Embedding models are trained with multiple parameters. (Antoniak and Mimno, 2018) found that nearest neighbors are highly sensitive to small changes in embedding training corpus. (Pierrejean and Tanguy, 2018) explored the impact of changing window size, embedding dimension and training corpora. In this work, we want to know if the type of training corpora affects SA task performance. In other words, is it better to train embeddings with task-specific (polar) corpora in SA framework? or generic corpora are more efficient? And what about corpora size? For rigorous study, we build embedding models trained with three types of corpora: polar, non polar and mixed at word and lem"
2020.lrec-1.610,W14-3623,0,0.0580902,"Missing"
2020.lrec-1.610,P14-1023,0,0.0525022,"cificity in embedding models. (Salama et al., 2018) studied the effect of incorporating morphological information to word embedding in 2 ways: (i) including POS tags with words before embedding and, (ii) performing lemma abstraction of morphological embeddings obtained in (i). (Barhoumi et al., 2019) proceeded differently and built embeddings for different Arabic lexical unit (word, token, tokenclitics, lemma, light stem and stem). 2.2. Embedding evaluation techniques Embedding evaluation techniques fall into two categories: intrinsic and extrinsic. On one hand, intrinsic evaluation methods (Baroni et al., 2014; Schnabel et al., 2015) consist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translati"
2020.lrec-1.610,C16-1228,0,0.226637,"r methodology to propose embeddings based on word and lemma in section 3. In section 4, we present our neural architecture used for Arabic SA task. We report, in section 5, the experimental framework and discuss obtained results in section 6 Finally, we conclude, in section 7, and give some outlooks to future works. 2. 2.1. Related works Sentiment analysis task Sentiment analysis research has benefited from scientific advances in deep learning techniques, and several recent works have been done with this type of learning for Arabic 1 . (Al Sallab et al., 2015) tested different deep networks. (Dahou et al., 2016; Barhoumi et al., 2018; Barhoumi et al., 2019) used a convolutional neural network (CNN) architecture. (Hassan, 2017; Heikal et al., 2018; Al-Smadi et al., 2018) used recurrent neural network (RNN) and its variants. 1 For an overview of Arabic SA field, (Al-Ayyoub et al., 2018; Al-Ayyoub et al., 2019; Badaro et al., 2019) build a complete survey. 4955 The majority of neural networks takes as input continuous vector representations of words (word embeddings). Word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2016) are the most common algorithms for learning pre-trained embedding"
2020.lrec-1.610,P17-2072,0,0.0227207,"er NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 2018; Lachraf et al., 2019), Sentiment analysis (Dahou et al., 2016; Soliman et al., 2017; Fouad et al., 2019), Information retrieval (El Mahdaouy et al., 2018), etc. Many downstream applications show the usefulness of word embeddings. For intrinsic evaluation of Arabic word embeddings, (Elrazzaz et al., 2017) is the only work up to our knowledge. It quantifies syntactic and semantic analogies in embedding spaces. In this work, we evaluate embeddings with both intrinsic and extrinsic methods within SA frame. We propose a new protocol for intrinsic evaluation showing the sentiment stability of neighbors in embedding spaces that will be detailed in section 6.1. 3. Methodology In this section, we explain specificity of Arabic language in subsection 3.1, and justify our choice of word and lemma as lexical units in subsection 3.2. Then, we present our intuitions for embedding construction in subsection"
2020.lrec-1.610,L18-1550,0,0.0295686,"55 The majority of neural networks takes as input continuous vector representations of words (word embeddings). Word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2016) are the most common algorithms for learning pre-trained embeddings. Contextualized word embeddings Elmo (Peters et al., 2018) recently appear to handle both linguistic contexts and word syntax/semantic. There are some embedding resources that are freely available for Arabic language: (Dahou et al., 2016; Soliman et al., 2017) built word embedding sets obtained by training skip-gram and CBOW versions of word2vec, (Grave et al., 2018) distribute pre-trained word vectors Arabic, trained on Common Crawl and Wikipedia using fastText. (Barhoumi et al., 2018) presents a rigorous comparison of these embedding resources and shows that their systems suffer from a low coverage of pre-trained embeddings at word level. To the best of our knowledge, (Salama et al., 2018; Barhoumi et al., 2019) are the only works dealing with Arabic specificity in embedding models. (Salama et al., 2018) studied the effect of incorporating morphological information to word embedding in 2 ways: (i) including POS tags with words before embedding and, (ii)"
2020.lrec-1.610,D14-1181,0,0.0064727,"rds, is it better to train embeddings with task-specific (polar) corpora in SA framework? or generic corpora are more efficient? And what about corpora size? For rigorous study, we build embedding models trained with three types of corpora: polar, non polar and mixed at word and lemma levels. This will be detailed in section 5.1. In addition to corpora type, we investigate the impact of epoch number used for model training on the neighborhood of polar units. 4. Description of our sentiment analysis system Several papers show that CNN architecture gives good performance for sentiment analysis (Kim, 2014; Dahou et al., 2016; Barhoumi et al., 2018). In the same way, we choose to consider a CNN architecture. We develop a CNN architecture similar to the one described in (Dahou et al., 2016) and train it according two modes: with or without adaptation of embeddings. Indeed, we test static and non static CNN learning ways (Kim, 2014) in order to evoke trainable and non trainable aspects of embeddings. Trainable embeddings obtained with non static CNN allow obtaining task-specific embeddings. They are updated while learning task system. Non trainable embeddings are obtained with static CNN, they ar"
2020.lrec-1.610,W19-4605,0,0.0243064,"nsist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 2018; Lachraf et al., 2019), Sentiment analysis (Dahou et al., 2016; Soliman et al., 2017; Fouad et al., 2019), Information retrieval (El Mahdaouy et al., 2018), etc. Many downstream applications show the usefulness of word embeddings. For intrinsic evaluation of Arabic word embeddings, (Elrazzaz et al., 2017) is the only work up to our knowledge. It quantifies syntactic and semantic analogies in embedding spaces. In this work, we evaluate embeddings with both intrinsic and extrinsic methods within SA frame. We propose a new protocol for intrinsic evaluation showing the sentiment stability of neighbors in embedding spac"
2020.lrec-1.610,W10-0204,0,0.0400086,"timentanalysis/ 5 4958 phrase) and s: a sentiment score. In this work, we collected all available sentiment lexicons up to our knowledge (Badaro et al., 2014; ElSahar and ElBeltagy, 2015; Saif M. Mohammad and Kiritchenko, 2016; Al-Moslmi et al., 2018). This represents a set of 15 lexicons constructed with different methods. The first method consists in automatically translating English lexicons. Indeed, translated resources (Saif M. Mohammad and Kiritchenko, 2016) are obtained by translating the following four English lexicons: MPQA (Wilson et al., 2005), S140 (Kiritchenko et al., 2014), NRC (Mohammad and Turney, 2010; Mohammad and Yang, 2011; Mohammad et al., 2013) and Bing liu lexicon (Hu and Liu, 2004). The second method is based on Pointwise Mutual Information (PMI) between words and two labels (positive and negative). Indeed, the sentiment orientation (SO) of a word (Mohammad and Turney, 2013) represents the difference between PMI scores. The used lexicons have various size and different structures. In fact, each word is described with different features. These features vary from one lexicon to another. (Badaro et al., 2014) built 4 616 word annotated with their part-of-speech tags, positive and negat"
2020.lrec-1.610,W11-1709,0,0.0358473,"se) and s: a sentiment score. In this work, we collected all available sentiment lexicons up to our knowledge (Badaro et al., 2014; ElSahar and ElBeltagy, 2015; Saif M. Mohammad and Kiritchenko, 2016; Al-Moslmi et al., 2018). This represents a set of 15 lexicons constructed with different methods. The first method consists in automatically translating English lexicons. Indeed, translated resources (Saif M. Mohammad and Kiritchenko, 2016) are obtained by translating the following four English lexicons: MPQA (Wilson et al., 2005), S140 (Kiritchenko et al., 2014), NRC (Mohammad and Turney, 2010; Mohammad and Yang, 2011; Mohammad et al., 2013) and Bing liu lexicon (Hu and Liu, 2004). The second method is based on Pointwise Mutual Information (PMI) between words and two labels (positive and negative). Indeed, the sentiment orientation (SO) of a word (Mohammad and Turney, 2013) represents the difference between PMI scores. The used lexicons have various size and different structures. In fact, each word is described with different features. These features vary from one lexicon to another. (Badaro et al., 2014) built 4 616 word annotated with their part-of-speech tags, positive and negative scores, offsets in Ar"
2020.lrec-1.610,S13-2053,0,0.0675151,"Missing"
2020.lrec-1.610,W16-2504,0,0.0187056,"embedding and, (ii) performing lemma abstraction of morphological embeddings obtained in (i). (Barhoumi et al., 2019) proceeded differently and built embeddings for different Arabic lexical unit (word, token, tokenclitics, lemma, light stem and stem). 2.2. Embedding evaluation techniques Embedding evaluation techniques fall into two categories: intrinsic and extrinsic. On one hand, intrinsic evaluation methods (Baroni et al., 2014; Schnabel et al., 2015) consist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 2018; Lachraf et al., 2019), Sentiment analysis (Dahou et al., 2016; Soliman et al., 2017; Fouad et al., 2019), Information retrieval (El Mahdaouy et al., 2018), e"
2020.lrec-1.610,N18-1202,0,0.0436749,", 2019) used a convolutional neural network (CNN) architecture. (Hassan, 2017; Heikal et al., 2018; Al-Smadi et al., 2018) used recurrent neural network (RNN) and its variants. 1 For an overview of Arabic SA field, (Al-Ayyoub et al., 2018; Al-Ayyoub et al., 2019; Badaro et al., 2019) build a complete survey. 4955 The majority of neural networks takes as input continuous vector representations of words (word embeddings). Word2vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2016) are the most common algorithms for learning pre-trained embeddings. Contextualized word embeddings Elmo (Peters et al., 2018) recently appear to handle both linguistic contexts and word syntax/semantic. There are some embedding resources that are freely available for Arabic language: (Dahou et al., 2016; Soliman et al., 2017) built word embedding sets obtained by training skip-gram and CBOW versions of word2vec, (Grave et al., 2018) distribute pre-trained word vectors Arabic, trained on Common Crawl and Wikipedia using fastText. (Barhoumi et al., 2018) presents a rigorous comparison of these embedding resources and shows that their systems suffer from a low coverage of pre-trained embeddings at word level. To the be"
2020.lrec-1.610,N18-4005,0,0.0183873,"educing sparsity between word and lemma levels. In addition to lemmas, we decide to evaluate also words in order to see closely the gain obtained by lemmas. 4956 رواية جميلة استمتعت حقا بقراءتها Positive :) Negative :( n x k representation of review with non static channel Convolutional layer with multiple filters Max-pooling Output layer Figure 1: CNN architecture for an example review 3.3. Embedding sets Embedding models are trained with multiple parameters. (Antoniak and Mimno, 2018) found that nearest neighbors are highly sensitive to small changes in embedding training corpus. (Pierrejean and Tanguy, 2018) explored the impact of changing window size, embedding dimension and training corpora. In this work, we want to know if the type of training corpora affects SA task performance. In other words, is it better to train embeddings with task-specific (polar) corpora in SA framework? or generic corpora are more efficient? And what about corpora size? For rigorous study, we build embedding models trained with three types of corpora: polar, non polar and mixed at word and lemma levels. This will be detailed in section 5.1. In addition to corpora type, we investigate the impact of epoch number used fo"
2020.lrec-1.610,L16-1006,0,0.0366674,"Missing"
2020.lrec-1.610,D15-1036,0,0.022541,"models. (Salama et al., 2018) studied the effect of incorporating morphological information to word embedding in 2 ways: (i) including POS tags with words before embedding and, (ii) performing lemma abstraction of morphological embeddings obtained in (i). (Barhoumi et al., 2019) proceeded differently and built embeddings for different Arabic lexical unit (word, token, tokenclitics, lemma, light stem and stem). 2.2. Embedding evaluation techniques Embedding evaluation techniques fall into two categories: intrinsic and extrinsic. On one hand, intrinsic evaluation methods (Baroni et al., 2014; Schnabel et al., 2015) consist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 201"
2020.lrec-1.610,W18-1201,0,0.018954,"hnabel et al., 2015) consist in quantifying directly various linguistic regularities in embedding space. Syntactic and semantic analogies (Mikolov et al., 2013; Nayak et al., 2016) are the most used intrinsic methods. On the other hand, extrinsic evaluation methods assess the quality of embeddings for other NLP tasks such as part-of-speech tagging, chunking, named-entity recognition, sentiment analysis, etc. The majority of works dealing with Arabic word embedding evaluation use extrinsic technique. Many Arabic embeddings models have been evaluated in applications such as machine translation (Shapiro and Duh, 2018; Lachraf et al., 2019), Sentiment analysis (Dahou et al., 2016; Soliman et al., 2017; Fouad et al., 2019), Information retrieval (El Mahdaouy et al., 2018), etc. Many downstream applications show the usefulness of word embeddings. For intrinsic evaluation of Arabic word embeddings, (Elrazzaz et al., 2017) is the only work up to our knowledge. It quantifies syntactic and semantic analogies in embedding spaces. In this work, we evaluate embeddings with both intrinsic and extrinsic methods within SA frame. We propose a new protocol for intrinsic evaluation showing the sentiment stability of neig"
2020.lrec-1.610,H05-1044,0,0.164465,"o.cloudapp.net/farasa/ https://lium.univ-lemans.fr/en/ arsentimentanalysis/ 5 4958 phrase) and s: a sentiment score. In this work, we collected all available sentiment lexicons up to our knowledge (Badaro et al., 2014; ElSahar and ElBeltagy, 2015; Saif M. Mohammad and Kiritchenko, 2016; Al-Moslmi et al., 2018). This represents a set of 15 lexicons constructed with different methods. The first method consists in automatically translating English lexicons. Indeed, translated resources (Saif M. Mohammad and Kiritchenko, 2016) are obtained by translating the following four English lexicons: MPQA (Wilson et al., 2005), S140 (Kiritchenko et al., 2014), NRC (Mohammad and Turney, 2010; Mohammad and Yang, 2011; Mohammad et al., 2013) and Bing liu lexicon (Hu and Liu, 2004). The second method is based on Pointwise Mutual Information (PMI) between words and two labels (positive and negative). Indeed, the sentiment orientation (SO) of a word (Mohammad and Turney, 2013) represents the difference between PMI scores. The used lexicons have various size and different structures. In fact, each word is described with different features. These features vary from one lexicon to another. (Badaro et al., 2014) built 4 616"
2020.lrec-1.829,W03-1004,0,0.0764248,"n-based neural approach. While such an approach could be investigated in the future, we could not consider it in this work due to the lack of reference data. (Glavas et al., 2016) use semantic relatedness graph representation of text then derive semantically coherent segments from maximal cliques of the graph. One issue of this approach is that searching for large segments in big texts requires decreasing the threshold which exponentially increases computational cost, eventually making our task intractable. 2.1. Alignment Alignment has already been studied for corpus creation. In particular, (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006) extract related segments from the Encyclopedia Britannica and Britannica Elementary (a simpler version). It is different from our work since we are looking for a total alignment, i.e. both documents must be fully aligned, not just partially extracted. Furthermore, alignment of oral speech to its written form has been studied by (Braunschweiler et al., 2010) in the context of audio books and by (Lecouteux et al., 2012) for subtitles and transcripts (e.g. of news report) in order to improve Automatic Speech Recognition engines. While such approaches sound similar to o"
2020.lrec-1.829,D18-1443,0,0.0848736,"n is the task of producing a short text that captures the most salient points of a longer one. However, a large variety of tasks could fit this definition. Many factors are critical in the summarization process, such as whether to rephrase the source (abstractiveness) or use part of the source as-is (extractiveness); the length ratio of target and source (compression factor); the source and target lengths and their variances; and the information distribution – i.e. how important information is distributed along the text. Most of summarization benchmarks (See et al., 2017; Paulus et al., 2017; Gehrmann et al., 2018) rely on news articles from CNN and DailyMail (Hermann et al., 2015; Nallapati et al., 2016) which exhibit particular characteristics such as: (i) being quite extractive i.e. picking portions of text from the source, the opposite of abstractive (Liu et al., 2018); (ii) a high compression factor with the summary being up to 10 times shorter than the source (Liu et al., 2018); (iii) a low variance in both source and target length, and (iv) concentrating information mostly at the beginning of the article: for example, papers working on the CNN-DailyMail corpus (Hermann et al., 2015; Nallapati et"
2020.lrec-1.829,S16-2016,0,0.0791663,"k robustness to atypical participant behavior (which is common in our context); (ii) to work with word embeddings in order to capture similarity between query and answer in a dialogue context (Song et al., 2016). (Alemi and Ginsparg, 2015) also explore word embedding use in segmentation by incorporating it into existing algorithms and showing improvements. (Badjatiya et al., 2018) address the segmentation task with an end-to-end attention-based neural approach. While such an approach could be investigated in the future, we could not consider it in this work due to the lack of reference data. (Glavas et al., 2016) use semantic relatedness graph representation of text then derive semantically coherent segments from maximal cliques of the graph. One issue of this approach is that searching for large segments in big texts requires decreasing the threshold which exponentially increases computational cost, eventually making our task intractable. 2.1. Alignment Alignment has already been studied for corpus creation. In particular, (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006) extract related segments from the Encyclopedia Britannica and Britannica Elementary (a simpler version). It is different from"
2020.lrec-1.829,J97-1003,0,0.828311,"ge margin (almost +4 on all considered ROUGE metrics). Source code, data and reproduction instructions can be found at: https://github.com/pltrdy/autoalign. 2. Related Work This work aims to jointly segment two related files – a transcription and a report of the same meeting – so that the i -th segment of the report actually corresponds to the j -th segment of the transcription. Since report side segmentation is simple thanks to its structure, we focus on the transcription side. Bearing that in mind, the task is similar to a linear segmentation problem, i.e. finding borders between segments. (Hearst, 1997) proposed T EXT T ILING, a linear segmentation algorithm that compares adjacent blocks of text in order to find subtopic shifts (borders between segments) using a moving window over the text and identifying borders by thresholding. C99, as proposed by (Choi, 2000), uses similarity and ranking matrices instead, then clustering to locate topic boundaries. T EXT T ILING has been extended (i) to audio signals (Banerjee and Rudnicky, 2006) but is said to lack robustness to atypical participant behavior (which is common in our context); (ii) to work with word embeddings in order to capture similarit"
2020.lrec-1.829,P17-4012,0,0.016204,"we had to grid search T EXT T ILING parameters. G RAPH S EG from (Glavas et al., 2016) has been considered, but producing long enough segments to be comparable with our work requires a low relatedness threshold, which exponentially increases the computational cost. 4.3. Summarization We trained neural summarization models on our data, first using gold set only, then incorporating automati6721 cally aligned data. Pre-processing include filtering segments based on their number of words and sentences, i.e. we consider segments if 10 ≤ #wor d s ≤ 1000 and 3 ≤ #sent ences ≤ 50. Using OpenNMT-py2 (Klein et al., 2017) we train Transformer models (Vaswani et al., 2017) similar to the baseline presented in (Ziegler et al., 2019) with the difference that we do not use any copy-mechanism. Evaluation is conducted against our public_meetings test set and uses the ROUGE-F metric (Lin, 2004). 5. 5.1. Annotator Score ↑ (mean, med i an) 1rst iteration 2rst iteration 3rd iteration 12 88 38 22 public_meetings 18.63 – 15.73 50.44 – 53.56 57.23 – 55.02 72.67 – 80.08 Table 2: Human evaluation of automatic alignments Results Automatic Alignment Evaluation Table 1 compares performances of automatic alignment models. Diagon"
2020.lrec-1.829,W04-1013,0,0.241555,"es defined by the automatic speech recognition system. Word Embeddings 3.2. Sentence Embeddings Words Text Representation and Similarity function The alignment process consists in finding, for each transcription segments T m , its related report segment Rn , in other words the function: Sliding Windows alignment(m) = n , ∀m ∈ [1, M], n ∈ [1, N] ( alignment(m) ≤ alignment(m + 1) We consider a sentence-level similarity matrix S between the transcription  and the report  such as S i , j = scor e(t i , r j ) with (t i , r j ) ∈  × . For the score function, we experimented with (i) ROUGE from (Lin, 2004); (ii) cosine similarity on t f · i d f representations ; (iii) cosine similarity based on word embedding vectors. A pool i ng function (typically a sum) is applied to word embeddings to produce sentence embeddings, as shown in figure 1. By default, both  and  are sets of sentences1 , however we also use sliding windows with © ª overlap over sentences. For each document D ∈  ,  , the k -th sliding window D Wo,s is a set of s sentences having its first (respectively last) o sentences in common with previous window (resp. next). © ª D Wo,s (k) = s ks−ko , ..., s (k+1)s−ko |s i ∈ D (1) Slidin"
2020.lrec-1.829,K16-1028,0,0.142769,"ne. However, a large variety of tasks could fit this definition. Many factors are critical in the summarization process, such as whether to rephrase the source (abstractiveness) or use part of the source as-is (extractiveness); the length ratio of target and source (compression factor); the source and target lengths and their variances; and the information distribution – i.e. how important information is distributed along the text. Most of summarization benchmarks (See et al., 2017; Paulus et al., 2017; Gehrmann et al., 2018) rely on news articles from CNN and DailyMail (Hermann et al., 2015; Nallapati et al., 2016) which exhibit particular characteristics such as: (i) being quite extractive i.e. picking portions of text from the source, the opposite of abstractive (Liu et al., 2018); (ii) a high compression factor with the summary being up to 10 times shorter than the source (Liu et al., 2018); (iii) a low variance in both source and target length, and (iv) concentrating information mostly at the beginning of the article: for example, papers working on the CNN-DailyMail corpus (Hermann et al., 2015; Nallapati et al., 2016) often truncate the article to the first 400 words of the article (See et al., 201"
2020.lrec-1.829,E06-1021,0,0.0552842,"le such an approach could be investigated in the future, we could not consider it in this work due to the lack of reference data. (Glavas et al., 2016) use semantic relatedness graph representation of text then derive semantically coherent segments from maximal cliques of the graph. One issue of this approach is that searching for large segments in big texts requires decreasing the threshold which exponentially increases computational cost, eventually making our task intractable. 2.1. Alignment Alignment has already been studied for corpus creation. In particular, (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006) extract related segments from the Encyclopedia Britannica and Britannica Elementary (a simpler version). It is different from our work since we are looking for a total alignment, i.e. both documents must be fully aligned, not just partially extracted. Furthermore, alignment of oral speech to its written form has been studied by (Braunschweiler et al., 2010) in the context of audio books and by (Lecouteux et al., 2012) for subtitles and transcripts (e.g. of news report) in order to improve Automatic Speech Recognition engines. While such approaches sound similar to ours, they mostly look for e"
2020.lrec-1.829,J02-1002,0,0.0300231,"ity matrix while ensuring – by design – that transcription and report are aligned chronologically. We introduce the alignment matrix A that, for each coordinate (i , j ) corresponds to the similarity (eventually scaled to the power of p ) plus the maximal value from its top (i , j −1) or left (i − 1, j ) neighbor coordinates : A i , j = S i , j p + max(A i −1, j , A i , j −1 ) A 1,1 = S 1,1 p (4) 1 Sentences are determined by punctuation, which is predicted by the speech recognition system on the transcription side. 3.4. Evaluation Linear segmentation performance is measured using WindowDiff (Pevzner and Hearst, 2002), which compares boundaries predicted by the algorithm to the reference in a moving window of size k . WindowDiff is based on P k (Beeferman et al., 1999) but is meant to be fairer, with respect to false negatives, number of boundaries, segment size and near miss errors. We report WindowDiff scores for our experiments. We also consider simple metrics such as the segment accuracy and word accuracy. Experience scores are micro-averaged over reference files. 4. 4.1. Experiments Bootstrapping the corpus creation To build a corpus from scratch we iterate over three phases, (i) generating pre-alignm"
2020.lrec-1.829,P17-1099,0,0.423302,"ntroduction Automatic Text Summarization is the task of producing a short text that captures the most salient points of a longer one. However, a large variety of tasks could fit this definition. Many factors are critical in the summarization process, such as whether to rephrase the source (abstractiveness) or use part of the source as-is (extractiveness); the length ratio of target and source (compression factor); the source and target lengths and their variances; and the information distribution – i.e. how important information is distributed along the text. Most of summarization benchmarks (See et al., 2017; Paulus et al., 2017; Gehrmann et al., 2018) rely on news articles from CNN and DailyMail (Hermann et al., 2015; Nallapati et al., 2016) which exhibit particular characteristics such as: (i) being quite extractive i.e. picking portions of text from the source, the opposite of abstractive (Liu et al., 2018); (ii) a high compression factor with the summary being up to 10 times shorter than the source (Liu et al., 2018); (iii) a low variance in both source and target length, and (iv) concentrating information mostly at the beginning of the article: for example, papers working on the CNN-DailyMai"
2021.iwslt-1.20,2020.emnlp-main.480,0,0.0339641,"Missing"
2021.iwslt-1.20,2020.iwslt-1.2,1,0.756555,"7 hours (it) to 189 hours (es). Translation data is part of the ASR talks for a given source language. Our experiments were performed in the constrained setting where only the provided data for the task is used. Model architecture Our system is based on the Dual-decoder Transformer (Le et al., 2020) which consists of an encoder and two decoders. This architecture jointly transcribes and translates an input speech. Each of the decoders is responsible for one task (ASR or ST) while interacting with each other. We refer the reader to the paper for further details. We initially followed Le et al. (2020) and used 12 encoder layers, 6 decoder layers, and a hidden dimension of d = 256. However, this model produced poor results. We hypothesize that with this configuration, the model capacity is too large for the dataset described in the previous section. In the end, we ended up using only 6 encoder layers and 3 decoder layers (with the same d = 256). In addition, we also trained a Transformer model having the same encoder of 6 layers but with only one decoder as the baseline (hereafter called singledecoder model). 3.3 Speech-to-text translation (ST) consists in translating a speech utterance in"
2021.iwslt-1.20,P82-1020,0,0.744682,"Missing"
2021.iwslt-1.20,D18-2012,0,0.0335627,"Missing"
2021.iwslt-1.20,2020.coling-main.314,1,0.844969,") on the test datasets of the low-resource task for the submitted system. et al., 2021), in which there are four source languages (Spanish (es), French (fr), Portuguese (pt), and Italian (it)) and five target languages (the aforementioned source languages plus English (en)). The sizes of the ASR talks range from 107 hours (it) to 189 hours (es). Translation data is part of the ASR talks for a given source language. Our experiments were performed in the constrained setting where only the provided data for the task is used. Model architecture Our system is based on the Dual-decoder Transformer (Le et al., 2020) which consists of an encoder and two decoders. This architecture jointly transcribes and translates an input speech. Each of the decoders is responsible for one task (ASR or ST) while interacting with each other. We refer the reader to the paper for further details. We initially followed Le et al. (2020) and used 12 encoder layers, 6 decoder layers, and a hidden dimension of d = 256. However, this model produced poor results. We hypothesize that with this configuration, the model capacity is too large for the dataset described in the previous section. In the end, we ended up using only 6 enco"
2021.iwslt-1.20,P12-3005,0,0.0177994,"Missing"
2021.iwslt-1.20,D15-1166,0,0.188486,"Missing"
2021.iwslt-1.20,P02-1040,0,0.109262,"Missing"
2021.iwslt-1.20,2020.aacl-demo.6,0,0.0637687,"Missing"
esteve-etal-2010-epac,bazillon-etal-2008-manual,1,\N,Missing
F12-1098,devillers-etal-2004-french,0,0.0831867,"Missing"
F12-1098,esteve-etal-2010-epac,1,0.884492,"Missing"
F12-1098,gravier-etal-2004-ester,0,0.0432159,"Missing"
F12-1098,W11-0146,1,0.886156,"Missing"
F12-1098,W11-2039,1,0.883942,"Missing"
F12-1104,esteve-etal-2010-epac,1,0.853558,"Missing"
F12-1104,N06-2021,0,0.0607599,"Missing"
L16-1046,P14-2131,0,0.135761,"qualities of the famous word embedding families, which can be different from the ones provided by works previously published in the scientific literature. Keywords: Word embeddings, benchmarking, speech processing, natural language processing 1. Introduction Word embeddings are projections in a continuous space of words supposed to preserve the semantic and syntactic similarities between them. They have been shown to be a great asset for several Natural Language Processing (NLP) tasks, like part-of-speech tagging, chunking, named entity recognition, semantic role labeling, syntactic parsing (Bansal et al., 2014a; Turian et al., 2010; Collobert et al., 2011), and also for speech processing: for instance, word embeddings were recently involved in spoken language understanding (Mesnil et al., 2015), in detection of errors in automatic transcriptions, and in calibration of confidence measures provided by an automatic speech recognition system (Ghannay et al., 2015). These word representations were introduced through the construction of neural language models (Bengio et al., 2003; Schwenk, 2013). Different approaches have been proposed to compute them from large corpora. They include neural networks (Col"
L16-1046,P12-1015,0,0.0430995,"ic questions such as adjective-to-adverb (amazing:amazingly → calm:?) and comparative (bad:worse → big:?). Overall, there are 8,869 semantic and 10,675 syntactic questions. A question is correctly answered if the proposed word is exactly the same as the correct one. The question is answered using Mikolov (Mikolov et al., 2013a) approach named 3CosAdd (addition and subtruction) in the literature. Finally, we want to evaluate the different word embeddings on a variety of word similarity tasks, based on corpora WordSim353 (Finkelstein et al., 2001), rare words (RW) (Luong et al., 2013) and, MEN (Bruni et al., 2012). These datasets contain word pairs with human similarity ratings. The evaluation of the word representations is performed by ranking the pairs according to their cosine similarities and measuring the Spearman’s rank correlation coefficient with the human judgment. 4. 4.1. Experiments Experimental setup The word embeddings described in section 2. are estimated on the annotated Gigaword corpus, which is composed of over 4 billion words. It contains dependency parses used for training w2vf-deps embeddings, and the unlabeled version is used to train the other embeddings. Note that words occurring"
L16-1046,N06-2015,0,0.0138471,"s, locations and organizations. There are 21 begin-inside-outside encoded word-level labels. The system is evaluated on the CoNLL 2003 benchmark (Tjong Kim Sang and De Meulder, 2003). • GloVe: This approach is introduced by (Pennington et al., 2014), and relies on constructing a global co-occurrence matrix of words in the corpus. The embedding vectors are based on the analysis of cooccurrences of words in a window. • Mention detection (MENT): recognizing mentions of entities for coreference resolution. There are 3 labels (begin, inside, outside). The task is performed on the Ontonotes corpus (Hovy et al., 2006) with the CoNLL 2012 split. CSLM word embeddings CSLM word embeddings are computed from unlabeled data by the CSLM toolkit (Schwenk, 2013), which estimates a feedforward neural language model. This approach projects the n−1 word indexes onto a continuous space and, from these word embeddings representations, computes the n-gram probabilities of each word in a short-list of the most Benchmark tasks NLP tasks In this sub-section, we briefly introduce the NLP tasks on which we evaluate the performance of the different word embeddings: part-of-speech tagging (POS), syntactic chunking (CHK), named"
L16-1046,P14-2050,0,0.740915,"iptions, and in calibration of confidence measures provided by an automatic speech recognition system (Ghannay et al., 2015). These word representations were introduced through the construction of neural language models (Bengio et al., 2003; Schwenk, 2013). Different approaches have been proposed to compute them from large corpora. They include neural networks (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014), dimensionality reduction on the word co-occurrence matrix (Lebret and Collobert, 2013), and explicit representation in terms of the context in which words appear (Levy and Goldberg, 2014). One particular hypothesis behind word embeddings is that they are generic representations that shall suit most applications. Many studies have focused on the evaluation of word embeddings intrinsic quality, as well as their impact when they are used as input of systems. Turian et al. (Turian et al., 2010) evaluate different types of word representations and their concatenation on the chunking and named entity recognition tasks. This work was partially funded by the European Commission through the EUMSSI project, under the contract number 611057, in the framework of the FP7-ICT-2013-10 call,"
L16-1046,Q15-1016,0,0.375486,"different types of word representations and their concatenation on the chunking and named entity recognition tasks. This work was partially funded by the European Commission through the EUMSSI project, under the contract number 611057, in the framework of the FP7-ICT-2013-10 call, by the French National Research Agency (ANR) through the VERA project, under the contract number ANR-12-BS02-006-01, and by the R´egion Pays de la Loire. The evaluation can be performed as well on the word similarity and analogical reasoning tasks, like in (Levy and Goldberg, 2014; Ji et al., 2015; Gao et al., 2014; Levy et al., 2015). Recently, the study proposed by (Levy et al., 2015), focuses on the evaluation of neural-network-inspired word embedding models (Skip-gram and GloVe) and traditional counted-based distributional models - pointwise mutual information (PMI) and Singular Value Decomposition (SVD) models-. This study reveals that the hyperparameter optimizations and certain system design choices have a considerable impact on the performance of word embeddings, rather than the embedding algorithms themselves. Moreover, it shows that, by adapting and transferring the hyperparameters into the traditional distributi"
L16-1046,W13-3512,0,0.0499144,":?), and nine types of syntactic questions such as adjective-to-adverb (amazing:amazingly → calm:?) and comparative (bad:worse → big:?). Overall, there are 8,869 semantic and 10,675 syntactic questions. A question is correctly answered if the proposed word is exactly the same as the correct one. The question is answered using Mikolov (Mikolov et al., 2013a) approach named 3CosAdd (addition and subtruction) in the literature. Finally, we want to evaluate the different word embeddings on a variety of word similarity tasks, based on corpora WordSim353 (Finkelstein et al., 2001), rare words (RW) (Luong et al., 2013) and, MEN (Bruni et al., 2012). These datasets contain word pairs with human similarity ratings. The evaluation of the word representations is performed by ranking the pairs according to their cosine similarities and measuring the Spearman’s rank correlation coefficient with the human judgment. 4. 4.1. Experiments Experimental setup The word embeddings described in section 2. are estimated on the annotated Gigaword corpus, which is composed of over 4 billion words. It contains dependency parses used for training w2vf-deps embeddings, and the unlabeled version is used to train the other embeddi"
L16-1046,J93-2004,0,0.0543816,"ith arbitrary features. This model is a generalization of the skip-gram model with negative sampling introduced by (Mikolov et al., 2013a), and it needs labeled data for training. As in (Levy and Goldberg, 2014), we derive contexts from dependency trees: a word is used to predict its governor and dependents, jointly with their dependency labels. This effectively allows for variable-size. 3. 3.1. • Part-Of-Speech Tagging (POS): categorizing words among 48 morpho-syntactic labels (noun, verb, adjective, etc.). The system is evaluated on the standard Penn Treebank benchmark train/dev/test split (Marcus et al., 1993). • Chunking (CHK): segmenting sentences in protosyntactic constituents. There are 22 begin-insideoutside encoded word-level labels. The system is evaluated on the CoNLL 2000 benchmark (Tjong Kim Sang and Buchholz, 2000). • Named Entity Recognition (NER): recognizing named entities in the text, such as persons, locations and organizations. There are 21 begin-inside-outside encoded word-level labels. The system is evaluated on the CoNLL 2003 benchmark (Tjong Kim Sang and De Meulder, 2003). • GloVe: This approach is introduced by (Pennington et al., 2014), and relies on constructing a global co-"
L16-1046,D14-1162,0,0.109932,"11), and also for speech processing: for instance, word embeddings were recently involved in spoken language understanding (Mesnil et al., 2015), in detection of errors in automatic transcriptions, and in calibration of confidence measures provided by an automatic speech recognition system (Ghannay et al., 2015). These word representations were introduced through the construction of neural language models (Bengio et al., 2003; Schwenk, 2013). Different approaches have been proposed to compute them from large corpora. They include neural networks (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014), dimensionality reduction on the word co-occurrence matrix (Lebret and Collobert, 2013), and explicit representation in terms of the context in which words appear (Levy and Goldberg, 2014). One particular hypothesis behind word embeddings is that they are generic representations that shall suit most applications. Many studies have focused on the evaluation of word embeddings intrinsic quality, as well as their impact when they are used as input of systems. Turian et al. (Turian et al., 2010) evaluate different types of word representations and their concatenation on the chunking and named ent"
L16-1046,W00-0726,0,0.167206,"derive contexts from dependency trees: a word is used to predict its governor and dependents, jointly with their dependency labels. This effectively allows for variable-size. 3. 3.1. • Part-Of-Speech Tagging (POS): categorizing words among 48 morpho-syntactic labels (noun, verb, adjective, etc.). The system is evaluated on the standard Penn Treebank benchmark train/dev/test split (Marcus et al., 1993). • Chunking (CHK): segmenting sentences in protosyntactic constituents. There are 22 begin-insideoutside encoded word-level labels. The system is evaluated on the CoNLL 2000 benchmark (Tjong Kim Sang and Buchholz, 2000). • Named Entity Recognition (NER): recognizing named entities in the text, such as persons, locations and organizations. There are 21 begin-inside-outside encoded word-level labels. The system is evaluated on the CoNLL 2003 benchmark (Tjong Kim Sang and De Meulder, 2003). • GloVe: This approach is introduced by (Pennington et al., 2014), and relies on constructing a global co-occurrence matrix of words in the corpus. The embedding vectors are based on the analysis of cooccurrences of words in a window. • Mention detection (MENT): recognizing mentions of entities for coreference resolution. Th"
L16-1046,W03-0419,0,0.231767,"Missing"
L16-1046,P10-1040,0,0.119677,"us word embedding families, which can be different from the ones provided by works previously published in the scientific literature. Keywords: Word embeddings, benchmarking, speech processing, natural language processing 1. Introduction Word embeddings are projections in a continuous space of words supposed to preserve the semantic and syntactic similarities between them. They have been shown to be a great asset for several Natural Language Processing (NLP) tasks, like part-of-speech tagging, chunking, named entity recognition, semantic role labeling, syntactic parsing (Bansal et al., 2014a; Turian et al., 2010; Collobert et al., 2011), and also for speech processing: for instance, word embeddings were recently involved in spoken language understanding (Mesnil et al., 2015), in detection of errors in automatic transcriptions, and in calibration of confidence measures provided by an automatic speech recognition system (Ghannay et al., 2015). These word representations were introduced through the construction of neural language models (Bengio et al., 2003; Schwenk, 2013). Different approaches have been proposed to compute them from large corpora. They include neural networks (Collobert et al., 2011; M"
L16-1166,bechet-etal-2012-decoda,1,0.897744,"Missing"
L16-1166,W15-4633,1,0.898256,"Missing"
L16-1166,nasr-etal-2014-automatically,1,0.806593,"Missing"
L16-1166,W15-0212,1,0.867136,"Missing"
L16-1166,bazillon-etal-2012-syntactic,1,\N,Missing
L18-1329,W15-4635,0,0.0332707,"Missing"
L18-1329,S17-2001,0,0.0131291,"domly selected from articles of the Brown corpus. These segments are grouped into 700 documents where each document is the concatenation of 10 segments. A segment is composed of the first n sentences of the original article. The ICSI corpus (Shriberg et al., 2004) (Shriberg et al., 2000) contains 75 documents transcribed automatically 1 https://lium.univ-lemans.fr/frnewslink/ Corpora for semantic textual similarity Semantic textual similarity measures the meaning similarity of texts, beyond lexical similarity. It has been treated in several evaluation campaigns in SemEval in the recent years (Cer et al., 2017). Available corpora are mainly in English, even though some have been introduced in Spanish and Arabic. In the semantic textual similarity task of SemEval, similarity is measured between short sentences, according to a scale of 5 levels, ranging from ”no relation at all” to ”paraphrase”. Another task of the SemEval campaign deals with semantic similarity in the context of Community Questions Answering (Nakov et al., 2017). In particular: one subtask addresses ”question-question” similarity, where the questions are questions posted on an english2087 speaking forum (Qatar Living Forum) which dea"
L18-1329,A00-2004,0,0.135092,"sics course recordings. In (Eisenstein and Barzilay, 2008), the authors have made available a medical book in which each section is considered as a new topic segment. In the domain of French TVBN topic segmentation (Guinaudeau, 2011) created a corpus of 57 news programs broadcasted in February and March 2007 on the French television channel France 2. It contained 1180 topic boundaries. As will be developed in section 5.1. the particularity of our corpus lies in the diversity of TVBN shows sources and formats. 2.2. 2. 2.1. Related work Corpora for topic segmentation The C99 corpus designed by (Choi, 2000) has been widely used for evaluating topic segmentation of written text. It is an artificial corpus composed of 7000 segments randomly selected from articles of the Brown corpus. These segments are grouped into 700 documents where each document is the concatenation of 10 segments. A segment is composed of the first n sentences of the original article. The ICSI corpus (Shriberg et al., 2004) (Shriberg et al., 2000) contains 75 documents transcribed automatically 1 https://lium.univ-lemans.fr/frnewslink/ Corpora for semantic textual similarity Semantic textual similarity measures the meaning sim"
L18-1329,D08-1035,0,0.0375718,"king annotations between topic segments and press articles. Therefore, this corpus is very useful for many tasks such as topic segmentation, topic titling, video linking, semantic similarity, events follow-up and topic modeling (grouping documents addressing similar topics). Section 6. presents several tasks that can be addressed with our corpus, along with evaluations of theses tasks performed on it. from meeting records (approximately one hour each). For each conversation turn the speaker, start time, end time, and word content are marked. This corpus has been exploited in several works as (Eisenstein and Barzilay, 2008) and (Galley et al., 2003). The TDT (Topic Detection and Tracking) corpus has become a standard for topic segmentation. This corpus contains English, Arabic and Chinese (Mandarin) documents. The corpus with its different versions (from TDT1 to TDT5) is used to evaluate many topic segmentation systems as (Rosenberg and Hirschberg, 2006) and (Xie et al., 2010). It is important to note that several works dedicated to topic segmentation use their own corpus (Malioutov and Barzilay, 2006), (Eisenstein and Barzilay, 2008). In (Malioutov and Barzilay, 2006), the authors have created their corpus from"
L18-1329,P03-1071,0,0.0764755,"nts and press articles. Therefore, this corpus is very useful for many tasks such as topic segmentation, topic titling, video linking, semantic similarity, events follow-up and topic modeling (grouping documents addressing similar topics). Section 6. presents several tasks that can be addressed with our corpus, along with evaluations of theses tasks performed on it. from meeting records (approximately one hour each). For each conversation turn the speaker, start time, end time, and word content are marked. This corpus has been exploited in several works as (Eisenstein and Barzilay, 2008) and (Galley et al., 2003). The TDT (Topic Detection and Tracking) corpus has become a standard for topic segmentation. This corpus contains English, Arabic and Chinese (Mandarin) documents. The corpus with its different versions (from TDT1 to TDT5) is used to evaluate many topic segmentation systems as (Rosenberg and Hirschberg, 2006) and (Xie et al., 2010). It is important to note that several works dedicated to topic segmentation use their own corpus (Malioutov and Barzilay, 2006), (Eisenstein and Barzilay, 2008). In (Malioutov and Barzilay, 2006), the authors have created their corpus from physics course recordings"
L18-1329,P13-1024,0,0.0203889,"data: the objective is to link a fragment to another fragment from the same source. Some other works attempt to link heterogeneous sources but from an alignment perspective (e.g. books and movies (Zhu et al., 2015) or video lectures and scientific papers (Mougard et al., 2015)). In the News domain there has been several studies about linking press articles with other information sources. (Aker et al., 2015) explore linking press articles and comments on the AT corpus (Das et al., 2014) which has been built from article of The Guardian. Linking press articles and Tweets have also been studied (Guo et al., 2013). Closer to our purpose is the work of (Bois et al., 2017a) who attempt to build graph representations for News browsing. The authors have collected over a 3 week period (May 20−Jun 8, 2015) a corpus of documents in French including press articles, videos (e.g. daily news from France 2, political news), and radio podcasts (e.g. news programs from France Inter, political interviews from RMC). This corpus is not distributed so far. The FrNewsLink corpus allows addressing several multi-modal linking tasks, with heterogeneous data from various sources and of various length. 3. 3.2. Press articles"
L18-1329,J97-1003,0,0.511183,"Missing"
L18-1329,P06-1004,0,0.0599047,"peaker, start time, end time, and word content are marked. This corpus has been exploited in several works as (Eisenstein and Barzilay, 2008) and (Galley et al., 2003). The TDT (Topic Detection and Tracking) corpus has become a standard for topic segmentation. This corpus contains English, Arabic and Chinese (Mandarin) documents. The corpus with its different versions (from TDT1 to TDT5) is used to evaluate many topic segmentation systems as (Rosenberg and Hirschberg, 2006) and (Xie et al., 2010). It is important to note that several works dedicated to topic segmentation use their own corpus (Malioutov and Barzilay, 2006), (Eisenstein and Barzilay, 2008). In (Malioutov and Barzilay, 2006), the authors have created their corpus from physics course recordings. In (Eisenstein and Barzilay, 2008), the authors have made available a medical book in which each section is considered as a new topic segment. In the domain of French TVBN topic segmentation (Guinaudeau, 2011) created a corpus of 57 news programs broadcasted in February and March 2007 on the French television channel France 2. It contained 1180 topic boundaries. As will be developed in section 5.1. the particularity of our corpus lies in the diversity of T"
L18-1329,N06-2032,0,0.0486759,"our corpus, along with evaluations of theses tasks performed on it. from meeting records (approximately one hour each). For each conversation turn the speaker, start time, end time, and word content are marked. This corpus has been exploited in several works as (Eisenstein and Barzilay, 2008) and (Galley et al., 2003). The TDT (Topic Detection and Tracking) corpus has become a standard for topic segmentation. This corpus contains English, Arabic and Chinese (Mandarin) documents. The corpus with its different versions (from TDT1 to TDT5) is used to evaluate many topic segmentation systems as (Rosenberg and Hirschberg, 2006) and (Xie et al., 2010). It is important to note that several works dedicated to topic segmentation use their own corpus (Malioutov and Barzilay, 2006), (Eisenstein and Barzilay, 2008). In (Malioutov and Barzilay, 2006), the authors have created their corpus from physics course recordings. In (Eisenstein and Barzilay, 2008), the authors have made available a medical book in which each section is considered as a new topic segment. In the domain of French TVBN topic segmentation (Guinaudeau, 2011) created a corpus of 57 news programs broadcasted in February and March 2007 on the French televisio"
L18-1329,W04-2319,0,0.0989752,"aries. As will be developed in section 5.1. the particularity of our corpus lies in the diversity of TVBN shows sources and formats. 2.2. 2. 2.1. Related work Corpora for topic segmentation The C99 corpus designed by (Choi, 2000) has been widely used for evaluating topic segmentation of written text. It is an artificial corpus composed of 7000 segments randomly selected from articles of the Brown corpus. These segments are grouped into 700 documents where each document is the concatenation of 10 segments. A segment is composed of the first n sentences of the original article. The ICSI corpus (Shriberg et al., 2004) (Shriberg et al., 2000) contains 75 documents transcribed automatically 1 https://lium.univ-lemans.fr/frnewslink/ Corpora for semantic textual similarity Semantic textual similarity measures the meaning similarity of texts, beyond lexical similarity. It has been treated in several evaluation campaigns in SemEval in the recent years (Cer et al., 2017). Available corpora are mainly in English, even though some have been introduced in Spanish and Arabic. In the semantic textual similarity task of SemEval, similarity is measured between short sentences, according to a scale of 5 levels, ranging f"
L18-1329,D15-1237,0,0.0626871,"Missing"
L18-1499,D14-1179,0,0.0280263,"Missing"
L18-1499,L16-1046,1,0.891952,"Missing"
L18-1499,H90-1021,0,0.587985,"ision about the current label output. A more detailed description of the NN-EDA system is given in (Simonnet et al., 2017). 3.2.3. CRF system Past experiments described in (Hahn et al., 2011) have shown that the best semantic annotation performance on manual and automatic transcriptions of the MEDIA corpus were obtained with CRF systems. More recently in (Vukotic et al., 2015), this architecture has been compared to popular bi-directional RNN (bi-RNN). The result was that CRF systems outperform a bi-RNN architecture on the MEDIA corpus, while better results were observed by biRNN on the ATIS (Hemphill et al., 1990) corpus. This is probably explained by the fact that MEDIA contains semantic contents whose mentions are more difficult to disambiguate, and CRFs make it possible to exploit complex contexts more efficiently. For the sake of comparison with the best SLU system proposed in (Hahn et al., 2011), the Wapiti toolkit was used (Lavergne et al., 2010) in our study. Nevertheless, the set of input features used by the system proposed in this paper is different from the one used in (Hahn et al., 2011). Among the novelties used in our system, we consider syntactic and ASR confidence features and our confi"
L18-1499,P10-1052,0,0.0563784,"Missing"
L18-1499,P14-2050,0,0.0321407,"hese experiments are carried on the French MEDIA cor3157 pus, on which CRF still perform better than neural approaches (Vukotic et al., 2015; Simonnet et al., 2017). 2. ASR confusability measure and simulation of ASR errors The proposed confusability measure is based on the use of linguistic and acoustic similarities. These similarities are computed from cosine similarities between linguistic and acoustic word embeddings. The linguistic word embeddings correspond to a combination through a principal component analysis (PCA) of different kinds of word embeddings: word2vecf on dependency trees (Levy and Goldberg, 2014), skip-gram provided by word2vec (Mikolov et al., 2013), and GloVe (Pennington et al., 2014), as described in (Ghannay et al., 2016). The acoustic embeddings correspond to the projection of an arbitrary or fixed dimensional speech segment in a fixeddimensional space, in a manner that preserves acoustic similarity between words. The approach used to build the acoustic word embeddings was proposed by (Bengio and Heigold, 2014). 2.2. Simulating errors To simulate ASR errors, we apply the confusability measure conf us(x, y) in order to substitute some correct words from manual transcriptions by on"
L18-1499,D14-1162,0,0.0798488,"Missing"
L18-1500,H91-1098,0,0.549958,"Missing"
L18-1500,rousseau-etal-2014-enhancing,1,0.892501,"Missing"
lefevre-etal-2012-leveraging,esteve-etal-2010-epac,1,\N,Missing
lefevre-etal-2012-leveraging,W11-0146,1,\N,Missing
lefevre-etal-2012-leveraging,W11-2039,1,\N,Missing
lefevre-etal-2012-leveraging,devillers-etal-2004-french,0,\N,Missing
masmoudi-etal-2014-corpus,N09-1045,1,\N,Missing
masmoudi-etal-2014-corpus,habash-etal-2012-conventional,1,\N,Missing
masmoudi-etal-2014-corpus,zribi-etal-2014-conventional,1,\N,Missing
rousseau-etal-2012-ted,2011.iwslt-evaluation.3,0,\N,Missing
rousseau-etal-2012-ted,2011.iwslt-evaluation.1,0,\N,Missing
rousseau-etal-2014-enhancing,P10-2041,0,\N,Missing
rousseau-etal-2014-enhancing,rousseau-etal-2012-ted,1,\N,Missing
rousseau-etal-2014-enhancing,2011.iwslt-evaluation.10,1,\N,Missing
rousseau-etal-2014-enhancing,2011.iwslt-evaluation.1,0,\N,Missing
W14-5212,W09-1501,0,0.0282413,"gines don’t need to be aware that they are being applied to an ASR view or an OCR view; they just see a regular text document. SofA-aware components, however, can explicitly work on annotations from different views and can therefore be used to integrate and combine the information coming from different sources or layers, and create new, integrated views with the output from that integration and reasoning process. 6 Flow management UIMA provides a platform for execution of analysis components (Analysis Engines or AEs), as well as for managing the flow between those components. CPE or uimaFIT7 (Ogren and Bethard, 2009) can be used to design and execute pipelines made up of a sequence of AEs (and potentially some more complex flows), and UIMA-AS8 (Asynchronous Scaleout) permits the distribution of the process among various machines or even a cluster (with the help of UIMA DUCC9 ). Analysis Engines can either be “natively” written for UIMA or can be wrappers that translate inputs and outputs for existing analysis components so they can be integrated in UIMA. All text analysis components, as well as the integration and reasoning components, will be available as UIMA AEs and can therefore be configured and exec"
W16-2511,L16-1046,1,0.832297,"Missing"
W16-2511,Q15-1016,0,0.108772,"Missing"
W16-2511,esteve-etal-2010-epac,1,0.874245,"Missing"
W17-1307,W12-3705,0,0.0375077,"Missing"
W17-1307,N12-1006,0,0.0202831,"Missing"
W17-1307,maamouri-etal-2014-developing,0,0.0608587,", 2015) (Abdulla et al., 2013) 2000 com 8861 com Syrian dialects Twitter Jeeran/qaym/ Twitter/Facebook/ Google Play (Mohammad et al., 2015) (Al-Moslmi et al., 2017) Table 1: Publically available Arabic SA datasets. Sizes are presented by the number of documents (doc) and commentaries (com). Arabic script guage Processing (NLP). The development of SA system for Tunisian dialect faces many challenges due to: (1) the very limited number of previous research conducted in this dialect, (2) the lack of freely available resources for SA in this dialect, (3) and the absence of standard orthographies (Maamouri et al., 2014) (Zribi et al., 2014) and tools dedicated to this dialect. Indeed, textual content of social networks is characterized by an intense orthographic heterogeneity which made its processing a serious challenge for NLP tools. This heterogeneity is augmented by the lack of normalization of dialectal writing system. Moreover, social networks communication is very impacted by the personal experience of each user. For instance, Tunisian users usually uses code-switching with English or French which depends of their second language. Table 2 presents an example to highlight the orthographic heterogeneity"
W17-1307,N15-1078,0,0.14164,"Missing"
W17-1307,D15-1299,0,0.334747,"Missing"
W17-1307,R11-1108,0,0.0438154,"Missing"
