2020.clinicalnlp-1.9,W07-1017,0,0.0590537,"lenging in terms of hyper-parameter tuning. Incorporating such models into clinical data processing pipelines is thus an advantage only if they can demonstrate a clear advantage over their simpler counterparts. Dipaola et al. (2019) developed linear classifiers with manually and automatically selected n-grams as features for classifying syncope in Italian medical records. A frequent target of investigations has been the 2007 Computational Medicine Challenge (CMC) dataset, focusing on automatic ICD coding in radiology reports. Both rule-based (Farkas and Szarvas, 2008) and statistical methods (Crammer et al., 2007) including neural ones (Karimi et al., 2017), have been tested and sometimes compared on this data. Karimi et al. (2017) reported that the performance of a Support Vector Machine (SVM) with term frequency–inverse document frequency (TF-IDF) bag-of-words (B OW) features remained considerably below the results of a Convolutional Neural Network (CNN) with dynamic in-domain pre-trained Word2Vec embeddings with F1 scores 3 Dataset Our data consisted of de-identified discharge summaries from Akershus University Hospital Hospital. Half of the notes were diagnosed syncope cases (S YN), the other half"
2020.clinicalnlp-1.9,N19-1423,0,0.00590899,"ation and data subset used. More recently, using another dataset, MIMIC-III (Johnson et al., 2016), experiments presented by Baumel et al. (2018) indicated that neural methods outperform linear models for the same type of multi-class classification of ICD codes, although not always by a large margin. Mascio et al. (2020) also described a comparison between linear and neural models, but for different clinical binary classification tasks (e.g. status and negation prediction) and showed that recurrent neural networks tuned for their task performed on par with the more recent, transformer models (Devlin et al., 2019). Rule-based baselines were often not included in these recent studies (Karimi et al., 2017; Baumel et al., 2018; Mascio et al., 2020), the practical advantage of different approaches therefore remains somewhat unclear compared to methods based on heuristics. Background Since medical language is rather terminologyheavy, rule-based methods can often go a long way in clinical NLP tasks and are, therefore, still rather wide-spread (Koleck et al., 2019). Statistical approaches handle better linguistic phenomena such as synonyms, code-switching and negation, however, they are computationally more e"
2020.clinicalnlp-1.9,W17-0237,0,0.0201352,"Missing"
2020.clinicalnlp-1.9,W17-2342,0,0.352212,"the clinical domain in recent years. The amount of performance gain, however, may not always be proportional to the increased complexity and decreased transparency that their use might entail, especially in data-sparse domains and target languages. The limited availability of data and its linguistic characteristics, i.e. a high density of terminology, repetitions, abbreviations and misspellings (Allvin et al., 2011), are aspects that influence greatly the efficiency of the NLP methods applied. These have been compared to some extent in previous work (Baumel et al., 2018; Mascio et al., 2020; Karimi et al., 2017), however, they are often evaluated on the same (and often limited) openly available datasets (Pestian et al., 2007; Johnson et al., 2016). The real-world utility of various approaches in clinical text processing, especially for languages other than English, however, remains still to be investigated (Ching et al., 2018). Moreover, comparison to a simple rule-based baseline is 79 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 79–84 c November 19, 2020. 2020 Association for Computational Linguistics diagnosis coding system for Norwegian, which is currently not availa"
2020.clinicalnlp-1.9,D14-1181,0,0.0126808,"ed for evaluating the proportion of syncope cases with no diagnosis code. 80 and order of these sections. A previous study (Røst et al., 2020) using EHRs from Akershus University Hospital in a text classification task has also identified a need for improving interoperability when exporting such unstructured data. 4 layer. We used binary cross-entropy loss, the Adam Optimizer, a learning rate of 0.001 and a batch size of 32. We trained for 10 epochs with early stopping based on validation accuracy and a patience of 2 epochs. We experimented with different embedding initializations, inspired by Kim (2014): a randomly initialized one (W 2 V-R) and two where weights were based on pre-trained embeddings. In one case, weights were not trainable during the learning process (static) and in the other, we continued training these weights (dynamic). This type of transfer learning consisting of fine-tuning pre-trained embeddings for a specific task is often beneficial when the size of the available training data is small (Kim, 2014). Experimental Setup The first pre-processing step consisted of tokenization with UDPipe (Straka et al., 2016). Diagnosis information reflecting the labels used for classific"
2020.clinicalnlp-1.9,L16-1680,0,0.0294882,"Missing"
2020.clinicalnlp-1.9,2020.bionlp-1.9,0,0.0821129,"eld of NLP, including the clinical domain in recent years. The amount of performance gain, however, may not always be proportional to the increased complexity and decreased transparency that their use might entail, especially in data-sparse domains and target languages. The limited availability of data and its linguistic characteristics, i.e. a high density of terminology, repetitions, abbreviations and misspellings (Allvin et al., 2011), are aspects that influence greatly the efficiency of the NLP methods applied. These have been compared to some extent in previous work (Baumel et al., 2018; Mascio et al., 2020; Karimi et al., 2017), however, they are often evaluated on the same (and often limited) openly available datasets (Pestian et al., 2007; Johnson et al., 2016). The real-world utility of various approaches in clinical text processing, especially for languages other than English, however, remains still to be investigated (Ching et al., 2018). Moreover, comparison to a simple rule-based baseline is 79 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 79–84 c November 19, 2020. 2020 Association for Computational Linguistics diagnosis coding system for Norwegian, which i"
2020.clinicalnlp-1.9,W07-1013,0,0.0889161,"e increased complexity and decreased transparency that their use might entail, especially in data-sparse domains and target languages. The limited availability of data and its linguistic characteristics, i.e. a high density of terminology, repetitions, abbreviations and misspellings (Allvin et al., 2011), are aspects that influence greatly the efficiency of the NLP methods applied. These have been compared to some extent in previous work (Baumel et al., 2018; Mascio et al., 2020; Karimi et al., 2017), however, they are often evaluated on the same (and often limited) openly available datasets (Pestian et al., 2007; Johnson et al., 2016). The real-world utility of various approaches in clinical text processing, especially for languages other than English, however, remains still to be investigated (Ching et al., 2018). Moreover, comparison to a simple rule-based baseline is 79 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 79–84 c November 19, 2020. 2020 Association for Computational Linguistics diagnosis coding system for Norwegian, which is currently not available. The research questions we investigate in this context are: (i) How do linear and neural models compare to a si"
2020.gebnlp-1.11,W19-3805,0,0.232853,"Missing"
2020.gebnlp-1.11,W19-3809,0,0.269824,"s systems, which seemed to give higher sentiment predictions for sentences associated with one given race or gender. Hoyle et al. (2019) use a generative latent-variable model to represent collocations of positive and negative adjective and verb choices, given a gendered head noun. Their analyses goes beyond qualitative analysis, and shed light on the differences on how men and women are described differently. They use a corpus of books spanning various genres, and show for example that positive adjectives used to describe women are related to their bodies more often than is the case for men. Bhaskaran and Bhallamudi (2019) analyse the existence of occupational gender stereotypes in sentiment analysis models. They show that all their tested models (BOW+logistic regression, BiLSTM, BERT (Devlin et al., 2019)) contain occupational gender stereotypes to some extent. They also show that simple models seem to show biases in training data, while contextual models might reflect biases introduced while pretraining. Voigt et al. (2018) present an annotated corpus for the gender of the addressee and the sentiment and relevance of comments. The corpus comprised comments from responses to Facebook and Reddit comments, TED t"
2020.gebnlp-1.11,2020.acl-main.485,0,0.0503272,"Missing"
2020.gebnlp-1.11,D11-1120,0,0.0496621,"idely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender modeling and the study of gender bias in English. Social psychological research on gender bias in language has shown that there are sociocultural stereotypes inherent in the language used to describe females and male"
2020.gebnlp-1.11,2020.acl-main.418,0,0.0707233,"Missing"
2020.gebnlp-1.11,2020.lrec-1.502,0,0.246408,"Missing"
2020.gebnlp-1.11,N19-1423,0,0.0247156,"ons of positive and negative adjective and verb choices, given a gendered head noun. Their analyses goes beyond qualitative analysis, and shed light on the differences on how men and women are described differently. They use a corpus of books spanning various genres, and show for example that positive adjectives used to describe women are related to their bodies more often than is the case for men. Bhaskaran and Bhallamudi (2019) analyse the existence of occupational gender stereotypes in sentiment analysis models. They show that all their tested models (BOW+logistic regression, BiLSTM, BERT (Devlin et al., 2019)) contain occupational gender stereotypes to some extent. They also show that simple models seem to show biases in training data, while contextual models might reflect biases introduced while pretraining. Voigt et al. (2018) present an annotated corpus for the gender of the addressee and the sentiment and relevance of comments. The corpus comprised comments from responses to Facebook and Reddit comments, TED talks, and posts on Fitocracy. This work has similarities to ours, since they look at the responses to gender e.g. how the content can differ based on the gender of the person being addres"
2020.gebnlp-1.11,W19-3821,0,0.119578,"Missing"
2020.gebnlp-1.11,W19-3803,0,0.471861,"t there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majo"
2020.gebnlp-1.11,W16-4301,0,0.326765,"ale authors. We investigate if this newly annotated dataset contains differences in how the works of male and female authors are critiqued, in particular in terms of positive and negative sentiment. We also explore the differences in how this is done by male and female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on a"
2020.gebnlp-1.11,W19-3621,0,0.0210253,"ns seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the more recent efforts. Another line of work has focused on investigating gender re"
2020.gebnlp-1.11,P19-1167,0,0.117481,"Missing"
2020.gebnlp-1.11,P19-1160,0,0.252388,"female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren an"
2020.gebnlp-1.11,S18-2005,0,0.0918061,"set contains differences in how the works of male and female authors are critiqued, in particular in terms of positive and negative sentiment. We also explore the differences in how this is done by male and female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Range"
2020.gebnlp-1.11,D19-1530,0,0.0945771,"ed are written by males. Also, words related to achievements with regards to literary genre or the process of publishing in general are positively used when describing the work of males, and negatively for the works of females. These observations seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference res"
2020.gebnlp-1.11,N19-1063,0,0.0208207,"ases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the more recent efforts. Another line of work has focused on investigating gender representations in corpora and models, and release gender-neutral corpora (corpora in which either the distribution of genders is balanced, or where gender stereotypes and gendered words are removed). Schofield and Mehr (2016) use film scripts to analyse the linguistic and structure variations in di"
2020.gebnlp-1.11,D10-1021,0,0.0454859,"itten by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender modeling and the study of gender bias in English. Social psychological research on gender bias in language has shown that there are sociocultural stereotypes"
2020.gebnlp-1.11,P19-2031,0,0.0185432,"itigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the more recent efforts. Another line of work has focused on investigating gender representations in corpora and models, and release gender-neutral corpora (corpora in which either the distribution of genders is balanced, or where gender stereotypes and gendered words are removed). Schofield and Mehr (2016) use film scripts to analyse the linguistic and structure variations in dialogues and how these differ based on"
2020.gebnlp-1.11,N18-2002,0,0.05749,"Missing"
2020.gebnlp-1.11,W19-6104,0,0.0375803,"egala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender modeling and the study of gender bias in English. Social psychological research on gender bias in language has shown that there are sociocultural stereotypes inherent in the language used to describe females and males (Menegatti and Rubini, 2017). While the descriptions of females tend to focus on their communal traits, males are described for their agentic traits (Menegatti and Rubini, 2017). Madera et al. (2009) show that the gender of the writer can also influence how females and males are described. They show that gender stereotypes can d"
2020.gebnlp-1.11,W16-0204,0,0.288063,"this newly annotated dataset contains differences in how the works of male and female authors are critiqued, in particular in terms of positive and negative sentiment. We also explore the differences in how this is done by male and female critics. We show that there are differences in how critics assess the works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identificatio"
2020.gebnlp-1.11,L18-1445,0,0.15174,"tations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender modeling and the study of gender bias in English. Social psychological research on gender bias in language has shown that there are sociocultural stereotypes inherent in the language used to describe females and males (Menegatti and Rubini, 2017). While the descriptions of females tend to focus on their communal traits, males are described for their agentic traits (Menegatti and Rubini, 2017). Madera et al. (2009) show that the gender of the writer can also influ"
2020.gebnlp-1.11,N18-2003,0,0.0236081,"rk of males, and negatively for the works of females. These observations seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the mo"
2020.gebnlp-1.11,D18-1521,0,0.042522,"rk of males, and negatively for the works of females. These observations seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 2018a; Rudinger et al., 2018), and machine translation (Escud´e Font and Costa-juss`a, 2019), just to name a few of the mo"
2020.gebnlp-1.11,2020.acl-main.260,0,0.279259,"he works of authors of the same or opposite gender. For example, male critics rate crime novels written by females, and romantic and sentimental works written by males, more negatively. 1 Introduction Gender is a widely studied source of bias in textual content (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018). There has been considerable previous work analyzing gender bias in NLP models and in particular, in input representations such as static and contextualized word embeddings (Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016; Zhao et al., 2020; Basta et al., 2019). Gender-annotated datasets largely focus on the gender of the author of a specific piece of text, such as a blog (Mukherjee and Liu, 2010; Liu and Mihalcea, 2007) or a tweet (Burger et al., 2011) and has given rise to considerable research focused on author gender identification (Mukherjee and Liu, 2010; Rangel and Rosso, 2019). Datasets which enable the study of response to gender in text, however, are considerably fewer (Voigt et al., 2018). With a few noteworthy exceptions (Zhao et al., 2020; Sahlgren and Olsson, 2019), a majority of previous work has focused on gender"
2020.gebnlp-1.11,P19-1161,0,0.0599984,"ievements with regards to literary genre or the process of publishing in general are positively used when describing the work of males, and negatively for the works of females. These observations seem to maintain the existing social hierarchies that tend to focus on emotional traits when describing females, while focusing on competence traits when describing males (Menegatti and Rubini, 2017). 2 Related work Much of the previous work on bias in ML models within NLP has focused on identifying biases in word embeddings and how to mitigate them (Maudslay et al., 2019; Kaneko and Bollegala, 2019; Zmigrod et al., 2019; Friedman et al., 2019; Garg et al., 2018; Bolukbasi et al., 2016), or even make them gender neutral (Zhao et al., 2018b). However, such efforts have received criticism by Gonen and Goldberg (2019) who argue that the biases have not been removed, but only “hidden” and kept at a deeper level in the embedding space. Bias has also been investigated in several other settings, like multilingual embeddings (Zhao et al., 2020), deep contextual representations (Basta et al., 2019; May et al., 2019), language models (Qian et al., 2019), coreference resolution (Cao and Daum´e III, 2020; Zhao et al., 20"
2020.lrec-1.234,W13-2322,0,0.0891257,"tically vacuous surface tokens). DM and PSD nodes are labeled with lemmas, parts of speech, and (for verbs only, in the PSD case) frame or sense identifiers; jointly, these properties define a semantic predicate. Edges represent semantic argument roles: DM mostly uses overtly order-coded labels, e.g. ARG1, ARG2, etc. Abstractly similar, PSD labels like ACT(or), PAT(ient), or ADDR(essee) indicate ‘participant’ positions in an underlying valency frame. Regarding lexical anchoring, on the opposite end of the range of frameworks in the MRP 2019 shared task is Abstract Meaning Representation (AMR; Banarescu et al. (2013)), which by design does not spell out how nodes relate to sub-strings of the underlying parser input; Figure 2 shows the same example sentence in AMR. Without an explicit relation to the surface string, several of the ‘querying’ possible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 mod (domain) mod (domain) other (ARG1)-of exemplify-01 ARG0 and op1 cotton soybean op2 op3 rice op4 et-cetera Figure 2: Sample unanchored Abstract Meaning Representation (AMR) graph for the same sentence as in Figure 1. 1903 dimensions of McDonald and Nivre (2011) will need to eith"
2020.lrec-1.234,W06-2920,0,0.111386,"tly top-performing semantic parsers. Finally, § 5. concludes the paper and discusses avenues for future research. 2. Background The following paragraphs establish relevant methodological and technological context for our work, out of necessity summarizing prior efforts in rather broad strokes. A Tale of Two Parsers One inspiration for this study is the contrastive error analysis of graph-based vs. transitionbased syntactic dependency parsers carried out by McDonald and Nivre (2007) and McDonald and Nivre (2011). Based on data from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006), they analyzed the performance of the two parser types in relation to a number of structural factors, such as sentence length, dependency length, and tree depth, as well as linguistic categories, notably parts of speech and dependency types. The analysis showed that, although the best graphbased and transition-based syntactic dependency parsers at the time achieved very similar accuracy on average, they had quite distinctive error profiles. Moreover, these differences could be explained by inherent strengths and weaknesses of the two algorithmic approaches. Thus, for exam1902 top ARG2 BV ARG1"
2020.lrec-1.234,K19-2007,0,0.338406,"these, the first two abstractly parallel the two families represented in the studies by McDonald and Nivre (2011), whereas composition-based parsing approaches are not found in syntactic parsing. We consider participating systems in the MRP 2019 competition, and, within each family of approaches, choose the top-performing systems for the PSD 1 See https://github.com/cfmrp/mtool for details. Figure 3: Distribution of sentences by length (node count), binned to ten aggregates. and DM frameworks.2 Among the transition-based systems in MRP 2019, the best-performing parser is the HIT-SCIR parser (Che et al., 2019), which is also the top-performing overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the"
2020.lrec-1.234,N19-1423,0,0.087353,"l, but degraded more because of error propagation in greedy decoding. Conversely, graph-based parsers showed a more graceful degradation thanks to global optimization and exact decoding, but had a disadvantage for local structures because of a more restricted feature model. More recently, Kulmizev et al. (2019) replicated this analysis for neural graph-based and transition-based parsers and showed that, although the distinct error profiles are still discernible, the differences are now much smaller and are further reduced by the use of deep contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). MRP 2019 The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks (Oepen et al., 2019). For the first time, this task combined formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. The training and evaluation data for the task comprised five distinct approaches— which all encode core predicate–argument structure, among other things—to the representation of sentence meaning in the form of directed graphs, packaged in a u"
2020.lrec-1.234,K19-2006,0,0.286659,"s in the MRP 2019 competition, and, within each family of approaches, choose the top-performing systems for the PSD 1 See https://github.com/cfmrp/mtool for details. Figure 3: Distribution of sentences by length (node count), binned to ten aggregates. and DM frameworks.2 Among the transition-based systems in MRP 2019, the best-performing parser is the HIT-SCIR parser (Che et al., 2019), which is also the top-performing overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smaller graph fragments. A BiLSTM sequence labeling model is used for semantic tagging of word tokens, and the BiLSTM"
2020.lrec-1.234,W17-6810,0,0.0189096,"erforming overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smaller graph fragments. A BiLSTM sequence labeling model is used for semantic tagging of word tokens, and the BiLSTM ‘feature extractor’ architecture of Kiperwasser and Goldberg (2016) is employed for predicting dependency trees, with input representations combining ELMo 2 We use framework-specific performance on PSD and DM, rather than the overall ranking across frameworks within the shared task, as the selection criterion, given that this pilot study is focused on comparing and analysing the results of parsing into these p"
2020.lrec-1.234,hajic-etal-2012-announcing,0,0.0969522,"Missing"
2020.lrec-1.234,W12-3602,1,0.781193,"niform abstract structure and serialization. This task design seeks to enable cross-framework comparison of different parsing approaches and to advance learning from complementary knowledge sources (e.g. via parameter sharing). The MRP 2019 competition received submissions from eighteen teams, and there will be a follow-up shared task, again hosted by CoNLL, in 2020. Figure 1 shows two example graphs for one sentence from the venerable Wall Street Journal (WSJ) corpus in the two bi-lexical MRP frameworks (of five total), DELPHIN MRS Bi-Lexical Dependencies (DM) of Oepen and Lønning (2006) and Ivanova et al. (2012), and Prague Semantic Dependencies (PSD) by Hajiˇc et al. (2012) and Miyao et al. (2014). The DM and PSD frameworks are bi-lexical in the MRP collection, characterized by a one-toone relation between graph nodes and surface tokens. But even within this limiting assumption, which makes these graphs formally somewhat similar to standard syntactic dependency trees, the examples in Figure 1 exhibit all the nontree properties sketched in § 1. above (reentrancies, multiple roots, and semantically vacuous surface tokens). DM and PSD nodes are labeled with lemmas, parts of speech, and (for verbs only,"
2020.lrec-1.234,Q16-1023,0,0.0230753,"1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smaller graph fragments. A BiLSTM sequence labeling model is used for semantic tagging of word tokens, and the BiLSTM ‘feature extractor’ architecture of Kiperwasser and Goldberg (2016) is employed for predicting dependency trees, with input representations combining ELMo 2 We use framework-specific performance on PSD and DM, rather than the overall ranking across frameworks within the shared task, as the selection criterion, given that this pilot study is focused on comparing and analysing the results of parsing into these particular frameworks. 1905 Figure 4: Overall MRP precision, recall, and F1 by sentence length for DM (left) and PSD (right). (Peters et al., 2018), and BERT (Devlin et al., 2019) contextualised word embeddings. Additionally, a decomposition step into sub"
2020.lrec-1.234,P19-4002,1,0.844548,"eworks) their evaluation data is publicly available (Oepen et al., 2016). All statistics in this section are against the standard 3,359-sentence PSD and DM test set, comprising gold-standard graphs drawn from the WSJ and Brown corpora. Overall and component-wise MRP evaluation scores were computed using an instrumented version of the official scorer, the mtool Swiss Army knife of meaning representation.1 4.2. Parsing Systems Our choice of models for contrastive evaluation was motivated by the characterisation of systems into three broad families of approaches, as presented, amongst others, by Koller et al. (2019) and Oepen et al. (2019): transition-, factorisation-, and composition-based parsers. Of these, the first two abstractly parallel the two families represented in the studies by McDonald and Nivre (2011), whereas composition-based parsing approaches are not found in syntactic parsing. We consider participating systems in the MRP 2019 competition, and, within each family of approaches, choose the top-performing systems for the PSD 1 See https://github.com/cfmrp/mtool for details. Figure 3: Distribution of sentences by length (node count), binned to ten aggregates. and DM frameworks.2 Among the t"
2020.lrec-1.234,J16-4009,1,0.939361,"Missing"
2020.lrec-1.234,D19-1277,1,0.894859,"Missing"
2020.lrec-1.234,K19-2004,0,0.0119447,"omposition-based parsing approaches are not found in syntactic parsing. We consider participating systems in the MRP 2019 competition, and, within each family of approaches, choose the top-performing systems for the PSD 1 See https://github.com/cfmrp/mtool for details. Figure 3: Distribution of sentences by length (node count), binned to ten aggregates. and DM frameworks.2 Among the transition-based systems in MRP 2019, the best-performing parser is the HIT-SCIR parser (Che et al., 2019), which is also the top-performing overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smalle"
2020.lrec-1.234,P19-1450,0,0.120028,"the best-performing parser is the HIT-SCIR parser (Che et al., 2019), which is also the top-performing overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smaller graph fragments. A BiLSTM sequence labeling model is used for semantic tagging of word tokens, and the BiLSTM ‘feature extractor’ architecture of Kiperwasser and Goldberg (2016) is employed for predicting dependency trees, with input representations combining ELMo 2 We use framework-specific performance on PSD and DM, rather than the overall ranking across frameworks within the shared task, as the selection criterion, given"
2020.lrec-1.234,D07-1013,1,0.852393,"structures make the parsing task much more complex—often moving from techniques with polynomial worst-case complexity to problems that are in principle NP-hard. Among other things, meaning representations transcend syntactic trees in allowing nodes with in-degree greater than one (‘reentrancies’), multiple roots, and ignoring semantically ‘vacuous’ parts of the parser input. Besides greatly increased modeling and algorithmic complexity, meaning representation parsing also poses its own set of methodological challenges for parser evaluation and diagnostics. The contrastive studies initiated by McDonald and Nivre (2007) and McDonald and Nivre (2011) have been influential in comparing the performance of two core types of approaches to syntactic dependency parsing, i.e. different families of parsing approaches. In this work, we investigate to what degree these techniques can be transferred to meaning representation parsing, and how they can be adapted and extended to reflect the formal and linguistic † We acknowledge and thank (Zhang and Clark, 2008) for inspiring our title. differences in the nature of target representations. We develop the blueprint of a general framework for quantitative diagnostic evaluati"
2020.lrec-1.234,J11-1007,1,0.442505,"sk much more complex—often moving from techniques with polynomial worst-case complexity to problems that are in principle NP-hard. Among other things, meaning representations transcend syntactic trees in allowing nodes with in-degree greater than one (‘reentrancies’), multiple roots, and ignoring semantically ‘vacuous’ parts of the parser input. Besides greatly increased modeling and algorithmic complexity, meaning representation parsing also poses its own set of methodological challenges for parser evaluation and diagnostics. The contrastive studies initiated by McDonald and Nivre (2007) and McDonald and Nivre (2011) have been influential in comparing the performance of two core types of approaches to syntactic dependency parsing, i.e. different families of parsing approaches. In this work, we investigate to what degree these techniques can be transferred to meaning representation parsing, and how they can be adapted and extended to reflect the formal and linguistic † We acknowledge and thank (Zhang and Clark, 2008) for inspiring our title. differences in the nature of target representations. We develop the blueprint of a general framework for quantitative diagnostic evaluation and experimentally seek to"
2020.lrec-1.234,S14-2056,1,0.833406,"ork comparison of different parsing approaches and to advance learning from complementary knowledge sources (e.g. via parameter sharing). The MRP 2019 competition received submissions from eighteen teams, and there will be a follow-up shared task, again hosted by CoNLL, in 2020. Figure 1 shows two example graphs for one sentence from the venerable Wall Street Journal (WSJ) corpus in the two bi-lexical MRP frameworks (of five total), DELPHIN MRS Bi-Lexical Dependencies (DM) of Oepen and Lønning (2006) and Ivanova et al. (2012), and Prague Semantic Dependencies (PSD) by Hajiˇc et al. (2012) and Miyao et al. (2014). The DM and PSD frameworks are bi-lexical in the MRP collection, characterized by a one-toone relation between graph nodes and surface tokens. But even within this limiting assumption, which makes these graphs formally somewhat similar to standard syntactic dependency trees, the examples in Figure 1 exhibit all the nontree properties sketched in § 1. above (reentrancies, multiple roots, and semantically vacuous surface tokens). DM and PSD nodes are labeled with lemmas, parts of speech, and (for verbs only, in the PSD case) frame or sense identifiers; jointly, these properties define a semanti"
2020.lrec-1.234,oepen-lonning-2006-discriminant,1,0.643309,"ected graphs, packaged in a uniform abstract structure and serialization. This task design seeks to enable cross-framework comparison of different parsing approaches and to advance learning from complementary knowledge sources (e.g. via parameter sharing). The MRP 2019 competition received submissions from eighteen teams, and there will be a follow-up shared task, again hosted by CoNLL, in 2020. Figure 1 shows two example graphs for one sentence from the venerable Wall Street Journal (WSJ) corpus in the two bi-lexical MRP frameworks (of five total), DELPHIN MRS Bi-Lexical Dependencies (DM) of Oepen and Lønning (2006) and Ivanova et al. (2012), and Prague Semantic Dependencies (PSD) by Hajiˇc et al. (2012) and Miyao et al. (2014). The DM and PSD frameworks are bi-lexical in the MRP collection, characterized by a one-toone relation between graph nodes and surface tokens. But even within this limiting assumption, which makes these graphs formally somewhat similar to standard syntactic dependency trees, the examples in Figure 1 exhibit all the nontree properties sketched in § 1. above (reentrancies, multiple roots, and semantically vacuous surface tokens). DM and PSD nodes are labeled with lemmas, parts of sp"
2020.lrec-1.234,S14-2008,1,0.942226,"Missing"
2020.lrec-1.234,L16-1630,1,0.874805,"Missing"
2020.lrec-1.234,K19-2001,1,0.908652,"Missing"
2020.lrec-1.234,N18-1202,0,0.219124,"a richer feature model, but degraded more because of error propagation in greedy decoding. Conversely, graph-based parsers showed a more graceful degradation thanks to global optimization and exact decoding, but had a disadvantage for local structures because of a more restricted feature model. More recently, Kulmizev et al. (2019) replicated this analysis for neural graph-based and transition-based parsers and showed that, although the distinct error profiles are still discernible, the differences are now much smaller and are further reduced by the use of deep contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). MRP 2019 The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks (Oepen et al., 2019). For the first time, this task combined formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. The training and evaluation data for the task comprised five distinct approaches— which all encode core predicate–argument structure, among other things—to the representation of sentence meaning in the form of directed g"
2020.lrec-1.234,D08-1059,0,0.0609159,"meaning representation parsing also poses its own set of methodological challenges for parser evaluation and diagnostics. The contrastive studies initiated by McDonald and Nivre (2007) and McDonald and Nivre (2011) have been influential in comparing the performance of two core types of approaches to syntactic dependency parsing, i.e. different families of parsing approaches. In this work, we investigate to what degree these techniques can be transferred to meaning representation parsing, and how they can be adapted and extended to reflect the formal and linguistic † We acknowledge and thank (Zhang and Clark, 2008) for inspiring our title. differences in the nature of target representations. We develop the blueprint of a general framework for quantitative diagnostic evaluation and experimentally seek to validate this proposal through a small-scale pilot study. The remainder of the paper is structured as follows: In § 2., we present the relevant background, including previous studies in syntactic parsing that provide our point of departure and the 2019 shared task on meaning representation parsing. § 3. gives a review of established dimensions of contrastive diagnostic evaluation for syntactic dependency"
2020.lrec-1.559,Q17-1010,0,0.027588,"Missing"
2020.lrec-1.559,M98-1028,0,0.123751,"not have a unique entity as a reference, but rather exploit an entity as part of their semantics. Even so, if a user of the annotated data wishes to extract all information about a particular named entity, these may still be relevant, hence should be marked separately. Entity types not included Of the categories discussed above, location, person, and organization comprise the core inventory of named entity types in the literature. They formed part of the pioneering shared tasks on NER hosted by CoNLL 2002/2003 (Sang, 2002; Sang and Meulder, 2003), MUC-6 (Grishman and Sundheim, 1995) and MUC7 (Chinchor, 1998), and have been part of all major NER annotation efforts since. However, the CoNLL shared tasks also included a fourth category for names of miscellaneous entities not belonging to the aforementioned three. During the annotation of NorNE we similarly operated with an entity type MISC, but eventually we decided to discard this label in the final release of the data as it was annotated too rarely to be useful in practice (with a total of 8 occurrences in the training data but 0 occurrences in both the development and held-out splits). The MUC shared tasks, on the other hand, additionally include"
2020.lrec-1.559,Q16-1026,0,0.0362688,"appings of the label set, different label encodings (IOB2, etc), different embedding dimensionalities, as well as joint modeling of the Bokmål and Nynorsk variants. Apart from the joint modeling, the other experiments will target only the Bokmål section of the dataset. Before moving on to the results, we first briefly outline the experimental setup. 5.1. Experimental Setup The modeling is performed using NCRF++ (Yang and Zhang, 2018) – a configurable sequence labeling toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings from the NLPL vector repository2 (Fares et al., 2017). Further details about the latter are provided in the next section. Across all experiments we fix and re-use the same random seed fo"
2020.lrec-1.559,W17-0237,1,0.824183,"g toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings from the NLPL vector repository2 (Fares et al., 2017). Further details about the latter are provided in the next section. Across all experiments we fix and re-use the same random seed for initializing the models, as to reduce the effect of non-determinism, and otherwise fix the parameters to their default values.3 For model evaluation we follow the scheme defined by the SemEval 2013 task 9.1 (Segura-Bedmar et al., 2013), using the re-implementation offered by David S. Batista.4 We report F1 for exact match on the entity level, i.e., both the predicted boundary and entity label must be correct. (This measure was dubbed strict in SemEval 2013 task"
2020.lrec-1.559,M95-1001,0,0.442269,"types of forms is that these words do not have a unique entity as a reference, but rather exploit an entity as part of their semantics. Even so, if a user of the annotated data wishes to extract all information about a particular named entity, these may still be relevant, hence should be marked separately. Entity types not included Of the categories discussed above, location, person, and organization comprise the core inventory of named entity types in the literature. They formed part of the pioneering shared tasks on NER hosted by CoNLL 2002/2003 (Sang, 2002; Sang and Meulder, 2003), MUC-6 (Grishman and Sundheim, 1995) and MUC7 (Chinchor, 1998), and have been part of all major NER annotation efforts since. However, the CoNLL shared tasks also included a fourth category for names of miscellaneous entities not belonging to the aforementioned three. During the annotation of NorNE we similarly operated with an entity type MISC, but eventually we decided to discard this label in the final release of the data as it was annotated too rarely to be useful in practice (with a total of 8 occurrences in the training data but 0 occurrences in both the development and held-out splits). The MUC shared tasks, on the other"
2020.lrec-1.559,W17-0217,1,0.902326,"Missing"
2020.lrec-1.559,N16-1030,0,0.111561,"fferent label encodings (IOB2, etc), different embedding dimensionalities, as well as joint modeling of the Bokmål and Nynorsk variants. Apart from the joint modeling, the other experiments will target only the Bokmål section of the dataset. Before moving on to the results, we first briefly outline the experimental setup. 5.1. Experimental Setup The modeling is performed using NCRF++ (Yang and Zhang, 2018) – a configurable sequence labeling toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings from the NLPL vector repository2 (Fares et al., 2017). Further details about the latter are provided in the next section. Across all experiments we fix and re-use the same random seed for initializing the models"
2020.lrec-1.559,markert-nissim-2002-towards,0,0.201038,"e special category DRV. 3.3.2. Ambiguity and Metonymy Ambiguity is a frequent source of doubt when annotating. This is often caused by so-called metonymical usage, where an entity is referred to by the name of another, closely related entity (Lakoff and Johnson, 1980). In the annotation of the NorNE corpus we have tried to resolve the ambiguity and choose the entity type based on the context (the document). We assume that every entity has a base, or literal, meaning and that when there is ambiguity, either genuinely or due to a lack of context, we resort to the literal meaning of the word(s) (Markert and Nissim, 2002). For instance, in the example in (11) below, the context does not clearly indicate whether this is a reference to a geo-political location or organization. We here assume that the location sense is the literal sense of the word Vietnam and that the organization sense is by metonymical usage, hence the annotation is GPE_LOC. (11) VietnamGPE _LOC er flott. Vietnam is great. ‘Vietnam is great.’ 3.4. Annotation Process The annotation of the NorNE corpus was performed by two trained linguists, and all documents in the Bokmål section were doubly annotated. As the second phase of the project, the Ny"
2020.lrec-1.559,L16-1699,0,0.0518445,"Missing"
2020.lrec-1.559,L16-1250,1,0.866112,"Missing"
2020.lrec-1.559,W09-1119,0,0.100265,"rained on NorNE-full using IOB, IOBE, IOBS and IOBES encodings, when evaluated on the NorNE development set, using label sets of different granularities. 5.3.2. Label Encoding The annotations of NorNE are distributed using the standard IOB(2) scheme.5 However, this can be easily mapped to other variations like IOBES, where the extra E-label indicates the end-token of an entity and the S-label indicates single, unit-length entities. (This latter scheme also goes by other names like BIOLU.) Several studies have reported slight performance increases when using the IOBES encoding compared to IOB (Ratinov and Roth, 2009; Yang et al., 2018; Reimers and Gurevych, 2017). However, it is typically not clear whether the benefits stem from adding the Eor S-labels or both. 5.3.3. Results Table 6 report experimental results for all of these variations – i.e. isolating the effects of the E- and S-labels – and across all the three different sets of entity types discussed above. There are several things to notice here. IOBE seems to have a negative performance impact regardless of the chosen label set. Also, compared to the standard IOB encoding, IOBES also has a negative impact paired with NorNEfull, but gives improved"
2020.lrec-1.559,W03-0419,0,0.847037,"r the special treatment of these types of forms is that these words do not have a unique entity as a reference, but rather exploit an entity as part of their semantics. Even so, if a user of the annotated data wishes to extract all information about a particular named entity, these may still be relevant, hence should be marked separately. Entity types not included Of the categories discussed above, location, person, and organization comprise the core inventory of named entity types in the literature. They formed part of the pioneering shared tasks on NER hosted by CoNLL 2002/2003 (Sang, 2002; Sang and Meulder, 2003), MUC-6 (Grishman and Sundheim, 1995) and MUC7 (Chinchor, 1998), and have been part of all major NER annotation efforts since. However, the CoNLL shared tasks also included a fourth category for names of miscellaneous entities not belonging to the aforementioned three. During the annotation of NorNE we similarly operated with an entity type MISC, but eventually we decided to discard this label in the final release of the data as it was annotated too rarely to be useful in practice (with a total of 8 occurrences in the training data but 0 occurrences in both the development and held-out splits)"
2020.lrec-1.559,W02-2024,0,0.473641,"he reason for the special treatment of these types of forms is that these words do not have a unique entity as a reference, but rather exploit an entity as part of their semantics. Even so, if a user of the annotated data wishes to extract all information about a particular named entity, these may still be relevant, hence should be marked separately. Entity types not included Of the categories discussed above, location, person, and organization comprise the core inventory of named entity types in the literature. They formed part of the pioneering shared tasks on NER hosted by CoNLL 2002/2003 (Sang, 2002; Sang and Meulder, 2003), MUC-6 (Grishman and Sundheim, 1995) and MUC7 (Chinchor, 1998), and have been part of all major NER annotation efforts since. However, the CoNLL shared tasks also included a fourth category for names of miscellaneous entities not belonging to the aforementioned three. During the annotation of NorNE we similarly operated with an entity type MISC, but eventually we decided to discard this label in the final release of the data as it was annotated too rarely to be useful in practice (with a total of 8 occurrences in the training data but 0 occurrences in both the develop"
2020.lrec-1.559,solberg-etal-2014-norwegian,1,0.83317,"Missing"
2020.lrec-1.559,E12-2021,0,0.113855,"Missing"
2020.lrec-1.559,W17-0201,1,0.902569,"Missing"
2020.lrec-1.559,P18-4013,0,0.0148249,"rimental Results and Analysis In this section we present some preliminary experimental results for named entity recognition using NorNE. We investigate the effects of using different mappings of the label set, different label encodings (IOB2, etc), different embedding dimensionalities, as well as joint modeling of the Bokmål and Nynorsk variants. Apart from the joint modeling, the other experiments will target only the Bokmål section of the dataset. Before moving on to the results, we first briefly outline the experimental setup. 5.1. Experimental Setup The modeling is performed using NCRF++ (Yang and Zhang, 2018) – a configurable sequence labeling toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings"
2020.lrec-1.559,C18-1327,0,0.0774336,"for named entity recognition using NorNE. We investigate the effects of using different mappings of the label set, different label encodings (IOB2, etc), different embedding dimensionalities, as well as joint modeling of the Bokmål and Nynorsk variants. Apart from the joint modeling, the other experiments will target only the Bokmål section of the dataset. Before moving on to the results, we first briefly outline the experimental setup. 5.1. Experimental Setup The modeling is performed using NCRF++ (Yang and Zhang, 2018) – a configurable sequence labeling toolkit built upon PyTorch. Following Yang et al. (2018), our particular model configuration is similar to the architecture of Chiu and Nichols (2016) and Lample et al. (2016), achieving results that are close to state-of-the-art for English on the CoNLL-2003 dataset: it combines a characterlevel CNN and a word-level BiLSTM, finally feeding into a CRF inference layer. The input to the word-level BiLSTM is provided by the concatenation of (1) the character sequence representations from the CNN using max-pooling in addition and (2) pre-trained word embeddings from the NLPL vector repository2 (Fares et al., 2017). Further details about the latter are"
2020.lrec-1.618,L16-1429,0,0.026579,"Missing"
2020.lrec-1.618,P19-2035,0,0.0284519,"reatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific models for fine-grained sentiment. Liang et al. (2019) propose an aspect-specific gate to improve GRUs. 3. Annotations In the following we present our fine-grained sentiment annotation effort in more detail. We provide an overview of the annotation guidelines and present statistics on interannotator agreement. The complete set of guidelines is distributed with"
2020.lrec-1.618,L18-1104,1,0.847384,"olar Expression Polarity Type Positive Negative Evaluative Fact-Implied Non-Personal Intensity Slight Standard Strong Target H ol de r Not-On-Topic Re l Strong Postitive Target Relation Denne this TARGET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Person Implicit Figure 1: Annotation schema for the NoReCfine dataset. ally, there has been an increased effort to create fine-grained resources for low-resource languages, such as Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model pe"
2020.lrec-1.618,P19-1052,0,0.0246947,"creased effort to create fine-grained resources for low-resource languages, such as Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a wei"
2020.lrec-1.618,N19-1423,0,0.0256568,"Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there"
2020.lrec-1.618,W17-0237,1,0.867604,"This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored. Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam (Kingma and Ba, 2014) for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layers. The word embeddings are 100 dimensional fastText SkipGram (Bojanowski et al., 2016) vectors trained on the NoWaC corpus (Guevara, 2010) and made available from the NLPL vector repository (Fares et al., 2017).3 The pre-trained embeddings are further fine-tuned during training. We report held-out test results for the model that achieves the best performance on the development set and use the standard train/development/test split provided with the dataset (shown in Table 3). All results are reported using the Proportional and Binary precision, recall and F1 scores, computed as described in Section 3.6 above. 5.2. 2 In follow-up work we plan to further enrich the annotations with additional compositional information relevant to sentiment, most importantly negation but also other forms of valence shif"
2020.lrec-1.618,W10-1501,0,0.039701,"the binary polarity of the latter, giving us nine tags in total.2 This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored. Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam (Kingma and Ba, 2014) for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layers. The word embeddings are 100 dimensional fastText SkipGram (Bojanowski et al., 2016) vectors trained on the NoWaC corpus (Guevara, 2010) and made available from the NLPL vector repository (Fares et al., 2017).3 The pre-trained embeddings are further fine-tuned during training. We report held-out test results for the model that achieves the best performance on the development set and use the standard train/development/test split provided with the dataset (shown in Table 3). All results are reported using the Proportional and Binary precision, recall and F1 scores, computed as described in Section 3.6 above. 5.2. 2 In follow-up work we plan to further enrich the annotations with additional compositional information relevant to s"
2020.lrec-1.618,P19-1048,0,0.0654472,"; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific models for fine-grained sentiment. Liang et al. (2019) propose an aspect-specific gate to improve GRUs. 3. Annotations In the following we present our fine-grained sentiment annotation effort in"
2020.lrec-1.618,P19-1051,0,0.0704886,"ts for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific model"
2020.lrec-1.618,P16-1087,0,0.0418271,"Missing"
2020.lrec-1.618,W16-0410,0,0.0247285,"the distribution of polarity labels and their intensity scores. We see that the intensities are clearly dominated by standard strength, while there are also 1270 strong labels for positive. Regardless of intensity, we see that positive valence is more prominent than negative, and this reflects a similar skew for the document-level ratings in this data (Velldal et al., 2018). The slight intensity is infrequent, with 190 positive and 330 negative polar expressions with this label. The relative difference here can be explained by the tendency to hedge negative statements more than positive ones (Kiritchenko and Mohammad, 2016). Slight negative is the minority class, with only 190 examples, followed by strong negative with 292 examples. Overall, the distribution of intensity Figure 9: Distribution of labels and intensities. scores in NoReCfine is very similar to what is reported for other fine-grained sentiment datasets for English and Dutch (Van de Kauter et al., 2015). As we can see from Table 3, the average number of tokens spanned by a polar expression is 4.6. Interestingly, if we break this number down further, we find that the negative expressions are on average longer than the positives for all intensities: w"
2020.lrec-1.618,klinger-cimiano-2014-usage,0,0.0798025,"r annotation scheme is followed by Van de Kauter et al. (2015), working on financial news texts in Dutch and English, also taking account of implicit expressions of sentiment in polar facts. The SemEval 2014 shared task (Pontiki et al., 2014) proposes a different annotation scheme. Given an English tweet, the annotators identify targets, the aspect category they belong to, and the polarity expressed towards the target. They do not annotate holders or polar expressions. While most fine-grained sentiment datasets are in English, there are datasets available in several languages, such as German (Klinger and Cimiano, 2014), Czech (Steinberger et al., 2014), Arabic, Chinese, Dutch, French, Russian, Spanish, Turkish (Pontiki et al., 2016), Hungarian (Szabó et al., 2016), and Hindi (Akhtar et al., 2016). Addition5025 Polar Expression Polarity Type Positive Negative Evaluative Fact-Implied Non-Personal Intensity Slight Standard Strong Target H ol de r Not-On-Topic Re l Strong Postitive Target Relation Denne this TARGET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Pers"
2020.lrec-1.618,N16-1030,0,0.027119,"eriments To provide an idea of the difficulty of the task, here we report some preliminary experimental results for the new dataset, intended as benchmarks for further experiments. Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results. 5.1. Experimental Setup We train a Bidirectional LSTM with a CRF inference layer, which has shown to be competitive for several other sequence labeling tasks (Huang et al., 2015; Lample et al., 2016; Panchendrarajan and Amaresan, 2018). We use the IOB2 label encoding for sources, targets, and polar expressions, including the binary polarity of the latter, giving us nine tags in total.2 This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored. Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam (Kingma and Ba, 2014) for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layer"
2020.lrec-1.618,D19-1559,0,0.0264073,"ches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific models for fine-grained sentiment. Liang et al. (2019) propose an aspect-specific gate to improve GRUs. 3. Annotations In the following we present our fine-grained sentiment annotation effort in more detail. We provide an overview of the annotation guidelines and present statistics on interannotator agreement. The complete set of guidelines is distributed with the corpus. Sentence-level annotations We build on the sentencelevel annotation of evaluative sentences in the NoReCeval corpus (Mæhlum et al., 2019), where two types of evaluative sentences were annotated: simple evaluative sentences (labeled EVAL), or the special case of evaluative factim"
2020.lrec-1.618,W19-6113,1,0.739617,"wegian 1. Introduction In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, analysing opinions in terms of their polar expressions, targets, and holders. The dataset, including the annotation guidelines, is made publicly available1 and is the first of its kind for Norwegian. The underlying texts are taken from the Norwegian Review Corpus (NoReC) (Velldal et al., 2018) – a corpus of professionally authored reviews across a wide variety of domains, including literature, video games, music, various product categories, movies, TV-series, restaurants, etc. In Mæhlum et al. (2019), a subset of the documents, dubbed NoReCeval , were annotated at the sentence-level, indicating whether a sentence contains an evaluation or not. These prior annotations strictly indicated evaluativeness and did not include negative or positive polarity, as this can be mixed at the sentence-level. In the current work, the previous annotation effort has been considerably extended to include the span of polar expressions and the corresponding targets and holders of the opinion. We also indicate the intensity of the positive or negative polarity on a three-point scale, along with a number of oth"
2020.lrec-1.618,N18-1054,0,0.0377143,"Missing"
2020.lrec-1.618,Y18-1061,0,0.0270771,"n idea of the difficulty of the task, here we report some preliminary experimental results for the new dataset, intended as benchmarks for further experiments. Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results. 5.1. Experimental Setup We train a Bidirectional LSTM with a CRF inference layer, which has shown to be competitive for several other sequence labeling tasks (Huang et al., 2015; Lample et al., 2016; Panchendrarajan and Amaresan, 2018). We use the IOB2 label encoding for sources, targets, and polar expressions, including the binary polarity of the latter, giving us nine tags in total.2 This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored. Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam (Kingma and Ba, 2014) for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layers. The word embeddings are 100 dimens"
2020.lrec-1.618,N18-1202,0,0.0404156,"e languages, such as Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment ta"
2020.lrec-1.618,S14-2004,0,0.108751,"Missing"
2020.lrec-1.618,S16-1002,0,0.0475557,"Missing"
2020.lrec-1.618,W14-2605,0,0.019705,"Van de Kauter et al. (2015), working on financial news texts in Dutch and English, also taking account of implicit expressions of sentiment in polar facts. The SemEval 2014 shared task (Pontiki et al., 2014) proposes a different annotation scheme. Given an English tweet, the annotators identify targets, the aspect category they belong to, and the polarity expressed towards the target. They do not annotate holders or polar expressions. While most fine-grained sentiment datasets are in English, there are datasets available in several languages, such as German (Klinger and Cimiano, 2014), Czech (Steinberger et al., 2014), Arabic, Chinese, Dutch, French, Russian, Spanish, Turkish (Pontiki et al., 2016), Hungarian (Szabó et al., 2016), and Hindi (Akhtar et al., 2016). Addition5025 Polar Expression Polarity Type Positive Negative Evaluative Fact-Implied Non-Personal Intensity Slight Standard Strong Target H ol de r Not-On-Topic Re l Strong Postitive Target Relation Denne this TARGET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Person Implicit Figure 1: Annotation s"
2020.lrec-1.618,E12-2021,0,0.164792,"Missing"
2020.lrec-1.618,L16-1459,0,0.0133461,"ssions of sentiment in polar facts. The SemEval 2014 shared task (Pontiki et al., 2014) proposes a different annotation scheme. Given an English tweet, the annotators identify targets, the aspect category they belong to, and the polarity expressed towards the target. They do not annotate holders or polar expressions. While most fine-grained sentiment datasets are in English, there are datasets available in several languages, such as German (Klinger and Cimiano, 2014), Czech (Steinberger et al., 2014), Arabic, Chinese, Dutch, French, Russian, Spanish, Turkish (Pontiki et al., 2016), Hungarian (Szabó et al., 2016), and Hindi (Akhtar et al., 2016). Addition5025 Polar Expression Polarity Type Positive Negative Evaluative Fact-Implied Non-Personal Intensity Slight Standard Strong Target H ol de r Not-On-Topic Re l Strong Postitive Target Relation Denne this TARGET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Person Implicit Figure 1: Annotation schema for the NoReCfine dataset. ally, there has been an increased effort to create fine-grained resources for low"
2020.lrec-1.618,P19-1053,0,0.0236808,"rchitectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018; He et al., 2019; Li et al., 2019). Alternatively, researchers have proposed attention-based methods which are adapted for fine-grained sentiment (Tang et al., 2019; Bao et al., 2019). These methods make use of an attention mechanism (Bahdanau et al., 2014) which allows the model to learn a weighted representation of sentences with respect to sentiment targets. Finally, there are approaches which create task-specific models for fine-grained sentiment. Liang et al. (2019) propose an aspect-specific gate to improve GRUs. 3. Annotations In the following we present our fine-grained sentiment annotation effort in more detail. We provide an overview of the annotation guidelines and present statistics on interannotator agreement. The complete set of guidelines"
2020.lrec-1.618,P10-1059,0,0.199893,"at polarity is directed towards entities (either implicitly or explicitly mentioned). In this section we provide a brief overview of related work, first in terms of datasets and then modeling. 2.1. Datasets One of the earliest datasets for fine-grained opinion mining is the MPQA corpus (Wiebe et al., 2005), which contains annotations of private states in English-language texts taken from the news domain. The authors propose a detailed annotation scheme in which annotators identify subjective expressions, as well as their targets and holders. Working with sentiment in English consumer reviews, Toprak et al. (2010) annotate targets, holders and polar expressions, in addition to modifiers like negation, intensifiers and diminishers. The intensity of the polarity is marked on a three-point scale (weak, average, strong). In addition to annotating explicit expressions of subjective opinions, Toprak et al. (2010) annotate polar facts that may imply an evaluative opinion. A similar annotation scheme is followed by Van de Kauter et al. (2015), working on financial news texts in Dutch and English, also taking account of implicit expressions of sentiment in polar facts. The SemEval 2014 shared task (Pontiki et a"
2020.lrec-1.618,L18-1661,1,0.433552,"analysis of inter-annotator agreement. We also report the first experimental results on the dataset, intended as a preliminary benchmark for further experiments. Keywords: Sentiment analysis, opinion mining, Norwegian 1. Introduction In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, analysing opinions in terms of their polar expressions, targets, and holders. The dataset, including the annotation guidelines, is made publicly available1 and is the first of its kind for Norwegian. The underlying texts are taken from the Norwegian Review Corpus (NoReC) (Velldal et al., 2018) – a corpus of professionally authored reviews across a wide variety of domains, including literature, video games, music, various product categories, movies, TV-series, restaurants, etc. In Mæhlum et al. (2019), a subset of the documents, dubbed NoReCeval , were annotated at the sentence-level, indicating whether a sentence contains an evaluation or not. These prior annotations strictly indicated evaluativeness and did not include negative or positive polarity, as this can be mixed at the sentence-level. In the current work, the previous annotation effort has been considerably extended to inc"
2020.lrec-1.618,P13-1161,0,0.0169767,"ET disken disk er is P OLAR svært stillegående very quiet-going Figure 2: Annotation of an EVAL sentence (transl. ‘This disk runs very quietly’). Target-is-General Implicit at io n Holder Not-First-Person Implicit Figure 1: Annotation schema for the NoReCfine dataset. ally, there has been an increased effort to create fine-grained resources for low-resource languages, such as Basque and Catalan (Barnes et al., 2018). No datasets for fine-grained SA have previously been created for Norwegian, however. 2.2. Modeling Fine-grained sentiment is most often approached as a sequence labeling problem (Yang and Cardie, 2013; Vo and Zhang, 2015) or simplified to a classification problem when the target or aspect is given (Pontiki et al., 2014). Although the specific architectures vary greatly, state-ofthe-art methods for fine-grained sentiment analysis tend to be transfer-learning approaches (Chen and Qian, 2019), often using pre-trained language models (Peters et al., 2018; Devlin et al., 2019) to improve model performance (Hu et al., 2019). Additionally, several approaches use multitask learning to incorporate useful information from similar tasks into fine-grained sentiment models (Marasovi´c and Frank, 2018;"
2020.multilingualbio-1.2,P09-1113,0,0.0315888,"Missing"
2020.multilingualbio-1.2,D19-6125,1,0.874715,"Missing"
2020.multilingualbio-1.2,W18-5613,1,0.80491,"for use in NLP from such domain-specific resources (Johnson, 1999; Liu et al., 2012). 1 https://ehelse.no/kodeverk/ laboratoriekodeverket 9 Category CONDITION the International Classification of Primary Care (ICPC-2), which includes diagnosis terms as well as health problem and medical procedure names. The FEST (Forskrivnings- og ekspedisjonsstøtte, ‘Prescribing and dispensing support’) database2 contains information about all medicines and other goods that can be prescribed in Norway. FEST is a publicly available resource published by Statens legemiddelverk ‘The Norwegian Medicines Agency’. Rama et al. (2018) present a corpus of synthetically produced clinical statements about family history in Norwegian (here dubbed FAM-HIST). The corpus is annotated with clinical entities relating to family history, such as Family Member, Condition and Event, as well as relations between these. 4. DISCIPLINE MICROORG PERSON PROCEDURE SUBSTANCE TOOL Table 1: Suffix mapping. Automatic Dictionary Entry Mapping Method To create a list of keywords for the mapping, we inspect the 200 most frequent nouns in the definitions and manually map the ones with a strong indication of a single category. We complement this with"
2020.multilingualbio-1.2,D18-1230,0,0.0138563,"on Automated extraction of medical entities from clinical text has been the topic of several research efforts more recently, a majority aimed at English (Xu et al., 2010; Jagannatha and Yu, 2016) and Chinese clinical text (Wu et al., 2018). For a language that is very closely related to Norwegian, Skeppstedt et al. (2014) developed and evaluated an entity detection system for Findings, Disorders and Body Parts in Swedish. In order to alleviate the need for manual annotation, distant supervision has recently been applied also to entity recognition in the medical domain for English and Chinese (Shang et al., 2018; Nooralahzadeh et al., 2019). Named Entity Recognition (NER) is a common task within the area of clinical Natural Language Processing (NLP) with the aim of extracting critical information such as diseases and treatments from unstructured texts (Friedman et al., 1994; Xu et al., 2010; Jagannatha and Yu, 2016). Current neural approaches to NER typically require a large amount of annotated data for a reliable performance (Ma and Hovy, 2016; Lample et al., 2016). Distant supervision (Mintz et al., 2009), however, relaxes this constraint on the training data size thanks to the combined use of info"
2020.multilingualbio-1.2,L16-1680,0,0.0299493,"Missing"
2020.multilingualbio-1.2,Q16-1002,0,0.0658084,"Missing"
2020.multilingualbio-1.2,N16-1056,0,0.0465161,"Missing"
2020.multilingualbio-1.2,N16-1030,0,0.0103746,"manual annotation, distant supervision has recently been applied also to entity recognition in the medical domain for English and Chinese (Shang et al., 2018; Nooralahzadeh et al., 2019). Named Entity Recognition (NER) is a common task within the area of clinical Natural Language Processing (NLP) with the aim of extracting critical information such as diseases and treatments from unstructured texts (Friedman et al., 1994; Xu et al., 2010; Jagannatha and Yu, 2016). Current neural approaches to NER typically require a large amount of annotated data for a reliable performance (Ma and Hovy, 2016; Lample et al., 2016). Distant supervision (Mintz et al., 2009), however, relaxes this constraint on the training data size thanks to the combined use of information from lexical resources, a small amount of training data and large amounts of raw data. This technique has been successfully applied also in the biomedical and clinical domain (Fries et al., 2017; Shang et al., 2018). In absence of even a small amount of annotated data, categorized lexical resources can also be used as gazetteers in rule-based approaches. There is currently no large and freely available lexical resource with categorized entity types fo"
2020.multilingualbio-1.2,P16-1101,0,0.0202376,"viate the need for manual annotation, distant supervision has recently been applied also to entity recognition in the medical domain for English and Chinese (Shang et al., 2018; Nooralahzadeh et al., 2019). Named Entity Recognition (NER) is a common task within the area of clinical Natural Language Processing (NLP) with the aim of extracting critical information such as diseases and treatments from unstructured texts (Friedman et al., 1994; Xu et al., 2010; Jagannatha and Yu, 2016). Current neural approaches to NER typically require a large amount of annotated data for a reliable performance (Ma and Hovy, 2016; Lample et al., 2016). Distant supervision (Mintz et al., 2009), however, relaxes this constraint on the training data size thanks to the combined use of information from lexical resources, a small amount of training data and large amounts of raw data. This technique has been successfully applied also in the biomedical and clinical domain (Fries et al., 2017; Shang et al., 2018). In absence of even a small amount of annotated data, categorized lexical resources can also be used as gazetteers in rule-based approaches. There is currently no large and freely available lexical resource with categ"
2020.multilingualbio-1.2,P86-1018,0,0.409153,"Missing"
2021.acl-long.263,P15-1040,0,0.0606129,"Missing"
2021.acl-long.263,2020.acl-main.421,0,0.0150168,"1 the gains are more limited (3.3 pp/ 3.8 pp) and similarly for NSF1 and SF1 (3.6 pp/ 3.9 pp). The gains are 3394 NoReCFine MultiBEU MultiBCA MPQA DSUnis 57.0 (1.5) 75.7 (0.8) 71.7 (2.4) 38.5 (1.4) 44.5 (2.4) Table 6: Polarity F1 scores (unweighted and weighted) of models augmented with mBERT on the head-final setup. We report average and standard deviation over 5 runs. largest for the English datasets (MPQA, DSUnis ) followed by NoReCFine , and finally MultiBCA and MultiBEU . This corroborates the bias towards English and similar languages that has been found in multilingual language models (Artetxe et al., 2020; Conneau et al., 2020) and motivates the need for language-specific contextualized embeddings. 7.4 Analysis of polarity predictions Acknowledgements This work has been carried out as part of the SANT project (Sentiment Analysis for Norwegian Text), funded by the Research Council of Norway (grant number 270908). The computations were performed on resources provided by UNINETT Sigma2 - the National Infrastructure for High Performance Computing and Data Storage in Norway. References In this section we zoom in on polarity, in order to quantify how well models perform at predicting only polarity."
2021.acl-long.263,L18-1104,1,0.902863,"Missing"
2021.acl-long.263,D12-1091,0,0.015329,"entations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository8 (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF1 on the dev set (Targeted F1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Appendix) and run all of our models five times with different random seeds and report the mean (standard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference between the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3 shows the results for all datasets. On NoReCFine , the baselines IMN, RACL, and RACL-BERT perform well at extracting targets (35.9, 45.6, and 47.2 F1 , respectively) and expressions (48.7/55.4/56.3), but struggle with the full targeted sentiment task (18.0/20.1/30.3). The graphbased models extract targets better (50.1/54.8) and have comparable scores for expressions (54.4/55.5). The holder extraction scores have a similar range (51.1/60.4). These patterns hold throughout the other datasets, where th"
2021.acl-long.263,W17-0237,1,0.844981,"ty (holder, target, expression, polarity). A true positive is defined as an exact match at graph-level, weighting the overlap in predicted and gold spans for each element, averaged across all three spans. For precision we weight the number of correctly predicted tokens divided by the total number of predicted tokens (for recall, we divide instead by the number of gold tokens). We allow for empty holders and targets. 3392 6 Experiments All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository8 (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF1 on the dev set (Targeted F1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Appendix) and run all of our models five times with different random seeds and report the mean (standard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference between the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3"
2021.acl-long.263,P19-1048,0,0.110331,"s have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016) augment the ABSA datasets with sentiment expressions, but provide no details on the annotation process or any inter-annotator agreement. He et al. (2019) make use of this data and propose a multi-layer CNN (IMN) to c"
2021.acl-long.263,P11-1016,0,0.0618142,"entiment lexicons, dependency parsers, named-entity taggers) (Choi et al., 2006; Yang and Cardie, 2012) are strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction ("
2021.acl-long.263,P16-1087,0,0.162601,"between these elements, and v) assigning polarity. Previous work on information extraction has used pipeline methods which first extract the holders, targets, and expressions (tasks i - iii) and subsequently predict their relations (task iv), mostly on the MPQA dataset (Wiebe et al., 2005). CRFs and a number of external resources (sentiment lexicons, dependency parsers, named-entity taggers) (Choi et al., 2006; Yang and Cardie, 2012) are strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and mult"
2021.acl-long.263,J16-4009,1,0.787534,"nes, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representatio"
2021.acl-long.263,2020.iwpt-1.3,1,0.851073,"rall resolution of sentiment, or do not take into account the inter-dependencies of the various sub-tasks. As such, we propose a unified approach to structured sentiment which jointly predicts all elements of an opinion tuple and their relations. Moreover, we cast sentiment analysis as a dependency graph parsing problem, where the sentiment expression is the root node, and the other elements have arcs which model the relationships between them. This methodology also enables us to take advantage of recent improvements in semantic dependency parsing (Dozat and Manning, 2018; Oepen et al., 2020; Kurtz et al., 2020) to efficiently learn a sentiment graph parser. This perspective also allows us to unify a number of approaches, including targeted, and opinion tuple mining. We aim to answer RQ1: whether graph-based approaches to structured sentiment outperform state-of-the-art sequence labeling approaches, and RQ2: how to best encode structured sentiment as parsing graphs. We perform experiments on five standard datasets in four languages (English, Norwegian, Basque, Catalan) and show that graph-based approaches outperform state-ofthe-art baselines on all datasets on several standard metrics, as well as our"
2021.acl-long.263,S12-1042,1,0.750098,"ationship between target and expression. Finally, the recently proposed aspect sentiment triplet extraction (Peng et al., 2019; ?) attempts to extract targets, expressions and their polarity. However, the datasets used are unlikely to be adequate, as they augment available targeted datasets, but do not report annotation guidelines, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to pre"
2021.acl-long.263,D19-5505,0,0.0118353,"(Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016) augment the ABSA datasets with sentiment expressions, but provide no details on the annotation process or any inter-annotator agreement. He et al. (201"
2021.acl-long.263,P09-1113,0,0.117439,"s, generally only include sentiment-bearing words (not phrases), and do not specify the relationship between target and expression. Finally, the recently proposed aspect sentiment triplet extraction (Peng et al., 2019; ?) attempts to extract targets, expressions and their polarity. However, the datasets used are unlikely to be adequate, as they augment available targeted datasets, but do not report annotation guidelines, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented"
2021.acl-long.263,D13-1171,0,0.0551436,"Missing"
2021.acl-long.263,S12-1035,0,0.0332557,"representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representations to dependency structures that directly encode the often overlapping relations between the building blocks of multiple negation instances at the same time. In a simpler fashion, Yu et al. (2020) exploit the parser of Dozat and Manning (2018) to predict spans of named entities. 3 Datasets are shown in Table 1. The largest available structured sentiment dataset is the NoReCFine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian, annotated for structu"
2021.acl-long.263,morante-daelemans-2012-conandoyle,0,0.0525443,"Missing"
2021.acl-long.263,N10-1120,0,0.0553484,"Missing"
2021.acl-long.263,2020.conll-shared.1,1,0.80928,"Missing"
2021.acl-long.263,S15-2082,0,0.0767941,"Missing"
2021.acl-long.263,S14-2004,0,0.0356346,"strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016"
2021.acl-long.263,2020.acl-demos.14,0,0.0179855,"own in Table 8 in the Appendix. The implementation of the graph structure has a large effect on all metrics, although the specific results depend on the dataset. We plot the average effect of each implementation across all datasets in Figure 3, as well as each individual dataset (Figures 4–8 in the Appendix). +inlabel tends to improve results on the non-English datasets, consistently increasing target and expression extraction and targeted sentiment. It also generally improves the graph scores UF1 and LF1 on the non-English datasets. 9 We use SpaCy (Honnibal et al., 2020) for English, Stanza (Qi et al., 2020) for Basque and Catalan and UDPipe (Straka and Strakov´a, 2017) for Norwegian. Dep. edges has the strongest positive effect on the NSF1 and SF1 (an avg. 2.52 and 2.22 percentage point (pp) over Head-final, respectively). However, this average is pulled down by poorer performance on the English datasets. Removing these two, the average benefit is 5.2 and 4.2 for NSF1 and SF1 , respectively. On span extraction and targeted sentiment, however, Dep. edges leads to poorer scores overall. Dep. labels does not lead to any consistent improvements. These results indicate that incorporating syntactic de"
2021.acl-long.263,D13-1170,0,0.0568037,"Missing"
2021.acl-long.263,2020.acl-main.577,0,0.0253182,"l dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representations to dependency structures that directly encode the often overlapping relations between the building blocks of multiple negation instances at the same time. In a simpler fashion, Yu et al. (2020) exploit the parser of Dozat and Manning (2018) to predict spans of named entities. 3 Datasets are shown in Table 1. The largest available structured sentiment dataset is the NoReCFine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian, annotated for structured sentiment. MultiBEU and MultiBCA (Barnes et al., 2018) are hotel reviews in Basque and Catalan, respectively. MPQA (Wiebe et al., 2005) annotates news wire text in English. Finally, DSUnis (Toprak et al., 2010) annotate English reviews of online universities and e-commerce. In our experiments, we"
2021.acl-long.263,K17-3009,0,0.056707,"Missing"
2021.acl-long.323,Q16-1026,0,0.0194192,"voided. The removal of personal information necessarily entails some data utility loss. Because the ultimate purpose behind data releases is to produce usable data, the best anonymisation methods are those that optimise the trade-off between minimising the disclosure risk and preserving the data utility. 3 3.1 NLP Approaches De-identification NLP research on text anonymisation has focused to a large extent on the tasks of de-identification, and, to a lesser extent, pseudonymisation. Deidentification is generally modelled as a sequence labelling task, similar to Named Entity Recognition (NER) (Chiu and Nichols, 2016; Lample et al., 2016). Most work to date has been performed in the area of clinical NLP, where the goal is to detect Protected Health Information (PHI) in clinical texts (Meystre et al., 2010; Aberdeen et al., 2010). Several shared tasks have contributed to increased activity within this area, in particular through the release of datasets manually annotated with PHIs. The 2014 i2b2/UTHealth shared task (Stubbs and Uzuner, 2015) includes diabetic patient medical records annotated for an extended set of PHI categories. Another influential dataset stems from the 2016 CEGS N-GRID shared task (Stu"
2021.acl-long.323,R19-1030,0,0.0164935,"collect and standardise annotated clinical notes, while Megyesi et al. (2018) present a pseudonymised learner language corpus. For Spanish, a recently held shared task on clinical de-identification released a synthetic Spanishlanguage dataset (Marimon et al., 2019). The problem of replacing identifiers with surrogate values is rarely addressed in NLP. Most approaches simply replace detected identifiers with dummy values such as X, although some models attempt to preserve the gender of person names and provide dedicated rules for e.g. dates and addresses (Sweeney, 1996; Alfalahi et al., 2012; Eder et al., 2019; Chen et al., 2019) or to a somewhat broader range of identifiers (Volodina et al., 2020). A few studies have analysed the re-identification risk of de-identified or pseudonymised texts (Carrell et al., 2013; Meystre et al., 2014b). The data utility of de-identified texts is analysed in Meystre et al. (2014a), concluding that the impact of deidentification is small, but non-negligible. 3.2 Obfuscation Methods Beyond de-identification, several research efforts have looked at detecting and obfuscating social media texts based on quasi-identifying categories such as gender (Reddy and Knight, 201"
2021.acl-long.323,2020.lrec-1.550,0,0.0198333,"labelled data can be employed to adapt models to a new domain (Yang et al., 2019). Finally, Friedrich et al. (2019) present an adversarial approach for learning privacy-preserving text representations, thereby allowing data to be more easily shared to train de-identification tools. Outside of the clinical domain, Medlock (2006) presents a dataset of e-mails annotated with both direct identifiers (person names, transactional codes, etc.) and quasi-identifiers (organisations, course names, etc.). Some annotation efforts are also geared towards de-identification for languages other than English. Eder et al. (2020) present a deidentification dataset consisting of German e-mails. For Swedish, Velupillai et al. (2009); Alfalahi et al. (2012) present efforts to collect and standardise annotated clinical notes, while Megyesi et al. (2018) present a pseudonymised learner language corpus. For Spanish, a recently held shared task on clinical de-identification released a synthetic Spanishlanguage dataset (Marimon et al., 2019). The problem of replacing identifiers with surrogate values is rarely addressed in NLP. Most approaches simply replace detected identifiers with dummy values such as X, although some mode"
2021.acl-long.323,2020.findings-emnlp.123,0,0.0345315,"sed in Meystre et al. (2014a), concluding that the impact of deidentification is small, but non-negligible. 3.2 Obfuscation Methods Beyond de-identification, several research efforts have looked at detecting and obfuscating social media texts based on quasi-identifying categories such as gender (Reddy and Knight, 2016) or race (Blodgett et al., 2016). A number of recent approaches have sought to transform latent representations of texts to protect confidential attributes, using adversarial learning (Elazar and Goldberg, 2018), reinforcement learning (Mosallanezhad et al., 2019) or encryption (Huang et al., 2020). However, those methods operate at the level of latent vector representations and do not modify the texts themselves. One notable exception is the text rewriting approach of Xu et al. (2019) which edits the texts using back-translations. 3.3 Challenges NLP approaches to anonymisation suffer from a number of shortcomings. Most importantly, they are limited to predefined categories of entities and ignore how less conspicuous text elements may also play a role in re-identifying the individual. For instance, the family status or physical appearance of a person may lead to re-identification but wi"
2021.acl-long.323,D18-1002,0,0.028874,"(Carrell et al., 2013; Meystre et al., 2014b). The data utility of de-identified texts is analysed in Meystre et al. (2014a), concluding that the impact of deidentification is small, but non-negligible. 3.2 Obfuscation Methods Beyond de-identification, several research efforts have looked at detecting and obfuscating social media texts based on quasi-identifying categories such as gender (Reddy and Knight, 2016) or race (Blodgett et al., 2016). A number of recent approaches have sought to transform latent representations of texts to protect confidential attributes, using adversarial learning (Elazar and Goldberg, 2018), reinforcement learning (Mosallanezhad et al., 2019) or encryption (Huang et al., 2020). However, those methods operate at the level of latent vector representations and do not modify the texts themselves. One notable exception is the text rewriting approach of Xu et al. (2019) which edits the texts using back-translations. 3.3 Challenges NLP approaches to anonymisation suffer from a number of shortcomings. Most importantly, they are limited to predefined categories of entities and ignore how less conspicuous text elements may also play a role in re-identifying the individual. For instance, t"
2021.acl-long.323,N16-1030,0,0.0335708,"ersonal information necessarily entails some data utility loss. Because the ultimate purpose behind data releases is to produce usable data, the best anonymisation methods are those that optimise the trade-off between minimising the disclosure risk and preserving the data utility. 3 3.1 NLP Approaches De-identification NLP research on text anonymisation has focused to a large extent on the tasks of de-identification, and, to a lesser extent, pseudonymisation. Deidentification is generally modelled as a sequence labelling task, similar to Named Entity Recognition (NER) (Chiu and Nichols, 2016; Lample et al., 2016). Most work to date has been performed in the area of clinical NLP, where the goal is to detect Protected Health Information (PHI) in clinical texts (Meystre et al., 2010; Aberdeen et al., 2010). Several shared tasks have contributed to increased activity within this area, in particular through the release of datasets manually annotated with PHIs. The 2014 i2b2/UTHealth shared task (Stubbs and Uzuner, 2015) includes diabetic patient medical records annotated for an extended set of PHI categories. Another influential dataset stems from the 2016 CEGS N-GRID shared task (Stubbs et al., 2017) base"
2021.acl-long.323,P19-1584,0,0.0341847,"Missing"
2021.acl-long.323,2020.findings-emnlp.111,0,0.0219176,"of data points through noise is a common type of transformation in data privacy (McSherry and Talwar, 2007). This idea of perturbation has notably been applied to word embeddings (Feyisetan et al., 2019), but it produces perturbed word distributions rather than readable documents. Semantic noise has also been defined to perturb nominal values (Rodr´ıguez-Garc´ıa et al., 2017). Formally, one can define an editor model edit(d) taking a document d and outputting an edited document d0 after applying a sequence of masking operations. This model can be e.g. expressed as a neural text editing model (Mallinson et al., 2020). 4195 Its optimisation objective should include both minimising the risk of letting an adversary disclose at least some of the protected entities C through semantic inferences (as described in the previous section) and minimising the number of masking operations necessary to map d to d0 . 6.3 Evaluation Metrics Let D be a set of documents transformed into D0 by an anonymisation tool. How can we empirically evaluate the quality of the anonymisation? The most common method is to rely on human annotators to manually mark identifiers in each document d ∈ D, and then compare the system output with"
2021.acl-long.323,medlock-2006-introduction,0,0.101752,"d or unlabelled data for domain adaptation with in-domain testing and off-theshelf de-identification tools, and show that manual labelling of even small amounts of PHI examples yields performance above existing tools. 4190 Further, embeddings trained on larger amounts of in-domain, unlabelled data can be employed to adapt models to a new domain (Yang et al., 2019). Finally, Friedrich et al. (2019) present an adversarial approach for learning privacy-preserving text representations, thereby allowing data to be more easily shared to train de-identification tools. Outside of the clinical domain, Medlock (2006) presents a dataset of e-mails annotated with both direct identifiers (person names, transactional codes, etc.) and quasi-identifiers (organisations, course names, etc.). Some annotation efforts are also geared towards de-identification for languages other than English. Eder et al. (2020) present a deidentification dataset consisting of German e-mails. For Swedish, Velupillai et al. (2009); Alfalahi et al. (2012) present efforts to collect and standardise annotated clinical notes, while Megyesi et al. (2018) present a pseudonymised learner language corpus. For Spanish, a recently held shared t"
2021.acl-long.323,W18-7106,0,0.0574778,"Missing"
2021.acl-long.323,D19-1240,0,0.0238627,"ata utility of de-identified texts is analysed in Meystre et al. (2014a), concluding that the impact of deidentification is small, but non-negligible. 3.2 Obfuscation Methods Beyond de-identification, several research efforts have looked at detecting and obfuscating social media texts based on quasi-identifying categories such as gender (Reddy and Knight, 2016) or race (Blodgett et al., 2016). A number of recent approaches have sought to transform latent representations of texts to protect confidential attributes, using adversarial learning (Elazar and Goldberg, 2018), reinforcement learning (Mosallanezhad et al., 2019) or encryption (Huang et al., 2020). However, those methods operate at the level of latent vector representations and do not modify the texts themselves. One notable exception is the text rewriting approach of Xu et al. (2019) which edits the texts using back-translations. 3.3 Challenges NLP approaches to anonymisation suffer from a number of shortcomings. Most importantly, they are limited to predefined categories of entities and ignore how less conspicuous text elements may also play a role in re-identifying the individual. For instance, the family status or physical appearance of a person m"
2021.acl-long.323,W16-5603,0,0.0200859,"12; Eder et al., 2019; Chen et al., 2019) or to a somewhat broader range of identifiers (Volodina et al., 2020). A few studies have analysed the re-identification risk of de-identified or pseudonymised texts (Carrell et al., 2013; Meystre et al., 2014b). The data utility of de-identified texts is analysed in Meystre et al. (2014a), concluding that the impact of deidentification is small, but non-negligible. 3.2 Obfuscation Methods Beyond de-identification, several research efforts have looked at detecting and obfuscating social media texts based on quasi-identifying categories such as gender (Reddy and Knight, 2016) or race (Blodgett et al., 2016). A number of recent approaches have sought to transform latent representations of texts to protect confidential attributes, using adversarial learning (Elazar and Goldberg, 2018), reinforcement learning (Mosallanezhad et al., 2019) or encryption (Huang et al., 2020). However, those methods operate at the level of latent vector representations and do not modify the texts themselves. One notable exception is the text rewriting approach of Xu et al. (2019) which edits the texts using back-translations. 3.3 Challenges NLP approaches to anonymisation suffer from a n"
2021.acl-long.323,2020.coling-main.32,0,0.0152295,"t a pseudonymised learner language corpus. For Spanish, a recently held shared task on clinical de-identification released a synthetic Spanishlanguage dataset (Marimon et al., 2019). The problem of replacing identifiers with surrogate values is rarely addressed in NLP. Most approaches simply replace detected identifiers with dummy values such as X, although some models attempt to preserve the gender of person names and provide dedicated rules for e.g. dates and addresses (Sweeney, 1996; Alfalahi et al., 2012; Eder et al., 2019; Chen et al., 2019) or to a somewhat broader range of identifiers (Volodina et al., 2020). A few studies have analysed the re-identification risk of de-identified or pseudonymised texts (Carrell et al., 2013; Meystre et al., 2014b). The data utility of de-identified texts is analysed in Meystre et al. (2014a), concluding that the impact of deidentification is small, but non-negligible. 3.2 Obfuscation Methods Beyond de-identification, several research efforts have looked at detecting and obfuscating social media texts based on quasi-identifying categories such as gender (Reddy and Knight, 2016) or race (Blodgett et al., 2016). A number of recent approaches have sought to transform"
2021.acl-long.323,W19-8633,0,0.0234384,"at detecting and obfuscating social media texts based on quasi-identifying categories such as gender (Reddy and Knight, 2016) or race (Blodgett et al., 2016). A number of recent approaches have sought to transform latent representations of texts to protect confidential attributes, using adversarial learning (Elazar and Goldberg, 2018), reinforcement learning (Mosallanezhad et al., 2019) or encryption (Huang et al., 2020). However, those methods operate at the level of latent vector representations and do not modify the texts themselves. One notable exception is the text rewriting approach of Xu et al. (2019) which edits the texts using back-translations. 3.3 Challenges NLP approaches to anonymisation suffer from a number of shortcomings. Most importantly, they are limited to predefined categories of entities and ignore how less conspicuous text elements may also play a role in re-identifying the individual. For instance, the family status or physical appearance of a person may lead to re-identification but will rarely be considered as categories to detect. On the other hand, those methods may also end up removing too much information, as they will systematically remove all occurrences of a given"
2021.eacl-main.5,P16-1087,0,0.0266305,"and 2) classification of polarity with respect to targets (“aspect term polarity”). As most targeted datasets only contain a single target, or multiple targets with the same polarity, sentence-level classifiers are strong baselines. In order to mitigate this, Jiang et al. (2019) create a Challenge dataset which has both multiple targets and multiple polarities in each sentence. Similarly, Wang et al. (2017) also point out that most targeted sentiment methods perform poorly with multiple targets and propose TDParse, a corpus of UK election tweets with multiple targets per tweet. 2.2 Modelling Katiyar and Cardie (2016) explore jointly extracting holders, targets, and expressions with LSTMs. They find that adding sentence-level and relationlevel dependencies (IS - FROM or IS - ABOUT) improve extraction, but find that the LSTM models lag behind CRFs with rich features. Regarding modelling the interaction between elements, there are several previous attempts to jointly learn to extract and classify targets, using factor graphs (Klinger and Cimiano, 2013), multitask learning (He et al., 2019) or sequence tagging with collapsed tagsets representing both tasks (Li et al., 2019). In general, the benefits are small"
2021.eacl-main.5,P14-1146,0,0.0527381,"en in the target (3) averaging all embeddings in the target phrase, (4) taking the max of the target embeddings, (5) concatenating the max, mean, and min). 2. F IRST: uses the contextualized BERT embedding from the first token of the target in context. 3. M EAN: instead takes the average of the BERT embeddings for the tokens in the target. 4. M AX: uses the max of the contextualized BERT embeddings for the tokens in the target. 5. M AX MM: takes the max, min, and mean pooled representations and passes the concatenation to the softmax layer, which has shown to perform well for sentiment tasks (Tang et al., 2014). However, this triples the size of the input representation to the softmax layer. 4.1 The TARG . and [C LS ] models correspond to the models used in Xu et al. (2019) and serve as baselines. The extraction and classification models are fine-tuned for 50 epochs using Adam with an initial learning rate of 3e−5, with a linear warmup of 0.1 and all other hyperparameters are left at default BERT settings (further details in Appendix B). The best model on the development set is used for testing. Combined with the four input manipulations (Table 2), this leads to eleven extraction experiments – TARG"
2021.eacl-main.5,P13-2147,0,0.0333764,"geted sentiment methods perform poorly with multiple targets and propose TDParse, a corpus of UK election tweets with multiple targets per tweet. 2.2 Modelling Katiyar and Cardie (2016) explore jointly extracting holders, targets, and expressions with LSTMs. They find that adding sentence-level and relationlevel dependencies (IS - FROM or IS - ABOUT) improve extraction, but find that the LSTM models lag behind CRFs with rich features. Regarding modelling the interaction between elements, there are several previous attempts to jointly learn to extract and classify targets, using factor graphs (Klinger and Cimiano, 2013), multitask learning (He et al., 2019) or sequence tagging with collapsed tagsets representing both tasks (Li et al., 2019). In general, the benefits are small and have suggested that there is only a weak relationship between target extraction and polarity classification (Hu et al., 2019). 3 Data One of the difficulties of working with finegrained sentiment analysis is that there are only a few datasets (even in English) and they come in incompatible, competing data formats, e.g., BRAT or various flavors of XML. With the goal of creating a simple unified format to work on fine-grained sentimen"
2021.eacl-main.5,P10-1059,0,0.261377,"s, etc.? (c) Can sentiment lexicons provide enough information on expressions to give improvements? 2.1 The Multi-purpose Question Answering dataset (MPQA) (Wiebe et al., 2005) is the first dataset that annotated opinion holders, targets, expressions and their relationships. The news wire data leads to complex opinions and a generally difficult task for sentiment models. Normally, the full opinion extraction task is modelled as extraction of the individual elements (holders, targets, and expressions) and the subsequent resolution of the relationship between them. The Darmstadt Review Corpora (Toprak et al., 2010) contain annotated opinions for consumer reviews of universities and services. The authors annotate holders, targets, expressions, polarity, modifiers, and intensity. They achieve between 0.5 and 0.8 agreement using the agr method (Wiebe et al., 2005), with higher disagreement on what they call “polar targets” – targets that have a polarity but no annotated sentiment expression – holders, and expressions. The Open Domain Targeted dataset (Mitchell et al., 2013) makes use of crowd sourcing to annotate NEs from scraped tweets in English and Spanish (Etter et al., 2013) with their polarities. The"
2021.eacl-main.5,E17-1046,0,0.357379,"atics {jeremycb,liljao,erikve}@ifi.uio.no Abstract as apposed to the author of the sentence can help us determine the overall polarity expressed in the sentence. Compared to document- or sentence-level sentiment analysis, where distant labelling schemes can be used to obtain annotated data, fine-grained annotation of sentiment does not occur naturally, which means that current machine learning models are often hampered by the small size of datasets. Furthermore, fine-grained annotation is demanding, leads to relatively small datasets, and has low inter-annotator agreement (Wiebe et al., 2005; Wang et al., 2017). This begs the question: is it worth it to annotate full fine-grained sentiment? Targeted sentiment (Mitchell et al., 2013; Zhang et al., 2015) is a reduction of the fine-grained sentiment task which concentrates on extracting sentiment targets and classifying their polarity, effectively ignoring sentiment holders and expressions. The benefit of this setup is that it is faster to annotate and simpler to model. But would targeted sentiment models benefit from knowing the sentiment holders and expressions? In this work, we attempt to determine whether holder and expression information is useful"
2021.eacl-main.5,W02-0109,0,0.0806583,"download, preprocess, and collect the datasets into a compatible JSON format, with the hope that this allows future research on the same data. 2 Datasets Related work Fine-grained approaches to sentiment analysis attempt to discover opinions from text, where each opinion is a tuple of (opinion holder, opinion target, opinion expression, polarity, intensity). Annotation of datasets for this granularity requires creating in-depth annotation guidelines, training 1 https://github.com/ltgoslo/ finegrained_modelling 50 a standard JSON format. The datasets are sentence and word tokenized using NLTK (Loper and Bird, 2002), except for MPQA, DS. Service and DS. Uni, which already contain sentence and token spans. All polarity annotations are mapped to positive, negative, neutral, and conflict2 . As such, each sentence contains a sentence id, the tokenized text, and a possibly empty set of opinions which contain a holder, target, expression, polarity, and intensity. We allow for empty holders and expressions in order generalize to the targeted corpora. Finally, we use 10 percent of the training data as development and another 10 percent for test for the corpora that do not contain a suggested train/dev/test split"
2021.eacl-main.5,D13-1171,0,0.0475814,"Missing"
2021.gebnlp-1.8,2020.coling-main.505,0,0.030253,"Natural Language Processing, pages 66–74 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement (Madotto et al., 2018), natural language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT with LDA topics, and show that this combination improves performance of semantic similarity. Ostendorff et al. (2019) use a combination of metadata about books to enrich a BERTbased multi-class classification model. They train a BERT model on the title and the texts of each book, and concatenate the output with metadata information and author embeddings from Wikipedia, and feed them into a Multilayer Perceptron (MLP). This work foc"
2021.gebnlp-1.8,P18-1224,0,0.0301768,"ng a downstream classifier. Typically, when a classifier is fitted on top of a pre-trained LM for a given task, only textual data is considered by the learned representations. In this work we investigate the effect of adding metadata information about demographic variables that are known to be associated with bias in the training data. Specifically, we focus on the task 66 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 66–74 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement (Madotto et al., 2018), natural language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT wi"
2021.gebnlp-1.8,2020.gebnlp-1.3,0,0.0870409,"Missing"
2021.gebnlp-1.8,2020.lrec-1.502,0,0.0610146,"Missing"
2021.gebnlp-1.8,2020.gebnlp-1.1,0,0.0256051,"with the underlying encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020)"
2021.gebnlp-1.8,N19-1423,0,0.174686,"ious work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zhao et al., 2020; Bartl et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Kurita et al., 2019). Recent works have shown the advantage of adding extra information to pre-trained language models for numerous tasks, e.g., dialog systems Several previous studies have focused on gender and gender bias in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) th"
2021.gebnlp-1.8,W19-3805,0,0.0268621,"Missing"
2021.gebnlp-1.8,W19-3821,0,0.0553101,"Missing"
2021.gebnlp-1.8,W19-3809,0,0.0179626,"et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Kurita et al., 2019). Recent works have shown the advantage of adding extra information to pre-trained language models for numerous tasks, e.g., dialog systems Several previous studies have focused on gender and gender bias in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) that can be used to mitigate biases towards a selection of genders and races. Occupational gender stereotypes exist in sentiment analysis models (Bhaskaran and Bhallamudi, 2019), both in training data and in pre-trained contextualized models. 67 Models have also been proposed to uncover gender biases (Hoyle et al., 2019). Incorporating extra demographic information into sentiment classification models have also been successful. Hovy (2015) has shown that incorporation gender information (as embeddings) in models can improve sentiment classification. They show that such an approach can reduce the bias towards minorities, as for example females, who tend to communicate differently from the norm. In this paper, we do not focus on biases present in existing systems , nor"
2021.gebnlp-1.8,W19-3803,0,0.0337151,"of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on"
2021.gebnlp-1.8,D19-1530,0,0.0175999,"um than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zh"
2021.gebnlp-1.8,W16-4301,0,0.0264283,"er, by explicitly incorporating this as a variable in the model. Note that there are also issues of what could be argued to be representational harm (Blodgett et al., 2020) associated with the underlying encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019)."
2021.gebnlp-1.8,P15-1073,0,0.0276674,"as in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) that can be used to mitigate biases towards a selection of genders and races. Occupational gender stereotypes exist in sentiment analysis models (Bhaskaran and Bhallamudi, 2019), both in training data and in pre-trained contextualized models. 67 Models have also been proposed to uncover gender biases (Hoyle et al., 2019). Incorporating extra demographic information into sentiment classification models have also been successful. Hovy (2015) has shown that incorporation gender information (as embeddings) in models can improve sentiment classification. They show that such an approach can reduce the bias towards minorities, as for example females, who tend to communicate differently from the norm. In this paper, we do not focus on biases present in existing systems , nor do we try to mitigate them in a traditional way. We use a dataset of Norwegian book reviews for which a previous study has indicated some degree of gender bias in the label distribution of review ratings (Touileb et al., 2020). Here, we investigate whether this bia"
2021.gebnlp-1.8,P19-1167,0,0.0221581,"re-trained language models for numerous tasks, e.g., dialog systems Several previous studies have focused on gender and gender bias in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) that can be used to mitigate biases towards a selection of genders and races. Occupational gender stereotypes exist in sentiment analysis models (Bhaskaran and Bhallamudi, 2019), both in training data and in pre-trained contextualized models. 67 Models have also been proposed to uncover gender biases (Hoyle et al., 2019). Incorporating extra demographic information into sentiment classification models have also been successful. Hovy (2015) has shown that incorporation gender information (as embeddings) in models can improve sentiment classification. They show that such an approach can reduce the bias towards minorities, as for example females, who tend to communicate differently from the norm. In this paper, we do not focus on biases present in existing systems , nor do we try to mitigate them in a traditional way. We use a dataset of Norwegian book reviews for which a previous study has indicated some degree"
2021.gebnlp-1.8,2020.acl-main.630,0,0.0352657,"language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT with LDA topics, and show that this combination improves performance of semantic similarity. Ostendorff et al. (2019) use a combination of metadata about books to enrich a BERTbased multi-class classification model. They train a BERT model on the title and the texts of each book, and concatenate the output with metadata information and author embeddings from Wikipedia, and feed them into a Multilayer Perceptron (MLP). This work focuses on gender bias, which we identify as the differences in language use between persons, on the unique basis of their genders. The concrete task tha"
2021.gebnlp-1.8,W16-0204,0,0.0228018,"g this as a variable in the model. Note that there are also issues of what could be argued to be representational harm (Blodgett et al., 2020) associated with the underlying encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and m"
2021.gebnlp-1.8,P19-1160,0,0.0572267,"Missing"
2021.gebnlp-1.8,2020.gebnlp-1.5,0,0.0201141,"et al., 2020) associated with the underlying encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a"
2021.gebnlp-1.8,W19-3823,0,0.017731,"d embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zhao et al., 2020; Bartl et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Kurita et al., 2019). Recent works have shown the advantage of adding extra information to pre-trained language models for numerous tasks, e.g., dialog systems Several previous studies have focused on gender and gender bias in sentiment analysis, both from data and model perspectives. To name a few: Kiritchenko and Mohammad (2018) propose an evaluation corpus (Equity Evaluation Corpus) that can be used to mitigate biases towards a selection of genders and races. Occupational gender stereotypes exist in sentiment analysis models (Bhaskaran and Bhallamudi, 2019), both in training data and in pre-trained contextuali"
2021.gebnlp-1.8,2020.gebnlp-1.11,1,0.705649,"ey train a BERT model on the title and the texts of each book, and concatenate the output with metadata information and author embeddings from Wikipedia, and feed them into a Multilayer Perceptron (MLP). This work focuses on gender bias, which we identify as the differences in language use between persons, on the unique basis of their genders. The concrete task that we deal with in the current paper is that of polarity classification of book reviews, using labels derived from the numerical ratings assigned by professional critics. We use an existing dataset of book reviews dubbed NoReCgender (Touileb et al., 2020), which is a subset of the Norwegian Review Corpus (Velldal et al., 2018), a dataset primarily used for document-level sentiment analysis. The subset NoReCgender has previously been augmented with information about the gender of both critics and book authors. Through experiments with gender predictions of both critics and book authors, we demonstrate the presence of gendered language in these reviews. Previous work has also shown that the distribution of ratings in the dataset to some degree is correlated with the gender of the critics and the authors. Consequently, work on sentiment classific"
2021.gebnlp-1.8,2021.nodalida-main.4,1,0.88802,"ibution of review ratings (Touileb et al., 2020). Here, we investigate whether this bias is reflected in the text, as measured by classification scores on two tasks, namely binary sentiment and gender classification, and whether adding metadata information explicitly providing the gender of the authors and critics of the reviews, or the sentiment score of the review increases classification performance. Similarly to (Ostendorff et al., 2019), we explore the effects of adding this metadata information to document classification tasks using a BERTbased model, in this case the Norwegian NorBERT (Kutuzov et al., 2021). 4 Unique critics Unique authors M F Total 125 1,435 74 882 199 2,317 Table 1: Total number of unique male and female critics and authors in NoReCgender . pos neg Train Dev. Test Total 568 568 69 60 71 55 708 683 Table 2: Total number of positive and negative reviews in the data splits of NoReCgender . NoReCgender (Touileb et al., 2020). As pointed out by Touileb et al. (2020), some of the reviews were written by children, unknown authors/critics, or by editors, these were not assigned genders and were therefore not included in our work. This results in a set of 4,083 documents. Table 1 shows"
2021.gebnlp-1.8,P18-2104,0,0.0122194,"classifier is fitted on top of a pre-trained LM for a given task, only textual data is considered by the learned representations. In this work we investigate the effect of adding metadata information about demographic variables that are known to be associated with bias in the training data. Specifically, we focus on the task 66 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 66–74 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement (Madotto et al., 2018), natural language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT with LDA topics, and show that this combination impr"
2021.gebnlp-1.8,P19-1139,0,0.0170885,"red by the learned representations. In this work we investigate the effect of adding metadata information about demographic variables that are known to be associated with bias in the training data. Specifically, we focus on the task 66 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 66–74 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement (Madotto et al., 2018), natural language inference (Chen et al., 2018), and machine translation (Zaremoodi et al., 2018). Knowledge graphs have also been used to enrich embedding information. Zhang et al. (2019) use entries from Wikidata, as well as their relation to each others, to represent and inject structural knowledge aggregates to a collection of large-scale corpora. They show that their approach reduces noisy data and improves BERT fine-tuning on limited datasets. Bourgonje and Stede (2020) enrich a German BERT model with linguistic knowledge represented as a lexicon as well as manually generated syntactic features. Peinelt et al. (2020) enrich a BERT with LDA topics, and show that this combination improves performance of semantic similarity. Ostendorff et al. (2019) use a combination of meta"
2021.gebnlp-1.8,2020.acl-main.260,0,0.0172219,"encoding of gender itself, since only the binary gender categories of male/female are present in the data. While the dataset we use only reflects binary gender categories, we acknowledge the fact that gender as an identity spans a wider spectrum than this. 3 When it comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work Stat"
2021.gebnlp-1.8,D18-1521,0,0.0284755,"o the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zhao et al., 2020; Bartl et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Fried"
2021.gebnlp-1.8,P19-1161,0,0.0182429,"comes to gender and gender bias, previous research has been devoted to the identification of bias in textual content and models (Garimella and Mihalcea, 2016; Schofield and Mehr, 2016; Kiritchenko and Mohammad, 2018), and in input representations as static and contextualised embeddings (Takeshita et al., 2020; Bartl et al., 2020; Zhao et al., 2020; Basta et al., 2019; Kaneko and Bollegala, 2019; Friedman et al., 2019; Bolukbasi et al., 2016). A considerable amount of previous work has also gone into either mitigating existing bias in embeddings (Takeshita et al., 2020; Maudslay et al., 2019; Zmigrod et al., 2019; Garg et al., 2018), making them gender neutral (Zhao et al., 2018), or using debiased embeddings (Escud´e Font and Costa-juss`a, 2019). Instead of debiasing and mitigating bias in embeddings, some work has focused on creating gender balanced corpora (Costajuss`a et al., 2020; Costa-juss`a and de Jorge, 2020). Related work State-of-the-art results for various NLP tasks nowadays typically build on some pre-trained transformer language models like BERT (Devlin et al., 2019). Despite their great achievements, these models have been shown to include various types of bias (Zhao et al., 2020; Bartl"
2021.nodalida-main.30,W10-3110,0,0.0491213,"d data set, before presenting the first benchmark results for negation resolution in Section 5. Before concluding, we finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotat"
2021.nodalida-main.30,dalianis-velupillai-2010-certain,0,0.0353211,"of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is similar to our work in terms of text type, wh"
2021.nodalida-main.30,W17-1808,0,0.0143249,"finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is simil"
2021.nodalida-main.30,P18-2077,0,0.0819798,"andidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detection. Finally, Kurtz et al. (2020) cast negation resolution as a graph parsing problem and perform full negation resolution using a dependency graph parser (Dozat and Manning, 2018) to jointly predict cues and scopes. The neural model uses a BiLSTM to create token-level representations, and then includes two feed-forward networks to create head- and dependent-specific token representations. Finally, each possible head-dependent combination is scored using a bilinear model. Despite the conceptual simplicity, this model achieves state-of-the-art results. As such, we use this model to evaluate our annotations and include further details in Section 5. 3 Annotations In the following section we present our negation annotation effort in more detail, including the underlying sou"
2021.nodalida-main.30,W17-1810,1,0.780334,"et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fan"
2021.nodalida-main.30,P16-1047,0,0.0170326,"t al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detection. Finally, Kurtz et al. (2020) cast negation resolution as a graph parsing problem and perform full negation resolution using a dependency graph parser (Dozat and Manning, 2018) to jointly pre"
2021.nodalida-main.30,W17-1806,0,0.0158299,"s related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is similar to our work in terms of text type, whereas ConanDoyle-neg is one of the most widely use"
2021.nodalida-main.30,E17-2010,0,0.0154438,"17). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detection. Finally, Kurtz et al. (2020) cast negation resolution as a graph parsing problem and perform full negation resolution using a dependency graph parser (Dozat and Manning, 2018) to jointly predict cues and scopes. The neural model uses a BiLSTM to create token-level representations, and then includes two feed-forward networks to create head- and dependent-specific token representations. Finally, each possible head-dependent combination is scored using a bilinear model. Despite the conceptual simplicity, this model"
2021.nodalida-main.30,D12-1091,0,0.0330494,"We run each experiment five times with different random seeds and report an averaged F1 score and its standard deviation in Table 3. The simplest graph representation point-to-root generally performs best, most visibly in FN F1 (66.8). We attribute the variation in performance to a loss of information in the head-first and headfinal variants, making it impossible to retrieve the correct governing negation cue for partially overlapping scopes, thus lowering the score. In order to see whether these performance differences are statistically significant, we perform bootstrap significance testing (Berg-Kirkpatrick et al., 2012) resampling the test set 106 times while setting the significance threshold to p = 0.05. Comparing point-to-root to head-first and headfinal shows that while the differences seem substantial they are not statistically significant. A manual error analysis on point-to-root shows that the model tends not to predict infrequent cues, e.g., null ‘zero’, istedenfor ‘instead-of’, savnet ‘missing’, while it overpredicts frequent cues, e.g., ikke ‘not’, ingen ‘no’, as well as overgeneralizing the affixal negation u- ‘un-/dis-/non-’ to other words that begin with ‘u’, but are not negated, e.g., utfrika ‘"
2021.nodalida-main.30,W16-5113,0,0.020701,"r negation resolution in Section 5. Before concluding, we finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus."
2021.nodalida-main.30,2020.cl-1.5,0,0.06738,"ng on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is similar to our work in terms of text type, whereas ConanDoyle-neg is one of the most widely used datasets in the field. The English (Konstantinova et al., 2012) and"
2021.nodalida-main.30,konstantinova-etal-2012-review,0,0.0166974,"Missing"
2021.nodalida-main.30,2020.iwpt-1.3,1,0.807032,"model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detection. Finally, Kurtz et al. (2020) cast negation resolution as a graph parsing problem and perform full negation resolution using a dependency graph parser (Dozat and Manning, 2018) to jointly predict cues and scopes. The neural model uses a BiLSTM to create token-level representations, and then includes two feed-forward networks to create head- and dependent-specific token representations. Finally, each possible head-dependent combination is scored using a bilinear model. Despite the conceptual simplicity, this model achieves state-of-the-art results. As such, we use this model to evaluate our annotations and include further"
2021.nodalida-main.30,S12-1042,1,0.760218,"to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus contains a re-annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et"
2021.nodalida-main.30,L18-1547,0,0.245822,"with important modifications. In ConanDoyle-neg, the cue is not included in the scope, and it annotates a wide range of cue types, i.e., both sub-token (affixal), single token and multi-token negation cues. Scopes may furthermore be discontinuous, often an effect of the requirement to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus contains a re-annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al.,"
2021.nodalida-main.30,matsuyoshi-etal-2014-annotating,0,0.0147736,"with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant to the current annotation effort: the SFU Corpus and the ConanDoyle-neg corpus. The SFU corpus also annotates review data, hence is similar to our work in terms of text type, whereas ConanDoyle-neg is one of the most widely used datasets in the field. The English (Ko"
2021.nodalida-main.30,S12-1035,0,0.696832,"n the guidelines developed for the biomedical BioScope corpus (Vincze et al., 2008), which largely employ syntactic criteria for the determination of scope, choosing the maximal syntactic unit that contains the negated content. Unlike BioScope, however, negation cues are not included within the scope in SFU. The corpus does not annotate affixal cues, e.g. im- in impossible. The English ConanDoyle-neg corpus contains Sherlock Holmes stories manually annotated for negation cues, scopes, and events (Morante and Daelemans, 2012) and was employed in the 2012 *SEM shared task on negation detection (Morante and Blanco, 2012). The annotation scheme is also based on the scheme employed for the BioScope corpus (Vincze et al., 2008), but with important modifications. In ConanDoyle-neg, the cue is not included in the scope, and it annotates a wide range of cue types, i.e., both sub-token (affixal), single token and multi-token negation cues. Scopes may furthermore be discontinuous, often an effect of the requirement to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus co"
2021.nodalida-main.30,W09-1105,0,0.339051,"annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems"
2021.nodalida-main.30,morante-daelemans-2012-conandoyle,0,0.266298,"tistics of the final annotated data set, before presenting the first benchmark results for negation resolution in Section 5. Before concluding, we finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the corpora that are most relevant"
2021.nodalida-main.30,2020.lrec-1.618,1,0.656673,"ts for a subset of the Norwegian Review Corpus (NoReC). In addition to providing in-depth discussion of the annotation guidelines, we also present a first set of benchmark results based on a graphparsing approach. 1 Introduction This paper introduces a new data set annotating negation for Norwegian. As shown in the example below, the annotations identify both negation cues (in bold) and their scopes (in brackets) within the sentence: (1) Men kanskje ikke [helt troverdig] . But maybe not completely credible . ‘But maybe not completely credible.’ The underlying corpus is the NoReCfine data set (Øvrelid et al., 2020) – a subset of the Norwegian Review Corpus (NoReC) (Velldal et al., 2018) annotated for fine-grained sentiment, comprising professional reviews from a range of different domains. The new data set introduced here, named NoReCneg , is the first data set of negation for Norwegian. We also present experimental results for negation resolution based on a graph-parsing approach shown to yield state-of-the-art results for other languages. All the resources described in the paper – the data set, the annotation guidelines, the models and the associated code – are made publicly available.1 The rest of th"
2021.nodalida-main.30,P14-1007,0,0.0204614,"inuous, often an effect of the requirement to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus contains a re-annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between"
2021.nodalida-main.30,D16-1078,0,0.0201949,"y employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Qian et al., 2016; Fancellu et al., 2017), relying instead on gold cues and focusing solely on the task of scope detect"
2021.nodalida-main.30,S12-1041,1,0.792251,"thermore be discontinuous, often an effect of the requirement to include the subject within the negation scope. This is in contrast to the annotation scheme found in the SFU corpus, where subjects are not included in the negation scope. Note that the NegPar corpus contains a re-annotated version of the ConanDoyle-neg corpus, which fixes known bugs and also adds Chinese data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over"
2021.nodalida-main.30,E12-2021,0,0.0856201,"Missing"
2021.nodalida-main.30,L18-1661,1,0.55644,"oviding in-depth discussion of the annotation guidelines, we also present a first set of benchmark results based on a graphparsing approach. 1 Introduction This paper introduces a new data set annotating negation for Norwegian. As shown in the example below, the annotations identify both negation cues (in bold) and their scopes (in brackets) within the sentence: (1) Men kanskje ikke [helt troverdig] . But maybe not completely credible . ‘But maybe not completely credible.’ The underlying corpus is the NoReCfine data set (Øvrelid et al., 2020) – a subset of the Norwegian Review Corpus (NoReC) (Velldal et al., 2018) annotated for fine-grained sentiment, comprising professional reviews from a range of different domains. The new data set introduced here, named NoReCneg , is the first data set of negation for Norwegian. We also present experimental results for negation resolution based on a graph-parsing approach shown to yield state-of-the-art results for other languages. All the resources described in the paper – the data set, the annotation guidelines, the models and the associated code – are made publicly available.1 The rest of the paper is structured as follows. We start by reviewing related work on n"
2021.nodalida-main.30,W08-0606,0,0.0590629,"hen summarize the statistics of the final annotated data set, before presenting the first benchmark results for negation resolution in Section 5. Before concluding, we finally provide a discussion of future work in Section 6. 2 Related Work Below we discuss related work on negation, starting with datasets before moving on to modeling. 2.1 Datasets While NoReCneg is the first dataset annotated for negation for Norwegian, there are a number of existing negation datasets for a range of other languages, such as Chinese (Zou et al., 2016), Dutch (Afzal et al., 2014), English (Pyysalo et al., 2007; Vincze et al., 2008; Morante and Daelemans, 2012; Councill et al., 2010; Konstantinova et al., 2012), German (Cotik et al., 2016), Spanish (Jiménez-Zafra et al., 2018; Diaz et al., 2017), Swedish (Dalianis and Velupillai, 2010; Skeppstedt, 2011), Italian (Altuna et al., 2017), and Japanese (Matsuyoshi et al., 2014). Jiménez-Zafra et al. (2020) provide a thorough survey of existing negation datasets. A large proportion of negation corpora are based on data from the biomedical or clinical domain (Vincze et al., 2008; Dalianis and Velupillai, 2010; Cotik et al., 2016; Diaz et al., 2017). We will here focus on the c"
2021.nodalida-main.30,S12-1044,0,0.026626,"se data (Liu et al., 2018). 2.2 Modeling Traditional approaches to the task of negation detection have typically employed a wide range of hand-crafted features, and often linguistically informed, derived from constituency parsing (Read et al., 2012; Packard et al., 2014), dependency parsing (Lapponi et al., 2012), or Minimal Recursion Semantics structures created by an HPSG parser (Packard et al., 2014). Scope resolution in particular has often been approached as a sequence labeling task, as pioneered by Morante and Daelemans (2009) and later done in several other works (Lapponi et al., 2012; White, 2012; Enger et al., 2017). More recently, neural approaches have been successfully applied to the task. Qian et al. (2016) propose a CNN model for negation scope detection on the abstracts section of the BioScope corpus, which operates over syntactic paths between the cue and candidate tokens. Fancellu et al. (2016) present and compare two neural architectures for the task of negation scope detection on the ConanDoyle-neg corpus: a simple feedforward network and a bidirectional LSTM. Note that these more recent neural systems disregard the task of cue detection altogether (Fancellu et al., 2016; Q"
2021.nodalida-main.4,K18-2005,0,0.104033,"rBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingual BERT model – mBERT – which was trained on data that also included Norwegian. Up until the release of the models described in the current paper, mBERT was the only BERT-instance that could be used for Norwegian.6 Another widely used architecture for contextualised LMs is Embeddings From Language Models or ELMo (Peters et al., 2018). The ElmoForManyLangs initiative (Che et al., 2018) trained and released monolingual ELMo models for a wide range of different languages, including Norwegian (with separate models for Bokm˚al and Nynorsk). However, these models were trained on very modestly sized corpora of 20 million words for each language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was published at https://github.com/botxo/nordic_bert in th"
2021.nodalida-main.4,N19-1423,0,0.598307,"iling the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. For additional background and access to the data, models, and software, please see: http://norlm.nlpl.eu 1 Introduction In this work, we present NorLM, an ongoing community initiative and emerging collection of largescale contextualised language models for Norwegian. We here introduce the NorELMo and NorBERT models, that have been trained on around two billion tokens of running Norwegian text. We describe the training procedure and compare these models with the multilingual mBERT model (Devlin et al., 2019), as well as an additional Norwegian BERT model developed contemporaneously, with some interesting differences in training data and setup. We report results over a number of Norwegian benchmark datasets, addressing a broad range of diverse NLP tasks: part-of-speech tagging, negation resolution, sentence-level and fine-grained sentiment analysis and named entity recognition (NER). All the models are publicly available for download from the Nordic Language Processing Laboratory (NLPL) Vectors Repository1 with a CC BY 4.0 license. They are also accessible locally, together with the training and s"
2021.nodalida-main.4,P18-2077,0,0.012311,"ssifier with dropout, identical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the common Targeted F1 metric, which requires correctly extracting a target (strict) and its polarity. We"
2021.nodalida-main.4,N03-3010,0,0.337854,"Missing"
2021.nodalida-main.4,W17-0237,1,0.852813,"e recipe for recreating the environment on other HPC systems, may contribute to ‘democratising’ large-scale NLP research; if nothing else, it eliminates dependency on commercial cloud computing services. 4 Related work Large-scale deep learning language models (LM) are important components of current NLP systems. They are often based on BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) and other contextualised architectures. A number of language-specific initiatives have in recent years released monolingual versions of these models for a number of languages (Fares et al., 2017; Kutuzov and Kuzmenko, 2017; Virtanen et al., 2019; de Vries et al., ˇ 2019; Ulˇcar and Robnik-Sikonja, 2020; Koutsikakis et al., 2020; Nguyen and Nguyen, 2020; Farahani et al., 2020; Malmsten et al., 2020). For our purposes, the most important such previous training effort is that of Virtanen et al. (2019) on creating a BERT model for Finnish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model,"
2021.nodalida-main.4,P18-1007,0,0.0130726,"e believe this to be extremely important for more linguistically-oriented studies, where it is critical to deal with words, not with arbitrarily fragmented pieces (even if they are well-performing in practical tasks). The vocabulary for the model is of size 30,000. It is much less than the 120,000 of mBERT, but it is compensated by these entities being almost exclusively Norwegian. The vocabulary was generated from raw text, without, e.g., separating punctuation from word tokens. This means one can feed raw text into NorBERT. For the vocabulary generation, we used the SentencePiece algorithm (Kudo, 2018) and Tokenizers library.10 The resulting Tokenizers model was converted to the standard BERT WordPiece format. The final vocabulary contains several thousand unused wordpiece slots which can be filled in with task-specific lexical entries for further finetuning by future NorBERT users. 6.1 Training technicalities NorBERT corresponds in its configuration to the Google’s Bert-Base Cased for English, with 12 layers and hidden size 768 (Devlin et al., 2019). We used the standard masked language modeling and next sentence prediction losses with the LAMB optimizer (You et al., 2020). The model was t"
2021.nodalida-main.4,2021.nodalida-main.3,0,0.260739,"ch language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was published at https://github.com/botxo/nordic_bert in the beginning of 2020. However, the vocabulary of this model seems to be broken, and to the best of our knowledge nobody has achieved any meaningful results with it. 6 TraM) project, has released a Norwegian BERT (Base, cased) model dubbed NB-BERT (Kummervold et al., 2021).7 The model is trained on the Colossal Norwegian Corpus, reported to comprise close to 18,5 billion words (109.1 GB of text). In raw numbers, this is about ten times more than the corpus we use for training the NorLM models. However, the vast majority of this is from OCR’ed historical sources, which is bound to introduce at least some noise. In Section 7 below, we demonstrate that in some NLP tasks, a language model trained on less (but arguably cleaner) data can outperform a model trained on larger but noisy corpora. 5 NorELMo NorELMo is a set of bidirectional recurrent ELMo language models"
2021.nodalida-main.4,2020.iwpt-1.3,1,0.691501,"ntical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the common Targeted F1 metric, which requires correctly extracting a target (strict) and its polarity. We also evaluate Labelle"
2021.nodalida-main.4,E17-3025,1,0.74015,"ing the environment on other HPC systems, may contribute to ‘democratising’ large-scale NLP research; if nothing else, it eliminates dependency on commercial cloud computing services. 4 Related work Large-scale deep learning language models (LM) are important components of current NLP systems. They are often based on BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) and other contextualised architectures. A number of language-specific initiatives have in recent years released monolingual versions of these models for a number of languages (Fares et al., 2017; Kutuzov and Kuzmenko, 2017; Virtanen et al., 2019; de Vries et al., ˇ 2019; Ulˇcar and Robnik-Sikonja, 2020; Koutsikakis et al., 2020; Nguyen and Nguyen, 2020; Farahani et al., 2020; Malmsten et al., 2020). For our purposes, the most important such previous training effort is that of Virtanen et al. (2019) on creating a BERT model for Finnish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingua"
2021.nodalida-main.4,2021.nodalida-main.30,1,0.712644,"models on the POS tagging of Bokm˚al (BM) and Nynorsk (NN) test sets in comparison with other large pretrained models for Norwegian. Running times in minutes are given for Bokm˚al. the POS tagging task, training the models for 20 epochs and keeping the model that performs best on the development data. For ELMo models, we used a BiLSTM with global max pooling, taking ELMo token embeddings from the top layer as an input. The evaluation metric is macro F1 . Negation detection Finally, the NoReCfine dataset has recently been annotated with negation cues and their corresponding in-sentence scopes (Mæhlum et al., 2021). The resulting dataset is dubbed NoReCneg .18 We use the same graph-based modeling approach as described for fine-grained sentiment above. We evaluate on the same metrics as in the *SEM 2012 shared task (Morante and Blanco, 2012): cue-level F1 (CUE), scope token F1 over individual tokens (ST), and the combined full negation F1 (FN). 7.2 Results We present the results for the various benchmarking tasks below. POS tagging As can be seen from Table 2, NorBERT outperforms mBERT on both tasks: on POS tagging for Bokm˚al by 5 percentage points and 1 percentage point for Nynorsk. NorBERT is almost o"
2021.nodalida-main.4,S12-1035,0,0.0391265,"r 20 epochs and keeping the model that performs best on the development data. For ELMo models, we used a BiLSTM with global max pooling, taking ELMo token embeddings from the top layer as an input. The evaluation metric is macro F1 . Negation detection Finally, the NoReCfine dataset has recently been annotated with negation cues and their corresponding in-sentence scopes (Mæhlum et al., 2021). The resulting dataset is dubbed NoReCneg .18 We use the same graph-based modeling approach as described for fine-grained sentiment above. We evaluate on the same metrics as in the *SEM 2012 shared task (Morante and Blanco, 2012): cue-level F1 (CUE), scope token F1 over individual tokens (ST), and the combined full negation F1 (FN). 7.2 Results We present the results for the various benchmarking tasks below. POS tagging As can be seen from Table 2, NorBERT outperforms mBERT on both tasks: on POS tagging for Bokm˚al by 5 percentage points and 1 percentage point for Nynorsk. NorBERT is almost on par with NB-BERT on POS tagging. NorELMo models are outperformed by NB-BERT and NorBERT, but are on par with mBERT in POS tagging. Note that their adaptation to the tasks (extracting token embeddings and learning a classifier) t"
2021.nodalida-main.4,2020.findings-emnlp.92,0,0.0467805,"Missing"
2021.nodalida-main.4,L16-1250,1,0.838941,". Below we first provide an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set. ELMo models were not fine-tuned, following the recommendations from Peters et al. (2019). Instead we trained a simple neural classifier (a feed forwar"
2021.nodalida-main.4,2020.lrec-1.618,1,0.9224,"resentations across all 3 layers) for all words. Then, these token embeddings are fed to a neural classifier with dropout, identical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the"
2021.nodalida-main.4,N18-1202,0,0.459415,"nish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingual BERT model – mBERT – which was trained on data that also included Norwegian. Up until the release of the models described in the current paper, mBERT was the only BERT-instance that could be used for Norwegian.6 Another widely used architecture for contextualised LMs is Embeddings From Language Models or ELMo (Peters et al., 2018). The ElmoForManyLangs initiative (Che et al., 2018) trained and released monolingual ELMo models for a wide range of different languages, including Norwegian (with separate models for Bokm˚al and Nynorsk). However, these models were trained on very modestly sized corpora of 20 million words for each language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was pub"
2021.nodalida-main.4,W19-4302,0,0.0601483,"Missing"
2021.nodalida-main.4,2020.acl-demos.14,0,0.119749,"is important for BERT-like models, because one of their training tasks is next sentence prediction). In total, our training corpus comprises about two billion (1,907,072,909) word tokens in 203 million (202,802,665) sentences. We conducted the following pre-processing steps: 1. Wikipedia texts were extracted from the dumps using the segment wiki script ˇ uˇrek and Sojka, from the Gensim project (Reh˚ 2010). 2. For the news texts from Norwegian Aviskorpus, we performed de-tokenization and conversion to UTF-8 encoding, where required. 3. The resulting corpus was sentencesegmented using Stanza (Qi et al., 2020). We left blank lines between documents (and 4 https://www.nb.no/sprakbanken/ ressurskatalog/oai-nb-no-sbr-4/ sections in the case of Wikipedia) so that the ‘next sentence prediction’ task of BERT does not span between documents. 3 Prerequisites: software and computing Developing very large contextualised language models is no small challenge, both in terms of engineering sophistication and computing demands. Training ELMo- and in particular BERT-like models presupposes access to specialised hardware – graphical processing units (GPUs) – over extended periods of time. Compared to the original"
2021.nodalida-main.4,solberg-etal-2014-norwegian,1,0.845115,"mBERT and to the recently released NB-BERT model described in Section 4. Where applicable, we show separate evaluation results for Bokm˚al and Nynorsk. Below we first provide an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set"
2021.nodalida-main.4,2020.lrec-1.582,0,0.0422658,"Missing"
2021.nodalida-main.4,L18-1661,1,0.755579,"Missing"
2021.nodalida-main.4,W17-0201,1,0.823933,"an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set. ELMo models were not fine-tuned, following the recommendations from Peters et al. (2019). Instead we trained a simple neural classifier (a feed forward network with one hidd"
2021.nodalida-main.4,K18-2001,0,0.0327235,"Missing"
2021.nodalida-main.41,W13-3819,0,0.0321089,"Missing"
2021.nodalida-main.41,2020.acl-main.493,0,0.0218462,"with far larger parameter spaces and more training data. This is something of a middle-of-the-road approach; future work could involve this sort of evaluation on downscaled transformer models, which we shy away from in order to provide a usable model release. We hope that the differences between these models provide some insight, and pave the way for further research, not only specifically addressing the question of sampling from a perspective of performance, but also analytically. There has already been considerable work in this direction on multilingual variants of BERT (Pires et al., 2019; Chi et al., 2020), and we hope that this work motivates papers applying the same to recurrent mELMo, as well as comparing and contrasting the two. The ELMo models described in this paper are publicly released via NLPL Vector Repository.1 Acknowledgements Our experiments were run on resources provided by UNINETT Sigma2 - the National Infrastructure for High Performance Computing and Data Storage in Norway, under the NeIC-NLPL umbrella. References Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhi"
2021.nodalida-main.41,2020.acl-main.747,0,0.0999181,"Missing"
2021.nodalida-main.41,D18-1269,0,0.0295844,"tions onto the PoS label space. PoS tagging (transfer): We use the same architecture as for regular PoS tagging, but train on English and evaluate on our target languages. Dependency parsing: We use dependencyannotated Universal Dependencies corpora; our metrics are both unlabelled and labelled attachment scores (UAS/LAS). Our parsing architecture is a biaffine graph-based parser (Dozat and Manning, 2018). XNLI: A transfer-based language inference task; we use Chen et al.’s 2017 ESIM architecture, train a tagging head on English, and evaluate on the translated dev portions of other languages (Conneau et al., 2018). Tatoeba: The task here is to pick out, for each sentence in our source corpus (English), the appropriate translation of the sentence in our target language corpus. This, in a sense, is the most ‘raw’ tasks; target language sentences are Figure 1: Performance difference between monolingual and multilingual models, on our monolingual tasks. Absent bars indicate that the language was missing. ranked based on similarity. We follow Hu et al. (2020) and use the Tatoeba dataset. We tokenize all our text using the relevant UDPipe (Straka et al., 2019) model, and train/evaluate on each task three tim"
2021.nodalida-main.41,P19-1493,0,0.0873757,", we contextualise our work in the present literature. Section 3 describes our experimental setup and Section 4 our results. Finally, we conclude with a discussion of our results in Section 5. 2 Prior work Multilingual embedding architectures (static or contextualised) are different from cross-lingual ones (Ruder et al., 2019; Liu et al., 2019) in that they are not products of aligning several monolingual models. Instead, a deep neural model is trained end to end on texts in multiple languages, thus making the whole process more straightforward and yielding truly multilingual representations (Pires et al., 2019). Following Artetxe et al. (2020), we will use the term ‘deep multilingual pretraining’ for such approaches. One of the early examples of deep multilingual pretraining was BERT, which featured a multilingual variant trained on the 104 largest languagespecific Wikipedias (Devlin et al., 2019). To counter the effects of some languages having overwhelmingly larger Wikipedias than others, Devlin et al. (2019) used exponentially smoothed data weighting; i.e., they exponentiated the probability of a token being in a certain language by a certain α, and re-normalised. This has the effect of ‘squashin"
2021.nodalida-main.41,W19-4212,0,0.043736,"Missing"
2021.nodalida-main.41,2020.lrec-1.582,0,0.0981297,"Missing"
bouma-etal-2010-towards,nivre-etal-2006-talbanken05,0,\N,Missing
bouma-etal-2010-towards,ritz-etal-2008-annotation,0,\N,Missing
bouma-etal-2010-towards,nivre-etal-2006-maltparser,0,\N,Missing
bouma-etal-2010-towards,J93-2004,0,\N,Missing
bouma-etal-2010-towards,W06-2920,0,\N,Missing
bouma-etal-2010-towards,2006.jeptalnrecital-invite.2,0,\N,Missing
bouma-etal-2010-towards,W07-2416,0,\N,Missing
bouma-etal-2010-towards,2005.mtsummit-papers.11,0,\N,Missing
C10-1155,W02-1503,0,0.0438619,"different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take additional features into account. The procedure to enable the data-driven parser to learn from the grammar-driven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al., 2008) and the English grammar developed within the ParGram project (Butt, Dyvik, King, Masuichi, & Rohrer, 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank—one gold standard and one with LFG annotation. We extend the gold standard treebank with additional information from the corresponding LFG analysis and train MaltParser on the enhanced data set. Table 2 shows the enhanced dependency representation of example (1) above, taken from the training data. For each token, the parsed data contains information on the word form, lemma, and part of speech (PoS), as well as on the head and dependency relation in columns 6 and 7. The added XLE i"
C10-1155,W01-0521,0,0.029999,"enn Treebank (PTB), converted to dependency format (Johansson & Nugues, 2007) and extended with XLE features, as described above. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8. PoS Tagging and Domain Variation Our parser is trained on financial news, and although stacking with a general-purpose LFG parser is ex1381 pected to aid domain portability, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea, 2001). MaltParser presupposes that inputs have been PoS tagged, leaving room for variation in preprocessing. On the one hand, we aim to make parser inputs maximally similar to its training data (i.e. the conventions established in the PTB); on the other hand we wish to benefit from specialized resources for the biomedical domain. The GENIA tagger (Tsuruoka et al., 2005) is particularly relevant in this respect (as could be the GENIA Treebank proper4 ). However, we found that GENIA tokenization does not match the PTB conventions in about one out of five sentences (for example wrongly splitting token"
C10-1155,W07-2416,0,0.0146943,"bove, taken from the training data. For each token, the parsed data contains information on the word form, lemma, and part of speech (PoS), as well as on the head and dependency relation in columns 6 and 7. The added XLE information resides in the F E A T S column, and in the XLE-specific head and dependency columns 8 and 9. Parser outputs, which in turn form the basis for our scope resolution rules discussed in Section 5, also take this same form. The parser employed in this work is trained on the Wall Street Journal sections 2 – 24 of the Penn Treebank (PTB), converted to dependency format (Johansson & Nugues, 2007) and extended with XLE features, as described above. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8. PoS Tagging and Domain Variation Our parser is trained on financial news, and although stacking with a general-purpose LFG parser is ex1381 pected to aid domain portability, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea, 2001). MaltParser presupposes that inputs have been PoS tagged, leavi"
C10-1155,W08-0607,0,0.0122864,"gest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the strong and weak points in current"
C10-1155,P07-1125,0,0.0132078,"he organizers further suggest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the stron"
C10-1155,W10-3006,0,0.331143,"approaches and technology. In this article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (of 24) on Task 1 and third (of 15) on Task 2, for an overall best average result (there appears to be very limited overlap among top performers for the two subtasks). 2 It turns out, in fact, that all the top-performing systems in Task 2 of the CoNLLShared Task rely on syntactic information provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the rol"
C10-1155,W09-1304,0,0.0614029,"anguage has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the strong and weak points in current approaches and technology. In this article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (o"
C10-1155,P06-2079,0,0.0325861,"out this paper, angle brackets highlight hedge cues, and curly braces indicate the scope of a given cue, as annotated in BioScope. sentences containing uncertainty; the objective of Task 2 is learning to resolve the in-sentence scope of hedge cues (Farkas, Vincze, Mora, Csirik, & Szarvas, 2010). The organizers further suggest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level,"
C10-1155,nivre-etal-2006-maltparser,0,0.0404639,"ndency representation of example (1), with MaltParser and XLE annotations. unit possible. For evaluation purposes, the task organizers provided newly annotated biomedical articles, following the same general BioScope principles. The CoNLL 2010 evaluation data comprises 5,003 additional utterances (138,276 tokens), of which 790 are annotated as hedged. The data contains a total of 1033 cues, of which 87 are so-called multiword cues (i.e. cues spanning multiple tokens), comprising 1148 cue tokens altogether. Stacked Dependency Parsing For syntactic analysis we employ the open-source MaltParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar se"
C10-1155,P08-1108,0,0.032951,"ltiple tokens), comprising 1148 cue tokens altogether. Stacked Dependency Parsing For syntactic analysis we employ the open-source MaltParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combinat"
C10-1155,P09-2010,1,0.841272,"altParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse his"
C10-1155,W10-3008,0,0.197988,"s article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (of 24) on Task 1 and third (of 15) on Task 2, for an overall best average result (there appears to be very limited overlap among top performers for the two subtasks). 2 It turns out, in fact, that all the top-performing systems in Task 2 of the CoNLLShared Task rely on syntactic information provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the role of syntactic informa"
C10-1155,W10-3007,1,0.834253,"formation provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the role of syntactic information rather than on machine learning specifics (§ 5 and § 6), providing an analysis and discussion of Task 2 errors (§ 7), and generally aiming to gauge the value of available annotated data and processing tools (§ 8). We present a hybrid, two-level approach for hedge resolution, where a statistical classifier detects cue words, and a small set of manually crafted rules operating over syntactic structures resolve scope. We show how syntactic information—produced"
C10-1155,W08-0606,0,0.470685,"result (in terms of both combined ranks and average F1 ) in the 2010 CoNLL Shared Task. 1 Background—Motivation Recent years have witnessed an increased interest in the analysis of various aspects of sentiment in natural language (Pang & Lee, 2008). The subtask of hedge resolution deals with the analysis of uncertainty as expressed in natural language, and the linguistic means (so-called hedges) by which speculation or uncertainty are expressed. Information of this kind is of importance for various mining tasks which aim at extracting factual data. Example (1), taken from the BioScope corpus (Vincze, Szarvas, Farkas, Móra, & Csirik, 2008), shows a sentence where uncertainty is signaled by the modal verb may.1 {The unknown amino acid hmayi be used by these species}. (1) The topic of the Shared Task at the 2010 Conference for Natural Language Learning (CoNLL) is hedge detection in biomedical literature—in a sense ‘zooming in’ on one particular aspect of the broader BioNLP Shared Task in 2009 (Kim, Ohta, Pyysalo, Kano, & Tsujii, 2009). It involves two subtasks: Task 1 is described as learning to detect 1 In examples throughout this paper, angle brackets highlight hedge cues, and curly braces indicate the scope of a given cue, as"
C10-1155,P09-1043,0,0.0301455,"s, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take additional features into account. The procedure to enable the data-driven parser to"
C10-1155,W09-1401,0,\N,Missing
C10-1155,W10-3001,0,\N,Missing
C10-2129,W06-2922,0,0.0131657,"ble combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second orde"
C10-2129,W09-1210,1,0.879438,"Missing"
C10-2129,W06-2920,0,0.0226861,"over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages, (Hajiˇc et al., 2009)) with data based on the TIGER 1123 corpus (Brants et al., 2002) in both cases. Since the original TIGER treebank is in a hybrid phrasestructural/dependency format with a relatively flat hierarchical structure, conversion to a pure dependency format involves some non-trivial steps. The 2008 ACL Workshop on Parsing German included a specific shared task on dependency parsing of German (K¨ubler, 2008), based on two sets of data: again the TIGER corpus – however with a different conversion routine than"
C10-2129,D07-1101,0,0.0256126,"learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second order parsing algorithm of McDonald and Pereira (2006) uses a separate algorithm for edge labeling. In addition to the first order factors, this algorithm uses the edges to those children which are closest to the dependent and has a complexity of O(n3 ). The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest i"
C10-2129,P01-1024,0,0.132315,"he dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages, (Hajiˇc et al., 2009)) with data based on the TIGER 1123 corpus (Brants et al., 2002) in both cases. Since the original TIGER treebank is in a hybrid phrasestructural/dependency format with a relatively flat hierarchical structure, conversion to a pure dependency format involves some non-trivial steps. The 2008 ACL Workshop on Parsing German included a specific shared task on dependency parsing of German (K"
C10-2129,W98-0509,0,0.562004,"Missing"
C10-2129,P05-1013,0,0.02721,"al., 2006) is a languageindependent system for data-driven dependency parsing which is freely available.7 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parsing actions. MaltParser employs a rich feature representation in order to guide parsing. For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al., 2006). Furthermore, we employ the technique of pseudo-projective parsing described in Nilsson and Nivre (2005) and a split prediction strategy for predicting parse transitions and arc labels (Nivre and Hall, 2008).8 In order to obtain automatic parses for the whole data set, we perform a 10fold split. For the parser stacking, we follow the approach of Nivre and McDonald (2008), using MaltParser as a guide for the MST parser with the hash kernel, i.e., providing the arcs and labels assigned by MaltParser as features. Table 5 shows the scores we obtain by parser stacking. Although our version of MaltParser does not quite have the same performance as for instance the version of Hall and Nivre (2008), its"
C10-2129,C96-1058,0,0.125123,"pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm a"
C10-2129,W08-1007,0,0.257232,"It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parsing actions. MaltParser employs a rich feature representation in order to guide parsing. For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al., 2006). Furthermore, we employ the technique of pseudo-projective parsing described in Nilsson and Nivre (2005) and a split prediction strategy for predicting parse transitions and arc labels (Nivre and Hall, 2008).8 In order to obtain automatic parses for the whole data set, we perform a 10fold split. For the parser stacking, we follow the approach of Nivre and McDonald (2008), using MaltParser as a guide for the MST parser with the hash kernel, i.e., providing the arcs and labels assigned by MaltParser as features. Table 5 shows the scores we obtain by parser stacking. Although our version of MaltParser does not quite have the same performance as for instance the version of Hall and Nivre (2008), its guidance leads to a small improvement in the overall parsing results. MaltParser our parser +stacking"
C10-2129,P08-1108,0,0.0752809,"ts significantly. This seems to support our intuition that number helps in disambiguating case values. However, adding gender information does not further increase this effect but hurts parser performance even more than case annotation alone. This leaves us with a puzzle here. Annotating case and number helps the parser, but case alone or having case, number and gender together affects performance negatively. A possible explanation might be that the effect of the gender information is masked by the increased number of feature values (24) which confuses the parsing algorithm. 7 Parser Stacking Nivre and McDonald (2008) show how two different approaches to data-driven dependency pars6 Person would be another syntactically relevant information. However, since we are dealing with a newspaper corpus, first and second person features appear very rarely. 1127 ing, the graph-based and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parsing results for different languages. MaltParser (Nivre et al., 2006) is a languageindependent system for data-driven dependency parsing which is freely available.7 It is based on a deterministic parsing strategy in com"
C10-2129,W06-2933,0,0.0343479,"Missing"
C10-2129,W03-3017,0,0.265855,"nation of different parsing strategies is advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parser"
C10-2129,P09-1040,0,0.0510405,"of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label. This algorithm has a quadratic complexity. The second order parsing algo"
C10-2129,W08-2123,0,0.0137423,") uses a separate algorithm for edge labeling. In addition to the first order factors, this algorithm uses the edges to those children which are closest to the dependent and has a complexity of O(n3 ). The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. The edge labeling is an integral part of the algorithm which requires an additional loop over the labels. This algorithm therefore has a complexity of O(n4 ). Johansson and Nugues (2008) reduced the required number of loops over the edge labels by considering only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination. Predating the surge of interest in data-based dependency parsing, there is a relatively long tradition of dependency parsing work on German, including for instance Menzel and Schr¨oder (1998) and Duchier and Debusmann (2001). German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages,"
C10-2129,P95-1037,0,0.0587336,"s of the noun phrase directly at PP level. This annotation was kept in the dependency version and it can cause problems for the parser since there are two different ways of annotating NPs: (i) for normal NPs where all dependents of the noun are attached as daughters of the head noun and (ii) for NPs in PPs where all dependents of the noun are attached as daughters to the preposition thus being sisters to their head noun. We changed the annotation of PPs by identifying the head noun in the PP and attaching all of its siblings to it. To find the correct head, we used a heuristic in the style of Magerman (1995). The head is chosen by taking the rightmost daughter of the preposition that has a category label according to the heuristic and is labeled with NK (noun kernel element). Table 1 shows the parser performance on the data after PP-restructuring.4 The explanation for the benefit of the restructuring is of course that 4 Note that we are evaluating against a gold standard here (and in the rest of the paper) which has been restructured as well. With a different gold standard one could argue that the absolute figures we obtain are not fully comparable with the original CoNLL shared task. However, si"
C10-2129,W96-0213,0,0.25313,"y likely to mislead the parser in its decision process. A lot of the parser’s features include PoS tags and reducing the amount of errors during PoS tagging will therefore reduce misleading feature values as well. Since the quality of the automatically assigned PoS tags in the German CoNLL ’09 data is not state-of-the-art (see Table 2 below), we decided to retag the data with our own tagger which uses additional information from a symbolic morphological analyzer to direct a statistical classifier. For the assignment of PoS tags, we apply a standard maximum entropy classification approach (see Ratnaparkhi (1996)). The classes of the classifier are the PoS categories defined in the Stuttgart-T¨ubingen Tag Set (STTS) (Schiller et al., 1999). We use standard binarized features like the word itself, its last three letters, whether the word is capitalized, contains a hyphen, a digit or whether it consists of digits only. As the only nonbinary feature, word length is recorded. These standard features are augmented by a number of binary features that support the classification process by providing a preselection of possible PoS tags. Every word is analyzed by DMOR, a finite state morphological analyzer, fro"
C10-2129,P10-1111,1,0.801761,"Missing"
C10-2129,W07-2218,0,0.0457627,"advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The fac"
C10-2129,W03-3023,0,0.0797297,"ferent parsing strategies is advantageous; we include a relatively simple parser stacking procedure in our pilot study (Section 7), and finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency str"
C10-2129,E06-1011,0,0.041155,"finally, we apply Integer Linear Programming in a targeted way to add some global constraints on possible combinations of arc labels with a single head (Section 8). Section 9 offers a brief conclusion. 2 Related Work and Data Basis We quickly review the situation in data-driven dependency parsing in general and on applying it to German specifically. The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006). Nivre (2009) introduced a transition based nonprojective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. Titov and Henderson (2007) combined a transition based parsing algorithm, using beam search, with a latent variable machine learning technique. Maximum spanning tree based dependency parsers decompose a dependency structure into factors. The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the depend"
C10-2129,W08-1008,0,\N,Missing
C10-2129,W09-1201,0,\N,Missing
C12-2088,P11-2123,0,0.0240643,"Missing"
C12-2088,J92-4003,0,0.350432,"age. In the Ontonotes data set all hyphens were in addition converted to the HYPH tag which is used in this data set. 2.3 Lexical categories The data sets described above were enriched with information about the lemma of each token using the NLTK WordNet lemmatizer (Bird et al., 2009). The lemmatizer requires information about part-of-speech, hence lemmatization was performed separately on gold and automatically tagged data sets. Following lemmatization, the data sets were further enriched with the cluster labels described in Turian et al. (2010), created using the Brown clustering algorithm (Brown et al., 1992) and induced from the RCV1 corpus, a corpus containing Reuters English newswire text, with approximately 63 million words and 3.3 million sentences. The Brown algorithm is a hierarchical clustering algorithm which clusters words by maximizing the mutual information of bigrams. Since the algorithm is hierarchical, cluster labels are simply unique identifiers of each node within the tree, expressing the path from the root, where 0 indicates a right branch and 1 a left branch. Furthermore, clusters may be extracted at various depths, giving clusters of different sizes. Brown clusters have previou"
C12-2088,W07-2217,0,0.0205672,"Missing"
C12-2088,de-marneffe-etal-2006-generating,0,0.0334363,"Missing"
C12-2088,I11-1100,0,0.20079,"Missing"
C12-2088,W01-0521,0,0.12777,"Missing"
C12-2088,gimenez-marquez-2004-svmtool,0,0.026531,"Missing"
C12-2088,P08-1068,0,0.396994,"ctic parsers in some form or other. Since the task of assigning word-to-word relations is at the core of dependency parsing, statistics regarding relations between different word forms in the training data provide vital information. These lexical statistics are, however, often sparse, and there exists a growing body of work which examines various strategies for generalizing over the distributions of words and using different kinds of lexical categories in syntactic parsing. Word clusters derived from unlabeled data have been shown to improve parsing accuracy for dependency parsing of English (Koo et al., 2008; Suzuki et al., 2009) and so have clusters derived from parsed data (Sagae and Gordon, 2009). Zhou et al. (2011) show that co-occurence based measures of word-to-word selectional preference derived from web-scale data sets can improve statistical dependency parsing. Furthermore, other types of lexical semantic information, such as named entity classes (Ciaramita and Attardi, 2007) and word sense information from WordNet (Agirre et al., 2011), have recently been shown to improve dependency parsing for English. In this article, we investigate the use of different lexical categories when parsing"
C12-2088,P09-1040,0,0.0548331,"ency parsing which is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history and may easily be extended to take additional features into account. We choose to use Maltparser primarily due to its extendible feature model which facilitates experimentation with additional features during parsing. As our baseline parser, we use the parse model described in Foster et al. (2011a), where Maltparser was employed to parse web 2.0 data. It employs the stacklazy algorithm (Nivre, 2009), along with the liblinear package (Fan et al., 2008) for inducing parse transition classifiers. The stacklazy algorithm operates over three data structures: a stack (S) of partially processed tokens, a list (I) of nodes that have been on the stack, and a “lookahead” list (L) of nodes that have not been on the stack. We refer to the top of the stack using S0 and subsequent nodes using S1 , S2 , etc., and the leftmost/rightmost dependent of S0 with S0l /S0r . 906 Parser wsj23ont o a2e Lexical categories – Ontonotes c2e p2.5a2e p2.5c2e eng sel BaseGold 89.27 84.85 82.22 84.99 86.11 83.89 83.61 C"
C12-2088,nivre-etal-2006-maltparser,0,0.039718,"els. First of all, we vary the number of clusters to be either 100, 320, 1000 or 3200 clusters. This means that the number of clusters is fixed prior to clustering. Koo et al. (2008) found the use prefixes of the cluster labels of various lengths (4 to 6) to be beneficial for parsing, so we adopt this approach in addition to using full-length labels. A third method for generalizing over the cluster labels is to use the lemma information directly in the assignment of clusters, so that all word forms with the same lemma are assigned identical cluster labels. 3 Parser features We use Maltparser (Nivre et al., 2006) (v. 1.4.1), a system for data-driven dependency parsing which is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history and may easily be extended to take additional features into account. We choose to use Maltparser primarily due to its extendible feature model which facilitates experimentation with additional features during parsing. As our baseline parser, we use the parse model described in Foster et al. (2011a), where Maltparser was employed to parse web 2."
C12-2088,W09-3829,0,0.172701,"Missing"
C12-2088,D09-1058,0,0.0479717,"me form or other. Since the task of assigning word-to-word relations is at the core of dependency parsing, statistics regarding relations between different word forms in the training data provide vital information. These lexical statistics are, however, often sparse, and there exists a growing body of work which examines various strategies for generalizing over the distributions of words and using different kinds of lexical categories in syntactic parsing. Word clusters derived from unlabeled data have been shown to improve parsing accuracy for dependency parsing of English (Koo et al., 2008; Suzuki et al., 2009) and so have clusters derived from parsed data (Sagae and Gordon, 2009). Zhou et al. (2011) show that co-occurence based measures of word-to-word selectional preference derived from web-scale data sets can improve statistical dependency parsing. Furthermore, other types of lexical semantic information, such as named entity classes (Ciaramita and Attardi, 2007) and word sense information from WordNet (Agirre et al., 2011), have recently been shown to improve dependency parsing for English. In this article, we investigate the use of different lexical categories when parsing a range of different"
C12-2088,P10-1040,0,0.030518,") and the pretrained model for English available from the tool web page. In the Ontonotes data set all hyphens were in addition converted to the HYPH tag which is used in this data set. 2.3 Lexical categories The data sets described above were enriched with information about the lemma of each token using the NLTK WordNet lemmatizer (Bird et al., 2009). The lemmatizer requires information about part-of-speech, hence lemmatization was performed separately on gold and automatically tagged data sets. Following lemmatization, the data sets were further enriched with the cluster labels described in Turian et al. (2010), created using the Brown clustering algorithm (Brown et al., 1992) and induced from the RCV1 corpus, a corpus containing Reuters English newswire text, with approximately 63 million words and 3.3 million sentences. The Brown algorithm is a hierarchical clustering algorithm which clusters words by maximizing the mutual information of bigrams. Since the algorithm is hierarchical, cluster labels are simply unique identifiers of each node within the tree, expressing the path from the root, where 0 indicates a right branch and 1 a left branch. Furthermore, clusters may be extracted at various dept"
C12-2088,P11-1156,0,0.0955317,"Missing"
C12-2088,L08-1000,0,\N,Missing
C18-1117,P14-1023,0,0.062705,"ch queries. Researchers also studied the increase or decrease in the frequency of a word A collocating with another word B over time, and based on this inferred changes in the meaning of A (Heyer et al., 2009). However, it is clear that semantic shifts are not always accompanied with changes in word frequency (or this connection may be very subtle and non-direct). Thus, if one were able to more directly model word meaning, such an approach should be superior to frequency-proxied methods. A number of recent publications have showed that distributional word representations (Turney et al., 2010; Baroni et al., 2014) provide an efficient way to solve these tasks. They represent meaning with sparse or dense (embedding) vectors, produced from word co-occurrence counts. Although conceptually the source of the data for these models is still word frequencies, they ‘compress’ this information into continuous lexical representations which are both efficient and convenient to work with. Indeed, Kulkarni et al. (2015) explicitly demonstrated that distributional models outperform the frequency-based methods in detecting semantic shifts. They managed to trace semantic shifts more precisely and with greater explanato"
C18-1117,D17-1118,0,0.714461,"review some of these law-like generalizations below, before finally describing a study that questions their validity. Dubossarsky et al. (2015) experimented with K-means clustering applied to SGNS embeddings trained for evenly sized yearly samples for the period 1850–2009. They found that the degree of semantic change for a given word – quantified as the change in self-similarity over time – negatively correlates with its distance to the centroid of its cluster. They proposed that the likelihood for semantic shift correlates with the degree of prototypicality (the ‘law of prototypicality’ in Dubossarsky et al. (2017)). Another relevant study is reported by Eger and Mehler (2016), based on two different graph models; one being a time-series model relating embeddings across time periods to model semantic shifts and the other modeling the self-similarity of words across time. Experiments were performed with time-indexed historical corpora of English, German and Latin, using time-periods corresponding to decades, years and centuries, respectively. To enable comparison of embeddings across time, second-order embeddings encoding similarities to other words were used, as described in 3.3, limited to the ‘core vo"
C18-1117,P16-2009,0,0.517426,"ted that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts. 3.1.2 Test sets Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is"
C18-1117,Q16-1003,0,0.655948,"g vein of research based on dynamic topic modeling (Blei and Lafferty, 2006; Wang and McCallum, 2006), which learns the evolution of topics over time. In Wijaya and Yeniterzi (2011), it helped solve a typical digital humanities task of finding traces of real-world events in the texts. Heyer et al. (2016) employed topic analysis to trace the so-called ‘context volatility’ of words. In the political science, topic models are also sometimes used as proxies to social trends developing over time: for example, Mueller and Rauh (2017) employed LDA to predict timing of civil wars and armed conflicts. Frermann and Lapata (2016) drew on these ideas to trace diachronic word senses development. But most scholars nowadays seem to prefer parametric distributional models, particularly predictionbased embedding algorithms like SGNS, CBOW or GloVe (Pennington et al., 2014). Following their widespread adoption in NLP in general, they have become the dominant representations for the analysis of diachronic semantic shifts as well. 3.3 Comparing vectors across time It is rather straightforward to train separate word embedding models using time-specific corpora containing texts from several different time periods. As a consequen"
C18-1117,W11-2508,0,0.66346,"erremans et al. (2010) study the very recent neologism detweet, showing the development of two separate usages/meanings for this word (‘to delete from twitter,’ vs ‘to avoid tweeting’) based on large amounts of web-crawled data. The usage-based view of lexical semantics aligns well with the assumptions underlying the distributional semantic approach (Firth, 1957) often employed in NLP . Here, the time spans studied are often considerably shorter (decades, rather than centuries) and we find that these distributional methods seem well suited for monitoring the gradual process of meaning change. Gulordava and Baroni (2011), for instance, showed that distributional models capture cultural shifts, like the word sleep acquiring more negative connotations related to sleep disorders, when comparing its 1960s contexts to its 1990s contexts. To sum up, semantic shifts are often reflected in large corpora through change in the context of the word which is undergoing a shift, as measured by co-occurring words. It is thus natural to try to detect semantic shifts automatically, in a ‘data-driven’ way. This vein of research is what we cover in the present survey. In the following sections, we overview the methods currently"
C18-1117,P16-1141,0,0.100611,"frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi1385 cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile. The availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word’s collocational patterns reflect ch"
C18-1117,D16-1057,0,0.0712855,"frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi1385 cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile. The availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word’s collocational patterns reflect ch"
C18-1117,D16-1229,0,0.0646158,"frequently co-occurs, or by the need for discrimination of synonyms caused by lexical borrowings from other languages. Semantic shifts may be also be caused by changes in the attitudes of speakers or in the general environment of the speakers. Thus, semantic shifts are naturally separated into two important classes: linguistic drifts (slow and regular changes in core meaning of words) and cultural shifts (culturally determined changes in associations of a given word). Researchers studying semantic shifts from a computational point of view have shown the existence of this division empirically (Hamilton et al., 2016c). In the traditional classification of Stern (1931), the semantic shift category of substitution describes a change that has a non-linguistic cause, namely that of technologi1385 cal progress. This may be exemplified by the word car which shifted its meaning from non-motorized vehicles after the introduction of the automobile. The availability of large corpora have enabled the development of new methodologies for the study of lexical semantic shifts within general linguistics (Traugott, 2017). A key assumption in much of this work is that changes in a word’s collocational patterns reflect ch"
C18-1117,J15-4004,0,0.04213,"s, but the overwhelming majority of publications still apply only to English corpora. It might be the case that the best methodologies are the same for different languages, but this should be shown empirically. • There is a clear need to devise algorithms that work on small datasets, as they are very common in historical linguistics, digital humanities, and similar disciplines. 1392 • Carefully designed and robust gold standard test sets of semantic shifts (of different kinds) should be created. This is a difficult task in itself, but the experience from synchronic word embeddings evaluation (Hill et al., 2015) and other NLP areas proves that it is possible. • There is a need for rigorous formal mathematical models of diachronic embeddings. Arguably, this will follow the vein of research in joint learning across several time spans, started by Bamler and Mandt (2017) and Yao et al. (2018), but other directions are also open. • Most current studies stop after stating the simple fact that a semantic shift has occurred. However, more detailed analysis of the nature of the shift is needed. This includes: 1. Sub-classification of types of semantic shifts (broadening, narrowing, etc). This problem was to s"
C18-1117,W09-4302,0,0.112362,"ely and with greater explanatory power. One of the examples from their work is the semantic evolution of the word gay: through time, its nearest semantic neighbors changed, manifesting the gradual move away from the sense of ‘cheerful’ to the sense of ‘homosexual.’ In fact, distributional models were being used in diachronic research long before the paper of Kulkarni et al. (2015), although there was no rigorous comparison to the frequentist methods. Already in 2009, it was proposed that one can use distributional methods to detect semantic shifts in a quantitative way. The pioneering work by Jurgens and Stevens (2009) described an insightful conceptualization of a sequence of distributional model updates through time: it is effectively a Word:Semantic Vector:Time tensor, in the sense that each word in a distributional model possesses a set of semantic vectors for each time span we are interested in. It paved the way for quantitatively comparing not only words with regard to their meaning, but also different stages in the development of word meaning over time. Jurgens and Stevens (2009) employed the Random Indexing (RI) algorithm (Kanerva et al., 2000) to create word vectors. Two years later, Gulordava and"
C18-1117,D17-1037,0,0.0283976,"r at least makes the models more comparable. Several works have appeared recently which aim to address the technical issues accompanying this approach of incremental updating. Among others, Peng et al. (2017) described a novel method of incrementally learning the hierarchical softmax function for the CBOW and Continuous Skipgram algorithms. In this way, one can update word embedding models with new data and new vocabulary much more efficiently, achieving faster training than when doing it from scratch, while at the same time preserving comparable performance. Continuing this line of research, Kaji and Kobayashi (2017) proposed a conceptually similar incremental extension for negative sampling, which is a method of training examples selection, widely used with prediction-based models as a faster replacement for hierarchical softmax. Even after the models for different time periods are made comparable in this or that way, one still has to choose the exact method of comparing word vectors across these models. Hamilton et al. (2016a) and Hamilton et al. (2016c) made an important observation that the distinction between linguistic and cultural semantic shifts is correlated with the distinction between global an"
C18-1117,W14-2517,0,0.580761,"etected word sense changes over several different time periods spanning from 3 to 200 years. 1 https://books.google.com/ngrams 1386 In more recent work, time spans tend to decrease in size and become more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of time spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular ‘bins’, time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018). In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collecti"
C18-1117,W16-4005,1,0.879246,"rough a regression analysis, Hamilton et al. (2016a) investigated how the change rates correlate with frequency and polysemy, and proposed another two ‘laws’: 1. frequent words change more slowly (‘the law of conformity’); 2. polysemous words (controlled for frequency) change more quickly (‘the law of innovation’). Azarbonyad et al. (2017) showed that these laws (at least the law of conformity) hold not only for diachronic corpora, but also for other ‘viewpoints’: for example, semantic shifts across models trained on texts produced by different political actors or written in different genres (Kutuzov et al., 2016). However, the temporal dimension allows for a view of the corpora under analysis as a sequence, making the notion of ‘semantic shift’ more meaningful. Later, Dubossarsky et al. (2017) questioned the validity of some of these proposed ‘laws’ of semantic change. In a series of replication and control experiments, they demonstrated that some of the regularities observed in previous studies are largely artifacts of the models used and frequency effects. In particular, they considered 10-year bins comprising equally sized yearly samples from Google Books 5-grams of English fiction for the period 1"
C18-1117,D17-1194,1,0.916294,"tated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts. 3.1.2 Test sets Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is more complicated. Ideally, diachronic approaches should be evaluated on human-annotated lists of semantically shifted words (ranked by the degree of the shift). However, such gold standard data is difficu"
C18-1117,W17-2705,1,0.917496,"tated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis of cultural semantic drift related to armed conflicts. 3.1.2 Test sets Diachronic corpora are needed not only as a source of training data for developing semantic shift detection systems, but also as a source of test sets to evaluate such systems. In this case, however, the situation is more complicated. Ideally, diachronic approaches should be evaluated on human-annotated lists of semantically shifted words (ranked by the degree of the shift). However, such gold standard data is difficu"
C18-1117,P12-2051,0,0.713835,"chosen before slicing the text collection into subcorpora. Earlier works dealt mainly with long-term semantic shifts (spanning decades or even centuries), as they are easier to trace. One of the early examples is Sagi et al. (2011) who studied differences between Early Middle, Late Middle and Early Modern English, using the Helsinki Corpus (Rissanen and others, 1993). The release of the Google Books Ngrams corpus1 played an important role in the development of the field and spurred work on the new discipline of ‘culturomics,’ studying human culture through digital media (Michel et al., 2011). Mihalcea and Nastase (2012) used this dataset to detect differences in word usage and meaning across 50-years time spans, while Gulordava and Baroni (2011) compared word meanings in the 1960s and in the 1990s, achieving good correlation with human judgments. Unfortunately, Google Ngrams is inherently limited in that it does not contain full texts. However, for many cases, this corpus was enough, and its usage as the source of diachronic data continued in Mitra et al. (2014) (employing syntactic ngrams), who detected word sense changes over several different time periods spanning from 3 to 200 years. 1 https://books.goog"
C18-1117,P14-1096,0,0.667009,"nt of the field and spurred work on the new discipline of ‘culturomics,’ studying human culture through digital media (Michel et al., 2011). Mihalcea and Nastase (2012) used this dataset to detect differences in word usage and meaning across 50-years time spans, while Gulordava and Baroni (2011) compared word meanings in the 1960s and in the 1990s, achieving good correlation with human judgments. Unfortunately, Google Ngrams is inherently limited in that it does not contain full texts. However, for many cases, this corpus was enough, and its usage as the source of diachronic data continued in Mitra et al. (2014) (employing syntactic ngrams), who detected word sense changes over several different time periods spanning from 3 to 200 years. 1 https://books.google.com/ngrams 1386 In more recent work, time spans tend to decrease in size and become more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of tim"
C18-1117,D14-1162,0,0.102176,"traces of real-world events in the texts. Heyer et al. (2016) employed topic analysis to trace the so-called ‘context volatility’ of words. In the political science, topic models are also sometimes used as proxies to social trends developing over time: for example, Mueller and Rauh (2017) employed LDA to predict timing of civil wars and armed conflicts. Frermann and Lapata (2016) drew on these ideas to trace diachronic word senses development. But most scholars nowadays seem to prefer parametric distributional models, particularly predictionbased embedding algorithms like SGNS, CBOW or GloVe (Pennington et al., 2014). Following their widespread adoption in NLP in general, they have become the dominant representations for the analysis of diachronic semantic shifts as well. 3.3 Comparing vectors across time It is rather straightforward to train separate word embedding models using time-specific corpora containing texts from several different time periods. As a consequence, these models are also time-specific. However, it is not that straightforward to compare word vectors across different models. It usually does not make sense to, for example, directly calculate cosine similarities between embeddings of one"
C18-1117,S15-2147,0,0.211295,"alone for other languages. General linguistics research on language change like that of Traugott and Dasher (2001) and others usually contain only a small number of hand-picked examples, which is not sufficient to properly evaluate an automatic unsupervised system. Various ways of overcoming this problem have been proposed. For example, Mihalcea and Nastase (2012) evaluated the ability of a system to detect the time span that specific contexts of a word undergoing a shift belong to (word epoch disambiguation). A similar problem was offered as SemEval-2015 Task 7: ‘Diachronic Text Evaluation’ (Popescu and Strapparava, 2015). Another possible evaluation method is so-called cross-time alignment, where a system has to find equivalents for certain words in different time periods (for example, ‘Obama’ in 2015 corresponds to ‘Trump’ in 2017). There exist several datasets containing such temporal equivalents for English (Yao et al., 2018). Yet another evaluation strategy is to use the detected diachronic semantic shifts to trace or predict real-world events like armed conflicts (Kutuzov et al., 2017b). Unfortunately, all these evaluation methods still require the existence of large manually annotated semantic shift dat"
C18-1117,N18-1044,0,0.105985,"ecome more granular. In general, corpora with smaller time spans are useful for analyzing socio-cultural semantic shifts, while corpora with longer spans are necessary for the study of linguistically motivated semantic shifts. As researchers are attempting to trace increasingly subtle cultural semantic shifts (more relevant for practical tasks), the granularity of time spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular ‘bins’, time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018). In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year fr"
C18-1117,D17-1121,0,0.340604,"ans. Szymanski (2017) frames this as the temporal word analogy problem, extending the word analogies concept into the temporal dimension. This work shows that 1391 diachronic word embeddings can successfully model relations like ‘word w1 at time period tα is like word w2 at time period tβ ’. To this end, embedding models trained on different time periods are aligned using linear transformations. Then, the temporal analogies are solved by simply finding out which word vector in the time period tβ is the closest to the vector of w1 in the time period tα . A variation of this task was studied in Rosin et al. (2017), where the authors learn the relatedness of words over time, answering queries like ‘in which time period were the words Obama and president maximally related’. This technique can be used for a more efficient user query expansion in generalpurpose search engines. Kutuzov et al. (2017a) modeled a different semantic relation: ‘words w1 and w2 at time period tα are in the same semantic relation as words w3 and w4 at time period tβ ’. To trace the temporal dynamics of these relations, they re-applied linear projections learned on sets of w1 and w2 pairs from the model for the period tn to the mod"
C18-1117,P17-2071,1,0.911369,"as a continuous differentiable value (Rosenfeld and Erk, 2018). In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) continued the usage of COHA (along with the Google Ngrams corpus). Kutuzov et al. (2017b) started to employ the yearly slices of the English Gigaword corpus (Parker et al., 2011) in the analysis"
C18-1117,P15-1063,0,0.535819,"e spans is decreasing: for example, Kim et al. (2014) and Liao and Cheng (2016) analyzed the yearly changes of words. Note that, instead of using granular ‘bins’, time can also be represented as a continuous differentiable value (Rosenfeld and Erk, 2018). In addition to the Google Ngrams dataset (with granularity of 5 years), Kulkarni et al. (2015) used Amazon Movie Reviews (with granularity of 1 year) and Twitter data (with granularity of 1 month). Their results indicated that computational methods for the detection of semantic shifts can be robustly applied to time spans less than a decade. Zhang et al. (2015) used another yearly text collection, the New-York Times Annotated Corpus (Sandhaus, 2008), again managing to trace subtle semantic shifts. The same corpus was employed by Szymanski (2017), with 21 separate models, one for each year from 1987 to 2007, and to some extent by Yao et al. (2018), who crawled the NYT web site to get 27 yearly subcorpora (from 1990 to 2016). The inventory of diachronic corpora used in tracing semantic shifts was expanded by Eger and Mehler (2016), who used the Corpus of Historical American (COHA2 ), with time slices equal to one decade. Hamilton et al. (2016a) contin"
D17-1194,P16-2009,0,0.0362405,"s of geographical locations where violent armed groups were active to the embeddings of these groups. These projections are then applied to the embeddings and gold standard data from the subsequent year, thus predicting what entities act as violent groups in the next time slice. To evaluate our approach, we adapt the UCDP Armed Conflict Dataset (Gleditsch et al., 2002; Allansson et al., Attempts to detect semantic change using unsupervised methods have a long history. Significant results have already been achieved in employing word embeddings to study diachronic language change. Among others, Eger and Mehler (2016) show that the embedding of a given word for a given time period to a large extent is a linear combination of its embeddings for the previous time periods. Hamilton et al. (2016) proposed an important distinction between cultural shifts and linguistic drifts. They proved that global embedding-based measures (comparing the similarities of words to all other words in the lexicon) are sensitive to regular processes of linguistic drift, while local measures (comparing nearest neighbors’ lists) are a better fit for more irregular cultural shifts in word meaning. Our focus here is on cultural shifts"
D17-1194,D16-1229,0,0.202409,"from the subsequent year, thus predicting what entities act as violent groups in the next time slice. To evaluate our approach, we adapt the UCDP Armed Conflict Dataset (Gleditsch et al., 2002; Allansson et al., Attempts to detect semantic change using unsupervised methods have a long history. Significant results have already been achieved in employing word embeddings to study diachronic language change. Among others, Eger and Mehler (2016) show that the embedding of a given word for a given time period to a large extent is a linear combination of its embeddings for the previous time periods. Hamilton et al. (2016) proposed an important distinction between cultural shifts and linguistic drifts. They proved that global embedding-based measures (comparing the similarities of words to all other words in the lexicon) are sensitive to regular processes of linguistic drift, while local measures (comparing nearest neighbors’ lists) are a better fit for more irregular cultural shifts in word meaning. Our focus here is on cultural shifts: it is not the dictionary meanings of the names denoting locations and armed groups that change, but rather their ‘image’ in the analyzed texts. Our measurement approach can als"
D17-1194,D17-1037,0,0.0565018,"thresholds. It was initially set to the value which produced a reasonable vocabulary size of several hundred thousand words. 1826 Pairs (size) @1 @5 @10 All (38) New (7) 44.7 14.3 73.7 28.6 84.2 42.9 our approach on the whole set of UCDP conflicts in the Gigaword years (1994–2010). Table 2: Projection accuracy for the isolated example experiment mapping from 2000 → 2001. models are simply trained further with the new texts. A possible alternative to this can be incremental training of hierarchical softmax functions proposed in (Peng et al., 2017) or incremental negative sampling proposed in (Kaji and Kobayashi, 2017); we leave it for future work. The experiment involves applying a learned transformation matrix across pairs of models. While in Section 4 we evaluate the approach across the entire Gigaword time period, this section reports a preliminary example experiment for the transition from 2000 to 2001 alone. This means we will have one model saved after sequential training for the years up to 2000, and one saved after year 2001. Our aim is to find out whether the Location–Insurgent projection learned on the first model is able to reveal conflicts that appear in 2001. Thus, we extract from the UCDP dat"
D17-1194,W17-2705,1,0.781474,"Missing"
D17-1194,P14-5010,0,0.00775205,"nts to assess the hypothesis that the embeddings contain semantic relationships of the type ‘insurgent participant of an armed conflict in the location’. To this end, we trained a CBOW model on the full English Gigaword corpus (about 4.8 billion tokens in total), with a symmetric context window of 5 words, vector size 300, 10 negative samples and 5 iterations. Words with a frequency less than 100 were ignored during training. We used Genˇ uˇrek and Sojka, 2010) for training, and sim (Reh˚ in terms of corpus pre-processing we performed lemmatization, PoS-tagging and NER using Stanford CoreNLP (Manning et al., 2014). Named entities were concatenated to one token (for example, United States became United::States_PROPN). Then, we used the 137 Location–Insurgent pairs derived in Section 2 to learn a projection matrix from the embeddings for locations to the embeddings for insurgents. The idea and the theory behind this approach are extensively described in (Mikolov et al., 2013b) and (Kutuzov et al., 2016), but essentially it involves training a linear regression which minimizes the error in transforming 1825 loc→group group→loc λ @1 @5 @10 @1 @5 @10 0.0 0.5 1.0 0.0 0.7 2.2 14.6 19.0 19.7 31.4 35.0 32.8 8.8"
D19-6125,P09-1113,0,0.121006,"t al., 2016; Ma and Hovy, 2016). These types of studies utilize character and/or word embeddings to encode sentence-level features automatically. Recently, the use of contextualized word representation (Peters et al., 2018; Akbik et al., 2018) significantly improves the state-of-the-art results in many sequence labeling tasks and specifically also in the NER benchmark. In the supervised NER paradigm, this task suffers from lack of large-scale labeled training data when moving to a new domain or new language. To alleviate the reliance on human annotated data, distant supervision is proposed by Mintz et al. (2009), to generate annotated data by heuristically aligning text to an existing domain-specific knowledge resource. It is widely used for relation extraction (Mintz et al., 2009; Riedel et al., 2010; Augenstein et al., 2014) and lately it has attracted attention also for NER (Ren et al., 2015; Fries et al., 2017; Shang et al., 2018b; Yang et al., 2018). Shang et al. (2018b) present the AutoNER model which employs a new type of tagging scheme (i.e., Tie or Break) rather than common ones (i.e., IOB, IOBES) without any CRF layer and achieves state-of-the-art unsupervised F 1 scores on several benchmar"
D19-6125,C18-1139,0,0.0834227,"chnique of partial annotation to address false negative cases and implement a reinforcement learning strategy with a neural network policy to identify false positive instances. Our results establish a new state-of-the-art on four benchmark datasets taken from different domains and different languages. We then go on to show that our model reduces the amount of manually annotated data required to perform NER in a new domain. 1 Introduction Named Entity Recognition (NER) is one of the primary tasks in information extraction pipelines. (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018). Traditional studies apply statistical techniques such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) using large amounts of features and extra resources (Ratinov and Roth, 2009; Passos et al., 2014). In recent years, deep learning approaches achieve state-of-the-art results in the task without any feature engineering (Ma and Hovy, 2016; Lample et al., 2016). Most of these works assume that there is a certain amount of annotated sentences in the training phase. However, avail• We combine the Partial-CRF approach with performance-driven, policy-based reinforcement learning t"
D19-6125,W14-1609,0,0.012756,"art on four benchmark datasets taken from different domains and different languages. We then go on to show that our model reduces the amount of manually annotated data required to perform NER in a new domain. 1 Introduction Named Entity Recognition (NER) is one of the primary tasks in information extraction pipelines. (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018). Traditional studies apply statistical techniques such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) using large amounts of features and extra resources (Ratinov and Roth, 2009; Passos et al., 2014). In recent years, deep learning approaches achieve state-of-the-art results in the task without any feature engineering (Ma and Hovy, 2016; Lample et al., 2016). Most of these works assume that there is a certain amount of annotated sentences in the training phase. However, avail• We combine the Partial-CRF approach with performance-driven, policy-based reinforcement learning to clean the noisy, distantly supervised data for NER in a pre-processing step. • We formulate the reward function in RL based on the change in the performance of 225 Proceedings of the 2nd Workshop on Deep Learning Appr"
D19-6125,N18-1202,0,0.0511771,"setup. We adopt a technique of partial annotation to address false negative cases and implement a reinforcement learning strategy with a neural network policy to identify false positive instances. Our results establish a new state-of-the-art on four benchmark datasets taken from different domains and different languages. We then go on to show that our model reduces the amount of manually annotated data required to perform NER in a new domain. 1 Introduction Named Entity Recognition (NER) is one of the primary tasks in information extraction pipelines. (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018). Traditional studies apply statistical techniques such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) using large amounts of features and extra resources (Ratinov and Roth, 2009; Passos et al., 2014). In recent years, deep learning approaches achieve state-of-the-art results in the task without any feature engineering (Ma and Hovy, 2016; Lample et al., 2016). Most of these works assume that there is a certain amount of annotated sentences in the training phase. However, avail• We combine the Partial-CRF approach with performance-driven, policy-based rei"
D19-6125,S14-2004,0,0.0230784,"EINFORCE algorithm (Williams, 1992). Since we calculate the reward as a difference between F 1 scores in two contiguous epochs, the agent will be compensated for a set of actions that has direct impact on the performance of the NER model in the current epoch. In other words, the different parts of the removed instances in each epoch are the reason of the change in F 1 scores. Accordingly, the policy will update using the following gradient: θ = θ + µ[5θ Ωi X log π(a|S; θ)ri (5) Ωi−1 + 5θ X • LaptopReview containing laptop aspect term is taken from the SemEval 2014 Challenge, Task 4 Subtask 1 (Pontiki et al., 2014). The 3,845 review sentences are annotated with 3,012 ’AspectTerm’ mentions. We extract 15,000 sentences from the Amazon laptop review dataset 4 as a raw text. Wang et al. (2011) design this dataset for the aspect-based sentiment analysis. Thanks to Shang et al. (2018b), they provide the dictionary of 13,457 computer terms crawled from a public website 5 . log π(a|S; θ)(−ri )] According to Qin et al. (2018), assuming Ψi is removed in epoch i : Ωi = Ψi − (Ψi ∩ Ψi−1 ) Ωi−1 = Ψi−1 − (Ψi ∩ Ψi−1 ) • EC is a Chinese dataset from the e-commerce domain. We choose this dataset in order to compare our r"
D19-6125,P18-1199,0,0.0544489,"Missing"
D19-6125,W17-5224,0,0.0477091,"Missing"
D19-6125,W09-1119,0,0.278811,"lish a new state-of-the-art on four benchmark datasets taken from different domains and different languages. We then go on to show that our model reduces the amount of manually annotated data required to perform NER in a new domain. 1 Introduction Named Entity Recognition (NER) is one of the primary tasks in information extraction pipelines. (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018). Traditional studies apply statistical techniques such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) using large amounts of features and extra resources (Ratinov and Roth, 2009; Passos et al., 2014). In recent years, deep learning approaches achieve state-of-the-art results in the task without any feature engineering (Ma and Hovy, 2016; Lample et al., 2016). Most of these works assume that there is a certain amount of annotated sentences in the training phase. However, avail• We combine the Partial-CRF approach with performance-driven, policy-based reinforcement learning to clean the noisy, distantly supervised data for NER in a pre-processing step. • We formulate the reward function in RL based on the change in the performance of 225 Proceedings of the 2nd Workshop"
D19-6125,N16-1030,0,0.512622,"ision in a supervised setup. We adopt a technique of partial annotation to address false negative cases and implement a reinforcement learning strategy with a neural network policy to identify false positive instances. Our results establish a new state-of-the-art on four benchmark datasets taken from different domains and different languages. We then go on to show that our model reduces the amount of manually annotated data required to perform NER in a new domain. 1 Introduction Named Entity Recognition (NER) is one of the primary tasks in information extraction pipelines. (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018). Traditional studies apply statistical techniques such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) using large amounts of features and extra resources (Ratinov and Roth, 2009; Passos et al., 2014). In recent years, deep learning approaches achieve state-of-the-art results in the task without any feature engineering (Ma and Hovy, 2016; Lample et al., 2016). Most of these works assume that there is a certain amount of annotated sentences in the training phase. However, avail• We combine the Partial-CRF approach with performance-dri"
D19-6125,P16-1101,0,0.437709,"lore distant supervision in a supervised setup. We adopt a technique of partial annotation to address false negative cases and implement a reinforcement learning strategy with a neural network policy to identify false positive instances. Our results establish a new state-of-the-art on four benchmark datasets taken from different domains and different languages. We then go on to show that our model reduces the amount of manually annotated data required to perform NER in a new domain. 1 Introduction Named Entity Recognition (NER) is one of the primary tasks in information extraction pipelines. (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018). Traditional studies apply statistical techniques such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) using large amounts of features and extra resources (Ratinov and Roth, 2009; Passos et al., 2014). In recent years, deep learning approaches achieve state-of-the-art results in the task without any feature engineering (Ma and Hovy, 2016; Lample et al., 2016). Most of these works assume that there is a certain amount of annotated sentences in the training phase. However, avail• We combine the Partial-CRF approach"
D19-6125,C08-1113,0,0.119877,"17; Shang et al., 2018b; Yang et al., 2018). For the task of NER, it identifies entity mentions if it exist in the knowledge base (e.g, domain-specific dictionary, glossary, ontology) and assigns the corresponding type according to the knowledge base. However, distant supervision approaches encounter two main limitations. First, due to limited coverage of the knowledge resources, unmatched tokens result in False Negatives (FNs). Second, since simple string matching is employed to detect entity mentions, ambiguity in the knowledge resource may lead to False Positives (FPs). For the FN problem, Tsuboi et al. (2008) incorporate partial annotations into CRFs and propose a parameter estimation method for CRFs using partially annotated corpora (here-in after referred to as Partial-CRF). In order to reduce the negative impact of FPs for relation extraction, Qin et al. (2018) propose a deep reinforcement learning (RL) agent where the the agent’s goal is to decide whether to remove or keep the distantly supervised instance. In this paper we make the following contributions: Existing named entity recognition (NER) systems rely on large amounts of human-labeled data for supervision. However, obtaining large-scal"
D19-6125,C18-1183,0,0.0969095,"adeh, Jan Tore Lønning, Lilja Øvrelid Department of Informatics University of Oslo, Norway {farhadno,jtl,liljao}@ifi.uio.no Abstract ability of large amounts of labeled data is problematic, particularly in specific domains. Distant supervision is proposed by Mintz et al. (2009) to address the challenge of obtaining training data for new domains using existing knowledge resources (dictionaries, ontologies). It has previously been successfully applied to tasks like relation extraction (Riedel et al., 2010; Augenstein et al., 2014) and entity recognition (Fries et al., 2017; Shang et al., 2018b; Yang et al., 2018). For the task of NER, it identifies entity mentions if it exist in the knowledge base (e.g, domain-specific dictionary, glossary, ontology) and assigns the corresponding type according to the knowledge base. However, distant supervision approaches encounter two main limitations. First, due to limited coverage of the knowledge resources, unmatched tokens result in False Negatives (FNs). Second, since simple string matching is employed to detect entity mentions, ambiguity in the knowledge resource may lead to False Positives (FPs). For the FN problem, Tsuboi et al. (2008) incorporate partial an"
E06-3008,W01-0716,0,0.293156,"Missing"
E06-3008,W04-0216,0,0.26444,"Missing"
E06-3008,J01-3003,0,\N,Missing
E09-1072,nivre-etal-2006-maltparser,0,0.149842,"on of the consistency of the human annotation in terms of linguistic level. In section 4, we present experiments in lexical acquisition of animacy based on morphosyntactic features extracted from a considerably larger corpus. Section 5 presents experiments with the acquired animacy information applied in the data-driven dependency parsing of Swedish. Finally, section 6 concludes the article and provides some suggestions for future research. 3 Human reference in Swedish Talbanken05 is a Swedish treebank which was created in the 1970’s and which has recently been converted to dependency format (Nivre et al., 2006b) and made freely available. The written sections of the treebank consist of professional prose and student essays and amount to 197,123 running tokens, spread over 11,431 sentences. Figure 2 shows the labeled dependency graph of example (2), taken from Talbanken05. 2 Animacy annotation Annotation for animacy is not a common component of corpora or treebanks. However, following from the theoretical interest in the property of animacy, there have been some initiatives directed at animacy annotation of corpus data. Corpus studies of animacy (Yamamoto, 1999; Dahl and Fraurud, 1996) have made use"
E09-1072,nivre-etal-2006-talbanken05,0,0.285336,"on of the consistency of the human annotation in terms of linguistic level. In section 4, we present experiments in lexical acquisition of animacy based on morphosyntactic features extracted from a considerably larger corpus. Section 5 presents experiments with the acquired animacy information applied in the data-driven dependency parsing of Swedish. Finally, section 6 concludes the article and provides some suggestions for future research. 3 Human reference in Swedish Talbanken05 is a Swedish treebank which was created in the 1970’s and which has recently been converted to dependency format (Nivre et al., 2006b) and made freely available. The written sections of the treebank consist of professional prose and student essays and amount to 197,123 running tokens, spread over 11,431 sentences. Figure 2 shows the labeled dependency graph of example (2), taken from Talbanken05. 2 Animacy annotation Annotation for animacy is not a common component of corpora or treebanks. However, following from the theoretical interest in the property of animacy, there have been some initiatives directed at animacy annotation of corpus data. Corpus studies of animacy (Yamamoto, 1999; Dahl and Fraurud, 1996) have made use"
E09-1072,W06-2933,0,0.068863,"Missing"
E09-1072,W06-2920,0,0.086378,"Missing"
E09-1072,W08-2104,1,0.84635,"information in the assignment of syntactic structure. and Marsi, 2006), where this parser was the best performing parser for Swedish. 5.2 Results The addition of automatically assigned animacy information for common nouns (Anim) causes a small, but significant improvement in overall results (p&lt;.04) compared to the baseline, as well as the corresponding gold standard experiment (p&lt;.04). In the gold standard experiment, the results are not significantly better than the baseline and the main, overall, improvement from the gold standard animacy information reported in Øvrelid and Nivre (2007) and Øvrelid (2008) stems largely from the animacy annotation of pronouns.15 This indicates that the animacy information for common nouns, which has been automatically acquired from a considerably larger corpus, captures distributional distinctions which are important for the general effect of animacy and furthermore that the differences from the gold standard annotation prove beneficial for the results. We see from Table 5, that the improvement in overall parse results is mainly in terms of dependency labeling, reflected in the LAS score. A closer error analysis shows that the performance of the two parsers emp"
E09-1072,W05-0509,0,0.17582,"Missing"
E09-1072,W03-0410,0,0.0230254,"fy animate referents. However, such a method is clearly restricted to languages for which large scale lexical resources, such as the WordNet, are available. The task of animacy classification bears some resemblance to the task of named entity recognition (NER) which usually makes reference to a ‘person’ class. However, whereas most NER systems make extensive use of orthographic, morphological or contextual clues (titles, suffixes) and gazetteers, animacy for nouns is not signaled overtly in the same way. Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus. This is thus equivalent to treatment of animacy as a lexical semantic property and the classification strategy is based on generalization of morphosyntactic behaviour of common nouns over large quantities of data. Due to the small size of the Talbanken05 treebank and the small amount of variation, this strategy was pursued for the acquisition of animacy information. In the animacy classification of common nouns we exploit well-documented correlations between morphosyntactic"
E09-1072,W04-0216,0,0.270647,"Missing"
E09-1072,J01-3003,0,0.151227,"resource in order to classify animate referents. However, such a method is clearly restricted to languages for which large scale lexical resources, such as the WordNet, are available. The task of animacy classification bears some resemblance to the task of named entity recognition (NER) which usually makes reference to a ‘person’ class. However, whereas most NER systems make extensive use of orthographic, morphological or contextual clues (titles, suffixes) and gazetteers, animacy for nouns is not signaled overtly in the same way. Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus. This is thus equivalent to treatment of animacy as a lexical semantic property and the classification strategy is based on generalization of morphosyntactic behaviour of common nouns over large quantities of data. Due to the small size of the Talbanken05 treebank and the small amount of variation, this strategy was pursued for the acquisition of animacy information. In the animacy classification of common nouns we exploit well-documented correlat"
J12-2005,A00-1031,0,0.0143254,"8, Number 2 exhibited GENIA tokenization problems. Our pre-processing approach thus deploys a cascaded ﬁnite-state tokenizer (borrowed and adapted from the open-source English Resource Grammar: Flickinger [2002]), which aims to implement the tokenization decisions made in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)—much like GENIA, in principle—but more appropriately treating corner cases like the ones noted here. 2.2 PoS Tagging and Lemmatization For part-of-speech (PoS) tagging and lemmatization, we combine GENIA (with its built-in, occasionally deviant tokenizer) and TnT (Brants 2000), which operates on pre-tokenized inputs but in its default model is trained on ﬁnancial news from the Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy provided by GENIA in the biomedical domain, while using our improved tokenization and producing inputs to the parsers that as much as possible resemble the conventions used in the original training data for the (dependency) parser (the Penn Treebank, once again). To this effect, for the vast majority of tokens we can align the GENIA tokenization with our own, and in these cases we typically use GENIA PoS tags"
J12-2005,W02-1503,0,0.0392941,"f linguistic information to draw upon for the speculation resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classiﬁers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take into account additional features. The procedure to enable the data-driven parser to learn from the grammar-driven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al. 2008) and the English grammar developed within the ParGram project (Butt et al. 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank—one gold-standard and one with LFG annotation. We extend the gold-standard treebank with additional information from the corresponding LFG analysis and train MaltParser on the enhanced data set. For a description of the parse model features and the dependency substructures proposed by XLE for each word token, see Nivre and McDonald (2008). For further background on the conversion and training procedures, see Øvrelid, Kuhn, and Spreyer (2009). Table 5 shows the enhanced dependency re"
J12-2005,W10-3110,0,0.168931,"Missing"
J12-2005,W01-0521,0,0.0163915,"urrent set of scope rules, or annotation of parallel constructions may in some cases differ in subtle ways (see Section 6.1.5). The overﬁtting effects caused by the data dependencies introduced by the various GENIA-based domain adaptation steps, as described in Section 2.3, must also be taken into account. 6.1.4 PoS Tagging and Domain Variation. As mentioned in Section 6.1.1, an advantage of stacking with a general-purpose LFG parser is that it can be expected to aid domain portability. Nonetheless, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea 2001), and our parser is trained on ﬁnancial news. MaltParser presupposes that inputs have been PoS tagged, however, leaving room for variation in preprocessing. In this article we have aimed, on the one hand, to make parser inputs conform as much as possible to the conventions established in its PTB training data, while on the other hand taking advantage of specialized resources for the biomedical domain. To assess the impact of improved, domain-adapted inputs on our scope resolution rules, we contrast two conﬁgurations: Running the parser in the exact same manner as Øvrelid, Kuhn, and Spreyer (20"
J12-2005,W07-2416,0,0.0153954,"e XLE analysis (Features). Id Form PoS Features XHead XDep Head DepRel 1 2 3 4 5 6 7 8 9 10 11 The unknown amino acid may be used by these species . DT JJ JJ NN MD VB VBN IN DT NNS . _ degree:attributive degree:attributive pers:3|case:nom|num:sg|ntype:common mood:ind|subcat:MODAL|tense:pres|clauseType:decl _ subcat:V-SUBJ-OBJ|vtype:main|passive:+ _ deixis:proximal num:pl|pers:3|case:obl|common:count|ntype:common _ 4 4 4 3 0 7 5 9 10 7 0 SPECDET ADJUNCT ADJUNCT SUBJ ROOT PHI XCOMP PHI SPECDET OBL-AG PUNC 4 4 4 5 0 5 6 7 10 8 5 NMOD NMOD NMOD SBJ ROOT VC VC LGS NMOD PMOD P to dependency format (Johansson and Nugues 2007) and extended with XLE features, as described previously. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8, which is lower than the current stateof-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and Nivre 2011, although not directly comparable given that they test exclusively on WSJ Section 23), but with the advantage of providing us with the deep linguistic information from the XLE. 6.1.2 Rule Overview. Our sco"
J12-2005,P99-1069,0,0.0433025,"English Resource Grammar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to a syntactic constituent and secondly, that we can automatically learn a ranking function that selects the correct constituent. Our ranking approach to scope resolution is abstractly related to statistical parse selection, and in particular work on discriminative parse selection for uniﬁcation based grammars, such as those by Johnson et al. (1999), Riezler et al. (2002), Malouf and van Noord (2004), and Toutanova et al. (2005). The overall goal is to learn a function for ranking syntactic structures, based on training data that annotates which tree(s) are correct and incorrect for each sentence. In our case, however, rather than discriminating between complete analyses for a given sentence, we want to learn a ranking function over candidate subtrees (i.e., constituents) within a parse (or possibly even within several parses). Figure 3 presents an example derivation tree that represents a complete HPSG analysis. Starting from the cue an"
J12-2005,W10-3010,0,0.271449,"extended on this system by also adding syntactic features, resulting in the top performing system of the CoNLL-2010 Shared Task at the scope-level (corresponding to the second subtask). It is interesting to note that all the top performers use various types of syntactic information in their scope resolution systems: The output from a dependency parser (MaltParser) (Morante, van Asch, and Daelemans 2010; Velldal, Øvrelid, and Oepen 2010), a tag sequence grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010). The majority of systems perform classiﬁcation at the token level, using some variant of machine learning with a BIO classiﬁcation scheme and a post-processing step to assemble the full scope (Farkas et al. 2010), although several of the top performers employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, Øvrelid, and Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010). 5. Identifying Speculation Cues We now turn to look at the details of our own system, starting in this section with describing a simple yet effective approach to identifying"
J12-2005,W04-3103,0,0.271137,"Missing"
J12-2005,I11-1028,1,0.819467,"carry over to any downstream components using this information. For the experiments described in this article, GENIA supplies lemmas for the n-gram features used by the cue classiﬁers, as well as PoS tags used in the input to both the dependency parser and the Head-driven Phrase Structure Grammar (HPSG) parser (which in turn provide the inputs to our various scope resolution components). For the HPSG parser, a subset of the GENIA corpus was also used as part of the training data for estimating an underlying statistical parse selection model, producing n-best lists of ranked candidate parses (MacKinlay et al. 2011). When reporting ﬁnal test results on the full papers (BSP or BSE) or the clinical reports (BSR), no such dependencies between information sources exists. It does mean, however, that we can reasonably expect to see some extra drop in performance when going from development results on data that includes the BioScope abstracts to the test results on these other data sets. 372 Velldal et al. Rules, Rankers, and the Role of Syntax 3. Evaluation Measures In this section we seek to clarify the type of measures we will be using for evaluating both the cue detection components (Section 3.1) and the sc"
J12-2005,J93-2004,0,0.0406023,"Missing"
J12-2005,D08-1017,0,0.00926869,".1.1) and quantifying the effect of using a domain-adapted PoS tagger (Section 6.1.4). 6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source MaltParser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-dr"
J12-2005,P07-1125,0,0.0169926,"a given system. The sequence of boolean values that results (FP = 0, TP = 1) can be directly paired with the corresponding sequence for a different system so that the sign-test can be applied as above. Note that our modiﬁed scorer for negation is available from our Web page of supplemental materials,2 together with the system output (in XML following the BioScope DTD) for all end-to-end runs with our ﬁnal model conﬁgurations. 4. Related Work on Speculation Labeling Although there exists a body of earlier work on identifying uncertainty on the sentence level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the task of resolving the in-sentence scope of speculation cues was ﬁrst pioneered by Morante and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al. 2010) entered largely uncharted territory and contributed to an increased interest for this task. Virtually all systems for resolving speculation scope implement a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem,"
J12-2005,morante-2010-descriptive,0,0.138902,"ules applied directly without modiﬁcations achieve 48.67 and 56.25. In order to further improve on these results, we introduce a few new rules to account speciﬁcally for negation. The general rule machinery is identical to the speculation scope rules described in Section 6.1: The rules are triggered by the part of speech of the cue and operate over the dependency representations output by the stacked dependency parser described in Section 6.1.1. In developing the rules we consulted the BioScope guidelines (Vincze et al. 2008), as well as a descriptive study of negation in the BioScope corpus (Morante 2010). 401 Computational Linguistics Volume 38, Number 2 Table 13 Additional dependency-based scope rules for negation, with information source (MaltParser or XLE), organized by PoS of the cue. PoS DT NN NNnone VB RBvb RBother Description Source Determiners scope over their head node and its descendants Nouns scope over their descendants none take scope over entire sentence if subject and otherwise over its descendants Verbs scope over their descendants Adverbs with verbal head scope over the descendants of the lexical verb Adverbs scope over the descendants of the head M M M M M, X M, X 7.2.1 Rule"
J12-2005,W09-1304,0,0.0634709,"d biomedical articles for evaluation purposes, constituting an additional 5,003 utterances. This latter data set (also detailed in Table 1) will be used for held-out testing of our speculation models. We will be using the following abbreviations when referring to the various parts of the data: BSA (BioScope abstracts), BSP (full papers), BSE (the heldout evaluation data), and BSR (clinical reports). Note that, when we get to the negation task we will be using the original version of the BioScope data. Furthermore, as BSE does not annotate negation, we instead follow the experimental set-up of Morante and Daelemans (2009b) for the negation task, reporting 10-fold cross validation on BSA and held-out testing on BSP and BSR. 2.1 Tokenization The BioScope data (and other data sets in the CoNLL-2010 Shared Task), are provided sentence-segmented only, and otherwise non-tokenized. Unsurprisingly, the GENIA tagger (Tsuruoka et al. 2005) has a central role in our pre-processing set-up. We found that its tokenization rules are not always optimally adapted for the type of text in BioScope, however. For example, GENIA unconditionally introduces token boundaries for some punctuation marks that can also occur token-intern"
J12-2005,W09-1105,0,0.644974,"(2009b) Cue classiﬁer & Scope Rules + Ranking 66.31 69.30 65.27 72.89 65.79 71.05 BSP Held-out Morante et al. (2009b) Cue classiﬁer & Scope Rules + Ranking 42.49 58.58 39.10 68.09 40.72 62.98 BSR Held-out Scope Level Data Morante et al. (2009b) Cue classiﬁer & Scope Rules + Ranking 74.03 89.62 70.54 89.41 72.25 89.52 405 Computational Linguistics Volume 38, Number 2 To some degree, some of the differences are to be expected, perhaps, at least with respect to BSP. For example, the BSP evaluation represents a held-out setting for both the cue and scope component in the machine learned system of Morante and Daelemans (2009b). While also true for our cue classiﬁer and subtree ranker, it is not strictly speaking the case for the dependency rules, and so the potential effect of any overﬁtting during learning might be less visible. The small set of manually deﬁned rules are general in nature, targeting the general syntactic constructions expressing negation, as shown in Table 13. In addition to being based on the BioScope annotation guidelines, however, both the abstracts and the full papers were consulted for patterns, and the fact that rule development has included intermediate testing on BSP (although mostly dur"
J12-2005,D08-1075,0,0.0736588,"Missing"
J12-2005,W10-3006,0,0.343028,"Missing"
J12-2005,nivre-etal-2006-maltparser,0,0.110423,"Missing"
J12-2005,P08-1108,0,0.0459012,"the effect of using a domain-adapted PoS tagger (Section 6.1.4). 6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source MaltParser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-driven dependency parsing an"
J12-2005,C10-1155,1,0.869645,"Missing"
J12-2005,N07-1051,0,0.0167452,"system output of Morante and Daelemans (2009b), however, we also computed cue-level scores for their system. Morante and Daelemans (2009b) identify cues using a small list of unambiguous cue words compiled from the abstracts in combination with applying a decision tree classiﬁer to the remaining words. Their features record information about neighboring word forms, PoS, and chunk information from GENIA. Zhu et al. (2010) train an SVM to classify tokens according to a BIO-scheme using surface-oriented n-gram features in addition to various syntactic features extracted using the Berkley parser (Petrov and Klein 2007) trained on the GENIA treebank. Looking at the results in Table 12, we see that the performance of our cue classiﬁer compares favorably with the systems of both Morante and Daelemans (2009b) and Zhu et al. (2010), achieving a higher cue-level F1 across all data sets (with differences in classiﬁer decisions with respect to Morante and Daelemans [2009b] being statistically signiﬁcant for all of them). For the 10-fold run, the biggest difference concerns token-level precision, where both the system of Zhu et al. (2010) and our own achieves a substantially higher score than that of Morante and Dae"
J12-2005,W10-3008,0,0.0167739,"ask: as a sequence labeling task and using only token-level, lexical information. Morante, van Asch, and Daelemans (2010) then extended on this system by also adding syntactic features, resulting in the top performing system of the CoNLL-2010 Shared Task at the scope-level (corresponding to the second subtask). It is interesting to note that all the top performers use various types of syntactic information in their scope resolution systems: The output from a dependency parser (MaltParser) (Morante, van Asch, and Daelemans 2010; Velldal, Øvrelid, and Oepen 2010), a tag sequence grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010). The majority of systems perform classiﬁcation at the token level, using some variant of machine learning with a BIO classiﬁcation scheme and a post-processing step to assemble the full scope (Farkas et al. 2010), although several of the top performers employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, Øvrelid, and Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010). 5. Identifying Speculation Cues We now tur"
J12-2005,P02-1035,0,0.0191491,"ar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to a syntactic constituent and secondly, that we can automatically learn a ranking function that selects the correct constituent. Our ranking approach to scope resolution is abstractly related to statistical parse selection, and in particular work on discriminative parse selection for uniﬁcation based grammars, such as those by Johnson et al. (1999), Riezler et al. (2002), Malouf and van Noord (2004), and Toutanova et al. (2005). The overall goal is to learn a function for ranking syntactic structures, based on training data that annotates which tree(s) are correct and incorrect for each sentence. In our case, however, rather than discriminating between complete analyses for a given sentence, we want to learn a ranking function over candidate subtrees (i.e., constituents) within a parse (or possibly even within several parses). Figure 3 presents an example derivation tree that represents a complete HPSG analysis. Starting from the cue and working through the t"
J12-2005,P08-1033,0,0.0127087,"nce of boolean values that results (FP = 0, TP = 1) can be directly paired with the corresponding sequence for a different system so that the sign-test can be applied as above. Note that our modiﬁed scorer for negation is available from our Web page of supplemental materials,2 together with the system output (in XML following the BioScope DTD) for all end-to-end runs with our ﬁnal model conﬁgurations. 4. Related Work on Speculation Labeling Although there exists a body of earlier work on identifying uncertainty on the sentence level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the task of resolving the in-sentence scope of speculation cues was ﬁrst pioneered by Morante and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al. 2010) entered largely uncharted territory and contributed to an increased interest for this task. Virtually all systems for resolving speculation scope implement a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem, putting emphasi"
J12-2005,W10-3002,0,0.723173,"a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem, putting emphasis of the best performers from the two corresponding subtasks of the CoNLL-2010 Shared Task, cue detection (Task 1) and scope resolution (Task 2). 4.1 Related Work on Identifying Speculation Cues The top-ranked system for Task 1 in the ofﬁcial CoNLL-2010 Shared Task evaluation approached cue identiﬁcation as a sequence labeling problem (Tang et al. 2010). Similarly to the decision-tree approach of Morante and Daelemans (2009a), Tang et al. (2010) set out to label tokens according to a BIO-scheme; indicating whether they are at the Beginning, Inside, or Outside of a speculation cue. In the “cascaded” system architecture of Tang et al. (2010), the predictions of both a Conditional Random Field (CRF) sequence classiﬁer and an SVM-based Hidden Markov Model (HMM) are both combined in a second CRF. In terms of the overall approach, namely, viewing the problem as a sequence labeling task, Tang et al. (2010) are actually representative of the majorit"
J12-2005,W10-3007,1,0.912943,"Missing"
J12-2005,W08-0606,0,0.622666,"ce in this respect, where the topic was speculation detection for the domain of biomedical research literature ∗ University of Oslo, Department of Informatics, PB 1080 Blindern, 0316 Oslo, Norway. E-mail: {erikve,liljao,jread,oe}@ifi.uio.no. Submission received: 5 April 2011; revised submission received: 30 September 2011; accepted for publication: 2 December 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 2 (Farkas et al. 2010). This particular area has been the focus of much current research, triggered by the release of the BioScope corpus (Vincze et al. 2008)—a collection of scientiﬁc abstracts, full papers, and clinical reports with manual annotations of words that signal speculation or negation (so-called cues), as well as of the scopes of these cues within the sentences. The following examples from BioScope illustrate how sentences are annotated with respect to speculation. Cues are here shown using angle brackets, with braces corresponding to their annotated scopes: (1) {The speciﬁc role of the chromodomain is unknown} but chromodomain swapping experiments in Drosophila {suggest that they {might be protein interaction modules}} [18]. (2)"
J12-2005,W10-3003,0,0.0796274,"Missing"
J12-2005,P09-1043,0,0.0183721,"nd genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-driven dependency parsing and “deep” parsing with a hand-crafted grammar—and thus provides us with a broad range of different types of linguistic information to draw upon for the speculation resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classiﬁers for"
J12-2005,P11-2033,0,0.0199828,"nt|ntype:common _ 4 4 4 3 0 7 5 9 10 7 0 SPECDET ADJUNCT ADJUNCT SUBJ ROOT PHI XCOMP PHI SPECDET OBL-AG PUNC 4 4 4 5 0 5 6 7 10 8 5 NMOD NMOD NMOD SBJ ROOT VC VC LGS NMOD PMOD P to dependency format (Johansson and Nugues 2007) and extended with XLE features, as described previously. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8, which is lower than the current stateof-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and Nivre 2011, although not directly comparable given that they test exclusively on WSJ Section 23), but with the advantage of providing us with the deep linguistic information from the XLE. 6.1.2 Rule Overview. Our scope resolution rules take as input a parsed sentence that has been further tagged with speculation cues. We assume the default scope to start at the cue word and span to the end of the sentence (modulo punctuation), and this scope also provides the baseline when evaluating our rules. In developing the rules, we made use of the information provided by the guidelines for scope annotation in the"
J12-2005,D10-1070,0,0.0588464,"Missing"
J12-2005,W07-2207,1,\N,Missing
J12-2005,W10-3001,0,\N,Missing
J12-2005,E99-1043,0,\N,Missing
K16-1012,J94-2001,0,0.338051,"et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work done on the related problem of unsupervised PoS acquisition. As discussed in Christodoulopoulos et al. (2010), we can separate at least three main directions within this line of work: Disambiguation approaches (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009) that start out from a dictionary providing possible tags for different words; prototype-driven approaches (Haghighi and Klein, 2006; Christodoulopoulos 3 PoS clusters in distributional models Our hypothesis is that for the majority of words their parts of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word em116 beddings do contain PoS-related data, the prop"
K16-1012,N13-1090,0,0.264179,"classes is indeed stored in distributional models. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing for their separation from words belonging to other parts of speech. Recently, among others, Tsuboi (2014) and Plank et al. (2016) have demonstrated how word embeddings can improve supervised PoS-tagging. Mikolov et al. (2013b) showed that there also exist regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, Liu et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work d"
K16-1012,D10-1056,0,0.112098,"continuum which does not exhibit sharp boundaries between the categories’. When annotating natural language texts for parts of speech, the choice of a PoS tag in many 115 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 115–125, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics et al., 2010) based on a small number of prototypical examples for each PoS; induction approaches that are completely unsupervised and make no use of prior knowledge. This is also the main focus of the comparative survey provided by (Christodoulopoulos et al., 2010). Work on PoS induction has a long history – including the use of distributional methods – going back at least to Schütze (1995), and recent work has demonstrated that word embeddings can be useful for this task as well (Yatbaz et al., 2012; Lin et al., 2015; Ling et al., 2015a). In terms of positioning this study relative to previous work, it falls somewhere in between the distinctions made above. It is perhaps closest to disambiguation approaches, but it is not unsupervised given that we make use of existing tag annotations when training our embeddings and predictors. The goal is also differ"
K16-1012,N06-1041,0,0.173179,"istributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work done on the related problem of unsupervised PoS acquisition. As discussed in Christodoulopoulos et al. (2010), we can separate at least three main directions within this line of work: Disambiguation approaches (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009) that start out from a dictionary providing possible tags for different words; prototype-driven approaches (Haghighi and Klein, 2006; Christodoulopoulos 3 PoS clusters in distributional models Our hypothesis is that for the majority of words their parts of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word em116 beddings do contain PoS-related data, the properly trained classifier will correctly predict PoS tags for the majority of words: it means that these lexical entities conform to a dominant distributional pattern of their part of spe"
K16-1012,petrov-etal-2012-universal,0,0.0709563,"Missing"
K16-1012,P16-2067,0,0.025077,"h. For several years already it has been known that some information about morphological word classes is indeed stored in distributional models. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing for their separation from words belonging to other parts of speech. Recently, among others, Tsuboi (2014) and Plank et al. (2016) have demonstrated how word embeddings can improve supervised PoS-tagging. Mikolov et al. (2013b) showed that there also exist regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, Liu et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. V"
K16-1012,P09-1057,0,0.0238424,"same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work done on the related problem of unsupervised PoS acquisition. As discussed in Christodoulopoulos et al. (2010), we can separate at least three main directions within this line of work: Disambiguation approaches (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009) that start out from a dictionary providing possible tags for different words; prototype-driven approaches (Haghighi and Klein, 2006; Christodoulopoulos 3 PoS clusters in distributional models Our hypothesis is that for the majority of words their parts of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word em116 beddings do contain PoS-related data, the properly trained classifier will correctly predict PoS ta"
K16-1012,N15-1144,0,0.0459081,"Missing"
K16-1012,E95-1020,0,0.444155,"of a PoS tag in many 115 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 115–125, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics et al., 2010) based on a small number of prototypical examples for each PoS; induction approaches that are completely unsupervised and make no use of prior knowledge. This is also the main focus of the comparative survey provided by (Christodoulopoulos et al., 2010). Work on PoS induction has a long history – including the use of distributional methods – going back at least to Schütze (1995), and recent work has demonstrated that word embeddings can be useful for this task as well (Yatbaz et al., 2012; Lin et al., 2015; Ling et al., 2015a). In terms of positioning this study relative to previous work, it falls somewhere in between the distinctions made above. It is perhaps closest to disambiguation approaches, but it is not unsupervised given that we make use of existing tag annotations when training our embeddings and predictors. The goal is also different; rather than performing PoS acquisition or tagging for its own sake, the main focus here is on analyzing the boundaries of d"
K16-1012,D15-1161,0,0.0187252,"ermany, August 7-12, 2016. 2016 Association for Computational Linguistics et al., 2010) based on a small number of prototypical examples for each PoS; induction approaches that are completely unsupervised and make no use of prior knowledge. This is also the main focus of the comparative survey provided by (Christodoulopoulos et al., 2010). Work on PoS induction has a long history – including the use of distributional methods – going back at least to Schütze (1995), and recent work has demonstrated that word embeddings can be useful for this task as well (Yatbaz et al., 2012; Lin et al., 2015; Ling et al., 2015a). In terms of positioning this study relative to previous work, it falls somewhere in between the distinctions made above. It is perhaps closest to disambiguation approaches, but it is not unsupervised given that we make use of existing tag annotations when training our embeddings and predictors. The goal is also different; rather than performing PoS acquisition or tagging for its own sake, the main focus here is on analyzing the boundaries of different PoS classes. In Section 5, this analysis is complemented by experiments with using word embeddings for PoS prediction on unlabeled data, and"
K16-1012,silveira-etal-2014-gold,0,0.0485505,"Missing"
K16-1012,N15-1142,0,0.0194484,"ermany, August 7-12, 2016. 2016 Association for Computational Linguistics et al., 2010) based on a small number of prototypical examples for each PoS; induction approaches that are completely unsupervised and make no use of prior knowledge. This is also the main focus of the comparative survey provided by (Christodoulopoulos et al., 2010). Work on PoS induction has a long history – including the use of distributional methods – going back at least to Schütze (1995), and recent work has demonstrated that word embeddings can be useful for this task as well (Yatbaz et al., 2012; Lin et al., 2015; Ling et al., 2015a). In terms of positioning this study relative to previous work, it falls somewhere in between the distinctions made above. It is perhaps closest to disambiguation approaches, but it is not unsupervised given that we make use of existing tag annotations when training our embeddings and predictors. The goal is also different; rather than performing PoS acquisition or tagging for its own sake, the main focus here is on analyzing the boundaries of different PoS classes. In Section 5, this analysis is complemented by experiments with using word embeddings for PoS prediction on unlabeled data, and"
K16-1012,N03-1033,0,0.0596725,"m Hibs fans in Edinburgh pre season?’), they are very close to adjectives or adverbs, so the predictions of the distributional classifier once again suggest shifting parts of speech boundaries a bit. Error analysis on the vocabulary from the Universal Dependencies Treebank showed pretty much the same results, except for some differences already mentioned above. There exists another way to retrieve this kind of data: to process tagged data with a conventional PoS tagger and analyze the resulting confusion matrix. We tested this approach by processing the whole BNC with the Stanford PoS Tagger (Toutanova et al., 2003). Note that as an input to the tagger we used not the whole sentences from the corpora, but separate tokens, to mimic our # 172675 47202 40218 24075 9723 Actual Predicted NNP VB JJ NN JJ NN NN NN JJ VB workflow with the distributional predictor. Prior to this, BNC tags were converted to the Penn Treebank tagset3 to match the output of the tagger. As we are interested in coarse, ‘overarching’ word classes, inflectional forms were merged into one tag. That was easy to accomplish by dropping all characters of the tags after the first two (excluding proper noun tags, which were all converted to NN"
K16-1012,D14-1101,0,0.0263202,"to parts of speech. For several years already it has been known that some information about morphological word classes is indeed stored in distributional models. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing for their separation from words belonging to other parts of speech. Recently, among others, Tsuboi (2014) and Plank et al. (2016) have demonstrated how word embeddings can improve supervised PoS-tagging. Mikolov et al. (2013b) showed that there also exist regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, Liu et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation"
K16-1012,D15-1243,0,0.0196381,"s should retain some similarity allowing for their separation from words belonging to other parts of speech. Recently, among others, Tsuboi (2014) and Plank et al. (2016) have demonstrated how word embeddings can improve supervised PoS-tagging. Mikolov et al. (2013b) showed that there also exist regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, Liu et al. (2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and Tsvetkov et al. (2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing an evaluation dataset based on this. Various types of distributional information has also played an important role in previous work done on the related problem of unsupervised PoS acquisition. As discussed in Christodoulopoulos et al. (2010), we can separate at least three main directions within this line of work: Disambiguation approaches (Merialdo, 1994; Toutanova and Johnson, 2007; Ravi and Knight, 2009) that start out from a dictionary providing possible tags for different words; p"
K16-1012,D12-1086,0,0.0413494,"Missing"
K16-1012,L16-1262,0,\N,Missing
K16-1012,J15-4006,0,\N,Missing
K16-2002,J12-2005,1,0.934737,"o an overall account of discourse structure and of having annotation decisions concentrate on the individual instances of discourse relations, rather than on their interactions. Previous work on this task has usually broken it down into a set of sub-problems, which are solved in a pipeline architecture (roughly: identify connectives, then arguments, then discourse senses; Lin et al., 2014). While adopting a similar pipeline approach, the OPT discourse parser also builds on and extends a method that has previously achieved state-of-the-art results for the detection of speculation and negation (Velldal et al., 2012; Read 3 Relation Identification Explicit Connectives Our classifier for detecting explicit discourse connectives extends the work by Velldal et al. (2012) for identifying expressions of speculation and negation. The approach treats the set of connectives observed in the training data as a closed class, and ‘only’ attempts to disambiguate occurrences of these token sequences in new data. Connectives can be single- or multitoken sequences (e.g. ‘as’ vs. ‘as long as’). In cases 20 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 20–26, c Berlin, Germ"
K16-2002,K15-2002,0,0.285154,"r sense classifier described has been developed specifically for OPT. The OPT submission to the Shared Task of the 2016 Conference on Natural Language Learning (CoNLL) implements a ‘classic’ pipeline architecture, combining binary classification of (candidate) explicit connectives, heuristic rules for non-explicit discourse relations, ranking and ‘editing’ of syntactic constituents for argument identification, and an ensemble of classifiers to assign discourse senses. With an end-toend performance of 27.77 F1 on the English ‘blind’ test data, our system advances the previous state of the art (Wang & Lan, 2015) by close to four F1 points, with particularly good results for the argument identification sub-tasks. 1 2 System Architecture Our system overview is shown in Figure 1. The individual modules interface through JSON files which resemble the desired output files of the Task. Each module adds the information specified for it. We will describe them here in thematic blocks, while the exact order of the modules can be seen in the figure. Relation identification (§3) includes the detection of explicit discourse connectives and the stipulation of non-explicit relations. Our argument identification mod"
K16-2002,K15-2001,0,0.136259,"21.6 48.1 27.8 F1 91.8 52.4 75.2 44.0 34.5 64.6 76.4 52.0 21.9 48.2 27.8 Table 3: Per-component breakdown of system performance, compared to top performers in 2015/16. 6 optimizing the primal objective and setting the error penalty term C to 0.3. Experimental Results Overall Results Table 3 summarizes OPT system performance in terms of the metrics computed by the official scorer for the Shared Task, against both the WSJ and ‘blind’ test sets. To compare against the previous state of the art, we include results for the top-performing systems from the 2015 and 2016 competitions (as reported by Xue et al., 2015, and Xue et al., 2016, respectively). Where applicable, best results (when comparing F1 ) are highlighted for each sub-task and -metric. The highlighting makes it evident that the OPT system is competitive to the state of the art across the board, but particularly so on the argument identification sub-task and on the ‘blind’ test data: In terms of the WSJ test data, OPT would have ranked second in the 2015 competition, but on the ‘blind’ data it outperforms the previous state of the art on all but one metric for which contrastive results are provided by Xue et al.. Where earlier systems tend"
K16-2002,P09-2004,0,0.792028,"ve heads only, these are the unit of disambiguation in OPT. Disambiguation is performed as point-wise (‘per-connective’) classification using the support vector machine implementation of the SVMlight toolkit (Joachims, 1999). Tuning of feature configurations and the error-to-margin cost parameter (C) was performed by ten-fold cross validation on the Task training set. candidate configurations against the development data. The model used in the system submission includes n-grams of up to three preceding and following positions, full feature conjunction for the ‘self’ and ‘parent’ categories of Pitler & Nenkova (2009), but limited conjunctions involving their ‘left’ and ‘right’ sibling categories, and none of the ‘connected context’ features suggested by Wang & Lan (2015). This model has some 1.2 million feature types. Non-Explicit Relations According to the PDTB guidelines, non-explicit relations must be stipulated between each pair of sentences iff four conditions hold: two sentences (a) are adjacent; (b) are located in the same paragraph; and (c) are not yet ‘connected’ by an explicit connective; and (d) a coherence relation can be inferred or an entity-based relation holds between them. We proceed stra"
K16-2002,prasad-etal-2008-penn,0,\N,Missing
K16-2002,K15-2003,0,\N,Missing
K16-2002,S12-1041,1,\N,Missing
K16-2002,K16-2001,0,\N,Missing
K18-2002,J13-3002,1,0.895149,"negation metric used in the EPE context—counting as true positives only perfectly retrieved full scopes, including an exact match on negated events. and PoS tags. Conversely, the opinion holder extraction and reranking modules make central use of structural information, i.e. paths and topological properties in one or more syntactico-semantic dependency graph(s). In the EPE context, we evaluated how well the participating systems extract the three types of structures mentioned above: expressions, holders, and polarities. In each case, soft-boundary precision and recall measures were computed (Johansson and Moschitti, 2013; Johansson, 2017). Furthermore, for the detailed analysis we evaluated the opinion holder extractor separately, using goldstandard opinion expressions. We refer to this task as in-vitro holder extraction, and this score is used for the overall ranking of submissions when averaging F1 scores across the three EPE downstream applications. The reason for highlighting this score is that it is the one most strongly affected by the design of the dependency representation. Participating Teams Nine teams participated in EPE 2017, in the order of overall rank: Stanford– Paris (Schuster et al., 2017), S"
K18-2002,W09-1401,0,0.358818,"generated to a large degree from syntactic dependency parses. All classification tasks are implemented using the SVMmulticlass classifier (Joachims, 1999). TEES has been developed using corpora from the Biomedical Natural Language Processing (BioNLP) domain, in particular the event corpora from the BioNLP Shared Tasks. These tasks define their own annotation schemes and provide standardized evaluation services. In the context of the EPE challenge we use the BioNLP 2009 GENIA corpus and its associated evaluation program to measure the impact of different parses on event extraction performance (Kim et al., 2009). The metric used for comparing the EPE submissions is the primary ‘approximate span and recursive mode’ metric of the original Shared Task, a micro-averaged F1 score for the nine event classes of the corpus. The specialized domain language presents unique challenges for parsers not specifically optimized for this domain, so using this data set to evaluate open-domain parses may result in overall lower performance than with parsers specifically trained on e.g. the GENIA treebank (Tateisi et al., 2005). When using the EPE parse data, TEES features encompass the type and direction for the depend"
K18-2002,J16-4009,1,0.850677,"ation in the context of EPE 2017 is interpreted as a graph whose nodes are anchored in surface lexical units, and whose edges represent labeled directed relations between two nodes. Each node corresponds to a sub-string of the underlying linguistic signal (input string), identified by character stand-off pointers. Node labels can comprise a non-recursive attribute–value matrix (or ‘feature structure’), for example to encode lemma and part of speech information. Each graph can optionally designate one or more ‘top’ nodes, broadly interpreted as the root-level head or highest-scoping predicate (Kuhlmann and Oepen, 2016). In principle, this notion of dependency representations is broad in that it allows nodes that do not correspond to (full) surface tokens, partial or full overlap of nodes, as well as graphs that transcend fully connected rooted trees. Participating teams in the original EPE 2017 initiative did in fact take advantage of all these degrees of freedom, whereas in connection to the 2018 UD parsing task such variation is excluded by design. Biological Event Extraction The Turku Event Extraction System (TEES) (Björne, 2014) is a program developed for the automated extraction of events, complex rela"
K18-2002,S12-1042,1,0.849881,"on tasks. The first step is entity detection where each token in the sentence is predicted as an entity node or as negative. In the second step of edge detection, argument edges are predicted for all valid, directed pairs of nodes. In the third, unmerging step, overlapping events are ‘pulled apart’ by duplicating trigger nodes. In the optional fourth step of modifier detection, binary modifiers (such as speculation or negation) can be predicted for the detected events. All of the classification steps in the TEES system Negation Resolution The EPE negation resolution system is called Sherlock (Lapponi et al., 2012, 2017) and implements the perspective on negation defined by Morante and Daelemans (2012) through the creation of the Conan Doyle Negation Corpus for the Shared Task of the 2012 Joint Conference on Lexical and Computational Semantics (*SEM 2012). Negation instances are annotated as tri-partite structures: Negation cues can be full tokens (e.g. not), multi-word expressions (by no means), or sub-tokens (un in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as"
K18-2002,P10-1052,0,0.0518508,"in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as negated events or states, provided that the sentence is factual and the events in question did not take place. In the EPE context, gold-standard negation cues are provided, because this sub-task has been found relatively insensitive to grammatical structure (Velldal et al., 2012). Sherlock approaches negation resolution as a sequence labeling problem, using a Conditional Ran23 dom Field (CRF) classifier (Lavergne et al., 2010). The token-wise negation annotations contain multiple layers of information. Tokens may or may not be negation cues and they can be either in or out of scope for a specific cue; in-scope tokens may or may not be negated events. Moreover, multiple negation instances may be (partially or fully) overlapping. Before presenting the CRF with the annotations, Sherlock ‘flattens’ all negation instances in a sentence, assigning a six-valued extended ‘begin– inside–outside’ labeling scheme. After classification, hierarchical (overlapping) negation structures are reconstructed using a set of post-proces"
K18-2002,de-marneffe-etal-2006-generating,0,0.18263,"Missing"
K18-2002,W12-3602,1,0.835691,"sity of Washington (Peng et al., 2017). These teams submitted 49 distinct runs that encompassed many different families of dependency representations, various approaches to preprocessing and parsing, and variable types and volumes of training data. The dependency representations employed by the participants varied from more syntactically oriented schemes—e.g. Stanford Basic (de Marneffe et al., 2006), CoNLL 2008–style (Surdeanu et al., 2008), and UD—to more semantically oriented representations, such as the Deep Syntactic Structures of Ballesteros et al. (2015), DELPH-IN MRS Dependencies (DM; Ivanova et al., 2012), or Enju Predicate–Argument Structures (PAS; Miyao, 2006). The teams also employed wildly variable volumes of training data, ranging from around 200,000 tokens (the English UD treebanks) to 1,7 million tokens (combining the venerable Wall Street Journal, Brown, and GENIA treebanks). Opinion Analysis The system by Johansson and Moschitti (2013) marks up expressions of opinion and emotion in a pipeline comprised of three separate classification steps, combined with endto-end reranking; it was previously generalized and adapted for the EPE framework by Johansson (2017). The system is based on th"
K18-2002,P13-2017,0,0.136318,"Missing"
K18-2002,S12-1035,0,0.0462694,"of the classifier include different combinations of token-level observations, such as surface forms, part-of-speech tags, lemmas, and dependency labels. In addition, we extract both token and dependency distance to the nearest cue, together with the full shortest dependency path. Standard evaluation measures from the original shared task include scope tokens (ST), scope match (SM), event tokens (ET), and full negation (FN) F1 scores. ST and ET are token-level scores for inscope and negated event tokens, respectively, where a true positive is a correctly retrieved token of the relevant class (Morante and Blanco, 2012). FN is the strictest of these measures and the primary negation metric used in the EPE context—counting as true positives only perfectly retrieved full scopes, including an exact match on negated events. and PoS tags. Conversely, the opinion holder extraction and reranking modules make central use of structural information, i.e. paths and topological properties in one or more syntactico-semantic dependency graph(s). In the EPE context, we evaluated how well the participating systems extract the three types of structures mentioned above: expressions, holders, and polarities. In each case, soft"
K18-2002,I05-2038,0,0.173019,"ted evaluation program to measure the impact of different parses on event extraction performance (Kim et al., 2009). The metric used for comparing the EPE submissions is the primary ‘approximate span and recursive mode’ metric of the original Shared Task, a micro-averaged F1 score for the nine event classes of the corpus. The specialized domain language presents unique challenges for parsers not specifically optimized for this domain, so using this data set to evaluate open-domain parses may result in overall lower performance than with parsers specifically trained on e.g. the GENIA treebank (Tateisi et al., 2005). When using the EPE parse data, TEES features encompass the type and direction for the dependencies combined wit the text span and a single part of speech for the tokens; lemmas are not used. The term (bi-lexical) dependency representation in the context of EPE 2017 is interpreted as a graph whose nodes are anchored in surface lexical units, and whose edges represent labeled directed relations between two nodes. Each node corresponds to a sub-string of the underlying linguistic signal (input string), identified by character stand-off pointers. Node labels can comprise a non-recursive attribut"
K18-2002,J12-2005,1,0.852735,"tances are annotated as tri-partite structures: Negation cues can be full tokens (e.g. not), multi-word expressions (by no means), or sub-tokens (un in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as negated events or states, provided that the sentence is factual and the events in question did not take place. In the EPE context, gold-standard negation cues are provided, because this sub-task has been found relatively insensitive to grammatical structure (Velldal et al., 2012). Sherlock approaches negation resolution as a sequence labeling problem, using a Conditional Ran23 dom Field (CRF) classifier (Lavergne et al., 2010). The token-wise negation annotations contain multiple layers of information. Tokens may or may not be negation cues and they can be either in or out of scope for a specific cue; in-scope tokens may or may not be negated events. Moreover, multiple negation instances may be (partially or fully) overlapping. Before presenting the CRF with the annotations, Sherlock ‘flattens’ all negation instances in a sentence, assigning a six-valued extended ‘beg"
K18-2002,P17-1186,0,0.022577,"Missing"
K18-2002,K18-2001,0,0.121439,"Missing"
L16-1250,de-marneffe-etal-2006-generating,0,0.195962,"Missing"
L16-1250,de-marneffe-etal-2014-universal,0,0.0695832,"Missing"
L16-1250,N13-1070,0,0.0226704,"dependency parsing experiments which report first results for the processing of the converted treebank. The full converted treebank was made available with the 1.2 release of the UD treebanks. Keywords: Dependency treebanks, Universal Dependencies, dependency parsing 1. Introduction With the increasing popularity of dependency-based representations of syntactic structure in recent years, a wealth of different dependency annotation schemes have surfaced. It has been shown that the choice of dependency scheme influences parsing results (Schwartz et al., 2012) as well as downstream applications (Elming et al., 2013) and even though attempts have been made to contrast different schemes theoretically (Ivanova et al., 2012), it is clear that the diversity of representation makes comparisons difficult. Cross-linguistically even more so, and it can often be difficult to tease apart aspects of annotation scheme from typological differences in cross-lingual learning (Søgaard, 2011; Skjærholt and Øvrelid, 2012). Universal Dependencies (UD) (de Marneffe et al., 2014; Nivre, 2015) is a recent community-driven effort to create cross-linguistically consistent syntactic annotation. UD is based on the Stanford depende"
L16-1250,gimenez-marquez-2004-svmtool,0,0.183432,"Missing"
L16-1250,W12-3602,1,0.844502,"e full converted treebank was made available with the 1.2 release of the UD treebanks. Keywords: Dependency treebanks, Universal Dependencies, dependency parsing 1. Introduction With the increasing popularity of dependency-based representations of syntactic structure in recent years, a wealth of different dependency annotation schemes have surfaced. It has been shown that the choice of dependency scheme influences parsing results (Schwartz et al., 2012) as well as downstream applications (Elming et al., 2013) and even though attempts have been made to contrast different schemes theoretically (Ivanova et al., 2012), it is clear that the diversity of representation makes comparisons difficult. Cross-linguistically even more so, and it can often be difficult to tease apart aspects of annotation scheme from typological differences in cross-lingual learning (Søgaard, 2011; Skjærholt and Øvrelid, 2012). Universal Dependencies (UD) (de Marneffe et al., 2014; Nivre, 2015) is a recent community-driven effort to create cross-linguistically consistent syntactic annotation. UD is based on the Stanford dependency scheme (de Marneffe et al., 2006) which has become a widely used dependency scheme for English in recen"
L16-1250,P13-2017,0,0.13518,"Missing"
L16-1250,nivre-etal-2006-talbanken05,0,0.1142,"Missing"
L16-1250,L16-1262,0,0.0931424,"Missing"
L16-1250,W15-5313,0,0.24043,"Missing"
L16-1250,petrov-etal-2012-universal,0,0.0938139,"g between NDT and UD parts-of-speech its main tenets are the primacy of content-words, i.e. content words, as opposed to function words, are syntactic heads wherever possible. It is intended to be a universal annotation scheme, i.e. applicable to any language, however also offers some possibilities for language-specific information. With reference to the NDT annotation choices in Table 1, the UD scheme adopts the reverse attachment for auxiliaries, infinitival markers and prepositions. 3. Parts-of-speech The part-of-speech tag set used in the UD scheme is based on the Universal PoS tag set of Petrov et al. (2012) and contains 17 tags. The NDT tag set contains 19 tags. The conversion of the part-of-speech information in NDT to the UD PoS tag set is fairly straightforward and largely relies on a direct mapping presented in Table 2. A few parts-ofspeech require conversion rules which make reference to additional information in the treebank, represented by disjunction in the mapping. Below we will discuss a few of these cases. The universal scheme makes a distinction between proper and common nouns at the part-of-speech level. This information can be found among the morphological features in NDT (prop), h"
L16-1250,P13-1051,0,0.0397437,"Missing"
L16-1250,W15-1821,0,0.0905524,". Cross-linguistically even more so, and it can often be difficult to tease apart aspects of annotation scheme from typological differences in cross-lingual learning (Søgaard, 2011; Skjærholt and Øvrelid, 2012). Universal Dependencies (UD) (de Marneffe et al., 2014; Nivre, 2015) is a recent community-driven effort to create cross-linguistically consistent syntactic annotation. UD is based on the Stanford dependency scheme (de Marneffe et al., 2006) which has become a widely used dependency scheme for English in recent years. A number of existing dependency treebanks have been converted to UD (Pyysalo et al., 2015; Nivre, 2014) and new data has also been annotated from scratch in order to enable multilingual parser development, cross-lingual learning and typological studies of syntactic structure. Treebanks involved in this effort represent a diverse range of languages such as English, German, Swedish, Spanish, Italian, Persian, Japanese, and the UD release 1.2 contains treebanks for as many as 33 different languages of varying sizes. This paper describes a fully automatic conversion procedure for the Norwegian Dependency Treebank (NDT) to UD. Due to differences both in the tag set, as well as structur"
L16-1250,rosa-etal-2014-hamledt,0,0.106338,"Missing"
L16-1250,C12-1147,0,0.0145564,"with the UD guidelines. We further present PoS tagging and dependency parsing experiments which report first results for the processing of the converted treebank. The full converted treebank was made available with the 1.2 release of the UD treebanks. Keywords: Dependency treebanks, Universal Dependencies, dependency parsing 1. Introduction With the increasing popularity of dependency-based representations of syntactic structure in recent years, a wealth of different dependency annotation schemes have surfaced. It has been shown that the choice of dependency scheme influences parsing results (Schwartz et al., 2012) as well as downstream applications (Elming et al., 2013) and even though attempts have been made to contrast different schemes theoretically (Ivanova et al., 2012), it is clear that the diversity of representation makes comparisons difficult. Cross-linguistically even more so, and it can often be difficult to tease apart aspects of annotation scheme from typological differences in cross-lingual learning (Søgaard, 2011; Skjærholt and Øvrelid, 2012). Universal Dependencies (UD) (de Marneffe et al., 2014; Nivre, 2015) is a recent community-driven effort to create cross-linguistically consistent"
L16-1250,P14-1088,0,0.111352,"Missing"
L16-1250,P11-2120,0,0.0246392,"e in recent years, a wealth of different dependency annotation schemes have surfaced. It has been shown that the choice of dependency scheme influences parsing results (Schwartz et al., 2012) as well as downstream applications (Elming et al., 2013) and even though attempts have been made to contrast different schemes theoretically (Ivanova et al., 2012), it is clear that the diversity of representation makes comparisons difficult. Cross-linguistically even more so, and it can often be difficult to tease apart aspects of annotation scheme from typological differences in cross-lingual learning (Søgaard, 2011; Skjærholt and Øvrelid, 2012). Universal Dependencies (UD) (de Marneffe et al., 2014; Nivre, 2015) is a recent community-driven effort to create cross-linguistically consistent syntactic annotation. UD is based on the Stanford dependency scheme (de Marneffe et al., 2006) which has become a widely used dependency scheme for English in recent years. A number of existing dependency treebanks have been converted to UD (Pyysalo et al., 2015; Nivre, 2014) and new data has also been annotated from scratch in order to enable multilingual parser development, cross-lingual learning and typological stud"
L16-1250,solberg-etal-2014-norwegian,1,0.695731,"Missing"
L16-1250,zeman-2008-reusable,0,0.266226,"Missing"
L18-1228,P14-1023,0,0.0443131,"n language technology, there is no agreement in the community on the best ways to evaluate these semantic representations of language2 . There exist a variety of benchmarks which are widely employed to assess the quality of word representations and to compare different distributional semantic models. Existing evaluation methods can largely be separated into two categories: ”intrinsic evaluation” and ”extrinsic evaluation”. Intrinsic evaluation tries to directly quantify how well various kinds of linguistic regularities can be detected with the model independent of its downstream applications (Baroni et al., 2014; Schnabel et al., 2015). On the other hand, the quality of a word vector may be assessed by its performance in downstream tasks through measuring changes in performance metrics specific to the tasks by extrinsic evaluation. The downstream language technology tasks on which the quality of a word embedding is examined, fall into syntactic (e.g. POS tagging, Chunking) and semantic (e.g. Named entity recognition, Sentiment Classification) categories (Schnabel et al., 2015; Chiu et al., 2016b). In this work we evaluate domain-specific word embedding models using both intrinsic and extrinsic evalua"
L18-1228,W16-2922,0,0.159693,"nds of linguistic regularities can be detected with the model independent of its downstream applications (Baroni et al., 2014; Schnabel et al., 2015). On the other hand, the quality of a word vector may be assessed by its performance in downstream tasks through measuring changes in performance metrics specific to the tasks by extrinsic evaluation. The downstream language technology tasks on which the quality of a word embedding is examined, fall into syntactic (e.g. POS tagging, Chunking) and semantic (e.g. Named entity recognition, Sentiment Classification) categories (Schnabel et al., 2015; Chiu et al., 2016b). In this work we evaluate domain-specific word embedding models using both intrinsic and extrinsic evaluation schemes. Although, word embeddings techniques have drawn significant interest in the field, they are not well equipped to deal with unseen and infrequent words, nor do they consider word relations found in knowledge resources. Recently, different solutions have been proposed to overcome these limitations (Pilehvar and Collier, 2016; Faruqui et al., 2015; Yu and Dredze, 2014). Among these, we choose Faruqui et al. (2015) in this work since it is a post-processing approach which is st"
L18-1228,W16-2501,0,0.351652,"nds of linguistic regularities can be detected with the model independent of its downstream applications (Baroni et al., 2014; Schnabel et al., 2015). On the other hand, the quality of a word vector may be assessed by its performance in downstream tasks through measuring changes in performance metrics specific to the tasks by extrinsic evaluation. The downstream language technology tasks on which the quality of a word embedding is examined, fall into syntactic (e.g. POS tagging, Chunking) and semantic (e.g. Named entity recognition, Sentiment Classification) categories (Schnabel et al., 2015; Chiu et al., 2016b). In this work we evaluate domain-specific word embedding models using both intrinsic and extrinsic evaluation schemes. Although, word embeddings techniques have drawn significant interest in the field, they are not well equipped to deal with unseen and infrequent words, nor do they consider word relations found in knowledge resources. Recently, different solutions have been proposed to overcome these limitations (Pilehvar and Collier, 2016; Faruqui et al., 2015; Yu and Dredze, 2014). Among these, we choose Faruqui et al. (2015) in this work since it is a post-processing approach which is st"
L18-1228,N15-1184,0,0.437992,"rocarbons. The vocabulary is technical and there is a real need for NLP tools to aid the work process. In this work we investigate whether word embedding models can capture domain-specific semantic relations by training domainspecific embeddings1 and evaluating these against a terminological resource. We conduct a comprehensive study including a wide range of evaluation criteria, contrasting several general and domain specific embedding models. We augment the domain-specific embeddings using a domain knowledge resource. To supply embeddings for rare words, we extend the retrofitting method by Faruqui et al. (2015). We then go on to examine the contribution of these models in the performance of a downstream classification task. 1 Link to the domain-specific model: http://vectors. nlpl.eu/repository/11/75.zip Related work Despite the pervasive use of word embedding in language technology, there is no agreement in the community on the best ways to evaluate these semantic representations of language2 . There exist a variety of benchmarks which are widely employed to assess the quality of word representations and to compare different distributional semantic models. Existing evaluation methods can largely be"
L18-1228,N16-2002,0,0.0400575,"Missing"
L18-1228,D16-1057,0,0.0234848,"ents a challenge to NLP applications. Recently, word embedding models have been shown to capture a range of semantic relations relevant to the interpretation of lexical items (Mikolov et al., 2013b) and furthermore provide useful input representations for a range of downstream tasks (Collobert et al., 2011). The majority of work dealing with intrinsic evaluation of word embeddings has focused on general domain embeddings and semantic relations between frequent and generic terms. However, it has been shown that embeddings differ from one domain to another due to lexical and semantic variation (Hamilton et al., 2016; Bollegala et al., 2015). Domain-specific terms are challenging for general domain embeddings since there are few statistical clues in the underlying corpora for these items (Bollegala et al., 2015; Pilehvar and Collier, 2016). The Oil and Gas domain is a highly technical and dataintensive domain. Experts working within this domain daily investigate selected geographical areas and use relevant information (scientific articles, reports and other textual sources) to evaluate the potential for undiscovered hydrocarbons. The vocabulary is technical and there is a real need for NLP tools to aid th"
L18-1228,J15-4004,0,0.217448,"Missing"
L18-1228,D14-1181,0,0.00523913,"91 38 483 298 387 49 Table 11: Classification data set The resulting data set contains 1,348 sentences in which experts assigned each sentence to 7 different properties. The sentences are pre-processed using the same approach as described in Section 3.2. Table 11 depicts the properties and number of sentences for each. It can be seen that the data set is unbalanced regarding to the properties and that the downstream task is a multi-label classification task. 7.2. Multi-label Classification Model We use a slight variant of the Convolutional Neural Network (CNN) architecture that is proposed by Kim (2014) for a sentence classification task. We keep the value of hyperparameters equal to the ones that are reported in the original work, however we update the dimension of the embeddings layer according to the dimension of the domainspecific embeddings model. Furthermore, since the architecture aims to assign a single label to each sentence, we update the activation function to sigmoid at the output layer, which produces a probability for each of the potential properties. During training, these probabilities are used to compute the error, while during testing, we round each of the probabilities to"
L18-1228,Q15-1016,0,0.317925,"traction process. We further proceed with the domain specific model generation by creating two sets of embeddings, employing both the CBOW and the Skip-gram architectures with default settings. In the initial evaluation step, we compare the outcomes of these two models to determine the better architecture. We then go on to compare different settings for the hyperparameters, while keeping all other settings constant. It has been shown that optimizations of hyper parameters and certain system choices constitute the main causes of differences in performance rather than the algorithms themselves (Levy et al., 2015). Here we investigate the impact of various system design choices in the evaluation of domain specific embeddings across the following parameters 4 : I) Vector size: dim ∈ 50, 100, 200, 300, 400, 500, 600 II) Context window size: win ∈ 2, 3, 5, 10, 15, 20. III) Negative sampling size: neg ∈ 3, 5, 10, 15. IV) Frequency cut off: min.count ∈ 2, 3, 5, 10. V) n-most-similar: The parameter n for top n-most-similar as output is fixed at value 5 (the maximum number of terms that are involved in each relation set in the query inventory). We evaluate these different system design settings based on our i"
L18-1228,P14-5010,0,0.00251007,"ulary that we want to capture in our model consists of multi-word entities. Thus we should take this into account during the training of embeddings. 3.2. Corpora and Pre-processing In order to train domain-specific embeddings we need a domain-specific corpus. We therefore compile a corpus consisting of technical reports and scientific articles in the Oil and Gas domain. Table 1 shows detailed information about these sources. The corpus contains 47, 423 documents and 8, 280, 935 sentences. It is pre-processed using 3 the following steps: 1) Tokenization and lemmatization using StanfordCoreNLP (Manning et al., 2014). English stop words and sentences with less than three words are also removed from the corpus. 2) Shuffling: we randomly shuffle the text in the dataset. During the training of embedding models the learning rate is linearly dropped as training progresses, text appearing early has a larger effect on the model. Shuffling makes the effect of all text almost equivalent (Chiu et al., 2016a). 3.3. Training of Word Embeddings For training of the word embeddings, we exploit the available word2vec (Mikolov et al., 2013a) implementation genˇ uˇrek and Sojka, 2010). The elements that have sim (Reh˚ an i"
L18-1228,N13-1090,0,0.351941,"tional neural network. We further adapt embedding enhancement methods to provide vector representations for infrequent and unseen terms. Experiments show that the adapted technique can provide improvements both in intrinsic and extrinsic evaluation. Keywords: word embeddings, intrinsic and extrinsic evaluation, domain knowledge resource, embeddings enhancement 1. Introduction 2. Domain-specific, technical vocabulary presents a challenge to NLP applications. Recently, word embedding models have been shown to capture a range of semantic relations relevant to the interpretation of lexical items (Mikolov et al., 2013b) and furthermore provide useful input representations for a range of downstream tasks (Collobert et al., 2011). The majority of work dealing with intrinsic evaluation of word embeddings has focused on general domain embeddings and semantic relations between frequent and generic terms. However, it has been shown that embeddings differ from one domain to another due to lexical and semantic variation (Hamilton et al., 2016; Bollegala et al., 2015). Domain-specific terms are challenging for general domain embeddings since there are few statistical clues in the underlying corpora for these items"
L18-1228,W16-2902,0,0.0808775,"e useful input representations for a range of downstream tasks (Collobert et al., 2011). The majority of work dealing with intrinsic evaluation of word embeddings has focused on general domain embeddings and semantic relations between frequent and generic terms. However, it has been shown that embeddings differ from one domain to another due to lexical and semantic variation (Hamilton et al., 2016; Bollegala et al., 2015). Domain-specific terms are challenging for general domain embeddings since there are few statistical clues in the underlying corpora for these items (Bollegala et al., 2015; Pilehvar and Collier, 2016). The Oil and Gas domain is a highly technical and dataintensive domain. Experts working within this domain daily investigate selected geographical areas and use relevant information (scientific articles, reports and other textual sources) to evaluate the potential for undiscovered hydrocarbons. The vocabulary is technical and there is a real need for NLP tools to aid the work process. In this work we investigate whether word embedding models can capture domain-specific semantic relations by training domainspecific embeddings1 and evaluating these against a terminological resource. We conduct"
L18-1228,D15-1036,0,0.0464892,", there is no agreement in the community on the best ways to evaluate these semantic representations of language2 . There exist a variety of benchmarks which are widely employed to assess the quality of word representations and to compare different distributional semantic models. Existing evaluation methods can largely be separated into two categories: ”intrinsic evaluation” and ”extrinsic evaluation”. Intrinsic evaluation tries to directly quantify how well various kinds of linguistic regularities can be detected with the model independent of its downstream applications (Baroni et al., 2014; Schnabel et al., 2015). On the other hand, the quality of a word vector may be assessed by its performance in downstream tasks through measuring changes in performance metrics specific to the tasks by extrinsic evaluation. The downstream language technology tasks on which the quality of a word embedding is examined, fall into syntactic (e.g. POS tagging, Chunking) and semantic (e.g. Named entity recognition, Sentiment Classification) categories (Schnabel et al., 2015; Chiu et al., 2016b). In this work we evaluate domain-specific word embedding models using both intrinsic and extrinsic evaluation schemes. Although,"
L18-1228,P06-2111,0,0.100187,"Missing"
L18-1228,P14-2089,0,0.0312415,"nking) and semantic (e.g. Named entity recognition, Sentiment Classification) categories (Schnabel et al., 2015; Chiu et al., 2016b). In this work we evaluate domain-specific word embedding models using both intrinsic and extrinsic evaluation schemes. Although, word embeddings techniques have drawn significant interest in the field, they are not well equipped to deal with unseen and infrequent words, nor do they consider word relations found in knowledge resources. Recently, different solutions have been proposed to overcome these limitations (Pilehvar and Collier, 2016; Faruqui et al., 2015; Yu and Dredze, 2014). Among these, we choose Faruqui et al. (2015) in this work since it is a post-processing approach which is straightforward to apply. 3. Intrinsic evaluation setup Intrinsic evaluation of word embeddings has two requirements. First, we require a query inventory as a gold standard, and second, a word embedding model that has been 2 RepEval @ACL 2016: The First Workshop on Evaluating Vector Space Representations for NLP 1438 Source Abbr. Description American Association of Petroleum Geologist C&C Reservoirs-Digital Analogs Elsevier Geological Society, London Memoirs Norwegian Petroleum Directory"
L18-1661,P13-2088,0,0.0761269,"Missing"
L18-1661,P07-1056,0,0.386504,"Missing"
L18-1661,P12-3005,0,0.0870595,"Missing"
L18-1661,P11-1015,0,0.0529325,"is of the reviews generalize to non-review texts, and we plan to annotate aspect-based sentiment for a selection of general-domain news texts as well. Related work The dataset described in the current paper is the first of its kind for Norwegian. For other languages, however, the field has seen a substantial amount of SA research based on rated reviews, either user-generated or by professional reviewers. This has often been based on single-domain datasets, and examples include (for English unless otherwise noted) movie reviews collected from aggregator sites like IMDb.com (Pang and Lee, 2004; Maas et al., 2011) and RottenTomatoes.com (Pang and Lee, 2005; Socher et al., 2013), hotel reviews from TripAdvisor (Wang et al., 2010), book reviews (in Arabic) (Aly and Atiya, 2013), app reviews compiled from Apple App store and Google Play (Guzman and Maalej, 2014), and reviews of restaurants and other businesses in the Yelp open dataset.5 However, the unbalanced nature of these datasets (single domains) can impose inherent limitations on the ability of https://www.yelp.com/dataset Future work 6. Summary and outlook The current paper has described the creation of the Norwegian Review Corpus; NoReC (Ver. 1.0."
L18-1661,L16-1262,0,0.0525466,"Missing"
L18-1661,L16-1250,1,0.85854,"Missing"
L18-1661,P04-1035,0,0.0256263,"developed on the basis of the reviews generalize to non-review texts, and we plan to annotate aspect-based sentiment for a selection of general-domain news texts as well. Related work The dataset described in the current paper is the first of its kind for Norwegian. For other languages, however, the field has seen a substantial amount of SA research based on rated reviews, either user-generated or by professional reviewers. This has often been based on single-domain datasets, and examples include (for English unless otherwise noted) movie reviews collected from aggregator sites like IMDb.com (Pang and Lee, 2004; Maas et al., 2011) and RottenTomatoes.com (Pang and Lee, 2005; Socher et al., 2013), hotel reviews from TripAdvisor (Wang et al., 2010), book reviews (in Arabic) (Aly and Atiya, 2013), app reviews compiled from Apple App store and Google Play (Guzman and Maalej, 2014), and reviews of restaurants and other businesses in the Yelp open dataset.5 However, the unbalanced nature of these datasets (single domains) can impose inherent limitations on the ability of https://www.yelp.com/dataset Future work 6. Summary and outlook The current paper has described the creation of the Norwegian Review Corp"
L18-1661,P05-1015,0,0.197626,"texts, and we plan to annotate aspect-based sentiment for a selection of general-domain news texts as well. Related work The dataset described in the current paper is the first of its kind for Norwegian. For other languages, however, the field has seen a substantial amount of SA research based on rated reviews, either user-generated or by professional reviewers. This has often been based on single-domain datasets, and examples include (for English unless otherwise noted) movie reviews collected from aggregator sites like IMDb.com (Pang and Lee, 2004; Maas et al., 2011) and RottenTomatoes.com (Pang and Lee, 2005; Socher et al., 2013), hotel reviews from TripAdvisor (Wang et al., 2010), book reviews (in Arabic) (Aly and Atiya, 2013), app reviews compiled from Apple App store and Google Play (Guzman and Maalej, 2014), and reviews of restaurants and other businesses in the Yelp open dataset.5 However, the unbalanced nature of these datasets (single domains) can impose inherent limitations on the ability of https://www.yelp.com/dataset Future work 6. Summary and outlook The current paper has described the creation of the Norwegian Review Corpus; NoReC (Ver. 1.0.1). The final dataset comprises more than 3"
L18-1661,D13-1170,0,0.00481102,"o annotate aspect-based sentiment for a selection of general-domain news texts as well. Related work The dataset described in the current paper is the first of its kind for Norwegian. For other languages, however, the field has seen a substantial amount of SA research based on rated reviews, either user-generated or by professional reviewers. This has often been based on single-domain datasets, and examples include (for English unless otherwise noted) movie reviews collected from aggregator sites like IMDb.com (Pang and Lee, 2004; Maas et al., 2011) and RottenTomatoes.com (Pang and Lee, 2005; Socher et al., 2013), hotel reviews from TripAdvisor (Wang et al., 2010), book reviews (in Arabic) (Aly and Atiya, 2013), app reviews compiled from Apple App store and Google Play (Guzman and Maalej, 2014), and reviews of restaurants and other businesses in the Yelp open dataset.5 However, the unbalanced nature of these datasets (single domains) can impose inherent limitations on the ability of https://www.yelp.com/dataset Future work 6. Summary and outlook The current paper has described the creation of the Norwegian Review Corpus; NoReC (Ver. 1.0.1). The final dataset comprises more than 35,000 full-text review"
L18-1661,K17-3009,0,0.0209689,"Missing"
L18-1661,L16-1680,0,0.0200464,"Missing"
L18-1661,W17-0201,1,0.878115,"Missing"
L18-1661,F13-2033,0,0.0202319,"ically also reported for user-generated reviews, though with a stronger preference for the highest score (Baccianella et al., 2009). In Figures 3 and 4 we see a more detailed view of the rating distribution for each category and source. In Figure 3 we see that the ‘stage’ and ‘products’ categories are most strongly skewed towards the rating of 5. As most of the product reviews were gathered from ‘DinSide.no’, we see a similar distribution for this source in Figure 4. 4189 models to generalize. Some datasets combine reviews from multiple domains for better balance, like the French SA corpus of Vincent and Winterstein (2013), combining reviews of movies, books and hotels (from Allocine.fr, Amazon.fr, and TripAdvisor.fr, respectively), or the Arabic SA corpus of ElSahar and El-Beltagy (2015), combining reviews of hotels, restaurants, movies, restaurants and product reviews (from TripAdvisor, elCinema.com, Qaym.com and Souq.com). There also exists several datasets based on product reviews from Amazon, which can potentially also have the advantage of covering a more diverse selection of domains. An example includes the Amazon dataset of Blitzer et al. (2007), comprising reviews of books, DVDs, electronics, and kitch"
L18-1710,de-marneffe-etal-2014-universal,0,0.0370897,"Missing"
L18-1710,L16-1248,0,0.539773,"able 2 presents the number of segments and tokens, and their distributions across the three different dialects. 4. Spoken language PoS In order to account for spoken language, some additional PoS tags are added to the tagset. Incomplete or interrupted words are tagged with the tag ufullst, pauses (’#’) with the tag pause and filled pauses or hesitations with nol. The category of interjections, we found, is quite frequent in our material. Therefore, a list of standardized interjections 5 The extended annotation guidelines for the syntactic annotation of the LIA material is built on the work of Dobrovoljc and Nivre (2016), who describe the annotation of the Slovenian spoken language treebank with Universal Dependencies. Below we describe our treatment of extra-linguistic tokens, various types of disfluencies, ellipsis and discourse elements. 4.2.1. Extra-linguistic tokens During transcription, some extra-linguistic tokens are introduced in order to mark phenomena such as pauses or unfinished/incomplete words. The examples in (1)-(3) illustrate phenomena that introduce extra-linguistic tokens and will be discussed further below. (1) ja # og køyrde mjølka ut i byen igjen yes # and drove milk out in town again ‘Y"
L18-1710,P13-2017,0,0.100104,"Missing"
L18-1710,nivre-etal-2006-talbanken05,0,0.0743009,"Missing"
L18-1710,L16-1262,0,0.0562973,"Missing"
L18-1710,W17-0232,1,0.743598,"deletions as we have called them, we introduce a subtype of the universal parataxis relation called deletion. Figure 2 shows the converted UD version of the sentence from Figure 1. We observe that the structure differs markedly from the structure in the NDT format. The NDT version in Figure 1 annotates the finite verb var ‘was’ as the root of the segment, whereas the UD version appoints the predicative argument trafikk ‘traffic’ as root with the verb as a dependent with the cop (copula) relation type. 6. Availability of the treebank The treebank will be made available for searching in Glossa (Nøklestad et al., 2017), which is a web-based corpus search interface being developed at the Text Laboratory, University of Oslo. This interface, which currently only supports searching in morphosyntactic information, will be extended with capabilities for searching in syntactic dependency structures as well. For syntactic search we aim to implement an examplebased approach along the lines of the GrETEL system6 , where the user can input an example of the kind of construction they are interested in, have the system analyse the example, select the relevant parts of the analysis (e.g. particular syntactic or morphosyn"
L18-1710,W17-0243,0,0.0221851,"nation of various kinds of linguistic information. For instance, in terms of PoS tags, the UD scheme offers a dedicated tag for proper nouns (PROPN), whereas NDT expresses information about noun type among its morphological features. UD further distinguishes auxiliary verbs (AUX) from main verbs (VERB). This distinction is not explicitly made in NDT, hence the conversion procedure makes use of the syntactic context of a verb; verbs that have a non-finite dependent are marked as auxiliaries. Further details about the conversion is given in Øvrelid and Hohle (2016), as well as in Velldal et al. (2017), which describes the extension of the conversion to cover the Nynorsk variant of Norwegian. When it comes to part-of-speech tags, the universal tagset must be employed and there are few possibilities for language-specific adaptation. For dependency relations, there is the possibility to add treebank-specific subtypes of the universal dependency relations (on the form udep:subtype). Table 3 shows the treatment of the spoken language specific PoS tags and dependency relations during conversion to UD. Hesitations, as in example (2), and incomplete words, as in (3), are assigned the PoS tag X whi"
L18-1710,L16-1250,1,0.841243,"pings of tags and labels that make reference to a combination of various kinds of linguistic information. For instance, in terms of PoS tags, the UD scheme offers a dedicated tag for proper nouns (PROPN), whereas NDT expresses information about noun type among its morphological features. UD further distinguishes auxiliary verbs (AUX) from main verbs (VERB). This distinction is not explicitly made in NDT, hence the conversion procedure makes use of the syntactic context of a verb; verbs that have a non-finite dependent are marked as auxiliaries. Further details about the conversion is given in Øvrelid and Hohle (2016), as well as in Velldal et al. (2017), which describes the extension of the conversion to cover the Nynorsk variant of Norwegian. When it comes to part-of-speech tags, the universal tagset must be employed and there are few possibilities for language-specific adaptation. For dependency relations, there is the possibility to add treebank-specific subtypes of the universal dependency relations (on the form udep:subtype). Table 3 shows the treatment of the spoken language specific PoS tags and dependency relations during conversion to UD. Hesitations, as in example (2), and incomplete words, as i"
L18-1710,petrov-etal-2012-universal,0,0.0288671,"e 1, where the discourse filler e ‘mm’ is attached to the following finite verb var ‘was’ with the FYLL relation. 4485 reparandum discourse:filler nsubj cop discourse det nsubj a˚ oh INTJ det it PRON amod var was VERB e mm X det it PRON var was AUX noko something DET forferdeleg terrible ADJ trafikk traffic NOUN Figure 2: Example sentence from the UD conversion of the LIA treebank with corresponding English gloss, PoS and dependency analysis. 5. Conversion to Universal Dependencies Universal Dependencies builds on several previous initiatives for universally common morphological (Zeman, 2008; Petrov et al., 2012) and syntactic dependency (McDonald et al., 2013; Rosa et al., 2014) annotation. Among its main tenets is the primacy of content words, i.e., content words, as opposed to function words, are syntactic heads wherever possible. It is intended to be a universal annotation scheme, i.e., applicable to any language, however it also offers some possibilities for language-specific information. With reference to the NDT annotation choices in Table 1, the UD scheme adopts the reverse attachment for auxiliaries, infinitival markers and prepositions. The NDT and UD schemes differ in terms of both PoS tags"
L18-1710,rosa-etal-2014-hamledt,0,0.0387125,"Missing"
L18-1710,P14-1088,0,0.0491989,"Missing"
L18-1710,solberg-etal-2014-norwegian,1,0.844239,"l part-of-speech pron but function syntactically as a determiner (DET). Table 2: Raw counts for the different informants in the data set. 3.3. 4.2. Annotation process Prior to the manual morphosyntactic annotation, the LIA data set was automatically tagged with OBT+stat, a rulebased Constraint Grammar tagger with a HMM-based overlay (Johannessen et al., 2012) and parsed with the MATE parser (Bohnet, 2010) trained on the Nynorsk part of NDT, which consists largely of newspaper text. This parser has been reported to achieve a labeled accuracy score (LAS) of 89.54 on the Nynorsk test set of NDT (Solberg et al., 2014). The automatic tag assignments are then corrected by trained linguists using a browser-based application5 . The dependency analyses are also manually corrected, following the extended guidelines described in section 4. below. Dependency annotation was performed using the TrEd application, which is the annotation tool developed for the annotation of the Prague Dependency Treebank (B¨ohmov´a et al., 2003). 3.4. Treebank data Our data set, at present, consists of elderly speakers (80+) of the Eidsberg, Austevoll and Vardø dialects. This represents a diverse set of dialects from different regions"
L18-1710,W17-0201,1,0.857638,"ence to a combination of various kinds of linguistic information. For instance, in terms of PoS tags, the UD scheme offers a dedicated tag for proper nouns (PROPN), whereas NDT expresses information about noun type among its morphological features. UD further distinguishes auxiliary verbs (AUX) from main verbs (VERB). This distinction is not explicitly made in NDT, hence the conversion procedure makes use of the syntactic context of a verb; verbs that have a non-finite dependent are marked as auxiliaries. Further details about the conversion is given in Øvrelid and Hohle (2016), as well as in Velldal et al. (2017), which describes the extension of the conversion to cover the Nynorsk variant of Norwegian. When it comes to part-of-speech tags, the universal tagset must be employed and there are few possibilities for language-specific adaptation. For dependency relations, there is the possibility to add treebank-specific subtypes of the universal dependency relations (on the form udep:subtype). Table 3 shows the treatment of the spoken language specific PoS tags and dependency relations during conversion to UD. Hesitations, as in example (2), and incomplete words, as in (3), are assigned the PoS tag X whi"
L18-1710,K17-3001,0,0.0274497,"Missing"
L18-1710,zeman-2008-reusable,0,0.0388518,"raph in Figure 1, where the discourse filler e ‘mm’ is attached to the following finite verb var ‘was’ with the FYLL relation. 4485 reparandum discourse:filler nsubj cop discourse det nsubj a˚ oh INTJ det it PRON amod var was VERB e mm X det it PRON var was AUX noko something DET forferdeleg terrible ADJ trafikk traffic NOUN Figure 2: Example sentence from the UD conversion of the LIA treebank with corresponding English gloss, PoS and dependency analysis. 5. Conversion to Universal Dependencies Universal Dependencies builds on several previous initiatives for universally common morphological (Zeman, 2008; Petrov et al., 2012) and syntactic dependency (McDonald et al., 2013; Rosa et al., 2014) annotation. Among its main tenets is the primacy of content words, i.e., content words, as opposed to function words, are syntactic heads wherever possible. It is intended to be a universal annotation scheme, i.e., applicable to any language, however it also offers some possibilities for language-specific information. With reference to the NDT annotation choices in Table 1, the UD scheme adopts the reverse attachment for auxiliaries, infinitival markers and prepositions. The NDT and UD schemes differ in"
P09-2010,W06-2920,0,0.130123,"ezler et al., 2002). 1 Introduction The divide between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent years witnessed a surge of interest in dependency-based approaches to syntactic parsing, spurred by the CoNLL shared tasks of dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). Nivre and McDonald (2008) show how two different approaches to dependency parsing, the graphbased and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parse results for a range of different languages. In this paper, we show how a data-driven dependency parser may straightforwardly be modified to learn directly from a grammar-driven parser. We evaluate on English and German and show significant improvements for both languages. Like Nivre 3 Dependency conversion and feature extraction In extracting information"
P09-2010,W02-1503,0,0.0983016,"nglish and German and show significant improvements stemming from the proposed dependency structure as well as various other, deep linguistic features derived from the respective grammars. 2 Grammar-driven LFG-parsing The XLE system (Crouch et al., 2007) performs unification-based parsing using hand-crafted LFG grammars. It processes raw text and assigns to it both a phrase-structural (‘c-structure’) and a feature structural, functional (‘f-structure’). In the work described in this paper, we employ the XLE platform using the grammars available for English and German from the ParGram project (Butt et al., 2002). In order to increase the coverage of the grammars, we employ the robustness techniques of fragment parsing and ‘skimming’ available in XLE (Riezler et al., 2002). 1 Introduction The divide between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent ye"
P09-2010,J08-1003,0,0.0471193,"Missing"
P09-2010,W08-1705,0,0.0186163,"ructural, functional (‘f-structure’). In the work described in this paper, we employ the XLE platform using the grammars available for English and German from the ParGram project (Butt et al., 2002). In order to increase the coverage of the grammars, we employ the robustness techniques of fragment parsing and ‘skimming’ available in XLE (Riezler et al., 2002). 1 Introduction The divide between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent years witnessed a surge of interest in dependency-based approaches to syntactic parsing, spurred by the CoNLL shared tasks of dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). Nivre and McDonald (2008) show how two different approaches to dependency parsing, the graphbased and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parse results f"
P09-2010,J93-2004,0,0.033081,"htforwardly be modified to learn directly from a grammar-driven parser. We evaluate on English and German and show significant improvements for both languages. Like Nivre 3 Dependency conversion and feature extraction In extracting information from the output of the deep grammars we wish to capture as much of the precise, linguistic generalizations embodied in the grammars as possible, whilst keeping with the requirements posed by the dependency parser. The process is illustrated in Figure 1. 3.1 Data The English data set consists of the Wall Street Journal sections 2-24 of the Penn treebank (Marcus et al., 1993), converted to dependency format. The treebank data used for German is the Tiger 37 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 37–40, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP PRED V TYPE S UBJ ‘halteh. . .i’ predicative “pro” P RED ‘Verhalten’ C ASE acc S PEC f3“das”            OBJ     A DJUNCT f4“damalige”   f2      P RED ‘f¨urh. . .i’   i  XCOMP-PRED PTYPE hnosem P RED ‘richtig’ O BJ S UBJ converted: X COMP -P RED S UBJ - OBJ S PEC S UBJ A DJCT O BJ Ich halte das damalige Verhalten f¨ur richtig. 1sg pred. acc SB nosem NK N"
P09-2010,P08-1108,0,0.553644,"between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent years witnessed a surge of interest in dependency-based approaches to syntactic parsing, spurred by the CoNLL shared tasks of dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). Nivre and McDonald (2008) show how two different approaches to dependency parsing, the graphbased and transition-based approaches, may be combined and subsequently learn to complement each other to achieve improved parse results for a range of different languages. In this paper, we show how a data-driven dependency parser may straightforwardly be modified to learn directly from a grammar-driven parser. We evaluate on English and German and show significant improvements for both languages. Like Nivre 3 Dependency conversion and feature extraction In extracting information from the output of the deep grammars we wish to"
P09-2010,nivre-etal-2006-maltparser,0,0.310635,"DJUNCT f4“damalige”   f2      P RED ‘f¨urh. . .i’   i  XCOMP-PRED PTYPE hnosem P RED ‘richtig’ O BJ S UBJ converted: X COMP -P RED S UBJ - OBJ S PEC S UBJ A DJCT O BJ Ich halte das damalige Verhalten f¨ur richtig. 1sg pred. acc SB nosem NK NK OA NK MO gold: f1 Figure 1: Treebank enrichment with LFG output; German example: I consider the past behaviour correct. treebank (Brants et al., 2004), where we employ the version released with the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006). use during parsing from the German and English XLE-parses. 3.2 MaltParser (Nivre et al., 2006a) is a languageindependent system for data-driven dependency parsing which is freely available.1 MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. MaltParser constructs parsing as a set of transitions between parse configurations. A parse configuration is a triple hS, I, Gi, where S represents the parse stack, I is the queue of remaining input tokens, and G represents the dependency graph defined thus far. The feature model in MaltParser defines the relevant attributes of tokens in a parse configuration."
P09-2010,W06-2933,0,0.051148,"Missing"
P09-2010,P02-1035,0,0.0355219,"rom the respective grammars. 2 Grammar-driven LFG-parsing The XLE system (Crouch et al., 2007) performs unification-based parsing using hand-crafted LFG grammars. It processes raw text and assigns to it both a phrase-structural (‘c-structure’) and a feature structural, functional (‘f-structure’). In the work described in this paper, we employ the XLE platform using the grammars available for English and German from the ParGram project (Butt et al., 2002). In order to increase the coverage of the grammars, we employ the robustness techniques of fragment parsing and ‘skimming’ available in XLE (Riezler et al., 2002). 1 Introduction The divide between grammar-driven and datadriven approaches to parsing has become less pronounced in recent years due to extensive work on robustness and efficiency for the grammar-driven approaches (Riezler et al., 2002; Cahill et al., 2008b). The linguistic generalizations captured in such knowledge-based resources are thus increasingly available for use in practical applications. The NLP-community has in recent years witnessed a surge of interest in dependency-based approaches to syntactic parsing, spurred by the CoNLL shared tasks of dependency parsing (Buchholz and Marsi,"
P09-2010,D07-1096,0,\N,Missing
P13-3005,H05-1066,0,0.326013,"Missing"
P13-3005,D12-1133,0,0.110153,"ntly perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats Malt (Nivre et al., 2007): transition-based dependency parser with local learning and greedy search. MST (McDonald et al., 2005): graph-based dependency parser with global near-exhaustive search. Bohnet and Nivre (2012) parser: transitionbased dependency parser with joint tagger that implements global learning and beam search. (b) the noun phrase is headed by the noun rather than by determiner; (c) prepositions or subordinating conjunctions, rather than their NP or clause arguments, serve as the head in prepositional phrase or subordinated clauses. Therefore we can expect (a) Malt and MST to have fewer errors on coordination structures parsing SB and CD than parsing DT, because SB and CD choose the first conjunct as the head and DT chooses the coordinating conjunction as the head; (b,c) no significant differ"
P13-3005,W06-2932,0,0.0192522,"s (Table 1, Gold PTB tags). The Bohnet and Nivre (2012) parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2, Predicted PTB tags). This is explained by the fact that the Bohnet and Nivre (2012) parser implements a novel approach to parsing: beam-search algorithm with global structure learning. MST “loses” more than Malt when parsing SB with gold supertags (Table 1, Gold supertags). This parser exploits context features “POS tag of each intervening word between head and dependent” (McDonald et al., 2006). Due to the far larger size of the supertag set compared to the PTB tagset, such features are sparse and have low frequencies. This leads to the lower scores of parsing accuracy for MST. For the Bohnet and Nivre (2012) parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies (Table 2, Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the Bohnet and Nivre (2012) parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2,"
P13-3005,de-marneffe-etal-2006-generating,0,0.0690973,"Missing"
P13-3005,P05-1067,0,0.0149815,"comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser. 1 Introduction Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: t"
P13-3005,N10-1115,0,0.0202642,"ford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST 2 Related work Schwartz et al. (2012) investigate which dependency representations of several syntactic structures are easier to parse with supervised versions of the Klein and Manning (2004) parser, ClearParser (Choi and Nicolov, 2009), MST Parser, Malt and the Easy First Non-directional parser (Goldberg and Elhadad, 2010). The results imply that all parsers consistently perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats Malt (Nivre et al., 2007): transition-based dependency parser with local learning and greedy search. MST (McDonald et al., 2005): graph-based dependenc"
P13-3005,C12-1147,0,0.230962,"igurations in order to determine how parsers, dependency representations and grammatical tagging methods interact with each other in application to automatic syntactic analysis. SB and CD are derived automatically from phrase structures of Penn Treebank to accommodate the needs of fast and accurate dependency parsing, whereas DT is rooted in the formal grammar theory HPSG and is independent from any specific treebank. For DT we gain more expressivity from the underlying linguistic theory, which challenges parsing with statistical tools. The structural analysis of the schemes in Ivanova et al. (2012) leads to the hypothesis that CD and DT are more similar to each other than SB to DT. We recompute similarities on a larger treebank and check whether parsing results reflect them. The paper has the following structure: an overview of related work is presented in Section 2; treebanks, tagsets, dependency schemes and parsers used in the experiments are introduced in Section 3; analysis of parsing results is discussed in Section 4; conclusions and future work are outlined in Section 5. In this paper we focus on practical issues of data representation for dependency parsing. We carry out an exper"
P13-3005,W12-3602,1,0.940897,"ies in all configurations in order to determine how parsers, dependency representations and grammatical tagging methods interact with each other in application to automatic syntactic analysis. SB and CD are derived automatically from phrase structures of Penn Treebank to accommodate the needs of fast and accurate dependency parsing, whereas DT is rooted in the formal grammar theory HPSG and is independent from any specific treebank. For DT we gain more expressivity from the underlying linguistic theory, which challenges parsing with statistical tools. The structural analysis of the schemes in Ivanova et al. (2012) leads to the hypothesis that CD and DT are more similar to each other than SB to DT. We recompute similarities on a larger treebank and check whether parsing results reflect them. The paper has the following structure: an overview of related work is presented in Section 2; treebanks, tagsets, dependency schemes and parsers used in the experiments are introduced in Section 3; analysis of parsing results is discussed in Section 4; conclusions and future work are outlined in Section 5. In this paper we focus on practical issues of data representation for dependency parsing. We carry out an exper"
P13-3005,P12-2020,0,0.0159754,"tream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST 2 Related work Schwartz et al. (2012) investigate which dependency r"
P13-3005,W07-2416,0,0.0449829,"ats (Stanford Dependencies, CoNLL-X, and Enju PAS). Intristic evaluation results show that all parsers have the highest accuracies with the CoNLL-X format. 3 3.1 3.3 In this work we extract DeepBank data in the form of bilexical syntactic dependencies, DELPH-IN Syntactic Derivation Tree (DT) format. We obtain the exact same sentences in Stanford Basic (SB) format from the automatic conversion of the PTB with the Stanford parser (de Marneffe et al., 2006) and in the CoNLL Syntactic Dependencies (CD) representation using the LTH Constituentto-Dependency Conversion Tool for Penn-style Treebanks (Johansson and Nugues, 2007). SB and CD represent the way to convert PTB to bilexical dependencies; in contrast, DT is grounded in linguistic theory and captures decisions taken in the grammar. Figure 1 demonstrates the differences between the formats on the coordination structure. According to Schwartz et al. (2012), analysis of coordination in SB and CD is easier for a statistical parser to learn; however, as we will see in section 4.3, DT has more expressive power distinguishing structural ambiguities illustrated by the classic example old men and women. Data and software Treebanks For the experiments in this paper we"
P13-3005,W06-1634,0,0.0112175,"emes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser. 1 Introduction Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsi"
P13-3005,P04-1061,0,0.0534082,"tral aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST 2 Related work Schwartz et al. (2012) investigate which dependency representations of several syntactic structures are easier to parse with supervised versions of the Klein and Manning (2004) parser, ClearParser (Choi and Nicolov, 2009), MST Parser, Malt and the Easy First Non-directional parser (Goldberg and Elhadad, 2010). The results imply that all parsers consistently perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats Malt (Nivre et al"
P13-3005,W11-2922,0,0.0383567,"Missing"
P13-3005,J93-2004,0,0.0444152,"Missing"
P13-3005,C10-1088,0,\N,Missing
P13-3005,W07-2218,0,\N,Missing
read-etal-2012-wesearch,flickinger-etal-2010-wikiwoods,1,\N,Missing
read-etal-2012-wesearch,bird-etal-2008-acl,0,\N,Missing
read-etal-2012-wesearch,P00-1061,0,\N,Missing
read-etal-2012-wesearch,D08-1050,0,\N,Missing
read-etal-2012-wesearch,W00-0901,0,\N,Missing
read-etal-2012-wesearch,I11-1100,0,\N,Missing
read-etal-2012-wesearch,W07-2202,0,\N,Missing
read-etal-2012-wesearch,W11-2925,0,\N,Missing
read-etal-2012-wesearch,P11-4002,0,\N,Missing
read-etal-2012-wesearch,W10-0508,0,\N,Missing
S12-1041,P05-1022,0,0.0956249,"be due to annotation errors (insuperable, unhappily, endless, listlessly). Among the FNs, two are due to MWCs not covered by our heuristics (e.g., no more), with the remainder concerning affixes. 3 Constituent-Based Scope Resolution During the development of our scope resolution system we have pursued both a rule-based and datadriven approach. Both are rooted in the assumption that the scope of negations corresponds to a syntactically meaningful unit. Our starting point here will be the syntactic analyses provided by the task organizers (see Figure 1), generated using the reranking parser of Charniak and Johnson (2005). However, as alignment between scope annotations and syntactic units is not straightforward for all cases, we apply several exception rules that ‘slacken’ the requirements for alignment, as discussed in Section 3.1. In Sections 3.2 and 3.3 we detail our rule-based and data-driven approaches, respectively. Note that the predictions of the rule-based component will be incorporated as features in the learned model, similarly to the set-up described by Read et al. (2011). Section 3.4 details the post-processing we apply to handle cases of discontinuous scope, beRB//VP/SBAR if SBARWH* RB//VP/S RB"
S12-1041,S12-1035,0,0.263085,"(UiO) to the 2012 *SEM Shared Task on resolving negation. Our submission is an adaption of the negation system of Velldal et al. (2012), which combines SVM cue classification with SVM-based ranking of syntactic constituents for scope resolution. The approach further extends our prior work in that we also identify factual negated events. While submitted for the closed track, the system was the top performer in the shared task overall. 1 (1) There was no answer. Introduction The First Joint Conference on Lexical and Computational Semantics (*SEM 2012) hosts a shared task on resolving negation (Morante and Blanco, 2012). This involves the subtasks of (i) identifying negation cues, (ii) identifying the in-sentence scope of these cues, and (iii) identifying negated (and factual) events. This paper describes a system submitted by the Language Technology Group at the University of Oslo (UiO). Our starting point is the negation system developed by Velldal et al. (2012) for the domain of biomedical texts, an SVM-based system for classifying cues and ranking syntactic constituents to resolve cue scopes. However, we extend and adapt this system in several important respects, such as in terms of the underlying lingui"
S12-1041,morante-daelemans-2012-conandoyle,0,0.137264,"nt is the negation system developed by Velldal et al. (2012) for the domain of biomedical texts, an SVM-based system for classifying cues and ranking syntactic constituents to resolve cue scopes. However, we extend and adapt this system in several important respects, such as in terms of the underlying linguistic formalisms that are used, the textual domain, handling of morphological cues and discontinuous scopes, and in that the current system also identifies negated events. The data sets used for the shared task include the following, all based on negation-annotated Conan Doyle (CD) stories (Morante and Daelemans, 2012): a training set of 3644 sentences (hereafter We describe two different system configurations, both of which were submitted for the closed track (hence we can only make use of the data provided by the task organizers). The systems only differ with respect to how they were optimized. In the first configuration, (hereafter I), all components in the pipeline had their parameters tuned by 10-fold cross-validation across CDTD. The second configuration (II) is tuned against CDD using CDT for training. The rationale for this strategy is to guard against possible overfitting effects that could result"
S12-1041,J12-2005,1,0.589072,"Oslo, Department of Informatics {jread,erikve,liljao,oe}@ifi.uio.no Abstract referred to as CDT), a development set of 787 sentences (CDD), and a held-out evaluation set of 1089 sentences (CDE). We will refer to the combination of CDT and CDD as CDTD. An example of an annotated sentence is shown in (1) below, where the cue is marked in bold, the scope is underlined, and the event marked in italics. This paper describes the first of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation. Our submission is an adaption of the negation system of Velldal et al. (2012), which combines SVM cue classification with SVM-based ranking of syntactic constituents for scope resolution. The approach further extends our prior work in that we also identify factual negated events. While submitted for the closed track, the system was the top performer in the shared task overall. 1 (1) There was no answer. Introduction The First Joint Conference on Lexical and Computational Semantics (*SEM 2012) hosts a shared task on resolving negation (Morante and Blanco, 2012). This involves the subtasks of (i) identifying negation cues, (ii) identifying the in-sentence scope of these"
S12-1042,W10-3110,0,0.570231,"in an interesting context of double negation; not dissatisfied. 3 Scope and event resolution In this work, we model negation scope resolution as a special instance of the classical IOB (Inside, Outside, Begin) sequence labeling problem, where negation cues are labeled to be sequence starters and scopes and events as two different kinds of chunks. CRFs allow the computation of p(X|Y), where X is a sequence of labels and Y is a sequence of observations, and have already been shown to be efficient in similar, albeit less involved, tasks of negation scope resolution (Morante and Daelemans, 2009; Councill et al., 2010). We employ the CRF implementation in the Wapiti toolkit, using default settings (Lavergne et al., 2010). A number of features were used to create the models. In addition to the information provided for each token in the CD corpus (lemma, part of speech and constituent), we extracted both left and right token distance to the closest negation cue. Features were expanded to include forward and backward bigrams and trigrams on both token and PoS level, as well as lexicalized PoS unigrams and bigrams2 . Table 2 presents a complete list of features. The more intricate, dependency-based features are"
S12-1042,de-marneffe-etal-2006-generating,0,0.0130195,"Missing"
S12-1042,J02-3001,0,0.11176,"cted Distance (DD) and Bidirectional Distance (BD). DD is extracted by following the reversed, directed edges from token X to the cue. If there is no such path, the value of the feature is -1. BD uses the Dijkstra shortest path algorithm on an undirected representation of the graph. The latter feature proved to be more effective than the former when not used together; using them in conjunction seemed to confuse the model, thus the final model utilizes only BD. We furthermore use the Dependency Graph Path (DGP) as a feature. This feature was inspired by the Parse Tree Path feature presented in Gildea and Jurafsky (2002) in the context of Semantic Role Labeling. It represents the path traversed from each token to the cue, encoding both the dependency relations and the direction of the arc that is traversed: for instance, the relation between our and no in Figure 1 is described as  poss  dobj  nsubj  det. Like Councill et al. (2010), we also encode the PoS of the first and second order syntactic head of each token. For the token no in Figure 1, for instance, we record the PoS of one and escaped, respectively. 3.2 Model-internal representation The token-wise annotations in the CD corpus contain multiple lay"
S12-1042,P10-1052,0,0.061443,"Missing"
S12-1042,W09-1105,0,0.144283,"oncern affixes, including one in an interesting context of double negation; not dissatisfied. 3 Scope and event resolution In this work, we model negation scope resolution as a special instance of the classical IOB (Inside, Outside, Begin) sequence labeling problem, where negation cues are labeled to be sequence starters and scopes and events as two different kinds of chunks. CRFs allow the computation of p(X|Y), where X is a sequence of labels and Y is a sequence of observations, and have already been shown to be efficient in similar, albeit less involved, tasks of negation scope resolution (Morante and Daelemans, 2009; Councill et al., 2010). We employ the CRF implementation in the Wapiti toolkit, using default settings (Lavergne et al., 2010). A number of features were used to create the models. In addition to the information provided for each token in the CD corpus (lemma, part of speech and constituent), we extracted both left and right token distance to the closest negation cue. Features were expanded to include forward and backward bigrams and trigrams on both token and PoS level, as well as lexicalized PoS unigrams and bigrams2 . Table 2 presents a complete list of features. The more intricate, depen"
S12-1042,morante-daelemans-2012-conandoyle,0,0.125404,"dency Features Emanuele Lapponi Erik Velldal Lilja Øvrelid Jonathon Read University of Oslo, Department of Informatics {emanuel,erikve,liljao,jread}@ifi.uio.no Abstract test set of 1089 sentences from The Cardboard Box and The Red Circle (CDE). In these sets, the concept of negation scope extends on the one adopted in the BioScope corpus in several aspects: Negation cues are not part of the scope, morphological (affixal) cues are annotated and scopes can be discontinuous. Moreover, in-scope states or events are marked as negated if they are factual and presented as events that did not happen (Morante and Daelemans, 2012). Examples (1) and (2) below are examples of affixal negation and discontinuous scope respectively: The cues are bold, the tokens contained within their scopes are underlined and the negated event is italicized. This paper describes the second of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation. The system combines SVM cue classification with CRF sequence labeling of events and scopes. Models for scopes and events are created using lexical and syntactic features, together with a fine-grained set of labels that capture the scopal behavio"
S12-1042,nivre-etal-2006-maltparser,0,0.0373962,"ependency relation Cue-dependent features Token distance Directed dependency distance Bidirectional dependency distance Dependency path Lexicalized dependency path Table 2: List of features used to train the CRF models. tations result from a conversion of Penn Treebankstyle phrase structure trees, combining ‘classic’ head finding rules with rules that target specific linguistic constructions, such as passives or attributive adjectives. The so-called basic format provides a dependency graph which is a directed tree, see Figure 1 for an example. For the open track submission we used Maltparser (Nivre et al., 2006) with its pre-trained parse model for English.4 The parse model has been trained on a conversion of sections 2-21 of the Wall Street Journal section of the Penn Treebank to Stanford dependencies, augmented with data from Question Bank. The parser was applied to the negation data, using the word tokens and supplied parts-of-speech as input to the parser. The features extracted via the dependency graphs aim at modeling the syntactic relationship between each token and the closest negation cue. Token distance was therefore complemented with two variants of dependency distance from each token to t"
S12-1042,S12-1041,1,0.522151,"nd hence this is also not reported as a development result. Note also that the official evaluation actually includes two different variants of the metrics mentioned above; a set of primary measures with precision computed as P=TP/(TP+FP) and a set of B measures where precision is rather computed as P=TP/SYS, where SYS is the total number of predictions made by the system. The reason why SYS is not identical with TP+FP is that partial matches are 1 Note that the cue classifier applied in the current paper is the same as that used in the other shared task submission from the University of Oslo (Read et al., 2012), and the two system descriptions will therefore have much overlap on this particular point. For all other components the architectures of the two system are completely different, however. 320 only counted as FNs (and not FPs) in order to avoid double penalties. We do not report the B measures for development testing as they were introduced for the final evaluation and hence were not considered in our system optimization. We note though, that the relative-ranking of participating systems for the primary and B measures is identical, and that the correlation between the paired lists of scores is"
S12-1042,J12-2005,1,0.607497,"ults are described in Section 2. Section 3 presents the system for scope and event resolution and details different features, the model-internal representation used for sequence-labeling, as well as the post-processing component. Error analyses for the cue, scope and event components are provided in the respective sections. Section 4 and 5 provide developmental and held-out results, respectively. Finally, we provide conclusions and some reflections regarding future work in Section 6. 2 Cue detection Identification of negation cues is based on the lightweight classification scheme presented by Velldal et al. (2012). By treating the set of cue words as a closed class, Velldal et al. (2012) showed that one could greatly reduce the number of examples presented to the learner, and correspondingly the number of features, while at the same time improving performance. This means that the classifier only attempts to “disambiguate” known cue words while ignoring any words not observed as cues in the training data. The classifier applied in the current submission is extended to also handle affixal negation cues, such as the prefix cue in impatience, the infix in carelessness, and the suffix of colourless. The typ"
S12-1042,W08-0606,0,0.390107,"Missing"
S18-1128,P15-2047,0,0.0182294,"networks (CNNs) have been effectively applied to extract lexical and sentence level features for relation classification (Zhang and Wang, 2015; Lee et al., 2017; Nguyen and Grishman, 2015). However, these works consider whole sentences or the context between two target entities as input for the CNN. Such representations suffer from irrelevant sub-sequences or clauses when target entities occur far from each other or there are other target entities in the same sentence. To avoid negative effects from irrelevant chunks or clauses and capture the relation between two entities, Xu et al. (2015a); Liu et al. (2015) and Xu et al. (2015b) employ a CNN to learn more robust and effective relation representations from the shortest dependency path (sdp) between two entities. The sdp between two entities in the dependency graph captures a condensed representation of the information required to assert a relationship between two entities (Bunescu and Mooney, 2005). In this work, we continue this line of work and present a system based on a CNN architecture over shortest dependency paths combined with domain-specific word embeddings to extract and classify semantic relations in scientific papers. This article pre"
S18-1128,D12-1133,0,0.0687997,"rs which are not further away than 6 tokens. From these entity pairs we generate negative instances with the NONE class and extract the corresponding sdp. Second, to preserve the directionality in the asymmetric relations, we add the ¬ symbol to the instances with reverse directionality (e.g., USAGE(e1,e2,REVERSE) becomes ¬USAGE(e1,e2)). The final label set for sub-task 2 thus consists of 12 relations. Given an encoded sentence, we find the sdp connecting two target entities for each relation instance using a syntactic parser, see below. For syntactic parsing we employ the parser described in Bohnet and Nivre (2012), a transitionbased parser which performs joint PoS-tagging and parsing. We train the parser on the standard training sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The constituency-based treebank is converted to dependencies using two different conversion tools: (i) the pennconverter software1 (Johansson and Nugues, 2007), which produces the so-called CoNLL-style dependencies employed in the CoNLL08 shared task on dependency parsing (Surdeanu et al., 2008)2 , and (ii) the Stanford parser using the option to produce basic Stanford dependenci"
S18-1128,P14-5010,0,0.0139997,"automatic features and representations to handle complex interpretation tasks. These approaches have yielded impressive results for many different NLP tasks. The use of deep neural networks for relation classification has been investigated in several recent studies (Socher et al., 2012; Lin et al., 2 System description In this section, we describe the various components of our system. Text pre-processing. For each relation instance in the training data set, we assign a sentence that contains the participant entities. Sentence and token boundaries are detected using the Stanford CoreNLP tool (Manning et al., 2014). Since most of the entities are multi-word units, in order to obtain a precise dependency path between entities, we replace the entities with their codes. The example sentence in (1) below is thus transformed to 805 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 805–810 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics (2). For the sentence in (1) and the two entities statistical models and structured data we thus extract the path in (3) below. (1) Syntax-based statistical machine translation (MT) aims at apply"
S18-1128,H05-1091,0,0.538603,"evant sub-sequences or clauses when target entities occur far from each other or there are other target entities in the same sentence. To avoid negative effects from irrelevant chunks or clauses and capture the relation between two entities, Xu et al. (2015a); Liu et al. (2015) and Xu et al. (2015b) employ a CNN to learn more robust and effective relation representations from the shortest dependency path (sdp) between two entities. The sdp between two entities in the dependency graph captures a condensed representation of the information required to assert a relationship between two entities (Bunescu and Mooney, 2005). In this work, we continue this line of work and present a system based on a CNN architecture over shortest dependency paths combined with domain-specific word embeddings to extract and classify semantic relations in scientific papers. This article presents the SIRIUS-LTG-UiO system for the SemEval 2018 Task 7 on Semantic Relation Extraction and Classification in Scientific Papers. First we extract the shortest dependency path (sdp) between two entities, then we introduce a convolutional neural network (CNN) which takes the shortest dependency path embeddings as input and performs relation cl"
S18-1128,J93-2004,0,0.0614099,"ns, we add the ¬ symbol to the instances with reverse directionality (e.g., USAGE(e1,e2,REVERSE) becomes ¬USAGE(e1,e2)). The final label set for sub-task 2 thus consists of 12 relations. Given an encoded sentence, we find the sdp connecting two target entities for each relation instance using a syntactic parser, see below. For syntactic parsing we employ the parser described in Bohnet and Nivre (2012), a transitionbased parser which performs joint PoS-tagging and parsing. We train the parser on the standard training sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The constituency-based treebank is converted to dependencies using two different conversion tools: (i) the pennconverter software1 (Johansson and Nugues, 2007), which produces the so-called CoNLL-style dependencies employed in the CoNLL08 shared task on dependency parsing (Surdeanu et al., 2008)2 , and (ii) the Stanford parser using the option to produce basic Stanford dependencies (de Marneffe et al., 2014)3 . The parser achieves a labeled accuracy score of 91.23 when trained on the CoNLL08 representation and 91.31 for the Stanford basic model, when evaluated against the standard evaluation"
S18-1128,W17-0237,0,0.0151508,"output by the parser, we extract the shortest dependency path connecting two entities. The path records the direction of arc traversal using left and right arrows (i.e. ← and →) as well as the dependency relation of the traversed arcs and the predicates involved, following Xu et al. (2015a). The entity codes in the final sdp are replaced with the corresponding word tokens at the end of the pre-processing step. Word embeddings. In our system, two different sets of pre-trained word embeddings are used for initialization. One is the 300-d pre-trained embeddings provided by the NLPL repository 4 (Fares et al., 2017), trained on English Wikipedia data with word2vec (Mikolov et al., 2013), here dubbed wiki-w2v. In addition, we train a second set of domain-specific embeddings on the ACL Anthology corpus. We obtain the XML versions of 22,878 articles from ACL Anthology 5 . After extracting the raw texts, for training of the 300-d word embeddings (acl-w2v), we exploit the available word2vec (Mikolov et al., 2013) implementaˇ uˇrek and Sojka, 2010) for traintion gensim (Reh˚ ing. Classification Model Our system is based on a Convolutional Neural Network (CNN) architecture similar to the one used for sentence c"
S18-1128,de-marneffe-etal-2014-universal,0,0.063233,"Missing"
S18-1128,S18-1111,0,0.0410589,"Missing"
S18-1128,W15-1506,0,0.0553656,"Missing"
S18-1128,S10-1006,0,0.0552495,"Missing"
S18-1128,W07-2416,0,0.0499146,"2 thus consists of 12 relations. Given an encoded sentence, we find the sdp connecting two target entities for each relation instance using a syntactic parser, see below. For syntactic parsing we employ the parser described in Bohnet and Nivre (2012), a transitionbased parser which performs joint PoS-tagging and parsing. We train the parser on the standard training sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The constituency-based treebank is converted to dependencies using two different conversion tools: (i) the pennconverter software1 (Johansson and Nugues, 2007), which produces the so-called CoNLL-style dependencies employed in the CoNLL08 shared task on dependency parsing (Surdeanu et al., 2008)2 , and (ii) the Stanford parser using the option to produce basic Stanford dependencies (de Marneffe et al., 2014)3 . The parser achieves a labeled accuracy score of 91.23 when trained on the CoNLL08 representation and 91.31 for the Stanford basic model, when evaluated against the standard evaluation set (section 23) of the WSJ. We also experimented with the pre-trained parsing model for English included in the Stanford CoreNLP toolkit (Manning et al., 2014)"
S18-1128,S10-1057,0,0.0225095,"sk. 1 Introduction Relation extraction and classification can be defined as follows: given a sentence where entities are manually annotated, we aim to identify the pairs of entities that are instances of the semantic relations of interest and classify them based on a pre-defined set of relation types. A range of different approaches have been applied to solve this task in previous work. Conventional classification approaches have made use of contextual, lexical and syntactic features combined with richer linguistic and background knowledge such as WordNet and FrameNet (Hendrickx et al., 2010; Rink and Harabagiu, 2010). Recently, the re-emergence of deep neural networks provides a way to develop highly automatic features and representations to handle complex interpretation tasks. These approaches have yielded impressive results for many different NLP tasks. The use of deep neural networks for relation classification has been investigated in several recent studies (Socher et al., 2012; Lin et al., 2 System description In this section, we describe the various components of our system. Text pre-processing. For each relation instance in the training data set, we assign a sentence that contains the participant e"
S18-1128,D14-1181,0,0.0417972,"ish Wikipedia data with word2vec (Mikolov et al., 2013), here dubbed wiki-w2v. In addition, we train a second set of domain-specific embeddings on the ACL Anthology corpus. We obtain the XML versions of 22,878 articles from ACL Anthology 5 . After extracting the raw texts, for training of the 300-d word embeddings (acl-w2v), we exploit the available word2vec (Mikolov et al., 2013) implementaˇ uˇrek and Sojka, 2010) for traintion gensim (Reh˚ ing. Classification Model Our system is based on a Convolutional Neural Network (CNN) architecture similar to the one used for sentence classification in Kim (2014). Figure 1 provides an overview 1 http://nlp.cs.lth.se/software/ treebank-converter/ 2 The pennconverter tool is run using the rightBranching=false flag. 3 The Stanford parser is run using the -basic flag to produce the basic version of Stanford dependencies. 4 5 806 http://vectors.nlpl.eu/repository/ https://acl-arc.comp.nus.edu.sg/ Figure 1: Model architecture with two channels for an example shortest dependency path (CNN model from Kim (2014)). of the proposed model. It consists of 4 main layers as follows: Look-up Table and Embedding layer: In the first step, the model takes a dependency p"
S18-1128,S17-2171,0,0.0265745,"Missing"
S18-1128,P16-1200,0,0.0705263,"Missing"
S18-1128,D12-1110,0,0.0425025,"vious work. Conventional classification approaches have made use of contextual, lexical and syntactic features combined with richer linguistic and background knowledge such as WordNet and FrameNet (Hendrickx et al., 2010; Rink and Harabagiu, 2010). Recently, the re-emergence of deep neural networks provides a way to develop highly automatic features and representations to handle complex interpretation tasks. These approaches have yielded impressive results for many different NLP tasks. The use of deep neural networks for relation classification has been investigated in several recent studies (Socher et al., 2012; Lin et al., 2 System description In this section, we describe the various components of our system. Text pre-processing. For each relation instance in the training data set, we assign a sentence that contains the participant entities. Sentence and token boundaries are detected using the Stanford CoreNLP tool (Manning et al., 2014). Since most of the entities are multi-word units, in order to obtain a precise dependency path between entities, we replace the entities with their codes. The example sentence in (1) below is thus transformed to 805 Proceedings of the 12th International Workshop on"
S18-1128,D15-1062,0,0.283014,"Missing"
S18-1128,D15-1206,0,0.133604,"nvolutional neural networks (CNNs) have been effectively applied to extract lexical and sentence level features for relation classification (Zhang and Wang, 2015; Lee et al., 2017; Nguyen and Grishman, 2015). However, these works consider whole sentences or the context between two target entities as input for the CNN. Such representations suffer from irrelevant sub-sequences or clauses when target entities occur far from each other or there are other target entities in the same sentence. To avoid negative effects from irrelevant chunks or clauses and capture the relation between two entities, Xu et al. (2015a); Liu et al. (2015) and Xu et al. (2015b) employ a CNN to learn more robust and effective relation representations from the shortest dependency path (sdp) between two entities. The sdp between two entities in the dependency graph captures a condensed representation of the information required to assert a relationship between two entities (Bunescu and Mooney, 2005). In this work, we continue this line of work and present a system based on a CNN architecture over shortest dependency paths combined with domain-specific word embeddings to extract and classify semantic relations in scientific pap"
S18-1128,P16-2034,0,0.045025,"Missing"
S18-1128,W09-2415,0,\N,Missing
solberg-etal-2014-norwegian,nivre-etal-2006-talbanken05,0,\N,Missing
solberg-etal-2014-norwegian,D12-1133,0,\N,Missing
solberg-etal-2014-norwegian,brants-hansen-2002-developments,0,\N,Missing
solberg-etal-2014-norwegian,nivre-etal-2006-maltparser,0,\N,Missing
solberg-etal-2014-norwegian,J93-2004,0,\N,Missing
solberg-etal-2014-norwegian,brants-2000-inter,0,\N,Missing
solberg-etal-2014-norwegian,W08-1301,0,\N,Missing
solberg-etal-2014-norwegian,P05-1067,0,\N,Missing
solberg-etal-2014-norwegian,H05-1066,0,\N,Missing
solberg-etal-2014-norwegian,W09-1201,0,\N,Missing
solberg-etal-2014-norwegian,C12-1147,0,\N,Missing
solberg-etal-2014-norwegian,J09-3003,0,\N,Missing
solberg-etal-2014-norwegian,W07-2416,0,\N,Missing
solberg-etal-2014-norwegian,W13-2304,1,\N,Missing
solberg-etal-2014-norwegian,P14-1088,1,\N,Missing
solberg-etal-2014-norwegian,W13-5644,1,\N,Missing
solberg-etal-2014-norwegian,ballesteros-nivre-2012-maltoptimizer-system,0,\N,Missing
spreyer-etal-2010-training,N04-1013,0,\N,Missing
spreyer-etal-2010-training,J93-2004,0,\N,Missing
spreyer-etal-2010-training,W09-1104,1,\N,Missing
spreyer-etal-2010-training,H01-1035,0,\N,Missing
spreyer-etal-2010-training,W06-1614,0,\N,Missing
spreyer-etal-2010-training,W06-2920,0,\N,Missing
spreyer-etal-2010-training,P08-1108,0,\N,Missing
spreyer-etal-2010-training,P09-2010,1,\N,Missing
spreyer-etal-2010-training,J03-1002,0,\N,Missing
spreyer-etal-2010-training,I08-1064,1,\N,Missing
spreyer-etal-2010-training,W04-1905,0,\N,Missing
spreyer-etal-2010-training,P06-1146,0,\N,Missing
W08-2104,P03-1054,0,0.00522144,"received extensive attention in the parsing community and impressive results have been obtained for a range of languages (Nivre et al., 2007). Even with high overall parsing accuracy, however, datadriven parsers often make errors in the assignment of argument relations such as subject and object and the exact influence of data-derived features on the parsing accuracy for specific linguistic constructions is still relatively poorly understood. There are a number of studies that investigate the influence of different features or representational choices on overall parsing accuracy, (Bod, 1998; Klein and Manning, 2003). There are also attempts at a more fine-grained analysis of accuracy, targeting specific linguistic constructions or grammatical functions (Carroll and Briscoe, 2002; K¨ubler and Proki´c, 2006; McDonald and Nivre, 2007). 2 Parsing Swedish Before we turn to a description of the treebank and the parser used in the experiments, we want to point to a few grammatical properties of Swedish that will be important in the following: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights res"
W08-2104,D07-1013,0,0.0257368,"make errors in the assignment of argument relations such as subject and object and the exact influence of data-derived features on the parsing accuracy for specific linguistic constructions is still relatively poorly understood. There are a number of studies that investigate the influence of different features or representational choices on overall parsing accuracy, (Bod, 1998; Klein and Manning, 2003). There are also attempts at a more fine-grained analysis of accuracy, targeting specific linguistic constructions or grammatical functions (Carroll and Briscoe, 2002; K¨ubler and Proki´c, 2006; McDonald and Nivre, 2007). 2 Parsing Swedish Before we turn to a description of the treebank and the parser used in the experiments, we want to point to a few grammatical properties of Swedish that will be important in the following: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Verb second (V2) Swedish is, like the majority of Germanic languages a V2-language; the finite verb always resides in second position in 25 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural L"
W08-2104,nivre-etal-2006-talbanken05,0,0.233449,"ext I:next−1 I:next+1 I:next+2 G: head of top G: left dep of top G: right dep of top G: left dep of next G: left dep of head of top G: left sibling of right dep of top G: right sibling of left dep of top G: right sibling of left dep of next PRO : VERB : POS DEP FEATS + + + + + + + + + + + + + + + + + + + + + + + + + + + Table 1: Baseline and extended (FEATS) feature model for Swedish; S: stack, I: input, G: graph; ±n = n positions to the left(−) or right (+) 2.1 Treebank: Talbanken05 Talbanken05 is a Swedish treebank converted to dependency format, containing both written and spoken language (Nivre et al., 2006a).1 For each token, Talbanken05 contains information on word form, part of speech, head and dependency relation, as well as various morphosyntactic and/or lexical semantic features. The nature of this additional information varies depending on part of speech: NOUN : FORM to-right pass over the input. The decision that needs to be made at any point during this derivation is (a) whether to add a dependency arc (with some label) between the token on top of the stack (top) and the next token in the input queue (next), and (b) whether to pop top from the stack or push next onto the stack. The feat"
W08-2104,W06-2933,0,0.136602,"Missing"
W08-2104,W04-1911,0,0.057495,"Missing"
W08-2104,C02-1013,0,0.0176899,"l parsing accuracy, however, datadriven parsers often make errors in the assignment of argument relations such as subject and object and the exact influence of data-derived features on the parsing accuracy for specific linguistic constructions is still relatively poorly understood. There are a number of studies that investigate the influence of different features or representational choices on overall parsing accuracy, (Bod, 1998; Klein and Manning, 2003). There are also attempts at a more fine-grained analysis of accuracy, targeting specific linguistic constructions or grammatical functions (Carroll and Briscoe, 2002; K¨ubler and Proki´c, 2006; McDonald and Nivre, 2007). 2 Parsing Swedish Before we turn to a description of the treebank and the parser used in the experiments, we want to point to a few grammatical properties of Swedish that will be important in the following: c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Verb second (V2) Swedish is, like the majority of Germanic languages a V2-language; the finite verb always resides in second position in 25 CoNLL 2008: Proceed"
W08-2104,D07-1096,0,\N,Missing
W09-4638,P06-2057,0,0.0198737,"of accuracy. 1 Introduction Semantic classification of natural language has in recent years received extensive attention. 1 Most approaches to these tasks make use of languagespecific, annotated data or lexical resources, such as FrameNet and WordNet, a fact which complicates a multilingual perspective on semantic annotation and classification. One way of approaching this is found in work on projection of semantic classifications, such as semantic roles, making use of parallel corpora and hence the relation of translation to acquire semantic relations for new languages (Pado and Lapata, 2005; Johansson and Nugues, 2006). Much recent work in semantic classification assumes that the syntactic distribution of lexical items constitutes a reliable predictor of semantics or meaning, at the type level (Lin, 1998). In the task of verb classification, for instance, it has been shown that features motivated in typological generalizations and found to be highly predictive for classification in one language (English) may be ‘re-used’ for the classification of verbs in other languages, such as Italian (Merlo et al., 1 Parts of the research reported in this paper has been supported by the Deutsche Forschungsgemeinschaft ("
W09-4638,P98-2127,0,0.129148,"cal resources, such as FrameNet and WordNet, a fact which complicates a multilingual perspective on semantic annotation and classification. One way of approaching this is found in work on projection of semantic classifications, such as semantic roles, making use of parallel corpora and hence the relation of translation to acquire semantic relations for new languages (Pado and Lapata, 2005; Johansson and Nugues, 2006). Much recent work in semantic classification assumes that the syntactic distribution of lexical items constitutes a reliable predictor of semantics or meaning, at the type level (Lin, 1998). In the task of verb classification, for instance, it has been shown that features motivated in typological generalizations and found to be highly predictive for classification in one language (English) may be ‘re-used’ for the classification of verbs in other languages, such as Italian (Merlo et al., 1 Parts of the research reported in this paper has been supported by the Deutsche Forschungsgemeinschaft (DFG, Sonderforschungsbereich 632, project D4). Kristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 246–249 2002). The semantic property of animacy influences"
W09-4638,J01-3003,0,0.101767,"Missing"
W09-4638,P02-1027,0,0.065269,"Missing"
W09-4638,nivre-etal-2006-maltparser,0,0.221502,"ge to another, investigating the application of a semantic classifier trained on distributional data for one language directly to data from another language. We present first experiments examining the porting of automatic classification for the semantic property of animacy between the closely related languages of Swedish and Danish. Unlike previous work, we do not assume a parallel corpus or a gold standard annotation for the second language (Danish). 2 Swedish animacy classification Talbanken05 is a Swedish treebank converted to dependency format, containing both written and spoken language (Nivre et al., 2006b).2 In addition to information on part-of-speech, dependency head and relation, Talbanken05 distinguishes animacy for all nominal constituents.3 The dimension of animacy roughly distinguishes between entities which are alive and entities which are not. Table 1 presents an overview 2 The written sections of the treebank consist of professional prose and student essays and amount to 197,123 running tokens, spread over 11,431 sentences. 3 To be precise, the annotation in Talbanken05 distinguishes between ‘person’ and ‘non-person’. Cross-lingual porting of distributional semantic classification C"
W09-4638,nivre-etal-2006-talbanken05,0,0.141559,"ge to another, investigating the application of a semantic classifier trained on distributional data for one language directly to data from another language. We present first experiments examining the porting of automatic classification for the semantic property of animacy between the closely related languages of Swedish and Danish. Unlike previous work, we do not assume a parallel corpus or a gold standard annotation for the second language (Danish). 2 Swedish animacy classification Talbanken05 is a Swedish treebank converted to dependency format, containing both written and spoken language (Nivre et al., 2006b).2 In addition to information on part-of-speech, dependency head and relation, Talbanken05 distinguishes animacy for all nominal constituents.3 The dimension of animacy roughly distinguishes between entities which are alive and entities which are not. Table 1 presents an overview 2 The written sections of the treebank consist of professional prose and student essays and amount to 197,123 running tokens, spread over 11,431 sentences. 3 To be precise, the annotation in Talbanken05 distinguishes between ‘person’ and ‘non-person’. Cross-lingual porting of distributional semantic classification C"
W09-4638,H05-1108,0,0.0345453,"h little loss in terms of accuracy. 1 Introduction Semantic classification of natural language has in recent years received extensive attention. 1 Most approaches to these tasks make use of languagespecific, annotated data or lexical resources, such as FrameNet and WordNet, a fact which complicates a multilingual perspective on semantic annotation and classification. One way of approaching this is found in work on projection of semantic classifications, such as semantic roles, making use of parallel corpora and hence the relation of translation to acquire semantic relations for new languages (Pado and Lapata, 2005; Johansson and Nugues, 2006). Much recent work in semantic classification assumes that the syntactic distribution of lexical items constitutes a reliable predictor of semantics or meaning, at the type level (Lin, 1998). In the task of verb classification, for instance, it has been shown that features motivated in typological generalizations and found to be highly predictive for classification in one language (English) may be ‘re-used’ for the classification of verbs in other languages, such as Italian (Merlo et al., 1 Parts of the research reported in this paper has been supported by the Deut"
W09-4638,W03-0410,0,0.0496421,"Missing"
W10-3007,J93-2004,0,0.0437243,"E P ). our own tokenization to a GENIA token, we rely on TnT annotation only. In the merging of annotations across components, and also in downstream processing we have found it most convenient to operate predominantly in terms of characterization, i.e. sub-strings of the raw input that need not align perfectly with token boundaries. lems. Our pre-processing approach therefore deploys a home-grown, cascaded finite-state tokenizer (borrowed and adapted from the opensource English Resource Grammar; Flickinger (2000)), which aims to implement the tokenization decisions made in the Penn Treebank (Marcus et al., 1993) – much like GENIA, in principle – but properly treating corner cases like the ones above. Synchronized via characterization, this tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre a"
W10-3007,W09-1304,0,0.272407,"Missing"
W10-3007,P08-1108,0,0.00733183,", 1993) – much like GENIA, in principle – but properly treating corner cases like the ones above. Synchronized via characterization, this tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ t"
W10-3007,P81-1022,0,0.0609688,"Missing"
W10-3007,W03-3017,0,0.0121603,"ice, as well as more semantic properties detailing, e.g., subcategorization frames, semantic conceptual categories such as human, time and location, etc., resides in the F E A T S column. The parser outputs, which in turn form the basis for our scope resolution rules discussed in Section 4, also take this same form. The parser employed in this work is trained on the Wall Street Journal sections 2 – 24 of the Penn Treebank, converted to dependency format (Johansson and Nugues, 2007) and extended with XLE features, as described above. Parsing is performed using the arc-eager mode of MaltParser (Nivre, 2003) and an SVM with a polynomial kernel. When tested using 10-fold cross-validation on this data set, the parser achieves a labeled accuracy 3 Identifying Hedge Cues For the task of identifying hedge cues, we developed a binary maximum entropy (MaxEnt) classifier. The identification of cue words is used for (i) classifying sentences as certain/uncertain (Task 1), and (ii) providing input to the syntactic rules that we later apply for resolving the in-sentence scope of the cues (Task 2). We also report evaluation scores for the sub-task of cue detection in isolation. As annotated in the training d"
W10-3007,A00-1031,0,0.0383377,"ich may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take into account new features of the PoS Tagging and Lemmatization For PoS tagging and lemmatization, we combine (with its built-in, occasionally deviant tokenizer) and TnT (Brants, 2000), which operates on pre-tokenized inputs but in its default model is trained on financial news from the Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy provided by GENIA in the biomedical domain, while using our improved tokenization and producing inputs to the parsing stage (see Section 2.3 below) that as much as possible resemble the conventions used in the original training data for the parser – the Penn Treebank, once again. To this effect, for the vast majority of tokens we can align the GENIA tokenization with our own, and in these cases we typically"
W10-3007,W02-1503,0,0.121758,"icles Total 11871 2670 14541 Hedged Cues Sentences 2101 519 2620 2659 668 3327 Multi-Word Cues Tokens Cue Tokens 364 84 448 309634 68579 378213 3056 782 3838 Table 2: Some descriptive figures for the shared task training data. Token-level counts are based on the tokenization described in Section 2.1. parse history. score of 89.8 (Øvrelid et al., 2010). Parser stacking The procedure to enable the data-driven parser to learn from the grammardriven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al., 2008) and the English grammar developed within the ParGram project (Butt et al., 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank – one gold standard and one with LFG-annotation. We extend the gold standard treebank with additional information from the corresponding LFG analysis and train the data-driven dependency parser on the enhanced data set. See Øvrelid et al. (2010) for details of the conversion and training of the parser. Table 1 shows the enhanced dependency representation of the English sentence The unknown amino acid may be used by these species, taken from the training data. For each token, the par"
W10-3007,P09-2010,1,0.868874,"his tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It"
W10-3007,W10-3001,0,0.124802,"its tokens like ‘3,926.50’, ‘methlycobamide:CoM’, or ‘Ca(2+)’. Conversely, GENIA fails to isolate some kinds of opening single quotes, because the quoting conventions assumed in BioScope differ from those used in the GENIA Corpus; furthermore, it mis-tokenizes LATEX-style n- and m-dashes. On average, one in five sentences in the CoNLL training data exhibited GENIA tokenization probThe CoNLL-2010 shared task1 comprises two sub-tasks. Task 1 is described as learning to detect sentences containing uncertainty, while the object of Task 2 is learning to resolve the in-sentence scope of hedge cues (Farkas et al., 2010). Paralleling this two-fold task definition, the architecture of our system naturally decomposes into two main steps. First, a maximum entropy (MaxEnt) classifier is applied to automatically detect cue words. For Task 1, a given sentence is labeled as uncertain if it contains a word classified as a cue. For Task 2, we then go on to determine the scope of the identified cues using a set of manually crafted rules operating on dependency representations. For both Task 1 and Task 2, our system participates in the stricter category of ‘closed’ or ‘indomain’ systems. This means that we do not use an"
W10-3007,W08-0606,0,0.562357,"oken- and sentence-level F-scores, the effect of incrementally including a larger percentage of training data into the 10-fold cycles. (As described also for the other development results, while we are training on both the articles and the abstracts, we are testing only on the articles.) Model Development, Data Sets and Evaluation Measures hedge cues (possibly spanning multiple tokens). These latter scores are computed using the official shared task scorer script. While the training data made available for the shared task consisted of both abstracts and full articles from the BioScope corpus (Vincze et al., 2008), the test data were pre-announced to consist of biomedical articles only. In order to make the testing situation during development as similar as possible to what could be expected for the held-out testing, we only tested on sentences taken from the articles part of the training data. When developing the classifiers we performed 10-fold training and testing over the articles, while always including all sentences from the abstracts in the training set as well. Table 2 provides some basic descriptive figures summarizing the training data. As can be seen in Table 3, we will be reporting precisio"
W10-3007,W02-2018,0,0.0202726,"e task of determining whether a cue word forms part of a larger multi-word cue, is performed by a separate post-processing step, further described in Section 3.2. 3.1 Maximum Entropy Classification In the MaxEnt framework, each training example – in our case a paired word and label hwi , yi i – is represented as a feature vector f (wi , yi ) = fi ∈ <d . Each dimension or feature function fij can encode arbitrary properties of the data. The particular feature functions we are using for the cue identification are described under Section 3.4 below. For model estimation we use the TADM3 software (Malouf, 2002). For feature extraction and model tuning, we build on the experimentation environment developed by Velldal (2008) (in turn extending earlier work by Oepen et al. 3 Toolkit for Advanced Discriminative Modeling; available from http://tadm.sourceforge.net/. 50 90 (2004)). Among other things, its highly optimized feature handling – where the potentially expensive feature extraction step is performed only once and then combined with several levels of feature caching – make it computationally feasible to perform large-scale ‘grid searches’ over different configurations of features and model paramet"
W10-3007,P09-1043,0,0.0954061,"atures For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich featur"
W10-3007,nivre-etal-2006-maltparser,0,\N,Missing
W10-3007,W07-2416,0,\N,Missing
W12-3602,branco-etal-2010-developing,0,0.0137066,"Specifically, the Deep Linguistic Processing with HPSG Initiative (DELPH-IN1 ) has produced both manually and automatically annotated resources making available comparatively fine-grained syntactic and semantic analyses in the framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994). For English, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take"
W12-3602,P05-1067,0,0.0176179,"ilar information can take quite different forms across frameworks. We further seek to shed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonym"
W12-3602,flickinger-etal-2010-wikiwoods,1,0.847029,"Missing"
W12-3602,W09-1201,0,0.0731308,"Missing"
W12-3602,W07-2416,0,0.121834,"e in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities,"
W12-3602,P03-1054,0,0.00636733,"he representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices with respect to head status are largely substantive. The dependency relations employed for this representation are PropBank semantic roles, such as A0 (proto-agent), A1 (protopatient), and various modifier roles. Stanford Basic Dependencies (SB) The Stanford Dependency scheme, a popular alternative to CoNLL-style syntactic dependencies (CD), was originally provided as an additional output format for the Stanford parser (Klein & Manning, 2003). It is a result of a conversion from PTB-style phrase structure trees (be they gold standard or automatically produced)—combining ‘classic’ head finding rules with rules that target specific linguistic constructions, such as passives or attributive adjectives (Marneffe et al., 2006). The so-called basic format provides a dependency graph which conforms to the criteria listed above, and the heads are largely content rather than function words. The grammatical relations are organized in a hierarchy, rooted in the generic relation ‘dependent’ and containing 56 different relations (Marneffe & Man"
W12-3602,J93-2004,0,0.0479705,"pendency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities, frameworks, and languages. Moreover, many of the gold-standard dependency banks were created by automated conversion from pre-existing constituency treebanks— notably the venerable Penn Treebank for English (PTB; Marcus et al., 1993)—and there exist several conversion toolkits which convert from constituent structures to dependency structures. This conversion is not always trivial, and the outputs can differ notably in choices concerning head status, relation inventories, and formal graph properties of the resulting depedency structure. Incompatibilty of representations and differences in the ‘granularity’ of linguistic information hinder the evaluation of parsers across communities (Sagae et al., 2008). In this paper, we pursue theoretical as well as practical goals. First, we hope to shed more light on commonalities and"
W12-3602,marimon-2010-spanish,0,0.0134134,"ep Linguistic Processing with HPSG Initiative (DELPH-IN1 ) has produced both manually and automatically annotated resources making available comparatively fine-grained syntactic and semantic analyses in the framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994). For English, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take as 1 See http:/"
W12-3602,de-marneffe-etal-2006-generating,0,0.00979249,"on are PropBank semantic roles, such as A0 (proto-agent), A1 (protopatient), and various modifier roles. Stanford Basic Dependencies (SB) The Stanford Dependency scheme, a popular alternative to CoNLL-style syntactic dependencies (CD), was originally provided as an additional output format for the Stanford parser (Klein & Manning, 2003). It is a result of a conversion from PTB-style phrase structure trees (be they gold standard or automatically produced)—combining ‘classic’ head finding rules with rules that target specific linguistic constructions, such as passives or attributive adjectives (Marneffe et al., 2006). The so-called basic format provides a dependency graph which conforms to the criteria listed above, and the heads are largely content rather than function words. The grammatical relations are organized in a hierarchy, rooted in the generic relation ‘dependent’ and containing 56 different relations (Marneffe & Manning, 2008), largely based on syntactic functions. sb-hd_mc_c { e12 sp-hd_n_c hd-cmp_u_c v_prd_is_le d_-_sg-nmd_le 1 :_a_q( BV x 6 ) e9 :_similar_a_to(ARG1 x 6 ) x 6 :_technique_n_1 e12 :_almost_a_1(ARG1 e3 ) e3 :_impossible_a_for(ARG1 e18 ) e18 :_apply_v_to(ARG2 x 6 , ARG3 x 19 ) 2"
W12-3602,W08-1301,0,0.126868,"Missing"
W12-3602,J11-1007,0,0.0165324,"ral of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities, frameworks, and languages. Moreover, many of the gold-standard dependency banks were created by automated conversion from pre-existing constituency treebanks— notably the venerable Penn Treebank for English (PTB; Marcus et al., 1993)—and there exist several conversion toolkits which convert from constituent structures to dependency structures. This conversion is not always trivial, and the outputs can differ notably in choices conc"
W12-3602,J05-1004,0,0.0583595,"ST collection were obtained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices wit"
W12-3602,W04-2705,0,0.0333196,"tained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices with respect to head stat"
W12-3602,D09-1001,0,0.0165104,"nt forms across frameworks. We further seek to shed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this"
W12-3602,read-etal-2012-wesearch,1,0.863524,"Missing"
W12-3602,oepen-lonning-2006-discriminant,1,0.569818,"ries. In Section 4 below, we convert DELPH-IN derivations into syntactic bilexical dependencies. DELPH-IN Minimal Recursion Semantics (DM) As part of the full HPSG sign, the ERG also makes available a logical-form representation of propositional semantics in the format of Minimal Recursion Semantics (MRS; Copestake et al., 2005). While MRS proper utilizes a variant of predicate calculus that affords underspecification of scopal relations, for our goal of projecting semantic forms onto bilexical dependencies, we start from the reduction of MRS into the Elementary Dependency Structures (EDS) of Oepen & Lønning (2006), as shown in Figure 2. EDS is a lossy (i.e. non-reversible) conversion from MRS into a variable-free dependency graph; graph nodes (one per line in Figure 2) correspond to elementary predications from the original logical form and are connected by arcs labeled with MRS argument indices: ARG1, ARG2, etc. (where BV is reserved for what is the bound variable of a quantifier in the full MRS).6 Note that, while EDS already brings us relatively close to the other formats, there are graph nodes that do not correspond to individual words from our running example, for example the underspecified quanti"
W12-3602,P09-2010,1,0.844405,"glish, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take as 1 See http://www.delph-in.net for background. 3 our point of departure; Section 3 contrasts analyses of select linguistic phenomena by example; and Section 4 develops an automated conversion from HPSG analyses to bilexical dependencies. 2 The Multi-Annotated PEST Corpus At the 2008 Conference on Computational"
W12-3602,W08-2121,0,0.0449557,"syntactic or semantic bilexical relations. For English, syntactic dependencies in the PEST collection were obtained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designate"
W12-3602,J09-3003,0,0.0079248,"hed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) fo"
W12-3602,J03-4003,0,\N,Missing
W12-3602,D07-1096,0,\N,Missing
W13-5707,adolphs-etal-2008-fine,1,0.821468,"al., 2009). Table 1 provides exact sentence, token, and type counts for these data sets. Tokenization Conventions A relevant peculiarity of the DeepBank and Redwoods annotations in this context is the ERG approach to tokenization. Three aspects in Figure 1 deviate from the widely used PTB conventions: (a) hyphens (and slashes) introduce token boundaries; (b) whitespace in multi-word lexical units (like ad hoc, of course, or Mountain View) does not force token boundaries; and (c) punctuation marks are attached as ‘pseudo-affixes’ to adjacent words, reflecting the rules of standard orthography. Adolphs et al. (2008) offer some linguistic arguments for this approach to tokenization, but for our purposes it suffices to note that these differences to PTB tokenization may in part counter-balance each other, but do increase the types-per-tokens ratio somewhat. This property of the DeepBank annotations, arguably, makes English look somewhat similar to languages with moderate inflectional morphology. To take advantage of the finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on"
W13-5707,H91-1060,0,0.216164,"ency parsing), instantiating a comparatively diverse range of domains and genres (Oepen et al., 2004). Adding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al"
W13-5707,D12-1133,0,0.176439,"bserve the strongest correspondence between DT and CoNLL (at a Jaccard index of 0.49, compared to 0.32 for DT and Stanford, and 0.43 between CoNLL and Stanford). posed to its developers until the grammar and disambiguation model were finalized and frozen for this release. Ivanova et al. (2013) complement this comparison of dependency schemes through an empirical assesment in terms of ‘parsability’, i.e. accuracy levels available for the different target representations when training and testing a range of state-of-the-art parsers on the same data sets. In their study, the dependency parser of Bohnet and Nivre (2012), henceforth B&N, consistently performs best for all schemes and output configurations. Furthermore, parsability differences between the representations are generally very small. Based on these observations, we conjecture that DT is as suitable a target representation for parser comparison as any of the others. Furthermore, two linguistic factors add to the attractiveness of DT for our study: it is defined in terms of a formal (and implemented) theory of grammar; and it makes available more finegrained lexical categories, ERG lexical types, than is common in PTB-derived dependency banks. Cross"
W13-5707,P06-2006,0,0.0126994,"dding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they showed that the loss of accu"
W13-5707,W97-1502,0,0.0106444,"s (e.g. subject–head or head–complement: sbhd_mc_c and hd-cmp_u_c, respectively; see below for more details on unary rules). Preterminals are labeled with fine-grained lexical categories, dubbed ERG lexical types, that augment common parts of speech with additional information, for example argument structure or the distinction between count, mass, and proper nouns. In total, the ERG distinguishes about 250 construction types and 1000 lexical types. DeepBank annotations were created by combining the native ERG parser, dubbed PET (Callmeier, 2002), with a discriminant-based tree selection tool (Carter, 1997; Oepen et al., 2004), thus making it possible for annotators to navigate the large space of possible analyses efficiently, identify and validate the intended reading, and record its full HPSG analysis in the treebank. Owing to this setup, DeepBank in its current version 1.0 lacks analyses for some 15 percent of the WSJ sentences, for which either the ERG parser failed to suggest a set of candidates (within certain bounds on time and memory usage), or the annotators found none of the available parses acceptable.3 Furthermore, DeepBank annotations to date only comprise the first 21 sections of"
W13-5707,cer-etal-2010-parsing,0,0.0534845,"Missing"
W13-5707,P07-1032,0,0.0143626,"n in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they showed that the loss of accuracy due to the mapping process can swamp any actual parser differences. As long as heuristic conversion is required before evaluation, cross-framework comparison inevitably includes a level of fuzziness. An alternative approach is possible when there is enough data available in a particular representation, and convers"
W13-5707,W08-1301,0,0.0449293,"Missing"
W13-5707,D13-1120,1,0.827329,"arser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior to forest construction yields greatly improved efficiency at a moderate accuracy loss. Her lexical pruning model is trained on DeepBank 00–19 too, hence compatible with our setup. We include the bestperforming configuration of Dridan (2013) in our experiments, a variant henceforth referred to as ERGe . Unlike the other parsers in our study, PET internally operates over an ambiguous token lattice, and there is no easy interface to feed the parser pre-tokenized inputs. W"
W13-5707,I11-1100,0,0.0285816,"Missing"
W13-5707,P10-1035,0,0.0184554,"rocess can swamp any actual parser differences. As long as heuristic conversion is required before evaluation, cross-framework comparison inevitably includes a level of fuzziness. An alternative approach is possible when there is enough data available in a particular representation, and conversion (if any) is deterministic. Cer et al. (2010) used Stanford Dependencies (de Marneffe & Manning, 2008) to evaluate a range of statistical parsers. Pre- or post-converting from PTB phrase structure trees to the Stanford dependency scheme, they were able to evaluate a large number of different parsers. Fowler and Penn (2010) formally proved that a range of Combinatory Categorial Grammars (CCGs) are context-free. They trained the PCFG Berkeley parser on CCGBank, the CCG annotation of the PTB WSJ text (Hockenmaier & Steedman, 2007), advancing the state of the art in terms of supertagging accuracy, PARSEVAL measures, and CCG dependency accuracy. In other words, a specialized CCG parser is not necessarily more accurate than the generalpurpose Berkeley parser; this study, however, fails to also take parser efficiency into account. In related work for Dutch, Plank and van Noord (2010) suggest that, intuitively, one sho"
W13-5707,W01-0521,0,0.0808273,"Missing"
W13-5707,J07-3004,0,0.0193412,"h is possible when there is enough data available in a particular representation, and conversion (if any) is deterministic. Cer et al. (2010) used Stanford Dependencies (de Marneffe & Manning, 2008) to evaluate a range of statistical parsers. Pre- or post-converting from PTB phrase structure trees to the Stanford dependency scheme, they were able to evaluate a large number of different parsers. Fowler and Penn (2010) formally proved that a range of Combinatory Categorial Grammars (CCGs) are context-free. They trained the PCFG Berkeley parser on CCGBank, the CCG annotation of the PTB WSJ text (Hockenmaier & Steedman, 2007), advancing the state of the art in terms of supertagging accuracy, PARSEVAL measures, and CCG dependency accuracy. In other words, a specialized CCG parser is not necessarily more accurate than the generalpurpose Berkeley parser; this study, however, fails to also take parser efficiency into account. In related work for Dutch, Plank and van Noord (2010) suggest that, intuitively, one should expected that a grammar-driven system can be more resiliant to domain shifts than a purely data-driven parser. In a contrastive study on parsing into Dutch syntactic dependencies, they substantiated this e"
W13-5707,P13-3005,1,0.838136,"petitions (Nivre et al., 2007) and the ‘basic’ variant of Stanford Dependencies. They observe that the three dependency representations are broadly comparable in granularity and that there are substantial structural correspondences between the schemes. Measured as average Jaccard similarity over unlabeled dependencies, they observe the strongest correspondence between DT and CoNLL (at a Jaccard index of 0.49, compared to 0.32 for DT and Stanford, and 0.43 between CoNLL and Stanford). posed to its developers until the grammar and disambiguation model were finalized and frozen for this release. Ivanova et al. (2013) complement this comparison of dependency schemes through an empirical assesment in terms of ‘parsability’, i.e. accuracy levels available for the different target representations when training and testing a range of state-of-the-art parsers on the same data sets. In their study, the dependency parser of Bohnet and Nivre (2012), henceforth B&N, consistently performs best for all schemes and output configurations. Furthermore, parsability differences between the representations are generally very small. Based on these observations, we conjecture that DT is as suitable a target representation fo"
W13-5707,W12-3602,1,0.908199,"hrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG analyses recorded in DeepBank, Zhang and Wang (2009) and Ivanova et al. (2012) define a reduction into bi-lexical syntactic dependencies, which they call Derivation TreeDerived Dependencies (DT). Through application of the converter of Ivanova et al. (2012) to DeepBank, we can thus obtain a DT-annotated version of the standard WSJ text, to train and test a data-driven dependency and phrase structure parser, respectively, and to compare parsing results to a hybrid, grammar-driven HPSG parser. Furthermore, we can draw on a set of additional corpora annotated in the same HPSG format (and thus amenable to conversion for both phrase structure and dependency parsing), instant"
W13-5707,W03-2401,0,0.0464787,"en et al., 2004). Adding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they sh"
W13-5707,J93-2004,0,0.042212,"reduction in linguistic detail. 2 In contrast, much earlier work on cross-framework comparison involved post-processing parser outputs in form and content, into a target representation for which gold-standard annotations were available. In § 2 below, we argue that such conversion inevitably introduces blur into the comparison. ited to purely data-driven (or statistical) parsers, i.e. systems where linguistic knowledge is exclusively acquired through supervised machine learning from annotated training data. For English, the venerable Wall Street Journal (WSJ) portion of the Penn Treebank (PTB; Marcus et al., 1993) has been the predominant source of training data, for phrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG a"
W13-5707,D07-1013,0,0.0391918,"nd (c) the distribution of LAS over different lexical categories. Among the different dependency types, we observe that the notion of an adjunct is difficult for all three parsers. One of the hardest dependency labels is hdn-aj (post-adjunction to a nominal head), the relation employed for relative clauses and prepositional phrases attaching to a nominal head. The most common error for this relation is verbal attachment. It has been noted that dependency parsers may exhibit systematic performance differences with respect to dependency length (i.e. the distance between a head and its argument; McDonald & Nivre, 2007). In our experiments, we find that the parsers perform comparably on longer dependency arcs (upwards of fifteen words), with ERGa constantly showing the highest accuracy, and Berkeley holding a slight edge over B&N as dependency length increases. In Figure 3, one can eyeball accuracy levels per lexical category, where conjunctions (c) and various types of prepositions (p and pp) are the most difficult for all three parsers. That the DT analysis of coordination is challenging is unsurprising. Schwartz et al. 68 CB SC VM (2012) show that choosing conjunctions as heads in coordinate structures is"
W13-5707,A00-2022,1,0.62018,"epBank annotations, arguably, makes English look somewhat similar to languages with moderate inflectional morphology. To take advantage of the finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on our parser comparison. PET: Native HPSG Parsing The parser most commonly used with the ERG is called PET (Callmeier, 2002), a highly engineered chart parser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior"
W13-5707,P06-1055,0,0.114573,"Missing"
W13-5707,W10-2105,0,0.0233349,"Missing"
W13-5707,C12-1147,0,0.0515236,"Missing"
W13-5707,W11-2923,0,0.0197785,"enization mismatches local to some sub-segment of the input will not ‘throw off’ token correspondences in other parts of the string.5 We will refer to this character-based variant of the standard CoNLL metrics as LASc and UASc . 4 PCFG Parsing of HPSG Derivations Formally, the HPSG analyses in the DeepBank and Redwoods treebanks transcend the class of contextfree grammars, of course. Nevertheless, one can pragmatically look at an ERG derivation as if it were a context-free phrase structure tree. On this view, standard, off-the-shelf PCFG parsing techniques are applicable to the ERG treebanks. Zhang and Krieger (2011) explore this space experimentally, combining the ERG, Redwoods (but not DeepBank), and massive collections of automatically parsed text. Their study, however, does not consider parser efficiency.6 . In contrast, our goal is to reflect on practical tradeoffs along multiple dimensions. We therefore focus on Berkeley, as one of the currently best-performing (and relatively efficient) PCFG engines. Due to its ability to internally rewrite node labels, this parser should be expected to adapt well also to ERG derivations. Compared to the phrase structure annotations in the PTB, there are two struct"
W13-5707,W07-2207,1,0.846594,"finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on our parser comparison. PET: Native HPSG Parsing The parser most commonly used with the ERG is called PET (Callmeier, 2002), a highly engineered chart parser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior to forest construction yields greatly improved efficiency at a moderate accuracy loss. Her lexical pruning model is trained on DeepBank 00–"
W13-5707,P09-1043,0,0.474619,"ce of training data, for phrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG analyses recorded in DeepBank, Zhang and Wang (2009) and Ivanova et al. (2012) define a reduction into bi-lexical syntactic dependencies, which they call Derivation TreeDerived Dependencies (DT). Through application of the converter of Ivanova et al. (2012) to DeepBank, we can thus obtain a DT-annotated version of the standard WSJ text, to train and test a data-driven dependency and phrase structure parser, respectively, and to compare parsing results to a hybrid, grammar-driven HPSG parser. Furthermore, we can draw on a set of additional corpora annotated in the same HPSG format (and thus amenable to conversion for both phrase structure and de"
W14-2616,P11-1016,0,0.345599,"ugo.hammer@hioa.no p.e.solberg@ifikk.uio.no Abstract a lexicon-based approach where we classify the sentiment of a sentence based on the polarity of sentiment words in relation to a set of target words in the sentence. We expect that statistically the importance of a sentiment word towards the target word is related to the number of words between the sentiment and target word as suggested by Ding et al. (2008). Information about the syntactic environment of certain words or phrases has in previous work also been shown to be useful for the task of sentiment classification (Wilson et al., 2009; Jiang et al., 2011). In this work we therefore compare the results obtained using a token-based distance measure with a novel syntax-based distance measure obtained using dependency graphs and further augmented with linguistically motivated syntactic patterns expressed as dependency paths. In order to evaluate the proposed methods, we furthermore present a freely available corpus of Norwegian political discussion related to religion and immigration, which has been manually annotated for the sentiment expressed towards a set of target words, as well as a manually translated sentiment lexicon. Online political dis"
W14-2616,D12-1133,0,0.0228907,"gression model in Section 5.1 Each sentence was manually annotated as to whether the commenter was positive, negative or neutral towards the target word in the sentence. Each sentence was evaluated individually. The sentences were annotated based on real-world knowledge, e.g. a sentence like “Muhammed is like Hitler” would be annotated as a negative sentiment towards Muhammed. Further, if a commenter presented a negative fact about the target word, the sentence would be denoted as negative. 4.2 Corpus postprocessing The sentiment corpus was PoS-tagged and parsed using the Bohnet&Nivre-parser (Bohnet and Nivre, 2012). This parser is a transition-based dependency parser with joint tagger that implements global learning and a beam search for nonprojective labeled dependency parsing. This latter parser has recently outperformed pipeline systems (such as the Malt and MST parsers) both in terms of tagging and parsing accuracy for typologically diverse languages such as Chinese, English, and German. It has been reported to obtain a labeled accuracy of 87.7 for Norwegian (Solberg et al., 2014). The parser is trained on the Norwegian Dependency Treebank (NDT). The NDT is a treebank created at the National Library"
W14-2616,solberg-etal-2014-norwegian,1,0.81835,"d as negative. 4.2 Corpus postprocessing The sentiment corpus was PoS-tagged and parsed using the Bohnet&Nivre-parser (Bohnet and Nivre, 2012). This parser is a transition-based dependency parser with joint tagger that implements global learning and a beam search for nonprojective labeled dependency parsing. This latter parser has recently outperformed pipeline systems (such as the Malt and MST parsers) both in terms of tagging and parsing accuracy for typologically diverse languages such as Chinese, English, and German. It has been reported to obtain a labeled accuracy of 87.7 for Norwegian (Solberg et al., 2014). The parser is trained on the Norwegian Dependency Treebank (NDT). The NDT is a treebank created at the National Library of Norway in the period 2011-2013, manually annotated with part-of-speech tags, morphological features, syntactic functions and dependency graphs (Solberg et al., 2014; Solberg, 2013). It consists of approximately 600 000 tokens, equally distributed 93 between Norwegian Bokm˚al and Nynorsk, the two Norwegian written standards. Only the Bokm˚al subcorpus has been used here. Detailed annotation guidelines in English will be made available in April 2014 (Kinn et al., 2014). 4."
W14-2616,W13-5644,1,0.887076,"Missing"
W14-2616,J09-3003,0,0.0250494,"y liljao@ifi.uio.no hugo.hammer@hioa.no p.e.solberg@ifikk.uio.no Abstract a lexicon-based approach where we classify the sentiment of a sentence based on the polarity of sentiment words in relation to a set of target words in the sentence. We expect that statistically the importance of a sentiment word towards the target word is related to the number of words between the sentiment and target word as suggested by Ding et al. (2008). Information about the syntactic environment of certain words or phrases has in previous work also been shown to be useful for the task of sentiment classification (Wilson et al., 2009; Jiang et al., 2011). In this work we therefore compare the results obtained using a token-based distance measure with a novel syntax-based distance measure obtained using dependency graphs and further augmented with linguistically motivated syntactic patterns expressed as dependency paths. In order to evaluate the proposed methods, we furthermore present a freely available corpus of Norwegian political discussion related to religion and immigration, which has been manually annotated for the sentiment expressed towards a set of target words, as well as a manually translated sentiment lexicon."
W15-1816,J92-4003,0,0.669024,". (2008), the lexicalized statistics important to disambiguation in parsing are often sparse, and modeling relationships on a more general level than the words themselves may therefore be helpful. The other motivation is domain adaptation, attempting to leverage a parsing model for use on data from a new domain. By including information about word clusters estimated from unlabeled in-domain data, one can hope to reduce the loss in performance expected from using a parser trained on an out-of-domain treebank. While previous approaches have typically relied on the n-gram–based Brown clustering (Brown et al., 1992), this paper instead describes experiments using dependency-based word clusters formed using the generic clustering algorithm Kmeans. After applying a baseline dependency parser to unlabeled text, K-means is applied to form word clusters with features based on the dependency structures produced by the parser. The parser is then re-trained using features that record information about the dependency-derived clusters, thereby introducing an element of selftraining. The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the Engl"
W15-1816,W10-1409,0,0.0409033,"Missing"
W15-1816,D07-1101,0,0.0779514,"Missing"
W15-1816,gimenez-marquez-2004-svmtool,0,0.0196534,"Missing"
W15-1816,P08-1068,0,0.498658,"rained using information about the clusters, yielding improved parsing accuracy on a range of different data sets, including WSJ and the English Web Treebank. We report improved results using both in-domain and out-of-domain data, and also include a comparison with using n-gram–based Brown clustering. 1 Introduction Several recent studies have attempted to improve dependency parsers by including information about word clusters into their statistical parsing models. This is typically motivated by at least two concerns, both of which relate to the shortage of labeled training data. As argued by Koo et al. (2008), the lexicalized statistics important to disambiguation in parsing are often sparse, and modeling relationships on a more general level than the words themselves may therefore be helpful. The other motivation is domain adaptation, attempting to leverage a parsing model for use on data from a new domain. By including information about word clusters estimated from unlabeled in-domain data, one can hope to reduce the loss in performance expected from using a parser trained on an out-of-domain treebank. While previous approaches have typically relied on the n-gram–based Brown clustering (Brown et"
W15-1816,J93-2004,0,0.0515779,"ng algorithm Kmeans. After applying a baseline dependency parser to unlabeled text, K-means is applied to form word clusters with features based on the dependency structures produced by the parser. The parser is then re-trained using features that record information about the dependency-derived clusters, thereby introducing an element of selftraining. The re-trained parser obtains improved parsing accuracy on a range of different data sets, including the five web domains of the English Web Treebank (EWT) (Bies et al., 2012) and the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993). We document improvements using both in-domain and outof-domain data, and also when compared to using Brown clusters. All our parsing experiments use MaltParser (Nivre et al., 2007), a data-driven transition-based dependency parser. The rest of the paper is structured as follows. Section 2 provides an overview of previous work. Section 3 details the data sets we use, including comments on the pre-processing. Section 4 then describes the experimental set-up, while the actual experiments and results are described in Section 5. A summary with thoughts about future directions is provided in Secti"
W15-1816,P05-1012,0,0.230474,"Missing"
W15-1816,P09-1040,0,0.0178482,"models for the baseline and the re-trained parser, where p = PoS-tag, w = word form, d = dependency label in the graph constructed so far (if any), and l = cluster label. MaltParser’s stacklazy algorithm operates over three data structures: a stack (S) of partially processed tokens, a list (I) of nodes that have been on the stack, and a “lookahead” list (L) of nodes that have not been on the stack. We refer to the top of the stack using S0 and subsequent nodes using S1 , S2 , etc., and the leftmost/rightmost dependent of S0 with S0l /S0r . Skjærholt (2012). It employs the stacklazy algorithm (Nivre, 2009), along with the LIBLINEAR package (Fan et al., 2008) for inducing parse transition SVM classifiers. 4.2.1 Parser features Table 3 describes the baseline feature set, along with three additional feature sets based on the models described in Øvrelid and Skjærholt (2012) and that in various ways include information about cluster labels: PoS simple, Form simple and Form all. These augmented feature sets are constructed by copying the full baseline feature set (all) or only the features that pertain to a single token (simple) and involve either the PoS-tag or the word form respectively. (Note that"
W15-1816,C12-2088,1,0.71791,"e created from the L’Est R´epublicain corpus (using up to 1,000 clusters), comprising 125 million words of news text, and cluster-based features are then added to the Berkeley PCFG parser with latent annotations (Petrov et al., 2006), before parsing the French Treebank (Abeill´e et al., 2003). Candito and Seddah (2010) analyze the results with respect to word frequency and find improvements in performance for all strata; unseen or rare words, as well as medium- to highfrequency words. Adding PoS-information to the lemmas also appeared beneficial, though depending on the quality of the tagger. Øvrelid and Skjærholt (2012) apply Brown clusters to improve dependency parsing of English web data using MaltParser. Augmenting a WSJ-trained parser with Brown clusters – using the cluster labels of Turian et al. (2010) computed for the Reuters corpus – is shown to improve parsing accuracy on a range of web texts, including the Twitter and user forum data from the web 2.0 data sets described by Foster et al. (2011) and web data from various sources in the OntoNotes corpus, release 4 (Weischedel et al., 2011). In the experiments of Øvrelid and Skjærholt (2012), cluster information was found to be more beneficial for pars"
W15-1816,P06-1055,0,0.0170831,"Missing"
W15-1816,W09-3829,0,0.0850245,"Experimenting with different tree cut-offs, producing different numbers of clusters, Øvrelid and Skjærholt (2012) found that using a smaller number of large and general clusters (100–320) worked better than using a higher number of smaller and more finegrained clusters (experimenting with up to 3200 clusters). As an alternative to the above approaches using n-gram-based Brown clusters, the current paper documents experiments with using syntactically informed clusters instead, generated with a generic clustering algorithm. One previous study following a related line of investigation is that of Sagae and Gordon (2009) who also used parsed data for creating syntactically informed clusters. The clustering is there performed by applying the general method of (average-link) hierarchical agglomerative clustering to the 5,000 most frequent words of Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 118 the BLLIP WSJ corpus, containing approximately 30 million words of WSJ news articles, parsed with the Charniak (2000) parser. The features used for the clustering encode phrase-structure tree paths that include direction information and non-terminal node labels, but does not inc"
W15-1816,C08-1095,0,0.0255675,"us, containing approximately 30 million words of WSJ news articles, parsed with the Charniak (2000) parser. The features used for the clustering encode phrase-structure tree paths that include direction information and non-terminal node labels, but does not include lexical information or part-of-speech tags. The clusters are then added as features in a data-driven transition-based dependency parser which is again used to identify predicate-argument dependencies extracted from the HPSG Treebank developed by Miyao et al. (2004) comprising the standard PTB WSJ sections. The pipeline described by Sagae and Tsujii (2008) thus include several layers of cross-framework interactions. Cutting the cluster hierarchy to include 600 clusters was shown to given the highest F-score, significantly improving the accuracy of the predicate-argument dependency parser. The goal of Sagae and Gordon (2009) is to improve the accuracy of a fast dependency parser by using a corpus which has previously been automatically annotated using a more accurate but slower phrase-structure parser. In our experiments we seek to improve a baseline dependency parser by using clusters formed directly on the basis of the annotations of the basel"
W15-1816,A00-2018,0,\N,Missing
W15-1816,P10-1040,0,\N,Missing
W16-0413,J92-4003,0,0.35061,"s in compliance with the root acomp nsubj advmod amod i m PRON VERB Features Based on the enriched corpus, as described above, we experiment with the following sources of information for defining our features: conj dobj fucking scared VERB ADJ kill VERB them d PRON X Figure 2: Dependency parse of example sentence from the corpus, with assigned uPOS tags. ClearNLP converter (Choi and Palmer, 2012), see Figure 2 for an example dependency graph from the corpus. The corpus was further enriched with the cluster labels described in Turian et al. (2010), created using the Brown clustering algorithm (Brown et al., 1992) and induced from the Reuters corpus of English newswire text (RCV1). We vary the number of clusters to be either 100, 320, 1000 or 3200 clusters and use the full cluster label. We also make use of the WordNet resource (Fellbaum, 1998) to include information about the synset of a word, as well as its parent and grandparent synsets. Classifiers We test three different classification frameworks in our development testing: Maximum Entropy (MaxEnt), Support Vector Machines (SVM), and Random Forests (RF). We approach the task as a binary classification task, using the implementations found in the s"
W16-0413,P09-2079,0,0.0319516,"rsal POS (uPOS) – Dependency Relation • Semantic: – Brown cluster label – WordNet synset, + parent and grandparent The features are structured according to a set of feature templates, which record varying degrees of linear order and syntactic context: bag-of features (unordered), bigrams, trigrams and dependency triples. Examples of the latter, given the sentence in Figure 2, would be: {&lt;m, nsubj, i&gt;, &lt;root, root, m&gt;, &lt;scared, amod, fucking&gt;, . . . } Our lexicalized features are very specific and require the exact combination of two lexical items in order to apply to a new instance. Following Joshi and Penstein-Rose (2009), we therefore experiment with generalizing features by ‘backing off’ to a more general category, e.g., from word form to lemma or POS. For example, a dependency triple over word forms like &lt;kill, dobj, them&gt; would thus be generalized to &lt;VERB, dobj, them&gt; using head-backoff, and &lt;kill, dobj, PRON&gt; using modifier-backoff. These additional backoff features are included for bigrams and trigrams as well as dependency triples. We impose a simple count-based reduction of the feature set; only features appearing at least twice in the training data are included in the model. 4.2 Development results W"
W16-0413,petrov-etal-2012-universal,0,0.0756479,"Missing"
W16-0413,P10-1040,0,0.0466524,"et of Petrov et al. (2012). The dependency parser assigns an analysis in compliance with the root acomp nsubj advmod amod i m PRON VERB Features Based on the enriched corpus, as described above, we experiment with the following sources of information for defining our features: conj dobj fucking scared VERB ADJ kill VERB them d PRON X Figure 2: Dependency parse of example sentence from the corpus, with assigned uPOS tags. ClearNLP converter (Choi and Palmer, 2012), see Figure 2 for an example dependency graph from the corpus. The corpus was further enriched with the cluster labels described in Turian et al. (2010), created using the Brown clustering algorithm (Brown et al., 1992) and induced from the Reuters corpus of English newswire text (RCV1). We vary the number of clusters to be either 100, 320, 1000 or 3200 clusters and use the full cluster label. We also make use of the WordNet resource (Fellbaum, 1998) to include information about the synset of a word, as well as its parent and grandparent synsets. Classifiers We test three different classification frameworks in our development testing: Maximum Entropy (MaxEnt), Support Vector Machines (SVM), and Random Forests (RF). We approach the task as a b"
W16-0413,W12-2103,0,0.0711624,"e detection of threats in a data set of Dutch tweets (Oostdijk and van Halteren, 2013a; Oostdijk and van Halteren, 2013b), which consists of a collection of 5000 threatening tweets. In addition, a large number of random tweets were collected for development and testing. The system relies on manually constructed recognition patterns in the form of n-grams, but details about the strategy used to construct these patterns are not given. In Oostdijk and van Halteren (2013b), a manually crafted shallow parser is added to the system. This improves results to a precision of 0.39 and a recall of 0.59. Warner and Hirschberg (2012) present a method for detecting hate speech in user-generated web text, which relies on machine learning in combination with template-based features. The task is approached as a word-sense disambiguation task, since the same words can be used in both hateful and nonhateful contexts. The features used in the classification were combinations of uni-, bi- and trigrams, part-of-speech-tags and Brown clusters. The best results were obtained using only unigram features, Proceedings of NAACL-HLT 2016, pages 66–71, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguisti"
W16-0413,J93-2004,0,\N,Missing
W17-0201,Q16-1022,0,0.0360415,"Missing"
W17-0201,W17-0217,1,0.732885,"t-diff script of the TnT-distribution, and scores are computed over the base PoS tags, disregarding morphological features. Mate is evaluated using the MaltEval tool (Nilsson and Nivre, 2008). For the second pipeline, we rely on UDPipe’s built-in evaluation support, which also implements MaltEval. 5.1 Test Table 1: Results on the UD development data for tagging and parsing the two written standards for Norwegian, Bokmål (BM) and Nynorsk (NN), including ‘cross-standard’ training and testing. Data split For Bokmål we use the same split for training, development and testing as defined for NDT by Hohle et al. (2017). As no pre-defined split was established for Nynorsk we defined this ourselves, following the same 80-10-10 proportions and also taking care to preserve contiguous texts in the various sections while also keeping them balanced in terms of genre. 5 Train UDPipe to adapt to errors made by the tagger. While this is often achieved using jackknifing (n-fold training and tagging of the labeled training data), we here simply apply the taggers to the very same data they have been trained on, reflecting the ‘training error’ of the taggers. We have found that training on such ‘silver-standard’ tags imp"
W17-0201,Q16-1031,0,0.0272639,"ank. For the purpose of the current work, the Nynorsk section has also been automatically converted to Universal Dependencies, making use of the conversion software described in Øvrelid and Hohle (2016) with minor modifications.2 The release of universal representations for PoS tags (Petrov et al., 2012) and dependency syntax (Nivre et al., 2016) has enabled research in crosslingual parsing that does not require a languagespecific conversion procedure. Tiedemann et al. (2014) utilize statistical MT for treebank translation in order to train cross-lingual parsers for a range of language pairs. Ammar et al. (2016) employ a combination of cross-lingual word clusters and embeddings, language-specific features and typological information in a neural network architecture where one and the same parser is used to parse many languages. UD conversion of NDT Nynorsk Figure 1 provides the UD graph for our Nynorsk example sentence. The NDT and UD schemes differ in terms of both PoS tagset and morphological features, as well as structural analyses. The conversion therefore requires non-trivial transformations of the dependency trees, in addition to mappings of tags and labels that make reference to a combination I"
W17-0201,A00-1031,0,0.0834174,"tion In this work the focus is on cross-standard, rather than cross-lingual, parsing. The two standards of Norwegian can be viewed as two highly related languages, which share quite a few lexical items, hence we assume that parser lexicalization will be beneficial. Like Tiedemann et al. (2014), we experiment with machine translation of train2 The data used for these experiments follows the UD v1.4 guidelines, but its first release as a UD treebank will be in v2.0. For replicability we therefore make our data available from the companion Git repository. 2 TnT & Mate The widely used TnT tagger (Brants, 2000), implementing a 2nd order Markov model, achieves high accuracy as well as very high speed. TnT was used by Petrov et al. (2012) when evaluating the proposed universal tag set. Solberg et al. (2014) found the Mate dependency parser (Bohnet, 2010) to have the best performance for parsing of NDT, and recent dependency parser comparisons (Choi et al., 2015) have also found Mate to perform very well for English. The fast training time of Mate also facilitates rapid experimentation. Mate implements the secondorder maximum spanning tree dependency parsing algorithm of Carreras (2007) with the passiv"
W17-0201,nilsson-nivre-2008-malteval,0,0.0236766,"one can not assume (as is sometimes done, often by necessity due to unavailable resources) that tools created for, say, Bokmål can be applied to Nynorsk without a substantial increase in errors. Evaluation The taggers are evaluated in terms of tagging accuracy (Acc in the following tables) while the parsers are evaluated by labeled and unlabeled attachment score (LAS and UAS). For the TnT tagger, accuracy is computed with the tnt-diff script of the TnT-distribution, and scores are computed over the base PoS tags, disregarding morphological features. Mate is evaluated using the MaltEval tool (Nilsson and Nivre, 2008). For the second pipeline, we rely on UDPipe’s built-in evaluation support, which also implements MaltEval. 5.1 Test Table 1: Results on the UD development data for tagging and parsing the two written standards for Norwegian, Bokmål (BM) and Nynorsk (NN), including ‘cross-standard’ training and testing. Data split For Bokmål we use the same split for training, development and testing as defined for NDT by Hohle et al. (2017). As no pre-defined split was established for Nynorsk we defined this ourselves, following the same 80-10-10 proportions and also taking care to preserve contiguous texts i"
W17-0201,D07-1101,0,0.0309202,"used TnT tagger (Brants, 2000), implementing a 2nd order Markov model, achieves high accuracy as well as very high speed. TnT was used by Petrov et al. (2012) when evaluating the proposed universal tag set. Solberg et al. (2014) found the Mate dependency parser (Bohnet, 2010) to have the best performance for parsing of NDT, and recent dependency parser comparisons (Choi et al., 2015) have also found Mate to perform very well for English. The fast training time of Mate also facilitates rapid experimentation. Mate implements the secondorder maximum spanning tree dependency parsing algorithm of Carreras (2007) with the passiveaggressive perceptron algorithm of Crammer et al. (2006) implemented with a hash kernel for faster processing times (Bohnet, 2010). of various kinds of linguistic information. For instance, in terms of PoS tags, the UD scheme offers a dedicated tag for proper nouns (PROPN), where NDT contains information about noun type among its morphological features. UD further distinguishes auxiliary verbs (AUX) from main verbs (VERB). This distinction is not explicitly made in NDT, hence the conversion procedure makes use of the syntactic context of a verb; verbs that have a non-finite de"
W17-0201,D14-1082,0,0.0224774,"ouns and possessive pronouns. 4 UDPipe UDPipe (Straka et al., 2016) provides an open-source C++ implementation of an entire end-to-end pipeline for dependency parsing. All components are trainable and default settings are provided based on tuning towards the UD treebanks. The two components of UDPipe used in our experiments comprise the MorphoDiTa tagger (Straková et al., 2014) and the Parsito parser (Straka et al., 2015). MorphoDiTa implements an averaged perceptron algorithm (Collins, 2002) while Parsito is a greedy transition-based parser based on the neural network classifier described by Chen and Manning (2014). When training the components, we use the same parametrization as reported in Straka et al. (2016) after tuning the parser for version 1.2 of the Bokmål UD data. For the parser, this includes form embeddings of dimension 50, PoS tag, FEATS and arc label embeddings of dimension 20, and a 200-node hidden layer. For each experiment, we pre-train the form embeddings on the training data (i.e., the raw text of whatever portion of the labeled training data is used for a given experiment) using word2vec (Mikolov et al., 2013), again with the same parameters as reported by Straka et al. (2015) for a"
W17-0201,P15-1038,0,0.0130111,"ta used for these experiments follows the UD v1.4 guidelines, but its first release as a UD treebank will be in v2.0. For replicability we therefore make our data available from the companion Git repository. 2 TnT & Mate The widely used TnT tagger (Brants, 2000), implementing a 2nd order Markov model, achieves high accuracy as well as very high speed. TnT was used by Petrov et al. (2012) when evaluating the proposed universal tag set. Solberg et al. (2014) found the Mate dependency parser (Bohnet, 2010) to have the best performance for parsing of NDT, and recent dependency parser comparisons (Choi et al., 2015) have also found Mate to perform very well for English. The fast training time of Mate also facilitates rapid experimentation. Mate implements the secondorder maximum spanning tree dependency parsing algorithm of Carreras (2007) with the passiveaggressive perceptron algorithm of Crammer et al. (2006) implemented with a hash kernel for faster processing times (Bohnet, 2010). of various kinds of linguistic information. For instance, in terms of PoS tags, the UD scheme offers a dedicated tag for proper nouns (PROPN), where NDT contains information about noun type among its morphological features."
W17-0201,L16-1250,1,0.698351,"possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and showed that parser lexicalization actually improved parsing results between these languages. 3 The Norwegian UD Treebank Universal Dependencies (UD) (de Marneffe et al., 2014; Nivre, 2015) is a community-driven effort to create cross-linguistically consistent syntactic annotation. Our experiments are based on the Universal Dependency conversion (Øvrelid and Hohle, 2016) of the Norwegian Dependency Treebank (NDT) (Solberg et al., 2014). NDT contains manually annotated syntactic and morphological information for both varieties of Norwegian; 311,000 tokens of Bokmål and 303,000 tokens of Nynorsk. The treebanked material mostly comprises newspaper text, but also includes government reports, parliament transcripts and blog excerpts. The UD version of NDT has until now been limited to the Bokmål sections of the treebank. For the purpose of the current work, the Nynorsk section has also been automatically converted to Universal Dependencies, making use of the conve"
W17-0201,W02-1001,0,0.0621248,"nd be vs bu for definiteness, and (b) rules that make reference to closed class lemmas, such as quantificational pronouns and possessive pronouns. 4 UDPipe UDPipe (Straka et al., 2016) provides an open-source C++ implementation of an entire end-to-end pipeline for dependency parsing. All components are trainable and default settings are provided based on tuning towards the UD treebanks. The two components of UDPipe used in our experiments comprise the MorphoDiTa tagger (Straková et al., 2014) and the Parsito parser (Straka et al., 2015). MorphoDiTa implements an averaged perceptron algorithm (Collins, 2002) while Parsito is a greedy transition-based parser based on the neural network classifier described by Chen and Manning (2014). When training the components, we use the same parametrization as reported in Straka et al. (2016) after tuning the parser for version 1.2 of the Bokmål UD data. For the parser, this includes form embeddings of dimension 50, PoS tag, FEATS and arc label embeddings of dimension 20, and a 200-node hidden layer. For each experiment, we pre-train the form embeddings on the training data (i.e., the raw text of whatever portion of the labeled training data is used for a give"
W17-0201,petrov-etal-2012-universal,0,0.144654,"Missing"
W17-0201,de-marneffe-etal-2014-universal,0,0.105722,"Missing"
W17-0201,P11-2120,0,0.0223967,"entence in Nynorsk (top row) and Bokmål (second row) with corresponding English gloss, UD PoS and dependency analysis. ing data, albeit using a rule-based MT system with no word alignments. Our main goal is to arrive at the best joint model that may be applied to both Norwegian standards. guage pairs and less related languages. This task has been approached via so-called ’annotation projection’, where parallel data is used to induce structure from source to target language (Hwa et al., 2005; Spreyer et al., 2010; Agi´c et al., 2016) and as delexicalized model transfer (Zeman and Resnik, 2008; Søgaard, 2011; Täckström et al., 2012). The basic procedure in the latter work has relied on a simple conversion procedure to map part-of-speech tags of the source and target languages into a common tagset and subsequent training of a delexicalized parser on (a possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and showed that parser lexicalization actually improved parsing results between these languages. 3 The"
W17-0201,spreyer-etal-2010-training,1,0.80925,"not any satisfactory multiuse-house with those money ADV DET ADJ NOUN ADP DET NOUN Figure 1: Example sentence in Nynorsk (top row) and Bokmål (second row) with corresponding English gloss, UD PoS and dependency analysis. ing data, albeit using a rule-based MT system with no word alignments. Our main goal is to arrive at the best joint model that may be applied to both Norwegian standards. guage pairs and less related languages. This task has been approached via so-called ’annotation projection’, where parallel data is used to induce structure from source to target language (Hwa et al., 2005; Spreyer et al., 2010; Agi´c et al., 2016) and as delexicalized model transfer (Zeman and Resnik, 2008; Søgaard, 2011; Täckström et al., 2012). The basic procedure in the latter work has relied on a simple conversion procedure to map part-of-speech tags of the source and target languages into a common tagset and subsequent training of a delexicalized parser on (a possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and sho"
W17-0201,L16-1680,0,0.0364895,"Missing"
W17-0201,P14-5003,0,0.0342168,"Missing"
W17-0201,N12-1052,0,0.0249746,"rsk (top row) and Bokmål (second row) with corresponding English gloss, UD PoS and dependency analysis. ing data, albeit using a rule-based MT system with no word alignments. Our main goal is to arrive at the best joint model that may be applied to both Norwegian standards. guage pairs and less related languages. This task has been approached via so-called ’annotation projection’, where parallel data is used to induce structure from source to target language (Hwa et al., 2005; Spreyer et al., 2010; Agi´c et al., 2016) and as delexicalized model transfer (Zeman and Resnik, 2008; Søgaard, 2011; Täckström et al., 2012). The basic procedure in the latter work has relied on a simple conversion procedure to map part-of-speech tags of the source and target languages into a common tagset and subsequent training of a delexicalized parser on (a possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and showed that parser lexicalization actually improved parsing results between these languages. 3 The Norwegian UD Treebank Uni"
W17-0201,W14-1614,0,0.0434666,"Missing"
W17-0201,2009.freeopmt-1.7,0,0.0142601,". Machine-translated training data The results above show that combining training data across standards can improve parsing performance. As mentioned in the introduction, though, there is a large degree of lexical divergence between the two standards. In our next suite of experiments, we therefore attempt to further improve the results by automatically machine-translating the training texts. Given the strong degree of structural equivalence between Norwegian Bokmål and Nynorsk, we can expect MT to yield relatively accurate translations. For this, we use the two-way Bokmål–Nynorsk MT system of Unhammer and Trosterud (2009), a rule-based shallowtransfer system built on the open-source MT platform Apertium (Forcada et al., 2011). The raw text passed to Apertium is extracted The results for the development data are shown in Table 2. Adding the MT data reinforces the trend observed for mixing the original training sets: Despite that PoS tagging accuracy typically (though not always) decreases when adding data, parsing accuracy improves. For the TnT+Mate pipeline, we see that the best parser performance is obtained with the single-standard models including the MT data, while UDPipe achieves the best results when usi"
W17-0201,I08-3008,0,0.0402951,"NOUN Figure 1: Example sentence in Nynorsk (top row) and Bokmål (second row) with corresponding English gloss, UD PoS and dependency analysis. ing data, albeit using a rule-based MT system with no word alignments. Our main goal is to arrive at the best joint model that may be applied to both Norwegian standards. guage pairs and less related languages. This task has been approached via so-called ’annotation projection’, where parallel data is used to induce structure from source to target language (Hwa et al., 2005; Spreyer et al., 2010; Agi´c et al., 2016) and as delexicalized model transfer (Zeman and Resnik, 2008; Søgaard, 2011; Täckström et al., 2012). The basic procedure in the latter work has relied on a simple conversion procedure to map part-of-speech tags of the source and target languages into a common tagset and subsequent training of a delexicalized parser on (a possibly filtered version of) the source treebank. Zeman and Resnik (2008) applied this approach to the highly related language pair of Swedish and Danish, and Skjærholt and Øvrelid (2012) extended the language inventory to also include Norwegian, and showed that parser lexicalization actually improved parsing results between these la"
W17-0201,solberg-etal-2014-norwegian,1,\N,Missing
W17-0201,L16-1262,0,\N,Missing
W17-0217,A00-1031,0,0.521107,"opment required many repeated cycles of training and testing for the various modified tagsets, we sought a PoS tagger that is both reasonably fast and accurate. There is often a considerable trade-off between the two factors, as the most accurate taggers tend to suffer in terms of speed due to their complexity. However, a widely used tagger that achieves both close to state-of-the-art accuracy as 1 Our defined train/dev./test split is available for download at http://github.com/petterhh/ndt-tools and will be distributed with future releases of the treebank. 144 well as very high speed is TnT (Brants, 2000), and hence we adopt this for the current study. Parser In choosing a syntactic parser for our experiments, we considered previous work on dependency parsing of Norwegian, specifically that of Solberg et al. (2014), who found the graphbased Mate parser (Bohnet, 2010) to have the best performance for NDT. Recent dependency parser comparisons (Choi et al., 2015) show very strong results for Mate also for English, outperforming a range of contemporary state-of-the-art parsers. We will be using Mate for gauging the effects of the tagset modifications in our experiments. Training Testing LAS UAS Go"
W17-0217,D14-1103,0,0.0181425,"rser to German using tagsets of varying granularity; the 12 tags of the Universal Tagset (UTS) (Petrov et al., 2012), the 54 tags of STTS and an extended version of STTS including all the morphological information from the treebanks used for training, resulting in up to 783 tags. Maier et al. (2014) experimented with six different PoS taggers, but found TnT to have the most consistent performance across different tagsets and settings. Predictably, tagger accuracy drops as granularity increases, but the best parsing performance was observed for the medium-sized tagset, i.e., the original STTS. Müller et al. (2014) attempt to improve dependency parsing with Mate by automatically defining a more fine-grained tagset using so-called split-merge training to create Hidden Markov models with latent annotations (HMM-LA). This entails iteratively splitting every tag into two subtags, but reverting to the original tag unless a certain improvement in the likelihood function is observed. Müller et al. (2014) argue that the resulting annotations “are to a considerable extent linguistically interpretable”. Similarly to the setup of Rehbein and Hirschmann (2013), two layers of taggers are used. While the modification"
W17-0217,P15-1038,0,0.073526,"both close to state-of-the-art accuracy as 1 Our defined train/dev./test split is available for download at http://github.com/petterhh/ndt-tools and will be distributed with future releases of the treebank. 144 well as very high speed is TnT (Brants, 2000), and hence we adopt this for the current study. Parser In choosing a syntactic parser for our experiments, we considered previous work on dependency parsing of Norwegian, specifically that of Solberg et al. (2014), who found the graphbased Mate parser (Bohnet, 2010) to have the best performance for NDT. Recent dependency parser comparisons (Choi et al., 2015) show very strong results for Mate also for English, outperforming a range of contemporary state-of-the-art parsers. We will be using Mate for gauging the effects of the tagset modifications in our experiments. Training Testing LAS UAS Gold Gold Auto Gold Auto Auto 90.15 85.68 87.01 92.51 88.98 90.19 Table 3: Results of parsing with Mate using various configurations of PoS tag sources in training and testing. Gold denotes gold standard tags while Auto denotes tags automatically predicted by TnT. such ‘silver-standard’ tags actually improves parsing scores substantially compared to training on"
W17-0217,petrov-etal-2012-universal,0,0.172604,"Missing"
W17-0217,W03-0419,0,0.586305,"Missing"
W17-0217,W14-6101,0,0.0228279,"treet Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993). Based on linguistic considerations, MacKinlay (2005) mapped the 45 tags of the original PTB tagset to more fine-grained tagsets to investigate whether additional linguistic information could assist the tagger. Experimenting with both lexically and syntactically conditioned modifications, they did not find any statistically significant improvements, arguing that their results do not support the hypothesis that it is possible to achieve significant performance improvements in PoS tagging by utilizing a finer-grained tagset. Maier et al. (2014) also experiment with applying the Berkeley constituency parser to German using tagsets of varying granularity; the 12 tags of the Universal Tagset (UTS) (Petrov et al., 2012), the 54 tags of STTS and an extended version of STTS including all the morphological information from the treebanks used for training, resulting in up to 783 tags. Maier et al. (2014) experimented with six different PoS taggers, but found TnT to have the most consistent performance across different tagsets and settings. Predictably, tagger accuracy drops as granularity increases, but the best parsing performance was obse"
W17-0217,solberg-etal-2014-norwegian,1,0.785034,"n NDT. tense. Selected subsets of these features are used in our tagset modifications, where the coarse PoS tag of relevant tokens is concatenated with one or more features to include more linguistic information in the tags. Table 1: Overview of the original PoS tagset of NDT (excluding punctuation tags). and demonstrate significant – and substantial – improvements in parser performance. 3 Syntactic Annotation The syntactic annotation choices in NDT are largely based on the Norwegian Reference Grammar (Faarlund et al., 1997). Some central annotation choices are outlined in Table 2, taken from Solberg et al. (2014), providing overview of the analyses of syntactic constructions that often distinguish dependency treebanks, such as coordination and the treatment of auxiliary and main verbs. The annotations comprise 29 dependency relations, including ADV (adverbial), SUBJ (subject) and KOORD (coordination). The Norwegian Dependency Treebank Our experiments are based on the newly developed Norwegian Dependency Treebank (NDT) (Solberg et al., 2014), the first publicly available treebank for Norwegian. It was developed at the National Library of Norway in collaboration with the University of Oslo, and contains"
W17-0217,W13-5644,0,0.0210758,"experimental setup. Data Set Split As there was no existing standardized data set split of NDT due to its recent development, we first needed to define separate sections for training, development and testing.1 Our proposed sectioning of the treebank follows a standard 80-10-10 split. In establishing the split, care has been taken to preserve contiguous texts in the various sections while also keeping them balanced in terms of genre. Morphological Annotation The morphological annotation and PoS tagset of NDT is based on the same inventory as used by the Oslo-Bergen Tagger (Hagen et al., 2000; Solberg, 2013), which in turn is largely based on the work of Faarlund et al. (1997). The tagset consists of 12 morphosyntactic PoS tags outlined in Table 1, with 7 additional tags for punctuation and symbols. The tagset is thus rather coarse-grained, with broad categories such as subst (noun) and verb (verb). The PoS tags are complemented by a large set of morphological features, providing information about morphological properties such as definiteness, number and Tagger As our experiments during development required many repeated cycles of training and testing for the various modified tagsets, we sought a"
W17-0217,L16-1680,0,0.0603878,"Missing"
W17-0217,W10-1401,0,0.0206416,"test set improves from 90.34 to 90.57 for English (using a HMM-LA tagset of 115 tags) and from 87.92 to 88.24 for German (using 107 tags). Moving beyond tagging, Seddah et al. (2009) focus on syntactic constituent parsing for French and show that extending the PoS tagset with information about mood and finiteness for verbs is indeed beneficial. Similarly, the recent shared tasks on parsing morphologically rich languages has seen quite a bit of work focused on evaluating the effect of various types of morphological information on syntactic parsing (both constituent-based and dependency-based) (Tsarfaty et al., 2010; Seddah et al., 2013). They find that the type of morphological information which is beneficial for parsing varies across languages and the quality of this information (i.e. whether it is gold standard or predicted) will also influence the results. Rehbein and Hirschmann (2013) report on experiments for parsing German, demonstrating small but significant improvements when introducing more fine-grained and syntactically motivated distinctions in the tagset, based on the Stuttgard-Tübingen Tagset (STTS). However, the In the current paper, we introduce linguistically motivated modifications acro"
W17-0217,J09-3003,0,0.014725,"mpacts the performance of tagging and syntactic dependency parsing. Our results show that parsing accuracy can be significantly improved by introducing more finegrained morphological information in the tagset, even if tagger accuracy is compromised. Our taggers and parsers are trained and tested using the annotations of the Norwegian Dependency Treebank. 1 Introduction Part-of-speech (PoS) tagging is an important preprocessing step for many NLP tasks, such as dependency parsing (Nivre et al., 2007; Hajiˇc et al., 2009), named entity recognition (Sang and Meulder, 2003) and sentiment analysis (Wilson et al., 2009). Whereas much effort has gone into the development of PoS taggers – to the effect that this task is often considered more or less a solved task – considerably less effort has been devoted to the empirical evaluation of the PoS tagsets themselves. Error analysis of PoS taggers indicate that, whereas tagging improvement through means of learning algorithm or feature engineering seems to have reached something of a plateau, linguistic and empirical assessment of the distinctions made in the PoS tagsets may be an avenue worth investigating further (Manning, 2011). Clearly, the utility of a PoS ta"
W17-0217,P11-2033,0,0.0397708,"arning algorithm or feature engineering seems to have reached something of a plateau, linguistic and empirical assessment of the distinctions made in the PoS tagsets may be an avenue worth investigating further (Manning, 2011). Clearly, the utility of a PoS tagset is tightly coupled with the downstream task for which it is performed. Even so, PoS tagsets are usually employed in a “one size fits all” fashion, regardless of the requirements posed by the task making use of this information. It is well known that syntactic parsing often benefits from quite fine-grained morphological distinctions (Zhang and Nivre, 2011; Seeker and 2 Previous Work This section reviews some of the previous work documenting the impact that PoS tagsets has on the performance of taggers and parsers. Megyesi (2002) trained and evaluated a range of PoS taggers on the Stockholm-Umeå Corpus (SUC) (Gustafson-Capková and Hartmann, 2006), 142 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 142–151, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press scope of the changes are limited to modifier distinctions and the new tagset only includes four new PoS tags, changing two of the"
W17-0217,J93-2004,0,\N,Missing
W17-0217,L16-1250,1,\N,Missing
W17-0242,D09-1097,0,0.0583021,"Missing"
W17-0242,N15-1169,0,0.0602173,"words (skip-gram). We used the word2vec implementation provided in the ˇ uˇrek and Sojka, free python library gensim (Reh˚ 2010), using the default parameters to train skipgram models. The defaults are a minimum of 5 occurrences in the corpus for the lemmas and an embedding dimension of size 100. Five iterations over the corpus was made. 5 5.3 There are several ways one could choose to evaluate the quality of the words that are automatically inserted into the hierarchy. For example, Yamada et al. (2009) chose to manually evaluate a random sample of 200 unseen words, while Jurgens and Pilehvar (2015) treat the words already encoded in the hierarchy as gold data and then try to reattach these. We here follow the latter approach. However, while Jurgens and Pilehvar (2015) restrict their evaluation to monosemous words, we also include polysemous words in order to make the evaluation more realistic. For evaluation and tuning we split the wordnet into a development set and a test set, with 1388 target words in each. Potential targets only comprise words that have a hypernym encoded (which, in fact, are not that many, as NWN is relatively flat) and occur in the news corpus sufficiently often (≥"
W17-0242,P06-1101,0,0.0727719,"Missing"
W17-1810,W06-2920,0,0.0186187,"CoNLL-style format. While the shared task also included detection of events and focus, we only focus on cues and scopes in this work. We use the same splits for training, development testing and held-out evaluation as supplied for the shared task. Examples (1)-(2) below show two examples taken from the corpus, where negation cues are in bold and their scopes are underlined. In (1), the cue is the adverb not, whereas (2) provides an example of the affixal cue un. System requirements The input given to the system can either be raw running text or parsed data in the CoNLL-X format (Buchholz and Marsi, 2006). If the user inputs raw text, we need to tokenize, tag and parse the text before we can classify the sentences. Because our training data uses PTB PoS-tags and Stanford dependencies (following conversion), we need a pipeline providing the same standard, and hence use the CoreNLP tool (Manning et al., 2014). Beyond Python 2.7 or newer, the negation tool has the following dependencies: scikit-learn, PyStruct, NumPy, and NetworkX (in addition to CoreNLP unless pre-parsed input is provided). (1) And yet it was not quite the last. (2) Since we have been so unfortunate as to miss him and have no no"
W17-1810,P14-1007,0,0.49475,"ion toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM classifier. The main goal of this work is to arrive at a lean and light-weight system with minimal use of"
W17-1810,P05-1022,0,0.0102334,"can classify the sentences. Because our training data uses PTB PoS-tags and Stanford dependencies (following conversion), we need a pipeline providing the same standard, and hence use the CoreNLP tool (Manning et al., 2014). Beyond Python 2.7 or newer, the negation tool has the following dependencies: scikit-learn, PyStruct, NumPy, and NetworkX (in addition to CoreNLP unless pre-parsed input is provided). (1) And yet it was not quite the last. (2) Since we have been so unfortunate as to miss him and have no notion [. . . ] The Conan Doyle corpus provides phrase structure trees produced by the Charniak and Johnson (2005) parser, and we have used the Stanford Parser (Manning et al., 2014) to convert these to Stanford basic dependency representations (de Marneffe et al., 2014) prior to training. Evaluation We use the evaluation script of the 2012 *SEM shared task (Morante and Blanco, 2012) for measuring precision, recall and F-score. For scopes, it provides two different measures; token-level and scope-level. For the token-level measure the evaluation is defined similarly as for cues, simply checking whether each token in the scope sequence is correctly labeled. For scopes on the other hand, a true positive req"
W17-1810,de-marneffe-etal-2014-universal,0,0.04922,"Missing"
W17-1810,S12-1041,1,0.3603,"at is affected by the negation. Our negation toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM classifier. The main goal of this work is to arrive at a lean"
W17-1810,P16-1047,0,0.775905,"d experimental results are provided for English. 1 Introduction The task of negation detection has recently seen quite a bit of interest in the NLP community, in part spurred by the availability of annotated data and evaluation software introduced by the shared tasks at CoNLL 2010 (Farkas et al., 2010) and *SEM 2012 (Morante and Blanco, 2012). While many research-based systems have been developed, with the aim of exploring features and algorithms to advance the state-of-the-art in terms of performance (Morante and Daelemans, 2009; Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014; Fancellu et al., 2016), many of them are difficult to employ in practice, due to layered architectures and many dependencies, and furthermore, most are simply not made publicly available in the first place. In this paper, we present an open-source portable toolkit for automatic negation detection, with experimental results reported for English. The system is implemented in Python on top of PyStruct (M¨uller and Behnke, 2014), a library for structured prediction based on a maximum-margin approach. The system implements two stages of negation analysis, namely cue detection, which detects words that signal negation, s"
W17-1810,J12-2005,1,0.803538,"and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM classifier. The main goal of this work is to arrive at a lean and light-weight system with minimal use of extra heuristics beyond machine learned models. While achieving the highest performance was not our main goal, the results are competitive with previously reported SoA results in the literature. Moreover, the system can be employed with both raw and parsed input data. This paper presents an open-source toolkit for negation detection. It identifies negation cues and their corresponding scope in either raw or parsed text using maximummargin classification. The system des"
W17-1810,S12-1042,1,0.94257,"d scope resolution, which identifies the span of the sentence that is affected by the negation. Our negation toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM"
W17-1810,W08-0606,0,0.621224,"ccurring in the data set that are not covered by the heuristics currently implemented in the system. Beyond the multi-word cue heuristics, our implementation is abstract in the sense that it is not hard-coded for negation, instead relying on models to be learned automatically from any data using a CoNLL style format similar to that of the *SEM shared task. Importantly, this means that the tool could be trained for other similar tasks, such as speculation detection, as long as cues and scopes are marked. One interesting direction here would be to convert the annotations of the BioScope corpus (Vincze et al., 2008) to the format used by the Conan Doyle corpus. This would allow training of both speculation and negation detection models for biomedical data, and also to test cross-domain effects. Such a conversion is not entirely trivial, however, as the resources differ not merely in terms of format but also the underlying annotation rules. Developing such a mapping could greatly benefit this research field, also making it possible to use data from different domains. 7 Conclusion This paper has presented an open-source tool for detecting negation cues and their in-sentence scopes. Despite the substantial"
W17-1810,S12-1044,0,0.557811,"ich identifies the span of the sentence that is affected by the negation. Our negation toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012), using a binary SVM classifier. Th"
W17-1810,P14-5010,0,0.00390781,"m the corpus, where negation cues are in bold and their scopes are underlined. In (1), the cue is the adverb not, whereas (2) provides an example of the affixal cue un. System requirements The input given to the system can either be raw running text or parsed data in the CoNLL-X format (Buchholz and Marsi, 2006). If the user inputs raw text, we need to tokenize, tag and parse the text before we can classify the sentences. Because our training data uses PTB PoS-tags and Stanford dependencies (following conversion), we need a pipeline providing the same standard, and hence use the CoreNLP tool (Manning et al., 2014). Beyond Python 2.7 or newer, the negation tool has the following dependencies: scikit-learn, PyStruct, NumPy, and NetworkX (in addition to CoreNLP unless pre-parsed input is provided). (1) And yet it was not quite the last. (2) Since we have been so unfortunate as to miss him and have no notion [. . . ] The Conan Doyle corpus provides phrase structure trees produced by the Charniak and Johnson (2005) parser, and we have used the Stanford Parser (Manning et al., 2014) to convert these to Stanford basic dependency representations (de Marneffe et al., 2014) prior to training. Evaluation We use t"
W17-1810,S12-1035,0,0.866366,"rresponding scope in either raw or parsed text using maximummargin classification. The system design draws on best practice from the existing literature on negation detection, aiming for a simple and portable system that still achieves competitive performance. Pretrained models and experimental results are provided for English. 1 Introduction The task of negation detection has recently seen quite a bit of interest in the NLP community, in part spurred by the availability of annotated data and evaluation software introduced by the shared tasks at CoNLL 2010 (Farkas et al., 2010) and *SEM 2012 (Morante and Blanco, 2012). While many research-based systems have been developed, with the aim of exploring features and algorithms to advance the state-of-the-art in terms of performance (Morante and Daelemans, 2009; Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014; Fancellu et al., 2016), many of them are difficult to employ in practice, due to layered architectures and many dependencies, and furthermore, most are simply not made publicly available in the first place. In this paper, we present an open-source portable toolkit for automatic negation detection, with experimental results reported for Englis"
W17-1810,W09-1105,0,0.485159,"Abstract and unfortunate, and scope resolution, which identifies the span of the sentence that is affected by the negation. Our negation toolkit builds on existing libraries that are actively maintained and easy to install, and the source1 is made freely available (GPL). While we make pre-trained classifiers available (for English), users will also be able to train their own models. The system design is based on best practices from previous work, in particular systems from the 2012 *SEM shared task. In particular, we adopt the practice of solving scope resolution as a sequence labeling task (Morante and Daelemans, 2009; Lapponi et al., 2012; White, 2012) based on syntactic features (Read et al., 2012; Lapponi et al., 2012; Packard et al., 2014). In contrast to many of the previous systems that have used constituency-based representations (Read et al., 2012; Packard et al., 2014), we base our syntactic features on dependency representations, similar to the approach of Lapponi et al. (2012). For cue detection, on the other hand, simply using surfaceoriented lexical features have been shown to be sufficient, and we here largely build on the specific approach described by Read et al. (2012; Velldal et al. (2012"
W17-1810,morante-daelemans-2012-conandoyle,0,0.300401,"2017. 2017 Association for Computational Linguistics using a maximum-margin approach. Cue detection is solved using a binary Support Vector Machine (SVM) classifier (Vapnik, 1995). As is fairly common, scope resolution is solved as a sequence labeling task, applying a discriminative linear-chain Conditional Random Fields (CRF) model.However, in a conventional CRF, the parameters are learned through maximum likelihood estimation. In PyStruct on the other hand, the parameters are estimated through maximum-margin learning based on SVMs, resulting in what may be called a maximum-margin CRF. pus (Morante and Daelemans, 2012) as used in the 2012 *SEM shared task (Morante and Blanco, 2012), based on a CoNLL-style format. While the shared task also included detection of events and focus, we only focus on cues and scopes in this work. We use the same splits for training, development testing and held-out evaluation as supplied for the shared task. Examples (1)-(2) below show two examples taken from the corpus, where negation cues are in bold and their scopes are underlined. In (1), the cue is the adverb not, whereas (2) provides an example of the affixal cue un. System requirements The input given to the system can ei"
W17-2705,D16-1229,0,0.494164,"time, we present a new task complete with an evaluation set and introduce the ‘anchor words’ method which outperforms previous approaches on this data. 1 Lilja Øvrelid Department of Informatics University of Oslo liljao@ifi.uio.no 1. Nothing has changed in the country conflict state year-to-year (class ‘stable’); Introduction Several recent studies have investigated how distributional word embeddings can be used for modeling language change, and particularly lexical semantic shifts. This includes tracing perspective change through time, usually for periods equal to centuries or decades; see (Hamilton et al., 2016b) among others. One of the main problems in these studies is the lack of proper ground truth resources describing the degree and direction of semantic change for particular words. Unfortunately, there is no such manually compiled compendium of all the semantic shifts that English words underwent in the last two centuries. The problem is even more severe for studies using more fine-grained time units spanning days or years, rather than decades, like in (Kulkarni et al., 2015) or (Kutuzov and Kuzmenko, 2016): When trying to uncover subtle changes of perspective (for example, ‘Trump’ 2. Armed co"
W17-2705,P16-1141,0,0.394823,"time, we present a new task complete with an evaluation set and introduce the ‘anchor words’ method which outperforms previous approaches on this data. 1 Lilja Øvrelid Department of Informatics University of Oslo liljao@ifi.uio.no 1. Nothing has changed in the country conflict state year-to-year (class ‘stable’); Introduction Several recent studies have investigated how distributional word embeddings can be used for modeling language change, and particularly lexical semantic shifts. This includes tracing perspective change through time, usually for periods equal to centuries or decades; see (Hamilton et al., 2016b) among others. One of the main problems in these studies is the lack of proper ground truth resources describing the degree and direction of semantic change for particular words. Unfortunately, there is no such manually compiled compendium of all the semantic shifts that English words underwent in the last two centuries. The problem is even more severe for studies using more fine-grained time units spanning days or years, rather than decades, like in (Kulkarni et al., 2015) or (Kutuzov and Kuzmenko, 2016): When trying to uncover subtle changes of perspective (for example, ‘Trump’ 2. Armed co"
W17-2705,D14-1162,0,0.11359,"Missing"
W18-2907,K17-1041,0,0.0240345,"ents of increasing complexity are combined in a pipeline architecture are being challenged by endto-end architectures that are trained on distributed word representations to directly produce different types of analyses traditionally assigned to downstream tasks. Syntactic parsing has been viewed as a crucial component for many tasks aimed at extracting various aspects of meaning from text, but recent work challenges many of these assumptions. For the task of semantic role labeling for instance, systems that make little or no use of syntactic information, have achieved state-of-theart results (Marcheggiani et al., 2017). For tasks where syntactic information is still viewed as useful, a variety of new methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Socher et al., 2013; Ebrahimi and Dou, 2015) , tree-structured attention mechanisms (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input representations, such as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often"
W18-2907,D12-1133,0,0.0199118,"ddings over dependency paths. 47 Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP, pages 47–53 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2 Dependency representations forward to use and re-train with the different dependency representations. We also compare to another widely used parser, namely the pre-trained parsing model for English included in the Stanford CoreNLP toolkit (Manning et al., 2014), which outputs Universal Dependencies only. However, it was clearly outperformed by our version of the Bohnet and Nivre (2012) parser in the initial development experiments. Figure 1 illustrates the three different dependency representations we compare: the socalled CoNLL-style dependencies (Johansson and Nugues, 2007) which were used for the 2007, 2008, and 2009 shared tasks of the Conference on Natural Language Learning (CoNLL), the Stanford ‘basic’ dependencies (SB) (Marneffe et al., 2006) and the Universal Dependencies (v1.3) (UD; McDonald et al., 2013; Marneffe et al., 2014; Nivre et al., 2016). We see that the analyses differ both in terms of their choices of heads vs. dependents and the inventory of dependency"
W18-2907,J93-2004,0,0.0609539,"content words as heads (e.g., the main verb treated). UD takes the tendency to select contentful heads one step further, analyzing the prepositional complement functions as a head, with the preposition as itself as a dependent case marker. This is in contrast to the CoNLL and Stanford scheme, where the preposition is head. For syntactic parsing we employ the parser described in Bohnet and Nivre (2012), a transitionbased parser which performs joint PoS-tagging and parsing. We train the parser on the standard training sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The constituency-based treebank is converted to dependencies using two different conversion tools: (i) the pennconverter software1 (Johansson and Nugues, 2007), which produces the CoNLL dependencies2 , and (ii) the Stanford parser using either the option to produce basic dependencies 3 or its default option which is Universal Dependencies v1.34 . The parser achieves a labeled accuracy score of 91.23 when trained on the CoNLL08 representation, 91.31 for the Stanford basic model and 90.81 for the UD representation, when evaluated against the standard evaluation set (section 23) of the WSJ. We"
W18-2907,de-marneffe-etal-2006-generating,0,0.056551,"Missing"
W18-2907,H05-1091,0,0.142599,"l variations. dependencies. We find that the effect of syntactic structure varies between the different relation types. However, the sdp information has a clear positive impact on all the relation types (Table 1). It can be attributed to the fact that the contextbased representations suffer from irrelevant subsequences or clauses when target entities occur far from each other or there are other target entities in the same sentence. The sdp between two entities in the dependency graph captures a condensed representation of the information required to assert a relationship between two entities (Bunescu and Mooney, 2005). best F1 (in 5-fold) Relation USAGE MODEL-FEATURE PART WHOLE TOPIC RESULT COMPARE macro-averaged without sdp with sdp Diff. 60.34 48.89 29.51 45.80 54.35 20.00 50.10 80.24 70.00 70.27 91.26 81.58 61.82 76.10 + 19.90 + 21.11 +40.76 +45.46 +27.23 + 41.82 +26.00 Table 1: Effect of using the shortest dependency path on each relation type. 4 Experiments 4.2 We run all the experiments with a multi-channel setting5 in which the first channel is initialized with pre-trained embeddings 6 in static mode (i.e. it is not updated during training) and the second one is initialized randomly and is fine-tune"
W18-2907,de-marneffe-etal-2014-universal,0,0.0401084,"Missing"
W18-2907,N15-1133,0,0.0198562,"o downstream tasks. Syntactic parsing has been viewed as a crucial component for many tasks aimed at extracting various aspects of meaning from text, but recent work challenges many of these assumptions. For the task of semantic role labeling for instance, systems that make little or no use of syntactic information, have achieved state-of-theart results (Marcheggiani et al., 2017). For tasks where syntactic information is still viewed as useful, a variety of new methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Socher et al., 2013; Ebrahimi and Dou, 2015) , tree-structured attention mechanisms (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input representations, such as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often motivated by their usefulness in (c) UD: Universal Dependencies Figure 1: Dependency representations for the example sentence. downstream application. There is currently a wide range of different types of dependency representations in use,"
W18-2907,N13-1070,0,0.0145263,"er syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often motivated by their usefulness in (c) UD: Universal Dependencies Figure 1: Dependency representations for the example sentence. downstream application. There is currently a wide range of different types of dependency representations in use, which vary mainly in terms of choices concerning syntactic head status. Some previous studies have examined the effects of dependency representations in various downstream applications (Miyao et al., 2008; Elming et al., 2013). Most recently, the Shared Task on Extrinsic Parser Evaluation (Oepen et al., 2017) was aimed at providing better estimates of the relative utility of different types of dependency representations and syntactic parsers for downstream applications. The downstream systems in this previous work have, however, been limited to traditional (non-neural) systems and there is still a need for a better understanding of the contribution of syntactic information in neural downstream systems. In this paper, we examine the use of syntactic representations in a neural approach to the task of relation classi"
W18-2907,S18-1111,0,0.066942,"Missing"
W18-2907,P08-1006,0,0.0259582,"uch as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often motivated by their usefulness in (c) UD: Universal Dependencies Figure 1: Dependency representations for the example sentence. downstream application. There is currently a wide range of different types of dependency representations in use, which vary mainly in terms of choices concerning syntactic head status. Some previous studies have examined the effects of dependency representations in various downstream applications (Miyao et al., 2008; Elming et al., 2013). Most recently, the Shared Task on Extrinsic Parser Evaluation (Oepen et al., 2017) was aimed at providing better estimates of the relative utility of different types of dependency representations and syntactic parsers for downstream applications. The downstream systems in this previous work have, however, been limited to traditional (non-neural) systems and there is still a need for a better understanding of the contribution of syntactic information in neural downstream systems. In this paper, we examine the use of syntactic representations in a neural approach to the t"
W18-2907,L16-1262,0,0.0271321,"Missing"
W18-2907,W07-2416,0,0.098001,"Association for Computational Linguistics 2 Dependency representations forward to use and re-train with the different dependency representations. We also compare to another widely used parser, namely the pre-trained parsing model for English included in the Stanford CoreNLP toolkit (Manning et al., 2014), which outputs Universal Dependencies only. However, it was clearly outperformed by our version of the Bohnet and Nivre (2012) parser in the initial development experiments. Figure 1 illustrates the three different dependency representations we compare: the socalled CoNLL-style dependencies (Johansson and Nugues, 2007) which were used for the 2007, 2008, and 2009 shared tasks of the Conference on Natural Language Learning (CoNLL), the Stanford ‘basic’ dependencies (SB) (Marneffe et al., 2006) and the Universal Dependencies (v1.3) (UD; McDonald et al., 2013; Marneffe et al., 2014; Nivre et al., 2016). We see that the analyses differ both in terms of their choices of heads vs. dependents and the inventory of dependency types. Where CoNLL analyses tend to view functional words as heads (e.g., the auxiliary verb are), the Stanford scheme capitalizes more on content words as heads (e.g., the main verb treated)."
W18-2907,D14-1181,0,0.00880688,") Number of feature maps for each filter region size: ∈ {10 : 1000} III) Activation function: ∈ {Sigmoid, ReLU, T anh, Sof tplus, Iden}. IV) Pooling strategy: ∈ {max, avg}. V) L2 regularEffect of syntactic information To evaluate the effects of syntactic information in general for the relation classification task, we compare the performance of the model with and without the dependency paths. In the syntaxagnostic setup, a sentence that contains the participant entities is used as input for the CNN. We keep the value of hyper-parameters equal to the ones that are reported in the original work (Kim, 2014). To provide the sdp for the syntax-aware version we compare to, we use our parser with Stanford 5 Initial experiments show that the multi-channel model works better than the single channel model 6 We train 300-d domain-specific embeddings on the ACL Anthology corpus using the available word2vec implementation gensim for training. 7 50 Default values are {3-4-5, 128, ReLU, max, 3, 1e-3, 0.5} Sentence class: PART WHOLE This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers. punctuation ← obj ← add → adv → in → pmod → transcribing →"
W18-2907,S18-1128,1,0.770982,"led accuracy score of 91.23 when trained on the CoNLL08 representation, 91.31 for the Stanford basic model and 90.81 for the UD representation, when evaluated against the standard evaluation set (section 23) of the WSJ. We acknowledge that these results are not state-of-the-art parse results for English, however, the parser is straight3 Relation extraction system We evaluate the relative utility of different types of dependency representations on the task of semantic relation extraction and classification in scientific papers, SemEval Task 7 (G´abor et al., 2018). We make use of the system of Nooralahzadeh et al. (2018): a CNN classifier with dependency paths as input, which ranked 3rd (out of 28) participants in the overall evaluation of the shared task. Here, the shortest dependency path (sdp) connecting two target entities for each relation instance is provided by the parser and is embedded in the first layer of a CNN. We extend on their system by (i) implementing a syntax-agnostic approach, (ii) implementing hyper-parameter tuning for each dependency representation, and (iii) adding Universal Dependencies as input representation. We thus train classifiers with sdps extracted from the different dependency"
W18-2907,E17-2093,0,0.016595,"al component for many tasks aimed at extracting various aspects of meaning from text, but recent work challenges many of these assumptions. For the task of semantic role labeling for instance, systems that make little or no use of syntactic information, have achieved state-of-theart results (Marcheggiani et al., 2017). For tasks where syntactic information is still viewed as useful, a variety of new methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Socher et al., 2013; Ebrahimi and Dou, 2015) , tree-structured attention mechanisms (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input representations, such as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often motivated by their usefulness in (c) UD: Universal Dependencies Figure 1: Dependency representations for the example sentence. downstream application. There is currently a wide range of different types of dependency representations in use, which vary mainly in terms of choices concerning syntactic head status"
W18-2907,C12-1147,0,0.0589868,"Missing"
W18-2907,D13-1170,0,0.00641456,"ditionally assigned to downstream tasks. Syntactic parsing has been viewed as a crucial component for many tasks aimed at extracting various aspects of meaning from text, but recent work challenges many of these assumptions. For the task of semantic role labeling for instance, systems that make little or no use of syntactic information, have achieved state-of-theart results (Marcheggiani et al., 2017). For tasks where syntactic information is still viewed as useful, a variety of new methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Socher et al., 2013; Ebrahimi and Dou, 2015) , tree-structured attention mechanisms (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input representations, such as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often motivated by their usefulness in (c) UD: Universal Dependencies Figure 1: Dependency representations for the example sentence. downstream application. There is currently a wide range of different types of dependenc"
W18-2907,P17-1065,0,0.0262833,"us aspects of meaning from text, but recent work challenges many of these assumptions. For the task of semantic role labeling for instance, systems that make little or no use of syntactic information, have achieved state-of-theart results (Marcheggiani et al., 2017). For tasks where syntactic information is still viewed as useful, a variety of new methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Socher et al., 2013; Ebrahimi and Dou, 2015) , tree-structured attention mechanisms (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input representations, such as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often motivated by their usefulness in (c) UD: Universal Dependencies Figure 1: Dependency representations for the example sentence. downstream application. There is currently a wide range of different types of dependency representations in use, which vary mainly in terms of choices concerning syntactic head status. Some previous studies have examined t"
W18-2907,D15-1062,0,0.0598581,"Missing"
W18-2907,D15-1206,0,0.154916,"ems that make little or no use of syntactic information, have achieved state-of-theart results (Marcheggiani et al., 2017). For tasks where syntactic information is still viewed as useful, a variety of new methods for the incorporation of syntactic information are employed, such as recursive models over parse trees (Socher et al., 2013; Ebrahimi and Dou, 2015) , tree-structured attention mechanisms (Kokkinos and Potamianos, 2017), multi-task learning (Wu et al., 2017), or the use of various types of syntactically aware input representations, such as embeddings over syntactic dependency paths (Xu et al., 2015b). Dependency representations have by now become widely used representations for syntactic analysis, often motivated by their usefulness in (c) UD: Universal Dependencies Figure 1: Dependency representations for the example sentence. downstream application. There is currently a wide range of different types of dependency representations in use, which vary mainly in terms of choices concerning syntactic head status. Some previous studies have examined the effects of dependency representations in various downstream applications (Miyao et al., 2008; Elming et al., 2013). Most recently, the Share"
W18-2907,P14-5010,0,\N,Missing
W18-5519,L16-1376,0,0.0146488,"t al. (2018) propose a decomposed graph entailment model that uses structure from the claim to calculate entailment probabilities for each node and edge in the graph structure and aggregates them for the final entailment computation. The original DGE model 6 uses Open IE (Khot et al., 2017) tuples as a graph representation for the claim. However, it is mentioned that the model can use any graph with labeled edges. Therefore, we provide a syntactic dependency parse tree using the Stanford dependency parser (Manning et al., 2014) which outputs the Enhanced Universal Dependencies representation (Schuster and Manning, 2016) as a graph representation for the claim. Decomposed Graph Entailment (DGE) model: periment with the TalosTree model Recall 21.14 19.50 18.24 15.19 Table 1: Evidence extraction results on developWe used the publicly available DA model 5 which is trained on the FEVER shared task dataset. We asked the model to predict an inference label for each claim based on the evidence set which is provided by the sentence selection component. 7 Frequency FEVER Baseline Decomposable Attention (DA) model (Parikh et al., 2016): Gradient-Boosted Decision Trees model: Inverse Precision 3.2 Evaluation In this sec"
W18-5519,S17-2051,0,0.0459978,"Missing"
W18-5519,P05-1045,0,0.0234165,"://dbpedia.org/resource/Template:"") && !regex(str(?resource), ""http://dbpedia.org/resource/List"") && !regex(str(?resource), ""(disambiguation)"") ) } System description In this section, we describe our system which consists of three components which solve the three following tasks: Wikipedia page retrieval, sentence selection and textual entailment. 2.1 Wiki-page Retrieval Each claim in the FEVER dataset contains a single piece of information about an entity that its original Wikipedia page describes. Therefore we first extract entities using the Stanford Named Entity Recognition (StanfordNER) (Finkel et al., 2005). We observe that StanfordNER is sometimes unable to extract entity names in the claim due to limited contextual information like in example 1 below: Example 1 A View to a Kill is an action movie. NER:[] Noun-Phrases:[A View to a Kill, an action Listing 1: SPARQL query to extract Wikipedia page candidates for entity mention (e.g. Meteora) 2.2 Sentence Selection Given a set of Wikipedia page candidates, the similarity between the claim and the individual text lines on the page is obtained. We here experiment with several methods for computing this similarity: movie] Sentences are ranked by unig"
W18-5519,P17-2049,0,0.0293689,"to 145,459 , 19,998 and 19,998 train, development and test instances, respectively. The claims are generated from information extracted from Wikipedia. The Wikipedia dump (version June 2017) was processed with Stanford CoreNLP, and the claims sampled from the introductory sections of approximately 50,000 popular pages. Khot et al. (2018) propose a decomposed graph entailment model that uses structure from the claim to calculate entailment probabilities for each node and edge in the graph structure and aggregates them for the final entailment computation. The original DGE model 6 uses Open IE (Khot et al., 2017) tuples as a graph representation for the claim. However, it is mentioned that the model can use any graph with labeled edges. Therefore, we provide a syntactic dependency parse tree using the Stanford dependency parser (Manning et al., 2014) which outputs the Enhanced Universal Dependencies representation (Schuster and Manning, 2016) as a graph representation for the claim. Decomposed Graph Entailment (DGE) model: periment with the TalosTree model Recall 21.14 19.50 18.24 15.19 Table 1: Evidence extraction results on developWe used the publicly available DA model 5 which is trained on the FEV"
W18-5519,N18-1074,0,0.103549,"nswering, knowledge extraction and reasoning. Fact extraction from unstructured text is a task central to knowledge base construction. While this process is vital for many NLP applications, misinformation (false information) or disinformation (deliberately false information) from unreliable sources, can provide false output and mislead the readers. Such risks could be properly managed by applying NLP techniques aimed at solving the task of fact verification, i.e., to detect and discriminate misinformation and prevent its propagation. The Fact Extraction and VERification (FEVER) shared task 4 (Thorne et al., 2018) addresses both problems. In this work, we introduce a pipeline system for each phase of the FEVER shared task. In our pipeline, we first identify entities in a given claim, then we extract candidate Wikipedia pages for each of the entities and the most similar senThis article presents the SIRIUS-LTG system for the Fact Extraction and VERification (FEVER) Shared Task. It consists of three components: 1) Wikipedia Page Retrieval: First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over DBpedia 2) Sentence sele"
W18-5519,P14-5010,0,0.0170104,"to extract Wikipedia page candidates for entity mention (e.g. Meteora) 2.2 Sentence Selection Given a set of Wikipedia page candidates, the similarity between the claim and the individual text lines on the page is obtained. We here experiment with several methods for computing this similarity: movie] Sentences are ranked by unigram TF-IDF similarity to the claim. We modified the fever-baseline code to consider the candidate list from the Wiki-page retrieval components. Cosine Similarity using TFIDF: To tackle this problem, we also extract noun phrases using the parse tree of Stanford CoreNLP (Manning et al., 2014) and the longest multi-word expression that contains words with the first letter in upper case. This enables us to provide a wide range of potential entities for the retrieval process. We then retrieve a set of Wikipedia page candidates for an entity in the claim using a SPARQL (Prud’hommeaux and Seaborne, 2008) query over DBpedia, i.e. the structured version of Wikipedia. The SPARQL query aids the retrieval process by providing a list of candidates to the subsequent system components, particularly when the claim is about film, song, music album, bands and etc. Listing 1 shows the query employ"
W18-5519,D16-1244,0,0.0997984,"Missing"
W18-5613,L16-1250,1,0.848852,"ed entity tags to predict the current tag. We also experimented with lowercasing a word and orthographic features such as prefixes and suffixes of length 3 which did not improve the performance of the SVM model. We evaluate the performance of the SVM model using weighted F1 score to account for class imbalance. On an average, these feature templates yielded 5000 features across the five cross-validation experiments. All the Universal POS tags are obtained through the CoNLL17 Baseline model (Zeman et al., 2017) trained on the publicly available Universal Dependencies Norwegian Bokmål treebank (Øvrelid and Hohle, 2016). We used the majority class “O” as the baseline in our experiments. The results of our experiments are given in table 4. It has to be noted that these results are not comparable to the IAA scores presented in table 3, which are calculated only over entities and completely disregard the remaining tokens. Moreover, the IAA 117 System Precision Recall F1 -score Baseline SVM 0.34 0.843 0.582 0.843 0.429 0.841 the quality of entity labels does improve the performance of the model. Finally, we present the confusion matrix for the best fold is presented in table 6. The SVM model makes most of the er"
W18-5613,W08-0602,0,0.230463,"ble resource of Norwegian clinical text. 1 Introduction The limited availability of clinical text corpora constitutes a major challenge for the development of clinical NLP tools. Such text originates in the (electronic) health record (EHR), and access to and use of the EHR is governed by strict data privacy and health service regulations, which usually restricts secondary use and prohibits re-distribution and sharing with the larger NLP community. Among notable exceptions are anonymized health record texts published as part of the i2b2 challenges (Uzuner and Stubbs, 2015) and the CLEF corpus (Roberts et al., 2008b). For languages other than English the situation is even more difficult, and despite notable annotation efforts (Dalianis et al., 2012), the underlying corpora are largely unavailable. Clinical texts are radically different in form and function from other biomedical texts: They are communicative, conveying information between 111 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 111–121 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ily history domain. We analyse inter-annotator agreement ba"
W18-5613,I05-1033,0,0.0110152,"s and do not perform relation extraction as performed in this paper. Both rule based systems (Abacha and Zweigenbaum, 2011) and machine learning methods such as Roberts et al. (2008a) and Minard et al. (2011) use multi-class SVMs to perform relation extraction from clinical reports. Our work in this paper is closest to the work of Roberts et al. (2008a) who manually annotated cancer narratives for entities and relations and, then, trained and tested a one-vs-rest SVM classifier for training and testing. In this paper, we employ widely used features in general purpose named entity recognition (Hong, 2005; Miwa and Sasaki, 2014) to train the SVM models. 3 Start Annotation Guideline Add to Guideline Add phrases Corpus Clinician annotation Independent annotation Annotated corpus Annotated corpus Alignment Corpus revision Incremental annotation guideline and synthetic corpus development Guideline revision One immediate goal of this work is to develop a tool for the extraction of family history information from Norwegian clinical text. Due to the unavailability of the real health records describing family histories, we developed a methodology for annotation guideline development which makes use of"
W18-5613,L18-1201,0,0.0627453,"Missing"
W18-5613,E12-2021,0,0.184613,"Missing"
W18-5613,R11-1086,0,0.0155363,"ine of rule based systems to detect family members and diagnosis concepts; and, then assign the family diagnosis to a specific family number. The authors run standard NLP tools such as sentence splitter and part-of-speech taggers on 112 discharge summary notes. The pipeline system is related to Friedlin and McDonald (2006) in only identifying diagnosis concepts that are present in standard medical dictionaries and do not perform relation extraction as performed in this paper. Both rule based systems (Abacha and Zweigenbaum, 2011) and machine learning methods such as Roberts et al. (2008a) and Minard et al. (2011) use multi-class SVMs to perform relation extraction from clinical reports. Our work in this paper is closest to the work of Roberts et al. (2008a) who manually annotated cancer narratives for entities and relations and, then, trained and tested a one-vs-rest SVM classifier for training and testing. In this paper, we employ widely used features in general purpose named entity recognition (Hong, 2005; Miwa and Sasaki, 2014) to train the SVM models. 3 Start Annotation Guideline Add to Guideline Add phrases Corpus Clinician annotation Independent annotation Annotated corpus Annotated corpus Align"
W18-6003,D17-1137,0,0.298496,"Missing"
W18-6003,L16-1262,1,0.89472,"Missing"
W18-6003,W05-0406,0,0.430106,"D treebanks, and present recommendations for the annotation of expletives so that more consistent annotation can be achieved in future releases. 1 Introduction Universal Dependencies (UD) is a framework for morphosyntactic annotation that aims to provide useful information for downstream NLP applications in a cross-linguistically consistent fashion (Nivre, 2015; Nivre et al., 2016). Many such applications require an analysis of referring expressions. In co-reference resolution, for example, it is important to be able to separate anaphoric uses of pronouns such as it from non-referential uses (Boyd et al., 2005; Evans, 2001; Uryupina et al., 2016). Accurate translation of pronouns is another challenging problem, sometimes relying on coreference resolution, and where one of the choices is to not translate a pronoun at all. The latter situation occurs for instance when translating from a 2 What is an Expletive? The UD initiative aims to provide a syntactic annotation scheme that can be applied cross18 Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 18–26 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics pronoun, and there, identical to"
W18-6003,W17-1505,0,0.0151815,"Gosse Bouma∗◦ Jan Hajic†◦ Dag Haug‡◦ Joakim Nivre•◦ Per Erik Solberg‡◦ Lilja Øvrelid?◦ ∗ University of Groningen, Centre for Language and Cognition Charles University in Prague, Faculty of Mathematics and Physics, UFAL ‡ University of Oslo, Department of Philosophy, Classics, History of Arts and Ideas • Uppsala University, Department of Linguistics and Philology ? University of Oslo, Department of Informatics ◦ Center for Advanced Study at the Norwegian Academy of Science and Letters † Abstract language that has expletives into a language that does not use expletives (Hardmeier et al., 2015; Werlen and Popescu-Belis, 2017). The ParCor co-reference corpus (Guillou et al., 2014) distinguishes between anaphoric, event referential, and pleonastic use of the English pronoun it. Lo´aiciga et al. (2017) train a classifier to predict the different uses of it in English using among others syntactic information obtained from an automatic parse of the corpus. Being able to distinguish referential from non-referential noun phrases is potentially important also for tasks like question answering and information extraction. Applications like these motivate consistent and explicit annotation of expletive elements in treebanks"
W18-6003,guillou-etal-2014-parcor,0,0.0313846,"berg‡◦ Lilja Øvrelid?◦ ∗ University of Groningen, Centre for Language and Cognition Charles University in Prague, Faculty of Mathematics and Physics, UFAL ‡ University of Oslo, Department of Philosophy, Classics, History of Arts and Ideas • Uppsala University, Department of Linguistics and Philology ? University of Oslo, Department of Informatics ◦ Center for Advanced Study at the Norwegian Academy of Science and Letters † Abstract language that has expletives into a language that does not use expletives (Hardmeier et al., 2015; Werlen and Popescu-Belis, 2017). The ParCor co-reference corpus (Guillou et al., 2014) distinguishes between anaphoric, event referential, and pleonastic use of the English pronoun it. Lo´aiciga et al. (2017) train a classifier to predict the different uses of it in English using among others syntactic information obtained from an automatic parse of the corpus. Being able to distinguish referential from non-referential noun phrases is potentially important also for tasks like question answering and information extraction. Applications like these motivate consistent and explicit annotation of expletive elements in treebanks and the UD annotation scheme introduces a dedicated dep"
W18-6003,W15-2501,0,0.0255507,"sal Dependency Treebanks Gosse Bouma∗◦ Jan Hajic†◦ Dag Haug‡◦ Joakim Nivre•◦ Per Erik Solberg‡◦ Lilja Øvrelid?◦ ∗ University of Groningen, Centre for Language and Cognition Charles University in Prague, Faculty of Mathematics and Physics, UFAL ‡ University of Oslo, Department of Philosophy, Classics, History of Arts and Ideas • Uppsala University, Department of Linguistics and Philology ? University of Oslo, Department of Informatics ◦ Center for Advanced Study at the Norwegian Academy of Science and Letters † Abstract language that has expletives into a language that does not use expletives (Hardmeier et al., 2015; Werlen and Popescu-Belis, 2017). The ParCor co-reference corpus (Guillou et al., 2014) distinguishes between anaphoric, event referential, and pleonastic use of the English pronoun it. Lo´aiciga et al. (2017) train a classifier to predict the different uses of it in English using among others syntactic information obtained from an automatic parse of the corpus. Being able to distinguish referential from non-referential noun phrases is potentially important also for tasks like question answering and information extraction. Applications like these motivate consistent and explicit annotation of"
W19-4318,L18-1320,1,0.899937,"Missing"
W19-4318,N16-1000,0,0.447174,"thus do not have much significant annotation burden. Most of these methods are, however, structured: they rely on the sentences in training data being ordered and not randomly sampled. The aptly named SkipThoughts (Kiros et al., 2015) is a well-known earlier work, and uses recurrent encoder-decoder models to ‘decode’ sentences surrounding the encoded sentence, using the final encoder state as the encoded sentence’s representation. Cer et al. (2018) evaluate two different encoders, a deep averaging network and a transformer, on unsupervised data drawn from a variety of web sources. Hill et al. (2016) describe a model based on denoising auto-encoders, and a simplified variant of SkipThoughts, that sums up source word embeddings, that they dub (FastSent). Another SkipThoughts variant (Logeswaran and Lee, 2018) uses a multiple-choice objective for contextual sentences, over the more complicated decoder-based objective. Several supervised approaches to building representations also exist. An earlier work is CharaIntroduction In recent years, there has been a considerable amount of research into attempting to represent contexts longer than single words with fixedlength vectors. These represent"
W19-4318,D15-1075,0,0.035921,"gram (Wieting et al., 2016), which uses paraphrase data and builds on character representations to arrive at sentence representations. More recent papers use a diverse variety of target tasks to ground representations, such as visual data (Kiela et al., 2017), machine translation data (McCann et al., 2017), and even multiple tasks, in a multi-task learning framework (Subramanian et al., 2018). Relevant to this paper is Conneau et al.’s (2017a) InferSent, that uses natural language inference (NLI) data to ground representations: they learn these representations on the well-known SNLI dataset (Bowman et al., 2015). 2.2 2.3 On evaluation Work on evaluating sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Conneau et al. (2018a) also created a set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 201"
W19-4318,D18-2029,0,0.0219873,"task. 1 2 2.1 Background Sentence representation learning Numerous methods for learning sentence representations exist. Many of these methods are unsupervised, and thus do not have much significant annotation burden. Most of these methods are, however, structured: they rely on the sentences in training data being ordered and not randomly sampled. The aptly named SkipThoughts (Kiros et al., 2015) is a well-known earlier work, and uses recurrent encoder-decoder models to ‘decode’ sentences surrounding the encoded sentence, using the final encoder state as the encoded sentence’s representation. Cer et al. (2018) evaluate two different encoders, a deep averaging network and a transformer, on unsupervised data drawn from a variety of web sources. Hill et al. (2016) describe a model based on denoising auto-encoders, and a simplified variant of SkipThoughts, that sums up source word embeddings, that they dub (FastSent). Another SkipThoughts variant (Logeswaran and Lee, 2018) uses a multiple-choice objective for contextual sentences, over the more complicated decoder-based objective. Several supervised approaches to building representations also exist. An earlier work is CharaIntroduction In recent years,"
W19-4318,J06-4003,0,0.0853372,"r probing architecture downstream, and evaluate classifier performance. First, we load our mapped word representations for the language that we intend to analyse. We use these word representations to build sentence 6 6.1 Data Probing data We build our probing datasets using the relevant language’s Wikipedia dump as a corpus. Specifically, we use Wikipedia dumps (dated 2019-0201), which we process using the WikiExtractor 160 Figure 2: Probing accuracies for our six encoders on Conneau et al.’s dataset (orig), compared to our Wikipediaderived dataset (eng) utility1 . We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. 6.2 Mapping data For mapping our sentence representations, we were restricted by the availability of large parallel corpora we could use for our mapping procedure. We used two such corpora: the Europarl corpus (Koehn, 2005), a multilingual collection of European Parliament proceedings, and the MultiUN corpus (Tiedemann, 2012), a collection of translated documents from the United Nations. We used Europarl"
W19-4318,L18-1269,0,0.0631441,"cent papers use a diverse variety of target tasks to ground representations, such as visual data (Kiela et al., 2017), machine translation data (McCann et al., 2017), and even multiple tasks, in a multi-task learning framework (Subramanian et al., 2018). Relevant to this paper is Conneau et al.’s (2017a) InferSent, that uses natural language inference (NLI) data to ground representations: they learn these representations on the well-known SNLI dataset (Bowman et al., 2015). 2.2 2.3 On evaluation Work on evaluating sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Conneau et al. (2018a) also created a set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. The authors, whose"
W19-4318,2005.mtsummit-papers.11,0,0.0196598,"60 Figure 2: Probing accuracies for our six encoders on Conneau et al.’s dataset (orig), compared to our Wikipediaderived dataset (eng) utility1 . We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. 6.2 Mapping data For mapping our sentence representations, we were restricted by the availability of large parallel corpora we could use for our mapping procedure. We used two such corpora: the Europarl corpus (Koehn, 2005), a multilingual collection of European Parliament proceedings, and the MultiUN corpus (Tiedemann, 2012), a collection of translated documents from the United Nations. We used Europarl for the official EU languages we analysed: German and Spanish. For Russian, we used MultiUN. We used both corpora for French, to attempt to analyse what, if any, effect the mapping corpus would have. We also truncated our MultiUN cororpora to 2 million sentences, to keep the corpus size roughly equivalent to Europarl, and also due to time and resource constraints: mapping representations on the complete 10 milli"
W19-4318,D18-1269,0,0.358793,"uses natural language inference (NLI) data to ground representations: they learn these representations on the well-known SNLI dataset (Bowman et al., 2015). 2.2 2.3 On evaluation Work on evaluating sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Conneau et al. (2018a) also created a set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. The authors, whose work focussed on evaluating representations for English, provided Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this"
W19-4318,N19-1112,0,0.114205,"o created a set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. The authors, whose work focussed on evaluating representations for English, provided Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe three pretrained contextualised word representation models – ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) and the OpenAI transformer (Radford et al., 2018) – with a “suite of sixteen diverse probing tasks”. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated probing corpora. They motivate the use of SVCCA (Raghu et al., 2017) to quantify precisely what an encoder learns by comparing the representations it generates with representations generated by an"
W19-4318,P08-1028,0,0.0763594,"e for contextual sentences, over the more complicated decoder-based objective. Several supervised approaches to building representations also exist. An earlier work is CharaIntroduction In recent years, there has been a considerable amount of research into attempting to represent contexts longer than single words with fixedlength vectors. These representations typically tend to focus on attempting to represent sentences, although phrase- and paragraph-centric mechanisms do exist. These have moved well beyond relatively na¨ıve compositional methods, such as additive and multiplicative methods (Mitchell and Lapata, 2008), one of the earlier papers on the subject. There have been several proposed approaches to learning these representations since, both unsupervised and supervised. Naturally, this has also sparked interest in evaluation methods for sentence representations; the focus of this paper is on probing-centric evaluations, and their extension to a multilingual domain. In Section 2, we provide a literature review of prior work in the numerous domains that our paper builds upon. Section 3 motivates the principle of cross-lingual probing and describes our 156 Proceedings of the 4th Workshop on Representat"
W19-4318,N16-1162,0,0.0228411,"rvised, and thus do not have much significant annotation burden. Most of these methods are, however, structured: they rely on the sentences in training data being ordered and not randomly sampled. The aptly named SkipThoughts (Kiros et al., 2015) is a well-known earlier work, and uses recurrent encoder-decoder models to ‘decode’ sentences surrounding the encoded sentence, using the final encoder state as the encoded sentence’s representation. Cer et al. (2018) evaluate two different encoders, a deep averaging network and a transformer, on unsupervised data drawn from a variety of web sources. Hill et al. (2016) describe a model based on denoising auto-encoders, and a simplified variant of SkipThoughts, that sums up source word embeddings, that they dub (FastSent). Another SkipThoughts variant (Logeswaran and Lee, 2018) uses a multiple-choice objective for contextual sentences, over the more complicated decoder-based objective. Several supervised approaches to building representations also exist. An earlier work is CharaIntroduction In recent years, there has been a considerable amount of research into attempting to represent contexts longer than single words with fixedlength vectors. These represent"
W19-4318,tiedemann-2012-parallel,0,0.0221718,"our Wikipediaderived dataset (eng) utility1 . We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. 6.2 Mapping data For mapping our sentence representations, we were restricted by the availability of large parallel corpora we could use for our mapping procedure. We used two such corpora: the Europarl corpus (Koehn, 2005), a multilingual collection of European Parliament proceedings, and the MultiUN corpus (Tiedemann, 2012), a collection of translated documents from the United Nations. We used Europarl for the official EU languages we analysed: German and Spanish. For Russian, we used MultiUN. We used both corpora for French, to attempt to analyse what, if any, effect the mapping corpus would have. We also truncated our MultiUN cororpora to 2 million sentences, to keep the corpus size roughly equivalent to Europarl, and also due to time and resource constraints: mapping representations on the complete 10 million sentence corpus would have required significant amounts of time. Having segmented our data, we used t"
W19-4318,D16-1157,0,0.0219843,"obing-centric evaluations, and their extension to a multilingual domain. In Section 2, we provide a literature review of prior work in the numerous domains that our paper builds upon. Section 3 motivates the principle of cross-lingual probing and describes our 156 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 156–168 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics bedding transfer: the authors use a parallel corpus as a ‘seed dictionary’ to fit a transformation matrix between their source and target languages. gram (Wieting et al., 2016), which uses paraphrase data and builds on character representations to arrive at sentence representations. More recent papers use a diverse variety of target tasks to ground representations, such as visual data (Kiela et al., 2017), machine translation data (McCann et al., 2017), and even multiple tasks, in a multi-task learning framework (Subramanian et al., 2018). Relevant to this paper is Conneau et al.’s (2017a) InferSent, that uses natural language inference (NLI) data to ground representations: they learn these representations on the well-known SNLI dataset (Bowman et al., 2015). 2.2 2."
W19-4318,N18-1101,0,0.0238777,"e of languages could have been more typologically diverse, we were restricted by three factors: these probing tasks is that most of them were created with the idea of evaluating representations built for English language data. In this spirit, what we propose is analogous to Abdou et al.’s (2018) work on generating multilingual evaluation corpora for word representations. Within the realm of evaluating multilingual sentence representations, Conneau et al. (2018b) describe the XNLI dataset, a set of translations of the development and test portions of the multi-genre MultiNLI inference dataset (Williams et al., 2018). This, in a sense, is an extension of a predominantly monolingual task to the multilingual domain; the authors evaluate sentence representations derived by mapping nonEnglish representations to an English representation space. The original XNLI paper provides a baseline representation mapping technique, based on minimising the mean-squared error (MSE) loss between sentence representations across a parallel corpus. Their English language sentence representations are derived from an encoder trained on NLI data (Conneau et al., 2017a), and their target language representations are randomly initi"
W19-4318,N18-1202,0,0.0531238,"pkes et al., 2017; Belinkov et al., 2017), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. The authors, whose work focussed on evaluating representations for English, provided Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe three pretrained contextualised word representation models – ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) and the OpenAI transformer (Radford et al., 2018) – with a “suite of sixteen diverse probing tasks”. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated probing corpora. They motivate the use of SVCCA (Raghu et al., 2017) to quantify precisely what an encoder learns by comparing the representations it generates with representations generated by an architecture trained specifically for a certain task, with the intuition that a higher similar"
W19-4318,N19-1329,0,0.0335263,"Missing"
W19-4318,K17-3009,0,0.0687333,"Missing"
W19-4318,P07-2045,0,\N,Missing
W19-4318,D16-1250,0,\N,Missing
W19-4318,W17-2619,0,\N,Missing
W19-4318,D17-1070,0,\N,Missing
W19-4318,P18-1198,0,\N,Missing
W19-4318,S19-1006,0,\N,Missing
W19-4409,P16-1068,0,0.0687646,"information and further employ a discriminative ranker that is shown to outperform a regression approach on the FCE corpus. Vajjala (2017) trains linear classifiers over the TOEFL11 corpus of non-native English (Blanchard et al., 2013) and the FCE corpus and makes use of a number of linguistic features for the task, including several different measures for lexical diversity, distribution of POS tags, and syntactic complexity, as well as features capturing discourse properties. Several of these features were based on previous work on measuring syntactic complexity in L2 writing by (Lu, 2010). Alikaniotis et al. (2016) and Taghipour and Ng (2016) both present neural systems trained and evaluated on the ASAP Kaggle dataset of student essays. Both formulate the AES task as a regression task and experiment with several types of neural architectures applied to the same dataset, showing the best results using a bidirectional LSTM with pre-trained embeddings. Whereas much previous work has been focused on English learner language, there has also been some work on AES using the CEFR scale for languages other than English, viz. Hancke (2013) for German, Vajjala and Loo (2014) for Estonian and Pil´an et al. (2016) f"
W19-4409,Q17-1010,0,0.0106618,"ion, SVC is support vector classification, and SVR is support vector regression. 6 CNN CNN+POS CNN Mix CNN Reg CNN Reg+POS CNN Reg Mix CNN Rank CNN Rank+POS CNN Rank Mix In this section, we train and evaluate a wide range of convolutional networks and gated RNNs for the AES task. We further experiment with the use of pre-trained word embeddings which are fine-tuned for the task. The embedding models have been trained on a large Norwegian corpus, the combination of Norsk aviskorpus (The Norwegian Newspaper Corpus) and NoWaC (Norwegian Web As Corpus; Stadsnes, 2018) using the FastText software (Bojanowski et al., 2017) and are available from the NLPL vector repository (Fares et al., 2017).3 Macro F1 Micro F1 0.168 0.146 0.201 0.230 0.236 0.258 0.177 0.187 0.231 0.398 0.374 0.398 0.382 0.341 0.398 0.374 0.382 0.382 0.388 0.398 0.383 0.439 0.383 0.412 0.392 0.397 0.379 0.732 0.748 0.724 0.724 0.724 0.642 0.740 0.748 0.715 Pre-trained, fine tuned embeddings CNN CNN+POS CNN Reg CNN Reg+POS CNN Rank CNN Rank+POS 0.208 0.161 0.242 0.232 0.198 0.181 0.382 0.366 0.341 0.366 0.350 0.325 0.384 0.402 0.463 0.411 0.384 0.401 0.724 0.756 0.724 0.715 0.724 0.756 Table 3: F1 scores of CNN classifiers on AES. +POS: Multi-c"
W19-4409,D15-1049,0,0.027746,"Missing"
W19-4409,W16-4114,0,0.034972,"Missing"
W19-4409,D14-1181,0,0.0131459,"Missing"
W19-4409,J18-3003,0,0.0246478,"We train both CNN, LSTM, and GRU architectures to predict the native language of the writers of the essays. We found that the RNN models performed better than CNN models and among the RNN models, GRU architectures performed better than their LSTM counterparts. Therefore, we report only the results of our best GRU model in table 5. The best model is a GRU model which employs pretrained embeddings and takes mean of the hidden states over the time steps to perform classification using a softmax layer. This model achieves a best accuracy of 0.537 which is lower than the score of 0.542 reported by Malmasi and Dras (2018) on the original essays. Collapsed labels Micro F1 Macro F1 Micro F1 Random init, unidirectional GRU Mean Max Attn +POS Mean +POS Max +POS Attn Mix Mean Mix Max Mix Attn 0.264 0.219 0.434 0.348 0.230 0.434 0.225 0.200 0.302 Mean Max Attn +POS Mean +POS Max +POS Attn Mix Mean Mix Max Mix Attn 0.314 0.160 0.459 0.373 0.175 0.460 0.231 0.200 0.275 0.374 0.325 0.431 0.398 0.374 0.423 0.333 0.398 0.455 0.455 0.487 0.806 0.450 0.500 0.718 0.388 0.398 0.509 0.675 0.683 0.805 0.642 0.748 0.813 0.634 0.756 0.780 0.444 0.460 0.805 0.425 0.503 0.687 0.395 0.405 0.617 0.667 0.691 0.805 0.683 0.748 0.821 0"
W19-4409,K17-3009,0,0.0693493,"Missing"
W19-4409,R15-1053,0,0.0870552,"eld-out test data in section 9. We summarize and conclude the paper in section 10. 1 This work was performed when the first author was a Masters student with the Language Technology Group at University of Oslo. Similarly, the second author took part in the BigMed project https://bigmed.no/ hosted at University of Oslo. 92 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 92–102 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 Related work 2016; Malmasi et al., 2017). Norwegian NLI has been studied by Malmasi et al. (2015), using the ASK corpus. In their methodology, they create artificial documents to train on by segmenting the learner texts into sentences, then putting all the sentences from learners with the same L1 into a bag and sampling sentences from the bag to create the new documents. Their rationale for the methodology is that all the resulting documents are of similar length, and that they eliminate the variation between individual writers that otherwise might present a stronger signal than the writer’s L1 alone. In a later study, Malmasi and Dras (2017) perform an NLI experiment on several corpora,"
W19-4409,D16-1193,0,0.281165,"using a LDA meta-classifier trained with bootstrap aggregation (bagging), they achieve an accuracy of 0.818 on the artificially Norwegian essay corpus. The authors’ methodology also involves generating artificial essays which discard the discourse properties of a text that could help in improving the system performance at NLI task. Therefore, we do not replicate their experiments but chose to test our best models tuned on development split and then report the best model on a separate test split. In this paper, we test several RNN models were implemented based on the architecture described in Taghipour and Ng (2016). We made necessary changes to the architectures in order to accommodate our data. For instance, Taghipour and Ng (2016) modelled the task as a regression problem, where the output layer consists of a single node with a value constrained to (0, 1) by the sigmoid function. This layer was replaced with a softmax layer which is described further in section 6. Yannakoudakis et al. (2011) present the CLC First Certificate of English (FCE) corpus as well as a system that makes use of deep linguistic features, such as PoS-tags and syntactic information and further employ a discriminative ranker that"
W19-4409,W17-5007,0,0.0429291,"the results of the best linear and neural models on the held-out test data in section 9. We summarize and conclude the paper in section 10. 1 This work was performed when the first author was a Masters student with the Language Technology Group at University of Oslo. Similarly, the second author took part in the BigMed project https://bigmed.no/ hosted at University of Oslo. 92 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 92–102 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 Related work 2016; Malmasi et al., 2017). Norwegian NLI has been studied by Malmasi et al. (2015), using the ASK corpus. In their methodology, they create artificial documents to train on by segmenting the learner texts into sentences, then putting all the sentences from learners with the same L1 into a bag and sampling sentences from the bag to create the new documents. Their rationale for the methodology is that all the resulting documents are of similar length, and that they eliminate the variation between individual writers that otherwise might present a stronger signal than the writer’s L1 alone. In a later study, Malmasi and D"
W19-4409,tenfjord-etal-2006-ask,0,0.0803418,"Missing"
W19-4409,L16-1250,1,0.847984,"gs, mistakes and corrections, paragraphs, sentences and more. First, the files were converted to plain text files where all the tags or correction labels were stripped of. The text files have one sentence per line, consisting of space-separated tokens, and an empty line separating paragraphs. These raw text files were then processed using the 94 periment to establish if AES is best modelled as a regression or classification task for the ASK dataset. UDPipe pipeline (Straka and Strakov´a, 2017) for PoS-tagging and dependency parsing, with pretrained models trained on the Norwegian UD treebank (Øvrelid and Hohle, 2016). Two different sets of output labels are used in the experiments: The original seven CEFR labels, and a collapsed set where the intermediate classes, such as ‘A2/B1’, are rounded up to the nearest canonical class, i.e., the CEFR label after the slash. This step yields only four different labels in the collapsed set: ‘A2’, ‘B1’, ‘B2’ and ‘C1’. 4.3 Classification vs. Regression As mentioned earlier, AES can be modelled both as a classification task and as a regression task. A disadvantage with using regression for the AES task is that while we know the correct order of classes, it is not obviou"
W19-4409,W13-1706,0,0.028465,"gual, cross-lingual and multi-lingual CEFR classification using the MERLIN corpus (Boyd et al., 2014). The ASK corpus (Tenfjord et al., 2006) employed in the current study, and further described in section 3, has been used in several linguistic studies on features of Norwegian learner language and transfer effects from different L1 (Pepper, 2012; Golden, 2016; Vigrestad, 2016). The ASK corpus has also been used in previous work to train machine learning systems for Native Language Identification (NLI). The task of NLI for English language learners has been the subject of several shared tasks (Tetreault et al., 2013; Schuller et al., 3 Dataset The ASK corpus (AndreSpr˚aksKorpus; Tenfjord et al., 2006) contains Norwegian learner essays from two different language tests: Spr˚akprøven i norsk for voksne innvandrere and Test i norsk – høyere niv˚a,2 which test proficiency at the B1 and B2 levels, respectively. Following the naming in 2 Translated to as “Language testing in Norwegian for adult immigrants” and “Test in upper Norwegian levels”. 93 et al., 2013), which only uses three distinct proficiency categories, or the corpus used in Vajjala and Rama (2018), the MERLIN corpus, where the CEFR scores range be"
W19-4409,I17-1102,0,0.0565073,"Missing"
W19-4409,W14-3509,0,0.136469,"of Informatics University of Oslo University of Oslo University of Oslo stigjb@gmail.com taraka.kasi@gmail.com liljao@ifi.uio.no Abstract 2010; Yannakoudakis et al., 2011; Vajjala, 2017), and neural end-to-end architectures (Taghipour and Ng, 2016; Alikaniotis et al., 2016). Previous work has furthermore adopted different formulations of the task, either as a regression problem (Phandi et al., 2015; Taghipour and Ng, 2016) or a classification task (Rudner and Liang, 2002; Briscoe et al., 2010; Vajjala and Rama, 2018). Most previous work however, with a few noteworty exceptions (Hancke, 2013; Vajjala and Loo, 2014; Pil´an et al., 2016), has been focused on English learner language. In this paper we present first results for the task of Automated Essay Scoring for Norwegian learner language. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either regression or classification, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying multi-task learning for joint prediction of essay scoring and native language identification. We find that a GRU-based attention mode"
W19-4409,W18-0515,1,0.89617,"an Berggren Taraka Rama Lilja Øvrelid Department of Informatics Department of Informatics Department of Informatics University of Oslo University of Oslo University of Oslo stigjb@gmail.com taraka.kasi@gmail.com liljao@ifi.uio.no Abstract 2010; Yannakoudakis et al., 2011; Vajjala, 2017), and neural end-to-end architectures (Taghipour and Ng, 2016; Alikaniotis et al., 2016). Previous work has furthermore adopted different formulations of the task, either as a regression problem (Phandi et al., 2015; Taghipour and Ng, 2016) or a classification task (Rudner and Liang, 2002; Briscoe et al., 2010; Vajjala and Rama, 2018). Most previous work however, with a few noteworty exceptions (Hancke, 2013; Vajjala and Loo, 2014; Pil´an et al., 2016), has been focused on English learner language. In this paper we present first results for the task of Automated Essay Scoring for Norwegian learner language. We analyze a number of properties of this task experimentally and assess (i) the formulation of the task as either regression or classification, (ii) the use of various non-neural and neural machine learning architectures with various types of input representations, and (iii) applying multi-task learning for joint predi"
W19-4409,P11-1019,0,0.199957,"s but chose to test our best models tuned on development split and then report the best model on a separate test split. In this paper, we test several RNN models were implemented based on the architecture described in Taghipour and Ng (2016). We made necessary changes to the architectures in order to accommodate our data. For instance, Taghipour and Ng (2016) modelled the task as a regression problem, where the output layer consists of a single node with a value constrained to (0, 1) by the sigmoid function. This layer was replaced with a softmax layer which is described further in section 6. Yannakoudakis et al. (2011) present the CLC First Certificate of English (FCE) corpus as well as a system that makes use of deep linguistic features, such as PoS-tags and syntactic information and further employ a discriminative ranker that is shown to outperform a regression approach on the FCE corpus. Vajjala (2017) trains linear classifiers over the TOEFL11 corpus of non-native English (Blanchard et al., 2013) and the FCE corpus and makes use of a number of linguistic features for the task, including several different measures for lexical diversity, distribution of POS tags, and syntactic complexity, as well as featu"
W19-4724,N16-2002,0,0.0193565,"st cosine similarity to ˆi in Mn+1 is assumed to be a candidate for an insurRelated work The issue of linguistic regularity manifested in relational similarity has been studied for a long time. Due to the long-standing criticism of strictly binary relation structure, SemEval-2012 offered the task to detect the degree of relational similarity (Jurgens et al., 2012). This means that multiple correct answers exist, but they should be ranked differently. Somewhat similar improvements to the well-known word analogies dataset from (Mikolov et al., 2013b) were presented in the BATS analogy test set (Gladkova et al., 2016), also featuring multiple correct answers.1 Our One-to-X analogy setup extends this by introducing the possibility of the correct answer being ’None’. In the cases when correct answers exist, they are equally ranked, but their number can be different. Using distributional word representations to trace diachronic semantic shifts (including those reflecting social and cultural events) has received substantial attention in the recent years. Our work shares some of the workflow with Kutuzov et al. (2017). They used a supervised approach to analogical reasoning, applying ‘semantic directions’ learn"
W19-4724,D16-1229,0,0.0302918,"inference with word embeddings in general in (Rogers et al., 2017). 197 gent armed group active in this location in the time period n + 1; however, a more involved approach is needed to handle cases when the number of insurgents (correct answers) can be different from 1 (including 0), described below. For this workflow to yield meaningful results, it is essential for the paired models to be ‘aligned’. This is why we train the models incrementally, thus ensuring that they share common structural properties. Another possible way to cope with this is by using the orthogonal Procrustes alignment (Hamilton et al., 2016). 3 Time span Locations Insurgents Conflict pairs New pairs share Conflict locations share Insurgents per location 1995–2010 52 127 136 0.37 0.46 1.65 2010–2017 42 78 102 0.39 0.56 1.50 &quot;Islamic State&quot;]’). Entities occurring less than 25 times in the corresponding yearly corpora were filtered out, since it is difficult for distributional models to learn meaningful embeddings for such rare words. We create one such conflict relation dataset for each news corpus; one corresponding to the time span of NOW and another for Gigaword. Table 1 shows various statistics across these UCDP subsets, includ"
W19-4724,S12-1047,0,0.178355,"such task is demonstrated, using a threshold based on a function of cosine distance to decrease the number of false positives; this approach is shown to be beneficial on two different corpora. Finally, we publish a readyto-use test set for one-to-X analogy evaluation on historical armed conflicts data. Performance on the task of analogical inference (or ‘word analogies’) is one of the most widespread means to evaluate distributional word representation models, with ‘KING is to QUEEN as MAN is to ? (WOMAN)’ being a famous example. It also has deep connections to the relational similarity task (Jurgens et al., 2012). Most often, analogical inference is formulated as a strict proportion, and the model has to provide exactly one best answer for each question (assuming that it is impossible that, e.g., WOMAN and GIRL are equally correct answers for the question above). We reformulate the analogical inference task and extend it to include multiple-ended or one-toX relations: one-to-one, one-to-many and one-tonone cases when an entity is not included in this particular relation type, so there is no correct answer for it. This way, the model has to provide as many correct answers as possible, while providing a"
W19-4724,C18-1117,1,0.843621,"arned on the previous year’s armed conflicts data to the subsequent year. We extend their research by significantly reformulating the analogy task, making it more realistic, and finding ways to cope with false positives (insurgent armed groups predicted for locations where no armed conflicts are registered this year). In comparison to their work, we also use newer and larger corpora of news texts and the most recent version of the UCDP dataset. For brevity, we do not describe the emerging field of diachronic word embeddings in details, referring the interested readers to the recent surveys of Kutuzov et al. (2018) and Tang (2018). 2 The model Mn+1 is initialized with the weights from the model Mn ; if there are new words in the n + 1 data which exceed the frequency threshold, then at the start of Mn+1 training they are added to it and assigned random vectors. 3 A similar approach has been used for naive translation of words from the language L1 to L2 by using monolingual word embeddings for both and a seed bilingual dictionary (set of one-to-one pairs) (Mikolov et al., 2013a). 1 See also the detailed criticism of analogical inference with word embeddings in general in (Rogers et al., 2017). 197 gent ar"
W19-4724,D17-1194,1,0.904717,"Missing"
W19-4724,W18-4501,0,0.0288729,"hese counts are accumulated and for each year standard precision, recall and F1 score are calculated. These metrics are then averaged across all years in the test set. Using false negatives ensures that we penalize the systems for yielding predictions for peaceful locations. Precision Recall F1 Baseline Threshold 0.19 0.46 0.51 0.41 0.28 0.41 Baseline Threshold 0.26 0.42 0.53 0.41 0.34 0.41 Table 3: Average diachronic performance Cosine threshold It is clear that such a system (dubbed ‘baseline’) will always yield k incorrect candidates for peaceful areas. Inspired partially by the ideas from Orlikowski et al. (2018), we implemented a simple approach based on the assumption that the correct armed groups vectors will tend to be closer to the ˆi point than other nearest neighbours. Thus, the system should pick only the candidates located within a hypersphere of a pre-defined radius r centered around ˆi. rn can be different for different years, and we infer it from the p training conflict pairs from the previous year by calculating the average cosine distance between the ‘armed conflict projections’ ˆi and armed groups: p   1X r= cos ˆip , gp + σ p Algorithm (1) p=0 199 where gp is the armed group in the p"
W19-4724,S17-1017,0,0.0355908,"Missing"
W19-4724,N18-1044,0,0.0610853,"Missing"
W19-4724,K17-3009,0,0.0478494,"Missing"
W19-4802,W14-2609,0,0.0122035,"ably stay in the shadow of its two older, more accessible Qatsi siblings” are difficult for sentiment classifiers that do not model this phenomenon explicitly. Modality None of the state-of-the-art sentiment systems deals explicitly with modality (38 total errors). While in many of the examples modality does not express a different sentiment than the same sentence without modality, in the dataset there are examples that do, e. g., “Still, I thought it could have been more.” Sarcasm/Irony Sarcasm and irony (58 errors), which are often treated separately from sentiment analysis (Filatova, 2012; Barbieri et al., 2014), are present mainly in negative and strong negative examples in the dataset. Correctly capturing sarcasm and irony is necessary to classify some negative and strong negative examples, e. g., “If Melville is creatively a great whale, this film is canned tuna.” Morphology While not the most prominent label (31 errors), the examples in the dataset that contain morphological features that effect sentiment are normally strong positive or strong negative. This most often contains creative use of English morphology, e. g., “It was fan-freakin-tastic!” or “It’s hyper-cliched”. Shifters Shifters (50 e"
W19-4802,L18-1104,1,0.898888,"Missing"
W19-4802,W17-5202,1,0.790875,"bi-attentive sentiment network currently give state-of-the-art results (Peters et al., 2018). T¨ackstr¨om dataset The T¨ackstr¨om dataset (T¨ackstr¨om and McDonald, 2011) contains product reviews which have been annotated at both document- and sentence-level for three-class sentiment, although the sentence-level annotations also have a “not relevant” label. We keep the sentencelevel annotations, which gives 3,662 sentences annotated for three-class sentiment. BiLSTM Bidirectional long short-term memory (BiLSTM) networks have shown to be strong baselines for sentiment tasks (Tai et al., 2015; Barnes et al., 2017). We implement a single-layered BiLSTM which takes pretrained skipgram embeddings as input, creates a sentence representation by concatenating the final hidden layer of both left and right LSTMs, and then passes this representation to a softmax layer for classification. Additionally, dropout serves as a regularizer. Thelwall dataset The Thelwall dataset derives from datasets provided with SentiStrength2 (Thelwall et al., 2010). It contains microblogs annotated for both positive and negative sentiment on a scale from 1 to 5. We map these to single sentiment labels such that sentences which are"
W19-4802,W12-3802,0,0.0218494,"ata corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore the effect of modality on sentiment classification and find that explicitly modeling certain modalities improves classification results. They advocate for a divide-and-conquer approach, which would address the various realizations of modality individually. Benamara et al. (2012) perform linguistic experiments using native speakers concerning the effects of both negation and modality on opinions, and similarly find that the type of negation and modality determines the final interpretation of polarity. The sentiment models inspected in these analyses, however, were lexicon- and word- and nTotal MPQA OP. Sem. SST Ta. Th. − 193 527 413 − 379 879 − 399 74 − 3,499 4,478 1,310 − 1,852 3,111 2,242 3,140 1,510 − 923 1,419 1,320 − − 2,727 1,779 1,828 − 1,133 1,731 9,287 11,855 3,662 6,334 Table 1: Statistics for the sentence-level annotations in each dataset. gram-based models"
W19-4802,D08-1083,0,0.0538621,"te (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore th"
W19-4802,W18-6219,0,0.015025,"atasets. Additionally, we use a bag-of-words model as it is a strong baseline for text classification. For the S INGLE setup, we train all models on the training and development data for each dataset and test on the corresponding test set, therefore avoiding domain problems. OpeNER The Open Polarity Enhanced Named Entity Recognition (OpeNER) sentiment datasets (Agerri et al., 2013) contain hotel reviews annotated for 4-class (strong positive, positive, negative, strong negative) sentiment classification. We take the English dataset, where self-attention networks give state-of-the-art results (Ambartsoumian and Popowich, 2018). BERT The BERT model (Devlin et al., 2018) is a bidirectional transformer that is pretrained on two tasks: 1) a cloze-like language modeling task and 2) a binary next-sentence prediction task. It is pretrained on 330 million words from the BooksCorpus (Zhu et al., 2015) and English Wikipedia. We fine-tune the available pretrained model3 on each sentiment dataset. SemEval The SemEval 2013 tweet classification dataset (Nakov et al., 2013) contains tweets collected and annotated for three-class (positive, neutral, negative) sentiment. The state-of-the-art model is a Convolutional Network (Severy"
W19-4802,P18-2006,0,0.0264231,"ence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al"
W19-4802,filatova-2012-irony,0,0.0370513,"like “Will probably stay in the shadow of its two older, more accessible Qatsi siblings” are difficult for sentiment classifiers that do not model this phenomenon explicitly. Modality None of the state-of-the-art sentiment systems deals explicitly with modality (38 total errors). While in many of the examples modality does not express a different sentiment than the same sentence without modality, in the dataset there are examples that do, e. g., “Still, I thought it could have been more.” Sarcasm/Irony Sarcasm and irony (58 errors), which are often treated separately from sentiment analysis (Filatova, 2012; Barbieri et al., 2014), are present mainly in negative and strong negative examples in the dataset. Correctly capturing sarcasm and irony is necessary to classify some negative and strong negative examples, e. g., “If Melville is creatively a great whale, this film is canned tuna.” Morphology While not the most prominent label (31 errors), the examples in the dataset that contain morphological features that effect sentiment are normally strong positive or strong negative. This most often contains creative use of English morphology, e. g., “It was fan-freakin-tastic!” or “It’s hyper-cliched”."
W19-4802,S18-2005,0,0.0325543,"orks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity s"
W19-4802,N18-1108,0,0.0244808,"ntiment. 12 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 12–23 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resol"
W19-4802,Q16-1037,0,0.145829,"essing and probing sentiment. 12 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 12–23 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). T"
W19-4802,P18-1031,0,0.0334611,"Missing"
W19-4802,L18-1547,0,0.0203621,"approach to creating a dataset is also easily transferable to other tasks which are affected by linguistic or paralinguistic phenomena, such as hate speech detection or sarcasm detection. It would be more useful to have some knowledge of the phenomena that could affect the task beforehand, but a careful error analysis can also lead to insights which can be translated into annotation labels. Regarding ways of moving forward, there are already many sources of data for the linguistic phenomena we have analyzed in this work, ranging from datasets annotated for negation (Morante and Blanco, 2012; Liu et al., 2018), irony (Van Hee et al., 2018), emoji (Barbieri et al., 2018), as well as datasets for idioms (Muzny and Zettlemoyer, 2013) and their relationship with sentiment (Jochim et al., 2018). We believe that discovering ways to explicitly incorporate this available information into state-of-the-art sentiment models may provide a way to improve current approaches. Multi-task learning (Caruana, 1993) and transfer learning (Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018) have shown promise in this respect, but have not been exploited for improving sentiment classification with regards"
W19-4802,S13-2053,0,0.0202954,"3) the negative label, and clearly neutral sentences ( 3 &lt; pos > 2 and 3 &lt; neg > 2) the neutral. We discard all other sentences, which finally leaves 6,334 annotated sentences. 2 The data are available sentistrength.wlv.ac.uk/ at Models Bag-of-Words classifier Finally, bag-of-words classifiers are strong baselines for sentiment and when combined with other features can still give 3 https://github.com/google-research/ bert 4 https://s3-us-west-2.amazonaws.com/ allennlp/models/sst-5-elmo-biattentiveclassification-network-2018.09.04.tar.gz http:// 14 state-of-the-art results for sentiment tasks (Mohammad et al., 2013). Therefore, we train a Linear SVM on a bag-of-words representation of the training sentences. 3.3 during the error annotation process. We further chose to manually annotate for the polarity of the sentence irrespective of the gold label in order to be able to locate possible annotation errors during our analysis. The annotation scheme and (manually constructed) examples of each label are shown in Table 6. Note that we did not limit the number of labels that the annotator could assign to each sentence and in principle they should assign all suitable labels during annotation. Model performance"
W19-4802,L18-1379,0,0.0518807,"Missing"
W19-4802,S12-1035,0,0.0156704,"uages. We expect that this approach to creating a dataset is also easily transferable to other tasks which are affected by linguistic or paralinguistic phenomena, such as hate speech detection or sarcasm detection. It would be more useful to have some knowledge of the phenomena that could affect the task beforehand, but a careful error analysis can also lead to insights which can be translated into annotation labels. Regarding ways of moving forward, there are already many sources of data for the linguistic phenomena we have analyzed in this work, ranging from datasets annotated for negation (Morante and Blanco, 2012; Liu et al., 2018), irony (Van Hee et al., 2018), emoji (Barbieri et al., 2018), as well as datasets for idioms (Muzny and Zettlemoyer, 2013) and their relationship with sentiment (Jochim et al., 2018). We believe that discovering ways to explicitly incorporate this available information into state-of-the-art sentiment models may provide a way to improve current approaches. Multi-task learning (Caruana, 1993) and transfer learning (Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018) have shown promise in this respect, but have not been exploited for improving sentiment classific"
W19-4802,N18-1171,0,0.044734,"Missing"
W19-4802,D13-1145,0,0.075092,"Missing"
W19-4802,W16-0410,0,0.0204991,"Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore the effect of modality on sentiment"
W19-4802,S13-2052,0,0.0424662,", negative, strong negative) sentiment classification. We take the English dataset, where self-attention networks give state-of-the-art results (Ambartsoumian and Popowich, 2018). BERT The BERT model (Devlin et al., 2018) is a bidirectional transformer that is pretrained on two tasks: 1) a cloze-like language modeling task and 2) a binary next-sentence prediction task. It is pretrained on 330 million words from the BooksCorpus (Zhu et al., 2015) and English Wikipedia. We fine-tune the available pretrained model3 on each sentiment dataset. SemEval The SemEval 2013 tweet classification dataset (Nakov et al., 2013) contains tweets collected and annotated for three-class (positive, neutral, negative) sentiment. The state-of-the-art model is a Convolutional Network (Severyn and Moschitti, 2015). ELMo We use the bi-attentive classification network4 from Peters et al. (2018). The network uses both word embeddings, as well as creating character-based embeddings from a character-level CNN-BiLSTM network. The word representations are first passed through a feedforward layer, and then through a sequence-to-sequence network with biattention. This new representation of the text is combined with the original repre"
W19-4802,S15-2079,0,0.0465899,"Missing"
W19-4802,D09-1019,0,0.0262923,"composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore the effect of modality on sentiment classification and find that explicitly modeling certain modalities improves classification results. They advocate for a divide-and-conquer approach, which would address the various realizations of modality individually. Benamara et al. (2012) perform linguistic experiments using native speakers concerning the effects of both negation and modality on opinions, and similarly find that the type of negation and modality determines the final interpretation of polarity. The sentiment models inspected in these analyses, however, were"
W19-4802,W02-1011,0,0.0555453,"Missing"
W19-4802,D13-1170,0,0.0282932,"Missing"
W19-4802,D18-1302,0,0.0212489,"ted work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008"
W19-4802,P11-2100,0,0.0820226,"Missing"
W19-4802,N18-1202,0,0.0353429,"pretrained on two tasks: 1) a cloze-like language modeling task and 2) a binary next-sentence prediction task. It is pretrained on 330 million words from the BooksCorpus (Zhu et al., 2015) and English Wikipedia. We fine-tune the available pretrained model3 on each sentiment dataset. SemEval The SemEval 2013 tweet classification dataset (Nakov et al., 2013) contains tweets collected and annotated for three-class (positive, neutral, negative) sentiment. The state-of-the-art model is a Convolutional Network (Severyn and Moschitti, 2015). ELMo We use the bi-attentive classification network4 from Peters et al. (2018). The network uses both word embeddings, as well as creating character-based embeddings from a character-level CNN-BiLSTM network. The word representations are first passed through a feedforward layer, and then through a sequence-to-sequence network with biattention. This new representation of the text is combined with the original representation and passed through another sequence-to-sequence network. Finally, a max, min, mean and self-attention pool representation is created from this last sequence. For classification, these features are sent to a maxout layer. Stanford Sentiment Treebank Th"
W19-4802,P15-1150,0,0.0262008,"ns combined with a bi-attentive sentiment network currently give state-of-the-art results (Peters et al., 2018). T¨ackstr¨om dataset The T¨ackstr¨om dataset (T¨ackstr¨om and McDonald, 2011) contains product reviews which have been annotated at both document- and sentence-level for three-class sentiment, although the sentence-level annotations also have a “not relevant” label. We keep the sentencelevel annotations, which gives 3,662 sentences annotated for three-class sentiment. BiLSTM Bidirectional long short-term memory (BiLSTM) networks have shown to be strong baselines for sentiment tasks (Tai et al., 2015; Barnes et al., 2017). We implement a single-layered BiLSTM which takes pretrained skipgram embeddings as input, creates a sentence representation by concatenating the final hidden layer of both left and right LSTMs, and then passes this representation to a softmax layer for classification. Additionally, dropout serves as a regularizer. Thelwall dataset The Thelwall dataset derives from datasets provided with SentiStrength2 (Thelwall et al., 2010). It contains microblogs annotated for both positive and negative sentiment on a scale from 1 to 5. We map these to single sentiment labels such tha"
W19-4802,W15-2914,0,0.0498716,"Missing"
W19-4802,P18-1079,0,0.0197541,"LP, pages 12–23 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et"
W19-4802,P02-1053,0,0.0376856,"Missing"
W19-4802,S18-1005,0,0.0378914,"Missing"
W19-4802,W10-3111,0,0.209928,"tworks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This"
W19-4802,D18-1534,0,0.02286,"2019. 2019 Association for Computational Linguistics 2 Related work Label ++ + 0 − −− Neural networks are now ubiquitous in NLP tasks, often giving state-of-the-art results. However, they are known for being “black boxes” which are not easily interpretable. Recent interest in interpreting these methods has led to new lines of research which attempt to discover what linguistic phenomena neural networks are able to learn (Linzen et al., 2016; Gulordava et al., 2018; Conneau et al., 2018), how robust neural networks are to perturbations in input data (Ribeiro et al., 2018; Ebrahimi et al., 2018; Schluter and Varab, 2018), and what biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic comp"
W19-4802,H05-1044,0,0.456305,"t biases they propagate (Park et al., 2018; Zhao et al., 2018; Kiritchenko and Mohammad, 2018). Specifically within the task of sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu e"
W19-4802,L18-1222,0,0.0133724,"f sentiment analysis, certain linguistic phenomena are known to be challenging. Negation is one of the aspects of language that most clearly affects expressions of sentiment and that has been studied widely within sentiment analysis (see Wiegand et al. (2010) for an early survey). The difficulties of resolving negation for sentiment analysis include determining negation scope (Hogenboom et al., 2011; Lapponi et al., 2012; Reitan et al., 2015), and semantic composition (Wilson et al., 2005; Choi and Cardie, 2008; Kiritchenko and Mohammad, 2016). Verbal polarity shifters have also been studied. Schulder et al. (2018) annotate verbal shifters at the sense-level. They conclude that, although individual negation words are more frequent in the Amazon Product Review Data corpus, the overall frequency of negation words and shifters is likely similar. This suggests that there is a Zipfian tail of shifters which are not often handled within sentiment analysis. Furthermore, the linguistic phenomenon of modality has also been shown to be problematic. Both Narayanan et al. (2009) and Liu et al. (2014) explore the effect of modality on sentiment classification and find that explicitly modeling certain modalities impr"
W19-4802,N19-1423,0,\N,Missing
W19-6113,W17-5202,1,0.768654,"Missing"
W19-6113,W16-4011,0,0.0524904,"Missing"
W19-6113,P13-1094,0,0.280803,"Missing"
W19-6113,N18-2074,0,0.0149121,". Dropout is used after the max pooling layer and ReLU layer for regularization. B I L STM is a one-layer bidirectional Long Short-Term Network (Graves et al., 2005) with word embeddings as input. The contextualized representation of each sentence is the concatenation of the final hidden states from the left-toright and right-to-left LSTM. This representation is then passed to a softmax layer for classification. Dropout is used before the LSTM layers and softmax layers for regularization. S AN is a one-layer self-attention network (Vaswani et al., 2017) with relative position representations (Shaw et al., 2018) and a single set of attention heads, which was previously shown to perform well for sentiment analysis (Ambartsoumian and Popowich, 2018). The network uses a variant of the attention mechanism (Bahdanau et al., 2014) which creates contextualized representations of the original input sequence, such that the contextualized representations encode both information about the original input, as well as how it relates to all other positions. 8.2 Experimental Setup We apply the models to five experimental setups. The main task is to classify each sentence as evaluative (EVAL), fact-implied non-person"
W19-6113,P10-1059,0,0.416835,"we analyze the corpus experimentally and present a series of preliminary classification experiments using a wide range of state-of-the-art sentiment models including CNNs, BiLSTMs and self-attention networks, before we in Section 9 conclude and outline some remaining avenues for future work. The dataset and the annotation guidelines are made available, along with code for replicating the experiments.1 1 https://github.com/ltgoslo/norec_eval 2 Background and related work In this section we briefly review some of the previous annotation efforts (for English) that are most relevant for our work. Toprak et al. (2010) present a sentimentannotated corpus of consumer reviews. In a first pass, sentences are annotated with respect to relevancy to the overall topic and whether they express an evaluation. In a second pass, sentences that were marked as relevant and evaluative are further annotated with respect to whether they are opinionated (i.e. express a subjective opinion) or polar-facts (i.e. factual information that implies evaluation). In addition to evaluations, they also identify sources (opinion holders), targets (the entity or aspect that the sentiment is directed towards), modifiers, positive/negativ"
W19-6113,W17-0237,1,0.898818,"Missing"
W19-6113,W10-1501,0,0.579569,"in these texts. 8 Experiments In this section we apply a range of different architectures to provide first baseline results for predicting the various labels in the new corpus. Data splits for training, validation and testing are inherited from NoReC. 8.1 Models We provide a brief description of the various classifiers below. Additionally, we provide a majority baseline which always predicts the EVAL class as a lower bound. Note that all classifiers except the bag-of-words model take as input 100 dimensional fastText skipgram embeddings (Bojanowski et al., 2016), trained on the NoWaC corpus (Guevara, 2010), which contains over 680 Million tokens in Bokm˚al Norwegian. The pre-trained word embeddings were re-used from the NLPL vector repository3 (Fares et al., 2017). B OW learns to classify the sentences with a linear separation estimated based on log likelihood optimization with an L2 prior using a bagof-words representation. AVE (Barnes et al., 2017) uses the same L2 logistic regression classifier as B OW, but instead using as input the average of the word vectors from a sentence. C NN (Kim, 2014) is a single-layer convolutional neural network with one convolutional layer 3 http://vectors.nlpl."
W19-6113,wilson-2008-annotating,0,0.357339,"entences – i.e. both subjective and objective sentences that are found to be sentimentbearing – based on mixed-domain professional reviews from various news-sources. We present both the annotation scheme and first results for classification experiments. The effort represents a step toward creating a Norwegian dataset for fine-grained sentiment analysis. 1 Introduction Sentiment analysis is often approached by first locating the relevant, sentiment-bearing sentences. Traditionally, one has distinguished between subjective and objective sentences, where only the former were linked to sentiment (Wilson, 2008). Objective sentences typically present facts about the world, whereas subjective sentences express personal feelings, views, or beliefs. More recently, however, it has become widely recognized in the literature that subjectivity should not be equated with opinion (Liu, 2015): On the one hand, there are many subjective sentences that do not express sentiment, e.g., I think that he went home, and on the other hand there are many objective sentences that do, e.g., The earphone broke in two days, to quote some examples from Liu (2015). Additionally, sentences often contain several polarities in a"
W19-6113,N18-1171,0,0.0248995,"ator agreement for the attribute categories ¬OT and ¬FP, restricted to the subset of sentences labeled EVAL,2 yielding F1 of 0.59 and 0.56, respectively. In other words, we see that the agreement is somewhat lower for these subcategories compared to the top-level label EVAL. Possible reasons for this might be that although problems with these attributes seem to be resolved quickly in annotator meetings, they might pose difficulties to the individual annotator, as sometimes these attributes can be context dependent to an extent that makes them difficult to infer from the review text by itself. Kenyon-Dean et al. (2018) problematizes a practice often seen in relation to sentiment annotation, namely that complicated cases – e.g. sentences were there is annotator disagreement – are discarded from the final dataset. This makes the 2 For the FACT-NP subset there were too few instances of these attributes (prior to adjudication) for agreement to be meaningfully quantified; 1 for ¬OT and 0 for ¬FP. EVAL FACT-NP NONE all 0.84 0.22 0.87 0.82 Table 1: F1 inter-annotator agreement for each top-level label. data non-representative of real text and will artificially inflate classification results on the annotations. In"
W19-6113,D14-1181,0,0.00898412,"Missing"
W19-6119,N18-1172,0,0.10188,"that are predictive for both tasks. MTL assumes that features that are useful for a certain task should also be predictive for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentimen"
W19-6119,P17-2054,0,0.0225945,"n this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentiment and evaluative-language classification. 3.1 English sentiment lexicon For English we use the sentiment lexicon compiled by Hu and Liu (2004), containi"
W19-6119,W17-0237,1,0.850985,"arning model (M TL) alternates between training one epoch on the main task and one epoch on the auxiliary task. Preliminary experiments showed that more complicated training strategies (alternating training between each batch or uniformly sampling batches from the two tasks) did not lead to improvements. For English we use 300 dimensional pre-trained embeddings from GoogleNews,6 while for Norwegian we use 100 dimensional skip-gram fastText embeddings (Bojanowski et al., 2016) trained on the NoWaC corpus (Guevara, 2010). The pre-trained embeddings were re-used from the NLPL vector repository7 (Fares et al., 2017). We train the model for 10 epochs using Adam (Kingma and Ba, 2014), performing early stopping determined by the improvement on the development set of the main task. Given that neural models are sensitive to the random initialization of their parameters, we perform five runs with different random seeds and show the mean and standard deviation as the final result for each model. We use the same five random seeds for all experiments to ensure a fair comparison between models. 6 Available at https://code.google.com/ archive/p/word2vec/. 7 http://vectors.nlpl.eu/repository Model SST NoReCeval 6 L"
W19-6119,P19-2035,0,0.279006,"icon approaches and that their model is more robust to domain shifts than machine learning models. Lexicons in neural approaches The general tendency in NLP when using neural approaches is to perform end-to-end learning without using external knowledge sources, relying instead solely on what can be inferred from (often pre-trained) word embeddings and the training corpus itself. This is also the case for neural sentiment modeling. However, there have been some attempts to include external knowledge like lexicon features into such models (Teng et al., 2016; Zou et al., 2018; Lei et al., 2018a; Bao et al., 2019a). One notable example is the work of Shin et al. (2017) where several approaches are tested for how to incorporate lexicon information into a CNN for sentiment classification on the SemEval 2016 Task 4 dataset and the Stanford Sentiment Treebank (SST). Shin et al. (2017) create feature vectors that encode the positive or negative polarity values of words across a broad selection of different sentiment lexicons available for English. These word-level sentiment-score vectors are then combined with standard word embeddings in different ways in the CNN: through simple concatenation, using multip"
W19-6119,D18-1178,1,0.814294,"Missing"
W19-6119,E17-2026,0,0.108169,"e for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentiment and evaluative-language classification. 3.1 English sentiment lexicon For English we use the sentiment lexicon compiled"
W19-6119,W17-0225,0,0.0223973,"he weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentiment and evaluative-language classification. 3.1 English sentiment lexicon For English we use the sentiment lexicon compiled by Hu and Liu (2004), containing 4,783 negative words and 2,006 positive words."
W19-6119,esuli-sebastiani-2006-sentiwordnet,0,0.203849,"eoretical approaches to emotion (Stone et al., 1962; Bradley et al., 1999). There are several freely available sentiment lexicons for English. One widely used lexicon is that of Hu and Liu (2004),3 which was created using a bootstrapping approach from WordNet and a corpus of product reviews. This is the lexicon that forms the basis of the experiments in the current paper and we return to it in § 3.1. Other available lexicons include the MPQA subjectivity lexicon (Wilson et al., 2005) which contains words and expressions manually annotated as positive, negative, both, or neutral. SentiWordnet (Esuli and Sebastiani, 2006) contains each synset of the English WordNet annotated with scores representing the sentiment orientation as being positive, negative, or objective. The So-Cal (Taboada et al., 2011) English sentiment lexicon contains separate lexicons of verbs, nouns, adjectives, and adverbs. The words were manually labeled on a scale from extremely positive (+5) to extremely negative (−5), and all words labeled as neutral (0) were excluded from the lexicons. While no high-quality sentiment lexicons for Norwegian are currently publicly available, there have been some previous attempts at generating lexicons f"
W19-6119,W10-1501,0,0.0188357,"e to measure the relative improvement. Multi-task model: During training, the multitask learning model (M TL) alternates between training one epoch on the main task and one epoch on the auxiliary task. Preliminary experiments showed that more complicated training strategies (alternating training between each batch or uniformly sampling batches from the two tasks) did not lead to improvements. For English we use 300 dimensional pre-trained embeddings from GoogleNews,6 while for Norwegian we use 100 dimensional skip-gram fastText embeddings (Bojanowski et al., 2016) trained on the NoWaC corpus (Guevara, 2010). The pre-trained embeddings were re-used from the NLPL vector repository7 (Fares et al., 2017). We train the model for 10 epochs using Adam (Kingma and Ba, 2014), performing early stopping determined by the improvement on the development set of the main task. Given that neural models are sensitive to the random initialization of their parameters, we perform five runs with different random seeds and show the mean and standard deviation as the final result for each model. We use the same five random seeds for all experiments to ensure a fair comparison between models. 6 Available at https://cod"
W19-6119,C04-1200,0,0.152495,"the top 10,000 most frequent words in the corpus and a list of adjectives generated from their corpus using SCARRIE. Their results showed that the translated lexicons outperformed all of their generated lexicons, but unfortunately only the latter were made publicly available. Lexicon-based approaches to SA Early approaches to sentiment analysis classified documents based on the sum of semantic orientation scores of adjectives in a document. Often, researchers used existing lexicons (Stone et al., 1962), or extended these resources in a semisupervised fashion, using WordNet (Hu and Liu, 2004; Kim and Hovy, 2004; Esuli and Sebastiani, 2006). Alternatively, an adjective’s semantic orientation could be determined as the strength of association with positive words (excellent) or negative words (poor) as measured by Pointwise Mutual Information (Turney and Littman, 2003). Researchers quickly discovered, however, that various linguistic phenomena, e.g. negation, intensifying adverbs, downtoners, etc, must be taken into account to correctly assign a sentiment score. Taboada et al. (2011) proposed an approach to determine the semantic orientation of documents which incorporates sentiment lexicons for adject"
W19-6119,konstantinova-etal-2012-review,0,0.0698456,"Missing"
W19-6119,W17-5206,0,0.0528967,"Missing"
W19-6119,W17-1903,0,0.0622486,"Missing"
W19-6119,P05-1015,0,0.243866,"n unchanged for SST. The model used by Shin et al. (2017) requires information from six different lexicons, which is overly restrictive for most languages besides English, where one will typically not have the luxury of several publicly available sentiment lexicons. Lei et al. (2018b) propose a different approach based on what they dub a ‘Multi-sentimentresource Enhanced Attention Network’, where lexicon information is used for guiding an attention mechanism when learning sentiment-specific sentence representations. The approach shows promising results on both SST and the Movie Review data of Pang and Lee (2005), although the model also incorporates other types of lexicons, like negation cues and intensifiers. In a similar spirit, Margatina et al. (2019) include features from a range of sentiment-related lexicons for guiding the self-attention mechanism in an LSTM. Bao et al. (2019b) generate features from several different lexicons that are added to an attention-based LSTM for aspect-based sentiment analysis. In the current paper we will instead explore whether lexicon information can be incorporated into neural models using the framework of multitask learning. This has two main advantages: 1) we re"
W19-6119,P18-2120,0,0.187132,"semi-supervised lexicon approaches and that their model is more robust to domain shifts than machine learning models. Lexicons in neural approaches The general tendency in NLP when using neural approaches is to perform end-to-end learning without using external knowledge sources, relying instead solely on what can be inferred from (often pre-trained) word embeddings and the training corpus itself. This is also the case for neural sentiment modeling. However, there have been some attempts to include external knowledge like lexicon features into such models (Teng et al., 2016; Zou et al., 2018; Lei et al., 2018a; Bao et al., 2019a). One notable example is the work of Shin et al. (2017) where several approaches are tested for how to incorporate lexicon information into a CNN for sentiment classification on the SemEval 2016 Task 4 dataset and the Stanford Sentiment Treebank (SST). Shin et al. (2017) create feature vectors that encode the positive or negative polarity values of words across a broad selection of different sentiment lexicons available for English. These word-level sentiment-score vectors are then combined with standard word embeddings in different ways in the CNN: through simple concaten"
W19-6119,W17-2612,0,0.0256611,"ive bias by restricting the search space of possible representations to those that are predictive for both tasks. MTL assumes that features that are useful for a certain task should also be predictive for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate senti"
W19-6119,C16-1059,0,0.0178372,"useful inductive bias by restricting the search space of possible representations to those that are predictive for both tasks. MTL assumes that features that are useful for a certain task should also be predictive for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approa"
W19-6119,W19-6113,1,0.84215,"pment and test splits (of 8,455 / 1,101 / 2,210 sentences, respectively). 4.2 Norwegian The Norwegian dataset used in this work forms part of the Norwegian Review Corpus NoReC (Velldal et al., 2018), consisting of full-text reviews from a range of different domains, such as restaurants, literature, and music, collected from several of the major Norwegian news sources. The particular subset used in the current work, dubbed NoReCeval , comprises 7961 sentences across 298 documents that have been manually annotated according to whether or not each sentence contains an evaluation, as described by Mæhlum et al. (2019). Two types of evaluative sentence categories are distinguished (in addition to non-evaluative sentences): simple evaluative and a special case of evaluative fact-implied nonpersonal. The latter follows the terminology of Liu (2015), denoting factual, objective sentences which are used with an evaluative intent but without reference to personal experience. Example 5 https://github.com/ltgoslo/norsentlex Unlike the English dataset discussed above, the annotation does not specify the polarity of the sentence. The rationale for this is that a sentence may contain more than one sentiment expressio"
W19-6119,P19-1385,0,0.0191389,"ost languages besides English, where one will typically not have the luxury of several publicly available sentiment lexicons. Lei et al. (2018b) propose a different approach based on what they dub a ‘Multi-sentimentresource Enhanced Attention Network’, where lexicon information is used for guiding an attention mechanism when learning sentiment-specific sentence representations. The approach shows promising results on both SST and the Movie Review data of Pang and Lee (2005), although the model also incorporates other types of lexicons, like negation cues and intensifiers. In a similar spirit, Margatina et al. (2019) include features from a range of sentiment-related lexicons for guiding the self-attention mechanism in an LSTM. Bao et al. (2019b) generate features from several different lexicons that are added to an attention-based LSTM for aspect-based sentiment analysis. In the current paper we will instead explore whether lexicon information can be incorporated into neural models using the framework of multitask learning. This has two main advantages: 1) we require only a single sentiment lexicon, unlike much previous work, and 2) our model is able to generalize to sentiment words not seen in the lexic"
W19-6119,E17-1005,0,0.014929,"h space of possible representations to those that are predictive for both tasks. MTL assumes that features that are useful for a certain task should also be predictive for similar tasks, and in this sense effectively acts as a regularizer, as it prevents the weights from adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an aux"
W19-6119,S13-2053,0,0.0353331,"ge points (ppt) worse than S TL. On NoReCeval , however, it performs much worse, which can be attributed to the the difficulty of determining if a sentence is nonevaluative or fact-implied using only unigram information, as these sentence types do not differ largely lexically. B OW +L EXICON performs better than B OW on both datasets, although the difference is larger on SST (1.5 ppt vs. 0.8 ppt). This is likely due to sentiment lexicon features being more predictive for the sentiment task. Additionally, it outperforms the S TL model by 1.1 ppt on SST, confirming that it is a strong baseline (Mohammad et al., 2013). L EX -E MB is the weakest model on the SST dataset with 34.7 F1 but performs better than the non-neural baselines on NoReCeval (48.9). S TL performs better than L EXICON, B OW, and L EX E MB on both tasks, as well as B OW +L EXICON on NoReCeval . Finally, M TL is the best performing model on both tasks, with a difference of 3.5 ppt between M TL and the next best performing model on SST, and 1.6 ppt on NoReCeval . Table 3: Macro F1 of models on the SST and NoReCeval sentence-level datasets. Neural models report mean and standard deviation of the scores over five runs. Lexicon embedding Model:"
W19-6119,morante-daelemans-2012-conandoyle,0,0.02648,"e show how MTL can enable a BiLSTM sentiment classifier to incorporate information from sentiment lexicons. Our MTL set-up is shown to improve model performance (compared to a single-task set-up) on both English and Norwegian sentence-level sentiment datasets. The paper also introduces a new sentiment lexicon for Norwegian. 1 Introduction Current state-of-the-art neural approaches to sentiment analysis tend not to incorporate available sources of external knowledge, such as polarity lexicons (Hu and Liu, 2004; Taboada et al., 2006; Mohammad and Turney, 2013), explicit negation annotated data (Morante and Daelemans, 2012; Konstantinova et al., 2012), or labels representing inter-annotator agreement (Plank et al., 2014). One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner (Socher et al., 2013; Irsoy and Cardie, 2014). Another often overlooked reason is that it is not always entirely straightforward how we can efficiently incorporate this available external knowledge in"
W19-6119,E14-1078,0,0.0196223,"Our MTL set-up is shown to improve model performance (compared to a single-task set-up) on both English and Norwegian sentence-level sentiment datasets. The paper also introduces a new sentiment lexicon for Norwegian. 1 Introduction Current state-of-the-art neural approaches to sentiment analysis tend not to incorporate available sources of external knowledge, such as polarity lexicons (Hu and Liu, 2004; Taboada et al., 2006; Mohammad and Turney, 2013), explicit negation annotated data (Morante and Daelemans, 2012; Konstantinova et al., 2012), or labels representing inter-annotator agreement (Plank et al., 2014). One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner (Socher et al., 2013; Irsoy and Cardie, 2014). Another often overlooked reason is that it is not always entirely straightforward how we can efficiently incorporate this available external knowledge in the model. Despite achieving strong results, neural models are known to be difficult to interpret,"
W19-6119,P13-1094,0,0.0767359,"Missing"
W19-6119,W17-5220,0,0.0683005,"domain shifts than machine learning models. Lexicons in neural approaches The general tendency in NLP when using neural approaches is to perform end-to-end learning without using external knowledge sources, relying instead solely on what can be inferred from (often pre-trained) word embeddings and the training corpus itself. This is also the case for neural sentiment modeling. However, there have been some attempts to include external knowledge like lexicon features into such models (Teng et al., 2016; Zou et al., 2018; Lei et al., 2018a; Bao et al., 2019a). One notable example is the work of Shin et al. (2017) where several approaches are tested for how to incorporate lexicon information into a CNN for sentiment classification on the SemEval 2016 Task 4 dataset and the Stanford Sentiment Treebank (SST). Shin et al. (2017) create feature vectors that encode the positive or negative polarity values of words across a broad selection of different sentiment lexicons available for English. These word-level sentiment-score vectors are then combined with standard word embeddings in different ways in the CNN: through simple concatenation, using multiple channels, or performing separate convolutions. While a"
W19-6119,D13-1170,0,0.0589301,"e available sources of external knowledge, such as polarity lexicons (Hu and Liu, 2004; Taboada et al., 2006; Mohammad and Turney, 2013), explicit negation annotated data (Morante and Daelemans, 2012; Konstantinova et al., 2012), or labels representing inter-annotator agreement (Plank et al., 2014). One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner (Socher et al., 2013; Irsoy and Cardie, 2014). Another often overlooked reason is that it is not always entirely straightforward how we can efficiently incorporate this available external knowledge in the model. Despite achieving strong results, neural models are known to be difficult to interpret, as well as highly dependent on the training data. Resources like sentiment lexicons, on the other hand, have the benefit of being completely transparent, as well as being easy to adapt or update. Additionally, lexicons are often less sensitive to domain and frequency effects and can provide high coverage and precision"
W19-6119,P16-2038,0,0.0351761,"adapting too much to a single task. The simplest approach to MTL, hard parameter sharing (Caruana, 1993), assumes that all layers are shared between tasks except for the final predictive layer. This approach tends to improve performance when the auxiliary task is carefully chosen (Plank, 2016; Peng and Dredze, 2017; Mart´ınez Alonso and Plank, 2017; Fares et al., 2018; Augenstein et al., 2018). What characteristics determine a useful auxiliary task, however, is still not completely clear (Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Søgaard and Goldberg (2016) propose an improvement over hard parameter sharing that uses the lower layers of a multi-layer recurrent neural network to make predictions for low-level auxiliary tasks, while allowing higher layers to focus on the main task. In this work, we adopt a similar approach to incorporate sentiment lexicon information as an auxiliary task to improve sentence-level sentiment and evaluative-language classification. 3.1 English sentiment lexicon For English we use the sentiment lexicon compiled by Hu and Liu (2004), containing 4,783 negative words and 2,006 positive words. The sentiment lexicon was a"
W19-6119,taboada-etal-2006-methods,0,0.133979,"earning (MTL) for incorporating external knowledge in neural models. Specifically, we show how MTL can enable a BiLSTM sentiment classifier to incorporate information from sentiment lexicons. Our MTL set-up is shown to improve model performance (compared to a single-task set-up) on both English and Norwegian sentence-level sentiment datasets. The paper also introduces a new sentiment lexicon for Norwegian. 1 Introduction Current state-of-the-art neural approaches to sentiment analysis tend not to incorporate available sources of external knowledge, such as polarity lexicons (Hu and Liu, 2004; Taboada et al., 2006; Mohammad and Turney, 2013), explicit negation annotated data (Morante and Daelemans, 2012; Konstantinova et al., 2012), or labels representing inter-annotator agreement (Plank et al., 2014). One reason for this is that neural models can already achieve good performance, even if they only use word embeddings given as input, as they are able to learn task-specific information (which words convey sentiment, how to resolve negation, how to resolve intensification) in a data-driven manner (Socher et al., 2013; Irsoy and Cardie, 2014). Another often overlooked reason is that it is not always entir"
W19-6119,J11-2001,0,0.975546,"(2004),3 which was created using a bootstrapping approach from WordNet and a corpus of product reviews. This is the lexicon that forms the basis of the experiments in the current paper and we return to it in § 3.1. Other available lexicons include the MPQA subjectivity lexicon (Wilson et al., 2005) which contains words and expressions manually annotated as positive, negative, both, or neutral. SentiWordnet (Esuli and Sebastiani, 2006) contains each synset of the English WordNet annotated with scores representing the sentiment orientation as being positive, negative, or objective. The So-Cal (Taboada et al., 2011) English sentiment lexicon contains separate lexicons of verbs, nouns, adjectives, and adverbs. The words were manually labeled on a scale from extremely positive (+5) to extremely negative (−5), and all words labeled as neutral (0) were excluded from the lexicons. While no high-quality sentiment lexicons for Norwegian are currently publicly available, there have been some previous attempts at generating lexicons for Norwegian. Hammer et al. (2014) used a set of 51 positive and 57 negative manually selected seed words to crawl three Norwegian thesauri in three iterations, to extract synonyms a"
W19-6119,D16-1169,0,0.0360708,"Missing"
W19-6119,P10-1059,0,0.0216209,"personal. The latter follows the terminology of Liu (2015), denoting factual, objective sentences which are used with an evaluative intent but without reference to personal experience. Example 5 https://github.com/ltgoslo/norsentlex Unlike the English dataset discussed above, the annotation does not specify the polarity of the sentence. The rationale for this is that a sentence may contain more than one sentiment expression and have a mixed polarity, hence this type of annotation is better performed sub-sententially following an initial annotation of evaluative or sentimentrelevant sentences (Toprak et al., 2010; Scheible and Sch¨utze, 2013). We use the training, development and test splits as defined by Mæhlum et al. (2019), see the summary of corpus statistics in Table 2. 5 Multi-task learning of lexicon information in neural models This section details our multi-task neural architecture for incorporating sentiment lexicon information into neural networks, as shown in Figure 1. Our multi-task model shares the lower layers (an embedding and fully connected layer), while allowing the higher layers to further adapt to the main and auxiliary tasks. Specifically, we use a sentiment prediction auxiliary"
W19-6119,H05-1044,0,0.146605,"Sentiment lexicons provide a valuable source of information about the prior affective orientation of words, oftentimes driven by theoretical approaches to emotion (Stone et al., 1962; Bradley et al., 1999). There are several freely available sentiment lexicons for English. One widely used lexicon is that of Hu and Liu (2004),3 which was created using a bootstrapping approach from WordNet and a corpus of product reviews. This is the lexicon that forms the basis of the experiments in the current paper and we return to it in § 3.1. Other available lexicons include the MPQA subjectivity lexicon (Wilson et al., 2005) which contains words and expressions manually annotated as positive, negative, both, or neutral. SentiWordnet (Esuli and Sebastiani, 2006) contains each synset of the English WordNet annotated with scores representing the sentiment orientation as being positive, negative, or objective. The So-Cal (Taboada et al., 2011) English sentiment lexicon contains separate lexicons of verbs, nouns, adjectives, and adverbs. The words were manually labeled on a scale from extremely positive (+5) to extremely negative (−5), and all words labeled as neutral (0) were excluded from the lexicons. While no high"
W19-6119,C18-1074,0,0.0201918,"xicons outperform semi-supervised lexicon approaches and that their model is more robust to domain shifts than machine learning models. Lexicons in neural approaches The general tendency in NLP when using neural approaches is to perform end-to-end learning without using external knowledge sources, relying instead solely on what can be inferred from (often pre-trained) word embeddings and the training corpus itself. This is also the case for neural sentiment modeling. However, there have been some attempts to include external knowledge like lexicon features into such models (Teng et al., 2016; Zou et al., 2018; Lei et al., 2018a; Bao et al., 2019a). One notable example is the work of Shin et al. (2017) where several approaches are tested for how to incorporate lexicon information into a CNN for sentiment classification on the SemEval 2016 Task 4 dataset and the Stanford Sentiment Treebank (SST). Shin et al. (2017) create feature vectors that encode the positive or negative polarity values of words across a broad selection of different sentiment lexicons available for English. These word-level sentiment-score vectors are then combined with standard word embeddings in different ways in the CNN: throu"
W19-6205,P17-1080,0,0.0197037,"ot too far in the past, work on evaluating shallow sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the one"
W19-6205,K18-2005,0,0.0260101,"ieved (then) state-of-theart results by plugging generated fixed-length vectors into downstream classifiers. Another system that represented a significant innovation was BERT (Devlin et al., 2018). BERT introduced a language modelling variant, dubbed masked language modelling, that allowed them to use transformer encoders as their underlying encoding mechanism. 2.2 Multilingual pre-training Multilingual variants of pre-trained encoders that provide contextual representations for nonEnglish languages have also been studied; there is, however, some diversity in precisely how they are generated. Che et al. (2018) provide ELMo models (Fares et al., 2017) for 44 languages; all of these were trained on data provided as part of the CoNLL 2018 shared task on dependency parsing Universal Dependencies treebanks (Zeman et al., 2018). This makes ‘multilingual’ a bit of a misnomer: whilst this is the most obvious approach to multilingual support, these models are all monolingual. This also leads to other issues downstream, such as a complete inability to deal with true multilingual phenomena like code-switching. Throughout this text, however, when not specifically referring to ELMo, our use of the term ‘multili"
W19-6205,W19-4828,0,0.0120124,"ask to the multilingual domain; the authors evaluate sentence representations derived by mapping non-English representations to an English representation space. 2.4 BERTology Relevant to the probing theme of this paper is the sudden recent growth in papers studying precisely what is retained with the internal representations of pre-trained encoders like BERT. These include, for instance, analyses of BERT’s attentions heads, such as Michel et al. (2019), where the authors prune heads, often reducing certain layers to single heads, without a significant drop in performance in certain scenarios. Clark et al. (2019) provide a perhead analysis and attempt to quantify what information each head retains; they discover that specific aspects of syntax are well-encoded per head, and find heads that correspond to certain linguistic properties, such as heads that attend to direct objects of verbs. Other papers provide analyses of BERT’s layers, such as Tenney et al. (2019), who discover that BERT’s layers roughly correspond to the notion of the classical ‘NLP pipeline’, with lower level tasks such as tagging lower down the layer hierarchy. Hewitt and Manning (2019) define a structural probe over BERT representat"
W19-6205,L18-1269,0,0.0216192,"and words masked in both source and target sentences words are predicted using context from both. The authors here also use their own implementation of BPE – FastBPE, for which they provide a vocabulary of around 120K entries. This vocabulary is shared across all of the languages and thus improves the alignment of embedded spaces, as shown in Lample et al. (2017). 2.3 On evaluation Evaluation of contextual representations goes beyond merely deep representations; not too far in the past, work on evaluating shallow sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained"
W19-6205,P18-1198,0,0.295024,"tions could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the ones we do, on a set of ‘sixteen diverse probing tasks’. (Tenney et al., 2018) probe deep pre-trained encoders for sentence structure. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare repre"
W19-6205,D18-1269,0,0.404632,"tions could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the ones we do, on a set of ‘sixteen diverse probing tasks’. (Tenney et al., 2018) probe deep pre-trained encoders for sentence structure. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare repre"
W19-6205,N19-1423,0,0.0796501,"Missing"
W19-6205,W17-0237,1,0.827386,"plugging generated fixed-length vectors into downstream classifiers. Another system that represented a significant innovation was BERT (Devlin et al., 2018). BERT introduced a language modelling variant, dubbed masked language modelling, that allowed them to use transformer encoders as their underlying encoding mechanism. 2.2 Multilingual pre-training Multilingual variants of pre-trained encoders that provide contextual representations for nonEnglish languages have also been studied; there is, however, some diversity in precisely how they are generated. Che et al. (2018) provide ELMo models (Fares et al., 2017) for 44 languages; all of these were trained on data provided as part of the CoNLL 2018 shared task on dependency parsing Universal Dependencies treebanks (Zeman et al., 2018). This makes ‘multilingual’ a bit of a misnomer: whilst this is the most obvious approach to multilingual support, these models are all monolingual. This also leads to other issues downstream, such as a complete inability to deal with true multilingual phenomena like code-switching. Throughout this text, however, when not specifically referring to ELMo, our use of the term ‘multilingual’ is inclusive of ELMo’s quasi-multi"
W19-6205,W18-2501,0,0.0537911,"Missing"
W19-6205,N19-1419,0,0.0283279,"significant drop in performance in certain scenarios. Clark et al. (2019) provide a perhead analysis and attempt to quantify what information each head retains; they discover that specific aspects of syntax are well-encoded per head, and find heads that correspond to certain linguistic properties, such as heads that attend to direct objects of verbs. Other papers provide analyses of BERT’s layers, such as Tenney et al. (2019), who discover that BERT’s layers roughly correspond to the notion of the classical ‘NLP pipeline’, with lower level tasks such as tagging lower down the layer hierarchy. Hewitt and Manning (2019) define a structural probe over BERT representations, that extracts notions of syntax that correspond strongly to linguistic notions of dependency syntax. 3 Corpora 3.1 Probing Our data consists of training, development and test splits for 9 linguistic tasks, that can broadly be grouped into surface, syntactic and semantic tasks. These are the same as the ones described in Conneau et al. (2018a), with minor modifications. Due to the differences in corpus domain, we alter some of their word-frequency parameters. We also exclude the top constituent (TopConst) task; we noticed that Wikipedia tend"
W19-6205,P18-1031,0,0.0435359,"Missing"
W19-6205,J06-4003,0,0.029126,"cludes morphologically agglutinative, fusional and (relatively) isolating languages, and it includes two scripts, Latin and Cyrillic. The languages also represent three families (IndoEuropean, Turkic and Uralic). We build our probing datasets using the relevant language’s Wikipedia dump as a corpus. Our motivation for doing so was that it a freely available corpus for numerous languages, large enough to extract the sizeable corpora that we need. Specifically, we use Wikipedia dumps (dated 2019-0201), which we process using the WikiExtractor utility1 . Preprocessing We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. Having segmented our data, we used the Moses (Koehn et al., 2007) tokeniser for the appropriate language, falling back to English tokenisation when unavailable. Next, we obtained dependency parses for our sentences, again using the UDPipe toolkit’s pretrained models, trained on Universal Dependencies treebanks (Nivre et al., 2015). We then processed these dependency parsed corpora to extract the appropria"
W19-6205,P07-2045,0,0.00448085,"t language’s Wikipedia dump as a corpus. Our motivation for doing so was that it a freely available corpus for numerous languages, large enough to extract the sizeable corpora that we need. Specifically, we use Wikipedia dumps (dated 2019-0201), which we process using the WikiExtractor utility1 . Preprocessing We use the Punkt tokeniser (Kiss and Strunk, 2006) to segment our Wikipedia dumps into discrete sentences. For Russian, which lacked a Punkt tokenisation model, we used the UDPipe (Straka and Strakov´a, 2017) toolkit to perform segmentation. Having segmented our data, we used the Moses (Koehn et al., 2007) tokeniser for the appropriate language, falling back to English tokenisation when unavailable. Next, we obtained dependency parses for our sentences, again using the UDPipe toolkit’s pretrained models, trained on Universal Dependencies treebanks (Nivre et al., 2015). We then processed these dependency parsed corpora to extract the appropriate sentences; while in principle, each task was meant to have 120K sentences, with 100K/10K/10K training/validation/test splits, often, for the rarer linguistic phenomena, we ran out of source data, in particular with Turkish and Finnish, although to a smal"
W19-6205,N19-1112,0,0.111492,"classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the ones we do, on a set of ‘sixteen diverse probing tasks’. (Tenney et al., 2018) probe deep pre-trained encoders for sentence structure. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated corpora. A visible limitation of the datasets provided by these probing tasks is that most of them were created with the idea of evaluating representations built for English language data. Within the realm of evaluating multilingual sentence repre"
W19-6205,A94-1016,0,0.0547331,"Missing"
W19-6205,N18-1202,0,0.641859,"cally, we probe each layer of a multiple monolingual RNN-based ELMo models, the transformer-based BERT’s cased and uncased multilingual variants, and a variant of BERT that uses a cross-lingual modelling scheme (XLM). 1 Introduction Recent trends in NLP have demonstrated the utility of pre-trained deep contextual representations in numerous downstream NLP tasks, where they have almost consistently resulted in significant performance improvements. Detailed evaluations have naturally followed: these have either been follow-up works to papers describing contextual representation systems, such as Peters et al. (2018b), or novel works evaluating a broad class of encoders on a broad variety of tasks (Perone et al., 2018). This paper is an example of the latter sort; we perform a comprehensive, large-scale evaluation of what linguistic phenomena these sequential encoders capture across a diverse set of languages. This has often been referred to in the literature as probing; we use this terminology throughout this work. Briefly, our goals are to probe our encoders in a multilingual setting – i.e., we use a series of probing tasks to quantify what sort of linguistic information our encoders retain, and how th"
W19-6205,N19-1329,0,0.0311831,"Missing"
W19-6205,P16-1162,0,0.00704079,"akes ‘multilingual’ a bit of a misnomer: whilst this is the most obvious approach to multilingual support, these models are all monolingual. This also leads to other issues downstream, such as a complete inability to deal with true multilingual phenomena like code-switching. Throughout this text, however, when not specifically referring to ELMo, our use of the term ‘multilingual’ is inclusive of ELMo’s quasi-multilingualism. This is contrasted with BERT’s approach to (true) multilingualism, which trains a single model that can handle all languages. The authors use WordPiece, a variant of BPE (Sennrich et al., 2016), for tokenisation, using a 110K-size vocabulary, and proceed to train a single gigantic model; they perform exponentially smoothed weighting of their data to avoid biasing their model towards better-resourced languages. Finally, XLM (Lample and Conneau, 2019) is another cross-lingual encoder based on BERT that implements a number of modifications. Along with BERT’s masked language modeling or Cloze task-based modelling (Devlin et al., 2018; Taylor, 1953), XLM training uses another similar objective during training that the authors call translation language modeling. Here, two parallel sentenc"
W19-6205,D16-1159,0,0.0132289,"g shallow sentence representations was encouraged by the release of the SentEval toolkit (Conneau and Kiela, 2018), which provided an easy-to-use framework that sentence representations could be ‘plugged’ into, for rapid downstream evaluation on numerous tasks: these include several classification tasks, textual entailment and similarity tasks, a paraphrase detection task, and caption/image retrieval tasks. Relevant to our paper is Conneau et al.’s (2018a) set of ‘probing tasks’, a variant on the theme of diagnostic classification (Hupkes et al., 2017; Belinkov et al., 2017; Adi et al., 2016; Shi et al., 2016), that would attempt to quantify precisely what sort of linguistic information was being retained by sentence representations. Based in part on Shi et al. (2016), Conneau et al. (2018a) focus on evaluating representations for English; they provide Spearman correlations between the performance of a particular representation mechanism on being probed for specific linguistic properties, and the downstream performance on a variety of NLP tasks. Along similar lines, and contemporaneously with this work, Liu et al. (2019) probe similar deep pre-trained to the ones we do, on a set of ‘sixteen diverse"
W19-6205,K17-3009,0,0.0651014,"Missing"
W19-6205,P19-1452,0,0.0364986,"Missing"
W19-6205,N18-1101,0,0.0204464,"ture. On a different note, Saphra and Lopez (2018) present a CCA-based method to compare representation learning dynamics across time and models, without explicitly requiring annotated corpora. A visible limitation of the datasets provided by these probing tasks is that most of them were created with the idea of evaluating representations built for English language data. Within the realm of evaluating multilingual sentence representations, Conneau et al. (2018b) describe the XNLI dataset, a set of translations of the development and test portions of the multi-genre MultiNLI inference dataset (Williams et al., 2018). This, in a sense, is an extension of a predominantly monolingual task to the multilingual domain; the authors evaluate sentence representations derived by mapping non-English representations to an English representation space. 2.4 BERTology Relevant to the probing theme of this paper is the sudden recent growth in papers studying precisely what is retained with the internal representations of pre-trained encoders like BERT. These include, for instance, analyses of BERT’s attentions heads, such as Michel et al. (2019), where the authors prune heads, often reducing certain layers to single hea"
W19-6205,K18-2001,0,0.0461695,"Missing"
