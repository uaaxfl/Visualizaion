2016.gwc-1.55,H93-1061,0,0.0220241,"Missing"
2016.gwc-1.55,R15-1077,1,0.425285,"Missing"
2018.gwc-1.33,N15-1165,0,0.0884121,"use the dependency relation ‘iobj’. In order to minimize some errors we enforced a condition that the dependency word should be a noun. 6 Experiments and Results In this section we describe the experimental set up and the results. Corpora preparation. The corpora that the algorithms for word embeddings are trained on can contain either natural language text (e.g. Wikipedia or newswire articles) or artificially generated pseudo texts. Such pseudo texts can be the output from the Random Walk algorithm, when it is set to the mode of selecting sequences of nodes from a knowledge graph (KG) — see (Goikoetxea et al., 2015) for generation of pseudo corpora from a WordNet knowledge graph and (Ristoski and Paulheim, 2016) for generation of pseudo corpora from RDF knowledge graphs such as DBPedia, GeoNames, FreeBase. Here we report results only for knowledge graphs based on WordNet and its extensions. The corpus for training of the embeddings reported here consists of two parts: (1) pseudo corpus generated over WordNet (PCWN); and (2) real text corpora (RTC). PCWN is used to ensure that the embeddings represent features extracted from the knowledge within the WordNet. RTC is used to represent relevant contexts for"
2018.gwc-1.33,P14-2050,0,0.0408666,"aper presents an approach for learning features by subtomic semantic representation. It is useful for addition of syntagmatic relations to WordNet. Our longterm plan is to design a learning approach for each semantic argument of predicates in a logical form. The results here are the first steps in this direction. In future we plan to do the following: (1) To include more arguments in the learning process like arguments of relational nouns and adjectives. They will impose mutual constraints on the learned features; (2) To experiment with different algorithms for learning of embeddings such as (Levy and Goldberg, 2014), where it is possible to select arbitrary contexts. Such contexts could be more appropriate for grammatical role embeddings learning; (3) To improve sense annotation in order to respectively improve sense embeddings; (4) To evaluate the subatom embeddings in other tasks such as coreference resolution, neural network word sense disambiguation; (5) To perform tuning to the linguistic knowledge already represented in WordNet, FrameNet and other lexical resources as well as manually annotated corpora, by techniques similar to retrofitting; and (6) To develop compositional semantics over this repr"
2018.gwc-1.33,lemnitzer-etal-2008-enriching,0,0.638361,"ted and the results are discussed. The last section concludes the paper. 2 Related Work The success of KWSD approaches apparently depends on the quality of the knowledge graph – whether the knowledge represented in terms of nodes and relations (arcs) between them is sufficient for the algorithm to pick the correct senses of ambiguous words. Several extensions of the knowledge graph constructed on the basis of WordNet have been proposed and implemented. With respect to the extension of WordNet with syntagmatic information there exist many works such as (Bentivogli and Pianta, 2004) and (Lothar Lemnitzer and Gupta, 2008). Here we present in more detail only one approach similar to ours. It is described in Agirre and Martinez (2002) and explores the extraction of syntactically supported semantic relations from manually annotated corpora. In this line of research SemCor — (Miller et al., 1993), being a semantically annotated corpus, was processed with the MiniPar dependency parser and the subject-verb and object-verb relations were consequently extracted. The new relations were represented on several levels: as word-to-class and class-to-class relations. The extracted selectional relations were then added to Wo"
2018.gwc-1.33,H93-1061,0,0.0343473,"the algorithm to pick the correct senses of ambiguous words. Several extensions of the knowledge graph constructed on the basis of WordNet have been proposed and implemented. With respect to the extension of WordNet with syntagmatic information there exist many works such as (Bentivogli and Pianta, 2004) and (Lothar Lemnitzer and Gupta, 2008). Here we present in more detail only one approach similar to ours. It is described in Agirre and Martinez (2002) and explores the extraction of syntactically supported semantic relations from manually annotated corpora. In this line of research SemCor — (Miller et al., 1993), being a semantically annotated corpus, was processed with the MiniPar dependency parser and the subject-verb and object-verb relations were consequently extracted. The new relations were represented on several levels: as word-to-class and class-to-class relations. The extracted selectional relations were then added to WordNet and used in the WSD task. The main differences with the approach described here are as follows: we used a bigger set of relations (since it includes also indirectobject-to-verb relations). Apart from that, the new relations reported in this paper are not added as select"
2018.gwc-1.33,N16-1054,0,0.0283443,"esentation will facilitate the selectional restrictions of the corresponding predicates. This is not possible to be done directly by the plf model. One direction of future research is to combine both approaches. This could be done by learning argument matrices that work in combination with the argument vector and the grammatical role vector. Our intuition is that such matrices could not be attached to a specific lexical unit, but to a class of lexical units. There is also a huge number of works on extending world knowledge oriented graphs with new relations (see (Minervini et al., 2015), and (Nguyen et al., 2016) among others). The main difference in our case is that we do not learn instances of the required relations from the corpora, but we learn semantic restrictions over the arguments of the relations. The candidate relations are generated from knowledge base itself (WordNet here). 3 WordNet Extensions with New Relations As mentioned above, in our previous works we extended PWN with syntagmatic relations using semantically annotated corpora such as SemCor. The idea was that if there is a subject-verb syntactic relation in the corpus, and the related verb and noun are manually annotated with synset"
2018.gwc-1.33,P14-1009,0,0.072789,"erb. Such an explicit embedding of the valency frame elements has many potential applications. In this work we exploit these embeddings for adding new syntagmatic relations to PWN with the aim to improve applications such as KWSD. Evaluation in the paper is performed by automatically extending WordNet with ranked relations within the context of KWSD. We show that adding higher ranked relations improves the performance of KWSD. Further we provide manual inspection and validation of the new relations that also supports the feasibility of our approach. Our approach is similar to the approach of (Paperno et al., 2014) who started with the lexical function model where each functional lexical item is represented via n+1 tensor if it is an n-ary functor. In order to escape from using tensor 1 By subatom level we mean the arguments of a predicate. with three and more dimensions they proposed a representation where for each argument a matrix is used. Each matrix determines the incorporation of the corresponding argument semantics into the compositional semantics of the whole phrase. Our method is also similar to the other popular methods for relation extraction. The main difference is that we do not implement r"
2018.gwc-1.33,R15-1077,1,0.903812,"l role embeddings in order to add new syntagmatic relations to WordNet. The evaluation of the new relations quality has been done extrinsically through the Knowledge-based Word Sense Disambiguation task. 1 Introduction In this paper we present an approach to extending the knowledge graph, based on Princeton English WordNet (PWN) — (Fellbaum, 1998) — with syntagmatic relations. Our aim is to improve the knowledge-based word sense disambiguation (KWSD). In several papers we showed that adding syntagmatic relations from syntactic and semantic annotated corpora improves the performance of KWSD — (Simov et al., 2015) and (Simov et al., 2016). The main types of syntagmatic relations extracted from these corpora are the ones corresponding to the grammatical roles: verb-subject (has-subj), verb-direct object (has-dobj) and verb-indirect object (has-iobj). Although we managed to extract good sets of new relations, the main problem is that corpora annotated with semantic and syntactic information contain only a fraction of all the possible syntagmatic relations. The inheritance over the hierarchies of PWN is problematic because the hierarchies of PWN are not monotonic. For that reason, in this paper we use fea"
2019.gwc-1.37,lopez-de-lacalle-etal-2014-predicate,0,0.0518309,"Missing"
2019.gwc-1.37,E12-1059,0,0.231359,"Missing"
2019.gwc-1.37,kingsbury-palmer-2002-treebank,0,0.479095,"Missing"
2019.gwc-1.37,2018.gwc-1.8,0,0.303643,"irections: (1) the number of senses for lemmas that are already in BTBWN; and (2) the instances of the concepts. However, the BulTreeBank appeared to contain only a small number of named entities. Thus the extension was insufficient and it required the use of the Wikipedia URIs and DBpedia classes for the missing NEs. The authors also report on the automatic extension of BTB-WN with automatically derived Bulgarian synsets on the basis of the English ones through the usage of the English Wiktionary. After manual checking of around 11000 suggestions, BTB-WN was enriched by around 5000 synsets. (McCrae, 2018) reports on the manual mapping of the Princeton WordNet (PWN) instances to the English Wikipedia. He proposes that a subset of PWN instance concept synsets is automatically linked and manually evaluated on Wikipedia articles in order to “provide a gold standard for link discovery”. This is done by matching PWN lemmas to all Wikipedia titles containing the lemma. Then by using a special tool, human annotators evaluate the links. This tool shows the PWN definitions and the first paragraph from the Wikipedia article so the annotators are able to confirm or reject the mapping. The same paper also"
2019.gwc-1.37,simov-etal-2004-language,1,0.717897,"one lemma, this entry will be added to several groups if there are appropriate Wikipedia pages. In the figure we can observe two expanded groups — one for the lemma “Iceberg” and one for the lemma “Aquarium”. For each page the layout shows the Bulgarian title of the page, then the English title (if there is a link to an English Wikipedia page). Thus, the annotator2 could understand the sense described by the Wikipedia article without expanding the structure of the page. Of course, if necessary, the annotator could read more from the content of the page. For each entry 1 For a description see (Simov et al., 2004b). The system could be downloaded from http://bultreebank. org/en/clark/. 2 We call the people that manually establish the mapping between the two resources annotators, but a more appropriate term is necessary such as mappers or knowledge relaters. Figure 1: Representation of the groups of matched Wikipedia pages and BTB-WN synsets (represented via <entry&gt; element.) the layout shows the PWN identifier; the mapping to Wikipedia page (if such has been selected); the list of lemmas for the synset; and finally the definition related to the synset. Again, the annotator might read the important inf"
2020.lrec-1.395,I13-1057,0,0.0253544,"reated dynamically for semantic relationship annotation. scribed our methodology in Section 3, we further elaborate on the challenges of sense annotation in Section 4. We evaluate the datasets in Section 5 and finally, conclude the paper in Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of m"
2020.lrec-1.395,erjavec-fiser-2006-building,0,0.137609,"Missing"
2020.lrec-1.395,E12-1059,0,0.155275,"geneity in content, which makes aligning information across resources and languages a challenging task. Word sense alignment (WSA) is a more specific task of linking dictionary content at sense level which has been proved to be beneficial in various NLP tasks, such as wordsense disambiguation (Navigli and Ponzetto, 2012), semantic role labeling (Palmer, 2009) and information extraction (Moro et al., 2013). Moreover, combining LSRs can enhance domain coverage in terms of the number of lexical items and types of lexical-semantic information (Shi and 1 Mihalcea, 2005; Ponzetto and Navigli, 2010; Gurevych et al., 2012). Given the current progress of artificial intelligence and the usage of data to train neural networks, annotated data with specific features play a crucial role to tackle data-driven challenges, particularly in NLP. In recent years, a few efforts have been made to create gold-standard dataset, i.e., a dataset of instances used for learning and fitting parameters, for aligning senses across monolingual resources including collaboratively-curated ones such as Wikipedia2 , and expert-made ones such as WordNet. However, the previous work is limited to a handful of languages and much of it is not"
2020.lrec-1.395,W97-0800,0,0.569498,"2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen Sprache (Klein and Geyken, 2010)) (Henrich et al., 2014). Gurevych et al. (2012) present UKB–a large-scale lexical-semantic resource containing pairwise sense alignments between a subset of nine resources in English and German which are mapped to a uniform representation. For Danish, aligning senses across modern lexical resources has been carried out in several projects in recent years (Pedersen et al.,"
2020.lrec-1.395,W14-0109,0,0.0200074,"resent a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen Sprache (Klein and Geyken, 2010)) (Henrich et al., 2014). Gurevych et al. (2012) present UKB–a large-scale lexical-semantic resource containing pairwise sense alignments between a subset of nine resources in English and German which are mapped to a uniform representation. For Danish, aligning senses across modern lexical resources has been carried out in several projects in recent years (Pedersen et al., 2018), and a next natural step is to link these to historical Danish dictionaries. 3 https://www.wiktionary.org/ Pedersen et al. (2009) describe the semi-automatic compilation of a WordNet for Danish, DanNet, based on a monolingual dictionary, the"
2020.lrec-1.395,bel-etal-2000-simple,1,0.672745,"Missing"
2020.lrec-1.395,Q13-1013,0,0.110061,"nment of LSRs and applied it to the production of a three-way alignment of the English WordNet, Wikipedia and Wiktionary. Niemann and Gurevych (2011) propose a threshold-based Personalized PageRank method for extracting a set of Wikipedia articles as alignment candidates and automatically aligning them with WordNet synsets. This method yields a sense inventory of higher coverage in comparison to taxonomy mapping techniques where Wikipedia categories are aligned to WordNet synsets (Ponzetto and Navigli, 2009). Matuschek and Gurevych present the Dijkstra-WSA algorithm as a graph-based approach (Matuschek and Gurevych, 2013) and a machine learning approach where features such as sense distances and gloss similarities are used for the task of WSA (Matuschek and Gurevych, 2014). It should be noted that all of these approaches produce results that are of lower reliability than gold standard datasets such as the ones presented in this paper. 3233 3. Methodology The main goal of the current study is to provide semantic relationships between two sets of senses for the same lemmas in two monolingual dictionaries. As an example, Figure 1 illustrates the senses for the entry “clog” (verb) in the English WordNet (Miller, 1"
2020.lrec-1.395,C14-1025,0,0.31488,"ose a threshold-based Personalized PageRank method for extracting a set of Wikipedia articles as alignment candidates and automatically aligning them with WordNet synsets. This method yields a sense inventory of higher coverage in comparison to taxonomy mapping techniques where Wikipedia categories are aligned to WordNet synsets (Ponzetto and Navigli, 2009). Matuschek and Gurevych present the Dijkstra-WSA algorithm as a graph-based approach (Matuschek and Gurevych, 2013) and a machine learning approach where features such as sense distances and gloss similarities are used for the task of WSA (Matuschek and Gurevych, 2014). It should be noted that all of these approaches produce results that are of lower reliability than gold standard datasets such as the ones presented in this paper. 3233 3. Methodology The main goal of the current study is to provide semantic relationships between two sets of senses for the same lemmas in two monolingual dictionaries. As an example, Figure 1 illustrates the senses for the entry “clog” (verb) in the English WordNet (Miller, 1995) (left) and the Webster’s Dictionary 1913 (Webster and Slater, 1828) (right). For further clarification, we provide two case studies of Danish and Ita"
2020.lrec-1.395,2018.gwc-1.8,1,0.745977,"Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 19"
2020.lrec-1.395,I11-1099,0,0.152048,"he recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of t"
2020.lrec-1.395,miller-gurevych-2014-wordnet,0,0.125981,"Missing"
2020.lrec-1.395,P06-1014,0,0.254458,"ies, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp and Feldweg, 1997) with the German Wiktionary (Henrich et al., 2011), with the German Wikipedia (Henrich et al., 2012) and with the Digital Dictionary of the German Language (Digitales W¨orterbuch der Deutschen S"
2020.lrec-1.395,W11-0122,0,0.627617,"ally, conclude the paper in Section 6. 2. Related work Aligning senses across lexical resources has been attempted in several lexicographical milieus over the recent years. Such resources mainly include open-source dictionaries, WordNet and collaboratively-curated resources, such as Wikipedia. The latter has been shown to be reliable resources to construct accurate sense classifiers (Dandala et al., 2013). There has been a significant body of research in aligning English resources, particularly, Princeton WordNet with Wikipedia (including (Ruiz-Casado et al., 2005; Ponzetto and Navigli, 2010; Niemann and Gurevych, 2011; McCrae, 2018)), with the Longman Dictionary of Contemporary English and with Roget’s thesaurus (Kwong, 1998), with Wiktionary3 (Meyer and Gurevych, 2011) or with the Oxford Dictionary of English (Navigli, 2006). Meyer and Gurevych (2011) also present a manually-annotated dataset for WSA between the English WordNet and Wiktionary. On the other hand, there are a fewer number of manually aligned monolingual resources in other languages. For instance, there have been considerable efforts in aligning lexical semantic resources (LSRs) in German, particularly, the GermaNet–the German Wordnet (Hamp"
2020.lrec-1.395,P10-1154,0,0.289699,"nces in structure and heterogeneity in content, which makes aligning information across resources and languages a challenging task. Word sense alignment (WSA) is a more specific task of linking dictionary content at sense level which has been proved to be beneficial in various NLP tasks, such as wordsense disambiguation (Navigli and Ponzetto, 2012), semantic role labeling (Palmer, 2009) and information extraction (Moro et al., 2013). Moreover, combining LSRs can enhance domain coverage in terms of the number of lexical items and types of lexical-semantic information (Shi and 1 Mihalcea, 2005; Ponzetto and Navigli, 2010; Gurevych et al., 2012). Given the current progress of artificial intelligence and the usage of data to train neural networks, annotated data with specific features play a crucial role to tackle data-driven challenges, particularly in NLP. In recent years, a few efforts have been made to create gold-standard dataset, i.e., a dataset of instances used for learning and fitting parameters, for aligning senses across monolingual resources including collaboratively-curated ones such as Wikipedia2 , and expert-made ones such as WordNet. However, the previous work is limited to a handful of language"
2020.lrec-1.395,roventini-ruimy-2008-mapping,0,0.0409542,"the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the basic structures representing word-sense. SIMPLE contains 20,000 SemUs and we used the definitions of these SemUs for the task. Both lexicons share a set of common “base concepts” that provided the basis of a previous (semi-)automatic mapping of the two lexicons on the basis of their respective ontological organisations (Roventini et al., 2007; Roventini and Ruimy, 2008). Although this mapping did not make the five-fold distinction, i.e., exact, narrower, broader, related, and none, it did constitute a useful starting point and a basis for comparison for the task. The teams that had originally compiled IWN and SIMPLE shared many members in common and so, the definitions for corresponding senses across the two lexicons are sometimes very similar or differ solely on the basis of an extra clause. This made it easy to determine, in many cases, if two senses were ‘exact’ matches or if one was ‘broader’ or ‘narrower’ than the other by just comparing strings. The ap"
2020.lrec-1.395,roventini-etal-2000-italwordnet,0,0.0791879,"Missing"
2020.lrec-1.395,roventini-etal-2002-integrating,0,0.0935961,"n incapacitated adult). An opposite case where the historical sense is ‘narrower’ than the modern one can be illustrated by the adjective spids (‘sharp’) where ODS describes two specific senses, one about sound and another one about smell, while DDO merges the two senses into one: ‘pungent in an unpleasant way (about smell, taste or sound)’. 4.2. ItalWordNet and SIMPLE Regarding Italian, the team at ILC-CNR chose ItalWordNet (IWN) and SIMPLE, two Italian language lexical resources which had been previously developed in the institute. The former, IWN, is a lexical semantic network for Italian (Roventini et al., 2002) which is part of the WordNet family (Miller, 1995). As such it is organised around the notion of a synset of word senses and the network structure based on lexical-semantic relations which hold between senses across synsets. The 50,000 Italian synsets contained in IWN are linked to the Princeton Wordnet. The latter resource, SIMPLE, constitutes the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the"
2020.lrec-1.395,P07-2041,0,0.0611602,"ce, SIMPLE, constitutes the semantic level of a quadripartite Italian lexicon. Its structure is inspired by Generative Lexicon theory (Pustejovsky, 1995) and in particular the notion of qualia structure which is used to organise the Semantic Units (SemUs) which constitute the basic structures representing word-sense. SIMPLE contains 20,000 SemUs and we used the definitions of these SemUs for the task. Both lexicons share a set of common “base concepts” that provided the basis of a previous (semi-)automatic mapping of the two lexicons on the basis of their respective ontological organisations (Roventini et al., 2007; Roventini and Ruimy, 2008). Although this mapping did not make the five-fold distinction, i.e., exact, narrower, broader, related, and none, it did constitute a useful starting point and a basis for comparison for the task. The teams that had originally compiled IWN and SIMPLE shared many members in common and so, the definitions for corresponding senses across the two lexicons are sometimes very similar or differ solely on the basis of an extra clause. This made it easy to determine, in many cases, if two senses were ‘exact’ matches or if one was ‘broader’ or ‘narrower’ than the other by ju"
2020.lrec-1.395,2019.gwc-1.37,1,0.782709,"onary, the Danish Dictionary (Den Danske Ordbog (DDO)). Later, the semantic links between these two resources facilitated the compilation of a comprehensive thesaurus (Den Danske Begrebsordbog) (Nimb et al., 2014). The semantic links between thesaurus and dictionary made it possible to combine verb groups and dictionary valency information, used as input for the compilation of the Danish FrameNet Lexicon (Nimb, 2018). Furthermore, they constitute the basis for the automatically integrated information on related words in DDO, on the fly for each dictionary sense (Nimb et al., 2018). Similarly, Simov et al. (2019) report the manual mapping of the Bulgarian Word-Net BTB-WN with the Bulgarian Wikipedia. Given the amount of the effort required to construct and maintain expert-made resources, various solutions have been proposed to automatically link and merge existing LSRs at different levels. LSRs being very diverse in domain coverage (Meyer, 2010; Burgun and Bodenreider, 2001), previous works have focused on methods to increase domain coverage, enrich sense representations and decrease sense granularity (Miller, 2016). Miller and Gurevych (2014) describe a technique for constructing an n-way alignment o"
2020.lrec-1.571,N19-4010,0,0.0438138,"newswire articles. We plan to use this annotation in our work on co-reference resolution. Table 4. presents the change in the number of documents, in both training and test sets, after annotation correction, deduplication, and randomization. 4. Methods and Experiments In this section, we first present our approach for training the neural network models, and then — the results from the experiments. 4.1. Methods For the experimental work, Flair1 was used, an NLP library implemented by Zalando Research on top of PyTorch2 . Apart from their own pre-trained Flair contextualized string embeddings (Akbik et al., 2019b), the library provides access to many other state-of-the-art language models, such as FastText (Grave et al., 2018), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018) and the Transformers provided by HuggingFace3 : BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet . Stacking the embeddings is one of the most important features of the library and the functionality is used in the experiments to concatenate language models together. The developers of the library claim that this method often gives the best results and lately has become a common technique in sequence labeling models (Grave e"
2020.lrec-1.571,N19-1078,0,0.0498592,"newswire articles. We plan to use this annotation in our work on co-reference resolution. Table 4. presents the change in the number of documents, in both training and test sets, after annotation correction, deduplication, and randomization. 4. Methods and Experiments In this section, we first present our approach for training the neural network models, and then — the results from the experiments. 4.1. Methods For the experimental work, Flair1 was used, an NLP library implemented by Zalando Research on top of PyTorch2 . Apart from their own pre-trained Flair contextualized string embeddings (Akbik et al., 2019b), the library provides access to many other state-of-the-art language models, such as FastText (Grave et al., 2018), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018) and the Transformers provided by HuggingFace3 : BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet . Stacking the embeddings is one of the most important features of the library and the functionality is used in the experiments to concatenate language models together. The developers of the library claim that this method often gives the best results and lately has become a common technique in sequence labeling models (Grave e"
2020.lrec-1.571,Q17-1010,0,0.0358712,"or morphologically rich languages like Bulgarian. Their work was mostly devoted to constructing a set of orthographic and domain-specific features. Using gazetteers, local/nonlocal morphology, feature induction and mutual information in the form of unlabeled texts, they achieved F1=89.40. Simeonova et al. (2019) employ an LSTM-CRF architecture on top of a word embedding layer. The authors explore a rich feature set for the language, using the positionaware morphosyntactic tags proposed by Simov and Osenova (2004). The word embeddings used in these experiments are Bulgarian FastText Vectors by Bojanowski et al. (2017). The final vector representations of the words are formed by combining FastText with character embeddings, thus further improving the test scores by using POS and morphological representations. The best score achieved by this system is F1=92.20. The identification of named entity mentions in texts is often implemented using a sequence tagger, where each token is labeled with an IOB tag, indicating whether the token begins an NE — (B), whether it is inside of an NE (I), or whether it is outside of an NE (O). This type of annotation has been proposed for the first time at CoNLL-2003 (Tjong Kim"
2020.lrec-1.571,R09-1022,1,0.738097,"and for the Nord Stream corpus it was F1=89.6. Since the test data contains both topics, in table 5 we report the mean score on 4647 the whole testset F1=88.55. Piskorski et al. (2019) report a relaxed evaluation metric, which means that an entity mentioned in a given document is considered to be extracted correctly if the system response includes at least one annotation of a named mention of this entity (regardless of whether the extracted mention was in its base form). All the other systems reported in this paper, including ours, use the strict evaluation introduced at the SemEval’13 task. Georgiev et al. (2009) employ a rich set of features in their solution. At that time, using conditional random fields (CRFs) was the dominant approach to NER, but it required extensive and manual feature engineering, especially for morphologically rich languages like Bulgarian. Their work was mostly devoted to constructing a set of orthographic and domain-specific features. Using gazetteers, local/nonlocal morphology, feature induction and mutual information in the form of unlabeled texts, they achieved F1=89.40. Simeonova et al. (2019) employ an LSTM-CRF architecture on top of a word embedding layer. The authors e"
2020.lrec-1.571,L18-1550,0,0.101823,"nge in the number of documents, in both training and test sets, after annotation correction, deduplication, and randomization. 4. Methods and Experiments In this section, we first present our approach for training the neural network models, and then — the results from the experiments. 4.1. Methods For the experimental work, Flair1 was used, an NLP library implemented by Zalando Research on top of PyTorch2 . Apart from their own pre-trained Flair contextualized string embeddings (Akbik et al., 2019b), the library provides access to many other state-of-the-art language models, such as FastText (Grave et al., 2018), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018) and the Transformers provided by HuggingFace3 : BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet . Stacking the embeddings is one of the most important features of the library and the functionality is used in the experiments to concatenate language models together. The developers of the library claim that this method often gives the best results and lately has become a common technique in sequence labeling models (Grave et al., 2018). The pretrained language models used for the embedding layer are described in the following paragraphs."
2020.lrec-1.571,R19-2008,1,0.77665,"ations. The best score achieved by this system is F1=92.20. The identification of named entity mentions in texts is often implemented using a sequence tagger, where each token is labeled with an IOB tag, indicating whether the token begins an NE — (B), whether it is inside of an NE (I), or whether it is outside of an NE (O). This type of annotation has been proposed for the first time at CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003). Here we follow this format of representation. Also, we focus only on the NER task, leaving NEL as a next step. In our experiments we use the setup proposed in Marinova (2019), with F1=96.29 on the BulTreeBank dataset. The model is explored further in Section 4. 3. Data Preparation The corpus we have used for the work presented here was annotated by our team in accordance with the guidelines for the Second Multilingual Named Entity Challenge in Slavic languages organized for the 7th Balto-Slavic NLP Workshop 2019 (Piskorski et al., 2019). The shared task covered four Slavic languages – Bulgarian, Czech, Polish, and Russian – with Bulgarian being the only one that does not express the grammatical function of a noun phrase with an inflectional ending, and at the same"
2020.lrec-1.571,D14-1162,0,0.0816397,"Missing"
2020.lrec-1.571,N18-1202,0,0.0190923,"ts, after annotation correction, deduplication, and randomization. 4. Methods and Experiments In this section, we first present our approach for training the neural network models, and then — the results from the experiments. 4.1. Methods For the experimental work, Flair1 was used, an NLP library implemented by Zalando Research on top of PyTorch2 . Apart from their own pre-trained Flair contextualized string embeddings (Akbik et al., 2019b), the library provides access to many other state-of-the-art language models, such as FastText (Grave et al., 2018), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018) and the Transformers provided by HuggingFace3 : BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet . Stacking the embeddings is one of the most important features of the library and the functionality is used in the experiments to concatenate language models together. The developers of the library claim that this method often gives the best results and lately has become a common technique in sequence labeling models (Grave et al., 2018). The pretrained language models used for the embedding layer are described in the following paragraphs. The first one is the Bulgarian flair-forward and backward mod"
2020.lrec-1.571,W19-3709,1,0.895419,"Missing"
2020.lrec-1.571,R19-1127,1,0.798121,"er, including ours, use the strict evaluation introduced at the SemEval’13 task. Georgiev et al. (2009) employ a rich set of features in their solution. At that time, using conditional random fields (CRFs) was the dominant approach to NER, but it required extensive and manual feature engineering, especially for morphologically rich languages like Bulgarian. Their work was mostly devoted to constructing a set of orthographic and domain-specific features. Using gazetteers, local/nonlocal morphology, feature induction and mutual information in the form of unlabeled texts, they achieved F1=89.40. Simeonova et al. (2019) employ an LSTM-CRF architecture on top of a word embedding layer. The authors explore a rich feature set for the language, using the positionaware morphosyntactic tags proposed by Simov and Osenova (2004). The word embeddings used in these experiments are Bulgarian FastText Vectors by Bojanowski et al. (2017). The final vector representations of the words are formed by combining FastText with character embeddings, thus further improving the test scores by using POS and morphological representations. The best score achieved by this system is F1=92.20. The identification of named entity mention"
2020.lrec-1.571,simov-etal-2002-building,1,0.392886,"al perspective. Recently, a shared task was launched for Slavic languages on NER and linking to the same real-world entity in a crosslingual context. The second edition in 2019 (Piskorski et al., 2019) included the following languages: Bulgarian, Czech, Polish, Russian. It focused on 5 types (or sorts and categories) of Named Entities (NEs): Person (PER), Location (LOC), Organization (ORG), Event (EVT), and Product (PRO). In the work reported here we exploit this newly constructed Bulgarian dataset for NER. The second dataset was extracted from the Bulgarian HPSG-based treebank — BulTreeBank (Simov et al., 2002). The named entities in BulTreeBank were syntactically annotated and the category for each NE was assigned to the highest node in the syntactic annotation. For the aims of this work, the categories were inherited to the token level. In BulTreeBank, only form categories were annotated: Person (PER), Location (LOC), Organization (ORG), and Other (OTH). The category Other covers a wide range of entity types like Event (EVT), Product (PRO), and some of their sub-types — different kinds of products, for example. The preparation of the Bulgarian corpus within a highly multilingual guidance environme"
2020.lrec-1.571,W03-0419,0,0.0389294,"Missing"
2020.tlt-1.14,2020.acl-main.255,0,0.0626717,"Missing"
2020.tlt-1.14,D18-1532,0,0.0355014,"Missing"
2020.tlt-1.14,2020.lrec-1.571,1,0.701361,"e have used the Universal Dependencies files for training the POS tagger, dependency parser (including labeled attachment), and sentence splitter, which depends on the parser module to correctly segment sentences.6 We present the accuracy results on the development and test sets in the next section. 2.2.2 Named entity recognition BulTreeBank has been annotated with information about Person, Location and Organization entities, but since NER data is not included in the UD version, we trained the NER module separately, after the POS tagger and parser. We also included data from the BSNLP corpus (Marinova et al., 2020), which has been originally compiled for a special task on NER and includes Event, Product and Other types, in addition to Person, Location and Organization. The two corpora were processed into the spaCy-readable IOB format, concatenated and shuffled, to balance them between the training (20803 sentences) and development (2312 sentences) portions. We provide evaluation metrics for NER in the next section. 2.2.3 Word sense disambiguation The final module included in the pipeline carries out word sense disambiguation (WSD). The spaCy framework does not provide off-the-shelf WSD functionality, th"
2020.tlt-1.14,E17-1010,0,0.0462055,"Missing"
2020.tlt-1.14,W16-0604,1,0.804501,"tization modules are implemented as rules in the CLaRK system, while POS tagging and dependency parsing are carried out by statistical models trained on annotated data. The Mate Tools2 software has been used for training the models, where the training data is an older version of the BTB treebank, which includes fewer sentences and differs in annotation types from the Universal Dependencies (UD) version. Later versions of this pipeline include additional processing modules (e.g. word sense disambiguation), which however are not trained on annotated data and are implemented in other frameworks (Simov et al., 2016). Currently there is no freely-available end-to-end processing pipeline for Bulgarian that meets the following criteria: • is implemented within a single framework; • includes semantic analysis capabilities (word sense disambiguation, named entity recognition); • achieves competitive accuracy scores; • affords processing speeds suitable for real applications; • can handle big volumes of data. This short paper reports on the implementation process of such a pipeline. The creation of a language processor for Bulgarian is inevitably related to the Bulgarian treebank — BulTreeBank (BTB), a version"
2020.tlt-1.14,K18-2011,0,0.0522856,"Missing"
2020.tlt-1.14,K18-2001,0,0.0328845,"Missing"
bekavac-etal-2004-making,tadic-2002-building,1,\N,Missing
bekavac-etal-2004-making,W03-1004,0,\N,Missing
E12-1050,P11-1070,0,0.0268914,"Missing"
E12-1050,A00-1031,0,0.691468,"Missing"
E12-1050,J95-4004,0,0.812378,"Missing"
E12-1050,P04-1015,0,0.118221,"Missing"
E12-1050,W02-1001,0,0.374605,"Missing"
E12-1050,W96-0102,0,0.115256,"Missing"
E12-1050,P08-2009,0,0.152721,"came in happy.’ (veselo is an adjective) and ‘The child came in happily.’ (it is an adverb); however, the latter is much more likely. Overall, the following factors should be taken into account when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). 1 The problem also exists for English, e.g., the annotators of the Penn Treebank were allowed to use tag combinations for inherently ambiguo"
E12-1050,W09-4105,1,0.912873,"Missing"
E12-1050,gimenez-marquez-2004-svmtool,0,0.644047,"Missing"
E12-1050,P05-1071,0,0.11926,"Missing"
E12-1050,P01-1035,0,0.0209176,"Missing"
E12-1050,J93-2004,0,0.046523,"his is not trivial since words can play different syntactic roles in different contexts, e.g., can is a noun in “I opened a can of coke.” but a verb in “I can write.” Traditionally, linguists have classified English words into the following eight basic POS categories: noun, pronoun, adjective, verb, adverb, preposition, conjunction, and interjection; this list is often extended a bit, e.g., with determiners, particles, participles, etc., but the number of categories considered is rarely more than 15. Computational linguistics works with a larger inventory of POS tags, e.g., the Penn Treebank (Marcus et al., 1993) uses 48 tags: 36 for partof-speech, and 12 for punctuation and currency symbols. This increase in the number of tags is partially due to finer granularity, e.g., there are special tags for determiners, particles, modal verbs, cardinal numbers, foreign words, existential there, etc., but also to the desire to encode morphological information as part of the tags. For example, there are six tags for verbs in the Penn Treebank: VB (verb, base form; e.g., sing), VBD (verb, past tense; e.g., sang), VBG (verb, gerund or present participle; e.g., singing), VBN (verb, past participle; e.g., sung) VBP"
E12-1050,W96-0213,0,0.407514,"count when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). 1 The problem also exists for English, e.g., the annotators of the Penn Treebank were allowed to use tag combinations for inherently ambiguous cases: JJ|NN (adjective or noun as prenominal modifier), JJ|VBG (adjective or gerund/present participle), JJ|VBN (adjective or past participle), NN|VBG (noun or gerund), and R"
E12-1050,P07-1096,0,0.0328539,".’, where it is a neuter pronoun. Finally, there are ambiguities that are very hard or even impossible1 to resolve, e.g., “Deteto vleze veselo.” can mean both ‘The child came in happy.’ (veselo is an adjective) and ‘The child came in happily.’ (it is an adverb); however, the latter is much more likely. Overall, the following factors should be taken into account when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexi"
E12-1050,E03-2015,1,0.844935,"Missing"
E12-1050,H05-1060,0,0.0405124,"Missing"
E12-1050,P11-2009,0,0.153555,"Missing"
E12-1050,C02-1027,0,0.475658,"Missing"
E12-1050,N03-1033,0,0.150525,"Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). 1 The problem also exists for English, e.g., the annotators of the Penn Treebank were allowed to use tag combinations for inherently ambiguous cases: JJ|NN (adjective or noun as prenominal modifier), JJ|VBG (adjective or gerund/present participle), JJ|VBN (adjective or past participle), NN|VBG (noun or gerund), and RB|RP (adverb or particle). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token. We also used 70 linguistically-motivated, highprecision rules in order to further reduce the number of possible tags suggested by the lexicon. The rules are similar to those proposed by Hinrichs and Trushkina (2004) for German; we implemented them as constraints in the CLaRK system (Simov et al., 2003). Here is an ex"
E12-1050,H05-1059,0,0.0154815,"aparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). 1 The problem also exists for English, e.g., the annotators of the Penn Treebank were allowed to use tag combinations for inherently ambiguous cases: JJ|NN (adjective or noun as prenominal modifier), JJ|VBG (adjective or gerund/present participle), JJ|VBN (adjective or past participle), NN|VBG (noun or gerund), and RB|RP (adverb or particle). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token. We also used 70 linguistically-motivated, highprecision rules in order to further reduce the number of possible tags suggested by the lexicon. The rules are similar to those proposed by Hinrichs and Trushkina (2004) for German; we implemented them as constraints in the CLaRK system (Simov et al., 2003). Here is an example of a rule: If a wordform i"
E12-1050,W11-0328,0,0.0497215,"Missing"
E12-1050,C10-2146,0,0.0238712,"Missing"
ghayoomi-etal-2014-constituency,W06-2920,0,\N,Missing
ghayoomi-etal-2014-constituency,P03-1054,0,\N,Missing
ghayoomi-etal-2014-constituency,J92-4003,0,\N,Missing
ghayoomi-etal-2014-constituency,P06-1055,0,\N,Missing
ghayoomi-etal-2014-constituency,D11-1022,0,\N,Missing
L16-1483,agerri-etal-2014-ixa,0,0.0139978,"us into a pivot language, English, and this into the remaining six languages. The current annotated corpus covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, s"
L16-1483,E09-1005,1,0.857409,"ese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the LKB used for this processing. Bulgarian The basic version of Bulgarian WSD is implemented on the assumption of one sense per discourse and bigram statistics. Czech Two different approaches were used for Czech WSD. The first approach based on the work of Duˇsek et al. (2015) focuses on verbal WSD. The second approach followed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the"
L16-1483,barreto-etal-2006-open,1,0.822658,"Missing"
L16-1483,bojar-etal-2012-joy,1,0.897675,"Missing"
L16-1483,E06-2024,1,0.746535,"It offers the “disambiguate” and “candidates” service endpoints. The former takes the spotted text input and it returns the DBpedia resource page for each entity. The later is similar to disambiguate, but returns a ranked list of candidates. Portuguese The NED module for Portuguese, LX-NED, uses DBpedia Spotlight to find links to resources about entities identified in pre-processed input text. It creates a process to run a Portuguese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the"
L16-1483,W02-2004,0,0.181331,"Missing"
L16-1483,W02-1001,0,0.104764,"covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, such as, titles for persons, types of geographical objects or organizations, etc. The disambiguation"
L16-1483,W15-2111,1,0.864612,"Missing"
L16-1483,hajic-etal-2012-announcing,1,0.885975,"Missing"
L16-1483,W03-1901,0,0.0627303,"n to comparing the cores and the morphological information (gender and number) of the two expressions. As such, we found it easier to directly implement equivalent tests in-code instead of having to feed the extracted features to the Weka J48 classifier proper. 3.5. Annotation formats Basque, Bulgarian, Czech, English and Spanish These corpora are annotated in the NAF format. The NAF format (Fokkens et al., 2014) is a linguistic annotation format designed for complex NLP pipelines that combines strengths of the Linguistic Annotation Framework (LAF) and the NLP Interchange Formats described by Ide and Romary (2003). Because of its layered extensible format, it can easily be incorporated in a variety of NLP modules that may require different linguistic information as their input. Portuguese The corpus for Portuguese is divided into 4 text files - the raw corpus, and one file for the output of each of the three tools used to process it (WSD, NED and coreference). For each of the three tools output is provided in a standoff annotation format, consisting of one token per line (ID of each token in a markable pair in the case of the coreference tool), the appropriate output element of the respective tools (wo"
L16-1483,2005.mtsummit-papers.11,0,0.506663,"ces, the less language-specific differences will remain between the representations of the meaning of the source and target texts. As a result, chances of success are expected to increase considerably by MT systems that are based on deeper semantic engineering approaches. Following this assumption, one of the approaches taken by the QTLeap project1 is to enrich MT training resources with lexico-semantic information. In this work, we present a solid effort to build multilingual parallel corpora annotated at multiple semantic levels. Our overall goal is to enrich two parallel corpora, Europarl (Koehn, 2005) and the QTLeap corpus (Agirre et al., 2015b), with token, lemma, part-of-speech (POS), namedentity recognition and classification (NERC), named-entity disambiguation (NED), word-sense disambiguation (WSD) and coreference for six languages covered in the QTLeap project, namely, Basque (EU), Bulgarian (BG), Czech (CS), English (EN), Portuguese (PT) and Spanish (ES). Specifically, this paper presents the first release of such corpora, which includes NERC, NED, WSD and coreferencelevel annotation for these six languages. Additionally, some languages have extra annotations, such as wikification (E"
L16-1483,W11-1902,0,0.0321161,"ce in Czech. English and Spanish ixa-pipe-coref is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pai"
L16-1483,J13-4004,0,0.0359801,"owed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the Czech corpus contains the same sentences as the English corpus, the English WordNet ID annotation from this corpus is projected onto Czech words using GIZA++ word alignment. Portuguese The Portuguese WSD tool, LX-WSD, is also based on UKB. The LKB from which UKB returns word senses within the pipeline has been generated from an extraction of the Portuguese MultiWordNet6 . 3.4. Coreference Basque ixa-pipe-coref-eu is an adaptation of the Stanford Deterministic Coreference Resolution (Lee et al., 2013), which gives state-of-the art performance for English. The original system applies a succession of ten independent deterministic coreference models or sieves. During the adaptation process, firstly, a baseline system has been created which receives as input texts processed by Basque analysis tools and uses specifically adapted static lists to identify language dependent features like gender, animacy or number. Afterwards, improvements over the baseline system have been applied, adapting and replacing some of the original sieves (Soraluze et al., 2015), taking into account that morphosyntactic"
L16-1483,S07-1008,0,0.017093,"Missing"
L16-1483,S07-1006,0,0.117234,"Missing"
L16-1483,W11-1901,0,0.087359,"Missing"
L16-1483,D10-1048,0,0.0133306,"h sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pair of expressions, the classifier returns a true or false value that indicates whether those expressions are coreferent. The classifier was trained over the Summit Corpus (Collovini et al., 2"
L16-1483,P14-5003,1,0.902631,"Missing"
L16-1483,tiedemann-2012-parallel,0,0.0371283,"with corresponding document IDs. Then, sentence boundaries were identified and aligned (for further collection and processing information, see Koehn (2005)). The Europarl corpus consists of monolingual data as well as bilingual parallel data with English as pivot language. In our effort, we have annotated the BG, CS, ES and PT parts of the corpus separately while the EN side of the ESEN language pair was used as pivot language to link all six languages. Given that Europarl does not include Basque, we annotated an alternative publicly available Basque-English parallel corpus, the GNOME corpus (Tiedemann, 2012), which includes GNOME localization files. 2.2. QTLeap corpus The QTLeap corpus consists of 4,000 pairs of questions and respective answers in the domain of IT troubleshooting http://hdl.handle.net/11234/1-1477 4 http://www.statmt.org/europarl/ 3023 for both hardware and software, distributed in four 1,000pair batches (Gaudio et al., 2016). This material was collected using a real-life, commercial online support service via chat. The QTLeap corpus is a unique resource in that it is a multilingual data set with parallel utterances in different languages (Basque, Bulgarian, Czech, Dutch, English"
L16-1483,E14-4045,0,0.0404676,"Missing"
L16-1483,M95-1005,0,0.280516,"Missing"
osenova-etal-2012-treebank,bielicky-smrz-2008-building,0,\N,Missing
osenova-etal-2012-treebank,W09-0306,0,\N,Missing
osenova-etal-2012-treebank,simov-osenova-2010-constructing,1,\N,Missing
osenova-simov-2012-political,W06-1639,0,\N,Missing
osenova-simov-2012-political,savkov-etal-2012-linguistic,1,\N,Missing
osenova-simov-2012-political,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
osenova-simov-2012-political,P10-4005,0,\N,Missing
R09-1022,D08-1030,0,0.0545769,"Missing"
R09-1022,M98-1001,0,0.0436201,"Missing"
R09-1022,W99-0613,0,0.0554257,"Missing"
R09-1022,W09-3538,0,0.0469039,"Missing"
R09-1022,P98-2206,0,0.109459,"Missing"
R09-1022,W03-0430,0,0.151189,"Missing"
R09-1022,N04-1043,0,0.0688611,"Missing"
R09-1022,popov-etal-2004-creation,0,0.37681,"Missing"
R09-1022,W95-0107,0,0.0643128,"Missing"
R09-1022,W02-2024,0,0.17217,"Missing"
R09-1022,W00-1219,0,0.0559088,"Missing"
R09-1022,J92-4003,0,\N,Missing
R09-1022,P02-1060,0,\N,Missing
R09-1022,C98-2201,0,\N,Missing
R09-1022,W03-0419,0,\N,Missing
R09-1022,P02-1062,0,\N,Missing
R11-1065,W02-1502,0,0.0976216,"Missing"
R11-1065,2005.mtsummit-osmtw.3,0,0.0631264,"Missing"
R11-1065,copestake-flickinger-2000-open,0,0.105776,"set is discussed. In the following section the HPSGbased Bulgarian grammar BURGER is briefly outlined. Finally, the basic rules for the construction of the RMRS analyses from dependency parses are described. 2 Background Our approach is inspired by the work on MRS and RMRS (see (Copestake, 2003; 2007)) and the previous work on transfer of dependency analyses into RMRS structures described in (Spreyer and Frank, 2005) and (Jakob et al, 2010). MRS is introduced as an underspecified semantic formalism (Copestake et al, 2005). It is used to support semantic analyses in HPSG English grammar – ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG. The main idea is the formalism to rule out spurious analyses resulting from the representa471 Proceedings of Recent Advances in Natural Language Processing, pages 471–478, Hissar, Bulgaria, 12-14 September 2011. tion of logical operators and the scope of quantifiers. Here we will present only basic definitions from (Copestake et al, 2005). For more details the cited publication should be consulted. An MRS structure is a tuple <GT, R, C&gt;, where GT is the top handle, R is a bag of EPs (elementary predicates) and C is a bag of handle constraints, s"
R11-1065,jakob-etal-2010-mapping,0,0.472359,"the rules for the construction of RMRS analyses over the dependency parses. It is structured as follows: First, the context of our work is presented. Then our dependency tagset is discussed. In the following section the HPSGbased Bulgarian grammar BURGER is briefly outlined. Finally, the basic rules for the construction of the RMRS analyses from dependency parses are described. 2 Background Our approach is inspired by the work on MRS and RMRS (see (Copestake, 2003; 2007)) and the previous work on transfer of dependency analyses into RMRS structures described in (Spreyer and Frank, 2005) and (Jakob et al, 2010). MRS is introduced as an underspecified semantic formalism (Copestake et al, 2005). It is used to support semantic analyses in HPSG English grammar – ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG. The main idea is the formalism to rule out spurious analyses resulting from the representa471 Proceedings of Recent Advances in Natural Language Processing, pages 471–478, Hissar, Bulgaria, 12-14 September 2011. tion of logical operators and the scope of quantifiers. Here we will present only basic definitions from (Copestake et al, 2005). For more details the c"
R11-1065,nivre-etal-2006-maltparser,0,0.160641,"Missing"
R11-1065,N06-1032,0,0.0410961,"Missing"
R11-1065,W08-0319,0,0.0169663,"er, and then retrieving RMRS analyses by exploring the linguistic knowledge within BulTreeBank-DP. 1 Introduction Recently a number of machine translation efforts have focused on grammatical formalisms in performing source language analysis, transfer rule application and target language generation. It is worth mentioning several works, such as (Bond, 2005) exploiting DELPH-IN infrastructure for developing of HPSG grammars, (Riezler and Maxwell III, 2006) using LFG grammar, (Oepen et al, 2007) working on a hybrid architecture consisting of an LFG grammar, an HPSG grammar, partial parsing, and (Bojar and Hajic, 2008) using the Functional Generative Description framework to language analysis on analytical and tectogrammatical level. All the approaches rely on the advances in the development of deep grammar natural language parsing. The approaches share similar architecture and techniques to overcome the drawbacks of the deep processing in comparison to statistical shallow methods. Within the above mentioned context, we are constructing a Bulgarian-to-English translation system, based on HPSG. The transfer rules are implemented on the level of MRS (Minimal RePetya Osenova LMD, IICT-BAS petya@bultreebank.org"
R13-1098,D12-1133,0,0.0317717,"e discuss the relevant annotations available in the Bulgarian treebank; Section 4 presents the proposed approach for joint modeling, Section 5 elaborates on our experimental settings and the obtained results; Section 6 concludes the paper. 2 Related work We are not aware of other studies that propose joint models for Bulgarian, and to the best of our knowledge, attemps at combining the three tasks (POS tagging, dependency parsing and coreference resolution) in a joint model have not been described in the literature either. Our approach is inspired by works such as (Finkel and Manning, 2010), (Bohnet and Nivre, 2012) and (Qian and Liu, 2012). Finkel and Manning (2010) report on combining NER and parsing tasks in a joint model. One similarity with our task is the understanding that the separate tasks can help each other in various not-always-subsequent executions. Another one is the fact that the explored algorithm is extended. The difference is that the authors rely on a feature-rich CRF parser, while our algorithm is based on an online largemargin learning algorithm. Bohnet and Nivre (2012) studies the combination of two tasks (POS tagging and Dependency labeled non-projective parsing) against datasets i"
R13-1098,P10-1074,0,0.0313451,"k; Section 4 presents the proposed approach for joint modeling, Section 5 elaborates on our experimental settings and the obtained results; Section 6 concludes the paper. 2 Related work We are not aware of other studies that propose joint models for Bulgarian, and to the best of our knowledge, attemps at combining the three tasks (POS tagging, dependency parsing and coreference resolution) in a joint model have not been described in the literature either. Our approach is inspired by works such as (Finkel and Manning, 2010), (Bohnet and Nivre, 2012) and (Qian and Liu, 2012). Finkel and Manning (2010) report on combining NER and parsing tasks in a joint model. One similarity with our task is the understanding that the separate tasks can help each other in various not-always-subsequent executions. Another one is the fact that the explored algorithm is extended. The difference is that the authors rely on a feature-rich CRF parser, while our algorithm is based on an online largemargin learning algorithm. Bohnet and Nivre (2012) studies the combination of two tasks (POS tagging and Dependency labeled non-projective parsing) against datasets in four languages, and the reported results indicate"
R13-1098,P05-1012,0,0.0523343,"cs and nouns (within a prepositional phrase, expressing the indirect object of the same verb) have common number and gender features; (2) unexpressed subjects participate in co-reference chains of control, binding, etc. constructions. We propose a model capable of handling such interactions among the different linguistic levels. We define an extended dependency tree that incorporates service nodes and links, through which additional knowledge, such as POS tag candidates, correct POS tags and co-reference relations, can be fed into the MSTParser algorithm for non-projective dependency parsing (McDonald et al., 2005). The sentences in the treebank are projected as extended dependency trees, and the parser is applied to their new representation. Although the proposed model addresses the Bulgarian language, it is also applicable to other languages, provided that all necessary resources are available. Introduction Advanced language technology applications depend on various forms of preprocessing, such as POS tagging, parsing, co-reference resolution, word sense disambiguation, etc. Although in ideal settings these tasks have satisfactory solutions on their own, their combination in a pipeline is related to a"
R13-1098,D12-1046,0,0.161049,"gging, dependency parsing and co-reference resolution (within a sentence). The experiments are performed on data from the Bulgarian HPSG-based treebank — BulTreeBank. Our motivation to attempt solving these particular problems via a single model is many-fold: (1) avoiding the accumulation of errors inherent to pipeline processing, (2) overcoming the low speed 755 Proceedings of Recent Advances in Natural Language Processing, pages 755–762, Hissar, Bulgaria, 7-13 September 2013. der to find an appropriate solution. Therefore, a straight adaptation of the transition-based model is not possible. Qian and Liu (2012) focuses on the modelling of three tasks for Chinese - word segmentation, POS tagging and parsing. The models for each task are trained separately, while the unification of predictions is performed during the decoding phase. As in the previous paper, the authors report improvements over the pipeline results for Chinese. The similarity is that our approach also considers three tasks in one model for one language with a modified algorithm. Our approach differs in the following aspects: the third task is not identical. In our case it is the addition of co-reference chains instead of the specific"
R13-1098,W09-2411,0,0.067895,"Missing"
R13-1098,E12-1050,1,\N,Missing
R13-1098,S10-1001,0,\N,Missing
R15-1077,E09-1005,0,0.305103,"h and are connected to their potential senses via directed edges (by default, a context window of at least 20 words is used for each disambiguation). These newly introduced nodes serve to inject initial probability mass (via the v vector) and thus to make their associated sense nodes especially relevant in the knowledge graph. Applying the Personalized PageRank algorithm iteratively over the resulting graph determines the most appropriate sense for each ambiguous word. The method has been boosted by the addition of new relations and by developing variations and optimizations of the algorithm (Agirre and Soroa, 2009). It has also been applied to the task of Named Entity Disambiguation (Agirre et al., 2015). Montoyo et al. (2005) present a combination of knowledge-based and supervised systems for WSD, which demonstrates that the two approaches can boost one another, due to the fundamentally different types of knowledge they utilise (paradigmatic vs. syntagmatic). They explore a knowledge-based system that uses heuristics for WSD depending on the position of word potential senses in the WordNet knowledge base. In terms of supervised machine learning based on an annotated corpus, it explores a Maximum Entrop"
R15-1077,osenova-etal-2012-treebank,1,0.842246,"tegy, but it does so by doing the combination in a postprocessing step, i.e. by merging the output of two separate systems; also, it still relies on manually-annotated data for the supervised disambiguation. Building on the already mentioned work on graph-based approaches, it is possible to combine paradigmatic and syntagmatic information in another way – by incorporating both into the knowledge graph. This approach is described in the current paper. 3 The Bulgarian Sense Annotated Treebank The sense annotation process over BulTreeBank (BTB) was organized in three layers: verb valency frames (Osenova et al., 2012); senses of verbs, nouns, adjectives and adverbs; DBpedia URIs over named entities. However, in the experiment presented here, we used mainly the annotated senses of nouns and verbs (together with the valency information), as well as the concept mappings to WordNet. For that reason we do not discuss the DBpedia annotation here. A brief outline can be found in Popov et al. (2014). The sense annotation was organized as follows: the lemmatized words per part-of-speech (POS) from BTB received all their possible senses from the explanatory dictionary of Bulgarian and from our Core WordNet2 . When t"
R15-1077,J14-1003,0,\N,Missing
R15-1084,P12-2059,0,0.0265942,"do. Like us, he trains machine translation Moses models on parallel lists of proper names. The language pairs for which he obtains transliteration and backtransliteration models are English and Arabic, and English and Chinese. Unlike him, we are only interested in forward transliteration to Bulgarian. And our approach differs from his in several other aspects. First, we do not lowercase our training data. Second, we explore not only unigram models, but also bigram ones. And finally, we employ heuristics to decide whether to transliterate or not. The construction of our models was inspired by (Nakov and Tiedemann, 2012). They train the only transliteration models for Bulgarian known to us. They use automatic transliteration as a substitution for machine transla2 Or it might be a combination of the three. Here we will not deal with mixed cases as such. We will consider as cases of direct adoption only those where the whole name has been directly adopted. We will treat as translation cases all cases where at least some part of the name has been translated, and we will take the rest to be transliteration ones. 655 tion between very closely related languages, namely Bulgarian and Macedonian. Their models are of"
R15-1084,W12-4401,0,0.0682782,"Missing"
R19-1110,W18-2505,0,0.0791915,"Missing"
R19-1110,agirre-soroa-2008-using,0,0.0702999,"Missing"
R19-1110,kingsbury-palmer-2002-treebank,0,0.2855,"The difference between paradigmatic and syntagmatic relations necessitates different weighting approaches. Using the cosine similarity measure over the entire embedding space seems to be a suboptimal blanket strategy. In our future research we plan to train relation embeddings, following the approach for generating pseudo corpora, as in the case of training synset and lemma embeddings. Additionally, we plan on further investigating strategies for generalizing knowledge from external resources such as VerbNet and FrameNet, as well as other ones which can be mapped to WordNet, such as PropBank (Kingsbury and Palmer, 2002) and the OntoNotes sense groupings (Snow et al., 2007). As has been demonstrated, structuring a lexico-semantic graph in an optimal way can make a big difference. Detailed error analysis and sophisticated linguistic theory should be employed in order to capture the principles underlying a good knowledge base. Conslusion We have presented results from a series of experiments with a KBWSD system with state-of-the-art default parametrization and have shown that accuracy can be further improved through the manipulation and extension of the KB. The present models achieve the highest reported accura"
R19-1110,E09-1005,0,0.36772,"of hurdles, such as: the non-trivial issue of how to structure a KB and what to put inside it; how to explore a KB most effectively; how to integrate various pieces of knowledge into a holistic representation of meaning. One of the most successful KBWSD approaches has been to use algorithms from the Random Walks on Graph family in order to obtain sense representations over particular textual contexts. For instance, Mihalcea (2005) constructs a subgraph with the possible word senses in a context and then runs PageRank (Page et al., 1999) over it in order to calculate the most prominent senses. Agirre and Soroa (2009) present an influential update on this method, within the UKB tool for WSD2 . In addition to the static version of PageRank (Spr; introduced in Agirre and Soroa (2008)), which also constructs a sub-graph from the WordNet semantic network as a preliminary 2 step, they put forward Personalized PageRank, in which the whole KB is used, with the context words serving to inject probability mass into all candidate word senses. The final state of the PageRank vector over the graph indicates which are the most relevant concepts in the particular context. Two variants are described: Ppr and Ppr_w2w, whe"
R19-1110,D18-1170,0,0.0160267,"from the annotated gloss corpus; SC – the relations from the automatically parsed SemCor corpus; VNM – the sense groupings from VerbNet; FNM – the sense groupings from FrameNet; FNR – the links between FrameNet predicate senses and role-type senses. All experiments use the default parametrization from Agirre et al. (2018), with the exception of the cases marked with a subscript ctx=num, where the context windows have been changed to include num words. The best result (line 14) is achieved for contexts with 10, 15, 25 and 30 words. SNE stands for Senseval and SME stands for SemEval. WSD system Luo et al. (2018a) Luo et al. (2018b) Raganato et al. (2017b) Iacobacci et al. (2016)† Melamud et al. (2016)† Agirre et al. (2018)† Moro et al. (2014)† WN 1st sense† This workbest SNE-2 72.8 72.2 72.0 73.3 72.3 68.8 67.0 66.8 70.3 SNE-3 70.3 70.5 69.1 69.6 68.2 66.1 63.5 66.2 67.9 SME-07 —* —* 64.8* 61.1 61.5 53.0 51.6 55.2 57.8 SME-13 68.5 67.2 66.9 66.7 67.2 68.8 66.4 63.0 69.8 SME-15 72.8 72.6 71.5 70.4 71.7 70.3 70.3 67.8 71.8 ALL 71.1 70.6 69.9 69.7 69.4 67.3 65.5 65.2 69.0 Table 2: The state of the art across WSD systems. The dagger symbol indicates that the result is reported in the UEF (Raganato et al"
R19-1110,P98-1013,0,0.422559,"Missing"
R19-1110,P18-1230,0,0.0146324,"from the annotated gloss corpus; SC – the relations from the automatically parsed SemCor corpus; VNM – the sense groupings from VerbNet; FNM – the sense groupings from FrameNet; FNR – the links between FrameNet predicate senses and role-type senses. All experiments use the default parametrization from Agirre et al. (2018), with the exception of the cases marked with a subscript ctx=num, where the context windows have been changed to include num words. The best result (line 14) is achieved for contexts with 10, 15, 25 and 30 words. SNE stands for Senseval and SME stands for SemEval. WSD system Luo et al. (2018a) Luo et al. (2018b) Raganato et al. (2017b) Iacobacci et al. (2016)† Melamud et al. (2016)† Agirre et al. (2018)† Moro et al. (2014)† WN 1st sense† This workbest SNE-2 72.8 72.2 72.0 73.3 72.3 68.8 67.0 66.8 70.3 SNE-3 70.3 70.5 69.1 69.6 68.2 66.1 63.5 66.2 67.9 SME-07 —* —* 64.8* 61.1 61.5 53.0 51.6 55.2 57.8 SME-13 68.5 67.2 66.9 66.7 67.2 68.8 66.4 63.0 69.8 SME-15 72.8 72.6 71.5 70.4 71.7 70.3 70.3 67.8 71.8 ALL 71.1 70.6 69.9 69.7 69.4 67.3 65.5 65.2 69.0 Table 2: The state of the art across WSD systems. The dagger symbol indicates that the result is reported in the UEF (Raganato et al"
R19-1110,K16-1006,0,0.0737732,"Missing"
R19-1110,lopez-de-lacalle-etal-2014-predicate,0,0.0482681,"Missing"
R19-1110,H05-1052,0,0.182454,"Missing"
R19-1110,N15-1165,0,0.0604626,"Missing"
R19-1110,H93-1061,0,0.71198,"Missing"
R19-1110,P16-1085,0,0.0445079,"Missing"
R19-1110,Q14-1019,0,0.0325963,"ty mass into all candidate word senses. The final state of the PageRank vector over the graph indicates which are the most relevant concepts in the particular context. Two variants are described: Ppr and Ppr_w2w, where the second one is modified so as to put additional emphasis on connected concepts and away from the target senses themselves. The second strategy is used to prevent competing but related word senses from bolstering each other inordinately, but also makes the algorithm significantly slower. Another related KBWSD system is Babelfy, which uses a Random Walk with Restart algorithm (Moro et al., 2014). Two discussion points regarding Random Walks on Graph approaches are central in the context of this article: knowledge base enrichment and algorithm parametrization. Simov et al. (2016) have shown that the addition of new relation sets to the baseline WordNet 3.0 semantic network can have significant positive effects on the performance of the PageRank algorithm. Adding relations extracted from the manually disambiguated word sense glosses, for instance, is a major improvement; including dependency-based relations between manually disambiguated words from SemCor has also led to big error redu"
R19-1110,D14-1162,0,0.091778,"extend the coverage of the mapping. In this way all WordNet sense identifiers that could be mapped to predicates in VerbNet classes and FrameNet frames have been organized in structures that reflect this kind of membership in the external resources. This has been done in a way that is similar to the one described earlier in relation to graphically connecting the various word senses in a synset. That is, the word sense • GoogleNews. A word embedding model trained over 100 billion running words. The vectors are of size 300. • Glove. Word embeddings trained over global contexts, as described in Pennington et al. (2014). • WN30WN30glConOne. Synset and lemma embeddings trained by Simov et al. (2017). • WN30WN30glConOneWiki. In this case a lemmatized Wikipedia corpus has been added to the pseudo corpus, in order to balance information from the knowledge graph with actual text data. The weights associated with the different word senses in the new lexicon, which also play an important role for the optimal performance of the UKB system, are the same as those associated with synset identifiers in the original lexicon in the UKB distribution. This is so because the frequencies have been originally determined on the"
R19-1110,E17-1010,0,0.0871425,"hieve the highest accuracy scores, problems related to the lack of gold corpora, data sparseness and suboptimal granularity of the computational lexicons have plagued the field and prevented significant breakthroughs. This paper presents results achieved with knowledge-based word sense disambiguation (KBWSD) algorithms as an alternative pathway. It builds on previous work in the subfield and demonstrates that KBWSD can achieve accuracy scores near and 2 Related Work State-of-the-art results in the broader field of WSD have been recently summarized in the Unified Evaluation Framework1 (UEF) by Raganato et al. (2017a), which focuses on the all-words 1 http://lcl.uniroma1.it/wsdeval 949 Proceedings of Recent Advances in Natural Language Processing, pages 949–958, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_110 disambiguation task based on WordNet 3.0 (Fellbaum, 2012). The top-performing systems in UEF, across supervised and knowledge-based ones, are an SVM model – an extension of the popular IMS system (Zhong and Ng, 2010), and a recurrent neural network model – context2vec (Melamud et al., 2016), with F1 scores on the concatenation of all evaluation corpora ranging between"
R19-1110,D17-1120,0,0.0753637,"hieve the highest accuracy scores, problems related to the lack of gold corpora, data sparseness and suboptimal granularity of the computational lexicons have plagued the field and prevented significant breakthroughs. This paper presents results achieved with knowledge-based word sense disambiguation (KBWSD) algorithms as an alternative pathway. It builds on previous work in the subfield and demonstrates that KBWSD can achieve accuracy scores near and 2 Related Work State-of-the-art results in the broader field of WSD have been recently summarized in the Unified Evaluation Framework1 (UEF) by Raganato et al. (2017a), which focuses on the all-words 1 http://lcl.uniroma1.it/wsdeval 949 Proceedings of Recent Advances in Natural Language Processing, pages 949–958, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_110 disambiguation task based on WordNet 3.0 (Fellbaum, 2012). The top-performing systems in UEF, across supervised and knowledge-based ones, are an SVM model – an extension of the popular IMS system (Zhong and Ng, 2010), and a recurrent neural network model – context2vec (Melamud et al., 2016), with F1 scores on the concatenation of all evaluation corpora ranging between"
R19-1110,K15-1037,0,0.0462938,"Missing"
R19-1110,P10-4014,0,0.0750008,"Missing"
R19-1127,N19-1078,0,0.113306,"Missing"
R19-1127,Q17-1010,0,0.0765556,"e such as Bulgarian using POS and grammatical information can improve the results. Thus, we mix automatically learned features — the word and the character embeddings —, with hand-crafted features encoded as a grammatical vector. In the rest of this section, we describe the different components of our system. LSTM-CRF Implementation For the implementation of the general LSTM-CRF architecture, we use Tensorflow (Sak et al., 2014). Word Embedding Nowadays there are many different approaches to train word vectors such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and many more. In our experiments, we use the pre-trained Bulgarian word embeddings from FastText (Bojanowski et al., 2017).4 This choice was motivated by the fact that FastText uses the structure of the words by taking into consideration character n-grams, thus modeling morphology and many out-of-vocabulary words. 4 In this work, we do not use any contextualization of the word embeddings such as ElMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as our Bi-LSTM architecture already performs contextualization. Character Bi-LSTM Embedding In order to produce character embeddings, we us"
R19-1127,Q16-1026,0,0.0296897,"nguage and per entity types have been expected. Such a task, however, is also good motivation for improving the NER systems for Slavic languages, including Bulgarian. There is some previous work on NER for Bulgarian. Georgiev et al. (2009) presented a model using Conditional Random Fields with several hand-crafted features. They combined wellestablished features used for other languages with language-specific lexical, syntactic, and morphological information. Their result is the previous state-of-the-art for Bulgarian. So far, the highest reported results for NER are for English. For example, Chiu and Nichols (2016) reported an F1 score of 91.20 using BiLSTM + CNN + gazetteers + linking, while Passos et al. (2014) achieved an F1 score of 90.90 using a new form of learning word embeddings that can leverage information from relevant lexicons. For German, Gillick et al. (2016) achieved an F1 score of 82.84, which shows that the rich morphology causes a drop in the performance. Currently, the prevalent paradigm in NLP is to use neural networks, typically based on LSTMs or CNNs. As we have mentioned above, Lample et al. (2016) proposed an LSTM-CRF model for NER.3 The model uses a bi-directional LSTM to encode"
R19-1127,N19-1423,0,0.165751,"are many different approaches to train word vectors such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and many more. In our experiments, we use the pre-trained Bulgarian word embeddings from FastText (Bojanowski et al., 2017).4 This choice was motivated by the fact that FastText uses the structure of the words by taking into consideration character n-grams, thus modeling morphology and many out-of-vocabulary words. 4 In this work, we do not use any contextualization of the word embeddings such as ElMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as our Bi-LSTM architecture already performs contextualization. Character Bi-LSTM Embedding In order to produce character embeddings, we use a bidirectional LSTM over the character representation of the text. For each character in the text, each of the two LSTMs produces an hidden vector. For each word, the hidden vector for the last character produced by the left-to-right LSTM models information about the suffix of the word. Similarly, the hidden vector for the first character produced by the right-to-left LSTM models information about the prefix of the word. Following the approach, used in"
R19-1127,doddington-etal-2004-automatic,0,0.0193656,"12). In our work here, we use deep neural networks for Bulgarian NER. Lample et al. (2016) have shown remarkable results for English, using a combination of Bi-LSTMs (Bi-directional Long Short-Term Memory) and CRF. However, the approach is problematic for morphologically rich languages. The main problem is the missing information within word embeddings for the numerous word forms involved in multiword names that require additional grammatical knowledge in order to be processed properly. Here we incorporate such information as additional input to our neural model. 1 Other schemata such as ACE (Doddington et al., 2004) used a richer inventory of entity types. 1104 Proceedings of Recent Advances in Natural Language Processing, pages 1104–1113, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_127 Our contributions are as follows: Our work is based on Bulgarian, but we claim that it is appropriate also for other languages with rich morphological systems like Slavic and Romance languages, for example. For that reason, we present first the best results for NER in other Slavic languages having in mind that they are synthetic, while Bulgarian is a predominantly analytic language whose mor"
R19-1127,R09-1022,1,0.791678,"Slavic languages (Bulgarian, Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian), their normalization/lemmatization as well as cross-lingual linking. Our evaluation on NER in this paper is more similar to the relaxed evaluation parameter where the string is detected and classified, not the invariant. Considering the complexity of the task, the drop of the results per language and per entity types have been expected. Such a task, however, is also good motivation for improving the NER systems for Slavic languages, including Bulgarian. There is some previous work on NER for Bulgarian. Georgiev et al. (2009) presented a model using Conditional Random Fields with several hand-crafted features. They combined wellestablished features used for other languages with language-specific lexical, syntactic, and morphological information. Their result is the previous state-of-the-art for Bulgarian. So far, the highest reported results for NER are for English. For example, Chiu and Nichols (2016) reported an F1 score of 91.20 using BiLSTM + CNN + gazetteers + linking, while Passos et al. (2014) achieved an F1 score of 90.90 using a new form of learning word embeddings that can leverage information from relev"
R19-1127,N16-1155,0,0.0254207,"onditional Random Fields with several hand-crafted features. They combined wellestablished features used for other languages with language-specific lexical, syntactic, and morphological information. Their result is the previous state-of-the-art for Bulgarian. So far, the highest reported results for NER are for English. For example, Chiu and Nichols (2016) reported an F1 score of 91.20 using BiLSTM + CNN + gazetteers + linking, while Passos et al. (2014) achieved an F1 score of 90.90 using a new form of learning word embeddings that can leverage information from relevant lexicons. For German, Gillick et al. (2016) achieved an F1 score of 82.84, which shows that the rich morphology causes a drop in the performance. Currently, the prevalent paradigm in NLP is to use neural networks, typically based on LSTMs or CNNs. As we have mentioned above, Lample et al. (2016) proposed an LSTM-CRF model for NER.3 The model uses a bi-directional LSTM to encode the left and the right context of the current input word. Then it passes the concatenation of the two hidden vectors (one produced by the left LSTM and one by the right LSTM) to a CRF model. Its task is to ensure the global consistency of the NER tags. 2 http://"
R19-1127,C96-1079,0,0.665641,"al role in the processing of texts with application to many real-world Natural Language Processing (NLP) tasks such as Question Answering, Information Extraction, Machine Translation, Dialog Systems, and chatbots, where it is sometimes called Concept Segmentation and Labeling (Saleh et al., 2014). Preslav Nakov Qatar Computing Research Institute, HBKU Qatar pnakov@qf.org.qa Traditionally, NER has focused on recognizing entities such as person (PER), organization (ORG), location (LOC), and miscellaneous (MISC). This tradition goes back to the Message Understanding Conference (MUC) for English (Grishman and Sundheim, 1996), and the subsequent CoNLL 2002/2003 shared tasks, which also targeted other European Languages such as Spanish, Dutch, and German (Tjong Kim Sang and De Meulder, 2003).1 This same setup was followed in more recent work for a number of other languages, and we also follow it in the present work. Early systems relied on hand-crafted rules with pattern-matching (Appelt et al., 1995). Unfortunately, this required an large pre-annotated datasets, collecting which was time-consuming and error-prone. The next step was to add gazetteers and lexicons that were generated automatically or semi-automatica"
R19-1127,N16-1030,0,0.578157,"matching (Appelt et al., 1995). Unfortunately, this required an large pre-annotated datasets, collecting which was time-consuming and error-prone. The next step was to add gazetteers and lexicons that were generated automatically or semi-automatically (Popescu and Etzioni, 2005). Adding such resources required special approaches to resolve the ambiguity between names and common words. Such problems were solved using models such as Hidden Markov Models (Zhou and Su, 2002) and Conditional Random Fields (Sutton and McCallum, 2012). In our work here, we use deep neural networks for Bulgarian NER. Lample et al. (2016) have shown remarkable results for English, using a combination of Bi-LSTMs (Bi-directional Long Short-Term Memory) and CRF. However, the approach is problematic for morphologically rich languages. The main problem is the missing information within word embeddings for the numerous word forms involved in multiword names that require additional grammatical knowledge in order to be processed properly. Here we incorporate such information as additional input to our neural model. 1 Other schemata such as ACE (Doddington et al., 2004) used a richer inventory of entity types. 1104 Proceedings of Rece"
R19-1127,N10-1000,0,0.104846,"Missing"
R19-1127,D15-1176,0,0.0991376,"Missing"
R19-1127,P19-1441,0,0.0262885,"elp identify loan words in Bulgarian. Another promising research direction is to compare the differences in the graphical representation of named entities in Bulgarian and English. For example, in English all components of a named entity are capitalized (except for the functional words). In order to have comparable data, we envision to pre-transform the Bulgarian dataset to which to apply the English capitalization rule for the phrasal named entities. Finally, we plan to experiment with different monolingual representations from ElMo (Peters et al., 2018), BERT (Devlin et al., 2019), ROBERTa (Liu et al., 2019c), XLNet (Yang et al., 2019), and Ernie 2.0 (Sun et al., 2019), pooled representations from Flair (Akbik et al., 2019), distilled representations from MT-DNN (Liu et al., 2019a,b) or cross-language representations from XLM (Lample and Conneau, 2019). 8 Acknowledgements This research was partially supported by the Bulgarian National Interdisciplinary Research eInfrastructure for Resources and Technologies in favor of the Bulgarian Language and Cultural Heritage, part of the EU infrastructures CLARIN and DARIAH – CLaDA-BG, Grant number DO01164/28.08.2018 We would like to thank the anonymous rev"
R19-1127,2021.ccl-1.108,0,0.0979666,"Missing"
R19-1127,W14-1609,0,0.0301426,"ving the NER systems for Slavic languages, including Bulgarian. There is some previous work on NER for Bulgarian. Georgiev et al. (2009) presented a model using Conditional Random Fields with several hand-crafted features. They combined wellestablished features used for other languages with language-specific lexical, syntactic, and morphological information. Their result is the previous state-of-the-art for Bulgarian. So far, the highest reported results for NER are for English. For example, Chiu and Nichols (2016) reported an F1 score of 91.20 using BiLSTM + CNN + gazetteers + linking, while Passos et al. (2014) achieved an F1 score of 90.90 using a new form of learning word embeddings that can leverage information from relevant lexicons. For German, Gillick et al. (2016) achieved an F1 score of 82.84, which shows that the rich morphology causes a drop in the performance. Currently, the prevalent paradigm in NLP is to use neural networks, typically based on LSTMs or CNNs. As we have mentioned above, Lample et al. (2016) proposed an LSTM-CRF model for NER.3 The model uses a bi-directional LSTM to encode the left and the right context of the current input word. Then it passes the concatenation of the t"
R19-1127,D14-1162,0,0.0883647,"t for a morphologically rich language such as Bulgarian using POS and grammatical information can improve the results. Thus, we mix automatically learned features — the word and the character embeddings —, with hand-crafted features encoded as a grammatical vector. In the rest of this section, we describe the different components of our system. LSTM-CRF Implementation For the implementation of the general LSTM-CRF architecture, we use Tensorflow (Sak et al., 2014). Word Embedding Nowadays there are many different approaches to train word vectors such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and many more. In our experiments, we use the pre-trained Bulgarian word embeddings from FastText (Bojanowski et al., 2017).4 This choice was motivated by the fact that FastText uses the structure of the words by taking into consideration character n-grams, thus modeling morphology and many out-of-vocabulary words. 4 In this work, we do not use any contextualization of the word embeddings such as ElMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as our Bi-LSTM architecture already performs contextualization. Character Bi-LSTM Embedding In order to"
R19-1127,N18-1202,0,0.300472,". Word Embedding Nowadays there are many different approaches to train word vectors such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and many more. In our experiments, we use the pre-trained Bulgarian word embeddings from FastText (Bojanowski et al., 2017).4 This choice was motivated by the fact that FastText uses the structure of the words by taking into consideration character n-grams, thus modeling morphology and many out-of-vocabulary words. 4 In this work, we do not use any contextualization of the word embeddings such as ElMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as our Bi-LSTM architecture already performs contextualization. Character Bi-LSTM Embedding In order to produce character embeddings, we use a bidirectional LSTM over the character representation of the text. For each character in the text, each of the two LSTMs produces an hidden vector. For each word, the hidden vector for the last character produced by the left-to-right LSTM models information about the suffix of the word. Similarly, the hidden vector for the first character produced by the right-to-left LSTM models information about the prefix of the word."
R19-1127,D17-1283,0,0.0130415,"s are trained in an unsupervised manner on external data, while the characterbased LSTM embeddings are trained on the training data as part of the end-to-end training of the full LSTM-CRF model. This model does not need any explicit feature engineering nor does it need manual gazetteers; yet, it achieved state-of-the-art performance for four languages: English, German, Dutch, and Spanish. Here we take this model as a basis, and we augment it to model part-of-speech (POS) and grammatical information, which turns out to be very important for a morphologically complex language such as Bulgarian. Strubell et al. (2017) extended the above model by substituting the LSTM with Iterated Dilated Convolutional Neural Networks, a variant of CNN, which permit fixed-depth convolutions to run in parallel across entire documents, thus making use of GPUs, which yields up to 20-fold speed up, while retaining performance comparable to that of the LSTM-CRF model. They further aggregated context from the entire input document, which they found to be helpful. In our preliminary monolingual experiments, this model performed very similarly, but slightly worse, than the LSTMCRF model, and thus we chose LSTM-CRF for our experime"
R19-1127,W19-3709,0,0.54809,"Missing"
R19-1127,W17-1412,0,0.013574,"Markov Model. The feature modeling also proved to be working in Czech, as their best results used features based on morphological analysis, two-stage prediction, word clustering, and gazetteers. For Polish, Piskorski et al. (2004) achieved precision of 91.0, recall of 77.5, and F1 score of 82.4. They used the SProUT system, which is an NLP platform, consisting of pattern/action rules. In the last years, the interest in NER for Slavic languages grew. Two shared tasks were organized —- the first and the second Multilingual Named Entity Challenge in Slavic Languages. They have been descibed in (Piskorski et al., 2017) and (Piskorski et al., 2019). The challenges included several tasks: recognition of mentions of named entities in Web documents in seven Slavic languages (Bulgarian, Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian), their normalization/lemmatization as well as cross-lingual linking. Our evaluation on NER in this paper is more similar to the relaxed evaluation parameter where the string is detected and classified, not the invariant. Considering the complexity of the task, the drop of the results per language and per entity types have been expected. Such a task, however, is also go"
R19-1127,W03-0419,0,0.829024,"Missing"
R19-1127,H05-1043,0,0.109539,"the subsequent CoNLL 2002/2003 shared tasks, which also targeted other European Languages such as Spanish, Dutch, and German (Tjong Kim Sang and De Meulder, 2003).1 This same setup was followed in more recent work for a number of other languages, and we also follow it in the present work. Early systems relied on hand-crafted rules with pattern-matching (Appelt et al., 1995). Unfortunately, this required an large pre-annotated datasets, collecting which was time-consuming and error-prone. The next step was to add gazetteers and lexicons that were generated automatically or semi-automatically (Popescu and Etzioni, 2005). Adding such resources required special approaches to resolve the ambiguity between names and common words. Such problems were solved using models such as Hidden Markov Models (Zhou and Su, 2002) and Conditional Random Fields (Sutton and McCallum, 2012). In our work here, we use deep neural networks for Bulgarian NER. Lample et al. (2016) have shown remarkable results for English, using a combination of Bi-LSTMs (Bi-directional Long Short-Term Memory) and CRF. However, the approach is problematic for morphologically rich languages. The main problem is the missing information within word embed"
R19-1127,C14-1020,1,0.88617,"Missing"
R19-1127,P02-1060,0,0.180519,"ecent work for a number of other languages, and we also follow it in the present work. Early systems relied on hand-crafted rules with pattern-matching (Appelt et al., 1995). Unfortunately, this required an large pre-annotated datasets, collecting which was time-consuming and error-prone. The next step was to add gazetteers and lexicons that were generated automatically or semi-automatically (Popescu and Etzioni, 2005). Adding such resources required special approaches to resolve the ambiguity between names and common words. Such problems were solved using models such as Hidden Markov Models (Zhou and Su, 2002) and Conditional Random Fields (Sutton and McCallum, 2012). In our work here, we use deep neural networks for Bulgarian NER. Lample et al. (2016) have shown remarkable results for English, using a combination of Bi-LSTMs (Bi-directional Long Short-Term Memory) and CRF. However, the approach is problematic for morphologically rich languages. The main problem is the missing information within word embeddings for the numerous word forms involved in multiword names that require additional grammatical knowledge in order to be processed properly. Here we incorporate such information as additional in"
R19-1127,N03-1031,0,\N,Missing
R19-1127,H05-2017,0,\N,Missing
R19-1127,W02-2024,0,\N,Missing
savkov-etal-2012-linguistic,varadi-etal-2008-clarin,0,\N,Missing
savkov-etal-2012-linguistic,simov-etal-2004-language,1,\N,Missing
savkov-etal-2012-linguistic,N03-1033,0,\N,Missing
savkov-etal-2012-linguistic,P08-2009,0,\N,Missing
savkov-etal-2012-linguistic,E12-1050,1,\N,Missing
savkov-etal-2012-linguistic,R11-1065,1,\N,Missing
savkov-etal-2012-linguistic,gimenez-marquez-2004-svmtool,0,\N,Missing
savkov-etal-2012-linguistic,P10-4005,0,\N,Missing
simov-etal-2002-building,W00-1908,0,\N,Missing
simov-etal-2004-clark,W02-1712,1,\N,Missing
simov-etal-2004-clark,E03-2015,1,\N,Missing
simov-etal-2014-system,D07-1013,0,\N,Missing
simov-etal-2014-system,D10-1004,0,\N,Missing
simov-etal-2014-system,nivre-etal-2006-maltparser,0,\N,Missing
simov-etal-2014-system,N09-2066,0,\N,Missing
simov-etal-2014-system,P08-1108,0,\N,Missing
simov-etal-2017-towards,J15-4004,0,\N,Missing
simov-etal-2017-towards,H93-1061,0,\N,Missing
simov-etal-2017-towards,C04-1162,0,\N,Missing
simov-etal-2017-towards,N15-1165,0,\N,Missing
simov-osenova-2004-hybrid,A97-1012,0,\N,Missing
simov-osenova-2004-hybrid,W02-1712,1,\N,Missing
simov-osenova-2004-hybrid,P03-1014,0,\N,Missing
simov-osenova-2006-shallow,W03-2403,1,\N,Missing
simov-osenova-2006-shallow,J00-4005,0,\N,Missing
simov-osenova-2006-shallow,W05-0303,0,\N,Missing
simov-osenova-2006-shallow,W07-1525,0,\N,Missing
simov-osenova-2006-shallow,W03-2410,0,\N,Missing
simov-osenova-2010-constructing,W04-2607,0,\N,Missing
simov-osenova-2010-constructing,J02-2001,0,\N,Missing
ule-simov-2004-unexpected,W02-1015,0,\N,Missing
ule-simov-2004-unexpected,C02-1021,0,\N,Missing
W02-1712,grover-etal-2000-lt,0,\N,Missing
W03-2403,W00-1908,0,\N,Missing
W03-2403,simov-etal-2002-building,1,\N,Missing
W03-2904,P98-1050,0,0.215895,"Missing"
W03-2904,dzeroski-etal-2000-morphosyntactic,1,0.832959,"allel corpus; lexical resources (Ide et al., 1998); and tool resources for the seven languages. One of the objectives of M ULTEXT -East has been to make its resources freely available for research purposes. In the scope of the TELRI concerted action the results of M ULTEXT -East have been extended with several new languages. This edition is now available via the TELRI Research Archive of Computational Tools and Resources, at http://www.tractor.de/. Following the TELRI release, the M ULTEXT East resources have been used in a number of studies and experiments, e.g., (Tufi¸s, 1999; Hajiˇc, 2000; Džeroski et al., 2000). In the course of such work, errors and inconsistencies were discovered in the M ULTEXT -East specifications and data, most of which were subsequently corrected. But because this work was done at different sites and in different manners, the encodings of the resources had begun to drift apart. The EU Copernicus project C ONCEDE , Consortium for Central European Dictionary Encoding, which ran from ’98 to ’00 and comprised most of the same partners as M ULTEXT -East, offered the possibility to bring the versions back on a common footing. Although C ONCEDE was primarily devoted to machine readab"
W03-2904,erjavec-etal-2000-concede,1,0.795265,"d inconsistencies were discovered in the M ULTEXT -East specifications and data, most of which were subsequently corrected. But because this work was done at different sites and in different manners, the encodings of the resources had begun to drift apart. The EU Copernicus project C ONCEDE , Consortium for Central European Dictionary Encoding, which ran from ’98 to ’00 and comprised most of the same partners as M ULTEXT -East, offered the possibility to bring the versions back on a common footing. Although C ONCEDE was primarily devoted to machine readable dictionaries and lexical databases (Erjavec et al., 2000), one of its workpackages did consider the integration of the dictionary data with the M ULTEXT -East corpus. In the scope of this workpackage, the corrected morphosyntactically annotated corpus was normalised and re-encoded. This release of the M ULTEXT East resources (Erjavec, 2001a; Erjavec, 2001b) contains the revised and expanded morphosyntactic specifications, the revised lexica, and the significantly corrected and re-encoded 1984 corpus. In Table 1, we give all these connected resources by language, type and release. The ones marked by T belong to the TELRI edition, and those with C to"
W03-2904,A00-2013,0,0.126704,"Missing"
W03-2904,C94-1097,0,0.436931,"h. 1 Duško Vitas Faculty of Mathematics University of Belgrade vitas@matf.bg.ac.yu Introduction The mid-nineties saw — to a large extent via EU projects — the rapid development of multilingual language resources and standards for human language technologies. However, while the development of resources, tools, and standards was well on its way for EU languages, there had been no comparable efforts for the languages of Central and Eastern Europe. The M ULTEXT -East project (Multilingual Text Tools and Corpora for Eastern and Central European Languages) was a spin-off of the EU M ULTEXT project (Ide and Véronis, 1994); it developed standardised language resources for six languages (Dimitrova et al., 1998): Bulgarian, Czech, Estonian, Hungarian, Romanian, and Slovene, as well as for English, the ’hub’ language of the project. The main results of the project were an annotated multilingual corpus (Erjavec and Ide, 1998), comprising a speech corpus, a comparable corpus and a parallel corpus; lexical resources (Ide et al., 1998); and tool resources for the seven languages. One of the objectives of M ULTEXT -East has been to make its resources freely available for research purposes. In the scope of the TELRI con"
W03-2904,tadic-2002-building,1,0.799841,"e tags for the auxiliary verbs and a hybrid POS tag referring to family names and adjectives derived from names; the pronoun adverbials were made more fine-grained etc. This tagset is being used for the annotation of the BulTreeBank Text Archive. The lexicon is encoded as a regular grammar within the CLaRK system (Simov et al., 2001). 3.2 Croatian The Croatian specifications were compiled soon after the M ULTEXT -East project ended in 1997, using the project’s Final report as the template. These specifications are used in the PoS-tagging and lemmatisation of the Croatian National Corpus (Tadi´c, 2002). It was also selected for the format of MSDs accompanying word-forms in Croatian Morphological Lexicon (Tadi´c, 2003) which is conformant with M ULTEXT -East lexica. 3.3 Czech The morphological specifications for Czech were developed exclusively for the M ULTEXT -East project but the authors had already had some experience with the first draft of morphological specifications for Czech which is now thoroughly described in (Hajiˇc, 2002). These specifications and the resulting tagset developed by Hajiˇc are nowadays used as a standard for morphological and morphosyntactic annotations of the maj"
W03-2904,W03-2906,1,0.829387,"ronoun adverbials were made more fine-grained etc. This tagset is being used for the annotation of the BulTreeBank Text Archive. The lexicon is encoded as a regular grammar within the CLaRK system (Simov et al., 2001). 3.2 Croatian The Croatian specifications were compiled soon after the M ULTEXT -East project ended in 1997, using the project’s Final report as the template. These specifications are used in the PoS-tagging and lemmatisation of the Croatian National Corpus (Tadi´c, 2002). It was also selected for the format of MSDs accompanying word-forms in Croatian Morphological Lexicon (Tadi´c, 2003) which is conformant with M ULTEXT -East lexica. 3.3 Czech The morphological specifications for Czech were developed exclusively for the M ULTEXT -East project but the authors had already had some experience with the first draft of morphological specifications for Czech which is now thoroughly described in (Hajiˇc, 2002). These specifications and the resulting tagset developed by Hajiˇc are nowadays used as a standard for morphological and morphosyntactic annotations of the majority of Czech corpora, especially the 100 million word corpus of synchronic Czech developed within the Czech National"
W03-2904,C98-1049,0,\N,Missing
W07-1706,W06-2609,0,0.512131,"tive languages. These results are evaluated in section 5, where main problems, as well as some possible solutions, are discussed. Finally, section 6 concludes the paper. 43 Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have"
W07-1706,C92-2082,0,0.0138684,"Missing"
W07-1706,W04-1807,0,0.621777,"Missing"
W07-1706,C04-1199,0,0.2312,"Missing"
W07-1706,storrer-wellinghoff-2006-automated,0,0.346347,"ll as some possible solutions, are discussed. Finally, section 6 concludes the paper. 43 Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have been made for Slavic, with the exception of some work on Bulgarian (Tanev, 2004; Simov and"
W07-1706,W06-0203,0,0.205749,"s the paper. 43 Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have been made for Slavic, with the exception of some work on Bulgarian (Tanev, 2004; Simov and Osenova, 2005). Other work on Slavic information extraction has bee"
W07-1706,P99-1016,0,\N,Missing
W09-0305,papageorgiou-etal-2000-unified,1,0.757141,"ifficulties due the fuzziness in identifying sentence boundaries, and alignments at the phrase level were favored instead. 5 Handling and tokenization; following common practice, the Greek tokenizer makes use of a set of regular expressions, coupled with precompiled lists of abbreviations, and a set of simple heuristics (Papageorgiou et al., 2002) for the recognition of word and sentence boundaries, abbreviations, digits, and simple dates. • POS-tagging and lemmatization; a tagger that is based on Brill's TBL architecture (Brill, 1997), modified to address peculiarities of the Greek language (Papageorgiou et al., 2000) was used in order to assign morphosyntactic information to tokenized words. Furthermore, the tagger uses a PAROLEcompliant tagset of 584 different part-ofspeech tags. Following POS tagging, lemmas are retrieved from a Greek morphological lexicon. • Surface syntactic analysis; the Greek chunker is based on a grammar of 186 rules (Boutsis et al., 2000) developed for the automatic recognition of non-recursive phrasal categories: adjectives, adverbs, prepositional phrases, nouns, verbs (chunks) (Papageorgiou et al., 2002). • Term extraction; a Greek Term Extractor was used for spotting terms and"
W09-0305,W07-0906,0,0.0254402,"Missing"
W09-0305,P94-1051,0,0.158615,"Missing"
W09-0305,boutsis-etal-2000-robust,1,\N,Missing
W09-0305,papageorgiou-etal-2002-multi,1,\N,Missing
W09-0305,georgantopoulos-piperidis-2000-term,0,\N,Missing
W09-0305,giouli-etal-2006-multi,1,\N,Missing
W09-4105,J96-1002,0,0.0116859,"Missing"
W09-4105,P05-1022,0,0.0960431,"Missing"
W09-4105,J93-2004,0,0.0365081,"both timeconsuming and error-prone. Still, using machine learning has one major limitation: it requires manually annotated corpora as training data, which can be quite costly to create. Fortunately, for Bulgarian such a rich resource already exists – the BulTreeBank1 , an HPSGbased Syntactic Treebank with rich annotations at various linguistic levels. The existence of such a resource makes it possible to adapt to Bulgarian various nlp tools that have been originally developed for other languages, e.g., English, and that have been trained on similar kinds of resources, e.g., the Penn Treebank [4]. 1 (1) The features used in the OpenNLP framework combine heterogeneous contextual information such as words around the end of a sentence for the English sentence splitter, or word, character ?-grams and partof-speech tag alone and in various combinations for the English chunker. These features are based on the publications of Sha and Pereira [7] for the chunker, and on the dissertation of Ratnaparkhi [6] for the POS tagger and the syntactic parser. The remainder of this paper is organized as follows: Section 2 describes the process of converting the BulTreeBank XML data to Penn Treebank-styl"
W09-4105,N03-1028,0,0.0610098,"vels. The existence of such a resource makes it possible to adapt to Bulgarian various nlp tools that have been originally developed for other languages, e.g., English, and that have been trained on similar kinds of resources, e.g., the Penn Treebank [4]. 1 (1) The features used in the OpenNLP framework combine heterogeneous contextual information such as words around the end of a sentence for the English sentence splitter, or word, character ?-grams and partof-speech tag alone and in various combinations for the English chunker. These features are based on the publications of Sha and Pereira [7] for the chunker, and on the dissertation of Ratnaparkhi [6] for the POS tagger and the syntactic parser. The remainder of this paper is organized as follows: Section 2 describes the process of converting the BulTreeBank XML data to Penn Treebank-style bracketing, Section 3 describes the experiments and discusses the results, and Section 4 concludes and suggests directions for future work. Created at the Linguistic Modelling Laboratory (LML), Institute for Parallel Processing, Bulgarian Academy of Sciences. See http://www.bultreebank.org for details. 2 3 http://opennlp.sourceforge.net http://m"
W09-4105,P07-1096,0,0.128102,"Missing"
W11-4305,kruijff-korbayova-etal-2006-annotation,0,0.171164,"Missing"
W11-4305,C00-2163,0,0.0525937,"rictive. Finally, the use of the so called possible (also ambiguous, fuzzy or weak) links that signal correspondence between semantically and/or structurally nonequivalent words or phrases is also a matter of dispute. While some argue that alignment with possible links should be determined by unambiguous rules, formulated with consideration of inter-annotation agreement, others (Lambert et al. 2006) allow for different decisions to be kept, which is true to the role originally ascribed to this kind of links: “P (possible) alignment which is used for alignments which might or might not exist” (Och and Ney 2000). Both approaches for generation of MRS over the sentences are lexicalized; • Non-experts in linguistics can do the alignments successfully on word level; • Different rules for generation/testing are possible. Both parsers (for Bulgarian and English), which we use for the creation of MRSes, are lexicalized in their nature. Thus, they first assign elementary predicates to the lexical elements in the sentences, and then, on the base of the syntactic analysis, these elementary predicates are composed into MRSes for the corresponding phrases, and finally of the whole sentence. Our belief is that h"
W11-4305,ahrenberg-etal-2000-evaluation,0,0.0133027,"–38, Hissar, Bulgaria, 12-14 September 2011. • tradition established by the guidelines used in similar projects, aiming at the creation of golden standards for different language pairs, such as the Blinker project for English-French alignment (Melamed 1998), the alignment task for the Prague Czech-English Dependency Treebank 1.0 (Kruijff-Korbayová et al. 2006), the Dutch parallel Corpus project (Macken 2010), among others. As Lambert et al. (2006) point out, the alignment decisions presented in the guidelines reflect different tasks. There are projects such as ARCADE (Vèronis, 2000) and PLUG (Ahrenberg et al., 2000), which aim at building a reference corpora with word, not sentence pairs, and have a different annotation strategy in contrast to those that focus on sentence level. Different linguistic theoretical backgrounds appear to be another source of divergence that affects the rules of phrase alignments as well as the specific grammatical techniques. This holds especially in correspondences between synsemantic words (like prepositions, determiners, particles, auxiliary verbs) and synsemantic and/or autosemantic words (Macken 2010). In addition, some tools for manual word alignment, e.g. HandAlign1, a"
W11-4305,R11-1065,1,0.809618,"on a sentence level. Then the alignment is done on the level of MRS. This level of abstraction makes possible the usage of different tools for producing these alignments, since MRS is meant to be compatible with various syntactic frameworks. The chosen procedure is as follows: first, the Bulgarian sentences are parsed with BURGER. If it succeeds, then the produced MRSes are used for the alignment. In case BURGER fails, the sentences are parsed with Malt Parser, and then MRSes are constructed on the base of the dependency analysis. The latter MRSes are created via a set of transfer rules (see Simov and Osenova 2011). In both cases we keep the syntactic analyses for the parallel sentences. With respect to the MRS alignments, a very pragmatic approach has been adopted – namely, the MRS alignments originated from the word level alignment. This approach is based on the following observations and requirements: Abstract The paper describes the basic strategies behind the word and semantic level alignment in the Bulgarian-English treebank. The word level alignment has taken into consideration the experience within other NLP groups in the context of the Bulgarian language specific features. The semantic level al"
W11-4305,copestake-flickinger-2000-open,0,\N,Missing
W11-4305,nivre-etal-2006-maltparser,0,\N,Missing
W12-0116,J99-2004,0,0.0674614,"y Koehn and Hoang (2007), as an extension of the traditional phrasebased SMT framework. Instead of using only the word form of the text, it allows the system to take a vector of factors to represent each token, both for the source and target languages. The vector of factors can be used for different levels of linguistic annotations, like lemma, part-of-speech (POS), or other linguistic features. Furthermore, this extension actually allows us to incorporate various kinds of features if they can be (somehow) represented as annotations to the tokens. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex 2 http://www.bultreebank.org/dpbtb/ http://ufal.mff.cuni.cz/conll2009-st/ task-description.html 3 constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language"
W12-0116,W02-1502,0,0.116822,"49 50 48 49 41 49 2 0 3 5 5 6 11 12 5 3 7 5 0 4 3 2 3 5 5 10 7 20 5 4 32 37 34 46 51 34 29 23 30 8 34 39 41 31 37 37 44 38 52 51 5 46 40 43 44 34 43 38 34 34 9 44 40 38 47 41 40 40 36 76 94 Total 487 479 483 520 501 482 443 410 433 242 485 482 478 487 483 480 503 471 652 689 Table 4: Manual evaluation of the grammaticality exist quite extensive implemented formal HPSG grammars for English (Copestake and Flickinger, 2000), German (M¨uller and Kasper, 2000), and Japanese (Siegel, 2000; Siegel and Bender, 2002). HPSG is the underlying theory of the international initiative LinGO Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a first version of the Bulgarian Resource Grammar - BURGER. In the research reported here, we use the linguistic modeled knowledge from the existing English and Bulgarian grammars. Since the Bulgarian grammar has limited coverage on news data, dependency parsing has been performed instead. Then, mapping rules have been defined for the construction"
W12-0116,W07-0702,0,0.290181,"annotations to the tokens. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex 2 http://www.bultreebank.org/dpbtb/ http://ufal.mff.cuni.cz/conll2009-st/ task-description.html 3 constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. We primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed in and out of our project. In particular, we consider the following factors on the source language side (Bulgarian): • WF - word form is just the original text token. • L EMMA is the lexical invariant of the original word form. We use the lemmatizer described in Section 3, which operates on the output from the POS tag"
W12-0116,2005.mtsummit-osmtw.3,0,0.300693,"ammars. Since the Bulgarian grammar has limited coverage on news data, dependency parsing has been performed instead. Then, mapping rules have been defined for the construction of RMRSes. However, the MRS representation is still quite close to the syntactic level, which is not fully language independent. This requires a transfer at the MRS level, if we want to do translation from the source language to the target language. The transfer is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON project (Oepen et al., 2004), the transfer rules were hand-written (Bond et al., 2005; Oepen et al., 2007), which included a large amount of manual work. Graham and van Genabith (2008) and Graham et al. (2009) explored the automatic rule induction approach in a transfer-based MT setting involving two lexical functional grammars (LFGs), which was still restricted by the performance of both the parser and the generator. Lack of robustness for target side generation is one of the main issues, when various ill-formed or fragmented structures come out after transfer. Oepen et al. (2007) use their generator to generate text fragments instead of full sentences, in order to increase t"
W12-0116,W11-2103,0,0.0710508,"Missing"
W12-0116,W09-0405,0,0.0263186,"Missing"
W12-0116,copestake-flickinger-2000-open,0,0.587553,"emantic analysis of Bulgarian text is inspired by the work on MRS and RMRS (Robust Minimal Recursion Semantic) (see (Copestake, 2003) and (Copestake, 2007)) and the previous work on transfer of dependency analyses into RMRS structures described in (Spreyer and Frank, 2005) and (Jakob et al., 2010). In this section we present first a short overview of MRS and RMRS. Then we discuss the new features added on the basis of the RMRS structures. MRS is introduced as an underspecified semantic formalism (Copestake et al., 2005). It is used to support semantic analyses in the English HPSG grammar ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG. The main idea is that the formalism avoids spelling out the complete set of readings resulting from the interaction of scope bearing operators and quantifiers, instead providing a single underspecified representation from which the complete set of readings can be constructed. Here we will present only basic definitions from (Copestake et al., 2005). For more details the cited publication should be consulted. An MRS structure is a tuple h GT , R, C i, where GT is the top handle, R is a bag of EPs (elementary predicates) and C is a bag of handle c"
W12-0116,E12-1050,1,0.83541,"Missing"
W12-0116,gimenez-marquez-2004-svmtool,0,0.107385,"Missing"
W12-0116,2008.eamt-1.10,0,0.341103,"Missing"
W12-0116,P07-1037,0,0.183167,"Missing"
W12-0116,jakob-etal-2010-mapping,0,0.197379,"eech (POS), or other linguistic features. Furthermore, this extension actually allows us to incorporate various kinds of features if they can be (somehow) represented as annotations to the tokens. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex 2 http://www.bultreebank.org/dpbtb/ http://ufal.mff.cuni.cz/conll2009-st/ task-description.html 3 constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. We primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed in and out of our project. In particular, we consider the following factors on the source language side (Bulgarian): • WF - word form is"
W12-0116,D07-1091,0,0.414107,"Missing"
W12-0116,P07-2045,0,0.0120552,"n the last two columns of Table 1. All these factors encoded within the corpus provide us with a rich selection of factors for different experiments. Some of them are presented within the next section. The model of encoding MRS information in the corpus as additional features does not depend on the actual semantic analysis — MRS or RMRS, because both of them provide enough semantic information. 6 Experiments 6.1 Experiments with the Bulgarian raw corpus To run the experiments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses4 (Koehn et al., 2007). For training the translation model, the parallel corpora (mentioned in Section 2) were preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: • We run GIZA++ (Och and Ney, 2003) for bidirectional word alignment, and then obtain the lexical translation table and phrase table. • A tri-gram language model is estimated using the SRILM toolkit (Stolcke, 2002). • Minimum error rate training (MERT) (Och, 2003) is applied to tune the weights for the set of feature weights that maximizes the official f-score evaluation metric on the developmen"
W12-0116,2009.mtsummit-papers.7,0,0.0526821,"various ways to do the combination/integration. Thurmair (2009) summarized several different architectures of hybrid systems using SMT and RBMT systems. Some widely used ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses. For the language pair Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the BulgarianEnglish translation direction (61.3), which inspired us to further investigate this direction. In this paper, we focus on the Bulgarian-toEnglish translation and mainly explore the approach of annotating the SMT baseline with linguistic features derived from the preprocessing and hand-crafted grammars. There are three motivations behind our approach: 1) the SMT baseline trained on a decent amount of parallel corpora outp"
W12-0116,J10-4005,0,0.0307297,"hine translation (SMT) system (as backbone) with deep linguistic features (as factors). The motivation is to take advantages of the robustness of the SMT system and the linguistic knowledge of morphological analysis and the hand-crafted grammar through system combination approach. The preliminary evaluation has shown very promising results in terms of BLEU scores (38.85) and the manual analysis also confirms the high quality of the translation the system delivers. 1 Introduction In the recent years, machine translation (MT) has achieved significant improvement in terms of translation quality (Koehn, 2010). Both data-driven approaches (e.g., statistical MT (SMT)) and knowledge-based (e.g., rule-based MT (RBMT)) have achieved comparable results shown in the evaluation campaigns (CallisonBurch et al., 2011). However, according to the human evaluation, the final outputs of the MT systems are still far from satisfactory. Fortunately, recent error analysis shows that the two trends of the MT approaches tend to be complementary to each other, in terms of the types of the errors they made (Thurmair, 2005; Chen et al., 2009). Roughly speaking, RBMT systems often have missing lexicon and thus lack of ro"
W12-0116,J03-1002,0,0.00388788,"n in the corpus as additional features does not depend on the actual semantic analysis — MRS or RMRS, because both of them provide enough semantic information. 6 Experiments 6.1 Experiments with the Bulgarian raw corpus To run the experiments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses4 (Koehn et al., 2007). For training the translation model, the parallel corpora (mentioned in Section 2) were preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: • We run GIZA++ (Och and Ney, 2003) for bidirectional word alignment, and then obtain the lexical translation table and phrase table. • A tri-gram language model is estimated using the SRILM toolkit (Stolcke, 2002). • Minimum error rate training (MERT) (Och, 2003) is applied to tune the weights for the set of feature weights that maximizes the official f-score evaluation metric on the development set. The rest of the parameters we use the default setting provided by Moses. 4 http://www.statmt.org/moses/ We split the corpora into the training set, the development set and the test set. For SETIMES, the split is 100,000/500/1,000"
W12-0116,P03-1021,0,0.0201119,"ments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses4 (Koehn et al., 2007). For training the translation model, the parallel corpora (mentioned in Section 2) were preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: • We run GIZA++ (Och and Ney, 2003) for bidirectional word alignment, and then obtain the lexical translation table and phrase table. • A tri-gram language model is estimated using the SRILM toolkit (Stolcke, 2002). • Minimum error rate training (MERT) (Och, 2003) is applied to tune the weights for the set of feature weights that maximizes the official f-score evaluation metric on the development set. The rest of the parameters we use the default setting provided by Moses. 4 http://www.statmt.org/moses/ We split the corpora into the training set, the development set and the test set. For SETIMES, the split is 100,000/500/1,000 and for EMEA, it is 700,000/500/1,000. For reference, we also run tests on the JRC-Acquis corpus5 . The final results under the standard evaluation metrics are shown in the following table in terms of BLEU (Papineni et al., 2002)"
W12-0116,2004.tmi-1.2,0,0.372979,"Missing"
W12-0116,2007.tmi-papers.18,0,0.672773,"Missing"
W12-0116,P02-1040,0,0.0879012,"me widely used ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses. For the language pair Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the BulgarianEnglish translation direction (61.3), which inspired us to further investigate this direction. In this paper, we focus on the Bulgarian-toEnglish translation and mainly explore the approach of annotating the SMT baseline with linguistic features derived from the preprocessing and hand-crafted grammars. There are three motivations behind our approach: 1) the SMT baseline trained on a decent amount of parallel corpora outputs surprisingly good results, in terms of both statistical evaluation metrics and preliminary manual evaluation; 2) the augmented model gives 119 Proceedings"
W12-0116,W02-1210,0,0.25698,"R EFERENCE 1 20 20 20 15 15 20 32 45 34 101 19 19 20 19 19 19 15 20 0 0 2 47 48 47 34 38 48 48 41 47 32 49 49 49 50 48 49 41 49 2 0 3 5 5 6 11 12 5 3 7 5 0 4 3 2 3 5 5 10 7 20 5 4 32 37 34 46 51 34 29 23 30 8 34 39 41 31 37 37 44 38 52 51 5 46 40 43 44 34 43 38 34 34 9 44 40 38 47 41 40 40 36 76 94 Total 487 479 483 520 501 482 443 410 433 242 485 482 478 487 483 480 503 471 652 689 Table 4: Manual evaluation of the grammaticality exist quite extensive implemented formal HPSG grammars for English (Copestake and Flickinger, 2000), German (M¨uller and Kasper, 2000), and Japanese (Siegel, 2000; Siegel and Bender, 2002). HPSG is the underlying theory of the international initiative LinGO Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a first version of the Bulgarian Resource Grammar - BURGER. In the research reported here, we use the linguistic modeled knowledge from the existing English and Bulgarian grammars. Since the Bulgarian grammar has limited coverage on news data,"
W12-0116,W10-3811,0,0.19524,", like lemma, part-of-speech (POS), or other linguistic features. Furthermore, this extension actually allows us to incorporate various kinds of features if they can be (somehow) represented as annotations to the tokens. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex 2 http://www.bultreebank.org/dpbtb/ http://ufal.mff.cuni.cz/conll2009-st/ task-description.html 3 constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. We primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed in and out of our project. In particular, we consider the following factors on the source language side (Bulgarian): • WF - word form is"
W12-0116,2009.mtsummit-posters.21,0,0.0438651,"ling linguistic phenomena requiring syntactic information better. SMT systems, on Petya Osenova and Kiril Simov Linguistic Modelling Department, IICT Bulgarian Academy of Sciences Sofia, Bulgaria {petya,kivs}@bultreebank.org the contrary, are in general more robust, but sometimes output ungrammatical sentences. In fact, instead of competing with each other, there is also a line of research trying to combine the advantages of the two sides using a hybrid framework. Although many systems can be put under the umbrella of “hybrid” systems, there are various ways to do the combination/integration. Thurmair (2009) summarized several different architectures of hybrid systems using SMT and RBMT systems. Some widely used ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses. For the language pair Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the Europea"
W12-4202,P08-1087,0,0.0178885,"04; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other popular domains like news. They reported a very high BLEU"
W12-4202,W07-0702,0,0.52452,"m deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part. There was a system published by Koehn et al. (2009), which was t"
W12-4202,2005.mtsummit-osmtw.3,0,0.426421,", 2003) and (Copestake, 2007)) and the previous work on transfer of dependency analyses into RMRS structures described in (Spreyer and Frank, 2005) and (Jakob et al., 2010). Although being a semantic representation, MRS is still quite close to the syntactic level, which is not fully language independent. This requires a transfer at the MRS level, if we want to do translation from the source language to the target language. The transfer is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON project (Oepen et al., 2004), the transfer rules were hand-written (Bond et al., 2005; Oepen et al., 2007), which included a large amount of manual work. Graham and van Genabith (2008) and Graham et al. (2009) explored the automatic rule induction approach in a transfer-based MT setting involving two lexical functional grammars (LFGs)1 , which was still restricted by the performance of both the parser and the generator. Lack of robustness for target side generation is one of the main issues, when various ill-formed or fragmented structures come out after transfer. Oepen et al. (2007) used their generator to generate text fragments instead of full sentences, in order to increas"
W12-4202,W11-2103,0,0.0593074,"language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them a"
W12-4202,P10-1146,0,0.0203833,"nual analysis confirms the high quality of the translation the system delivers. The whole framework is also extensible for incorporating information provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token le"
W12-4202,copestake-flickinger-2000-open,0,0.223024,"e) kompanii (companies) politicite (politicians-the) zloupotrebyavat (abuse) s (with) dyrzhavnite (state-the) predpriyatiya (enterprises). The translation in the original source is : “electricity audits prove politicians abusing public companies.” The result from the linguistic processing are presented in Table 1. As for the deep linguistic knowledge, we also extract features from the semantic analysis — Minimal Recursion Semantics (MRS). MRS is introduced as an underspecified semantic formalism (Copestake et al., 2005). It is used to support semantic analyses in the English HPSG grammar ERG (Copestake and Flickinger, 2000), but also in other grammar formalisms like LFG. The main idea is that the formalism avoids spelling out the complete set of readings resulting from the interaction of scope bearing operators and quantifiers, instead providing a single underspecified representation from which the complete set of readings can be constructed. Here we will present only basic definitions from (Copestake et al., 2005). For more details the cited publication should be consulted. An MRS structure is a tuple h GT , R, C i, where GT is the top handle, R is a bag of EPs (elementary predicates) and C is a bag of handle c"
W12-4202,W11-2107,0,0.0245914,"el is estimated using the SRILM toolkit (Stolcke, 2002). For the rest of the parameters we use the default setting provided by Moses. Notice that, since on the target language side (i.e., English) we do not have any other factors than the word form, the factor-based models we use here only differentiate from each other in the translation phase, i.e., there is no ‘generation’ models involved. 4.1 Automatic Evaluation Metrics The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al., 2002) and M ETEOR (Denkowski and Lavie, 2011). We then design various configurations to test the effectiveness of different linguistic annotations described in Section 3. The detailed configurations we considered are shown in the first column of Table 3. The first impression is that the BLEU scores in general are high. These models can be roughly 4 http://www.statmt.org/moses/ OPUS — an open source parallel corpus, http:// opus.lingfil.uu.se/ 6 http://optima.jrc.it/Acquis/ 7 We did not preform MERT (Och, 2003), as it is quite computationally heavy for such various configurations. 5 14 grouped into six categories (separated by double line"
W12-4202,N04-1035,0,0.038652,"for incorporating information provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Av"
W12-4202,E12-1050,1,0.611273,"ating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. Instead, we primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed within and out of our project. Firstly, the data was processed by the NLP pipe for Bulgarian (Savkov et al., 2012) including a morphological tagger, GTagger (Georgiev et al., 2012), a lemmatizer and a dependency parser2 . Then we consider the following factors on the source language side (Bulgarian): • WF – word form is just the original text token. • L EMMA is the lexical invariant of the original word form. We use the lemmatizer, which operates on the output from the POS tagging. Thus, the 3rd person, plural, imperfect tense verb form ‘varvyaha’ (‘walking-were’, They were walking) is lemmatized as the 1st person, present tense verb ‘varvya’. 2 We have trained the MaltParser3 (Nivre et al., 2007) on the dependency version of BulTreeBank: http://www. bultreebank.org/dpb"
W12-4202,2008.eamt-1.10,0,0.210197,"Missing"
W12-4202,P07-1037,0,0.241861,"Missing"
W12-4202,jakob-etal-2010-mapping,0,0.203626,"Missing"
W12-4202,D07-1091,0,0.349531,"ation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there ha"
W12-4202,P07-2045,0,0.0097964,"nformation provided by the MRS, e.g., we throw away the scopal information and the other arguments of the relations. Those kinds of information is not straightforward to be represented in such ‘tagging’-style models, which will be tackled in the future. The extra information for the example sentence is represented in Table 2. All these factors encoded within the corpus provide us with a rich selection of features for different experiments. 4 Experiments To run the experiments, we use the phrase-based translation model provided by the open-source statistical machine translation system, Moses4 (Koehn et al., 2007). For training the translation model, the SETIMES parallel corpus has been used, which is part of the OPUS parallel corpus5 . As for the choice of the datasets, the language is more diverse in the news articles, compared with other corpora in more controlled settings, e.g., the JRC-Acquis corpus6 used by Koehn et al. (2009). We split the corpus into the training set and the test set by 150,000 and 1,000 sentence pairs respectively7 . Both datasets are preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: We run GIZA++ (Och and Ney, 200"
W12-4202,2009.mtsummit-papers.7,0,0.444698,"token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other popular domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the Bulgarian-English translation direction (61.3). Apart from being morphologically-rich, Bulgarian has a number of challenging linguistic phenomena to consider, including free word order, long distance dependency, coreference relations, clitic doubling, etc. For instance, the following two sentences: (1) Momcheto j go dava buketa na Boy-the her-dat it-acc gives bouquet-the to momicheto. girl-the. The boy gives the bouquet t"
W12-4202,J10-4005,0,0.0428019,"morphological analysis as well as the hand-crafted grammar resources. The automatic evaluation has shown promising results and our extensive manual analysis confirms the high quality of the translation the system delivers. The whole framework is also extensible for incorporating information provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT"
W12-4202,P06-1077,0,0.0256157,"ormation provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn"
W12-4202,J03-1002,0,0.00273886,"hn et al., 2007). For training the translation model, the SETIMES parallel corpus has been used, which is part of the OPUS parallel corpus5 . As for the choice of the datasets, the language is more diverse in the news articles, compared with other corpora in more controlled settings, e.g., the JRC-Acquis corpus6 used by Koehn et al. (2009). We split the corpus into the training set and the test set by 150,000 and 1,000 sentence pairs respectively7 . Both datasets are preprocessed with the tokenizer and lowercase converter provided by Moses. Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table. A tri-gram language model is estimated using the SRILM toolkit (Stolcke, 2002). For the rest of the parameters we use the default setting provided by Moses. Notice that, since on the target language side (i.e., English) we do not have any other factors than the word form, the factor-based models we use here only differentiate from each other in the translation phase, i.e., there is no ‘generation’ models involved. 4.1 Automatic Evaluation Metrics The baseline results (non-factored model) under t"
W12-4202,P03-1021,0,0.00953047,"rd evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al., 2002) and M ETEOR (Denkowski and Lavie, 2011). We then design various configurations to test the effectiveness of different linguistic annotations described in Section 3. The detailed configurations we considered are shown in the first column of Table 3. The first impression is that the BLEU scores in general are high. These models can be roughly 4 http://www.statmt.org/moses/ OPUS — an open source parallel corpus, http:// opus.lingfil.uu.se/ 6 http://optima.jrc.it/Acquis/ 7 We did not preform MERT (Och, 2003), as it is quite computationally heavy for such various configurations. 5 14 grouped into six categories (separated by double lines): word form with linguistic features; lemma with linguistic features; models with dependency features; MRS elementary predicates (EP) and the type of the main argument of the predicate (E OV); EP features without word forms; and EP features with MRS ARGn features. In terms of the resulting scores, POS and Lemma seem to be effective features, as Model 2 has the highest BLEU score and Model 4 the best M ETEOR score. Model 3 indicates that linguistic features also im"
W12-4202,2004.tmi-1.2,0,0.501316,"Missing"
W12-4202,2007.tmi-papers.18,0,0.819256,"Missing"
W12-4202,P02-1040,0,0.0846502,"ocused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). However, all of them attempt to enrich the English part of the language pairs being translated. For the language pairs like Bulgarian-English, there has not been much study on it, mainly due to the lack of resources, including corpora, preprocessors, etc, on the Bulgarian part. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other popular domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the Bulgarian-English translation direction (61.3). Apart from being morphologically-rich, Bulgarian has a number of challenging linguistic phenomena to consider, including free word order, long distance dependency, coreference relations, clitic doubling, etc. For instance, the following two sentences: (1) Momcheto j go dava buketa na Boy-the her-dat it-acc gives bouquet-the to momicheto. girl-the. The boy gives the bouquet to the girl. (2) Momcheto j go dava. Boy-the her-dat it-acc gives. The boy gives it to her. 10 Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure"
W12-4202,W10-3811,0,0.355234,"llows the system to take a vector of factors to represent each token, both for the source and target languages. The vector of factors can be used for different levels of linguistic annotations, like lemma, part-of-speech, or other linguistic features, if they can be (somehow) represented as annotations to each token. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns “rich descriptions (supertags) that impose complex constraints in a local context”. In our case, all the linguistic features (factors) associated with each token form a supertag to that token. Singh and Bandyopadhyay (2010) had a similar idea of incorporating linguistic features, while they worked on Manipuri-English bidirectional translation. Our approach is slightly different from (Birch et al., 2007) and (Hassan et al., 2007), who mainly used the supertags on the target language side, English. Instead, we primarily experiment with the source language side, Bulgarian. This potentially huge feature space provides us with various possibilities of using our linguistic resources developed within and out of our project. Firstly, the data was processed by the NLP pipe for Bulgarian (Savkov et al., 2012) including a"
W12-4202,J97-3002,0,0.029077,"results and our extensive manual analysis confirms the high quality of the translation the system delivers. The whole framework is also extensible for incorporating information provided by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate ling"
W12-4202,S10-1009,0,0.0463388,"Missing"
W12-4202,P08-1064,0,0.0203049,"by different sources. 1 Introduction Incorporating linguistic knowledge into statistical models is an everlasting topic in natural language processing. The same story happens in the machine translation community. Along with the success of statistical machine translation (SMT) models (summarized by Koehn (2010)), various approaches have been proposed to include linguistic information, ranging from early work by Wu (1997) to recent work by Chiang (2010), from deep transferbased models (Graham and van Genabith, 2008) to mapping rules at the syntactic level (Galley et al., 2004; Liu et al., 2006; Zhang et al., 2008). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al., 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. Koehn and Hoang (2007) proposed a factored SMT model as an extension of the traditional phrase-based SMT model, which opens up an easy way to incorporate linguistic knowledge at the token level. Birch et al. (2007) and Hassan et al. (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused o"
W12-4202,J99-2004,0,\N,Missing
W12-4202,savkov-etal-2012-linguistic,1,\N,Missing
W13-5205,C02-2025,0,0.0419884,"names. A geographical database. The different LOD datasets are interlinked via owl:sameAs statements which state that some individuals (represented via different URIs) denote the same object in the world. 16 Proceedings of the Joint Workshop on NLP&LOD and SWAIE, pages 16–22, Hissar, Bulgaria, 12 September 2013. 2012), BulTreeBank (Simov et al., 2004) treebanks and the infrastructure for dynamic treebanking INESS (Ros´en et al., 2012). For example, the Redwoods treebank was compiled by coupling English Resource Grammar (ERG) and a tree selection module of [incr tsdb()] (see (Oepen, 1999) and (Oepen et al., 2002)). ERG produces very detailed syntacto-semantic analyses of the input sentence. For many sentences, HPSG processor (e.g. LKB — (Copestake, 2002)) overgenerates, producing analyses that are not acceptable. From the complete analyses different components can be extracted in order to highlight different views over the analyses: (1) derivation trees composed of identifiers of lexical items and constructions used to build the analysis; (2) phrase structure trees; and (3) underspecified MRS representations. From these types of information the most important with respect to the treebank construction"
W13-5205,E03-2015,1,0.748219,"ion cycle, not just the selection of correct analysis. The cycle would include also: creation of processing procedure directly over RDF graphs like regular grammars, rules, transformation scripts, etc. These services will include many standard tools that already exist in the world of Linked Open Data. But there are also needs for new specific tools which are specially designed for the creation and management of KBLR. Another group of tools necessary to be implemented are: visualization and editing facilities. In many respects we will follow the design and the implementation of CLaRK system — (Simov et al., 2003). In this context we consider RDF graphs corresponding to XML documents, ontologies corresponding to DTDs or XML schemas, etc. We believe that this is the way in which exploitation of language resources and technologies will be made widely used. Such a system is especially important within initiatives like CLARIN4 whose huge target group of end users would like to exploit the available language resources and tools for their specific tasks. Many of them are not familiar with the principles behind the language resources and technologies. Knowledge-based system are perfect to support such kind of"
W14-6102,N09-2066,0,0.0480908,"Missing"
W14-6102,Q13-1034,0,0.02225,"2012) employ an incremental joint approach to solve three tasks in Chinese: word segmentation, POS tagging, and dependency parsing. The motivation for solving them simultaneously is that some segmentation ambiguities in the language cannot be resolved without considering the surrounding grammatical constructions, while syntactic information can improve the segmentation of outof-vocabulary words. Parsing is done through a dynamic programming framework – a version of the shift-reduce algorithm. Joint morphological and syntactic analysis of several morphologically rich languages is presented in (Bohnet et al., 2013). They use an extended transition system for dependency parsing to incorporate POS tagging, tagging with morphological descriptions and lemmas. In addition they define new evaluation metrics. They include the standard POS accuracy, Labeled and Unlabled Arc Accuracy, but also accuracy of combination of features like POS tags, morphological description, lemmas and dependency arcs. Several experiments with different parameters controlling the selection of best tags and morphosyntactic descriptions are presented. The approach presented in our work is joint in the sense that we solve two tasks simu"
W14-6102,W06-2920,0,0.226302,"Missing"
W14-6102,D07-1022,0,0.0304032,"tion of trees. We show that the results also depend on the algorithm for tree construction. Joint models have been successfully used for processing other morphologically rich languages. For instance, (Lee et al., 2011) propose a joint model for inference of morphological properties and syntactic structures, which outperforms a standard pipelined solution when tested on highly-inflected languages such as Latin, Czech, Ancient Greek and Hungarian. It uses a graphical model that employs “local” and “link” factors to impose local word context constraints and to handle long-distance dependencies. (Cohen and Smith, 2007) and (Goldberg and Tsarfaty, 2008) focus on a joint model for morphological segmentation and syntactic parsing with application to Hebrew. The authors argue that syntactic context is crucial for the correct segmentation of tokens into lexemes and that a model wherein the segmentation and parsing modules share information during processing is better suited to carry out the task. To solve the two tasks jointly, the different morphological analyses of a given utterance are represented simultaneously in a lattice structure; a path through the lattice corresponds to a specific morphological segment"
W14-6102,ghayoomi-etal-2014-constituency,1,0.507716,"Missing"
W14-6102,P08-1043,0,0.0290457,"the results also depend on the algorithm for tree construction. Joint models have been successfully used for processing other morphologically rich languages. For instance, (Lee et al., 2011) propose a joint model for inference of morphological properties and syntactic structures, which outperforms a standard pipelined solution when tested on highly-inflected languages such as Latin, Czech, Ancient Greek and Hungarian. It uses a graphical model that employs “local” and “link” factors to impose local word context constraints and to handle long-distance dependencies. (Cohen and Smith, 2007) and (Goldberg and Tsarfaty, 2008) focus on a joint model for morphological segmentation and syntactic parsing with application to Hebrew. The authors argue that syntactic context is crucial for the correct segmentation of tokens into lexemes and that a model wherein the segmentation and parsing modules share information during processing is better suited to carry out the task. To solve the two tasks jointly, the different morphological analyses of a given utterance are represented simultaneously in a lattice structure; a path through the lattice corresponds to a specific morphological segmentation of the utterance. In (Cohen"
W14-6102,P12-1110,0,0.0217323,"ens into lexemes and that a model wherein the segmentation and parsing modules share information during processing is better suited to carry out the task. To solve the two tasks jointly, the different morphological analyses of a given utterance are represented simultaneously in a lattice structure; a path through the lattice corresponds to a specific morphological segmentation of the utterance. In (Cohen and Smith, 2007), paths in the lattice and parse trees are combined through a joint probability model and the best combination is found through chart parsing. 2 www.bultreebank.org/dpbtb/ 16 (Hatori et al., 2012) employ an incremental joint approach to solve three tasks in Chinese: word segmentation, POS tagging, and dependency parsing. The motivation for solving them simultaneously is that some segmentation ambiguities in the language cannot be resolved without considering the surrounding grammatical constructions, while syntactic information can improve the segmentation of outof-vocabulary words. Parsing is done through a dynamic programming framework – a version of the shift-reduce algorithm. Joint morphological and syntactic analysis of several morphologically rich languages is presented in (Bohne"
W14-6102,P11-1089,0,0.0185257,"oting. In our work we show that depending on the feature selection for meta-classification, it can actually outperform the voting approach. The experiments presented in (Surdeanu and Manning, 2010) do not use a specific algorithm for the selection of dependencies, and do not ensure that the result of voting is a well-formed dependency tree. In our work we use two algorithms to ensure the construction of trees. We show that the results also depend on the algorithm for tree construction. Joint models have been successfully used for processing other morphologically rich languages. For instance, (Lee et al., 2011) propose a joint model for inference of morphological properties and syntactic structures, which outperforms a standard pipelined solution when tested on highly-inflected languages such as Latin, Czech, Ancient Greek and Hungarian. It uses a graphical model that employs “local” and “link” factors to impose local word context constraints and to handle long-distance dependencies. (Cohen and Smith, 2007) and (Goldberg and Tsarfaty, 2008) focus on a joint model for morphological segmentation and syntactic parsing with application to Hebrew. The authors argue that syntactic context is crucial for t"
W14-6102,D10-1004,0,0.0469265,"Missing"
W14-6102,D11-1022,0,0.0448931,"Missing"
W14-6102,D07-1013,0,0.0380036,"rk on joint models and ensemble models; in Section 3 we introduce related work on Bulgarian parsing and POS tagging; in Section 4 we present our ensemble model; in Section 5 we report on our current experimental setup, including the construction of a parsebank of parses and tagging results; Section 6 presents the results from our ensemble experiments; the last section concludes the paper. 2 Related Work Our work on ensemble systems for dependency parsing is inspired by the in-depth performance analysis of two of the most influential dependency parsing models: transition-based and graph-based (McDonald and Nivre, 2007). This analysis shows that the two frameworks make different errors when trained and tested on the same datasets. The authors conclude the paper by proposing three approaches for using the advantages of both frameworks: (1) ensemble systems – weighted combinations of the output of both systems; (2) hybrid systems – a single system designed to integrate the strengths of the individual ones; and (3) novel approaches – based on a combination of new training and inference methods. In their further work (Nivre and McDonald, 2008) on the subject they present a hybrid system that combines the two mod"
W14-6102,P08-1108,0,0.0315297,"luential dependency parsing models: transition-based and graph-based (McDonald and Nivre, 2007). This analysis shows that the two frameworks make different errors when trained and tested on the same datasets. The authors conclude the paper by proposing three approaches for using the advantages of both frameworks: (1) ensemble systems – weighted combinations of the output of both systems; (2) hybrid systems – a single system designed to integrate the strengths of the individual ones; and (3) novel approaches – based on a combination of new training and inference methods. In their further work (Nivre and McDonald, 2008) on the subject they present a hybrid system that combines the two models. The work presented in this paper is along the lines of their first suggestion – a system to facilitate the combination of the outputs of several parsing and tagging models, in order to find an optimal solution. An experiment with ensemble systems is presented in (Surdeanu and Manning, 2010). This work describes several approaches to the combination of dependency parsers via different types of voting and meta-classification. Voting determines the correct dependency arcs by choosing the ones that are selected by the major"
W14-6102,nivre-etal-2006-maltparser,0,0.13345,"Missing"
W14-6102,simov-etal-2014-system,1,0.792263,"we define the experimental setup: the creation of parsebank where for each tree in the original treebank a dependency graph is created; the definition of voting approaches and the machine learning weighting. 5 Experimental Setup In this section we present in detail the way in which our ensemble experiment was set up, including the data format and choice of ranking and features for machine learning. 5.1 Tagger and Parser Models and Parsebank In the current experiments we use the five parsing models which achieved the highest LAS and UAS scores for the BulTreeBank data in previous experiments3 (Simov et al., 2014). They include two MaltParser models, MLT07 and MLT09, one MSTParser model, MST05, one TurboParser model, Turbo02, and one Mate-tools Parser model, MATE01. The following configurations were used for each model: 1. MLT07 - Convington non-projective algorithm with extended feature set for lemmas. 2. MLT09 - Stack eager algorithm with extended feature set for morphosyntactic descriptions. 3. MST05 - default parser settings, with the exception of the order of features. The parser was set to use features over pairs of adjacent edges (second-order: true). 3 The names of the models were left unchange"
W14-6102,N10-1091,0,0.31114,"reer word order. In this paper we present several experiments in which we simultaneously solve two of the aforementioned tasks – tagging and parsing. The motivation behind this idea is that the two tasks are highly dependent on each other when working with a morphologically rich language, and thus a better solution could be found for each if they are solved jointly. We assemble the outputs of three morphosyntactic taggers (POS taggers) and five dependency parsers in a single step. The ensemble approach uses weights in order to select the best solution from a number of alternatives. We follow (Surdeanu and Manning, 2010) and use two classes of approaches for selecting weights for the alternatives: voting, where the weights are assigned by simple calculations over the number of used models and their performance measures; machine learning weighting1 , where machine learning is exploited in order to rank the alternatives on the basis of a joint feature model. We refer to both types of approaches as ranking. The language of choice in our experiments is Bulgarian, but the techniques presented here are easily applicable to other languages, given the availability of training data. The interaction between the two lev"
W14-6102,R13-1098,1,0.683775,"Missing"
W14-6102,E12-1050,1,\N,Missing
W15-2135,P05-1034,0,0.0508508,"a sentence x = w1 , ..., wn . A tagged dependency tree is a directed tree T = (V, A, π, λ, ω, δ) where: iobj root cc bought and ate det an apple Figure 1: A complete dependency tree and some of its catenae. which overcomes the problems of long-distance paths and elliptical sentences. The employment of catenae in NLP applications is additional motivation for us to use it in the modelling of the interface between the treebank and the lexicon. Terminology note: an alternative term for catena is treelet. It has been used in the area of machine translation as a unit for translation transfer (see (Quirk et al., 2005)). Their definition is equivalent to the definition of catena. Also (Kuhlmann, 2010) uses treelet for a node and its children (if any). In the paper we resort to the term catena because it is closer to the spirit of the issues discussed here. 3 1. V = {0, 1, ..., n} is an ordered set of nodes that corresponds to an enumeration of the words in the sentence (the root of the tree has index 0); 2. A ⊆ V × V is a set of arcs. For each node i, 1 ≤ i ≤ n, there is exactly one arc in A: hi, ji ∈ A, 0 ≤ j ≤ n, i 6= j. There is exactly one arc hi, 0i ∈ A; Formal Definition of Catena 3. π : V − {0} → LA"
W15-2135,R11-1065,1,0.815152,"osition of word form in a catena or a sentence). Each of the information is depicted in the node representation on a different row. In order to model the behavior in a better way we need to add semantics to the dependency representation. We will not be able to do this in full in this paper. In order to represent the interaction between lexical items and their valency frames in the lexicon, we assume a semantic analysis based on Minimal Recursion Semantics (MRS) (see (Copestake et al., 2005)). For dependency analyses, the MRS structures are constructed in a way similar to the one presented in (Simov and Osenova, 2011). In this work, the root of a subtree of a given dependency tree is associated with the 7 Notice that there are alternative analyses in which the auxiliary verb is not a head of the sentence, but a dependent of the copula. 324 basis of syntactic analyses, includes information about the main form (lemma) of the word, the valency frame with all the elements, their forms, grammatical features and semantics (Osenova et al., 2012). The lexical entry for each lexical item also includes the semantics of the main form and information on how this semantics incorporates the semantics of each frame eleme"
W15-2135,Y10-1018,0,0.0962226,"a mechanism for representing the syntactic structure of idioms. He shows that for this task there is need for a definition of syntactic patterns that do not coincide with constituents. He defines the catena in the following way: The words A, B, and C (order irrelevant) form a chain if and only if A immediately dominates B and C, or if and only if A immediately dominates B and B immediately dominates C. In recent years the notion of catena revived again and was applied also to dependency representations. Catenae have been used successfully for the modelling of problematic language phenomena. (Gross 2010) presents the problems in syntax and morphology that have led to the introduction of the subconstituent catena level. Constituency-based analysis faces non-constituent structures in ellipsis, idioms, verb complexes. Apart from the linguistic modelling of language phenomena, catenae have been used in a number of NLP applications. (Maxwell et al., 2013), for example, presents an approach to Information Retrieval based on catenae. The authors consider the catena as a mechanism for semantic encoding Catenae were introduced initially to handle linguistic expressions with non-constituent structure a"
W15-2135,P13-1050,0,0.029705,"nly if A immediately dominates B and B immediately dominates C. In recent years the notion of catena revived again and was applied also to dependency representations. Catenae have been used successfully for the modelling of problematic language phenomena. (Gross 2010) presents the problems in syntax and morphology that have led to the introduction of the subconstituent catena level. Constituency-based analysis faces non-constituent structures in ellipsis, idioms, verb complexes. Apart from the linguistic modelling of language phenomena, catenae have been used in a number of NLP applications. (Maxwell et al., 2013), for example, presents an approach to Information Retrieval based on catenae. The authors consider the catena as a mechanism for semantic encoding Catenae were introduced initially to handle linguistic expressions with non-constituent structure and idiosyncratic semantics. It was shown in a number of publications that this unit is appropriate for both - the analysis of syntactic (for example, ellipsis, idioms) and morphological phenomena (for example, compounds). One of the important questions in NLP is how to establish a connection between the lexicon and the text dimension in an operable wa"
W15-2135,osenova-etal-2012-treebank,1,0.846433,"Minimal Recursion Semantics (MRS) (see (Copestake et al., 2005)). For dependency analyses, the MRS structures are constructed in a way similar to the one presented in (Simov and Osenova, 2011). In this work, the root of a subtree of a given dependency tree is associated with the 7 Notice that there are alternative analyses in which the auxiliary verb is not a head of the sentence, but a dependent of the copula. 324 basis of syntactic analyses, includes information about the main form (lemma) of the word, the valency frame with all the elements, their forms, grammatical features and semantics (Osenova et al., 2012). The lexical entry for each lexical item also includes the semantics of the main form and information on how this semantics incorporates the semantics of each frame element. Here we first present the structure of the lexical entry for the verb ‘бягам’ (’byagam’, run) in the sense &quot;run away from facts&quot;. The verb takes an indirect object in the form of a prepositional phrase starting with the preposition ‘от’ (’ot’, from). Here are the examples: MRS structure corresponding to the whole subtree. This means that for the semantic interpretation of MWEs we will use the root of the corresponding cat"
W15-5313,W08-1301,0,0.0926098,"Missing"
W15-5313,de-marneffe-etal-2014-universal,0,0.112451,"Missing"
W15-5313,W02-1502,0,0.0996954,"Missing"
W15-5313,berovic-etal-2012-croatian,0,0.130558,"Missing"
W15-5313,E12-1050,1,0.881241,"Missing"
W15-5313,ghayoomi-etal-2014-constituency,1,0.828285,"lausal ones – CLDA (subordinate clause introduced by the auxiliary particle да to), CLCHE (subordinate clause introduced by the subordinator че that), etc. Tracing back to the developments of BulTreeBank, its first ‘glocalization’ happened in 2006, when it was converted into the shared CoNLL dependency format – (Chanev et al., 2006), (Chanev et al., 2007). The rich structure was flattened to a set of 18 relations.3 This part consists of 196 000 tokens, because the sentences with ellipses were not considered. Alternative versions of BulTreeBank exist in two other popular formats: PennTreebank (Ghayoomi et al., 2014) and Stanford Dependencies (Rosa et al., 2014). The former was used for constituent parsing of Bulgarian, while the latter was part of a bigger endevour towards universalizing syntactic annotation schemes of many languages. Now, BulTreeBank is part of the common efforts that evolved from the previous initiatives towards the creation of comparable syntactically annotated multilingual datasets. For the Universal Dependencies initiative we used the original BulTreeBank constituent-based format, because in the previous conversions to dependency format some important information was either lost, or"
W15-5313,W06-2920,0,0.0725465,"from linguistic and technical point of view. The mapping from the original resource to the new one has been done on morphological and syntactic level. The first release of the treebank was issued in May 2015. It contains 125 000 tokens, which cover roughly half of the corpus data. 1 Introduction The efforts within the NLP community towards universalized language datasets for getting comparable, objective and scalable results in parsing and other tasks are not so recent. Concerning syntax, some shared representations have been proposed and used at CoNLL contests on dependency parsing in 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Another stream of sharing the same annotation framework was the adoption of the schemes of already existing treebanks. For example, a number of syntactic annotation works followed the style of Prague Dependency Treebank (Bejˇcek et al., 2013) (i.e., Slovene (Džeroski et al., 2006), Croatian (Berovic et al., 2012), Tamil (Ramasamy and Žabokrtsky, 2012) etc.); many other treebanks followed the Penn Treebank style (i.e., Arabic (Maamouri et al., 2008), Chinese (Xue et al., 2005), etc.). An alternative way of pursuing a common annotation architecture is the pre-shar"
W15-5313,E14-4028,0,0.0335619,"Missing"
W15-5313,W03-2403,1,0.644159,"ition of language specific features for the next stage. The only language specific feature considered in this version is the morphologically marked count form – remnant of the old Slavic dual form within the category of Number. The morphological mapping includes partsof-speech and their lexical as well as inflectional features. The syntactic mapping focuses on dependency relations. In this section we do not aim at exhaustive description of the mappings, but rather at illustrating the varieties between the models. BulTreeBank Resource in a Nutshell The original BulTreeBank (Simov et al., 2004; Simov and Osenova, 2003) that has been used in the conversion to the universal format comprises 214,000 tokens, which form a little more than 15,000 sentences. Each token has been annotated with elaborate morphosyntactic information. The original XML format of the BulTreeBank is based on HPSG. The syntactic structure is presented through a set of constituents with headdependant markings. The phrasal constituents contain two types of information: the domain of the constituent (NP, VP etc.) and the type of the phrase (head-complement (NPC, VPC etc.), head4.1 Morphology In morphology the following mapping cases occurred"
W15-5313,W14-6102,1,0.746613,"Missing"
W15-5313,petrov-etal-2012-universal,0,0.0668059,"endevour towards universalizing syntactic annotation schemes of many languages. Now, BulTreeBank is part of the common efforts that evolved from the previous initiatives towards the creation of comparable syntactically annotated multilingual datasets. For the Universal Dependencies initiative we used the original BulTreeBank constituent-based format, because in the previous conversions to dependency format some important information was either lost, or underspecified. Related Work The Universal Dependency initiative evolved mainly from the Stanford Type Dependency efforts and Google attempts (Petrov et al., 2012) in universalizing parts-of-speech. However, it is also ideologically related to CoNLL contests (2006 and 2007). The universalizing activities started with two main directions of research. The first can be illustrated by the work of Rosa et al. (2014) where 30 treebanks have been harmonized into a common Prague Dependency style, and then converted into Stanford Dependencies.2 It does not handle language specific features. BulTreeBank was also among the harmonized treebanks. The second can be exemplified by the work of Sanguinetti and Bosco (2014) and Bosco and Sanguinetti (2014). The authors d"
W15-5313,zeman-2008-reusable,0,0.175982,"een annotated with elaborate morphosyntactic information. The original XML format of the BulTreeBank is based on HPSG. The syntactic structure is presented through a set of constituents with headdependant markings. The phrasal constituents contain two types of information: the domain of the constituent (NP, VP etc.) and the type of the phrase (head-complement (NPC, VPC etc.), head4.1 Morphology In morphology the following mapping cases occurred from the direction of the original tagset to 2 This initiative as well as the Universal Dependencies stream build on the idea of interset, proposed by Zeman (2008). 3 82 http://www.bultreebank.org/dpbtb/ the UD tagset: identical parts-of-speech, division of one POS into more parts-of-speech and changing the POS. It should be noted however that all the processes are interrelated. in the original tagset: animacy, degree and passive forms. Concerning animacy, in Bulgarian the grammar-related dichotomy is more specific – Person vs. Non-Person. Thus, it is derivable from some explicit grammatical features, such as the case in some pronouns, the count form of the masculine nouns and the masculine form of the numerals. Concerning degree, the original tagset do"
W15-5313,W15-1821,0,0.104128,"not handle language specific features. BulTreeBank was also among the harmonized treebanks. The second can be exemplified by the work of Sanguinetti and Bosco (2014) and Bosco and Sanguinetti (2014). The authors describe the conversion of the parallel treebank ParTUT (Italian, English, French) into Stanford dependencies. In the same context is the work of Lipenkova and Souˇcek (2014) on Russian dependency treebank. Later on came also work on the conversion of the treebanks into Universal Dependencies. These include the conversion of the Swedish treebank (Nivre, 2014) and the Finnish treebank (Pyysalo et al., 2015). The experiments with the converted Finnish treebank showed that the parsing results are better with the Universal Dependencies (UD). 3 4 Universalizing Morphology and Syntax At this stage our conversion adheres fully to the universal annotation schemes. This means that we postponed the addition of language specific features for the next stage. The only language specific feature considered in this version is the morphologically marked count form – remnant of the old Slavic dual form within the category of Number. The morphological mapping includes partsof-speech and their lexical as well as i"
W15-5313,rosa-etal-2014-hamledt,0,0.324962,"Missing"
W15-5712,J99-2004,0,0.173896,"Missing"
W15-5712,W02-1502,0,0.0350745,"deep representations. One such setting was developed in the framework of the Head-driven Phrase Structure Grammar (HPSG) within the DELPH-IN community2 . The deep representation is delivered by the Minimal Recursion Semantics (MRS) analyses. They are usually delivered together with the syntactic analyses of the text. There already exist quite extensive implemented formal HPSG grammars for English (Copestake and Flickinger, 2000), Spanish (Marimon, 2010), German (M¨uller and Kasper, 2000), and Japanese (Siegel, 2000; Siegel and Bender, 2002). All grammars are harmonized with a Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a Bulgarian Resource Grammar – BURGER3 . The transfer in this setting is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON project (Oepen et al., 2004), the transfer rules were hand-written (Bond et al., 2005; Oepen et al., 2007), which involved a large amount of manual work. Graham and van Genabith (2008) and"
W15-5712,W07-0702,0,0.0740419,"Missing"
W15-5712,2005.mtsummit-osmtw.3,0,0.0112287,"apanese (Siegel, 2000; Siegel and Bender, 2002). All grammars are harmonized with a Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a Bulgarian Resource Grammar – BURGER3 . The transfer in this setting is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON project (Oepen et al., 2004), the transfer rules were hand-written (Bond et al., 2005; Oepen et al., 2007), which involved a large amount of manual work. Graham and van Genabith (2008) and Graham et al. (2009) explored the automatic rule induction approach in a transfer-based MT setting two Lexical Functional Grammars (LFGs), which was still restricted by the performance of both – the parser and the generator. Lack of robustness for target side generation is one of the main issues, when various ill-formed or fragmented structures come out after transfer. Oepen et al. (2007) use their generator to generate text fragments instead of full sentences, in order to increase the robus"
W15-5712,W11-2103,0,0.0261561,"y as factors in the MOSES system. The tests were performed on the QTLeap corpus data in IT domain for Pilot 1. The training was done on news parallel data as well as on IT domain data. The BLEU scores show that the addition of linguistic knowledge improves the Machine Translation. 1 Introduction In the recent years, machine translation (MT) has achieved significant improvement in terms of translation quality (Koehn, 2010). Both data-driven approaches (e.g., statistical MT (SMT)) and knowledgebased (e.g., rule-based MT (RBMT)) have achieved comparable results shown in the evaluation campaigns (Callison-Burch et al., 2011). However, according to the human evaluation, the final outputs of the MT systems are still far from satisfactory. For that reason, we explore an approach that incrementally incorporates linguistic knowledge into an SMT system. There has not been much study on the language pair Bulgarian – English, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the Bulgar"
W15-5712,copestake-flickinger-2000-open,0,0.307862,"Our work is closely connected to the transfer-based MT models. Ideally, given the availability of two deep grammars for some language pair, we would be able to translate through the transfer of the deep representations. One such setting was developed in the framework of the Head-driven Phrase Structure Grammar (HPSG) within the DELPH-IN community2 . The deep representation is delivered by the Minimal Recursion Semantics (MRS) analyses. They are usually delivered together with the syntactic analyses of the text. There already exist quite extensive implemented formal HPSG grammars for English (Copestake and Flickinger, 2000), Spanish (Marimon, 2010), German (M¨uller and Kasper, 2000), and Japanese (Siegel, 2000; Siegel and Bender, 2002). All grammars are harmonized with a Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a Bulgarian Resource Grammar – BURGER3 . The transfer in this setting is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON p"
W15-5712,E12-1050,1,0.894007,"Missing"
W15-5712,2008.eamt-1.10,0,0.067787,"Missing"
W15-5712,P07-1037,0,0.0682762,"Missing"
W15-5712,jakob-etal-2010-mapping,0,0.0387657,"Missing"
W15-5712,D07-1091,0,0.354095,"vent, a reference variable or their subtypes. Notice that we do not take all the information provided by the MRS, e.g., we throw away the scopal information and the other arguments of the relations. This kind of information is not straightforward to be represented in such ‘tagging’-style models, but it will be tackled in the future. All these factors encoded within the corpus provide us with a rich selection of factors for various experiments. 5 Experiments For our entry level deep machine translation system (Pilot 1) we make use of the Moses open source toolkit to build a factored SMT model (Koehn and Hoang, 2007b). As it was mentioned above in the analysis stage, we create a representation of the text which encodes various levels of linguistic information as factors. These include morphological, syntactic and semantic abstractions in the source and target language. We have experimented with several combinations of factors derived from the preprocessing with the Bulgarian and English analysis pipelines, together with semantic factors based on Minimal Recursion Semantics (see Table 1 for a subset of the results). The following are some examples of factors for this model: word form, lemma, and morphosyn"
W15-5712,2009.mtsummit-papers.7,0,0.0212654,"-driven approaches (e.g., statistical MT (SMT)) and knowledgebased (e.g., rule-based MT (RBMT)) have achieved comparable results shown in the evaluation campaigns (Callison-Burch et al., 2011). However, according to the human evaluation, the final outputs of the MT systems are still far from satisfactory. For that reason, we explore an approach that incrementally incorporates linguistic knowledge into an SMT system. There has not been much study on the language pair Bulgarian – English, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the Bulgarian – English translation direction (61.3). The direction from English to Bulgarian was even less explored. In the QTLeap project1 linguistic knowledge is gradually added to SMT systems with the aim to achieve better translation in both directions: EN-to-X language and X language-to-English. The incremental process is organized in several pilots. Pilot 0 sets the baseline, which means that no linguistic knowledge is added."
W15-5712,J10-4005,0,0.0265633,"ract In this paper, we present some preliminary results on Statistical Machine Translation from Bulgarian-to-English and English-to-Bulgarian. Linguistic knowledge has been added gradually as factors in the MOSES system. The tests were performed on the QTLeap corpus data in IT domain for Pilot 1. The training was done on news parallel data as well as on IT domain data. The BLEU scores show that the addition of linguistic knowledge improves the Machine Translation. 1 Introduction In the recent years, machine translation (MT) has achieved significant improvement in terms of translation quality (Koehn, 2010). Both data-driven approaches (e.g., statistical MT (SMT)) and knowledgebased (e.g., rule-based MT (RBMT)) have achieved comparable results shown in the evaluation campaigns (Callison-Burch et al., 2011). However, according to the human evaluation, the final outputs of the MT systems are still far from satisfactory. For that reason, we explore an approach that incrementally incorporates linguistic knowledge into an SMT system. There has not been much study on the language pair Bulgarian – English, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system pu"
W15-5712,marimon-2010-spanish,0,0.0174442,"sfer-based MT models. Ideally, given the availability of two deep grammars for some language pair, we would be able to translate through the transfer of the deep representations. One such setting was developed in the framework of the Head-driven Phrase Structure Grammar (HPSG) within the DELPH-IN community2 . The deep representation is delivered by the Minimal Recursion Semantics (MRS) analyses. They are usually delivered together with the syntactic analyses of the text. There already exist quite extensive implemented formal HPSG grammars for English (Copestake and Flickinger, 2000), Spanish (Marimon, 2010), German (M¨uller and Kasper, 2000), and Japanese (Siegel, 2000; Siegel and Bender, 2002). All grammars are harmonized with a Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a Bulgarian Resource Grammar – BURGER3 . The transfer in this setting is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON project (Oepen et al., 200"
W15-5712,2004.tmi-1.2,0,0.0647455,"Missing"
W15-5712,2007.tmi-papers.18,0,0.0789417,"Missing"
W15-5712,P02-1040,0,0.107618,"paigns (Callison-Burch et al., 2011). However, according to the human evaluation, the final outputs of the MT systems are still far from satisfactory. For that reason, we explore an approach that incrementally incorporates linguistic knowledge into an SMT system. There has not been much study on the language pair Bulgarian – English, mainly due to the lack of resources, including corpora, preprocessors, etc. There was a system published by Koehn et al. (2009), which was trained and tested on the European Union law data, but not on other domains like news. They reported a very high BLEU score (Papineni et al., 2002) on the Bulgarian – English translation direction (61.3). The direction from English to Bulgarian was even less explored. In the QTLeap project1 linguistic knowledge is gradually added to SMT systems with the aim to achieve better translation in both directions: EN-to-X language and X language-to-English. The incremental process is organized in several pilots. Pilot 0 sets the baseline, which means that no linguistic knowledge is added. Pilot 1 introduces some initial linguistic knowledge through the incorporation of some features such as part-of-speech, lemma, etc. In the setting that involve"
W15-5712,W02-1210,0,0.0465747,"some language pair, we would be able to translate through the transfer of the deep representations. One such setting was developed in the framework of the Head-driven Phrase Structure Grammar (HPSG) within the DELPH-IN community2 . The deep representation is delivered by the Minimal Recursion Semantics (MRS) analyses. They are usually delivered together with the syntactic analyses of the text. There already exist quite extensive implemented formal HPSG grammars for English (Copestake and Flickinger, 2000), Spanish (Marimon, 2010), German (M¨uller and Kasper, 2000), and Japanese (Siegel, 2000; Siegel and Bender, 2002). All grammars are harmonized with a Grammar Matrix (Bender et al., 2002). At the moment, precise and linguistically motivated grammars, customized on the base of the Grammar Matrix, have been or are being developed for Norwegian, French, Korean, Italian, Modern Greek, Spanish, Portuguese, Chinese, etc. There also exists a Bulgarian Resource Grammar – BURGER3 . The transfer in this setting is usually implemented in the form of rewriting rules. For instance, in the Norwegian LOGON project (Oepen et al., 2004), the transfer rules were hand-written (Bond et al., 2005; Oepen et al., 2007), which i"
W15-5712,W10-3811,0,0.0516731,"Missing"
W15-5712,W08-0325,0,0.148895,"re doing is still along the lines of previous work utilizing deep grammars, but we build a more ‘light-weighted’ transfer model over dependency parses. One of the MRS-related semantic formalisms is the Abstract Meaning Representation (AMR4 ), which also aims at achieving whole-sentence deep semantics instead of addressing various isolated holders of semantic information (such as NER, coreferences, temporal anchors, etc.). AMR also builds on the available syntactic trees, thus contributing to the efforts on sembanking. ˇ Another stream of research is related to the TectoMT approach (Zabokrtsk´ y et al., 2008). The Prague 5 Dependency Treebank (PDT) is a Czech treebank, annotated in accordance to the linguistic theory of Functional Generative Description (P. Sgall and Panevova, 1986). The tectogrammatical layer6 is the third layer of the PDT. It represents the syntactic-semantic interface, adding the functional dimension and collapsing the structural information, thus aiming at a more language-independent level of abstraction. The other two layers are the morphological and analytical ones. The morphological layer operates over tokens, assigning to them POS and lemma tags. The analytical layer refle"
W15-5712,W12-0116,1,0.372335,"abstract level of language representation is achieved, which then is used for the transfer step within the MT systems. The result on the tectogrammatical level heavily depends on the results from the processed analytical level. In the future, we plan to have transfer architectures for Bulgarian and English in both directions in both approaches – MRS and TectoMT. However, since these endeavors require more work, for the moment we test our ideas in the already built-in setting of the factored-based MOSES system. Thus, we build on the previous language model translation experience described in (Wang et al., 2012a) and (Wang et al., 2012b). However, while in the above-mentioned publications only Bulgarian-to-English translation was explored, in this paper also the English-to-Bulgarian direction is presented. 3 Data Preparation Two types of data are used in our experiments. The first type includes parallel news data. It is the training data. The second type includes parallel QTLeap data in the IT domain. It is the training and test data. The parallel news data comprises the following sources: 1. SETIMES parallel corpus, which is part of the OPUS parallel corpus7 . 2. EuroParl parallel corpus8 . 3. Libr"
W15-5712,W12-4202,1,0.677701,"abstract level of language representation is achieved, which then is used for the transfer step within the MT systems. The result on the tectogrammatical level heavily depends on the results from the processed analytical level. In the future, we plan to have transfer architectures for Bulgarian and English in both directions in both approaches – MRS and TectoMT. However, since these endeavors require more work, for the moment we test our ideas in the already built-in setting of the factored-based MOSES system. Thus, we build on the previous language model translation experience described in (Wang et al., 2012a) and (Wang et al., 2012b). However, while in the above-mentioned publications only Bulgarian-to-English translation was explored, in this paper also the English-to-Bulgarian direction is presented. 3 Data Preparation Two types of data are used in our experiments. The first type includes parallel news data. It is the training data. The second type includes parallel QTLeap data in the IT domain. It is the training and test data. The parallel news data comprises the following sources: 1. SETIMES parallel corpus, which is part of the OPUS parallel corpus7 . 2. EuroParl parallel corpus8 . 3. Libr"
W16-0604,E09-1005,0,0.0189412,"ted with “a” and “q” subscripts: Batch3a and Batch3q. As training data we used the following corpora: the Setimes parallel corpus, the Europarl parallel corpus and a corpus created on the basis of the documentation of LibreOffice. The corpora are linguistically processed with the IXA1 pipeline for the English part and the BTB pipeline for the Bulgarian. The analyses include POS tagging, lemmatization and WSD, using the UKB system2 , which provides graph-based methods for Word Sense Disambiguation and measuring lexical similarity. The tool uses the Personalized PageRank algorithm, described in Agirre and Soroa (2009). It has been used to perform Named Entity Disambiguation as well (Agirre et al., 2015). We have exploited the mapping between Bulgarian WordNet (BTB-WordNet) and Princeton English WordNet (PWN) in order to perform the Bulgarian WSD task — (Simov et al., 2015a). For the baseline MT system, the following factors have been constructed: WordForm|Lemma|POStag. We have trained the Moses system using these factors. The results are presented in Table 1. We also explored the impact of the bilingual morphological lexicons in the translation process, due to the occurrences of the so-called outof-trainin"
W16-0604,P05-1048,0,0.0500259,"and RBMT systems. Some widely explored ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses. In our case, after WSD, we use the alignment between Bulgarian and English WordNets for the generation of rules for word substitution. On the basis of these rules we generate a corpus with factors, on which the Moses system (Koehn et al., 2 Related Work Previous work on using WSD for SMT has yielded mixed results. (Carpuat and Wu, 2005) report a negative impact on BLEU scores. They used a supervised WSD system to select translation candidates for the SMT system, but, contrary to common sense expectations, this only made the translation model perform worse. Several reasons for this are suggested, chiefly that the SMT model works well enough on its own and state-of-the-art WSD systems cannot really boost it in a significant number of cases, and also that SMT architectures might not be welladapted to make use of the output of WSD systems. (Cabezas and Resnik, 2005) present an approach to using WSD for SMT, whereby target langua"
W16-0604,D07-1007,0,0.0372047,"kfilling, thus showing that WSD and SMT have a lot in common and improving one should be helpful for improving the other. (Chan et al., 2007) present another study in which WSD is beneficial to SMT. Disambiguation is performed between the possible translations of each source phrase. Translations are selected so as to maximize the length of the chunk proposed by the WSD model; the score provided by the WSD model is also taken into consideration. This approach yields a statistically significant improvement in terms of BLEU score. In a study that builds on their previously discouraging results, (Carpuat and Wu, 2007) show how a deeper integration of WSD into SMT systems can help systematically and significantly. Instead of performing disambiguation on single words, their system performs multiword phrasal disambiguation, thus achieving improvements over the baseline, as measured by eight different translation metrics. The rich context provided by the supervised WSD system helps rank correct translations higher than erroneous ones suggested by the baseline SMT system; also, it helps the decoder pick longer translation sequences, which often results in better translations. 3 Parallel Corpora Processing We ar"
W16-0604,P07-1005,0,0.0258709,"ation, pages 22–26, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics line that is, according to the authors, stronger than the one used in (Carpuat and Wu, 2005). (Vickrey et al., 2005) recasts WSD as a translation task, defining the different sense options for the separate words as the words or phrases aligned to them in a parallel corpus. The authors demonstrate that this approach is successful, as tested on word translation and blankfilling, thus showing that WSD and SMT have a lot in common and improving one should be helpful for improving the other. (Chan et al., 2007) present another study in which WSD is beneficial to SMT. Disambiguation is performed between the possible translations of each source phrase. Translations are selected so as to maximize the length of the chunk proposed by the WSD model; the score provided by the WSD model is also taken into consideration. This approach yields a statistically significant improvement in terms of BLEU score. In a study that builds on their previously discouraging results, (Carpuat and Wu, 2007) show how a deeper integration of WSD into SMT systems can help systematically and significantly. Instead of performing"
W16-0604,P07-2045,0,0.00411645,"Missing"
W16-0604,R15-1077,1,0.842146,"ocessed with the IXA1 pipeline for the English part and the BTB pipeline for the Bulgarian. The analyses include POS tagging, lemmatization and WSD, using the UKB system2 , which provides graph-based methods for Word Sense Disambiguation and measuring lexical similarity. The tool uses the Personalized PageRank algorithm, described in Agirre and Soroa (2009). It has been used to perform Named Entity Disambiguation as well (Agirre et al., 2015). We have exploited the mapping between Bulgarian WordNet (BTB-WordNet) and Princeton English WordNet (PWN) in order to perform the Bulgarian WSD task — (Simov et al., 2015a). For the baseline MT system, the following factors have been constructed: WordForm|Lemma|POStag. We have trained the Moses system using these factors. The results are presented in Table 1. We also explored the impact of the bilingual morphological lexicons in the translation process, due to the occurrences of the so-called outof-training word forms in the texts. See more in (Simov et al., 2015b). The bilingual lexicon was constructed by exploiting the following resources: BTB-Morphological lexicon containing all word-forms for more than 110 000 Bulgarian lemmas; BTB-bilingual Bulgarian-Engl"
W16-0604,W15-5712,1,0.837782,"Missing"
W16-0604,2009.mtsummit-posters.21,0,0.01959,"gy. The preprocessing strategies are presented. A method for the substitution of English word forms with the synsets or Bulgarian representative lemmas is discussed. Finally, the creation of a factored model in the Moses system is described. 1 Introduction In this paper we present first results from the implementation of a hybrid machine translation system between Bulgarian and English (en↔bg) using Word Sense Annotation (WSD) of the source language. There is an existing line of research that aims to combine the advantages of competing approaches to machine translation in a hybrid framework. (Thurmair, 2009) summarized several different architectures of hybrid systems using SMT and RBMT systems. Some widely explored ones are: 1) using an SMT to post-edit the outputs of an RBMT; 2) selecting the best translations from several hypotheses coming from different SMT/RBMT systems; and 3) selecting the best segments (phrases or words) from different hypotheses. In our case, after WSD, we use the alignment between Bulgarian and English WordNets for the generation of rules for word substitution. On the basis of these rules we generate a corpus with factors, on which the Moses system (Koehn et al., 2 Relat"
W16-0604,H05-1097,0,0.043303,"D systems. (Cabezas and Resnik, 2005) present an approach to using WSD for SMT, whereby target language lexical items are treated as ”sense tags”, given as soft translation alternatives to the translation model, which chooses the final version in accordance with its language model. The study reported a small gain against a base22 Proceedings of the 2nd Workshop on Semantics-Driven Machine Translation, pages 22–26, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics line that is, according to the authors, stronger than the one used in (Carpuat and Wu, 2005). (Vickrey et al., 2005) recasts WSD as a translation task, defining the different sense options for the separate words as the words or phrases aligned to them in a parallel corpus. The authors demonstrate that this approach is successful, as tested on word translation and blankfilling, thus showing that WSD and SMT have a lot in common and improving one should be helpful for improving the other. (Chan et al., 2007) present another study in which WSD is beneficial to SMT. Disambiguation is performed between the possible translations of each source phrase. Translations are selected so as to maximize the length of the"
W16-2332,N03-1017,0,0.00891535,"provide several translation options for each node along with their estimated probability. The best options are then selected using a Hidden Markov Tree Model (HMTM) ˇ with a target-language tree model (Zabokrtsk´ y and Popel, 2009). For this specific task, where we need to work on a specific domain, an extended version of TectoMT was used allowing interpolation of multiple TMs (Rosa et al., 2015). Moses All the systems submitted that were based on Moses have been trained on a phrase-based model by Giza++ or mGiza with “grow-diag-finaland” symmetrization and “msd-bidirectional-fe” reordering (Koehn et al., 2003). For the language pairs where big quantities of domain-specific monolingual data were available along with the generic domain data, separate language models (domain-specific and generic) were interpolated against our ICT domain-specific development set. For LM training and interpolation, the SRILM toolkit (Stolcke, 2002) was used. The method of truecasing has been adopted for several language pairs where it proved useful. 3 TectoMT The deep translation is based on the TectoMT system, an open-source MT system based on the Treex platform for general natural-language processing. TectoMT uses a c"
W16-2332,C10-3009,0,0.0136209,"obabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms e"
W16-2332,W16-2334,1,0.789389,"Missing"
W16-2332,2005.mtsummit-papers.11,0,0.0123228,"PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first"
W16-2332,P14-5010,0,0.00318097,"ed Moses with the following factors: ENWordForm-BGLemma|Lemma|BGPOStag, where ENWordForm-BGLemma is an English word form when there is no appropriate Bulgarian one, or the Bulgarian lemma; BGPOStag is the appropriate Bulgarian tag representing grammatical features like number, tense, etc. adaptation and MERT training. Batch2 domain corpus was used for testing during development. The Moses system, EU-Moses, uses factored models to allow lemma-based word-alignment. After word alignment, the rest of the training process is based on lowercased word-forms and standard parameters: Stanford CoreNLP (Manning et al., 2014) and Eustagger (Alegria et al., 2002) tools are used for tokenization and lemmatization, MGIZA for word alignment with the ”growdiag-final-and” symmetrization heuristic, a maximum length of 75 tokens per sentence and 5 tokens per phrase, translation probabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development se"
W16-2332,W15-4101,1,0.800257,"was performed by the Moses tokenizer. No lemmatization or compound splitting was used and the casing was obtained with the Moses truecaser. For the training, a phrase-based model was used with a language model order of 5, with Kneser-Ney smoothing, which was interpolated using the SRILM tool. The word alignment was done with Giza++ on full forms and the final tuning was done using MERT. The Europarl corpus was used for the training data, both as monolingual data for training language models and as parallel data for training the phrase-table. Regarding the English-to-Portuguese TectoMT system (Silva et al., 2015)(Rodrigues et al., 2016a), PT-Treex, in order to get the a-layer the Portuguese system resorted to LX-Suite (Branco and Silva, 2006), a set of pre-existing shallow processing tools for Portuguese that include a sentence segmenter, a tokenizer, a POS tagger, a morphological analyser and a dependency parser, all with state-of-the-art performance. Treex blocks were created to be called and interfaced with these tools. After running the shallow processing tools, the dependency output of the parser is converted into Universal Dependencies (UD) (de Marneffe et al., 2014). These dependencies are then"
W16-2332,W10-1730,1,0.894585,"Missing"
W16-2332,W15-5712,1,0.718195,"ansfer, and synthesis 4 Basque Both English-Basque submissions are trained on the same training corpora. That is, the PaCO2eneu corpus for translation and language modeling, and the in-domain Batch1 corpus for domain 436 tors retrieved from POS tagged, lemmatized parallel corpora; and BG-DeepMoses — a system that also is based on standard factored Moses but the translation is done in two steps: (1) semanticsbased translation of the source language text to a mixed source-target language text which is then (2) translated to the target language via Moses. The latter system builds on Simov et al. (2015). As training data for both systems the following corpora were used: the Setimes parallel corpus, the Europarl parallel corpus and a corpus created on the basis of the documentation of LibreOffice. The corpora are linguistically processed with the IXA2 pipeline for the English part and the BTB pipeline for the Bulgarian. The analyses include POS tagging, lemmatization and WSD, using the UKB system,3 which provides graph-based methods for Word Sense Disambiguation and lexical similarity measurements. For the BG-Moses system, the following factors have been constructed: WordForm|Lemma|POStag. Fo"
W16-2332,H05-1066,0,0.184414,"Missing"
W16-2332,P14-5003,0,0.0466518,"Missing"
W16-2332,2006.jeptalnrecital-invite.2,1,0.754357,"Missing"
W16-2332,tiedemann-2012-parallel,0,0.0377489,"extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn,"
W16-2332,L16-1094,1,0.833385,"ag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn, 2005) and the second model composed of the Batch1, the Microsoft Terminology Collection and ˇ the LibreOffice localization data (Stajner et al., 2016). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. The TectoMT synthesis (Rodrigues et al., 2016b) included other two lexical-semanticsrelated modules, the HideIT and gazetteers. The HideIT module handles entities that do not require translation such as URLs and shell commands. The gazetteers are specialized lexicons that handle the translation of named entities from the ITdomain such as menu items and button names. Finally, synset IDs were used as additional contextual feature"
W16-2332,P09-2037,1,0.925288,"uage-specific additions and distinguishes two levels of syntactic description: and Spanish, Charles University in Prague for Czech, by University of Groningen for Dutch, by University of Lisbon for Portuguese and by IICTBAS of the Bulgarian Academy of Sciences for Bulgarian. For each language two different systems were submitted, corresponding to different phases of the project, namely a phrase-based MT system built using Moses (Koehn et al., 2007), and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was imˇ plemented using TectoMT (Zabokrtsk´ y and Popel, 2009). For Bulgarian, its second MT system is not based on TectoMT, but on exploiting deep factors in Moses. All 12 systems are constrained, that is trained only on the data provided by the WMT16 IT-task organizers. We present briefly the Moses common setting and the TectoMT structure and then more detailed information for each language system are provided. In the last Section, results based on BLEU and TrueSkill are given and discussed. 2 • Surface dependency syntax (a-layer) – surface dependency trees containing all the tokens in the sentence. • Deep syntax (t-layer) – dependency trees that conta"
W16-2332,W08-0325,0,0.300026,"Missing"
W16-2332,L16-1438,1,0.826183,"Missing"
W16-2332,zeman-2008-reusable,0,0.0263149,"djusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactical analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used, to identify the elements that s"
W16-2332,W15-5711,1,0.91741,"Missing"
W16-2332,de-marneffe-etal-2014-universal,0,\N,Missing
W16-2332,E06-2024,1,\N,Missing
W16-2332,P07-2045,0,\N,Missing
W16-2332,W13-2208,0,\N,Missing
W16-2332,bojar-etal-2012-joy,1,\N,Missing
W16-2332,L16-1441,1,\N,Missing
W16-6403,2005.mtsummit-osmtw.3,0,0.0106193,"cursion Semantics (MRS) as an example of underspecified semantic formalisms. MRS underspecifies scope ambiguities for quantifiers and other scope-bearing elements. The selection of MRS is motivated by several facts: (1) it has already been implemented as part of HPSG grammars for several project languages: English (Copestake and Flickinger, 2000), German (Crysmann, 2007), Spanish (Marimon et al., 2007), Portuguese (Branco and Costa, 2008), (Costa and Branco, 2010) and Bulgarian (Osenova, 2010); (2) it is already used as a basis for semantic transfer in MT systems for several language pairs — (Bond et al., 2005) and (Oepen et al., 2004); (3) it allows the construction of semantic representation over shallow analyses or dependency syntactic structures — (Copestake, 20042006), (Copestake, 2007); (4) there exist corpora annotated with MRS structures, including some parallel ones — (Flickinger et al., 2012b) and (Flickinger et al., 2012a). At the same time, it should be noted that the existence of precise and robust linguistic grammars for various languages requires time-and labour-consuming work, in spite of the attempts of the community to provide various start-up kits for better enhancement of new lan"
W16-6403,W13-3802,0,0.0314791,"corporated are: WordNets for both languages; a valency lexicon for Bulgarian; aligned parallel corpora. The architecture comprises a predominantly statistical component (factor-based SMT in Moses) with some focused rule-based elements. The experiments show promising results and room for further improvements within the MT architecture. 1 Introduction The paper presents a hybrid approach for Deep Semantic Machine Translation. For that purpose, however, the linguistic phenomena that constitute deep semantics have to be defined. A list of such phenomena have been considered in (Hajiˇc, 2011) and (Bos, 2013), among others. They include but are not limited to the following ones: Semantic Roles (words vs. predicates, Lexical Semantics (Word Sense Disambiguation (WSD)), Multiword Expressions (MWE), Logical Form (LF), Metonymy, Named Entities (NE), Co-reference (pronominal, bridging anaphora), Verb Phrase Ellipsis, Collective/Distributive NPs, Scope (Negation, Quantifiers), Presuppositions, Tense and Aspect, Illocution Force, Textual Entailment, Discourse Structure/ Rhetorical Relations, neo-Davidsonian Events, Background Knowledge, Information Structure etc. All the mentioned phenomena represent var"
W16-6403,W08-2224,0,0.0276816,"dependent elements in the semantic content of the text and we hope that this ensures a better semantic transfer. In this section we present the main parameters behind the Minimal Recursion Semantics (MRS) as an example of underspecified semantic formalisms. MRS underspecifies scope ambiguities for quantifiers and other scope-bearing elements. The selection of MRS is motivated by several facts: (1) it has already been implemented as part of HPSG grammars for several project languages: English (Copestake and Flickinger, 2000), German (Crysmann, 2007), Spanish (Marimon et al., 2007), Portuguese (Branco and Costa, 2008), (Costa and Branco, 2010) and Bulgarian (Osenova, 2010); (2) it is already used as a basis for semantic transfer in MT systems for several language pairs — (Bond et al., 2005) and (Oepen et al., 2004); (3) it allows the construction of semantic representation over shallow analyses or dependency syntactic structures — (Copestake, 20042006), (Copestake, 2007); (4) there exist corpora annotated with MRS structures, including some parallel ones — (Flickinger et al., 2012b) and (Flickinger et al., 2012a). At the same time, it should be noted that the existence of precise and robust linguistic gram"
W16-6403,briscoe-carroll-2002-robust,0,0.0344266,"RMRS analyses generate over partial and shallow analyses. The idea is to extract as much as possible semantic information from a partial or shallow processed text. In the worst 5 The anchors determine the tokens which generate the corresponding elementary predicates and related arguments. This information facilitates the transfer of information from the source text to target one. 25 case — from POS tagged text. (Copestake, 2007) demonstrates how RMRS structures can be constructed over the output of a robust statistical parser RASP, which does not have access to subcategorisation information (Briscoe and Carroll, 2002). The input for the RMRS structures module is based on the following linguistic annotation — the lemma (Lemma) for the given wordform; the morphosyntactic tag (M ST ag) of the wordform, and the dependent relations (Rel) in the dependency tree. In cases of quantifiers we have access to the lexicon used in the Bulgarian HPSG grammar. The algorithm for producing of RMRS from a dependency parse is implemented via two types of rules: < Lemma, M ST ag &gt;→ EP − RM RS The rules of this type produce an RMRS structure representing an elementary predicate. < DRM RS, Rel, HRM RS &gt; HRM RS 0 The rules of thi"
W16-6403,copestake-flickinger-2000-open,0,0.36118,"ansliterated equivalents. 23 sentences. The addition of background knowledge provides some language independent elements in the semantic content of the text and we hope that this ensures a better semantic transfer. In this section we present the main parameters behind the Minimal Recursion Semantics (MRS) as an example of underspecified semantic formalisms. MRS underspecifies scope ambiguities for quantifiers and other scope-bearing elements. The selection of MRS is motivated by several facts: (1) it has already been implemented as part of HPSG grammars for several project languages: English (Copestake and Flickinger, 2000), German (Crysmann, 2007), Spanish (Marimon et al., 2007), Portuguese (Branco and Costa, 2008), (Costa and Branco, 2010) and Bulgarian (Osenova, 2010); (2) it is already used as a basis for semantic transfer in MT systems for several language pairs — (Bond et al., 2005) and (Oepen et al., 2004); (3) it allows the construction of semantic representation over shallow analyses or dependency syntactic structures — (Copestake, 20042006), (Copestake, 2007); (4) there exist corpora annotated with MRS structures, including some parallel ones — (Flickinger et al., 2012b) and (Flickinger et al., 2012a)."
W16-6403,W07-1219,0,0.0289884,"he addition of background knowledge provides some language independent elements in the semantic content of the text and we hope that this ensures a better semantic transfer. In this section we present the main parameters behind the Minimal Recursion Semantics (MRS) as an example of underspecified semantic formalisms. MRS underspecifies scope ambiguities for quantifiers and other scope-bearing elements. The selection of MRS is motivated by several facts: (1) it has already been implemented as part of HPSG grammars for several project languages: English (Copestake and Flickinger, 2000), German (Crysmann, 2007), Spanish (Marimon et al., 2007), Portuguese (Branco and Costa, 2008), (Costa and Branco, 2010) and Bulgarian (Osenova, 2010); (2) it is already used as a basis for semantic transfer in MT systems for several language pairs — (Bond et al., 2005) and (Oepen et al., 2004); (3) it allows the construction of semantic representation over shallow analyses or dependency syntactic structures — (Copestake, 20042006), (Copestake, 2007); (4) there exist corpora annotated with MRS structures, including some parallel ones — (Flickinger et al., 2012b) and (Flickinger et al., 2012a). At the same time, it sho"
W16-6403,jakob-etal-2010-mapping,0,0.0543336,"Missing"
W16-6403,kingsbury-palmer-2002-treebank,0,0.0216414,"RS (RMRS)). The MWE and NE are parts of the lexicons. We should note that there are also other appropriate LF frameworks that are briefly mentioned below. One of the MRS-related semantic formalisms is the Abstract Meaning Representation (AMR1 ), which aims at achieving whole-sentence deep semantics instead of addressing various isolated holders of semantic information (such as, NER, coreferences, temporal anchors, etc.). AMR also builds on the available syntactic trees, thus contributing to the efforts on sembanking. It is English-dependent and it makes an extensive use of PropBank framesets (Kingsbury and Palmer, 2002) and (Palmer et al., 2005). Its concepts are either English words or special keywords. AMR uses approximately 100 relations. They include: frame arguments, general semantic relations, relations for quantities and date-entities, etc. The Groningen Meaning Bank (GMB) integrates various phenomena in one formalism. It has a linguistically motivated, theoretically solid (CCG2 /DRT3 ) background. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by"
W16-6403,D07-1091,0,0.0588564,"nguage, but can also access the (partially) constructed target language MRS structure. Thus elements in each rule could include parts from both MRS structures. This approach requires a good deep grammar for the source language which produces complete MRS structures, then a complete set of rules for transferring of the source language MRSes to the target language MRSes and a generation grammar for the target language. As already discussed, there are no many languages equipped with such grammars and rules. The factor-based translation model is built on top of the factored SMT model proposed by (Koehn and Hoang, 2007), as an extension of the traditional phrase-based SMT framework. Instead of using only the word form of the text, it allows the system to take a vector of factors to represent each token, both for the source and target languages. The vector of factors can be used for different levels of linguistic annotations, like lemma, part-of-speech, or other linguistic features, if they can be (somehow) represented as annotations to each token. In our set-up we have used factored-based transfer of elementary predicates from MRSes in our earlier systems. The results were only slightly better or slightly wo"
W16-6403,W07-1214,0,0.0365757,"knowledge provides some language independent elements in the semantic content of the text and we hope that this ensures a better semantic transfer. In this section we present the main parameters behind the Minimal Recursion Semantics (MRS) as an example of underspecified semantic formalisms. MRS underspecifies scope ambiguities for quantifiers and other scope-bearing elements. The selection of MRS is motivated by several facts: (1) it has already been implemented as part of HPSG grammars for several project languages: English (Copestake and Flickinger, 2000), German (Crysmann, 2007), Spanish (Marimon et al., 2007), Portuguese (Branco and Costa, 2008), (Costa and Branco, 2010) and Bulgarian (Osenova, 2010); (2) it is already used as a basis for semantic transfer in MT systems for several language pairs — (Bond et al., 2005) and (Oepen et al., 2004); (3) it allows the construction of semantic representation over shallow analyses or dependency syntactic structures — (Copestake, 20042006), (Copestake, 2007); (4) there exist corpora annotated with MRS structures, including some parallel ones — (Flickinger et al., 2012b) and (Flickinger et al., 2012a). At the same time, it should be noted that the existence"
W16-6403,2004.tmi-1.2,0,0.0312858,"as an example of underspecified semantic formalisms. MRS underspecifies scope ambiguities for quantifiers and other scope-bearing elements. The selection of MRS is motivated by several facts: (1) it has already been implemented as part of HPSG grammars for several project languages: English (Copestake and Flickinger, 2000), German (Crysmann, 2007), Spanish (Marimon et al., 2007), Portuguese (Branco and Costa, 2008), (Costa and Branco, 2010) and Bulgarian (Osenova, 2010); (2) it is already used as a basis for semantic transfer in MT systems for several language pairs — (Bond et al., 2005) and (Oepen et al., 2004); (3) it allows the construction of semantic representation over shallow analyses or dependency syntactic structures — (Copestake, 20042006), (Copestake, 2007); (4) there exist corpora annotated with MRS structures, including some parallel ones — (Flickinger et al., 2012b) and (Flickinger et al., 2012a). At the same time, it should be noted that the existence of precise and robust linguistic grammars for various languages requires time-and labour-consuming work, in spite of the attempts of the community to provide various start-up kits for better enhancement of new languages. Thus, at the mome"
W16-6403,J05-1004,0,0.118898,"rts of the lexicons. We should note that there are also other appropriate LF frameworks that are briefly mentioned below. One of the MRS-related semantic formalisms is the Abstract Meaning Representation (AMR1 ), which aims at achieving whole-sentence deep semantics instead of addressing various isolated holders of semantic information (such as, NER, coreferences, temporal anchors, etc.). AMR also builds on the available syntactic trees, thus contributing to the efforts on sembanking. It is English-dependent and it makes an extensive use of PropBank framesets (Kingsbury and Palmer, 2002) and (Palmer et al., 2005). Its concepts are either English words or special keywords. AMR uses approximately 100 relations. They include: frame arguments, general semantic relations, relations for quantities and date-entities, etc. The Groningen Meaning Bank (GMB) integrates various phenomena in one formalism. It has a linguistically motivated, theoretically solid (CCG2 /DRT3 ) background. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.isi.edu"
W16-6403,W16-0604,1,0.802531,"kens in the target language and is used for post processing. The NLP preprocessing uses the Core NLP package tool for English. The result from the linguistic processing is stored on token level in the source text. The translation steps are performed by a pipeline of two Moses models (Moses1 and Moses2 in the Fig. 1). The first Moses model is phrase based trained on Europarl, Setimes, LibreOffice parallel corpora. The result is used to form intermediate text (Source/Target Language (S/T) text). The construction of S/L text is based on Word Sense Disambiguation similar to approach described in (Simov et al., 2016): it substitutes the words based on the inferred WordNet synsets (if available) and the substitution rules from English to Bulgarian. They take the most frequent lemma in the Bulgarian synset (calculated against a corpus of 70 million words) doctor#1, doc#1, physician#1, MD#2, Dr.#2, medico#2 = doktor#1, lekar#1, d-r#1. Additionally, some words in 22 the source language are transferred to the target language lemmas on the basis of the results from Moses model one (Moses1). The second-level transfer is achieved through training Moses (Moses2 in the figure) on the factored, partially translated"
W17-7617,N15-1165,0,0.024726,"e dependency relation ‘iobj’. In order to minimize some errors we enforced a condition that the dependency word should be a noun. Here is a real example from RTC that was processed: few high-quality SUBJ_address address long-term DOBJ_address In the example both subject and direct object are substituted with pseudo words. All of the word forms are substituted with lemmas because our goal is getting sense embeddings. The PCWN consists of pseudo texts that are the output from the Random Walk algorithm, when it is set to the mode of selecting sequences of nodes from a knowledge graph (KG) — see (Goikoetxea et al., 2015) for generation of pseudo corpora from a WordNet knowledge graph and (Ristoski and Paulheim, 2016) for generation of pseudo corpora from RDF knowledge graphs such as DBPedia, GeoNames, FreeBase. Here we report results only for knowledge graphs based on WordNet and its extensions. The pseudo corpus is generated using the UKB system5 for knowledge-based word sense disambiguation. Here is an example of a pseudo sentence from PCWN: unfit function use undertake disposal The pseudo sentences in PCWN represent sequences of related words on the basis of relations within WordNet. Such pseudo corpora pr"
W17-7617,P14-5010,0,0.00316854,"predicates. Subjects and obliques are not explicitly marked as passive/agent, but in some cases this information can be derived automatically on the base of the predicate form. Needless to say, not all mappings are straightforward and trivial. The assignment of sense embeddings to valency slots in the valency lexicon follows our previous work on grammatical role embeddings for English — (Simov et al., 2018). In this work we used two corpora: real text corpora (RTC) and pseudo corpus generated over WordNet (PCWN). The RTC was annotated with POS tags and parsed with Stanford CoreNLP pipeline — (Manning et al., 2014). Then on the basis of syntactic information we substituted the subject, direct object and indirect object lemmas with pseudo words representing the corresponding grammatical roles for the corresponding verb. Then we mixed the RTC with PCWN in order to train sense embeddings for the senses represented in the joint corpus in the same vector space. This allowed us to compare the embeddings for the grammatical roles with the embeddings for noun senses. This approach proved to be successful for English and we evaluated it via an extension of the Princeton WordNet with new syntagmatic relations bet"
W17-7617,W15-5313,1,0.609466,"Missing"
W17-7617,osenova-etal-2012-treebank,1,0.829691,"tences have been transferred into two different dependency formats: (1) CoNLL 2006 format where we used our own list of dependency relations and (2) Universal Dependency (UD) format where we focused rather on universal mappings of our data than on the language specific relations. As a follow-up, all newly annotated sentences adhere directly to the UD format. In addition to the mainly syntactic information, in the last few years we annotated the treebank with senses from the BulTreeBank Bulgarian WordNet (BTB-WN), aligned to Princeton WordNet (Osenova and Simov, 2017), and with valency frames (Osenova et al., 2012). On the basis of the available rich linguistic information within the original HPSG-based treebank as well as the semantic annotation and valency frames information, new extensions were performed in two directions: (1) transferring linguistic information from the HPSG-based annotation to the UD format with the goal to facilitate the addition of the so-called enhanced dependencies; and (2) assigning sense embeddings to valency slots in the valency lexicon for supporting better feature representations that are learned from huge corpora. In this paper we discuss these two developments as well as"
W17-7617,L16-1376,0,0.020403,"notated on discourse level — (Zikánová et al., 2015) and Italian Syntactic-Semantic Treebank (Montemagni et al., 2003), among others). However, they are not so many considering the multilinguality dimensions. At the same time, the NLP applications started to require the availability of richer cross-level linguistic knowledge. Hence, the idea of the enhanced dependencies reflects exactly the linguistic multilevel interfaces (syntax, semantics, discourse). More precisely, it aims “to make implicit relations between content words more explicit by adding relations and augmenting relation names.” (Schuster and Manning, 2016). They build on the basic dependencies and include the following phenomena:1 • Null nodes for elided predicates. This dependency involves the addition of special null nodes in clauses with an elided predicate. An example is: ‘I go to Varna, and you [NULL NODE] to Sofia’. With this ellipsis recovery the grammatical relations are maintained also in the clause without an explicit predicate. • Propagation of conjuncts. Apart from attaching the governor and dependents of a conjoined phrase to the first conjunct, dependencies are established between the other conjuncts and the governor, and dependen"
W17-7617,2018.gwc-1.33,1,0.88902,"relations have been annotated explicitly in the original treebank, while others stayed implicit, but they might be derived when necessary from the present annotations. Such a case are the arguments of passive predicates. Subjects and obliques are not explicitly marked as passive/agent, but in some cases this information can be derived automatically on the base of the predicate form. Needless to say, not all mappings are straightforward and trivial. The assignment of sense embeddings to valency slots in the valency lexicon follows our previous work on grammatical role embeddings for English — (Simov et al., 2018). In this work we used two corpora: real text corpora (RTC) and pseudo corpus generated over WordNet (PCWN). The RTC was annotated with POS tags and parsed with Stanford CoreNLP pipeline — (Manning et al., 2014). Then on the basis of syntactic information we substituted the subject, direct object and indirect object lemmas with pseudo words representing the corresponding grammatical roles for the corresponding verb. Then we mixed the RTC with PCWN in order to train sense embeddings for the senses represented in the joint corpus in the same vector space. This allowed us to compare the embedding"
W17-7617,D17-1270,0,0.101799,"task of transferring lexical semantic relations from the English WordNet to the Bulgarian one, but we will not report on this issue here. We performed the training of sense embeddings and grammatical role embedding in a similar way as for English, but first we extended the English WordNet with Bulgarian Synsets that lack the same meanings among the English Synsets. Then we generated a pseudo corpus using the UKB system2 for knowledge-based word sense disambiguation. The sense embeddings were trained again over a joint corpus real texts and pseudo corpus. Our work seems similar to the work of (Vulić et al., 2017). In their paper they consider three research questions: (Q1) Given their fundamental dependence on the distributional hypothesis, to what extent can unsupervised methods for inducing vector spaces facilitate the automatic induction of VerbNet-style verb classes across different languages? (Q2) Can one boost verb classification for lower-resource languages by exploiting general-purpose cross-lingual resources to construct better word vector spaces for these languages? (Q3) Based on the stipulated cross-linguistic validity of VerbNet-style classification, can one exploit rich sets of readily av"
W19-5109,J17-4005,0,0.0681699,"Missing"
W19-5109,kordoni-simova-2014-multiword,0,0.0650941,"Missing"
W19-5109,2018.gwc-1.8,0,0.0740614,"Missing"
W19-5109,W15-2135,1,0.930655,"al Linguistics Disambiguation, Machine Translation, etc. A similar approach to MWEs has been undertaken also in the creation of the Basque WordNet — (Agirre et al., 2006). As already mentioned above, we do not introduce a new relation, but directly annotate the words in MWEs with the appropriate literal meanings. Furthermore, we do not restrict the annotation only to the compositional parts of MWEs. Whole MWEs are annotated as well. For the moment no phrasets have been added to BTB-WN, but we consider such a step as a good development in future. In series of papers (Simov and Osenova, 2014), (Simov and Osenova, 2015), and (Osenova and Simov, 2018b) we presented the modeling of MWEs in terms of catena. These papers demonstrate how the (partial) variability and compositionality can be represented. The last paper reflects the multilingual application of the model. In our work here we extend this model to represent also the literal meanings of the distinct components in MWEs. coming from the WordNet into the catena model. The structure of the paper is as follows: the next section outlines the related work; Section 3 presents a classification of the MWEs in BTBWN. Section 4 proposes an extension of the catena"
W19-5109,2019.gwc-1.37,1,0.74856,"roduction In this paper we present the distribution and treatment of MultiWord Expressions (MWEs) within BTB-WN — a data-driven Bulgarian WordNet.1 Currently BTB-WN contains about 22 000 synsets covering CoreWordNet synsets, all the content words within BulTreeBank (about 8 000 lemmas) and the top part of a frequency list over 70 million running words. For the purpose of this work we use two subsets: (1) the current version of BTBWN; and (2) a subset mapped to the Bulgarian Wikipedia in order to establish a connection between the lexical information in BTB-WN and the encyclopedic knowledge — (Simov et al., 2019). The second set is used to evaluate the impact of the encyclopedic domain on the distribution of the MWEs. From the first subset 981 examples of MWEs have been extracted, while from the second one – 506 examples. In the past few years extensive literature has been dedicated to MWEs. In spite of that there is no single guiding principle or widely accepted classification, since MWEs are not homogeneous and can be classified at different levels that interact in various ways: morphology, lexicology, syntax, and semantics. Also, the typology becomes more 2 http://clarino.uib.no/iness/page? page-id"
W19-5109,R15-1077,1,0.835065,"if they are as71 елен лопатар, “fallow deer”, or is foreign уеб страница, “web page”. sociated with a particular type of genre, speech act or otherwise conventionalized (Calzolari et al., 2002). One such example is the terminological unit промяна на климата (change of climate, ‘climate change’), which corresponds to the two MWE forms in the PWN synset {climate change, global climate change}. A small number of the two or three word sequences extracted from BTB-WN appeared to be marginal for the MWE spectrum. They were born in the process of the bidirectional mapping of BTB-WN and PWN synsets (Simov et al., 2015) as instances of periphrastic translation; whenever there is no word or MWE in the target language to express the concept, dictionaries offer descriptive phrases whose length and syntactic level of complexity may vary. Consider these two examples: the VP гледам гневно of the structural type V + Adv, ‘look disapprovingly’ which is mapped to the PWN synset {glower, glare} or the four-word sequence казвам|изговарям буква по буква, lit. ‘say|pronounce letter per letter’ which is used to translate the English verb ""spell"". While the meaning of both Bulgarian expressions can be derived from the mean"
W19-8014,O14-1013,0,0.0178106,"next section related work is outlined briefly. Section 3 focuses on modeling ellipsis in the original BulTreeBank. Section 4 compares the annotation of elliptical phenomena in the original treebank with the strategies within UD. Section 5 concludes the paper. 1 Coordination is an exception. It is considered as non-headed phrase. 2 Related Work There is extensive literature on the ellipsis and its treatment in one or more languages, on grammatical and cognitive levels, etc. Here, however, only few findings will be mentioned that mainly focus on annotations within dependency-based frameworks. (Mikulova, 2014) presents the typology of ellipsis in Czech in the dependency theory of Functional Generative Description. Since this is a multistratal theory and thus distinguishes between surface and deep levels, ellipsis is mainly modeled on deep (tectogrammatical) level. Within the scope of the surface ellipses the author includes the so-called‘structural ellipses’ (type 1). Here belong the following subtypes: a) ellipsis of the governing verb (I like coffee, but you [like] tea) and b) ellipsis of governing noun (Central [Europe] and Eastern Europe). Within the scope of the deep syntactic ellipses the aut"
W19-8014,W17-7617,1,0.841347,"Missing"
