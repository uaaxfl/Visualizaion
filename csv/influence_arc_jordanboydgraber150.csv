2020.acl-main.201,N19-1253,0,0.0358768,"to the final layer, run Adam (Kingma and Ba, 2015) with default parameters for ten epochs, and report the average accuracy of ten runs. 2 https://github.com/facebookresearch/ MUSE/issues/24 3 A pilot study confirms that retrofitting to infrequent word pairs is less effective. Dependency Parsing We also test on dependency parsing, a structured prediction task. We use Universal Dependencies (Nivre et al., 2019, 4.2 Intrinsic Evaluation: BLI 2217 v2.4) with the standard split. We use the biaffine parser (Dozat and Manning, 2017) in AllenNLP (Gardner et al., 2017) with the same hyperparameters as Ahmad et al. (2019). To focus on the influence of CLWE, we remove part-of-speech features (Ammar et al., 2016). We report the average unlabeled attachment score (UAS) of five runs. Results Although training dictionary retrofitting lowers BLI test accuracy, it improves both downstream tasks’ test accuracy (Figure 3). This confirms that over-optimizing the test BLI accuracy can hurt downstream tasks because training dictionary words are also important. The synthetic dictionary further improves downstream models, showing that generalization to downstream tasks must balance between BLI training and test accuracy. Qu"
2020.acl-main.201,N19-1391,0,0.0225807,"ely refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accuracy, which further helps downstream models on average. BLI test accuracy does not always correlate with downstream task accuracy because words from the"
2020.acl-main.201,D16-1250,0,0.0392399,"ained by a projection-based method, where X′ = WX are the projected source embeddings and Z′ = Z are ˆ and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly aligned. In practice, we use a non-zero α for regularization, but the upd"
2020.acl-main.201,P17-1042,0,0.0458555,"s. Our pilot experiments found similar trends when replacing retrofitting with Counter-fitting (Mrkši´c et al., 2016) and Attract-Repel (Mrkši´c et al., 2017), so we focus on retrofitting. Recent work applies semantic specialization to CLWE by using multilingual ontologies (Mrkši´c et al., 2017), transferring a monolingual ontology across languages (Ponti et al., 2019), and asking bilingual speakers to annotate task-specific keywords (Yuan et al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wan"
2020.acl-main.201,D18-1366,0,0.0250512,"and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly aligned. In practice, we use a non-zero α for regularization, but the updated CLWE still have perfect training BLI accuracy (Figure 2). If the training dictionary covers predict"
2020.acl-main.201,2020.acl-main.747,0,0.0490558,"t al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accuracy, which further helps"
2020.acl-main.201,D19-1572,0,0.0142572,"Ponti et al., 2019), and asking bilingual speakers to annotate task-specific keywords (Yuan et al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then"
2020.acl-main.201,E14-1049,0,0.0627564,"target languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We use 300-dimensional fastText vectors trained on Wikipedia and Common Crawl (Grave et al., 2018). We lowercase all words, only keep the 200K most frequent words, and apply five rounds of Iterative Normalization (Zhang et al., 2019). We use dictionaries from MUSE (Conneau et al., 2018), a popular BLI benchmark, with standard splits: train on 5K source word translations and test on 1.5K words for BLI. For each language, we train three projection-based CLWE: canonical correlation analysis (Faruqui and Dyer, 2014, CCA), 2216 1 Code at https://go.umd.edu/retro_clwe. Original +retrofit +synthetic 100 80 60 60 40 40 DE ES FR IT JA RU ZH 20 AVG Original +retrofit +synthetic 80 DE Document classification with PROC 60 60 40 DE ES FR IT JA RU ZH 20 AVG Original +retrofit +synthetic DE ES 60 40 ES FR IT JA RU FR IT ZH AVG JA RU ZH AVG ZH 20 AVG Original +retrofit +synthetic 80 60 DE RU Dependency parsing with CCA 80 40 JA Original +retrofit +synthetic Document classification with CCA 100 IT 80 80 40 FR Dependency parsing with PROC Original +retrofit +synthetic 100 ES DE Document classification with RCSLS ES F"
2020.acl-main.201,P19-1489,1,0.850029,"dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation. 1 Target Embedding Original CLWE Retrofit Updated CLWE Figure 1: To fully exploit the training dictionary, we retrofit projection-based CLWE to the training dictionary as a post-processing step (pink parts). To preserve correctly aligned translations in the original CLWE, we optionally retrofit to a synthetic dictionary induced from the original CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g.,"
2020.acl-main.201,P19-1070,0,0.289461,"m tasks and explains why BLI is a flawed CLWE evaluation. 1 Target Embedding Original CLWE Retrofit Updated CLWE Figure 1: To fully exploit the training dictionary, we retrofit projection-based CLWE to the training dictionary as a post-processing step (pink parts). To preserve correctly aligned translations in the original CLWE, we optionally retrofit to a synthetic dictionary induced from the original CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor searc"
2020.acl-main.201,L18-1550,0,0.0372087,"onary keeps closely aligned word pairs in the original CLWE, which sometimes improves downstream models. Experiments We retrofit three projection-based CLWE to their training dictionaries and synthetic dictionaries.1 We evaluate on BLI and two downstream tasks. While retrofitting decreases test BLI accuracy, it often improves downstream models. 4.1 Embeddings and Dictionaries We align English embeddings with six target languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We use 300-dimensional fastText vectors trained on Wikipedia and Common Crawl (Grave et al., 2018). We lowercase all words, only keep the 200K most frequent words, and apply five rounds of Iterative Normalization (Zhang et al., 2019). We use dictionaries from MUSE (Conneau et al., 2018), a popular BLI benchmark, with standard splits: train on 5K source word translations and test on 1.5K words for BLI. For each language, we train three projection-based CLWE: canonical correlation analysis (Faruqui and Dyer, 2014, CCA), 2216 1 Code at https://go.umd.edu/retro_clwe. Original +retrofit +synthetic 100 80 60 60 40 40 DE ES FR IT JA RU ZH 20 AVG Original +retrofit +synthetic 80 DE Document classi"
2020.acl-main.201,P15-1119,0,0.0414222,"ced from the original CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor search) and compared against gold translations. BLI accuracy is easy to compute and captures the desired property of CLWE that translation pairs should be close. However, BLI accuracy Equal contribution Project Training Dictionary Introduction ⋆ ∗ Synthetic Dictionary Let’s think about why that might be. BLI accuracy is only computed on test words. Consequently, BLI hides linear pro"
2020.acl-main.201,D19-1090,0,0.111892,"istances between translation pairs in a training dictionary: ∑ min ∥Wxi − zj ∥22 . (1) W weakness is not transparent when using BLI as the standard evaluation for CLWE, because BLI test sets omit training dictionary words. However, when the training dictionary covers words that help downstream tasks, underfitting limits generalization to other tasks. Some BLI benchmarks use frequent words for training and infrequent words for testing (Mikolov et al., 2013; Conneau et al., 2018). This mismatch often appears in real-world data, because frequent words are easier to find in digital dicitonaries (Czarnowska et al., 2019). Therefore, training dictionary words are often more important in downstream tasks than test words. 3 Retrofitting to Dictionaries To fully exploit the training dictionary, we explore a simple post-processing step that overfits the dictionary: we first train projection-based CLWE and then retrofit to the training dictionary (pink parts in Figure 1). Retrofitting was originally introduced for refining monolingual word embeddings with synonym constraints from a lexical ontology (Faruqui et al., 2015). For CLWE, we retrofit using the training dictionary D as the ontology. Intuitively, retrofitti"
2020.acl-main.201,D19-1252,0,0.0167114,"o annotate task-specific keywords (Yuan et al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test"
2020.acl-main.201,D18-1330,0,0.326515,"minimizing deviation from the original CLWE. Let X′ and Z′ be CLWE trained by a projection-based method, where X′ = WX are the projected source embeddings and Z′ = Z are ˆ and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly ali"
2020.acl-main.201,D14-1181,0,0.00501098,"t language. We first compare BLI accuracy on both training and test dictionaries (Figure 2). We use CSLS to translate words with default parameters. The original projection-based CLWE have the highest test accuracy but underfit the training dictionary. Retrofitting to the training dictionary perfectly Document Classification Our first downstream task is document-level classification. We use MLDoc, a multilingual classification benchmark (Schwenk and Li, 2018) using the standard split with 1K training and 4K test documents. Following Glavas et al. (2019), we use a convolutional neural network (Kim, 2014). We apply 0.5 dropout to the final layer, run Adam (Kingma and Ba, 2015) with default parameters for ten epochs, and report the average accuracy of ten runs. 2 https://github.com/facebookresearch/ MUSE/issues/24 3 A pilot study confirms that retrofitting to infrequent word pairs is less effective. Dependency Parsing We also test on dependency parsing, a structured prediction task. We use Universal Dependencies (Nivre et al., 2019, 4.2 Intrinsic Evaluation: BLI 2217 v2.4) with the standard split. We use the biaffine parser (Dozat and Manning, 2017) in AllenNLP (Gardner et al., 2017) with the s"
2020.acl-main.201,C12-1089,0,0.0937553,"synthetic dictionary induced from the original CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor search) and compared against gold translations. BLI accuracy is easy to compute and captures the desired property of CLWE that translation pairs should be close. However, BLI accuracy Equal contribution Project Training Dictionary Introduction ⋆ ∗ Synthetic Dictionary Let’s think about why that might be. BLI accuracy is only computed on test words. Consequently, BL"
2020.acl-main.201,N16-1018,0,0.124468,"Missing"
2020.acl-main.201,Q17-1022,0,0.121936,"Missing"
2020.acl-main.201,P17-1135,0,0.0140593,"ts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor search) and compared against gold translations. BLI accuracy is easy to compute and captures the desired property of CLWE that translation pairs should be close. However, BLI accuracy Equal contribution Project Training Dictionary Introduction ⋆ ∗ Synthetic Dictionary Let’s think about why that might be. BLI accuracy is only computed on test words. Consequently, BLI hides linear projection’s inability to align all train"
2020.acl-main.201,D19-1226,0,0.167317,"Missing"
2020.acl-main.201,L18-1560,0,0.133547,"pendency parsing. We fix the embeddng layer of the model to CLWE and use the zero-shot setting, where a model is trained in English and evaluated in the target language. We first compare BLI accuracy on both training and test dictionaries (Figure 2). We use CSLS to translate words with default parameters. The original projection-based CLWE have the highest test accuracy but underfit the training dictionary. Retrofitting to the training dictionary perfectly Document Classification Our first downstream task is document-level classification. We use MLDoc, a multilingual classification benchmark (Schwenk and Li, 2018) using the standard split with 1K training and 4K test documents. Following Glavas et al. (2019), we use a convolutional neural network (Kim, 2014). We apply 0.5 dropout to the final layer, run Adam (Kingma and Ba, 2015) with default parameters for ten epochs, and report the average accuracy of ten runs. 2 https://github.com/facebookresearch/ MUSE/issues/24 3 A pilot study confirms that retrofitting to infrequent word pairs is less effective. Dependency Parsing We also test on dependency parsing, a structured prediction task. We use Universal Dependencies (Nivre et al., 2019, 4.2 Intrinsic Eva"
2020.acl-main.201,2020.acl-main.536,0,0.0150763,", 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accuracy, which further helps downstream models on average. BLI test accuracy does not always correlate with downstream task accuracy because words from the training dictionary are ignored. An obvious fix is adding t"
2020.acl-main.201,D19-1077,0,0.0295053,"ific keywords (Yuan et al., 2019). We instead re-use the training dictionary of the CLWE. Synthetic dictionaries are previously used to iteratively refine a linear projection (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accurac"
2020.acl-main.201,N15-1104,0,0.0985627,"′ and Z′ be CLWE trained by a projection-based method, where X′ = WX are the projected source embeddings and Z′ = Z are ˆ and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly aligned. In practice, we use a non-zero α for regul"
2020.acl-main.201,P19-1307,1,0.909316,"rojected source embeddings and Z′ = Z are ˆ and the target embeddings. We learn new CLWE X ˆ Z by minimizing L = La + Lb , (2) where La is the squared distance between the updated CLWE from the original CLWE: ˆ − X′ ∥2 + α∥Z ˆ − Z′ ∥ 2 , La = α∥X (3) and Lb is the total squared distance between translations in the dictionary: ∑ ˆi − z ˆ j ∥2 . Lb = βij ∥x (4) (i,j)∈D (i,j)∈D Recent work improves this method with different optimization objectives (Dinu et al., 2015; Joulin et al., 2018), orthogonal constraints on W (Xing et al., 2015; Artetxe et al., 2016; Smith et al., 2017), pre-processing (Zhang et al., 2019), and subword features (Chaudhary et al., 2018; Czarnowska et al., 2019; Zhang et al., 2020). Projection-based methods underfit—a linear projection has limited expressiveness and cannot perfectly align all training pairs. Unfortunately, this We use the same α and β as Faruqui et al. (2015) to balance the two objectives. Retrofitting tends to overfit. If α is zero, minimizing Lb collapses each training pair to the same vector. Thus, all training pairs are perfectly aligned. In practice, we use a non-zero α for regularization, but the updated CLWE still have perfect training BLI accuracy (Figure"
2020.acl-main.201,N16-1156,0,0.0499522,"nal CLWE (orange parts). does not always correlate with accuracy on downstream tasks such as cross-lingual document classification and dependency parsing (Ammar et al., 2016; Fujinuma et al., 2019; Glavas et al., 2019). Cross-lingual word embeddings (CLWE) map words across languages to a shared vector space. Recent supervised CLWE methods follow a projection-based pipeline (Mikolov et al., 2013). Using a training dictionary, a linear projection maps pre-trained monolingual embeddings to a multilingual space. While CLWE enable many multilingual tasks (Klementiev et al., 2012; Guo et al., 2015; Zhang et al., 2016; Ni et al., 2017), most recent work only evaluates CLWE on bilingual lexicon induction (BLI). Specifically, a set of test words are translated with a retrieval heuristic (e.g., nearest neighbor search) and compared against gold translations. BLI accuracy is easy to compute and captures the desired property of CLWE that translation pairs should be close. However, BLI accuracy Equal contribution Project Training Dictionary Introduction ⋆ ∗ Synthetic Dictionary Let’s think about why that might be. BLI accuracy is only computed on test words. Consequently, BLI hides linear projection’s inability"
2020.acl-main.201,N19-1162,0,0.021627,"tion (Artetxe et al., 2017; Conneau et al., 2018). These methods still underfit because of the linear constraint. We instead retrofit to the synthetic dictionary to fit the training dictionary better while keeping some generalization power of projection-based CLWE. Recent work investigates cross-lingual contextualized embeddings as an alternative to CLWE (Eisenschlos et al., 2019; Lample and Conneau, 2019; Huang et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Our method may be applicable, as recent work also applies projections to contextualized embeddings (Aldarmaki and Diab, 2019; Schuster et al., 2019; Wang et al., 2020; Wu et al., 2020). 6 Conclusion and Discussion Popular CLWE methods are optimized for BLI test accuracy. They underfit the training dictionary, which hurts downstream models. We use retrofitting to fully exploit the training dictionary. This post-processing step improves downstream task accuracy despite lowering BLI test accuracy. We then add a synthetic dictionary to balance BLI test and training accuracy, which further helps downstream models on average. BLI test accuracy does not always correlate with downstream task accuracy because words from the training dictionary ar"
2020.acl-main.201,N15-1184,0,\N,Missing
2020.acl-main.201,W18-2501,0,\N,Missing
2020.acl-main.353,P13-1025,1,0.86212,"Missing"
2020.acl-main.353,N09-1057,0,0.0262521,"lie looks like, they mention evasiveness, shorter messages, over-qualification, and creating false hypothetical scenarios (DePaulo et al., 2003). 2.3 Annotating truthfulness Previous work on the language of Diplomacy (Niculae et al., 2015) lacked access to players’ internal state and was limited to post-hoc analysis. We improve on this by designing our own interface that gathers players’ intentions and perceptions in real-time (Section 3.1). As with other highly subjective phenomena like sarcasm (González-Ibáñez et al., 2011; Bamman and Smith, 2015), sentiment (Pang et al., 2008) and framing (Greene and Resnik, 2009), the intention to deceive is reflective on someone’s internal state. Having individuals provide their own labels for their internal state is essential as third party annotators could not accurately access it (Chang et al., 2020). Most importantly, our gracious players have allowed this language data to be released in accordance with IRB authorized anonymization, encouraging further work on the strategic use of deception in long-lasting relations.2 2 Data available at http://go.umd.edu/diplomacy_data and as part of ConvoKit http://convokit.cornell.edu. 3813 3 Engaging a Community of Liars This"
2020.acl-main.353,P11-1032,0,0.520933,"Real-world examples of lying include perjury (Louwerse et al., 2010), calumny (Fornaciari and Poesio, 2013), emails from malicious hackers (Dhamija et al., 2006), and surreptitious user recordings. But real-world data comes with realworld complications and privacy concerns. The artifice of Diplomacy allows us to gather pertinent language data with minimal risk and to access both sides of deception: intention and perception. Other avenues for less secure research include analyzing dating profiles for accuracy in self-presentation (Toma and Hancock, 2012) and classifying deceptive online spam (Ott et al., 2011). 7 Conclusion In Dante’s Inferno, the ninth circle of Hell—a fate worse even than that reserved for murderers—is for betrayers. Dante asks Count Ugolino to name his betrayer, which leads him to say: but if my words can be the seed to bear the fruit of infamy for this betrayer who feeds my hunger, then I shall speak—in tears (Alighieri and Musa, 1995, Canto XXXIII) Similarly, we ask victims to expose their betrayers in the game of Diplomacy. The seeds of players’ negotiations and deceit could, we hope, yield fruit to help others: understanding multi-party negotiation and protecting Internet us"
2020.acl-main.662,D19-1605,1,0.844604,"ated [the dataset] in the first place” (Zaenen, 2006), we need to explicitly appreciate the unavoidable ambiguity instead of silently glossing over it.14 This is already an active area of research, with conversational QA being a new setting actively explored by several datasets (Reddy et al., 2018; Choi et al., 2018); and other work explicitly focusing on identifying useful clarification questions (Rao and Daumé III), thematically linked questions (Elgohary et al., 2018) or resolving ambiguities that arise from coreference or pragmatic constraints by rewriting underspecified question strings (Elgohary et al., 2019; Min et al., 2020). a ranking over numbers. This can be problematic for several reasons. The first is that single numbers have some variance; it’s better to communicate estimates with error bars. Revel in Spectacle However, with more complicated systems and evaluations, a return to the yearly evaluations of TRECQA may be the best option. This improves not only the quality of evaluation (we can have real-time human judging) but also lets the test set reflect the build it/break it cycle (Ruef et al., 2016), as attempted by the 2019 iteration of FEVER (Thorne et al., 2019). Moreover, another les"
2020.acl-main.662,D18-1134,1,0.843025,"id you mean. . . ”) instead of giving credit for happening to ‘fit the data’. To ensure that our datasets properly “isolate the property that motivated [the dataset] in the first place” (Zaenen, 2006), we need to explicitly appreciate the unavoidable ambiguity instead of silently glossing over it.14 This is already an active area of research, with conversational QA being a new setting actively explored by several datasets (Reddy et al., 2018; Choi et al., 2018); and other work explicitly focusing on identifying useful clarification questions (Rao and Daumé III), thematically linked questions (Elgohary et al., 2018) or resolving ambiguities that arise from coreference or pragmatic constraints by rewriting underspecified question strings (Elgohary et al., 2019; Min et al., 2020). a ranking over numbers. This can be problematic for several reasons. The first is that single numbers have some variance; it’s better to communicate estimates with error bars. Revel in Spectacle However, with more complicated systems and evaluations, a return to the yearly evaluations of TRECQA may be the best option. This improves not only the quality of evaluation (we can have real-time human judging) but also lets the test set"
2020.acl-main.662,D18-1091,0,0.0202883,"dge-based responses; for consistency, we still call them questions. character while warning against possible confusion [emphasis added]: He’s not Sherlock Holmes, but his address is 221B. He’s not the Janitor on Scrubs, but his father is played by R. Lee Ermy. [. . . ] For ten points, name this misanthropic, crippled, Vicodindependent central character of a FOX medical drama. ANSWER: Gregory House, MD In contrast, QA datasets often contain ambiguous and under-specified questions. While this sometimes reflects real world complexities such as actual under-specified or ill-formed search queries (Faruqui and Das, 2018; Kwiatkowski et al., 2019), ignoring this ambiguity is problematic. As a concrete example, Natural Questions (Kwiatkowski et al., 2019) answers “what year did the us hockey team win the Olympics” with 1960 and 1980, ignoring the US women’s team, which won in 1998 and 2018, and further assuming the query is about ice rather than field hockey (also an Olympic event). Natural Questions associates a page about the United States men’s national ice hockey team, arbitrarily removing the ambiguity post hoc. However, this does not resolve the ambiguity, which persists in the original question: informa"
2020.acl-main.662,D12-1118,1,0.799982,"questions should be structured to make the dataset as valuable as possible. 3.1 Avoiding ambiguity and assumptions Ambiguity in questions not only frustrates answerers who resolve the ambiguity ‘incorrectly’. Ambiguity also frustrates the goal of using questions to assess knowledge. Thus, the US Department of Transportation explicitly bans ambiguous questions from exams for flight instructors (Flight Standards Service, 2008); and the trivia community has likewise developed rules and norms that prevent ambiguity. While this is true in many contexts, examples are rife in format called Quizbowl (Boyd-Graber et al., 2012), whose very long questions6 showcase trivia writers’ tactics. For example, Quizbowl author Zhu Ying (writing for the 2005 PARFAIT tournament) asks participants to identify a fictional 6 Like Jeopardy!, they are not syntactically questions but still are designed to elicit knowledge-based responses; for consistency, we still call them questions. character while warning against possible confusion [emphasis added]: He’s not Sherlock Holmes, but his address is 221B. He’s not the Janitor on Scrubs, but his father is played by R. Lee Ermy. [. . . ] For ten points, name this misanthropic, crippled, V"
2020.acl-main.662,D18-1407,1,0.858896,"for a three-way distinction. In between easy questions (system answers correctly with probability 1.0) and hard (probability 0.0), questions with probabilities nearer to 0.5 are more interesting. Taking a cue from Vygotsky’s proximal development theory of human learning (Chaiklin, 2003), these discriminative questions—rather than the easy or the hard ones—should most improve QA systems. These Goldilocks4 questions (not random noise) decide who tops the leaderboard. Unfortunately, existing datasets have many easy questions. Sugawara et al. (2020) find that ablations like shuffling word order (Feng et al., 2018), shuffling sentences, or only offering the most similar sentence do not impair systems. Newer datasets such as DROP (Dua et al., 2019) and HellaSwag (Zellers et al., 2019) are harder for today’s systems; because Goldilocks is a moving target, we propose annual evaluations in Section 5. 2.4 Why so few Goldilocks questions? This is a common problem in trivia tournaments, particularly pub quizzes (Diamond, 2009), where Too Hard Discriminative Questions Too Easy Figure 1: Two datasets with 0.16 annotation error: the top, however, better discriminates QA ability. In the good dataset (top), most qu"
2020.acl-main.662,D18-1241,0,0.173921,"form hypotheses or write your paper, but they can tell you how to run a fun, well-calibrated, and discriminative tournament. Such tournaments are designed to effectively find a winner, which matches the scientific goal of knowing which model best answers questions. Our goal is not to encourage the QA community to adopt the quirks and gimmicks of trivia games. Instead, it’s to encourage experiments and datasets that consistently and efficiently find the systems that best answer questions. 2.1 Are we having fun? Many authors use crowdworkers to establish human accuracy (Rajpurkar et al., 2016; Choi et al., 2018). However, they are not the only humans who should answer a dataset’s questions. So should the dataset’s creators. In the trivia world, this is called a play test: get in the shoes of someone answering the questions. If you find them boring, repetitive, or uninteresting, so will crowdworkers. If you can find shortcuts to answer questions (Rondeau and Hazen, 2018; Kaushik and Lipton, 2018), so will a computer. Concretely, Weissenborn et al. (2017) catalog artifacts in SQuAD (Rajpurkar et al., 2018), the most popular QA leaderboard. If you see a list like “Along with Canada and the United Kingdo"
2020.acl-main.662,D19-1224,0,0.166546,"s on SOTA and leaderboards has focused attention on single automatically computable metrics—systems tend to be compared by their ‘SQuAD score’ or their ‘NQ score’, as if this were all there is to say about their relative capabilities. Like QA leaderboards, trivia tournaments need to decide on a single winner, but they explicitly recognize that there are more interesting comparisons. A tournament may recognize different background/resources—high school, small school, undergraduates (Hentzel, 2018). Similarly, more practical leaderboards would reflect training time or resource requirements (see Dodge et al., 2019) including ‘constrained’ or ‘unconstrained’ training (Bojar et al., 2014). Tournaments also give specific awards (e.g., highest score without 7423 incorrect answers). Again, there are obvious leaderboard analogs that would go beyond a single number. In SQuAD 2.0 (Rajpurkar et al., 2018), abstaining contributes the same to the overall F1 as a fully correct answer, obscuring whether a system is more precise or an effective abstainer. If the task recognizes both abilities as important, reporting a single score risks implicitly prioritizing one balance of the two. 2.3 Do my questions separate the"
2020.acl-main.662,N19-1246,0,0.137664,"tions with probabilities nearer to 0.5 are more interesting. Taking a cue from Vygotsky’s proximal development theory of human learning (Chaiklin, 2003), these discriminative questions—rather than the easy or the hard ones—should most improve QA systems. These Goldilocks4 questions (not random noise) decide who tops the leaderboard. Unfortunately, existing datasets have many easy questions. Sugawara et al. (2020) find that ablations like shuffling word order (Feng et al., 2018), shuffling sentences, or only offering the most similar sentence do not impair systems. Newer datasets such as DROP (Dua et al., 2019) and HellaSwag (Zellers et al., 2019) are harder for today’s systems; because Goldilocks is a moving target, we propose annual evaluations in Section 5. 2.4 Why so few Goldilocks questions? This is a common problem in trivia tournaments, particularly pub quizzes (Diamond, 2009), where Too Hard Discriminative Questions Too Easy Figure 1: Two datasets with 0.16 annotation error: the top, however, better discriminates QA ability. In the good dataset (top), most questions are challenging but not impossible. In the bad dataset (bottom), there are more trivial or impossible questions and annotation"
2020.acl-main.662,P19-1612,0,0.0293146,"ytesting can reveal these issues (Section 2.1), as implicit assumptions can rob a player of correctly answered questions. If you wanted to answer 2014 to “when did Michigan last win the championship”—when the Michigan State Spartans won the Women’s Cross Country championship—and you cannot because you chose the wrong school, the wrong sport, and the wrong gender, you would complain as a player; researchers instead discover latent assumptions that creep into the data.7 It is worth emphasizing that this is not a purely hypothetical problem. For example, Open Domain Retrieval Question Answering (Lee et al., 2019) deliberately avoids providing a reference context for the question in its framing but, in re-purposing data such as Natural Questions, opaquely relies on it for the gold answers. 3.2 Avoiding superficial evaluations A related issue is that, in the words of Voorhees and Tice (2000), “there is no such thing as a question with an obvious answer”. As a consequence, trivia question authors delineate acceptable and unacceptable answers. For example, in writing for the trivia tournament Harvard Fall XI, Robert Chu uses a mental model of an answerer to explicitly delineate the range of acceptable cor"
2020.acl-main.662,P17-1147,0,0.0411858,"buzzer is not live until Alex finishes reading the question. And if you buzz in before your buzzer goes live, you actually lock yourself out for a fraction of a second. So the big mistake on the show is people who are all adrenalized and are buzzing too quickly, too eagerly. Malone: OK. To some degree, Jeopardy! is kind of a video game, and a crappy video game where it’s, like, light goes on, press button—that’s it. Jennings: (Laughter) Yeah. Jeopardy!’s buzzers are a gimmick to ensure good television; however, Quizbowl buzzers discriminate knowledge (Section 2.3). Similarly, while Trivia QA (Joshi et al., 2017) is written by knowledgeable writers, the questions are not pyramidal. Pyramidal Recall that effective datasets discriminate the best from the rest—the higher the proportion of effective questions ρ, the better. Quizbowl’s ρ is nearly 1.0 because discrimination happens within a question: after every word, an answerer must decide if they know enough to answer. Quizbowl questions are arranged so that questions are maximally pyramidal: questions begin with hard clues—ones that require deep understanding— to more accessible clues that are well known. Well-Edited Quizbowl questions are created in p"
2020.acl-main.662,D18-1546,0,0.0214056,"urage experiments and datasets that consistently and efficiently find the systems that best answer questions. 2.1 Are we having fun? Many authors use crowdworkers to establish human accuracy (Rajpurkar et al., 2016; Choi et al., 2018). However, they are not the only humans who should answer a dataset’s questions. So should the dataset’s creators. In the trivia world, this is called a play test: get in the shoes of someone answering the questions. If you find them boring, repetitive, or uninteresting, so will crowdworkers. If you can find shortcuts to answer questions (Rondeau and Hazen, 2018; Kaushik and Lipton, 2018), so will a computer. Concretely, Weissenborn et al. (2017) catalog artifacts in SQuAD (Rajpurkar et al., 2018), the most popular QA leaderboard. If you see a list like “Along with Canada and the United Kingdom, what country. . . ”, you can ignore the rest of the question and just type Ctrl+F (Yuan et al., 2019; Russell, 2020) to find the third country—Australia in this case—that appears with “Canada and the UK”. Other times, a SQuAD playtest would reveal frustrating questions that are i) answerable given the information but not with a direct span,1 ii) answerable only given facts beyond the g"
2020.acl-main.662,P11-1106,0,0.0100815,"ivia nerds because they have useful information (not just about the election of 1876). Trivia is not just the accumulation of information but also connecting disparate facts (Jennings, 2006). These skills are exactly those we want computers to develop. Trivia nerds are writing questions anyway; we 12 This complex methodology can be an advantage. The underlying mechanisms of systems that can play Quizbowl (e.g., reinforcement learning) share properties with other tasks, such as simultaneous translation (Grissom II et al., 2014; Ma et al., 2019), human incremental processing (Levy et al., 2008; Levy, 2011), and opponent modeling (He et al., 2016). 7428 can save money and time if we pool resources.13 Computer scientists benefit if the trivia community writes questions that aren’t trivial for computers to solve (e.g., avoiding quotes and named entities). The trivia community benefits from tools that make their job easier: show related questions, link to Wikipedia, or predict where humans will answer. Likewise, the broader public has unique knowledge and skills. In contrast to low-paid crowdworkers, public platforms for question answering and citizen science (Bowser et al., 2013) are brimming with"
2020.acl-main.662,W04-1013,0,0.0345316,"orne et al., 2019). Moreover, another lesson the QA community could learn from trivia games is to turn it into a spectacle: exciting games with a telegenic host. This has a benefit to the public, who see how QA systems fail on difficult questions and to QA researchers, who have a spoonful of fun sugar to inspect their systems’ output and their competitors’. In between full automation and expensive humans in the loop are automatic metrics that mimic the flexibility of human raters, inspired by machine translation evaluations (Papineni et al., 2002; Specia and Farzindar, 2010) or summarization (Lin, 2004). However, we should not forget that these metrics were introduced as ‘understudies’—good enough when quick evaluations are needed for system building but no substitute for a proper evaluation. In machine translation, Laubli et al. (2020) reveal that crowdworkers cannot spot the errors that neural MT systems make—fortunately, trivia nerds are cheaper than professional translators. Finally, if you want to make human–computer comparisons, pick the right humans. Paraphrasing a participant of the 2019 MRQA workshop (Fisch et al., 2019), a system better than the average human at brain surgery does"
2020.acl-main.662,2020.emnlp-main.466,0,0.0924086,"he first place” (Zaenen, 2006), we need to explicitly appreciate the unavoidable ambiguity instead of silently glossing over it.14 This is already an active area of research, with conversational QA being a new setting actively explored by several datasets (Reddy et al., 2018; Choi et al., 2018); and other work explicitly focusing on identifying useful clarification questions (Rao and Daumé III), thematically linked questions (Elgohary et al., 2018) or resolving ambiguities that arise from coreference or pragmatic constraints by rewriting underspecified question strings (Elgohary et al., 2019; Min et al., 2020). a ranking over numbers. This can be problematic for several reasons. The first is that single numbers have some variance; it’s better to communicate estimates with error bars. Revel in Spectacle However, with more complicated systems and evaluations, a return to the yearly evaluations of TRECQA may be the best option. This improves not only the quality of evaluation (we can have real-time human judging) but also lets the test set reflect the build it/break it cycle (Ruef et al., 2016), as attempted by the 2019 iteration of FEVER (Thorne et al., 2019). Moreover, another lesson the QA communit"
2020.acl-main.662,P02-1040,0,0.107936,"(Ruef et al., 2016), as attempted by the 2019 iteration of FEVER (Thorne et al., 2019). Moreover, another lesson the QA community could learn from trivia games is to turn it into a spectacle: exciting games with a telegenic host. This has a benefit to the public, who see how QA systems fail on difficult questions and to QA researchers, who have a spoonful of fun sugar to inspect their systems’ output and their competitors’. In between full automation and expensive humans in the loop are automatic metrics that mimic the flexibility of human raters, inspired by machine translation evaluations (Papineni et al., 2002; Specia and Farzindar, 2010) or summarization (Lin, 2004). However, we should not forget that these metrics were introduced as ‘understudies’—good enough when quick evaluations are needed for system building but no substitute for a proper evaluation. In machine translation, Laubli et al. (2020) reveal that crowdworkers cannot spot the errors that neural MT systems make—fortunately, trivia nerds are cheaper than professional translators. Finally, if you want to make human–computer comparisons, pick the right humans. Paraphrasing a participant of the 2019 MRQA workshop (Fisch et al., 2019), a s"
2020.acl-main.662,Q19-1043,0,0.0167474,"9), a system better than the average human at brain surgery does not imply superhuman performance in brain surgery. Likewise, beating a distracted crowdworker on QA is not QA’s endgame. If your task is realistic, fun, and challenging, you will find experts to play against your computer. Not only will this give you human baselines worth reporting—they can also tell you how to fix your QA dataset. . . after all, they’ve been at it longer than you have. Be Honest in Crowning QA Champions Leaderboards are a ranking over entrants based on 14 Not surprisingly, ‘inherent’ ambiguity is not limited to Pavlick and Kwiatkowski (2019) show natural language inference has ‘inherent disagreements’ between humans and advocate for recovering the full range of accepted inferences. QA ; While—particularly for leaderboards—it is tempting to turn everything into a single number, there are often different sub-tasks and systems who deserve recognition. A simple model that requires less training data or runs in under ten milliseconds may be objectively more useful than a bloated, brittle monster of a system that has a slightly higher F1 (Dodge et al., 2019). While you may only rank by a single metric (this is what trivia tournaments d"
2020.acl-main.662,P18-2124,0,0.177325,"Are we having fun? Many authors use crowdworkers to establish human accuracy (Rajpurkar et al., 2016; Choi et al., 2018). However, they are not the only humans who should answer a dataset’s questions. So should the dataset’s creators. In the trivia world, this is called a play test: get in the shoes of someone answering the questions. If you find them boring, repetitive, or uninteresting, so will crowdworkers. If you can find shortcuts to answer questions (Rondeau and Hazen, 2018; Kaushik and Lipton, 2018), so will a computer. Concretely, Weissenborn et al. (2017) catalog artifacts in SQuAD (Rajpurkar et al., 2018), the most popular QA leaderboard. If you see a list like “Along with Canada and the United Kingdom, what country. . . ”, you can ignore the rest of the question and just type Ctrl+F (Yuan et al., 2019; Russell, 2020) to find the third country—Australia in this case—that appears with “Canada and the UK”. Other times, a SQuAD playtest would reveal frustrating questions that are i) answerable given the information but not with a direct span,1 ii) answerable only given facts beyond the given paragraph,2 iii) unintentionally embedded in a discourse, resulting in arbitrary correct answers,3 iv) or"
2020.acl-main.662,D16-1264,0,0.053,"ia nerds cannot help you form hypotheses or write your paper, but they can tell you how to run a fun, well-calibrated, and discriminative tournament. Such tournaments are designed to effectively find a winner, which matches the scientific goal of knowing which model best answers questions. Our goal is not to encourage the QA community to adopt the quirks and gimmicks of trivia games. Instead, it’s to encourage experiments and datasets that consistently and efficiently find the systems that best answer questions. 2.1 Are we having fun? Many authors use crowdworkers to establish human accuracy (Rajpurkar et al., 2016; Choi et al., 2018). However, they are not the only humans who should answer a dataset’s questions. So should the dataset’s creators. In the trivia world, this is called a play test: get in the shoes of someone answering the questions. If you find them boring, repetitive, or uninteresting, so will crowdworkers. If you can find shortcuts to answer questions (Rondeau and Hazen, 2018; Kaushik and Lipton, 2018), so will a computer. Concretely, Weissenborn et al. (2017) catalog artifacts in SQuAD (Rajpurkar et al., 2018), the most popular QA leaderboard. If you see a list like “Along with Canada a"
2020.acl-main.662,W18-2602,0,0.0219306,"es. Instead, it’s to encourage experiments and datasets that consistently and efficiently find the systems that best answer questions. 2.1 Are we having fun? Many authors use crowdworkers to establish human accuracy (Rajpurkar et al., 2016; Choi et al., 2018). However, they are not the only humans who should answer a dataset’s questions. So should the dataset’s creators. In the trivia world, this is called a play test: get in the shoes of someone answering the questions. If you find them boring, repetitive, or uninteresting, so will crowdworkers. If you can find shortcuts to answer questions (Rondeau and Hazen, 2018; Kaushik and Lipton, 2018), so will a computer. Concretely, Weissenborn et al. (2017) catalog artifacts in SQuAD (Rajpurkar et al., 2018), the most popular QA leaderboard. If you see a list like “Along with Canada and the United Kingdom, what country. . . ”, you can ignore the rest of the question and just type Ctrl+F (Yuan et al., 2019; Russell, 2020) to find the third country—Australia in this case—that appears with “Canada and the UK”. Other times, a SQuAD playtest would reveal frustrating questions that are i) answerable given the information but not with a direct span,1 ii) answerable on"
2020.acl-main.662,2010.jec-1.5,0,0.0100292,"s attempted by the 2019 iteration of FEVER (Thorne et al., 2019). Moreover, another lesson the QA community could learn from trivia games is to turn it into a spectacle: exciting games with a telegenic host. This has a benefit to the public, who see how QA systems fail on difficult questions and to QA researchers, who have a spoonful of fun sugar to inspect their systems’ output and their competitors’. In between full automation and expensive humans in the loop are automatic metrics that mimic the flexibility of human raters, inspired by machine translation evaluations (Papineni et al., 2002; Specia and Farzindar, 2010) or summarization (Lin, 2004). However, we should not forget that these metrics were introduced as ‘understudies’—good enough when quick evaluations are needed for system building but no substitute for a proper evaluation. In machine translation, Laubli et al. (2020) reveal that crowdworkers cannot spot the errors that neural MT systems make—fortunately, trivia nerds are cheaper than professional translators. Finally, if you want to make human–computer comparisons, pick the right humans. Paraphrasing a participant of the 2019 MRQA workshop (Fisch et al., 2019), a system better than the average"
2020.acl-main.662,Q19-1029,1,0.839876,"ards—are designed to test knowledge. Experts know when questions are ambiguous (Section 3.1); while “what play has a character whose father is dead” could be Hamlet, Antigone, or Proof, a good writer’s knowledge avoids the ambiguity. When authors omit these cues, the question is derided as a “hose” (Eltinge, 2013), which robs the tournament of fun (Section 2.1). One of the benefits of contrived formats is a focus on specific phenomena. Dua et al. (2019) exclude questions an existing MRQA system could answer to focus on challenging quantitative reasoning. One of the trivia experts consulted in Wallace et al. (2019) crafted a question that tripped up neural QA by embedding the phrase “this author opens Crime and Punishment” into a question; the top system confidently answers Fyodor Dostoyevski. However, that phrase was in a longer question “The narrator in Cogwheels by this author opens Crime and Punishment to find it has become The Brothers Karamazov”. Again, this shows the inventiveness and linguistic dexterity of the trivia community. A counterargument is that real-life questions— e.g., on Yahoo! Questions (Szpektor and Dror, 2013), Quora (Iyer et al., 2017) or web search (Kwiatkowski et al., 2019)—ig"
2020.acl-main.662,K17-1028,0,0.0216096,"ently find the systems that best answer questions. 2.1 Are we having fun? Many authors use crowdworkers to establish human accuracy (Rajpurkar et al., 2016; Choi et al., 2018). However, they are not the only humans who should answer a dataset’s questions. So should the dataset’s creators. In the trivia world, this is called a play test: get in the shoes of someone answering the questions. If you find them boring, repetitive, or uninteresting, so will crowdworkers. If you can find shortcuts to answer questions (Rondeau and Hazen, 2018; Kaushik and Lipton, 2018), so will a computer. Concretely, Weissenborn et al. (2017) catalog artifacts in SQuAD (Rajpurkar et al., 2018), the most popular QA leaderboard. If you see a list like “Along with Canada and the United Kingdom, what country. . . ”, you can ignore the rest of the question and just type Ctrl+F (Yuan et al., 2019; Russell, 2020) to find the third country—Australia in this case—that appears with “Canada and the UK”. Other times, a SQuAD playtest would reveal frustrating questions that are i) answerable given the information but not with a direct span,1 ii) answerable only given facts beyond the given paragraph,2 iii) unintentionally embedded in a discour"
2020.acl-main.662,P15-1128,0,0.0243724,"tent, ii) supported by a scoring metric that matches your goals, iii) using gold annotations that reward those who do the task well? Imagine someone who loves answering the questions your task poses: would they have fun on your task? This is the foundation of Gamification (von Ahn, 2006), which can create quality data from users motivated by fun rather than pay. Even if you pay crowdworkers, unfun questions may undermine your dataset goals. 2.2 Am I measuring what I care about? Answering questions requires multiple skills: identifying answer mentions (Hermann et al., 2015), naming the answer (Yih et al., 2015), abstaining when necessary (Rajpurkar et al., 2018), and justifying an answer (Thorne et al., 2018). In QA, the emphasis on SOTA and leaderboards has focused attention on single automatically computable metrics—systems tend to be compared by their ‘SQuAD score’ or their ‘NQ score’, as if this were all there is to say about their relative capabilities. Like QA leaderboards, trivia tournaments need to decide on a single winner, but they explicitly recognize that there are more interesting comparisons. A tournament may recognize different background/resources—high school, small school, undergrad"
2020.acl-main.662,P19-1472,0,0.0274006,"o 0.5 are more interesting. Taking a cue from Vygotsky’s proximal development theory of human learning (Chaiklin, 2003), these discriminative questions—rather than the easy or the hard ones—should most improve QA systems. These Goldilocks4 questions (not random noise) decide who tops the leaderboard. Unfortunately, existing datasets have many easy questions. Sugawara et al. (2020) find that ablations like shuffling word order (Feng et al., 2018), shuffling sentences, or only offering the most similar sentence do not impair systems. Newer datasets such as DROP (Dua et al., 2019) and HellaSwag (Zellers et al., 2019) are harder for today’s systems; because Goldilocks is a moving target, we propose annual evaluations in Section 5. 2.4 Why so few Goldilocks questions? This is a common problem in trivia tournaments, particularly pub quizzes (Diamond, 2009), where Too Hard Discriminative Questions Too Easy Figure 1: Two datasets with 0.16 annotation error: the top, however, better discriminates QA ability. In the good dataset (top), most questions are challenging but not impossible. In the bad dataset (bottom), there are more trivial or impossible questions and annotation error is concentrated on the challeng"
2020.emnlp-main.482,N15-1170,0,0.0177933,"vily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotations. We test CLIME on cross-lingual information triage in internat"
2020.emnlp-main.482,P18-1073,0,0.0259999,"propriate for languages with more data. Future work can extend the interactive component of CLIME to multilingual transformers. 5 Related Work Cross-Lingual Word Embeddings. Ruder et al. (2019) summarize previous CLWE methods. These methods learn from existing resources such as dictionaries, parallel text, and monolingual corpora. Therefore, the availability and quality of training data primarily determines the success of these methods (Søgaard et al., 2018). To improve the suitability of CLWE methods in low-resource settings, recent work focuses on learning without cross-lingual supervision (Artetxe et al., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al"
2020.emnlp-main.482,Q17-1010,0,0.0604242,"To simplify the task, we consider a binary classification problem of detecting whether the documents are associated with medical needs. Table 1 shows an example document. To balance the label distribution, we sample an equal number of negative examples. Word Embeddings. To transfer knowledge between languages, we build CLWE between English and each target language. We experiment with two methods to pre-train CLWE: (1) train monolingual embeddings with word2vec (Mikolov et al., 2013b) and align with CCA (Faruqui et al., 2015; Ammar et al., 2016), (2) train monolingual embeddings with fastText (Bojanowski et al., 2017) and align with RCSLS (Joulin et al., 2018). The English em2 Download from https://www.ldc.upenn.edu beddings are trained on Wikipedia and the target language embeddings are trained on unlabeled documents from the LORELEI language packs. For alignment, we use the small English dictionary in each pack. Low-resource language speakers are hard to find, so we do not try all combinations of languages and CLWE: we use CCA embeddings for Tigrinya and Uyghur, RCSLS embeddings for Ilocano. Since Sinhalese speakers are easier to find, we experiment with both CLWE for Sinhalese. Text Classifier. Our clas"
2020.emnlp-main.482,Q18-1039,0,0.0909114,"Missing"
2020.emnlp-main.482,2020.acl-main.747,0,0.0165428,"increase test accuracy on identifying health-related documents in less than an hour. CLIME is related to active learning (Settles, 2009), which also improves a classifier through user interaction. Therefore, we compare CLIME with an active learning baseline that asks a user to label target language documents. Under the same annotation time constraint, CLIME often has higher accuracy. Furthermore, the two methods are complementary. Combining active learning with CLIME increases accuracy even more, and the user-adapted model is competitive with a large, resource-hungry multilingual transformer (Conneau et al., 2020). 2 Interactive Neighborhood Reshaping This section introduces the interface designed to solicit human feedback on neighborhoods of CLWE and our keyword selection criterion. Suppose that we have two languages with vocabulary V1 and V2 . Let E be a pre-trained CLWE matrix, where Ew is the vector representation of word type w in the joint vocabulary V = V1 ∪ V2 . Our goal is to help a bilingual novice (i.e., not a machine learning expert) improve the CLWE E for a downstream task through inspection of neighboring words. 2.1 Keyword Selection With limited annotation time, users cannot vet the enti"
2020.emnlp-main.482,D09-1009,0,0.277055,"to help a bilingual novice (i.e., not a machine learning expert) improve the CLWE E for a downstream task through inspection of neighboring words. 2.1 Keyword Selection With limited annotation time, users cannot vet the entire vocabulary. Instead, we need to find a small salient subset of keywords K ⊆ V whose embeddings, if vetted, would most improve a downstream task. For example, if the downstream task is sentiment analysis, our keywords set should include sentiment words such as “good” and “bad”. Prior work in active learning solicits keywords using information gain (Raghavan et al., 2006; Druck et al., 2009; Settles, 2011), but this cannot be applied to continuous embeddings. Li et al. (2016) suggest that the contribution of one dimension of a word embedding to the loss function can be approximated by the absolute value of its partial derivative, and therefore they use partial derivatives to visualize the behavior of neural models. However, rather than understanding the importance of individual dimensions, we want to compute the salience of an entire word vector. Therefore, we extend their idea by defining the salience of a word embedding as the magnitude of the loss function’s gradient. This sc"
2020.emnlp-main.482,D17-1063,0,0.01969,"17). Therefore, future advances in these areas may also improve CLIME. Another line of future work is to investigate alternative user interfaces. For example, we could ask bilingual users to rank nearest neighbors (Sakaguchi and Van Durme, 2018) or provide scalar grades (Hill et al., 2015) instead of accepting/rejecting individual neighbors. We also explore a simple combination of active learning and CLIME. Simultaneously applying both methods is better than using either alone. In the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (Fang et al., 2017). CLIME CLWE Human-in-the-Loop Multilingual Systems. CLIME is inspired by human-in-the-loop systems that bridge language gaps. Brown and Grinter (2016) build an interactive translation platform to help refugee resettlement. Yuan et al. (2018) interactively align topic models across languages. Active Learning. A common solution to data scarcity is active learning, the framework in which the learner iteratively queries an oracle (often a human) to receive annotations on unlabeled data. Settles (2009) summarizes popular active learning methods. Most active learning methods solicit labels for trai"
2020.emnlp-main.482,D18-1407,1,0.853298,"curacy: active learning and CLIME are complementary. Singlesample t-tests confirm that CLIME is significantly better than Base and A+C is significantly better than Active (Appendix A.1). Keyword Detection. We inspect the list of the fifty most salient keywords (Section 2.1). Most keywords have obvious connections to our classification task of detecting medical emergencies, such as “ambulance”, “hospitals”, and “disease”. However, the list also contains some words that are unrelated to a medical emergency, including “over” and “given”. These words may be biases or artifacts from training data (Feng et al., 2018). 5989 CLIME −−−→ (a) Neighborhood of “plague” CLIME −−−→ (b) Neighborhood of “ill” Figure 5: T- SNE visualization of embeddings before (left) and after (right) CLIME updates. From one Sinhalese user study, we inspect two keywords, “ill” and “plague”, and their five closest neighbors in English (blue) and Sinhalese (green). The Sinhalese words are labeled with English translations. Shape denotes the type of feedback: “+” for positive neighbors and “x” for negative neighbors. Number of Keywords. To evaluate how feedback quantity changes accuracy, we vary the number of keywords and compare test"
2020.emnlp-main.482,P19-1070,0,0.0667672,"y and quality of training data primarily determines the success of these methods (Søgaard et al., 2018). To improve the suitability of CLWE methods in low-resource settings, recent work focuses on learning without cross-lingual supervision (Artetxe et al., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings"
2020.emnlp-main.482,P18-1004,0,0.038408,"Missing"
2020.emnlp-main.482,J15-4004,0,0.03786,"ME further improves the system. CLIME has a modular design with three components: keyword ranking, user interface, and embedding refinement. The keyword ranking and the embedding refinement modules build upon existing methods for interpreting neural networks (Li et al., 2016) and fine-tuning word embeddings (Mrkši´c et al., 2017). Therefore, future advances in these areas may also improve CLIME. Another line of future work is to investigate alternative user interfaces. For example, we could ask bilingual users to rank nearest neighbors (Sakaguchi and Van Durme, 2018) or provide scalar grades (Hill et al., 2015) instead of accepting/rejecting individual neighbors. We also explore a simple combination of active learning and CLIME. Simultaneously applying both methods is better than using either alone. In the future, we plan to train a policy that dynamically combines the two interactions with reinforcement learning (Fang et al., 2017). CLIME CLWE Human-in-the-Loop Multilingual Systems. CLIME is inspired by human-in-the-loop systems that bridge language gaps. Brown and Grinter (2016) build an interactive translation platform to help refugee resettlement. Yuan et al. (2018) interactively align topic mod"
2020.emnlp-main.482,D18-1043,0,0.0187236,"s with more data. Future work can extend the interactive component of CLIME to multilingual transformers. 5 Related Work Cross-Lingual Word Embeddings. Ruder et al. (2019) summarize previous CLWE methods. These methods learn from existing resources such as dictionaries, parallel text, and monolingual corpora. Therefore, the availability and quality of training data primarily determines the success of these methods (Søgaard et al., 2018). To improve the suitability of CLWE methods in low-resource settings, recent work focuses on learning without cross-lingual supervision (Artetxe et al., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 201"
2020.emnlp-main.482,P15-1162,1,0.784673,"often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results. 1 mediocre disappointing Figure 1: A hypothetical topographic map of an English–French embedding space tailored for sentiment analysis. Dots are English words, and squares are French words. Positive sentiment words are grouped in a clime (red), while negative sentiment words are grouped in another clime (blue). These climes help sentiment analysis. Introduction Modern text classification requires large labeled datasets and pre-trained word embeddings (Kim, 2014; Iyyer et al., 2015; Joulin et al., 2017). However, scarcity of both labeled and unlabeled data holds back applications in low-resource languages. Cross-lingual word embeddings (Mikolov et al., 2013a, CLWE) can bridge the gap by mapping words from different languages to a shared vector space. Using CLWE features, models trained in a resource-rich language (e.g., English) can predict labels for other languages. The success of CLWE relies on the domain and quality of training data (Søgaard et al., 2018). While these methods have impressive word translation accuracy, they are not tailored for downstream tasks such"
2020.emnlp-main.482,D18-1330,0,0.0246625,"sification problem of detecting whether the documents are associated with medical needs. Table 1 shows an example document. To balance the label distribution, we sample an equal number of negative examples. Word Embeddings. To transfer knowledge between languages, we build CLWE between English and each target language. We experiment with two methods to pre-train CLWE: (1) train monolingual embeddings with word2vec (Mikolov et al., 2013b) and align with CCA (Faruqui et al., 2015; Ammar et al., 2016), (2) train monolingual embeddings with fastText (Bojanowski et al., 2017) and align with RCSLS (Joulin et al., 2018). The English em2 Download from https://www.ldc.upenn.edu beddings are trained on Wikipedia and the target language embeddings are trained on unlabeled documents from the LORELEI language packs. For alignment, we use the small English dictionary in each pack. Low-resource language speakers are hard to find, so we do not try all combinations of languages and CLWE: we use CCA embeddings for Tigrinya and Uyghur, RCSLS embeddings for Ilocano. Since Sinhalese speakers are easier to find, we experiment with both CLWE for Sinhalese. Text Classifier. Our classifier is a convolutional neural network (K"
2020.emnlp-main.482,E17-2068,0,0.026799,"racy faster than an active learning baseline and can be easily combined with active learning to improve results. 1 mediocre disappointing Figure 1: A hypothetical topographic map of an English–French embedding space tailored for sentiment analysis. Dots are English words, and squares are French words. Positive sentiment words are grouped in a clime (red), while negative sentiment words are grouped in another clime (blue). These climes help sentiment analysis. Introduction Modern text classification requires large labeled datasets and pre-trained word embeddings (Kim, 2014; Iyyer et al., 2015; Joulin et al., 2017). However, scarcity of both labeled and unlabeled data holds back applications in low-resource languages. Cross-lingual word embeddings (Mikolov et al., 2013a, CLWE) can bridge the gap by mapping words from different languages to a shared vector space. Using CLWE features, models trained in a resource-rich language (e.g., English) can predict labels for other languages. The success of CLWE relies on the domain and quality of training data (Søgaard et al., 2018). While these methods have impressive word translation accuracy, they are not tailored for downstream tasks such as text classification"
2020.emnlp-main.482,2020.findings-emnlp.323,0,0.0221149,"ansfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotations. We test CLIME on cross-lingual information triage in international health emergencies for four low-resource languages. Bilingual users can quickly improve a model with the help of CLIME at a faster rate than an active learning baseline. Combining active learning with CLIME further improves the system. CLIME has a modular design with three components: keywor"
2020.emnlp-main.482,D14-1181,0,0.00697457,"ings. CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results. 1 mediocre disappointing Figure 1: A hypothetical topographic map of an English–French embedding space tailored for sentiment analysis. Dots are English words, and squares are French words. Positive sentiment words are grouped in a clime (red), while negative sentiment words are grouped in another clime (blue). These climes help sentiment analysis. Introduction Modern text classification requires large labeled datasets and pre-trained word embeddings (Kim, 2014; Iyyer et al., 2015; Joulin et al., 2017). However, scarcity of both labeled and unlabeled data holds back applications in low-resource languages. Cross-lingual word embeddings (Mikolov et al., 2013a, CLWE) can bridge the gap by mapping words from different languages to a shared vector space. Using CLWE features, models trained in a resource-rich language (e.g., English) can predict labels for other languages. The success of CLWE relies on the domain and quality of training data (Søgaard et al., 2018). While these methods have impressive word translation accuracy, they are not tailored for do"
2020.emnlp-main.482,C12-1089,0,0.276539,"too far ˆ away from the original embeddings E. The final cost function combines the feedback cost (Equation 3) and the regularizer (Equation 4): C(E) = Cf (E) + λR(E), (5) where the hyperparameter λ controls the strength of the regularizer. The updated embeddings enforce constraints from user feedback while preserving other structures from the original embeddings. After tuning in a pilot user study, we set λ to one. We use the Adam optimizer (Kingma and Ba, 2015) with default hyperparameters. 4 Cross-Lingual Classification Experiments We evaluate CLIME on cross-lingual documentclassification (Klementiev et al., 2012), where we build a text classifier for a low-resource target Ilocano ... Nagtalinaed dagiti pito a balod ti Bureau of Jail Management and Penology (BJMP) ditoy ciudad ti Laoag iti isolation room gapo iti tuko ... English ... Seven inmates from the Bureau of Jail Management and Penology (BJMP), Laoag City, have been transferred to the isolation room due to chicken pox ... Table 1: Excerpt of a positive Ilocano test example (top) and its English translation (bottom) that describes a medical emergency. language using labeled data in a high-resource source language through CLWE. Our task identifie"
2020.emnlp-main.482,N16-1082,0,0.287384,"downstream task through inspection of neighboring words. 2.1 Keyword Selection With limited annotation time, users cannot vet the entire vocabulary. Instead, we need to find a small salient subset of keywords K ⊆ V whose embeddings, if vetted, would most improve a downstream task. For example, if the downstream task is sentiment analysis, our keywords set should include sentiment words such as “good” and “bad”. Prior work in active learning solicits keywords using information gain (Raghavan et al., 2006; Druck et al., 2009; Settles, 2011), but this cannot be applied to continuous embeddings. Li et al. (2016) suggest that the contribution of one dimension of a word embedding to the loss function can be approximated by the absolute value of its partial derivative, and therefore they use partial derivatives to visualize the behavior of neural models. However, rather than understanding the importance of individual dimensions, we want to compute the salience of an entire word vector. Therefore, we extend their idea by defining the salience of a word embedding as the magnitude of the loss function’s gradient. This score summarizes salience of all dimensions from a word embedding. Formally, let x = hx1"
2020.emnlp-main.482,P18-1020,1,0.880351,"Missing"
2020.emnlp-main.482,L18-1560,0,0.193578,"Missing"
2020.emnlp-main.482,D11-1136,0,0.582572,"novice (i.e., not a machine learning expert) improve the CLWE E for a downstream task through inspection of neighboring words. 2.1 Keyword Selection With limited annotation time, users cannot vet the entire vocabulary. Instead, we need to find a small salient subset of keywords K ⊆ V whose embeddings, if vetted, would most improve a downstream task. For example, if the downstream task is sentiment analysis, our keywords set should include sentiment words such as “good” and “bad”. Prior work in active learning solicits keywords using information gain (Raghavan et al., 2006; Druck et al., 2009; Settles, 2011), but this cannot be applied to continuous embeddings. Li et al. (2016) suggest that the contribution of one dimension of a word embedding to the loss function can be approximated by the absolute value of its partial derivative, and therefore they use partial derivatives to visualize the behavior of neural models. However, rather than understanding the importance of individual dimensions, we want to compute the salience of an entire word vector. Therefore, we extend their idea by defining the salience of a word embedding as the magnitude of the loss function’s gradient. This score summarizes s"
2020.emnlp-main.482,N16-1018,0,0.0630818,"Missing"
2020.emnlp-main.482,Q17-1022,0,0.0346062,"Missing"
2020.emnlp-main.482,P10-1023,0,0.0137763,"otations, CLIME updates the embeddings to reflect their feedback. The algorithm reshapes the neighborhood so that words near a keyword share similar semantic attributes. Together, these embeddings form desired taskspecific connections between words across languages. Our update equations are inspired by ATTRACT- REPEL (Mrkši´c et al., 2017), which fine-tunes word embeddings with synonym and antonym constraints. The objective in ATTRACTREPEL pulls synonyms closer to and pushes antonyms further away from their nearest neighbors. This objective is useful for large lexical resources like BabelNet (Navigli and Ponzetto, 2010) with hundreds of thousands linguistic constraints, but our pilot experiment suggests that the method is not suitable for smaller constraint sets. Since CLIME is designed for low-resource languages, we optimize an objective that reshapes the neighborhood more drastically than ATTRACT- REPEL. 3.1 Feedback Cost For each keyword k ∈ K, we collect a positive set Pk and a negative set Nk (Section 2.2). To refine 5986 embeddings E with human feedback, we increase the similarity between k and each positive word p ∈ Pk , and decrease the similarity between k and each negative word n ∈ Nk . Formally, w"
2020.emnlp-main.482,D10-1103,0,0.0284959,"and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classific"
2020.emnlp-main.482,P18-1072,0,0.132892,"Missing"
2020.emnlp-main.482,L16-1521,0,0.0816641,"ti pito a balod ti Bureau of Jail Management and Penology (BJMP) ditoy ciudad ti Laoag iti isolation room gapo iti tuko ... English ... Seven inmates from the Bureau of Jail Management and Penology (BJMP), Laoag City, have been transferred to the isolation room due to chicken pox ... Table 1: Excerpt of a positive Ilocano test example (top) and its English translation (bottom) that describes a medical emergency. language using labeled data in a high-resource source language through CLWE. Our task identifies whether a document describes a medical emergency, useful for planning disaster relief (Strassel and Tracey, 2016). The source language is English and the four low-resource target languages are Ilocano, Sinhalese, Tigrinya, and Uyghur. Our experiments confirm that a bilingual user can quickly improve the test accuracy of crosslingual models through CLIME. Alternatively, we can ask an annotator to improve the model by labeling more training documents in the target language. Therefore, we compare CLIME to an active learning baseline that queries the user for document labels; CLIME often improves accuracy faster. Then, we combine CLIME and active learning to show an even faster improvement of test accuracy."
2020.emnlp-main.482,P09-1027,0,0.0243158,"of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotations. We test CLIME on cross-lingual information triage in international health emergencies for fou"
2020.emnlp-main.482,I08-1022,0,0.0386376,"l., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual"
2020.emnlp-main.482,P10-1114,0,0.0231499,"efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotat"
2020.emnlp-main.482,D19-1077,0,0.0685283,"Missing"
2020.emnlp-main.482,P17-1130,0,0.0173083,"nolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work"
2020.emnlp-main.482,2020.emnlp-main.637,1,0.733454,"Active Learning. A common solution to data scarcity is active learning, the framework in which the learner iteratively queries an oracle (often a human) to receive annotations on unlabeled data. Settles (2009) summarizes popular active learning methods. Most active learning methods solicit labels for training examples/documents, while CLIME asks for word-level annotation. Previous active learning methods that use feature-level annotation (Raghavan et al., 2006; Zaidan et al., 2007; Druck et al., 2009; Settles, 2011) are not applicable to neural networks and CLWE. Closely related to our work, Yuan et al. (2020) propose an active learning strategy that selects examples based on language modeling pre-training. Neural Network Interpretation. Our keyword detection algorithm expands upon prior work in interpreting neural networks. Li et al. (2016) uses the gradient of the objective function to linearly approximate salience of one dimension, which helps interpret and visualize word compositionality in neural networks. Their ideas are inspired by visual salience in computer vision (Simonyan et al., 2013; Zeiler and Fergus, 2014). We further extend the idea to compute the global salience of an entire word v"
2020.emnlp-main.482,N07-1033,0,0.0365299,"interactive translation platform to help refugee resettlement. Yuan et al. (2018) interactively align topic models across languages. Active Learning. A common solution to data scarcity is active learning, the framework in which the learner iteratively queries an oracle (often a human) to receive annotations on unlabeled data. Settles (2009) summarizes popular active learning methods. Most active learning methods solicit labels for training examples/documents, while CLIME asks for word-level annotation. Previous active learning methods that use feature-level annotation (Raghavan et al., 2006; Zaidan et al., 2007; Druck et al., 2009; Settles, 2011) are not applicable to neural networks and CLWE. Closely related to our work, Yuan et al. (2020) propose an active learning strategy that selects examples based on language modeling pre-training. Neural Network Interpretation. Our keyword detection algorithm expands upon prior work in interpreting neural networks. Li et al. (2016) uses the gradient of the objective function to linearly approximate salience of one dimension, which helps interpret and visualize word compositionality in neural networks. Their ideas are inspired by visual salience in computer vi"
2020.emnlp-main.482,2020.acl-main.201,1,0.837488,"al word embeddings (Mikolov et al., 2013a, CLWE) can bridge the gap by mapping words from different languages to a shared vector space. Using CLWE features, models trained in a resource-rich language (e.g., English) can predict labels for other languages. The success of CLWE relies on the domain and quality of training data (Søgaard et al., 2018). While these methods have impressive word translation accuracy, they are not tailored for downstream tasks such as text classification (Glavas ? ∗ indicates equal contribution Benjamin Van Durme Johns Hopkins University vandurme@jhu.edu et al., 2019; Zhang et al., 2020a). We develop CLassifying Interactively with Multilingual Embeddings (CLIME), that efficiently specializes CLWE with human interaction.1 Given a pre-trained CLWE , a bilingual speaker in the loop reviews the nearest-neighbor words. CLIME capitalizes on the intuition that neighboring words in an ideal embedding space should have similar semantic attributes. In an analogy to geographic climes—zones with distinctive meteorological features—we call areas in the embedding space where words share similar semantic features climes. Our goal is to convert neighborhoods in the embedding space into clas"
2020.emnlp-main.482,P19-1307,1,0.831817,"ultilingual transformers. 5 Related Work Cross-Lingual Word Embeddings. Ruder et al. (2019) summarize previous CLWE methods. These methods learn from existing resources such as dictionaries, parallel text, and monolingual corpora. Therefore, the availability and quality of training data primarily determines the success of these methods (Søgaard et al., 2018). To improve the suitability of CLWE methods in low-resource settings, recent work focuses on learning without cross-lingual supervision (Artetxe et al., 2018; Hoshen and Wolf, 2018) and normalizing monolingual embeddings before alignment (Zhang et al., 2019). In contrast, we design a human-in-the-loop system to efficiently improve CLWE. Moreover, previous CLWE methods are heavily tuned for the intrinsic evaluation task of dictionary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Z"
2020.emnlp-main.482,P16-1133,0,0.0243915,"ary induction, sometimes to the detriment of downstream tasks (Glavas et al., 2019; Zhang et al., 2020b). Our method is tailored for downstream tasks such as text classification. Cross-Lingual Document Classification. Prior approaches transfer knowledge with cross-lingual resources, such as bilingual dictionaries (Wu et al., 2008; Shi et al., 2010), parallel text (Xu and Yang, 2017), labeled data from related languages (Zhang et al., 2020a), structural correspondences (Peter Prettenhofer, 2010), multilingual topic models (Ni et al., 2011; Andrade et al., 2015), machine translation (Wan, 2009; Zhou et al., 2016), and CLWE (Klementiev et al., 2012). Our method instead brings a bilingual speaker in the loop to actively provide cross-lingual knowledge, which is more reliable in low-resource settings. Concurrent 5991 to our work, Karamanolakis et al. (2020) also show that keyword translation is very useful for crosslingual document classification. 6 Conclusion and Future Work is an interactive system that enhances for a task by asking a bilingual speaker for word-level similarity annotations. We test CLIME on cross-lingual information triage in international health emergencies for four low-resource langu"
2020.emnlp-main.482,W06-0605,0,\N,Missing
2020.emnlp-main.482,D14-1162,0,\N,Missing
2020.emnlp-main.482,P15-1027,0,\N,Missing
2020.emnlp-main.482,N15-1104,0,\N,Missing
2020.emnlp-main.482,N15-1028,0,\N,Missing
2020.emnlp-main.482,N16-1156,0,\N,Missing
2020.emnlp-main.482,D16-1250,0,\N,Missing
2020.emnlp-main.482,D18-1027,0,\N,Missing
2020.emnlp-main.482,D18-1042,0,\N,Missing
2020.emnlp-main.482,E14-1049,0,\N,Missing
2020.emnlp-main.637,2020.acl-main.244,0,0.0191721,"mpractical to use because the AL strategy depends on warm-starting the model with information about the task (Ash and Adams, 2019). Thus, a fitting solution to AL for deep classifiers is a cold-start approach, one that does not rely on classification loss or confidence scores. To develop a cold-start AL strategy, we should extract knowledge from pre-trained models like BERT (Devlin et al., 2019). The model encodes syntactic properties (Tenney et al., 2019), acts as a database for general world knowledge (Petroni et al., 2019; Davison et al., 2019), and can detect out-of-distribution examples (Hendrycks et al., 2020). Given the knowledge already encoded in pre-trained models, the annotation for a new task 7935 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7935–7948, c November 16–20, 2020. 2020 Association for Computational Linguistics should focus on the information missing from pretraining. If a sentence contains many words that perplex the language model, then it is possibly unusual or not well-represented in the pre-training data. Thus, the self-supervised objective serves as a surrogate for classification uncertainty. We develop ALPS (Active Learning by"
2020.emnlp-main.637,D12-1013,0,0.0293914,"ple, re-training the classifier may be costly, so users want a sampling algorithm that can query k sentences all at once. In other cases, annotators are not always available, so the number of obtainable annotations is unpredictable. Then, users would prefer an AL strategy that can query a variable number of sentences for any iteration. These cases illustrate practical needs for a cold-start algorithm like ALPS. 7 Related Work Active learning has shown success in tasks, such as named entity recognition (Shen et al., 2004), word sense disambiguation (Zhu and Hovy, 2007), and sentiment analysis (Li et al., 2012). Wang and Shang (2014) are the first to adapt prior AL work to deep learning. However, popular heuristics (Settles, 2009) for querying individual points do not work as well in a batch setting. Since then, more research has been conducted on batch AL for deep learning. Zhang et al. (2017) propose the first work on AL for neural text classification. They assume that the classifier is a convolutional neural network and use expected gradient length (Settles et al., 2008) to choose sentences that contain words with the most label-discriminative embeddings. Besides text classification, AL has been"
2020.emnlp-main.637,K18-1033,0,0.019227,"ividual points do not work as well in a batch setting. Since then, more research has been conducted on batch AL for deep learning. Zhang et al. (2017) propose the first work on AL for neural text classification. They assume that the classifier is a convolutional neural network and use expected gradient length (Settles et al., 2008) to choose sentences that contain words with the most label-discriminative embeddings. Besides text classification, AL has been applied to neural models for semantic parsing (Duong et al., 2018), named entity recognition (Shen et al., 2018), and machine translation (Liu et al., 2018). ALPS makes use of BERT , a model that excels at transfer learning. Other works also combine AL and transfer learning to select training data that reduce generalization error. Rai et al. (2010) measures domain divergence from the source domain to select the most informative texts in the target domain. Wang et al. (2014) use AL to query points for a target task through matching conditional distributions. Additionally, combining word-level and document-level annotations can improve knowledge transfer (Settles, 2011; Yuan et al., 2020). In addition to uncertainty and diversity sampling, other ar"
2020.emnlp-main.637,2021.ccl-1.108,0,0.118846,"Missing"
2020.emnlp-main.637,D19-1003,0,0.0215053,"e classifier f with weights W0 from a pre-trained encoder h and fine-tune f on D for new parameters θt . Then, the predicted classification label is yˆ = arg maxy∈Y f (x; θt )y . for Sentence Classification Assume that there is a large unlabeled dataset U = {(xi )}ni=1 of n sentences. The goal of AL is to sample a subset D ⊂ U efficiently so that fine-tuning the classifier f on subset D improves test accuracy. On each iteration t, the learner uses strategy A to acquire k sentences from dataset U and queries for their labels (Algorithm 1). Strategy A usually depends on an acquisition model Mt (Lowell and Lipton, 2019). If the strategy depends on model warm-starting, then the acquisition model Mt is f with parameters θt−1 from the previous iteration. Otherwise, we assume that Mt is the pre-trained model with parameters θ0 . After T rounds, we acquire labels for T k sentences. We provide more concrete details about AL simulation in Section 5. AL 3 The Uncertainty–Diversity Dichotomy This section provides background on prior work in AL. First, we discuss two general AL strategies: uncertainty sampling and diversity sampling. Then, we explain the dichotomy between the two concepts and introduce BADGE (Ash et a"
2020.emnlp-main.637,P11-1015,0,0.0587533,"queried per iteration. The dashed line is the test accuracy when the model is fine-tuned on the entire dataset. Overall, models trained with data sampled from ALPS have the highest test accuracy, especially for the earlier iterations. 5.2 Setup 5.3 For each sampling algorithm and dataset, we run the AL simulation five times with different random seeds. We set the maximum sequence length to 128. We fine-tune on a batch size of thirty-two for three epochs. We use AdamW (Loshchilov and Hutter, 2019) with learning rate of 2e-5, β1 = 0.9, β2 = 0.999, and a linear decay of learning rate. For IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), and AG NEWS (Zhang et al., 2015), the data encoder is the uncased BERT-Base model with 110M parameters.2 For PUBMED (Dernoncourt and Lee, 2017), the data encoder is SCIBERT, a BERT model pre-trained on scientific texts (Beltagy et al., 2019). All experiments are run on GeForce GTX 1080 GPU and 2.6 GHz AMD Opteron 4180 CPU processor; runtimes in Table 2. 2 https://huggingface.co/transformers/ Results The model fine-tuned with data sampled by ALPS has higher test accuracy than the baselines (Figure 2). For AG NEWS, IMDB, and SST-2, this is true in earlier iteration"
2020.emnlp-main.637,D19-1250,0,0.0262339,"ploy. Ideally, AL could be most useful during low-resource situations. In reality, it is impractical to use because the AL strategy depends on warm-starting the model with information about the task (Ash and Adams, 2019). Thus, a fitting solution to AL for deep classifiers is a cold-start approach, one that does not rely on classification loss or confidence scores. To develop a cold-start AL strategy, we should extract knowledge from pre-trained models like BERT (Devlin et al., 2019). The model encodes syntactic properties (Tenney et al., 2019), acts as a database for general world knowledge (Petroni et al., 2019; Davison et al., 2019), and can detect out-of-distribution examples (Hendrycks et al., 2020). Given the knowledge already encoded in pre-trained models, the annotation for a new task 7935 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7935–7948, c November 16–20, 2020. 2020 Association for Computational Linguistics should focus on the information missing from pretraining. If a sentence contains many words that perplex the language model, then it is possibly unusual or not well-represented in the pre-training data. Thus, the self-supervised object"
2020.emnlp-main.637,W10-0104,0,0.0413822,"t classification. They assume that the classifier is a convolutional neural network and use expected gradient length (Settles et al., 2008) to choose sentences that contain words with the most label-discriminative embeddings. Besides text classification, AL has been applied to neural models for semantic parsing (Duong et al., 2018), named entity recognition (Shen et al., 2018), and machine translation (Liu et al., 2018). ALPS makes use of BERT , a model that excels at transfer learning. Other works also combine AL and transfer learning to select training data that reduce generalization error. Rai et al. (2010) measures domain divergence from the source domain to select the most informative texts in the target domain. Wang et al. (2014) use AL to query points for a target task through matching conditional distributions. Additionally, combining word-level and document-level annotations can improve knowledge transfer (Settles, 2011; Yuan et al., 2020). In addition to uncertainty and diversity sampling, other areas of deep AL focus on Bayesian approaches (Siddhant and Lipton, 2018; Kirsch et al., 2019) and reinforcement learning (Fang et al., 2017). An interesting research direction can integrate one o"
2020.emnlp-main.637,2020.acl-main.240,0,0.058632,"Missing"
2020.emnlp-main.637,P04-1075,0,0.213137,"th this flexibility in sampling, ALPS can accommodate different budget constraints. For example, re-training the classifier may be costly, so users want a sampling algorithm that can query k sentences all at once. In other cases, annotators are not always available, so the number of obtainable annotations is unpredictable. Then, users would prefer an AL strategy that can query a variable number of sentences for any iteration. These cases illustrate practical needs for a cold-start algorithm like ALPS. 7 Related Work Active learning has shown success in tasks, such as named entity recognition (Shen et al., 2004), word sense disambiguation (Zhu and Hovy, 2007), and sentiment analysis (Li et al., 2012). Wang and Shang (2014) are the first to adapt prior AL work to deep learning. However, popular heuristics (Settles, 2009) for querying individual points do not work as well in a batch setting. Since then, more research has been conducted on batch AL for deep learning. Zhang et al. (2017) propose the first work on AL for neural text classification. They assume that the classifier is a convolutional neural network and use expected gradient length (Settles et al., 2008) to choose sentences that contain word"
2020.emnlp-main.637,D18-1318,0,0.0760878,"s at transfer learning. Other works also combine AL and transfer learning to select training data that reduce generalization error. Rai et al. (2010) measures domain divergence from the source domain to select the most informative texts in the target domain. Wang et al. (2014) use AL to query points for a target task through matching conditional distributions. Additionally, combining word-level and document-level annotations can improve knowledge transfer (Settles, 2011; Yuan et al., 2020). In addition to uncertainty and diversity sampling, other areas of deep AL focus on Bayesian approaches (Siddhant and Lipton, 2018; Kirsch et al., 2019) and reinforcement learning (Fang et al., 2017). An interesting research direction can integrate one of these approaches with ALPS. 8 Conclusion Transformers are powerful models that have revolutionized NLP. Nevertheless, like other deep models, their accuracy and stability require fine-tuning on large amounts of data. AL should level the playing field by directing limited annotations most effectively so that labels complement, rather than duplicate, unsupervised data. Luckily, transformers have generalized knowledge about language that can help acquire data for fine-tuni"
2020.emnlp-main.637,D13-1170,0,0.00419102,"dashed line is the test accuracy when the model is fine-tuned on the entire dataset. Overall, models trained with data sampled from ALPS have the highest test accuracy, especially for the earlier iterations. 5.2 Setup 5.3 For each sampling algorithm and dataset, we run the AL simulation five times with different random seeds. We set the maximum sequence length to 128. We fine-tune on a batch size of thirty-two for three epochs. We use AdamW (Loshchilov and Hutter, 2019) with learning rate of 2e-5, β1 = 0.9, β2 = 0.999, and a linear decay of learning rate. For IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), and AG NEWS (Zhang et al., 2015), the data encoder is the uncased BERT-Base model with 110M parameters.2 For PUBMED (Dernoncourt and Lee, 2017), the data encoder is SCIBERT, a BERT model pre-trained on scientific texts (Beltagy et al., 2019). All experiments are run on GeForce GTX 1080 GPU and 2.6 GHz AMD Opteron 4180 CPU processor; runtimes in Table 2. 2 https://huggingface.co/transformers/ Results The model fine-tuned with data sampled by ALPS has higher test accuracy than the baselines (Figure 2). For AG NEWS, IMDB, and SST-2, this is true in earlier iterations. We often see the most gain"
2020.emnlp-main.637,P19-1355,0,0.0300235,"tely train a robust model. Modern transformer models dominate the leaderboards for several NLP tasks (Devlin et al., 2019; Yang et al., 2019). Yet the price of adopting ? ∗ Work done while visiting National Taiwan University. transformer-based models is to use more data. If these models are not fine-tuned on enough examples, their accuracy drastically varies across different hyperparameter configurations (Dodge et al., 2020). Moreover, computational resources are a major drawback as training one model can cost thousands of dollars in cloud computing and hundreds of pounds in carbon emissions (Strubell et al., 2019). These problems motivate further work in AL to conserve resources. Another issue is that traditional AL algorithms, like uncertainty sampling (Lewis and Gale, 1994), falter on deep models. These strategies use model confidence scores, but neural networks are poorly calibrated (Guo et al., 2017). High confidence scores do not imply high correctness likelihood, so the sampled examples are not the most uncertain ones (Zhang et al., 2017). Plus, these strategies sample one document on each iteration. The singledocument sampling requires training the model after each query and increases the overal"
2020.emnlp-main.637,D11-1136,0,0.130921,"018), named entity recognition (Shen et al., 2018), and machine translation (Liu et al., 2018). ALPS makes use of BERT , a model that excels at transfer learning. Other works also combine AL and transfer learning to select training data that reduce generalization error. Rai et al. (2010) measures domain divergence from the source domain to select the most informative texts in the target domain. Wang et al. (2014) use AL to query points for a target task through matching conditional distributions. Additionally, combining word-level and document-level annotations can improve knowledge transfer (Settles, 2011; Yuan et al., 2020). In addition to uncertainty and diversity sampling, other areas of deep AL focus on Bayesian approaches (Siddhant and Lipton, 2018; Kirsch et al., 2019) and reinforcement learning (Fang et al., 2017). An interesting research direction can integrate one of these approaches with ALPS. 8 Conclusion Transformers are powerful models that have revolutionized NLP. Nevertheless, like other deep models, their accuracy and stability require fine-tuning on large amounts of data. AL should level the playing field by directing limited annotations most effectively so that labels complem"
2020.emnlp-main.637,2020.emnlp-main.482,1,0.875669,"Missing"
2020.emnlp-main.637,D07-1082,0,0.0740249,"modate different budget constraints. For example, re-training the classifier may be costly, so users want a sampling algorithm that can query k sentences all at once. In other cases, annotators are not always available, so the number of obtainable annotations is unpredictable. Then, users would prefer an AL strategy that can query a variable number of sentences for any iteration. These cases illustrate practical needs for a cold-start algorithm like ALPS. 7 Related Work Active learning has shown success in tasks, such as named entity recognition (Shen et al., 2004), word sense disambiguation (Zhu and Hovy, 2007), and sentiment analysis (Li et al., 2012). Wang and Shang (2014) are the first to adapt prior AL work to deep learning. However, popular heuristics (Settles, 2009) for querying individual points do not work as well in a batch setting. Since then, more research has been conducted on batch AL for deep learning. Zhang et al. (2017) propose the first work on AL for neural text classification. They assume that the classifier is a convolutional neural network and use expected gradient length (Settles et al., 2008) to choose sentences that contain words with the most label-discriminative embeddings."
2020.emnlp-main.637,C08-1143,0,0.53864,"wever, this would increase the algorithmic complexity and runtime. Alternatively, one can map the feature representations to a space where simple clustering algorithms work well. During this transformation, important information for AL must be preserved and extracted. Our approach uses the MLM head, which has already been trained on extensive corpora, to map the BERT embeddings into the surprisal embedding space. As a result, simple k-MEANS can efficiently choose representative sentences. Surprisal Clusters Prior work use k-MEANS to cluster feature representations as a cold-start AL approach (Zhu et al., 2008; Bodó et al., 2011). Rather than clustering BERT embeddings, ALPS clusters surprisal embeddings. We compare the clusters between surprisal embeddings and BERT embeddings to understand the structure of the surprisal clusters. First, we use t-SNE (Maaten and Hinton, 2008) to plot the embeddings for each sentence in the IMDB training set (Figure 4). The labels are not well-separated for both embedding sets, but the surprisal embeddings seem easier to cluster. To Single-iteration Sampling In Section 5, we sample data iteratively (Algorithm 1) to fairly compare the different AL algorithms. However"
2020.findings-emnlp.12,Q17-1010,0,0.00661971,"of the sequence model can be either at the word or character level. 3.1 BiGRU Sequence Encoder Following Yang et al. (2016), we encode input sequences using the bidirectional GRU (BiGRU).3 Given an incomplete sentence prefix x = (x1 , x2 , · · · , xl ) of length l, BiGRU takes as input the embeddings (w1 , w2 , · · · , wl ), where wi is the d-dimensional embedding vector of xi . At time Exact Prediction We follow Grissom II et al. (2016), who formulate final verb prediction as sequential classification: a 127 2 Character and word embeddings are learned from scratch, as pretrained embeddings (Bojanowski et al., 2017) did not improve prediction. 3 While it may be initially counterintuitive to use a BiGRU for an incremental task, since we make predictions at each time step independently—i.e., without consulting prior predictions—there is no need to restrict ourselves to a unidirectional model. weights (Figure 3). By assigning each hidden state its attention at , we acquire an overall representation of the sequence: v= l ∑ at ht . (4) t=1 3.3 Verb Predictor For an incomplete input prefix x, the target verb is y ∈ Y = {1, 2, . . . , K}. Based on the high-level representation v of the input sequence, we comput"
2020.findings-emnlp.12,D14-1179,0,0.0141051,"Missing"
2020.findings-emnlp.12,W97-0800,0,0.786379,"Missing"
2020.findings-emnlp.12,N16-1111,1,0.834355,"2014; Chow et al., 2018) particularly for simultaneous interpretation, where an interpreter generates a translation in real time. Instead of waiting until the entire sentence is completed, simultaneous interpretation requires translation of the source text units while the interlocutor is speaking. When human simultaneous interpreters translate from an SOV language to an SVO one incrementally—without waiting for the final verb at the end of a sentence—they must use strategies to reduce the lag, or delay, between the time they hear the source words and the time they translate them (Wilss, 1978; He et al., 2016). One strategy is final verb prediction: since the verb comes late in the source sentence but early in the target translation, if the verb is predicted in advance, it can be translated before it is heard, allowing for a more “simultaneous” (or monotonic) translation (Jörg, 1997; Bevilacqua, 2009; He et al., 2015). Furthermore, Chernov et al. (2004) argue that simultaneous interpreters’ probabilty estimates and predictions of the verbal and semantic structure of preceeding messages facilitates simultaneity in human simultaneous interpretation. 1 German is rich in both SOV and SVO sentences. It"
2020.findings-emnlp.12,D15-1006,1,0.816236,"ltaneous interpreters translate from an SOV language to an SVO one incrementally—without waiting for the final verb at the end of a sentence—they must use strategies to reduce the lag, or delay, between the time they hear the source words and the time they translate them (Wilss, 1978; He et al., 2016). One strategy is final verb prediction: since the verb comes late in the source sentence but early in the target translation, if the verb is predicted in advance, it can be translated before it is heard, allowing for a more “simultaneous” (or monotonic) translation (Jörg, 1997; Bevilacqua, 2009; He et al., 2015). Furthermore, Chernov et al. (2004) argue that simultaneous interpreters’ probabilty estimates and predictions of the verbal and semantic structure of preceeding messages facilitates simultaneity in human simultaneous interpretation. 1 German is rich in both SOV and SVO sentences. It has been argued that its underlying structure is SOV (Bach, 1962; Koster, 1975), but this is not immediately relevant to our task. Like for human translation, simultaneous machine translation (SMT), becomes more monotonic for SOV–SVO with better verb prediction (Grissom II et al., 2014; Gu et al., 2017; Alinejad"
2020.findings-emnlp.12,henrich-hinrichs-2010-gernedit,0,0.0387066,"Missing"
2020.findings-emnlp.12,N19-1423,0,0.0136448,"ers and compare the predictions to human performance. We extend that approach with a modern model that explains which cues the model uses to predict verbs. Empirical work on German verb prediction first investigated German–English simultaneous interpreters in Jörg (1997): professional interpreters often predict verbs. Matsubara et al. (2000) introduce early verb prediction into Japanese–English SMT 133 In interactive translation (Peris et al., 2017) and simultaneous translation (Alinejad et al., 2018; Ma et al., 2019) systems, neural methods for next word prediction improve translation. BERT (Devlin et al., 2019) uses masked deep bidirectional language Figure 12: Imperfect synonym-aware prediction process on a German sentence. The predicted synonym “zeigen” (show/appear) in context is not a perfect replacement for the correct verb “einsetzen” (put in place), but it better preserves the general meaning of the sentence: “This money had been made available to the country for the process of EU membership and should now appear for refugee assistance.” 8 Japanese 0.5 0.3 100 Verbs Accuracy 0.4 We present a synonym-aware neural model for incremental verb prediction using BiGRU with selfattention. It outperfo"
2020.findings-emnlp.12,goldhahn-etal-2012-building,0,0.0292276,"Missing"
2020.findings-emnlp.12,K16-1010,1,0.767119,"Missing"
2020.findings-emnlp.12,D14-1140,1,0.70511,"Missing"
2020.findings-emnlp.12,W17-3208,0,0.041532,"Missing"
2020.findings-emnlp.12,P15-1020,0,0.02531,"t structure using verb-final bˇa-construction sentences in Chinese (Chow et al., 2015, 2018). Kamide et al. (2003) find that case markers facilitate verb predictions for humans, likely because they provide clues about the semantic roles of the marked words in sentences. In sentence production, Momma et al. (2015) suggest that humans plan verbs after selecting a subject but before objects. by predicting verbs in the target language. Grissom II et al. (2014) and Gu et al. (2017) use verb prediction in the source language and learn when to trust the predictions with reinforcement learning, while Oda et al. (2015) predict syntactic constituents and do the same. Grissom II et al. (2016) predict verbs with linear classifiers and compare the predictions to human performance. We extend that approach with a modern model that explains which cues the model uses to predict verbs. Empirical work on German verb prediction first investigated German–English simultaneous interpreters in Jörg (1997): professional interpreters often predict verbs. Matsubara et al. (2000) introduce early verb prediction into Japanese–English SMT 133 In interactive translation (Peris et al., 2017) and simultaneous translation (Alinejad"
2020.findings-emnlp.12,N18-1202,0,0.0134407,"arkers correlate with improved verb prediction compared to overall verb prediction (Figure 4). Some case markers, such as to, have large jumps in accuracy toward the end, while others, such as wo do not. We examine nominative (NOM), instructive (INS), accusative (ACC), dative (DAT), quotative (QUOT), and essive (ESS) markers. This material is based upon work supported by the National Science Foundation under Grant No. 1748663 (UMD). The views expressed in this paper are our own. We thank Graham Neubig and Hal Daumé III for useful feedback. References models and contextualized representations (Peters et al., 2018) for pretraining and gain improvements in word prediction and classification. We incorporate bidirectional encoding to verb prediction. Ashkan Alinejad, Maryam Siahbani, and Anoop Sarkar. 2018. Prediction improves simultaneous neural machine translation. In Conference of Empirical Methods in Natural Language Processing, pages 3022– 3027. Existing neural attention models for sequential classification are commonly trained on complete input (Yang et al., 2016; Shen and Lee, 2016; Bahdanau et al., 2014). Classification on incomplete sequences and long-distance sentence-final verb prediction remain"
2020.findings-emnlp.12,N16-1174,0,0.0284085,"networks (RNNs), such as LSTM s (Hochreiter and Schmidhuber, 1997) and gated recurrent units (Cho et al., 2014, GRUs), can capture long-range dependencies in text, which we need for effective verb prediction. We construct an RNN-based classifier with selfattention (Lin et al., 2017) for predicting sentencefinal verbs (Figure 3). This is a natural encoding of the problem, as it explicitly models how interpreters might receive information and update their verb predictions. The hidden states of the sequence model can be either at the word or character level. 3.1 BiGRU Sequence Encoder Following Yang et al. (2016), we encode input sequences using the bidirectional GRU (BiGRU).3 Given an incomplete sentence prefix x = (x1 , x2 , · · · , xl ) of length l, BiGRU takes as input the embeddings (w1 , w2 , · · · , wl ), where wi is the d-dimensional embedding vector of xi . At time Exact Prediction We follow Grissom II et al. (2016), who formulate final verb prediction as sequential classification: a 127 2 Character and word embeddings are learned from scratch, as pretrained embeddings (Bojanowski et al., 2017) did not improve prediction. 3 While it may be initially counterintuitive to use a BiGRU for an incr"
2020.findings-emnlp.167,C16-1291,0,0.396332,"replaced by manual alignments. Induced attention refers to the base model (§4). 5.1 Supervised Attention Our annotated lexical alignments resemble our base model’s attention mechanisms. At the encoding stage, question tokens and the relevant columns are aligned (e.g., “who” ↔ column “athlete”) which should induce higher weights in both question-tocolumn and column-to-question attention (Eq. (3) and Eq. (4)); similarly, for decoding, annotation reflects which question words are most relevant to the current output token. Inspired by improvements from supervised attention in machine translation (Liu et al., 2016; Mi et al., 2016), we train the base model’s attention mechanisms to minimize the Euclidean distance5 between the human-annotated alignment vector a? and the model-generated attention vector a: Similarly, we define literal string copying from q with another bilinear scoring matrix W STR . 5 ACCLF (Dev) 1 Latt = ka − a? k2 . 2 The vector a? is a one-hot vector when the annotation aligns to a single element, or a? represents a uniform distribution over the subset in cases where the annotation aligns multiple elements. 5.2 Oracle Experiments with Manual Alignments To present the potential of ali"
2020.findings-emnlp.167,D16-1249,0,0.159887,"alignments. Induced attention refers to the base model (§4). 5.1 Supervised Attention Our annotated lexical alignments resemble our base model’s attention mechanisms. At the encoding stage, question tokens and the relevant columns are aligned (e.g., “who” ↔ column “athlete”) which should induce higher weights in both question-tocolumn and column-to-question attention (Eq. (3) and Eq. (4)); similarly, for decoding, annotation reflects which question words are most relevant to the current output token. Inspired by improvements from supervised attention in machine translation (Liu et al., 2016; Mi et al., 2016), we train the base model’s attention mechanisms to minimize the Euclidean distance5 between the human-annotated alignment vector a? and the model-generated attention vector a: Similarly, we define literal string copying from q with another bilinear scoring matrix W STR . 5 ACCLF (Dev) 1 Latt = ka − a? k2 . 2 The vector a? is a one-hot vector when the annotation aligns to a single element, or a? represents a uniform distribution over the subset in cases where the annotation aligns multiple elements. 5.2 Oracle Experiments with Manual Alignments To present the potential of alignment annotations"
2020.findings-emnlp.167,P15-1142,0,0.428845,"The table-question-answer triplets come from W IKI TABLE Q UESTIONS. We provide the logical forms as SQL plus alignments between question and logical form. In the bottom example, for instance, “the highest” ↔ ORDER BY and LIMIT 1, as indicated by both matching highlight color ( blue ) and circled-number labels ( 2 ). We address this lack by introducing S QUALL,1 the first large-scale semantic-parsing dataset with manual lexical-to-logical alignments; and we investigate the potential accuracy boosts achievable from such alignments. The starting point for S QUALL is W IKI TABLE Q UESTIONS (WTQ; Pasupat and Liang, 2015), containing data tables, English questions regarding the tables, and table-based answers. We manually enrich the 11,276-instance subset of WTQ’s training data that is translatable to SQL 1 Equal contribution; listed in alphabetical order. S QUALL =“SQL+QUestion pairs ALigned Lexically”. 1849 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1849–1864 c November 16 - 20, 2020. 2020 Association for Computational Linguistics by providing expert annotations, consisting not only of target logical forms in SQL, but also labeled alignments between the input question tokens"
2020.findings-emnlp.167,P17-1089,0,0.0288395,"dataset release. Dua et al. (2020) show that these annotator rationales improve model accuracy for a given annotation budget on machine reading comprehension. The alignments we provide could, at a stretch, be considered a type of rationale for the output SQL annotation. Text-to-SQL Datasets There is growing interest in both the database and NLP communities in text-to-SQL applications. Widely-used domainspecific datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996; Popescu et al., 2003), Restaurants (Tang and Mooney, 2000; Popescu et al., 2003), and Scholar (Iyer et al., 2017). WikiSQL (Zhong et al., 2017) is among the first large-scale datasets with questionlogical form pairs querying a wide range of data tables extracted from Wikipedia, but WikiSQL’s logical forms are generated from a limited set of templates. In contrast, WTQ questions are authored by humans under no specific constraints, and as a result WTQ includes more diverse semantics and logical operations. The family of Spider datasets (Yu et al., 2018, 2019a,b) contain queries even more complex than in WTQ, including a higher percentage of nested queries and multiple table joins. We leave extensions of l"
2020.findings-emnlp.167,P16-1003,0,0.0413284,"Missing"
2020.findings-emnlp.167,N19-1357,0,0.0233556,"e entropy of the attention distributions in the question-to-column (q2c), column-to-question (c2q) and decoder-to-question (d2q) modules, comparing models trained with supervised encoder/decoder attention, none (SEQ 2 SEQ+ ), or both strategies (ALIGN). judgments. This is an arguably surprising benefit, since the supervised decoder was not trained with q2c supervision, and so one might have expected it to perform similarly to SEQ 2 SEQ+ . However, one needs to be careful in interpreting these results, as machine-induced attention distributions are not intended for direct human interpretation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Qualitative Analysis Our additional supervision helps when the question has little textual overlap with the referred columns. Figure 3 shows an example. With finer-grained supervision, ALIGN learns the column “Serial Name” corresponds to the question word “show”, but SEQ 2 SEQ+ selects the wrong column “Co-Star”. 7 Related Work Attention and Alignments Explicit supervision for attention mechanisms (Bahdanau et al., 2015) is helpful for many tasks, including machine translation (Liu et al., 2016; Mi et al., 2016), image captioning (Liu et al., 2017), and visual qu"
2020.findings-emnlp.167,P18-1033,0,0.0521508,"Missing"
2020.findings-emnlp.167,D14-1162,0,0.0824568,"Missing"
2020.findings-emnlp.167,2020.acl-main.742,0,0.597059,"es correspond to spans in the input questions. We used our alignment to generate gold selection spans, filtering out instances where literal values could not be reconstructed through fuzzy match from the gold spans. After post-processing, S QUALL contained 11,276 table-question-answer triplets with logical form and lexical alignment annotations. (State-of-the-Art)4 Base Model: Seq2seq with Attention and Copying 4 Recent state-of-the-art text-to-SQL models extend the sequence-to-sequence (seq2seq) framework with attention and copying mechanisms (Zhong et al., 2017; Dong and Lapata, 2016, 2018; Suhr et al., 2020, inter alia). We adopt this strong neural paradigm as our base model. The seq2seq model generates one output token at a time via a probability distribution conditioned on both the input sequence representations and the partially-generated Q|y| output sequence: P (y |x) = i=1 P (yi |y<i , x), where x and y are the feature representations for the input and output sequences, and <i denotes a prefix. The last token of y must be a special <STOP> token that terminates the output generation. The per-token probability distribution is modeled through Long-Short Term Memory networks (LSTMs, Hochreiter"
2020.lrec-1.214,P18-1001,0,0.0902292,"8.41 74.06 81.80 81.11 43.56 67.22 64.00 48.46 31.56 58.31 65.07 70.81 74.38 48.28 64.54 55.00 45.03 40.14 68.49 73.13 82.47 77.19 49.82 67.37 66.65 47.22 41.68 69.36 72.32 85.27 79.77 56.34 66.13 66.70 47.69 40.19 68.6 77.40 74.63 79.75 59.39 69.66 68.91 45.69 Table 6: Spearman’s correlation on non-contextual word similarity (MaxSim). GASI-β has higher correlation on three datasets and is competitive on the others. PFT- GM is trained with two components/senses while other models learn three. A full version including MSSG is in appendix. word similarity datasets.6 Like Lee and Chen (2017) and Athiwaratkun et al. (2018), we compute the word similarity based on senses by MaxSim (Reisinger and Mooney, 2010), which maximizes the cosine similarity over the combination of all sense pairs and does not require local contexts, MaxSim(w1 , w2 ) = max 0≤i≤K,0≤j≤K cos(s1i , s2j ). (13) GASI -β has better correlation on three datasets, is competitive on the rest (Table 6), and remains competitive without scaling. GASI is better than MUSE, the other hard-attention multi-prototype model, on six datasets and worse on three. Our model can reproduce word similarities as well or better than existing models through our sense s"
2020.lrec-1.214,E09-1013,0,0.0536033,"also does well in the human evaluations (Table 2). Both GASI without scaling and MUSE fail to learn distinguishable senses and cannot disambiguate senses. High word similarities do not necessarily indicate “good” sense embeddings quality; our human evaluation—contextual word sense selection—is complementary. 6. Related Work: Representation, Evaluation Schütze (1998) introduces context-group discrimination for senses and uses the centroid of context vectors as a sense representation. Other work induces senses by context clustering (Purandare and Pedersen, 2004) or probabilistic mixture models (Brody and Lapata, 2009). Reisinger and Mooney (2010) first introduce multiple sense-specific vectors for each word, inspiring other 6 RG-65 (Rubenstein and Goodenough, 1965); SimLex999 (Hill et al., 2015); WS-353 (Finkelstein et al., 2002); MEN3k (Bruni et al., 2014); MC-30 (Miller and Charles, 1991); YP130 (Yang and Powers, 2006); MTurk-287 (Radinsky et al., 2011); MTurk-771 (Halawi et al., 2012); RW-2k (Luong et al., 2013) 7 Given how good PDF - GM is, it could do better on contextual word similarity even though it ignores senses. Average and MaxSim are equivalent for this model; it ties GASI-β. multi-prototype se"
2020.lrec-1.214,D14-1110,0,0.0268296,"al., 2014); MC-30 (Miller and Charles, 1991); YP130 (Yang and Powers, 2006); MTurk-287 (Radinsky et al., 2011); MTurk-771 (Halawi et al., 2012); RW-2k (Luong et al., 2013) 7 Given how good PDF - GM is, it could do better on contextual word similarity even though it ignores senses. Average and MaxSim are equivalent for this model; it ties GASI-β. multi-prototype sense embedding models. Generally, to address polysemy in word embeddings, previous work trains on annotated sense corpora (Iacobacci et al., 2015, Gómez-Pérez and Denaux, 2019) or external sense inventories (Labutov and Lipson, 2013, Chen et al., 2014, Jauhar et al., 2015, Chen et al., 2015, Wu and Giles, 2015, Pilehvar and Collier, 2016, Mancini et al., 2017); Rothe and Schütze (2017) extend word embeddings to lexical resources without training; others induce senses via multilingual parallel corpora (Guo et al., 2014, Šuster et al., 2016, Ettinger et al., 2016). We contrast our GASI to unsupervised monolingual multiprototype models along two dimensions: sense induction methodology and differentiability. Our focus is unsupervised induction because for interpretability to be useful, we assume that sense inventories and disambiguations are e"
2020.lrec-1.214,P15-2003,0,0.0188272,"1991); YP130 (Yang and Powers, 2006); MTurk-287 (Radinsky et al., 2011); MTurk-771 (Halawi et al., 2012); RW-2k (Luong et al., 2013) 7 Given how good PDF - GM is, it could do better on contextual word similarity even though it ignores senses. Average and MaxSim are equivalent for this model; it ties GASI-β. multi-prototype sense embedding models. Generally, to address polysemy in word embeddings, previous work trains on annotated sense corpora (Iacobacci et al., 2015, Gómez-Pérez and Denaux, 2019) or external sense inventories (Labutov and Lipson, 2013, Chen et al., 2014, Jauhar et al., 2015, Chen et al., 2015, Wu and Giles, 2015, Pilehvar and Collier, 2016, Mancini et al., 2017); Rothe and Schütze (2017) extend word embeddings to lexical resources without training; others induce senses via multilingual parallel corpora (Guo et al., 2014, Šuster et al., 2016, Ettinger et al., 2016). We contrast our GASI to unsupervised monolingual multiprototype models along two dimensions: sense induction methodology and differentiability. Our focus is unsupervised induction because for interpretability to be useful, we assume that sense inventories and disambiguations are either unavailable or imperfect. On the d"
2020.lrec-1.214,N19-1423,0,0.0234796,"ion Before we explore human interpretability of sense induction, we first describe our simple models to disentangle word senses. Our two models are built on Word2Vec (Mikolov et al., 2013a, Mikolov et al., 2013b), which we review in Section 2.1. Both models use a straightforward attention mechanism to select which sense is used in a token’s context, which we contrast to alternatives for Gumbel softmax M Marginalization P (sik |wi , c˜i ) sense attention c¯i P (cij |wi ) predict contexts M ... G ci1 si1 chemical si2 007 cim ... Computers need to represent the meaning of words in context. BERT (Devlin et al., 2019) and ELM o (Peters et al., 2018) have dramatically changed how natural language processing represents text. Rather than one-size-fits-all word vectors that ignore the nuance of how words are used in context, these new representations have topped the leaderboards for question answering, inference, and classification. Contextual representations have supplanted multisense embeddings (Camacho-Collados and Pilehvar, 2018). While these methods learn a vector for each sense, they do not encode meanings in downstream tasks as well as contextual representations (Peters et al., 2018). However, computers"
2020.lrec-1.214,P09-1002,0,0.0498459,"ling one_hot(arg maxk (log αk + gk )) by yk = softmax((log αk + gk )/τ ). Sense Attention in Objective Function Assuming a center word wi has senses {si1 , si2 , . . . , siK }, the original SkipGram likelihood becomes a marginal distribution over all senses of wi with sense induction probability P (sik |wi ); we focus on the disambiguation given local context c˜i and estimate P (sik |wi ) ≈ P (sik |wi , c˜i ); and thus, P (cij |wi ) ≈ ∑ ∑ wi ∈V cij ∈˜ ci 2.3. Why Attention? Musing on Alternatives For fine-grained sense inventories, it makes sense to have graded assignment of tokens to senses (Erk et al., 2009, Jurgens and Klapaftis, 2015). However, for coarse senses— except for humor (Miller et al., 2017)—words typically are associated with a single sense, often a single sense per discourse (Gale et al., 1992). A good model should respect this. Previous models either use non-differentiable objectives or—in the case of the current state of the art, MUSE (Lee and Chen, 2017)—reinforcement learning to select word senses. By using Gumbel softmax, our model both approximates discrete sense selection and is differentiable. As we argue in the next section, applications with a human in the loop are best a"
2020.lrec-1.214,J13-3003,0,0.150267,"ning, matter, regarding, debated, legality Table 4: A case where MSSG has low overlap but confuses raters (agreement 0.33); the model chooses s1. mans’; its selections have the highest accuracy and assigns the largest probability assigned to the human choices (Table 2). Thus, GASI-β produces sense embeddings that are both more interpretable and distinguishable. GASI without a scaling factor, however, has low consistency and flat sense distribution. Model Confidence However, some contexts are more ambiguous than others. For fine-grained senses, best practice is to use graded sense assignments (Erk et al., 2013). Thus, we also show the model’s probability of the top human choice; 1 distributions close to K (0.33) suggest the model learns a distribution that cannot disambiguate senses. We consider granularity of senses further in Section 6. 1730 Inter-rater Agreement We use the confidence score computed by Figure-Eight to estimate the raters’ agreement for this task. GASI-β has the highest human-model agreement, while both M USE and GASI without scaling have the lowest. Error Analysis Next, we explore why crowdworkers disagree with the model even though the senses are interpretable (Table 1). Is it th"
2020.lrec-1.214,N16-1163,0,0.0221027,"lent for this model; it ties GASI-β. multi-prototype sense embedding models. Generally, to address polysemy in word embeddings, previous work trains on annotated sense corpora (Iacobacci et al., 2015, Gómez-Pérez and Denaux, 2019) or external sense inventories (Labutov and Lipson, 2013, Chen et al., 2014, Jauhar et al., 2015, Chen et al., 2015, Wu and Giles, 2015, Pilehvar and Collier, 2016, Mancini et al., 2017); Rothe and Schütze (2017) extend word embeddings to lexical resources without training; others induce senses via multilingual parallel corpora (Guo et al., 2014, Šuster et al., 2016, Ettinger et al., 2016). We contrast our GASI to unsupervised monolingual multiprototype models along two dimensions: sense induction methodology and differentiability. Our focus is unsupervised induction because for interpretability to be useful, we assume that sense inventories and disambiguations are either unavailable or imperfect. On the dimension of sense induction methodology, Huang et al. (2012) and Neelakantan et al. (2014) induce senses by context clustering; Tian et al. (2014) model a corpuslevel sense distribution; Li and Jurafsky (2015) model the sense assignment as a Chinese Restaurant Process; Qiu et"
2020.lrec-1.214,K17-1012,0,0.113263,"urther compare GASI-β with previous SOTA, MSSG -30 K and MUSE, on the Word in Context dataset (Pilehvar and Camacho-Collados, 2019, W i C ) which requires the model to identify whether a word has the same sense in two contexts. To reduce the variance in training and to focus on evaluating the sense selection module, we use an evaluation suited for unsupervised models: if the model selects different sense vectors given contexts, we mark that the word has different senses.5 For MUSE, MSSG and GASI-β, we use each model’s sense selection module; for DeConf (Pilehvar and Collier, 2016) and SW 2 V (Mancini et al., 2017), we follow Pilehvar and Camacho-Collados (2019) and Pelevina et al. (2016) by selecting the closest sense vectors to the context vector. DeConf results are comparable to supervised results (59.4± 0.7). GASI-β has the best result (55.3) apart from DeConf itself (58.55)(full results in Table 8 in appendix), which uses the same sense inventory (Miller and Fellbaum, 1998, WordNet) as WiC. Non-Contextual Word Similarity While contextual word similarity is best suited for our model and goals, other datasets without contexts (i.e., only word pairs and a rating) are both larger and ubiquitous for wor"
2020.lrec-1.214,D14-1113,0,0.53467,"2, and Table 5). Our final objective function for GASI-β is J(S, C) ∝ K ∑ ∑ ∑ wi ∈V wc ∈ci k=1 Model Sense Accuracy Judgment Accuracy Agreement 67.33 69.33 71.33 62.89 66.67 67.33 0.73 0.76 0.77 MUSE MSSG -30 K GASI -β γki log P (wc |sik ). (12) 3. Data and Training For fair comparisons, we try to remain consistent with previous work (Huang et al., 2012, Neelakantan et al., 2014, Lee and Chen, 2017) in all aspects of training. In particular, we train GASI on the same April 2010 Wikipedia snapshot (Shaoul and Westbury, 2010) with 1B tokens and the same vocabulary released by Neelakantan et al. (2014); set the number of senses K = 3 and dimension d = 300 for each word unless otherwise specified. More details are in the Appendix. Following Maddison et al. (2017), we fix the temperature τ = 0.5, and tune the scaling factor β = 0.4 using grid search within {0.1 . . . 0.9} on AvgSimC for contextual word similarity (Section 5); this tuning preceded all interpretability experiments. If not reprinted, numbers for competing models are either computed with pre-trained embeddings released by authors or trained on released code. 4. Evaluating Interpretability We turn to traditional evaluations of sen"
2020.lrec-1.214,S17-1015,0,0.01873,"vised induction because for interpretability to be useful, we assume that sense inventories and disambiguations are either unavailable or imperfect. On the dimension of sense induction methodology, Huang et al. (2012) and Neelakantan et al. (2014) induce senses by context clustering; Tian et al. (2014) model a corpuslevel sense distribution; Li and Jurafsky (2015) model the sense assignment as a Chinese Restaurant Process; Qiu et al. (2016) induce senses by minimizing an energy function on a context-depend network; Bartunov et al. (2016) model the sense assignment as a steak-breaking process; Nguyen et al. (2017) model the sense embeddings as a weighted combination of topic vectors with pre-computed weights by topic models; Athiwaratkun et al. (2018) model word representations as Gaussian Mixture embeddings where each Gaussian component captures different senses; Lee and Chen (2017) compute sense distribution by a separate set of sense induction vectors. The proposed GASI marginalizes the likelihood of contexts over senses and induces senses by local context vectors; the most similar sense selection module is a bilingual model (Šuster et al., 2016) except that it does not introduce lower bound for neg"
2021.acl-long.346,J08-4004,0,0.195718,"atistics in Appendix Figure 11. Identifying Annotation Error To test if IRT can identify annotation error, we inspect sixty SQuAD development set items. We select ten items from each of these groups: the most negative discriminability, discriminability nearest to zero, the highest discriminability, the least difficult, most difficult, and IRT model errors. For each, we annotate whether the item was correct, was “correct” yet flawed in some way, or simply wrong (Figure 7). 15 Inter-annotator agreement between three authors on this three-way annotation with Krippendorff’s α (Krippendorff, 2004; Artstein and Poesio, 2008) is 0.344. Despite only modest agreement, just as in the development of education tests, negative discriminability is predictive of bad items. When discriminability is negative, then the probability of getting the answer right is higher when ability is lower, which is undesirable: Ken consistently loses to Burt on those items. This could identify bad items in evaluation sets for removal. 6 Related Work draws together two primary threads: we use to understand datasets, which has been applied to other NLP tasks, and apply it to improving leaderboards. Finally, we explore how the insights of IRT"
2021.acl-long.346,2020.acl-main.662,1,0.935331,"feasibility of examples. Negative discriminability suggests an annotation error; for example, the question with most negative discriminability asks “Why did demand for rentals decrease?” when the answer is “demand for higher quality housing increased.” In this work we take leaderboards “as they are,” and imagine how they might better support research. Leaderboards establish differences between models on a fixed task. Hence, leaderboards should enable and encourage the comparison of models and inspection of examples. And leaderboards should also signal when they have outlived their usefulness (Boyd-Graber and Börschinger, 2020). 1.1 How to Direct Leaderboards’ Light To help focus attention on examples and models of interest, we propose Difficulty and Ability Discriminating (DAD) leaderboards that explicitly model both task and submissions jointly, rather than either in isolation. 1 DAD’s underlying model is based on 1 Work completed at University of Maryland. Source code, data, and visualizations at irt.pedro.ai. 4486 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4486–4503 August 1–6, 2021. ©20"
2021.acl-long.346,P18-1189,0,0.0167732,"bano (2016) notes that sampling mutually exclusive subsets has drawbacks—samples are not entirely independent. Lastly, our work is a proof of concept using SQuAD 2.0 as a test bed and our results may not generalize. 8 Future Work We see a few directions for future work. First, this paper is intended to validate IRT and its usefulness as an active part of the leaderboard lifecycle; the natural next step is to implement it in a leaderboard. Second, our IRT models do not incorporate the item content (e.g., example text) to predict responses, but in principle could; Bayesian models with metadata (Card et al., 2018) and ideal point models from political science (Poole and Rosenthal, 1985) that incorporate bills and speeches do exactly this (Gerrish and Blei, 2011; Nguyen et al., 2015; Kraft et al., 2016). Analogously, IRT for leaderboards can and should also incorporate text from passages, questions, and answers to better model what makes questions difficult. Such a model can also predict which characteristics would create discriminating or difficult items. Lastly, multidimensional IRT models to evaluate multiple skills could aid multitask or multi-metric leaderboards like MRQA (Fisch et al., 2019) and D"
2021.acl-long.346,D19-1224,0,0.0504347,"when training data are down-sampled (Lalor et al., 2019). Improving Leaderboards The rise NLP leaderboards has encouraged critical thought into improving them (Linzen, 2020), improving evaluation more broadly (Eger et al., 2020), and thoughtful consideration of their influence on the direction of research (Sculley et al., 2018; Dotan and Milli, 2020). DAD aims make leaderboard yardsticks (Hernandez-Orallo, 2020) more reliable, interpretable, and part of curating the benchmark itself. In line with our reliability goal, just as statistical tests should appear in publications (Dror et al., 2018; Dodge et al., 2019), they should be “freebies” for leaderboard participants (Ethayarajh and Jurafsky, 2020). Alternatively, Hou et al. (2019) posit that leaderboards could be automatically extracted from publications. How to aggregate multi-task benchmarks (Wang et al., 2019b,a; Fisch et al., 2019) and multi-metric benchmarks (Ma et al., 2021) is an open question which—although we do not address—is one use for IRT. This work implicitly argues that leaderboards should be continually updated. As a (static) leaderboard ages, the task(s) overfit (Recht et al., 2019) which—although mitigable (Blum and Hardt, 2015; An"
2021.acl-long.346,D19-5801,1,0.919968,"the direction of research (Sculley et al., 2018; Dotan and Milli, 2020). DAD aims make leaderboard yardsticks (Hernandez-Orallo, 2020) more reliable, interpretable, and part of curating the benchmark itself. In line with our reliability goal, just as statistical tests should appear in publications (Dror et al., 2018; Dodge et al., 2019), they should be “freebies” for leaderboard participants (Ethayarajh and Jurafsky, 2020). Alternatively, Hou et al. (2019) posit that leaderboards could be automatically extracted from publications. How to aggregate multi-task benchmarks (Wang et al., 2019b,a; Fisch et al., 2019) and multi-metric benchmarks (Ma et al., 2021) is an open question which—although we do not address—is one use for IRT. This work implicitly argues that leaderboards should be continually updated. As a (static) leaderboard ages, the task(s) overfit (Recht et al., 2019) which—although mitigable (Blum and Hardt, 2015; Anderson-Cook et al., 2019)—is best solved by continually collecting new data (Kiela et al., 2021). Ideally, new data should challenge models through adversarial collection (Wallace et al., 2019b; Nie et al., 2020) and related methods (Gardner et al., 2020). However, if making an e"
2021.emnlp-main.444,P17-1147,0,0.0792265,"Missing"
2021.emnlp-main.444,C10-2064,0,0.0379173,"Missing"
2021.emnlp-main.444,P19-1612,0,0.0172148,"tions are about the UK. NQ has the highest coverage of women through its focus on entertainment (Film/TV, Music and Sports). 3 What Questions can QA Answer? datasets have different representations of demographic characteristics; is this focus benign or do these differences carry through to model accuracy? We analyze a SOTA system for each of our four tasks. For NQ and SQuAD, we use a fine-tuned BERT (Alberti et al., 2019) with curated training data (e.g., downsample questions without answers and split documents into multiple training instances). For the open-domain TriviaQA task, we use ORQA (Lee et al., 2019) with a BERT-based reader and retriever components. Finally, for QB, we use the competition winner from Wallace et al. (2019), a BERT-based reranker of a TF - IDF retriever. Accuracy (exact-match) and average F1 are both common QA metrics (Rajpurkar et al., 2016). Since both are related and some statistical tests require binary scores, we focus on exact-match. 2.2 Who is in Questions? Rather than focus on aggregate accuracy, we foOur demographic analysis reveals skews in all cus on demographic subsets’ accuracy (Figure 1). datasets, reflecting differences in task focus (TaFor instance, while 6"
2021.emnlp-main.444,2020.acl-main.653,0,0.0163523,"es. While other work (Section 4) focuses on demographic representation in NLP resources, our focus is how well QA models generalize across demographic subsets. After mapping mentions to a knowledge base (Section 2), we show existing QA datasets lack diversity in the gender and national origin of the people mentioned: English-language QA datasets mostly ask about US men from a few professions (Section 2.2). This is problematic because most English speakers (and users of English QA systems) are not from the US or UK. Moreover, multilingual QA datasets are often translated from English datasets (Lewis et al., 2020; Artetxe et al., 2019). However, no work has verified that QA systems generalize to infrequent demographic groups. Section 3 investigates whether statistical tests reveal patterns on demographic subgroups. Despite skewed distributions, accuracy is not correlated with gender or nationality, though it is with Work completed while at Google Research SQ u AD QB Trivia QA Dev Train Dev Train Dev Train Dev 32.27 22.61 27.82 16.19 2.70 18.05 8.89 1.06 2631 58.49 15.88 34.50 27.98 9.83 20.37 8.87 0.88 112927 57.18 20.13 35.11 28.16 9.88 17.78 8.75 0.63 2216 14.56 37.27 33.52 3.15 5.08 21.34 3.16 1.39"
2021.emnlp-main.444,2020.emnlp-main.466,0,0.0367927,"Missing"
2021.emnlp-main.444,D17-1247,0,0.0132657,"ky et al., 2019), this is surprising. However, we caution against accepting this conclusion without further investigation given the strong correlation of gender with professional field (Goulden et al., 2011), where we do see significant effects. Taken together, the χ2 and logistic regression analysis give us reason to be optimistic: although data are skewed for all subsets, QA systems might well generalize from limited training data across gender and nationality. 4 Related Work Language is a reflection of culture. Like other cultural artifacts—encyclopedias (Reagle and Rhue, 2011), and films (Sap et al., 2017)—QA has more men than women. Other artifacts like children’s books have more gender balance but reflect other aspects of culture (Larrick, 1965). The NLP literature is also grappling with demographic discrepancies. Standard coreference systems falter on gender-balanced corpora (Webster et al., 2018), and Zhao et al. (2018) create synthetic training data to reduce bias. Similar coreference issues plague machine translation systems (Stanovsky et al., 2019), and Li et al. (2020) use QA to probe biases of NLP systems. Sen and Saffari (2020) show that there are shortcomings in QA datasets and evalu"
2021.emnlp-main.444,2020.emnlp-main.190,0,0.0213973,"al artifacts—encyclopedias (Reagle and Rhue, 2011), and films (Sap et al., 2017)—QA has more men than women. Other artifacts like children’s books have more gender balance but reflect other aspects of culture (Larrick, 1965). The NLP literature is also grappling with demographic discrepancies. Standard coreference systems falter on gender-balanced corpora (Webster et al., 2018), and Zhao et al. (2018) create synthetic training data to reduce bias. Similar coreference issues plague machine translation systems (Stanovsky et al., 2019), and Li et al. (2020) use QA to probe biases of NLP systems. Sen and Saffari (2020) show that there are shortcomings in QA datasets and evaluations by analysing their out-of-domain generalization capabilities and ability to handle question variation. Joint models of vision and language suggest that biases come from language, rather than from vision (Ross et al., 2021). However, despite a range of mitigation techniques (Zhao et al., 2017, inter alia) none, to our knowledge, have been successfully applied to QA, especially from the demographic viewpoint. Recall that logistic regression uses features to predict whether the QA system will get the answer right or not. Features as"
2021.emnlp-main.444,2021.emnlp-main.757,1,0.828513,"Missing"
2021.emnlp-main.444,P19-1164,0,0.0130292,"(Fahrmeir et al., 2007) estimate p-values. Although we initially use quadratic features they are all eliminated during feature reduction. Thus, we only report the linear features with a minimal significance (p-value &lt; 0.1). 3.3 How do Properties Affect Accuracy? “Who says that which we call a rose?” A: Juliet, A: William Shakespeare. For male and female genders, NQ has no statistically significant effect on accuracy, only questions about entities with multiple genders depresses accuracy. Given the many findings of gender bias in NLU (Zhao et al., 2017; Webster et al., 2018; Zhao et al., 2018; Stanovsky et al., 2019), this is surprising. However, we caution against accepting this conclusion without further investigation given the strong correlation of gender with professional field (Goulden et al., 2011), where we do see significant effects. Taken together, the χ2 and logistic regression analysis give us reason to be optimistic: although data are skewed for all subsets, QA systems might well generalize from limited training data across gender and nationality. 4 Related Work Language is a reflection of culture. Like other cultural artifacts—encyclopedias (Reagle and Rhue, 2011), and films (Sap et al., 2017"
2021.emnlp-main.756,P17-1171,0,0.0199906,"ey use standard IR techniques to find relevant passages and match them with answer strings. These methods succeed for simple questions where terms overlap with evidence passages, This no longer holds for multi-hop questions that require a reasoning chain as evidence to the answer: evidence pieces do not overlap with the question but rather depend on the previous evidence pieces. Unsupervised IR methods cannot capture such implicit relations. D IST DR removes the burden of little textual overlap through dense retrieval and its iterative process retrieves better evidence. Open-domain QA systems Chen et al. (2017) first combine information retrieval and (neural) reading comprehension for open-domain QA. Several works aim to improve the neural reader (Clark and Gardner, 2018; Wang et al., 2018, inter alia), or use generative models to compose an answer (Lewis et al., 2020; Izacard and Grave, 2021, inter alia). Recent progress (Karpukhin et al., 2020; Xiong et al., 2021a,b, inter alia) uses dense retrieval to aid both single-hop and multi-hop questions. However, a crucial distinction is that these approaches assume the evidence is given for training, while D IST DR iteratively finds evidence and uses it"
2021.emnlp-main.756,2020.acl-main.501,0,0.0136931,"ence pieces), building a reasoning chain to find the answer. Our work focuses on training ODQA systems 9612 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9612–9622 c November 7–11, 2021. 2021 Association for Computational Linguistics without these expensive annotations (Section 2): we only start with a question-answer pair. With that starting point, we use distant supervision to infer which evidence helps us get to the answer. The technical challenge is how to find these evidence from millions of candidates. Previous methods (Joshi et al., 2017; Cheng et al., 2020) use term matching (e.g., TF - IDF) for evidence retrieval, but their goal is a single piece of evidence: linking a question to a passage. As shown in Figure 1, the key to finding some evidence pieces does not appear in the question: for example, you only know to figure out that Rutgers University is in New Jersey after learning where Professor Cheong works. Fortunately, navigating to an answer given a question from a search engine is not impossible: humans do it every day, building on their existing knowledge toward the answer (Russell, 2019). With each round of searching to find additional c"
2021.emnlp-main.756,P19-1612,0,0.0172435,"relevant to question. Returning to our running example, while “named after a lawn ornament store in Wayne, New Jersey” has the state where Professor Cheong works, it is irrelevant to condensed matter physics. Thus, a reader filters spurious evidence (Reader Filter) to keep D IST DR on target. 3.1 Preliminary: Fully-Supervised ODQA This section reviews state-of-the-art systems for fully-supervised ODQA, where a dense retriever finds evidence from a large corpus, and a reader— multi-tasked with evidence reranking and span extraction—outputs a span as the answer. Dense Retrieval Dense retrieval (Lee et al., 2019) is based on a dual-encoder architecture, which uses BERT to represent both the query q and the passage p with dense vectors. The model learns a scoring function (e.g., dot product) between question and passage vectors: f (q, p) = sim(EncQ (q), EncP (p)). qt = [q; z1 ; . . . ; zt−1 ], (2) and retrieve a new evidence piece zt . During inference, a beam search finds the top-k evidence, where the score is the product of individual evidence pieces’ score. For training, given positive evidence z + ≡ z1+ , . . . , zn+ (In Figure 1, z1+ : Sang-Wook Cheong, z2+ : Rutgers University) and a set of negat"
2021.emnlp-main.756,N19-1246,0,0.123739,"wrong evidence, often because the extracted evidence only includes one span that matches the answer type, therefore the reader confidently outputs the span for the wrong reason. In the third example in Table 6, Nassau County is the only county it sees, and therefore the model has stumbled upon the right answer erroneously. Building a model with faithful predictions is an important ongoing research topic (Jacovi and Goldberg, 2020). 6 Related Work Question Answering Datasets There is growing interest in NLP communities to build large-scale datasets (Rajpurkar et al., 2016; Jia and Liang, 2017; Dua et al., 2019, inter alia) for QA research. In addition to questions and answers, benchmark datasets often include annotated evidence, but it requires significant annotation protocol design and human annotations. In SQUAD (Rajpurkar et al., 2016), among the first large-scale reading comprehension datasets, annotators write questions conditioned on a passage, which creates dataset artifacts (Jia and Liang, 2017). To overcome dataset artifacts, NATURAL Q UESTIONS (Kwiatkowski et al., 2019) use real Google queries as questions and ask annotators to label both evidence passages and short answers. But such anno"
2021.emnlp-main.756,D18-1167,0,0.052637,"Missing"
2021.emnlp-main.756,D18-1134,1,0.853519,"QA, annotators are presented with a linked Wikipedia passages, as pilot studies indicate that it is difficult to ask a meaningful multi-hop question with arbitrary passages. However, some questions in H OT P OT QA include shortcuts that are answerable by a single passage (Min et al., 2019b), which is confirmed by our analysis (Section 5.2). Distant supervision has been successfully adopted for many NLP tasks such as relation extraction (Mintz et al., 2009). Recent work builds QA datasets with distant supervision, such as T RIVIAQA (Joshi et al., 2017), S EARCH QA (Dunn et al., 2017), QBL INK (Elgohary et al., 2018) by automatically gathering evidence documents from a corpus as distant supervision for available questionanswer pairs. They use standard IR techniques to find relevant passages and match them with answer strings. These methods succeed for simple questions where terms overlap with evidence passages, This no longer holds for multi-hop questions that require a reasoning chain as evidence to the answer: evidence pieces do not overlap with the question but rather depend on the previous evidence pieces. Unsupervised IR methods cannot capture such implicit relations. D IST DR removes the burden of l"
2021.emnlp-main.756,2021.eacl-main.74,0,0.0325608,"evidence pieces do not overlap with the question but rather depend on the previous evidence pieces. Unsupervised IR methods cannot capture such implicit relations. D IST DR removes the burden of little textual overlap through dense retrieval and its iterative process retrieves better evidence. Open-domain QA systems Chen et al. (2017) first combine information retrieval and (neural) reading comprehension for open-domain QA. Several works aim to improve the neural reader (Clark and Gardner, 2018; Wang et al., 2018, inter alia), or use generative models to compose an answer (Lewis et al., 2020; Izacard and Grave, 2021, inter alia). Recent progress (Karpukhin et al., 2020; Xiong et al., 2021a,b, inter alia) uses dense retrieval to aid both single-hop and multi-hop questions. However, a crucial distinction is that these approaches assume the evidence is given for training, while D IST DR iteratively finds evidence and uses it to improve the model. Min et al. (2019a) also use hard-EM for weakly-supervised QA, but— orthogonal to our approach—they assume the evidence is given and find the most likely answer mentions in the evidence, while we aim to find evidence from a large corpus. 7 Conclusion atively finding"
2021.emnlp-main.756,2020.acl-main.386,0,0.0208478,"Missing"
2021.emnlp-main.756,D19-1284,0,0.120894,"l of the intermediate evidence pieces (e.g., in Figure 1, the evidence pieces for Sang-Wook Cheong’s workplace which point you to Rutgers University’s location) needed for the answer. Creating such intricate training data is expensive. For example, Kwiatkowski et al. (2019) use additional experts to justify the correctness of annotated evidence. The annotation protocol is even more nuanced for multi-hop questions. For example, Yang et al. (2018) ask annotators to write multi-hop questions based on two linked Wikipedia passages as a pre-defined reasoning chain, which creates dataset artifacts (Min et al., 2019b). While plenty of question-answer pairs are available without evidence labels, we cannot directly train SOTA models on such data. Open-domain question answering (ODQA) takes a question, retrieves evidence from a large corpus, and finds an answer based on that evidence (Voorhees et al., 1999). With the help of large scale datasets, state-of-the-art approaches to QA (Karpukhin et al., 2020; Zhao et al., 2021, inter alia) can answer both simple questions that require only a single evidence piece (i.e., one passage); and more challenging multi-hop questions: computers must jump or “hop” from pas"
2021.emnlp-main.756,P19-1416,0,0.209437,"l of the intermediate evidence pieces (e.g., in Figure 1, the evidence pieces for Sang-Wook Cheong’s workplace which point you to Rutgers University’s location) needed for the answer. Creating such intricate training data is expensive. For example, Kwiatkowski et al. (2019) use additional experts to justify the correctness of annotated evidence. The annotation protocol is even more nuanced for multi-hop questions. For example, Yang et al. (2018) ask annotators to write multi-hop questions based on two linked Wikipedia passages as a pre-defined reasoning chain, which creates dataset artifacts (Min et al., 2019b). While plenty of question-answer pairs are available without evidence labels, we cannot directly train SOTA models on such data. Open-domain question answering (ODQA) takes a question, retrieves evidence from a large corpus, and finds an answer based on that evidence (Voorhees et al., 1999). With the help of large scale datasets, state-of-the-art approaches to QA (Karpukhin et al., 2020; Zhao et al., 2021, inter alia) can answer both simple questions that require only a single evidence piece (i.e., one passage); and more challenging multi-hop questions: computers must jump or “hop” from pas"
2021.emnlp-main.756,P09-1113,0,0.158318,"ly question–answer pairs, which is significantly cheaper. Annotation is more fraught for multi-hop QA 9619 datasets (Yang et al., 2018). To construct H OTP OT QA, annotators are presented with a linked Wikipedia passages, as pilot studies indicate that it is difficult to ask a meaningful multi-hop question with arbitrary passages. However, some questions in H OT P OT QA include shortcuts that are answerable by a single passage (Min et al., 2019b), which is confirmed by our analysis (Section 5.2). Distant supervision has been successfully adopted for many NLP tasks such as relation extraction (Mintz et al., 2009). Recent work builds QA datasets with distant supervision, such as T RIVIAQA (Joshi et al., 2017), S EARCH QA (Dunn et al., 2017), QBL INK (Elgohary et al., 2018) by automatically gathering evidence documents from a corpus as distant supervision for available questionanswer pairs. They use standard IR techniques to find relevant passages and match them with answer strings. These methods succeed for simple questions where terms overlap with evidence passages, This no longer holds for multi-hop questions that require a reasoning chain as evidence to the answer: evidence pieces do not overlap wit"
2021.emnlp-main.756,D17-1215,0,0.0144287,", D IST DR finds the wrong evidence, often because the extracted evidence only includes one span that matches the answer type, therefore the reader confidently outputs the span for the wrong reason. In the third example in Table 6, Nassau County is the only county it sees, and therefore the model has stumbled upon the right answer erroneously. Building a model with faithful predictions is an important ongoing research topic (Jacovi and Goldberg, 2020). 6 Related Work Question Answering Datasets There is growing interest in NLP communities to build large-scale datasets (Rajpurkar et al., 2016; Jia and Liang, 2017; Dua et al., 2019, inter alia) for QA research. In addition to questions and answers, benchmark datasets often include annotated evidence, but it requires significant annotation protocol design and human annotations. In SQUAD (Rajpurkar et al., 2016), among the first large-scale reading comprehension datasets, annotators write questions conditioned on a passage, which creates dataset artifacts (Jia and Liang, 2017). To overcome dataset artifacts, NATURAL Q UESTIONS (Kwiatkowski et al., 2019) use real Google queries as questions and ask annotators to label both evidence passages and short answ"
2021.emnlp-main.756,2020.emnlp-main.88,0,0.0155452,"us, and using the evidence as distant supervision for model training. Without using any evidence labels, D IST DR matches the fully-supervised SOTA approaches on both multi-hop and single-hop QA benchmarks. Annotating evidence for existing questionanswer pairs is generally expensive, especially for complex questions. While D IST DR can accurately find evidence for arbitrary complex machine reading-style questions, future work needs to validate whether this can work for other types of questions. This could improve the reader to answer numerical reasoning (Dua et al., 2019), temporal reasoning (Ning et al., 2020), multi-model reasoning (Lei et al., 2018), or combination of these skills (Bartolo et al., 2020). Acknowledgments We thank CLIP members, Tianze Shi, anonymous reviewers and meta-reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the BETTER Program contract 201919051600005. Boyd-Graber is supported by NSF Grant IIS-1822494. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view o"
2021.emnlp-main.756,P17-1147,0,0.109744,"these passages evidence pieces), building a reasoning chain to find the answer. Our work focuses on training ODQA systems 9612 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9612–9622 c November 7–11, 2021. 2021 Association for Computational Linguistics without these expensive annotations (Section 2): we only start with a question-answer pair. With that starting point, we use distant supervision to infer which evidence helps us get to the answer. The technical challenge is how to find these evidence from millions of candidates. Previous methods (Joshi et al., 2017; Cheng et al., 2020) use term matching (e.g., TF - IDF) for evidence retrieval, but their goal is a single piece of evidence: linking a question to a passage. As shown in Figure 1, the key to finding some evidence pieces does not appear in the question: for example, you only know to figure out that Rutgers University is in New Jersey after learning where Professor Cheong works. Fortunately, navigating to an answer given a question from a search engine is not impossible: humans do it every day, building on their existing knowledge toward the answer (Russell, 2019). With each round of searching"
2021.emnlp-main.756,D16-1264,0,0.113201,"Missing"
2021.emnlp-main.756,N12-1087,0,0.0153616,"P (z |q) (For single-hop questions z is retrieved with single step; for multi-hop questions it takes multiple steps to find z: after the first retrieval step the query q is not just the original question bu a so appended ev dence as n Equa on 2 ) and selecting an answer from the question P (a |q, z) We use expec a on max m za on (EM) o nfer he latent variable z We compute the likelihood of each z given q yielding a vector of estimates zˆ (Es ep) hen upda e he mode parame ers based on zˆ (M-s ep) S nce ’s n rac ab e o enumera e a evidence candidates to compute the expectation we adopt hard-EM (Samdani et al 2012) and approxima e he E-s ep by p ck ng he mos ke y so u on as zˆ We pass over all questions in the training set and repeat this process for multiple iterations until the model converges retrieval steps for multi-hop questions specifically at each step t: zˆt = arg max P (ˆ zt |q, zˆ1 , ..., zˆt−1 ); zˆ∈Z (7) and single-step retrieval for single-hop questions We use an up-to-date retriever to find the top-k evidence from corpus (with beam search for the k-highest scoring chains Section 3 1) Given the retrieval output we will eventually need o re ra n he re r ever Th s requ res know ng which evid"
2021.emnlp-main.756,D18-1259,0,0.3683,"te-of-the-art systems use evidence labels for training, but acquiring labeled evidence pieces is expensive. State-of-the-art (SOTA) methods, however, are trained with all of the intermediate evidence pieces (e.g., in Figure 1, the evidence pieces for Sang-Wook Cheong’s workplace which point you to Rutgers University’s location) needed for the answer. Creating such intricate training data is expensive. For example, Kwiatkowski et al. (2019) use additional experts to justify the correctness of annotated evidence. The annotation protocol is even more nuanced for multi-hop questions. For example, Yang et al. (2018) ask annotators to write multi-hop questions based on two linked Wikipedia passages as a pre-defined reasoning chain, which creates dataset artifacts (Min et al., 2019b). While plenty of question-answer pairs are available without evidence labels, we cannot directly train SOTA models on such data. Open-domain question answering (ODQA) takes a question, retrieves evidence from a large corpus, and finds an answer based on that evidence (Voorhees et al., 1999). With the help of large scale datasets, state-of-the-art approaches to QA (Karpukhin et al., 2020; Zhao et al., 2021, inter alia) can answ"
2021.emnlp-main.756,2021.naacl-main.368,1,0.856608,"Missing"
2021.emnlp-main.757,2020.acl-main.662,1,0.861398,"Missing"
2021.emnlp-main.757,D19-5817,0,0.0181703,"lacking (e.g., our alias expansion obtains many more answers than NQ’s original multi-way annotation). AmbigQA (Min et al., 2020) aims to address the problem of ambiguous questions, where there are multiple interpretations of the same question and therefore multiple correct answer classes (which could in turn have many valid aliases for each class). We provide an orthogonal view as we are trying to expand equivalent answers to any given gold answer while AmbigQA aims to cover semantically different but valid answers. Evaluation of QA Models. There are other attempts to improve QA evaluation. Chen et al. (2019) find that current automatic metrics do not correlate well with human judgements, which motivated Chen et al. (2020) to construct a dataset with human annotated scores of candidate answers and use it to train a BERT-based regression model as the scorer. Feng and Boyd-Graber (2019) argue for instead of evaluating QA systems directly, we should instead evaluate downstream human accuracy when using QA output. Alternatively, Risch et al. (2021) use a cross-encoder to measure the semantic similarity between predictions and gold answers. For the visual QA task, Luo et al. (2021) incorporate alias an"
2021.emnlp-main.757,2020.emnlp-main.528,0,0.0327533,"et al., 2020) aims to address the problem of ambiguous questions, where there are multiple interpretations of the same question and therefore multiple correct answer classes (which could in turn have many valid aliases for each class). We provide an orthogonal view as we are trying to expand equivalent answers to any given gold answer while AmbigQA aims to cover semantically different but valid answers. Evaluation of QA Models. There are other attempts to improve QA evaluation. Chen et al. (2019) find that current automatic metrics do not correlate well with human judgements, which motivated Chen et al. (2020) to construct a dataset with human annotated scores of candidate answers and use it to train a BERT-based regression model as the scorer. Feng and Boyd-Graber (2019) argue for instead of evaluating QA systems directly, we should instead evaluate downstream human accuracy when using QA output. Alternatively, Risch et al. (2021) use a cross-encoder to measure the semantic similarity between predictions and gold answers. For the visual QA task, Luo et al. (2021) incorporate alias answers in visual QA evaluation. In this work, instead of proposing new evaluation metrics, we improve the evaluation"
2021.emnlp-main.757,P17-1171,0,0.0247614,"provide the correct answer to the tion. While some datasets provide comprehensive question. However, the modern formulation of answer sets (e.g., Joshi et al., 2017), subsequent QA usually assumes that each question has only datasets such as NQ have not. . . and we do not know one answer, e.g., SQuAD (Rajpurkar et al., 2016), whether this is a problem. We fill that lacuna. H otpot QA (Yang et al., 2018), DROP (Dua et al., Section 2 mines knowledge bases for alternative 2019). This is often a byproduct of the prevail- answers to named entities. Even this straightforing framework for modern QA (Chen et al., 2017; ward approach finds high-precision answers not inKarpukhin et al., 2020): a retriever finds passages cluded in official answer sets. We then incorporate that may contain the answer, and then a machine this in both training and evaluation of QA models reader identifies the (as in only) answer span. to accept alternate answers. We focus on three popIn a recent position paper, Boyd-Graber and ular open-domain QA datasets: NQ, TriviaQA and Börschinger (2020) argue that this is at odds with SQu AD. Evaluating models with a more permissive the best practices for human QA. This is also a prob- eval"
2021.emnlp-main.757,2020.tacl-1.30,0,0.0222858,"ll of the examples are indeed correct (Table 5), demonstrating the high precision of our answer expansion for augmented evaluation. In rare cases, for example, for the question “Who sang the song Tell Me Something Good?”, the model prediction Rufus is an alias entity, but the reference answer is Rufus and Chaka Khan. The authors disagree whether that would meet a user’s information-seeking need because Chaka Khan, the vocalist, was part of the band Rufus. Hence, it was labeled as debatable. 5 Related Work: Refuse thy Name Answer Annotation in QA Datasets. Some QA datasets such as NQ and TyDi (Clark et al., 2020) n-way annotate dev and test sets where they ask different annotators to annotate the dev and test set. However, such annotation is costly and the coverage is still largely lacking (e.g., our alias expansion obtains many more answers than NQ’s original multi-way annotation). AmbigQA (Min et al., 2020) aims to address the problem of ambiguous questions, where there are multiple interpretations of the same question and therefore multiple correct answer classes (which could in turn have many valid aliases for each class). We provide an orthogonal view as we are trying to expand equivalent answers"
2021.emnlp-main.757,N19-1423,0,0.0215448,"Missing"
2021.emnlp-main.757,N19-1246,0,0.0504729,"Missing"
2021.emnlp-main.757,P17-1147,0,0.0495382,"20 NeurIPS Efficient QA competition (Min et al., 2021), human annotators rate nearly a third of the predictions that do not match the gold annotation as “definitely correct” or “possibly correct”. 1 Introduction: A Name that is the Despite the near-universal acknowledgement of Enemy of Accuracy this problem, there is neither a clear measurement In question answering (QA), computers—given of its magnitude nor a consistent best practice solua question—provide the correct answer to the tion. While some datasets provide comprehensive question. However, the modern formulation of answer sets (e.g., Joshi et al., 2017), subsequent QA usually assumes that each question has only datasets such as NQ have not. . . and we do not know one answer, e.g., SQuAD (Rajpurkar et al., 2016), whether this is a problem. We fill that lacuna. H otpot QA (Yang et al., 2018), DROP (Dua et al., Section 2 mines knowledge bases for alternative 2019). This is often a byproduct of the prevail- answers to named entities. Even this straightforing framework for modern QA (Chen et al., 2017; ward approach finds high-precision answers not inKarpukhin et al., 2020): a retriever finds passages cluded in official answer sets. We then incor"
2021.emnlp-main.757,P19-1612,0,0.0462042,"this entry in the KB, we then use the “common.topic.alias” relation to extract all aliases of the entity (e.g., [Joe Robbie Stadium, Pro Player Park, Pro Player Stadium, Dolphins Stadium, Land Shark Stadium]). We expand the answer set by adding all aliases. We next describe how this changes evaluation and training. 2.3 Augmented Evaluation For evaluation, we report the exact match (EM) score, where a predicted span is correct only if the (normalized) span text matches with a gold answer exactly. This is the adopted metric for spanextraction datasets in most QA papers (Karpukhin et al., 2020; Lee et al., 2019; Min et al., 2019, inter alia). When we incorporate the alias entities in evaluation, we get an expanded answer set A ≡ {a1 , ..., an }. For a given span s predicted by the model, we compute EM score of s if the span matches any correct answer a in the set A: EM (s, A) = max{EM(s, a)}. a∈A (4) where Pi [0, :] represents the [CLS] token, and 2.4 Augmented Training wr , ws and we are learnable weights for passage selection, start span and end span. Training up- When we incorporate the alias entities in training, dates weights with one positive and m − 1 negative we treat each retrieved passage"
2021.emnlp-main.757,2021.eacl-main.240,0,0.0380479,"rove QA evaluation. Chen et al. (2019) find that current automatic metrics do not correlate well with human judgements, which motivated Chen et al. (2020) to construct a dataset with human annotated scores of candidate answers and use it to train a BERT-based regression model as the scorer. Feng and Boyd-Graber (2019) argue for instead of evaluating QA systems directly, we should instead evaluate downstream human accuracy when using QA output. Alternatively, Risch et al. (2021) use a cross-encoder to measure the semantic similarity between predictions and gold answers. For the visual QA task, Luo et al. (2021) incorporate alias answers in visual QA evaluation. In this work, instead of proposing new evaluation metrics, we improve the evaluation of ODQA models by augmenting gold answers with alias from knowledge bases. 6 Conclusion: Wherefore art thou Single Answer? Our approach for matching entities in a KB is a simple approach to improve QA accuracy. We expect future improvements—e.g.,, entity linking source passages would likely improve precision at the cost of recall. Future work should also investigate the role of context in deciding the correctness of predicted answers. Beyond entities, future"
2021.emnlp-main.757,2021.acl-long.316,0,0.0285901,"Missing"
2021.emnlp-main.757,D19-1284,0,0.0119387,"KB, we then use the “common.topic.alias” relation to extract all aliases of the entity (e.g., [Joe Robbie Stadium, Pro Player Park, Pro Player Stadium, Dolphins Stadium, Land Shark Stadium]). We expand the answer set by adding all aliases. We next describe how this changes evaluation and training. 2.3 Augmented Evaluation For evaluation, we report the exact match (EM) score, where a predicted span is correct only if the (normalized) span text matches with a gold answer exactly. This is the adopted metric for spanextraction datasets in most QA papers (Karpukhin et al., 2020; Lee et al., 2019; Min et al., 2019, inter alia). When we incorporate the alias entities in evaluation, we get an expanded answer set A ≡ {a1 , ..., an }. For a given span s predicted by the model, we compute EM score of s if the span matches any correct answer a in the set A: EM (s, A) = max{EM(s, a)}. a∈A (4) where Pi [0, :] represents the [CLS] token, and 2.4 Augmented Training wr , ws and we are learnable weights for passage selection, start span and end span. Training up- When we incorporate the alias entities in training, dates weights with one positive and m − 1 negative we treat each retrieved passage as positive if it"
2021.emnlp-main.757,2020.emnlp-main.466,0,0.129094,"nd Chaka Khan. The authors disagree whether that would meet a user’s information-seeking need because Chaka Khan, the vocalist, was part of the band Rufus. Hence, it was labeled as debatable. 5 Related Work: Refuse thy Name Answer Annotation in QA Datasets. Some QA datasets such as NQ and TyDi (Clark et al., 2020) n-way annotate dev and test sets where they ask different annotators to annotate the dev and test set. However, such annotation is costly and the coverage is still largely lacking (e.g., our alias expansion obtains many more answers than NQ’s original multi-way annotation). AmbigQA (Min et al., 2020) aims to address the problem of ambiguous questions, where there are multiple interpretations of the same question and therefore multiple correct answer classes (which could in turn have many valid aliases for each class). We provide an orthogonal view as we are trying to expand equivalent answers to any given gold answer while AmbigQA aims to cover semantically different but valid answers. Evaluation of QA Models. There are other attempts to improve QA evaluation. Chen et al. (2019) find that current automatic metrics do not correlate well with human judgements, which motivated Chen et al. (2"
2021.emnlp-main.757,D19-1261,0,0.0290383,"Missing"
2021.emnlp-main.757,D16-1264,0,0.0515468,"initely correct” or “possibly correct”. 1 Introduction: A Name that is the Despite the near-universal acknowledgement of Enemy of Accuracy this problem, there is neither a clear measurement In question answering (QA), computers—given of its magnitude nor a consistent best practice solua question—provide the correct answer to the tion. While some datasets provide comprehensive question. However, the modern formulation of answer sets (e.g., Joshi et al., 2017), subsequent QA usually assumes that each question has only datasets such as NQ have not. . . and we do not know one answer, e.g., SQuAD (Rajpurkar et al., 2016), whether this is a problem. We fill that lacuna. H otpot QA (Yang et al., 2018), DROP (Dua et al., Section 2 mines knowledge bases for alternative 2019). This is often a byproduct of the prevail- answers to named entities. Even this straightforing framework for modern QA (Chen et al., 2017; ward approach finds high-precision answers not inKarpukhin et al., 2020): a retriever finds passages cluded in official answer sets. We then incorporate that may contain the answer, and then a machine this in both training and evaluation of QA models reader identifies the (as in only) answer span. to accep"
2021.emnlp-main.757,2021.mrqa-1.15,0,0.0320945,"old answer while AmbigQA aims to cover semantically different but valid answers. Evaluation of QA Models. There are other attempts to improve QA evaluation. Chen et al. (2019) find that current automatic metrics do not correlate well with human judgements, which motivated Chen et al. (2020) to construct a dataset with human annotated scores of candidate answers and use it to train a BERT-based regression model as the scorer. Feng and Boyd-Graber (2019) argue for instead of evaluating QA systems directly, we should instead evaluate downstream human accuracy when using QA output. Alternatively, Risch et al. (2021) use a cross-encoder to measure the semantic similarity between predictions and gold answers. For the visual QA task, Luo et al. (2021) incorporate alias answers in visual QA evaluation. In this work, instead of proposing new evaluation metrics, we improve the evaluation of ODQA models by augmenting gold answers with alias from knowledge bases. 6 Conclusion: Wherefore art thou Single Answer? Our approach for matching entities in a KB is a simple approach to improve QA accuracy. We expect future improvements—e.g.,, entity linking source passages would likely improve precision at the cost of rec"
2021.emnlp-main.757,D18-1259,0,0.0195604,"e near-universal acknowledgement of Enemy of Accuracy this problem, there is neither a clear measurement In question answering (QA), computers—given of its magnitude nor a consistent best practice solua question—provide the correct answer to the tion. While some datasets provide comprehensive question. However, the modern formulation of answer sets (e.g., Joshi et al., 2017), subsequent QA usually assumes that each question has only datasets such as NQ have not. . . and we do not know one answer, e.g., SQuAD (Rajpurkar et al., 2016), whether this is a problem. We fill that lacuna. H otpot QA (Yang et al., 2018), DROP (Dua et al., Section 2 mines knowledge bases for alternative 2019). This is often a byproduct of the prevail- answers to named entities. Even this straightforing framework for modern QA (Chen et al., 2017; ward approach finds high-precision answers not inKarpukhin et al., 2020): a retriever finds passages cluded in official answer sets. We then incorporate that may contain the answer, and then a machine this in both training and evaluation of QA models reader identifies the (as in only) answer span. to accept alternate answers. We focus on three popIn a recent position paper, Boyd-Grabe"
2021.emnlp-main.757,2021.naacl-main.368,1,0.701639,"ion (Equations 2–3). To study the effect of equivalent answers in reader training, we focus on the distant supervision setting where we know what the answer is but not where it is (in contrast to full supervision where we know both). To use the answer to discover positive passages, we use string matching: any of the top-k retrieved passages that contains an answer is considered correct. We discard questions without any positive passages. This framework is consistent with modern state-of-the-art ODQA pipelines (Alberti et al., 2019; Karpukhin et al., 2020; Zhao et al., 2020; Asai et al., 2020; Zhao et al., 2021, inter alia). 2.2 Extracting Alias Entities We expand the original gold answer set by extracting aliases from Freebase (Bollacker et al., 2008), a large-scale knowledge base (KB). Specifically, for each answer in the original dataset (e.g., Sun Life Stadium), if we can find this entry in the KB, we then use the “common.topic.alias” relation to extract all aliases of the entity (e.g., [Joe Robbie Stadium, Pro Player Park, Pro Player Stadium, Dolphins Stadium, Land Shark Stadium]). We expand the answer set by adding all aliases. We next describe how this changes evaluation and training. 2.3 Aug"
2021.emnlp-main.757,2020.findings-emnlp.424,0,0.0409261,"Missing"
2021.emnlp-main.758,D13-1160,0,0.0431367,"Missing"
2021.emnlp-main.758,2020.acl-main.662,1,0.906661,"he questions in the Manchester paradigm. However, it did not follow the same principles of the Manchester paradigm, which led to the “shortcuts” that other investigators have discovered in the years since (Weissenborn et al., 2017). For example, priming made exploitable clues more frequent and Mechanical Turkers write each question as quickly as possible. Levesque (2014) anticipates this behaviour, specifically avoiding “cheap tricks” in their Manchester-paradigm Winograd challenge. In other Manchester paradigm questions, trivia question writers frequently take pride in wellcrafted questions (Boyd-Graber and Börschinger, 2020). Comparisons One of the primary inspirations for the Manchester paradigm is competitions (e.g., University Challenge). Because these competitions are meant to determine who the smartest answerer is, they are remarkably efficient. The world accepted the judgement that Watson was smarter than Ken Jennings and Brad Rutter after 122 answers. Why not? These competitions are designed to discriminate between player abilities. In contrast, the dev and test sets of Cranfield-inspired datasets have thousands of questions, and even that may not be enough (Card et al., 2020; Rodriguez et al., 2021). 6 Ca"
2021.emnlp-main.758,2020.emnlp-main.745,0,0.0198394,"ed questions (Boyd-Graber and Börschinger, 2020). Comparisons One of the primary inspirations for the Manchester paradigm is competitions (e.g., University Challenge). Because these competitions are meant to determine who the smartest answerer is, they are remarkably efficient. The world accepted the judgement that Watson was smarter than Ken Jennings and Brad Rutter after 122 answers. Why not? These competitions are designed to discriminate between player abilities. In contrast, the dev and test sets of Cranfield-inspired datasets have thousands of questions, and even that may not be enough (Card et al., 2020; Rodriguez et al., 2021). 6 Call to Action 2020). While Gardner et al. (2019) offer a trenchant enumeration of QA uses, 3 we think the onus of definition should fall on dataset creators, not on post-hoc analyses. Other than more explicitly naming two of the uses of QA after English University towns, our goal is to encourage researchers to recognize the tensions between these two uses and the opportunities created from recognizing the distinction. Make what you Value Explicit Each of these paradigms value different skills and embed these values in datasets and tasks. To make machine learning u"
2021.emnlp-main.758,P17-1171,0,0.0126817,"were first composed for computers: the Winograd schema challenge (Levesque et al., 2011) and its successor the Winogrand challenge (Sakaguchi et al., 2020). In this task, changing one word between two nearly identical binary questions also changes the answer. 2 Should a machine fail such questions, it does not evince intelligence—at least not like humans. While we set these paradigms in opposition to each other, we next discuss the swath of research that advances the goals of both. by dataset-specific quirks. Thus, a paradigmagnostic blueprint for QA (Chen and Yih, 2020) is to combine sparse (Chen et al., 2017) or dense retrieval (Guu et al., 2020; Karpukhin et al., 2020) followed by span selection (Seo et al., 2017) or generation (Lewis et al., 2020). As a consequence, researchers indifferent to which questions are answered can improve representations and algorithms for both paradigms (although as interactions become richer, this may not be the case, as we discuss at the end of Section 6). The paradigms’ evaluations also overlap; they benefit from expert annotators (Gardner et al., 2020; Feng and Boyd-Graber, 2019), crowd annotators, and alternative evaluations like behavioural testing (Ribeiro et"
2021.emnlp-main.758,2020.acl-tutorials.8,0,0.102408,"paradigm, a heed the distinction and how the paradigms can good QA system should answer the questions users inform each other. ask. What more could you want? 9630 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9630–9642 c November 7–11, 2021. 2021 Association for Computational Linguistics It is then unsurprising that Google and Microsoft adopted this setting for Natural Questions (Kwiatkowski et al., 2019) and MS MARCO (Nguyen et al., 2016). They found questions people asked online and answered them. A good QA system should automate that process (Chen and Yih, 2020). 3 Probing and Pushing Answerers: The Manchester Paradigm In the Manchester paradigm, we create tasks and datasets whose questions push answerers to better understand the world and create evaluations that probe for human-like capabilities. Since we identify and name the paradigm, we give three justifications that highlight three distinct reasons people ask questions beyond information seeking: to teach, to compare, and to probe. 3.1 Why the Name Manchester? For symmetry with the Cranfield paradigm, our proposed name is also an English city: Manchester. Because there are multiple aspects to th"
2021.emnlp-main.758,D18-1241,0,0.0123028,"Luis von Ahn and Laura Dabbish. 2008. Designing games with a purpose. Communications of the ACM, In the Cranfield paradigm, this is an opportu51(8):58–67. nity to correct false presuppositions: “when did Raphael paint the Mona Lisa” could flag that Da David Baber. 2015. Television Game Show Hosts: BiVinci painted it in 1503 and to explain multiple ographies of 32 Stars. McFarland. interpretations of a question (Min et al., 2020). In Ricardo Baeza-Yates, Aristides Gionis, Flavio Junthe Manchester paradigm, this can use dialog to queira, Vanessa Murdock, Vassilis Plachouras, and train systems (Choi et al., 2018), guide the sysFabrizio Silvestri. 2007. The impact of caching on search engines. In Proceedings of the ACM SIGIR tem to semantically equivalent answers (Si et al., Conference on Research and Development in Infor2021), or to learn from how humans answer the mation Retrieval. Association for Computing Masame questions (He et al., 2016). For example, if chinery. a computer answers Bush to the question “Who appointed Scalia to the supreme court”, a Manch- Michael Barbaro and Tom Zeller. 2006. A face is exposed for aol searcher no. 4417749. ester inquisitor would rightly follow up with “can you be"
2021.emnlp-main.758,P16-1145,0,0.0648582,"Missing"
2021.emnlp-main.758,P99-1042,0,0.378923,"Missing"
2021.emnlp-main.758,D19-1243,0,0.0244613,"Missing"
2021.emnlp-main.758,Q18-1023,0,0.0551714,"Missing"
2021.emnlp-main.758,Q19-1026,0,0.0268985,"Missing"
2021.emnlp-main.758,D17-1215,0,0.0245345,"y adversarial examples are more consistent with the Manchester paradigm. 5 Ignore the Distinction at your Peril How Adversarial is too Much? Common ground has its limits. That there is not a dichotomy between these two approaches can sometimes mask the importance of distinguishing motivations. Other proposals for robustness postulate that models should be robust to input modifications 4 What Cranfield and Manchester Share users would not make (Feng et al., 2018), challengAlthough these paradigms have different core goals, ing yet unnatural adversarial questions that users are unlikely to ask (Jia and Liang, 2017; Wallace research advancing the goals of one often advances et al., 2019; Bartolo et al., 2020; Kiela et al., 2021), the goals of the other. and testing a concept in multiple ways (Gardner While there are differences between QA datasets et al., 2020; Kaushik et al., 2020). While solving across paradigms (Cambazoglu et al., 2020; Zeng et al., 2020; Dzendzik et al., 2021), these dif- these challenges may eventually improve Cranfieldmotivated systems, in the short term solving these ferences are overshadowed within a paradigm challenges does not directly contribute to improv2 In “the trophy woul"
2021.emnlp-main.758,P17-1147,0,0.0232494,"ure. A variant in Blade Runner tests empathy—rather than intelligence— with probing questions (Joerden, 2012). Likewise, for tests of intelligence in the Manchester paradigm, the Turing Test “represents what it is that AI must endeavo[u]r eventually to accomplish scientifically” (Harnad, 1992). Methodologically, the Manchester paradigm iteratively imagines tasks where machines should rival humans (Levesque, 2014), develops systems, and then determines if systems pass the test. 3.2 Examples Questions derived from education (Clark, 2015), puzzles (Littman et al., 2002), and trivia competitions (Joshi et al., 2017) are in the Manchester camp (full categorisation in Appendix A). However, prominent Manchester paradigm questions were first composed for computers: the Winograd schema challenge (Levesque et al., 2011) and its successor the Winogrand challenge (Sakaguchi et al., 2020). In this task, changing one word between two nearly identical binary questions also changes the answer. 2 Should a machine fail such questions, it does not evince intelligence—at least not like humans. While we set these paradigms in opposition to each other, we next discuss the swath of research that advances the goals of both."
2021.emnlp-main.758,2020.emnlp-main.550,0,0.0109137,"allenge (Levesque et al., 2011) and its successor the Winogrand challenge (Sakaguchi et al., 2020). In this task, changing one word between two nearly identical binary questions also changes the answer. 2 Should a machine fail such questions, it does not evince intelligence—at least not like humans. While we set these paradigms in opposition to each other, we next discuss the swath of research that advances the goals of both. by dataset-specific quirks. Thus, a paradigmagnostic blueprint for QA (Chen and Yih, 2020) is to combine sparse (Chen et al., 2017) or dense retrieval (Guu et al., 2020; Karpukhin et al., 2020) followed by span selection (Seo et al., 2017) or generation (Lewis et al., 2020). As a consequence, researchers indifferent to which questions are answered can improve representations and algorithms for both paradigms (although as interactions become richer, this may not be the case, as we discuss at the end of Section 6). The paradigms’ evaluations also overlap; they benefit from expert annotators (Gardner et al., 2020; Feng and Boyd-Graber, 2019), crowd annotators, and alternative evaluations like behavioural testing (Ribeiro et al., 2020). Similarly, both paradigms value robustness (Dalvi"
2021.emnlp-main.758,N18-1023,0,0.0378929,"Missing"
2021.emnlp-main.758,D17-1082,0,0.0652099,"Missing"
2021.emnlp-main.758,D19-5808,0,0.0238432,"Missing"
2021.emnlp-main.758,2020.acl-main.465,0,0.129103,"lthough as interactions become richer, this may not be the case, as we discuss at the end of Section 6). The paradigms’ evaluations also overlap; they benefit from expert annotators (Gardner et al., 2020; Feng and Boyd-Graber, 2019), crowd annotators, and alternative evaluations like behavioural testing (Ribeiro et al., 2020). Similarly, both paradigms value robustness (Dalvi et al., 2004; Jia, 2020). Additionally, answering infrequently asked questions is important for search engines (Baeza-Yates et al., 2007), and building models that learn more from less qualifies as intelligent behaviour (Linzen, 2020). Creating systems robust to spelling mistakes (Wang and Pedersen, 2011) is a worthy goal. From the Cranfield perspective, systems hobbled by spelling mistakes lead to a poor user experience. On the other side, humans are impressively robust to poor spelling (Rayner et al., 2006), so from the Manchester perspective this form of robustness is also valuable. But this has its limits; in the next section, we argue why adversarial examples are more consistent with the Manchester paradigm. 5 Ignore the Distinction at your Peril How Adversarial is too Much? Common ground has its limits. That there is"
2021.emnlp-main.758,D18-1260,0,0.0347323,"Missing"
2021.emnlp-main.758,2020.emnlp-main.466,0,0.010945,"eacher or an evaluator. In both cases, Shneiderman (2021) argues that responsible AI should enable an interactive, responsive conversation be- References tween the system and the user. Luis von Ahn and Laura Dabbish. 2008. Designing games with a purpose. Communications of the ACM, In the Cranfield paradigm, this is an opportu51(8):58–67. nity to correct false presuppositions: “when did Raphael paint the Mona Lisa” could flag that Da David Baber. 2015. Television Game Show Hosts: BiVinci painted it in 1503 and to explain multiple ographies of 32 Stars. McFarland. interpretations of a question (Min et al., 2020). In Ricardo Baeza-Yates, Aristides Gionis, Flavio Junthe Manchester paradigm, this can use dialog to queira, Vanessa Murdock, Vassilis Plachouras, and train systems (Choi et al., 2018), guide the sysFabrizio Silvestri. 2007. The impact of caching on search engines. In Proceedings of the ACM SIGIR tem to semantically equivalent answers (Si et al., Conference on Research and Development in Infor2021), or to learn from how humans answer the mation Retrieval. Association for Computing Masame questions (He et al., 2016). For example, if chinery. a computer answers Bush to the question “Who appoint"
2021.emnlp-main.758,2020.emnlp-main.713,0,0.0121057,"s also the gift of the ability to solve riddles. (Burgess, 2013) This aspect of question answering also brings us to another Greek connection to asking questions: teaching through the Socratic method (Trepanier, 2017). Through asking the right questions, a teacher guides the answerer to understanding. While Cranfield questioners are seeking information from a more knowledgeable answerer, Manchester questioners often test less knowledgeable answerers. Similarly, perhaps by asking the right questions, the QA community can coax computers to understand more than they do now (Dunietz et al., 2020; Perez et al., 2020). To Compare: Granada Studios Another inspiration for this question answering approach is Manchester’s Granada Studios, creator of University Challenge (Taylor et al., 2012; Baber, 2015). This television programme juxtaposes two universities to see who is smarter. Just like the Sphinx, the dapper host of this game show, Bamber Gasgoine, knows the answer. Thus it is not an information-seeking task a la the Cranfield paradigm. It, like the riddle of the Sphinx, is a test of those answering the questions. It’s also a tried and true test of question answering researchers’ mettle, as when IBM Watso"
2021.emnlp-main.758,2021.findings-emnlp.315,1,0.679726,"dustry is naturally financially motivated towards this goal, and they have the user data (Zhang et al., 2019)—only a fraction of which is published to protect privacy (Barbaro and Zeller, 2006). Still, strategic and thoughtful partnerships like the Cranfield-inspired TREC workshops are valuable; without TREC , it is estimated that “US Internet users would have spent up to 3.15 billion additional hours using web search engines between 1999 and 2009” (Tassey et al., 2010). One of the goals of the Manchester paradigm should be to identify the linguistic phenomena or ethnic and linguistic groups (Peskov et al., 2021) that are not well-served by Cranfieldfocused data. Thus, before you begin your question answering research, make it clear what your goal is: are you trying to build AGI 4 or to serve users? That answer will then inform your evaluation methology. Academia’s Special Role It is no coincidence that our paradigms are named after the homes of universities, and universities are where the Manchester paradigm will thrive. Thus, there is a strategic opportunity for academia and funding agencies to support Manchester-aligned work abjured by 3 Our central plea is that researchers in QA and NLP These are:"
2021.emnlp-main.758,P18-2124,0,0.0299743,"Missing"
2021.emnlp-main.758,D16-1264,0,0.0919931,"Missing"
2021.emnlp-main.758,P18-1156,0,0.0325938,"Missing"
2021.emnlp-main.758,W17-2623,0,0.0603202,"Missing"
2021.emnlp-main.758,D18-1259,0,0.0408075,"Missing"
2021.findings-emnlp.315,P19-1120,0,0.0197317,"ture automatic methods. People need NLP systems that reflect their language and culture, but datasets are lacking: adaptation can help. There has been an explosion of English-language QA datasets, but other languages continue to lag behind. Several approaches try to transfer English’s bounty to other languages (Lewis et al., 2020; Artetxe et al., 2019), but most of the entities asked about in major QA datasets are American (Gor et al., 2021). Adapting entire questions will require not just adapting entities and non-entities in tandem but will also require integration with machine translation (Kim et al., 2019; Hangya and Fraser, 2019). Our automatic methods did not create precise adaptations, but the alternative “incorrect” adaptations may be useful for low-precision tasks, such as generating numerous simple open-ended questions or gauging the popularity of a entity. Given the existence of robust datasets in high resource languages can we adapt, rather than literally translate, them to other cultures and languages? 7 Acknowledgments Peskov was supported by a DAAD Research Fellowship and by the wonderful faculty and students of Ludwig Maximilians Universität München. Fraser is supported by the Euro"
2021.findings-emnlp.315,C08-1114,0,0.0227786,"plausible (Section 5.3). 2 Wer ist Bill Gates? We define cultural adaptation and motivate its application for tasks like creating culturally-centered training data for QA. Vinay and Darbelnet (1995) define adaptation as translation in which the relationship not the literal meaning between the receiver and the content needs to be recreated. You could formulate our task as a tradi3725 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3725–3750 November 7–11, 2021. ©2021 Association for Computational Linguistics tional analogy Drosten::Germany as Fauci::United States (Turney, 2008; Gladkova et al., 2016), but despite this superficial resemblance (explored in Section 4), traditional approaches to analogy ignore the influence of culture and are typically within a language. Hence, analogies are tightly bound with culture; humans struggle with analogies outside their culture (Freedle, 2003). We can use this task to identify named entities (Kasai et al., 2019; Arora et al., 2019; Jain et al., 2019) and for understanding other cultures (Katan and Taibi, 2004). 2.1 . . . and why Bill Gates? This task requires a list of named entities adaptable to other cultures. Our entities"
2021.findings-emnlp.315,P07-2045,0,0.00627561,"sfying as it was Barack Obama who sat across from Merkel for nearly a decade. To capture these more nuanced similarities, we turn to large text corpora in Section 4. 4 An Alternate Embedding Approach While the classic NLP vector example (Mikolov et al., 2013c) isn’t as magical as initially claimed (Rogers et al., 2017), it provides useful intuition. We can use the intuitions of the cliché: −−→ −−→ −−−−→ −−−→ King − Man + Woman = Queen (1) to adapt between languages. This, however, requires relevant embeddings. First, we use the entire Wikipedia in English and German, preprocessed using Moses (Koehn et al., 2007). We follow Mikolov et al. (2013b) and use named entity recognition (Honnibal et al., 2020) to tokenize entities such as Barack_Obama. We use word2vec (Mikolov et al., 2013b), rather than FastText (Bojanowski et al., 2017), as we do not want orthography to influence the similarity of entities. Angela Merkel in English and in German have quite different neighbors, and we intend to keep it that way by preserving the distinction between languages. However, the standard word2vec model assumes a single monolingual embedding space. We use unsupervised Vecmap (Artetxe et al., 2018), a leading tool fo"
2021.findings-emnlp.315,N13-1090,0,0.130532,"ate the similarity of the vectors with Faiss’s L2 distance (Johnson et al., 2021) and for each vector in A find the closest vector in D and vice versa. So who is the American Angela Merkel? One possible answer is Woodrow Wilson, a member of a “political party”, who had a “doctoral advisor” and a “religion”, and ended up with “awards”. This answer may be unsatisfying as it was Barack Obama who sat across from Merkel for nearly a decade. To capture these more nuanced similarities, we turn to large text corpora in Section 4. 4 An Alternate Embedding Approach While the classic NLP vector example (Mikolov et al., 2013c) isn’t as magical as initially claimed (Rogers et al., 2017), it provides useful intuition. We can use the intuitions of the cliché: −−→ −−→ −−−−→ −−−→ King − Man + Woman = Queen (1) to adapt between languages. This, however, requires relevant embeddings. First, we use the entire Wikipedia in English and German, preprocessed using Moses (Koehn et al., 2007). We follow Mikolov et al. (2013b) and use named entity recognition (Honnibal et al., 2020) to tokenize entities such as Barack_Obama. We use word2vec (Mikolov et al., 2013b), rather than FastText (Bojanowski et al., 2017), as we do not wa"
2021.mrqa-1.9,2020.acl-main.485,0,0.0834743,"Missing"
2021.mrqa-1.9,N19-3002,0,0.0274598,"hree ways: amplified, reproduced, or mitigated. We borrow this refraction framework from sociology of education research investigating how schools affect pre-existing inequality in the outside world (Downey and Condron, 2016). Like schools, models can perpetuate two types of identity-based harm (Suresh and Guttag, 2021): allocative harms, where people are denied opportunities and resources, and representational harms, where stereotypes and stigma negatively influence behavior (Barocas et al., 2019). Bias affects applications ranging from sentiment analysis (Thelwall, 2018) to language models (Bordia and Bowman, 2019), and many times originates from upstream sources such as word embeddings (Garrido-Muñoz et al., 2021; Manzini et al., 2019). Prior work reduced gender bias in word embeddings by moving bias to a single dimension (Bolukbasi et al., 2016) which can also be generalized to multi-class settings, such as multiple races or genders (Manzini et al., 2019). 2.2 3 Ambiguity: A social science perspective We adopt ideas from the social sciences which demonstrate ambiguity as a revelatory mechanism for bias. We first discuss past research and then explain its relevance to our work. Ambiguous questions, whi"
2021.mrqa-1.9,W17-1601,0,0.0174611,"he opening paragraph of the entity’s Wikipedia page as context. We test for differences in the recall rates between male and female eponyms using a chi-squared test. Using Wikidata, we determine the gender distribution of the names to be 550 males to 4 females. Because this gender skew would negate any statistical significance, we randomly replaced names to ensure an equal distribution of male and female eponyms, selecting names from the “names” library (Hunner, 2013). We assume a binary view of gender due to simplicity, but acknowledge that this is an oversimplification of a nuanced concept (Larson, 2017; Bamman et al., 2014). 5.2 Retrieval Ambiguity We give open-ended questions to a passage retriever to see if its passages are overly biased towards a certain demographic. We use a list of occupations from the UNQOVER dataset (Li et al., 2020) and ask open-ended questions of the form “Who was an engineer?” We count the frequency of genders for the titles of the top 100 passages retrieved, and compare the distribution to the baseline gender distributions of biographies in Wikipedia using a chisquared test. Formally, given a set of protected groups pi , we compute Bias Benchmarks We develop two"
2021.mrqa-1.9,2020.findings-emnlp.311,0,0.392169,"18). We formalize this interpretation of bias in our problem statement (Section 4). Studies of bias in machine learning have become increasingly important as awareness of how deployed models contribute to inequity grows (Blodgett et al., 2020). Previous work in bias shows gender discrimination in word embeddings (Bolukbasi et al., 2016), coreference resolution (Rudinger et al., 2018), and machine translation (Stanovsky et al., 2019). Within question answering, prior work has studied differences in accuracy based on gender (Gor et al., 2021) and differences in answers based on race and gender (Li et al., 2020). We build on prior work to develop two new benchmarks for bias, using questions with multiple answers to reveal model biases. Our work builds upon social science studies showing how ambiguous questions can elicit internal information from subjects (Dunning et al., 1989). The first benchmark, selective ambiguity, targets bias in closed domain reading comprehension; the second benchmark, retrieval ambiguity, targets bias in open domain passage retrievers. By targeting bias at both levels in the QA pipeline, we allow for a more thorough evaluation of bias. We apply our benchmarks to a set of neu"
2021.mrqa-1.9,N19-1062,0,0.0262048,"igating how schools affect pre-existing inequality in the outside world (Downey and Condron, 2016). Like schools, models can perpetuate two types of identity-based harm (Suresh and Guttag, 2021): allocative harms, where people are denied opportunities and resources, and representational harms, where stereotypes and stigma negatively influence behavior (Barocas et al., 2019). Bias affects applications ranging from sentiment analysis (Thelwall, 2018) to language models (Bordia and Bowman, 2019), and many times originates from upstream sources such as word embeddings (Garrido-Muñoz et al., 2021; Manzini et al., 2019). Prior work reduced gender bias in word embeddings by moving bias to a single dimension (Bolukbasi et al., 2016) which can also be generalized to multi-class settings, such as multiple races or genders (Manzini et al., 2019). 2.2 3 Ambiguity: A social science perspective We adopt ideas from the social sciences which demonstrate ambiguity as a revelatory mechanism for bias. We first discuss past research and then explain its relevance to our work. Ambiguous questions, which lack a clear answer or have multiple possible answers, force individuals to rely on unconscious biases and selfserving tr"
2021.mrqa-1.9,D14-1162,0,0.084918,"Missing"
2021.mrqa-1.9,D16-1264,0,0.339815,"ver to reader systems by using the output of the retriever as context for a QA model, selecting the answer with the highest confidence over the 100 articles. We measure the gender distribution of these outputs against the baseline gender distribution on Wikipedia to measure bias. |{f (qj , cj ) : f (qj , cj ) = aj , p(f (qj , cj )) = pi }| |{aj : p(aj ) = pi }| 6 We retrieve poly-eponymous discoveries from the Wikipedia pages “List of scientific laws named after people” and “Scientific phenomena named Experiments We apply our bias metrics on three QA models, each trained on the SQuAD dataset (Rajpurkar et al., 2016). 94 Question type Selectional Ambiguity Retrieval Ambiguity Example Q: Who discovered the Biot-Savart law? A: Jean-Baptiste Biot and Felix Savart Q: Who is an author? Sample A: Jane Austen Number of Questions 256 370 Table 1: Summary of the two question types in our study. Note that for retrieval ambiguity, any author is a valid response. 6.1 Experimental Setup plot the eight lowest and highest gender disparities and find a significant gender skew by occupation aligning with common stereotypes of males and females (Bekolli, 2013). For example, stereotypically female occupations such as nurse"
2021.mrqa-1.9,P19-1164,0,0.0607541,"Missing"
2021.naacl-main.32,N19-1423,0,0.117849,"5: Automatically build dataset Keep highest quality claims with their selected evidence FM2 Figure 1: Overview of the data generation pipeline. In stages 1 to 4, players write challenging claims either entailed or refuted by evidence from Wikipedia (Section 3.1). They are then tasked to spot the refuted claim among a group (Section 3.2). The claims and evidence are available for download. We review existing resources for the latter task in Section 2 and how they have spawned a vibrant subcommunity around related tasks. However, these datasets fail to challenge modern NLP models such as BERT (Devlin et al., 2019) or T5 (Raffel et al., 2020) that achieve “super-human performance” despite also exhibiting “annotation artifacts” that hurt their generalization potential (Gururangan et al., ∗ Work completed while a Visiting Research Scientist at 2018; Tsuchiya, 2018). Our goal is twofold: (1) Google. 1 to build a new, challenging dataset (statistics for https://github.com/google-research/ fool-me-twice F OOL M E T WICE in Table 1) that tests models’ abil352 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 35"
2021.naacl-main.32,2020.findings-emnlp.27,1,0.750603,"les 7 where the precision was less than 0.5. We evaluate two baselines. The first one follows Chen et al. (2017) and uses a TF - IDF retrieval model with unigrams and bigrams and 220 hash buckets. The title of page is added to the passage content for additional context. The second baseline uses Dense Passage Retrieval (Karpukhin et al., 2020, DPR), using the same fixed pre-trained passage embeddings and query encoder as the ones used in Petroni et al. (2021). 4.2 Entailment For the second component of the task, we follow state-of-the-art entailment models (Zhou et al., 2019; Liu et al., 2020; Eisenschlos et al., 2020): 5 Unlike FEVER, we do not allow authors to write claims that lack “enough information”. 6 http://github.com/facebookresearch/KILT/ 7 This happens because F OOL M E T WICE was constructed from a more recent version of Wikipedia than KILT. 356 Topic Claim Iceland After ending its personal union with Denmark, Iceland was last invaded by the United Kingdom in Operation Fork. Elizabeth II After Elizabeth II’s accession, she changed her house’s name from &quot;House of Saxe-Coburg and Gotha&quot; to the &quot;House of Windsor&quot;, rejecting the &quot;House of Edinburgh&quot;. Retrieved Evidence The Danish-Icelandic Act of Un"
2021.naacl-main.32,P19-1554,1,0.795999,"t Wikipedia pages. The number of tokens is an average value computed with a white-space tokenizer. Our dataset is balanced between entailed (true) and not entailed (false) claims. Entailment and Retrieval Datasets Despite the promise of entailment, it has not been a silver bullet for the NLP community to solve artificial intelligence. One possible explanation is highlighted by a line of work that shows existing entailment datasets have artifacts. Poliak et al. (2018) show entailment can often be solved by looking ity to retrieve evidence and verify claims and (2) only at the hypothesis, while Feng et al. (2019) to show that engineering the incentive structure show that artifacts can infect the premise as well. of data collection experiments can produce more This is especially common in the biggest datasets accurate and realistic outcomes. for NLI such as SNLI and MNLI (Gururangan et al., This dataset lends itself to automatic training 2018). While there are algorithmic solutions to and it characterizes what factual errors humans can addressing these issues (Utama et al., 2020), many most easily detect and which are most likely to have turned to building better datasets. fool them (Section 3.2). This"
2021.naacl-main.32,N18-2017,0,0.0466675,"Missing"
2021.naacl-main.32,2020.findings-emnlp.309,0,0.0780517,"Missing"
2021.naacl-main.32,2020.acl-main.655,0,0.0192257,"remove 1598 examples 7 where the precision was less than 0.5. We evaluate two baselines. The first one follows Chen et al. (2017) and uses a TF - IDF retrieval model with unigrams and bigrams and 220 hash buckets. The title of page is added to the passage content for additional context. The second baseline uses Dense Passage Retrieval (Karpukhin et al., 2020, DPR), using the same fixed pre-trained passage embeddings and query encoder as the ones used in Petroni et al. (2021). 4.2 Entailment For the second component of the task, we follow state-of-the-art entailment models (Zhou et al., 2019; Liu et al., 2020; Eisenschlos et al., 2020): 5 Unlike FEVER, we do not allow authors to write claims that lack “enough information”. 6 http://github.com/facebookresearch/KILT/ 7 This happens because F OOL M E T WICE was constructed from a more recent version of Wikipedia than KILT. 356 Topic Claim Iceland After ending its personal union with Denmark, Iceland was last invaded by the United Kingdom in Operation Fork. Elizabeth II After Elizabeth II’s accession, she changed her house’s name from &quot;House of Saxe-Coburg and Gotha&quot; to the &quot;House of Windsor&quot;, rejecting the &quot;House of Edinburgh&quot;. Retrieved Evidence The"
2021.naacl-main.32,2020.acl-main.441,0,0.0412714,"nline game to create a platform where ods prove to be ineffective for that goal, we view motivated authors can create plausible sounding NLI is as an important end task in itself (e.g., for “facts” that other users must debunk. misinformation, QA, dialogue, generation evaluNot only does this create more realistic claims— ation). Hence, we argue that constructing chalthe best must withstand human scrutiny—it also lenging entailment datasets is useful beyond just creates a way to better evaluate the evidence that transfer learning. support or refute claims. As we surface the eviLike this paper, Nie et al. (2020) focus on addence, humans use that evidence to decide which versarial entailment, but their authors only see a claims are true or false; these signals can further single piece of evidence. We expand this human-inimprove our systems (Figure 1). We apply baseline the-loop adversarial setting to include the essential models for retrieval and classification to our dataset retrieval component of fact verification. Thus, au(Section 4) and examine how their ability to detect wrong statements differs from humans’ (Section 5). thors have more strategies on hand; in addition to creating challenging exam"
2021.naacl-main.32,P02-1040,0,0.114898,"e retrieval setting in the KILT benchmark (Petroni et al., 2021), but the results are evaluated at the evidence level as opposed to the page level, to represent a more realistic use case. The evidence corpus can be found online 6 and consists of twenty-two million text passages, each having a length of a hundred words, from five million pages of the English Wikipedia image from August 2019. We align gold F OOL M E T WICE evidence to this knowledge source by selecting the passage with highest overlap with each evidence sentence, according to the modified n-gram precision component of the BLEU (Papineni et al., 2002). We remove 1598 examples 7 where the precision was less than 0.5. We evaluate two baselines. The first one follows Chen et al. (2017) and uses a TF - IDF retrieval model with unigrams and bigrams and 220 hash buckets. The title of page is added to the passage content for additional context. The second baseline uses Dense Passage Retrieval (Karpukhin et al., 2020, DPR), using the same fixed pre-trained passage embeddings and query encoder as the ones used in Petroni et al. (2021). 4.2 Entailment For the second component of the task, we follow state-of-the-art entailment models (Zhou et al., 20"
2021.naacl-main.32,2021.naacl-main.200,0,0.0463388,"Missing"
2021.naacl-main.32,S18-2023,0,0.0535999,"Missing"
2021.naacl-main.32,D19-1292,0,0.0203505,"it has caught the attention of a subcommunity focused on building systems for FEVER shared tasks. Despite this excitement, Schuster et al. (2019) show that FEVER has many of the same issues as entailment datasets. FEVER has broad or nonsensical claims (Table 2) and many of the claims are generated from the very first line of source Wikipedia documents. This is not just an artifact of crowd-sourcing; a more fundamental problem is that there is no clear definition of what makes a good FEVER example. To date, adversarial FEVER example generation uses automatic rules to increase their difficulty (Thorne et al., 2019). To address these identified weaknesses, Sections 3.1 and 3.2 define a game where the claim writers have a clear objective of “fooling” other human players. Gamification for Data Collection Top Bigrams by LMI (highest predictive power first) FEVER FM 2 Table 2: Examples from FEVER, which separates entailment examples into three categories. The crowdworkers who authored the examples often edit the first line of the Wikipedia article but not in ways that sound like a plausible hypothesis. We develop a game to build more complex, challenging examples. 2.2 Dataset split 3.1 Crafting Challenging C"
2021.naacl-main.32,L18-1239,0,0.0641954,"Missing"
2021.naacl-main.32,2020.emnlp-main.613,0,0.0144278,"ow entailment can often be solved by looking ity to retrieve evidence and verify claims and (2) only at the hypothesis, while Feng et al. (2019) to show that engineering the incentive structure show that artifacts can infect the premise as well. of data collection experiments can produce more This is especially common in the biggest datasets accurate and realistic outcomes. for NLI such as SNLI and MNLI (Gururangan et al., This dataset lends itself to automatic training 2018). While there are algorithmic solutions to and it characterizes what factual errors humans can addressing these issues (Utama et al., 2020), many most easily detect and which are most likely to have turned to building better datasets. fool them (Section 3.2). This is analogous to Both Bowman et al. (2020) and Vania et al. the creation of unsupported or refuted claims in (2020) propose alternative methods for collecting the wild, which are not random, but evolve as part entailment pairs from crowdworkers and measure of an information arms race (Rid, 2020). Unlike success via improvements in other general tasks previous datasets that rely on crowd-sourcing, we via transfer learning. While the proposed methdevelop an online game to"
2021.naacl-main.32,2020.aacl-main.68,0,0.0488249,"Missing"
2021.naacl-main.32,Q19-1029,1,0.844589,"t to keep claims that are easy to paired with evidence that either supports or refutes identify as true or false. If the average player can each claim. One prerequisite for this is that claims tell through artifacts or common sense that a claim avoid high lexical overlap with the knowledge corwill not be supported, it is uninteresting as an enpus. We thus need to encourage authors to craft claims that cannot be trivially matched to evidence. tailment example. For example, if someone sees While this approach has been used for question an- the claim “Tipper Gore was born in 1048” and reswering (Wallace et al., 2019; Bartolo et al., 2020), members that Al Gore was the vice president of the United States in the twentieth century, they can which has a similar retrieval step, to our knowledge identify that this claim is false. We also want claims it has not been applied to entailment or FEVER. We recruit users employed at Google, all pro- that require the voters to carefully read evidence from Wikipedia (Table 4). Voters can ask for hints ficient in English, to play-test the game. At the beginning of each round, we ask each user to gener- provided by our evidence selection system (Section 4.1). For each pie"
2021.naacl-main.32,N18-1101,0,0.0946708,"Missing"
2021.naacl-main.32,P19-1085,0,0.0242991,"i et al., 2002). We remove 1598 examples 7 where the precision was less than 0.5. We evaluate two baselines. The first one follows Chen et al. (2017) and uses a TF - IDF retrieval model with unigrams and bigrams and 220 hash buckets. The title of page is added to the passage content for additional context. The second baseline uses Dense Passage Retrieval (Karpukhin et al., 2020, DPR), using the same fixed pre-trained passage embeddings and query encoder as the ones used in Petroni et al. (2021). 4.2 Entailment For the second component of the task, we follow state-of-the-art entailment models (Zhou et al., 2019; Liu et al., 2020; Eisenschlos et al., 2020): 5 Unlike FEVER, we do not allow authors to write claims that lack “enough information”. 6 http://github.com/facebookresearch/KILT/ 7 This happens because F OOL M E T WICE was constructed from a more recent version of Wikipedia than KILT. 356 Topic Claim Iceland After ending its personal union with Denmark, Iceland was last invaded by the United Kingdom in Operation Fork. Elizabeth II After Elizabeth II’s accession, she changed her house’s name from &quot;House of Saxe-Coburg and Gotha&quot; to the &quot;House of Windsor&quot;, rejecting the &quot;House of Edinburgh&quot;. Retr"
2021.naacl-main.368,N19-1423,0,0.0417048,"Missing"
2021.naacl-main.368,P19-1612,0,0.0510493,"on 2) in the dense space to find and cache the most relevant candidate chains structured text of Wikipedia with its structured hyperlinks. While they show promise on bench- and iteratively compose the query by appending marks, it’s difficult to extend them beyond aca- the retrieval history. We improve the retrieval by encouraging the representation to discriminate hard demic testbeds because real-world datasets often negative evidence chains from the correct chains, lack this structure. For example, medical records which are refreshed by the model. lack links between reports. Dense retrieval (Lee et al., 2019; Guu et al., We evaluate Beam Dense Retrieval (B EAM DR) 2020; Karpukhin et al., 2020, inter alia) provides a on H OT P OT QA (Yang et al., 2018), a multi4635 1 Introduction Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4635–4641 June 6–11, 2021. ©2021 Association for Computational Linguistics hop question answering benchmark. When retrieving evidence chains directly from the corpus (full retrieval), B EAM DR is competitive to the state-of-the-art cascade reranking systems that use Wikipedi"
2021.naacl-main.368,D18-1167,0,0.0748215,"oduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of Nawhich is the focus of this work. Given a complex question, researcher"
2021.naacl-main.368,D19-1605,1,0.852995,"y, and use traditional sparse IR systems to select the passage, which complements B EAM DR’s approach. 6 Conclusion We introduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zha"
2021.naacl-main.368,2020.emnlp-main.466,0,0.0605023,"sparse IR systems to select the passage, which complements B EAM DR’s approach. 6 Conclusion We introduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by t"
2021.naacl-main.368,D18-1134,1,0.845231,"EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of Nawhich is the focus of this work. Given a complex question, researchers have in- tional Intelligence (ODNI), Intelligence Advanced Research P"
2021.naacl-main.368,D19-1258,0,0.0265495,"m size Figure 2 plots the Passage EM with different beam sizes. While initially increassing the beam size improves Passage Exact Match, the marginal improvement decreases after a beam size of forty. 3.3 Answer Extraction Evaluation Baselines We compare B EAM DR with (Zhao et al., 2020b), GRR (Asai et al., 2020) and the contemporaneous MDR (Xiong et al., 2021b). We use released code from GRR (Asai et al., 2020) following its settings on BERT base and large. We use four 2080Ti GPUs. TXH Baselines We compare B EAM DR with TF - IDF, Results Using the same implementation but on Semantic Retrieval (Nie et al., 2019, SR), which our reranked chains, B EAM DR outperforms GRR 4637 Retriever Reader Dev EM F1 Test EM F1 Models BERT base Reader 54.0 GRR GRR 52.7 B EAM DR GRR 54.9 BERT large wwm Reader GRR GRR 60.5 B EAM DR GRR 61.3 MDR ∗ MDR ∗ 61.5 ELECTRA large Reader MDR ∗ MDR ∗ 63.4 TXH TXH 66.2 65.8 68.0 51.6 - 64.1 - 73.3 74.1 74.7 60.0 60.4 - 73.0 73.2 - 76.2 62.3 75.3 GRR B EAM DR B EAM DR† Passage Recall First hop Second hop 85.1 86.4 88.0 85.3 78.9 87.1 Overlap 64.3 26.7 14.7 Table 3: Passage Recall and overlap comparison between B EAM DR and GRR with different hop passages. Systems with † filter seco"
2021.naacl-main.368,N15-1117,1,0.739585,"ch complements B EAM DR’s approach. 6 Conclusion We introduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of Nawhich is the f"
2021.naacl-main.368,D19-1261,0,0.179797,"a film starring Nicolas Cage and T´ea Leoni?” to the context “The Family Man is a 2000 American film written by David Diamond and David Weissman, and starring Nicolas Cage and T´ea Leoni.”. 5 Related Work step retrieval to select the first hop passages (or entity mentions), then find the next hop candidates directly from Wikipedia links and rerank them. Like B EAM DR, Asai et al. (2020) use beam search to find the chains but still rely on a graph neural network over Wikipedia links. B EAM DR retrieves evidence chains through dense representations without relying on the corpus semi-structure. Qi et al. (2019, 2020) iteratively generate the query from the question and retrieved history, and use traditional sparse IR systems to select the passage, which complements B EAM DR’s approach. 6 Conclusion We introduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should in"
2021.naacl-main.368,N18-1059,0,0.0566839,"al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of Nawhich is the focus of this work. Given a complex question, researchers have in- tional Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the BETvestigated multi-step retrieval techniques to find TER Program contract 2019-19051600005. Boydan evidence chain. Knowledge graph question Graber is supported by NSF Grant IIS-1822494. answering approaches (Talmor and Berant, 2018; Any opinions, findings, conclusions, or recommenZhang et al., 2018, inter alia) directly search the dations expressed here are those of the authors and evidence chain from the knowledge graph, but falter when KG coverage is sparse. With the re- do not necessarily reflect the view of the sponsors. lease of large-scale datasets (Yang et al., 2018), recent systems (Nie et al., 2019; Zhao et al., 2020b; Asai et al., 2020; Dhingra et al., 2020, inter alia) use Wikipedia abstracts (the first paragraph of a Wikipedia page) as the corpus to retrieve the evidence chain. Dhingra et al. (2020) treat Wi"
2021.naacl-main.368,D18-1259,0,0.374635,"they show promise on bench- and iteratively compose the query by appending marks, it’s difficult to extend them beyond aca- the retrieval history. We improve the retrieval by encouraging the representation to discriminate hard demic testbeds because real-world datasets often negative evidence chains from the correct chains, lack this structure. For example, medical records which are refreshed by the model. lack links between reports. Dense retrieval (Lee et al., 2019; Guu et al., We evaluate Beam Dense Retrieval (B EAM DR) 2020; Karpukhin et al., 2020, inter alia) provides a on H OT P OT QA (Yang et al., 2018), a multi4635 1 Introduction Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4635–4641 June 6–11, 2021. ©2021 Association for Computational Linguistics hop question answering benchmark. When retrieving evidence chains directly from the corpus (full retrieval), B EAM DR is competitive to the state-of-the-art cascade reranking systems that use Wikipedia links. Combined with standard reranking and answer span extraction modules, the gain from full retrieval propagates to findings answers (Section"
C18-1144,K15-1020,1,0.628583,"ncodes a specific instance indexed by i, annotation value c0 , and label value c. Finally, let noise parameters j for annotator j be tied to a confusion matrix with Dirichlet-distributed rows such that jk selects the value at cell (c, c0 ), encoding how likely j is to produce annotation c0 when shown a document whose true label is c. These settings recover the traditional item-response crowdsourcing model (Dawid and Skene, 1979). Using the same settings but defining p(y |x, θ) ∝ exp[θT f (x, y)], recovers a popular data-conditional crowdsourcing model (Raykar et al., 2010; Yan et al., 2014; Felt et al., 2015b). However, existing crowdsourcing models lack the representational richness of the measurements framework; we address this lacuna in the next section. 4 Per-annotator Normal Measurement Model for Classification Having shown how the annotator measurements framework can capture existing crowdsourcing models, this section presents a novel crowdsourcing model that instantiates the richness of the measurements framework that we use in Sections 5–6. For brevity, we refer to this model as PAN (per-annotator normal) measurement model.2 The generative story is: 1. Draw a stochastic vector θ over C cl"
C18-1144,N15-1089,1,0.778119,"ncodes a specific instance indexed by i, annotation value c0 , and label value c. Finally, let noise parameters j for annotator j be tied to a confusion matrix with Dirichlet-distributed rows such that jk selects the value at cell (c, c0 ), encoding how likely j is to produce annotation c0 when shown a document whose true label is c. These settings recover the traditional item-response crowdsourcing model (Dawid and Skene, 1979). Using the same settings but defining p(y |x, θ) ∝ exp[θT f (x, y)], recovers a popular data-conditional crowdsourcing model (Raykar et al., 2010; Yan et al., 2014; Felt et al., 2015b). However, existing crowdsourcing models lack the representational richness of the measurements framework; we address this lacuna in the next section. 4 Per-annotator Normal Measurement Model for Classification Having shown how the annotator measurements framework can capture existing crowdsourcing models, this section presents a novel crowdsourcing model that instantiates the richness of the measurements framework that we use in Sections 5–6. For brevity, we refer to this model as PAN (per-annotator normal) measurement model.2 The generative story is: 1. Draw a stochastic vector θ over C cl"
C18-1144,W10-0105,1,0.779298,"label accuracy and p∗ (x) is the empirical distribution. In reality the true values y are unobservable, but we can expect over them using our posterior approximation q˜; thus Rq˜ = Ep∗(x) maxyˆ Eq˜(y) [r(y, yˆ)] . With a label accuracy reward function the expected reward simpliP P (y) fies to Rq˜ = ˆ) = i maxyˆ νiˆ i maxyˆ q(yi = y y . For simplicity, we set the cost function Cq˜ to a constant, but leave it in the equations since future work should estimate and use annotation cost. By default, Algorithm 1 jointly selects an annotator j and measurement k, but it could be used in other ways. Haertel et al. (2010) argue that in realistic scenarios the active learning algorithm typically cannot control when annotators are available but rather must respond to annotator requests for work. Algorithm 1 1701 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: function M EASUREMENT S ELECTION(τ0 ) q0 ← Inference(τ0 ) while more measurements are desired do j, k ← NextMeasurement(τ0 , q0 ) τ0 ← τ0 ∪ ObserveMeasurement(j, k) q0 ← Inference(τ0 ) return q function N EXT M EASUREMENT(τ0 , q0 ) for annotator j do for measurement feature k do draw t samples from p(τjk |τ0 , q0 ) for sampled τjkt do τ1 ← τjkt ∪ τ0"
C18-1144,N13-1132,0,0.0456522,"nchev et al., 2010). Ganchev et al. (2010) explain each of these three frameworks can be derived as a special case of the learning from measurements framework of Liang et al. (2009) by making particular approximations for the sake of tractability. Traditional corpus construction assesses inferred label quality using annotator agreement heuristics such as Krippendorff’s alpha (Krippendorff, 2012). Passonneau and Carpenter (2014) argue that inference in probabilistic models yields higher quality labels at lower cost, and should be preferred over agreement heuristics. Among crowdsourcing models, Hovy et al. (2013) include Bernoulli switching variables to identify and eliminate malicious contributors (spammers). Raykar and Yu (2012) iteratively 1702 run inference and exclude problematic annotators in order to eliminate spammers. Raykar et al. (2010), Yan et al. (2014), and Felt et al. (2015a) model data jointly with labels, allowing patterns in the data to inform inferred labels. Simpson and Roberts (2015) model annotator dynamics, tracking the ways that annotator decision making changes over time in response to factors such as training and fatigue. Welinder et al. (2010) and Whitehill et al. (2009) bot"
C18-1144,N13-1062,0,0.0265065,"icht. Unser Modell, eine spezifische Instanz dieses Rahmens, schneidet im Vergleich zu fr¨uheren Arbeiten positiv ab. Dar¨uber hinaus erm¨oglicht es die aktive Stichprobenauswahl, indem Kommentator, Datenelement, und Annotationsstruktur gemeinsam ausgew¨ahlt werden, um den Annotationskosten zu reduzieren. 1 Introduction Supervised machine learning is data hungry: new approaches require massive training sets. These training sets can come from inexpensive crowdsourcing platforms, but consistency is often sacrificed for speed and thrift. Sophisticated models (Surowiecki, 2005; Snow et al., 2008; Jurgens, 2013) can overcome the intrinsic annotation noise by reconciling redundant annotation and predicting true labels by modeling the error patterns associated with individual labels, documents, or annotators. These models have typically assumed that we collect document-level labels and nothing else from annotators. But crowd workers could provide other valuable information. For example, if we wanted to predict the sentiment of documents about the weather (Figure 1), intuition says that words like “sunny”, This work is licensed under a Creative Commons Attribution 4.0 International License. License deta"
C18-1144,Q14-1025,0,0.0748319,"prior knowledge include constraint-driven learning based on integer linear programming (Chang et al., 2008), generalized expectation criteria (Druck et al., 2008), and the posterior regularization (Ganchev et al., 2010). Ganchev et al. (2010) explain each of these three frameworks can be derived as a special case of the learning from measurements framework of Liang et al. (2009) by making particular approximations for the sake of tractability. Traditional corpus construction assesses inferred label quality using annotator agreement heuristics such as Krippendorff’s alpha (Krippendorff, 2012). Passonneau and Carpenter (2014) argue that inference in probabilistic models yields higher quality labels at lower cost, and should be preferred over agreement heuristics. Among crowdsourcing models, Hovy et al. (2013) include Bernoulli switching variables to identify and eliminate malicious contributors (spammers). Raykar and Yu (2012) iteratively 1702 run inference and exclude problematic annotators in order to eliminate spammers. Raykar et al. (2010), Yan et al. (2014), and Felt et al. (2015a) model data jointly with labels, allowing patterns in the data to inform inferred labels. Simpson and Roberts (2015) model annotat"
C18-1144,D08-1027,0,0.235002,"Missing"
D07-1109,W99-0901,0,0.797076,"the ubiquitous hidden Markov model is a series of mixture models chained together.) Thus, developing a generative model for WSD gives other generative NLP algorithms a natural way to take advantage of the hidden senses of words. We develop latent Dirichlet allocation with W ORD N ET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable. We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpus and learning the domains in which to consider each word. Using the W ORD N ET hierarchy, we embed the construction of Abney and Light (1999) in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts. 1 In general, topic models are statistical models of text that posit a hidden space of topics in which the corpus is embedded (Blei et al., 2003). Given a corpus, posterior inference in topic models amounts to automatically discovering the underlying themes that permeate the collection. Topic models have recently been applied to information retrieval (Wei and Croft, 2006), text classification (Blei et al., 2003), and dialogue segmentation (Purver et al., 2006). Introduction Wo"
D07-1109,C00-1028,0,0.00946652,"of W ORD N ET’s breadth, rare senses also impact disambiguation. For example, the metonymical use of “door” to represent a whole building as in the phrase “girl next door” is under the same parent as sixty other synsets containing “bridge,” “balcony,” “body,” “arch,” “floor,” and “corner.” Surrounded by such common terms that are also likely to co-occur with the more conventional meanings of door, this very rare sense becomes the preferred disambiguation of “door.” 6 Related Work Abney and Light’s initial probabilistic WSD approach (1999) was further developed into a Bayesian network model by Ciaramita and Johnson (2000), who likewise used the appearance of monosemous terms close to ambiguous ones to “explain away” the usage of ambiguous terms in selectional restrictions. We have adapted these approaches and put them into 1031 the context of a topic model. Recently, other approaches have created ad hoc connections between synsets in W ORD N ET and then considered walks through the newly created graph. Given the difficulties of using existing connections in W ORD N ET, Mihalcea (2005) proposed creating links between adjacent synsets that might comprise a sentence, initially setting weights to be equal to the L"
D07-1109,H92-1045,0,0.406967,"Missing"
D07-1109,O97-1002,0,0.0300408,"Missing"
D07-1109,S01-1027,0,0.0180793,"nnections between synsets in W ORD N ET and then considered walks through the newly created graph. Given the difficulties of using existing connections in W ORD N ET, Mihalcea (2005) proposed creating links between adjacent synsets that might comprise a sentence, initially setting weights to be equal to the Lesk overlap between the pairs, and then using the PageRank algorithm to determine the stationary distribution over synsets. 6.1 Topics and Domains Yarowsky was one of the first to contend that “there is one sense for discourse” (1992). This has lead to the approaches like that of Magnini (Magnini et al., 2001) that attempt to find the category of a text, select the most appropriate synset, and then assign the selected sense using domain annotation attached to W ORD N ET. LDAWN is different in that the categories are not an a priori concept that must be painstakingly annotated within W ORD N ET and require no augmentation of W ORD N ET. This technique could indeed be used with any hierarchy. Our concepts are the ones that best partition the space of documents and do the best job of describing the distinctions of diction that separate documents from different domains. 6.2 Similarity Measures Our appr"
D07-1109,P04-1036,0,0.151886,"Missing"
D07-1109,H05-1052,0,0.0190122,"Abney and Light’s initial probabilistic WSD approach (1999) was further developed into a Bayesian network model by Ciaramita and Johnson (2000), who likewise used the appearance of monosemous terms close to ambiguous ones to “explain away” the usage of ambiguous terms in selectional restrictions. We have adapted these approaches and put them into 1031 the context of a topic model. Recently, other approaches have created ad hoc connections between synsets in W ORD N ET and then considered walks through the newly created graph. Given the difficulties of using existing connections in W ORD N ET, Mihalcea (2005) proposed creating links between adjacent synsets that might comprise a sentence, initially setting weights to be equal to the Lesk overlap between the pairs, and then using the PageRank algorithm to determine the stationary distribution over synsets. 6.1 Topics and Domains Yarowsky was one of the first to contend that “there is one sense for discourse” (1992). This has lead to the approaches like that of Magnini (Magnini et al., 2001) that attempt to find the category of a text, select the most appropriate synset, and then assign the selected sense using domain annotation attached to W ORD N"
D07-1109,H93-1061,0,0.0961899,"zed hill climbing algorithm on the posterior likelihood as a function of the configuration of hidden variables. The numerator of Equation 1 is proportional to that posterior and thus allows us to track the sampler’s progress. We assess convergence to a local mode of the posterior by monitoring this quantity. 4 Experiments In this section, we describe the properties of the topics induced by running the previously described Gibbs sampling method on corpora and how these topics improve WSD accuracy. Of the two data sets used during the course of our evaluation, the primary dataset was S EM C OR (Miller et al., 1993), which is a subset of the Brown corpus with many nouns manually labeled with the correct W ORD N ET sense. The words in this dataset are lemmatized, and multi-word expressions that are present in W ORD N ET are identified. Only the words in S EM C OR were used in the Gibbs sampling procedure; the synset assignments were only used for assessing the accuracy of the final predictions. We also used the British National Corpus, which is not lemmatized and which does not have multiword expressions. The text was first run through a lemmatizer, and then sequences of words which matched a multi-word e"
D07-1109,P06-1003,0,0.0544226,"Missing"
D10-1005,baccianella-etal-2010-sentiwordnet,0,0.0233095,"nference from corpora. We show M L SLDA can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment. Sentiment analysis (Pang and Lee, 2008) offers the promise of automatically discerning how people feel about a product, person, organization, or issue based on what they write online, which is potentially of great value to businesses and other organizations. However, the vast majority of sentiment resources and algorithms are limited to a single language, usually English (Wilson, 2008; Baccianella and Sebastiani, 2010). Since no single language captures a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when text in multiple languages is taken into account. In this paper, we introduce Multilingual Supervised Latent Dirichlet Allocation (M L SLDA), a model for sentiment analysis on a multilingual corpus. M L SLDA discovers a consistent, unified picture of sentiment across multiple languages by learning “topics,” probabilistic partitions of the vocabulary that are consistent in terms of"
D10-1005,D08-1014,0,0.0881432,"Missing"
D10-1005,D07-1109,1,0.703866,"topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge, since it is well known, offers convenient and intuitive terminology, and demonstrates the full flexibility of our approach. However, the model we describe generalizes to any tree-structured representation of multilingual knowledge; we"
D10-1005,N09-1009,0,0.0113895,"1 The latter property has also made LDA popular for information retrieval (Wei and Croft, 2006)). 46 p(hˇao|z) all tend to be high at the same time, or low at the same time. More generally, the structure of our model must encourage topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge,"
D10-1005,N09-1057,1,0.455555,"ary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, M L SLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws away information important to sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which wou"
D10-1005,P08-1088,0,0.0205951,"Missing"
D10-1005,isahara-etal-2008-development,0,0.013282,"“is a” more specific instantiation of its parent concept (thus, hyponomy is often called an “isa” relationship). For example, a “dog” is a “canine” is an “animal” is a “living thing,” etc. As an approximation, it is not unreasonable to assume that WordNet’s structure of meaning is language independent, i.e. the concept encoded by a synset can be realized using terms in different languages that share the same meaning. In practice, this organization has been used to create many alignments of international WordNets to the original English WordNet (Ordan and Wintner, 2007; Sagot and Fiˇser, 2008; Isahara et al., 2008). Using the structure of WordNet, we can now describe a generative process that produces a distribution over a multilingual vocabulary, which encourages correlations between words with similar meanings regardless of what language each word is in. For each synset h, we create a multilingual word distribution for that synset as follows: 1. Draw transition probabilities βh ∼ Dir (τh ) 2. Draw stop probabilities ωh ∼ Dir (κh ) 3. For each language l, draw emission probabilities for that synset φh,l ∼ Dir (πh,l ). For conciseness in the rest of the paper, we will refer to this generative process as"
D10-1005,P10-1117,0,0.0172073,"a” (Chinese for “I’m afraid that . . . ”) and “tuo” (Chienese for “discard”), and positive sentiment-bearing topics have reasonable words such as “great,” “good,” and “juwel” (German for “jewel”). The qualitative topics also betray some of the weaknesses of the model. For example, in one of the negative sentiment topics, the German word “gut” (good) is present. Because topics are distributions over words, they can encode the presence of negations like “kein” (no) and “nicht” (not), but not collocations like “nicht gut.” More elaborate topic models that can model local syntax and collocations (Johnson, 2010) provide options for addressing such problems. We do not report the results for sentiment prediction for this corpus because the baseline of predicting a positive review is so strong; most algorithms do extremely well by always predicting a positive review, ours included. 4.3 Sentiment Prediction We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over god us religion church human himmel gedanken glaube unsere kirche wahrheit -1.2 diet food eat weight eating healthy fat -0.4 book books one life per"
D10-1005,2005.mtsummit-papers.11,0,0.0134009,"distinct from the per-language expression. with these techniques, constructing appropriate hierarchies from these resources required many arbitrary decisions about cutoffs and which words to include. Thus, we do not consider them in this paper. 4 Experiments We evaluate M L SLDA on three criteria: how well it can discover consistent topics across languages for matching parallel documents, how well it can discover sentiment-correlated word lists from nonaligned text, and how well it can predict sentiment. 4.1 Matching on Multilingual Topics We took the 1996 documents from the Europarl corpus (Koehn, 2005) using three bridges: GermaNet, dictionary, and the uninformative flat matching.4 The model is unaware that the translations of documents in one language are present in the other language. Note that this does not use the supervised framework 4 For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content w"
D10-1005,P07-1055,0,0.0135154,"s to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, M L SLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws away information important to sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predic"
D10-1005,D09-1092,0,0.0675304,"enough data, a monolingual model is no longer helped by adding additional multilingual data. 5 Relationship to Previous Research The advantages of M L SLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another langu"
D10-1005,P05-1015,0,0.0171248,"gt” (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of “gut” (good) in one of the negative sentiment topics, showing the difficulty of learning collocations. Train DE EN EN + DE Test DE DE DE GermaNet 73.8 7.44 1.17 Dictionary 24.8 2.68 1.46 Flat 92.2 18.3 1.39 Table 1: Mean squared error on a film review corpus. All results are on the same German test data, varying the training data. Over-fitting prevents the model learning on the German data alone; adding English data to the mix allows the model to make better predictions. 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus.6 The results for predicting sentiment in German documents with 25 topics are presented in Table 1. On a small monolingual corpus, prediction is very poor. The model over-fits, especially when it has the entire vocabulary to select from. The slightly better performance using GermaNet and a dictionary as topic priors can be viewed as basic feature selection, removing proper names from the vocabulary to 6 We followed Pang and Lee’s method for creating a numerical score between 0 and 1 from a star rating. We then converted that to an integer by multipl"
D10-1005,P06-1003,0,0.0410904,"Missing"
D10-1005,P95-1050,0,0.0911089,"Missing"
D10-1005,W03-0404,0,0.0344506,"able. Although GermaNet is richer, its coverage is incomplete; the dictionary structure had a much larger vocabulary and could build a more complete multilingual topics. Using comparable input information, this more flexible model performed better on the matching task than the existing multilingual topic model available for unaligned text. The degenerate flat bridge did no better than the baseline of random guessing, as expected. 4.2 Qualitative Sentiment-Correlated Topics One of the key tasks in sentiment analysis has been the collection of lists of words that convey sentiment (Wilson, 2008; Riloff et al., 2003). These resources are often created using or in reference to resources like WordNet (Whitelaw et al., 2005; Baccianella and Sebastiani, 2010). M L SLDA provides a method for extracting topical and sentimentcorrelated word lists from multilingual corpora. If was updated more frequently. 51 a WordNet-like resource is used as the bridge, the resulting topics are distributions over synsets, not just over words. As our demonstration corpus, we used the Amherst Sentiment Corpus (Constant et al., 2009), as it has documents in multiple languages (English, Chinese, and German) with numerical assessment"
D10-1005,P08-1036,0,0.0331898,"ation. For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.). Future work could also more rigorously validate that the multilingual topics discovered by M L SLDA are sentiment-bearing via human judgments. In contrast, M L SLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008). Extending these models to multilingual data would be more straightforward. 6 Conclusions M L SLDA is a “holistic” statistical model for multilingual corpora that does not require parallel text or expensive multilingual resources. It discovers connections across languages that can recover latent structure in parallel corpora, discover sentimentcorrelated word lists in multiple languages, and make accurate predictions across languages that improve with more multilingual data, as demonstrated in the context of sentiment analysis. More generally, M L SLDA provides a formalism that can be used to"
D10-1005,P09-1027,0,0.0257049,"y, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, M L SLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws a"
D10-1005,P06-2124,0,0.282652,"ing additional multilingual data. 5 Relationship to Previous Research The advantages of M L SLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, M L SLDA views all languages through the lens of the top"
D10-1005,P06-4018,0,\N,Missing
D10-1005,I05-3027,0,\N,Missing
D10-1005,W02-0109,0,\N,Missing
D10-1028,N10-1081,0,0.298245,", in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıve Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association"
D10-1028,N09-1057,1,0.894952,"Missing"
D10-1028,P07-1107,0,0.0220563,"st to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can “grow” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıve Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB"
D10-1028,N09-1036,0,0.0230009,"er takes in a set of documents and replaces all words not in the vocabulary with “out of vocabulary” markers. This process ensures that in all experiments the vocabulary is composed entirely of words from the training set. After the groups have been filtered, the group used as the test set has its labels removed. The test and training set are then sent, along with the grammar, into the adaptor grammar inference engine. Each experiment ran for 3000 iterations. For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10 respectively, then slice sample (Johnson and Goldwater, 2009). We use the resulting sentence parses for classification. By design of the grammar, each sentence’s words will belong to one and only one distribution. We identify that distribution from each of the test set sentence parses and use it as the sentence level classification for that particular sentence. We then use majority rule on the individual sentence classifications in a document to obtain the document classification. (In most cases the sentence-level assignments are overwhelmingly dominated by one class.) 3.3 Results and Analysis Table 4 gives the results and compares to prior work. The su"
D10-1028,P08-1046,0,0.112975,"y of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıve Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, c MIT, Massachusetts, U"
D10-1028,P10-1117,0,0.733562,"known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıve Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics by the properties of th"
D10-1028,D07-1072,0,0.0190581,"ity. In this paper, we employ nonparametric Bayesian models (Orbanz and Teh, 2010) in order to address this limitation. In contrast to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can “grow” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıv"
D10-1028,W06-2915,0,0.768987,"es associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics by the properties of the dataset. We show that using adaptive na¨ıve Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al., 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. In Section 2, we review adaptor grammars, show how na¨ıve Bayes can be expressed within the formalism, and describe how — and how easily — an adaptive na¨ıve Bayes model can be created. Section 3 validates the approach via experimentation on the Bitter Lemons corpus. In Section 4, we summarize the contributions of the paper and discuss directions for future work. 2 Adapting Na¨ıve Bayes to be Less Na¨ıve In this work we apply the adaptor grammar formalism introduced by Johnson, Griffiths,"
D12-1118,C04-1046,0,0.0232506,"Missing"
D12-1118,P07-1107,0,0.122907,"ent or estimating the difficulty of the question. Mislead by the Content Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the closest match it can find is “Yasunari Kawabata”, who wrote the novel Snow Co"
D12-1118,D10-1028,1,0.844115,"re aggressive strategy, perhaps incorporating the identity of the opponent or estimating the difficulty of the question. Mislead by the Content Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the clo"
D12-1118,P11-1106,0,0.0372283,"nd broader corpora for training. In addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (Wood and Teh, 2009). We are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (Waugh et al., 2011). Further afield, our presentation of sentences closely resembles paradigms for cognitive experiments in linguistics (Thibadeau et al., 1982) but are much cheaper to conduct. If online processing effects (Levy et al., 2008; Levy, 2011) could be observed in buzzing behavior; e.g., if a confusingly worded phrase depresses buzzing probability, it could help validate cognitively-inspired models of online sentence processing. Incremental classification is a natural problem, both for humans and resource-limited machines. While our data set is trivial (in a good sense), learning how humans process data and make decisions in a cheap, easy crowdsourced application can help us apply new algorithms to improve performance in settings where features aren’t free, either because of computational or annotation cost. Acknowledgments We than"
D12-1118,P00-1071,0,0.0211785,"tent Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the closest match it can find is “Yasunari Kawabata”, who wrote the novel Snow Country, whose plot matches some of these keywords. To answer these"
D12-1118,J08-4003,0,0.0109721,"rning tech- answers might also help your opponent, e.g., by elimnique (Langford and Zadrozny, 2005; Abbeel and inating an incorrect answer. Moreover, strategies in a Ng, 2004; Syed et al., 2008): given a representation game setting (rather than a single question) are more of the state space, learn a classifier that can map from complicated. For example, if a right answer is worth a state to an action. This is also a common paradigm +10 points and the penalty for an incorrect question for other incremental tasks, e.g., shift-reduce pars- is −5, then a team leading by 15 points on the last ing (Nivre, 2008). question should never attempt to answer. InvestigatGiven examples of the correct answer given a con- ing such gameplay strategies would require a “roll figuration of the state space, we can learn a MDP out” of game states (Tesauro and Galperin, 1996) to without explicitly representing the reward function. explore the efficacy of such strategies. While interIn this section, we define our method of defining esting, we leave these issues to future work. actions and our representation of the state space. We also investigated learning a policy directly from users’ buzzes directly (Abbeel and Ng,"
D12-1118,W11-1808,0,0.0266342,"Missing"
D12-1118,D11-1136,0,0.032474,"al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify documents incrementally is quiz bowl, an academic competition between schools in English-speaking countries; hundreds of teams compete in dozens of tournaments each year (Jennings, 2006). Note the distinction between quiz bowl and Jeopardy, a recent application area (Ferrucci et al., 2010). While Jeopardy also uses signaling devices, these are only usable afte"
D12-1118,P06-4018,0,\N,Missing
D12-1118,W02-0109,0,\N,Missing
D14-1070,W13-3214,0,0.0242311,"vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed representations of paragraphs. Here we present a simpler approach where a single model is able to learn complex sentence representations and average them across paragraphs. 6.2 Factoid Question-Answering Factoid question answering is often functionally equivalent to information retrieval. Given a knowledge base and a query, the goal is to Thomas Mann Henrik Ibsen Joseph Conrad Henry James Franz Kafka Figure 6: A question on the German novelist Thomas Mann that contains no named en"
D14-1070,D12-1118,1,0.767192,"s have a property called pyramidality, which means that sentences early in a question contain harder, more obscure clues, while later sentences are “giveaways”. This design rewards players with deep knowledge of a particular subject and thwarts bag of words methods. Sometimes the first sentence contains no named entities—answering the question correctly requires an actual understanding of the sentence (Figure 1). Later sentences, however, progressively reveal more well-known and uniquely identifying terms. Previous work answers quiz bowl questions using a bag of words (na¨ıve Bayes) approach (Boyd-Graber et al., 2012). These models fail on sentences like the first one in Figure 1, a typical hard, initial clue. Recursive neural networks (rnns), in contrast to simpler models, can capture the compositional aspect of such sentences (Hermann et al., 2013). rnns require many redundant training examples to learn meaningful representations, which in the quiz bowl setting means we need multiple questions about the same answer. Fortunately, hundreds of questions are produced during the school year for quiz bowl competitions, yielding many different examples of questions asking about any entity of note (see Section 4"
D14-1070,de-marneffe-etal-2006-generating,0,0.0211844,"Missing"
D14-1070,W13-0112,0,0.0150837,"ronald_reagan woodrow_wilson Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al"
D14-1070,P13-1088,0,0.00846746,". from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed representations of paragraphs. Here we present a simpler approach where a single model is able to learn complex sentence representations and average them acr"
D14-1070,W13-3209,0,0.0158745,"rts bag of words methods. Sometimes the first sentence contains no named entities—answering the question correctly requires an actual understanding of the sentence (Figure 1). Later sentences, however, progressively reveal more well-known and uniquely identifying terms. Previous work answers quiz bowl questions using a bag of words (na¨ıve Bayes) approach (Boyd-Graber et al., 2012). These models fail on sentences like the first one in Figure 1, a typical hard, initial clue. Recursive neural networks (rnns), in contrast to simpler models, can capture the compositional aspect of such sentences (Hermann et al., 2013). rnns require many redundant training examples to learn meaningful representations, which in the quiz bowl setting means we need multiple questions about the same answer. Fortunately, hundreds of questions are produced during the school year for quiz bowl competitions, yielding many different examples of questions asking about any entity of note (see Section 4.1 for more details). Thus, we have built-in redundancy (the number of “askable” entities is limited), but also built-in diversity, as difficult clues cannot appear in every question without becoming well-known. Dependency-Tree Recursive"
D14-1070,P14-1136,0,0.00701547,"ector hs . The error for the sentence is C(S, θ) = XX L(rank(c, s, Z))max(0, s∈S z∈Z 1 − xc · hs + xz · hs ), (5) where the function rank(c, s, Z) provides the rank of correct answer c with respect to the incorrect answers Z. We transform this rank into a loss function4 shown by Usunier et al. (2009) to optimize the top of the ranked list, r P L(r) = 1/i. i=1 Since rank(c, s, Z) is expensive to compute, we approximate it by randomly sampling K incorrect answers until a violation is observed (xc · hs < 1 + xz · hs ) and set rank(c, s, Z) = (|Z |− 1)/K, as in previous work (Weston et al., 2011; Hermann et al., 2014). The model minimizes the sum of the error over all sentences T normalized by the number of nodes N in the training set, J(θ) = 1 X C(t, θ). N (6) t∈T The parameters θ = (Wr∈R , Wv , We , b), where R represents all dependency relations in the data, are optimized using AdaGrad(Duchi et al., 2011).5 In Section 4 we compare performance to an identical model (fixed-qanta) that excludes answer vectors from We and show that training them as part of θ produces significantly better results. The gradient of the objective function, ∂C 1 X ∂J(t) = , ∂θ N ∂θ (7) t∈T is computed using backpropagation throu"
D14-1070,P14-1105,1,0.253879,"ion described below. While we are not interested in obtaining a ranked list of answers,3 we observe better performance by adding the weighted approximaterank pairwise (warp) loss proposed in Weston et al. (2011) to our objective function. where R(n, k) is the dependency relation between node n and child node k. 3.2 Training Our goal is to map questions to their corresponding answer entities. Because there are a limited number of possible answers, we can view this as a multi-class classification task. While a softmax layer over every node in the tree could predict answers (Socher et al., 2011; Iyyer et al., 2014), this method overlooks that most answers are themselves words (features) in other questions (e.g., a question on World 635 Given a sentence paired with its correct answer c, we randomly select j incorrect answers from the set of all incorrect answers and denote this subset as Z. Since c is part of the vocabulary, it has a vector xc ∈ We . An incorrect answer z ∈ Z is also associated with a vector xz ∈ We . We define S to be the set of all nodes in the sentence’s dependency tree, where an individual node s ∈ S is associated with the 2 Of course, questions never contain their own answer as part"
D14-1070,P08-1028,0,0.0784578,"er is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed repr"
D14-1070,P14-1037,0,0.028984,"Missing"
D14-1070,N12-1085,1,0.502253,"641 Q A Q A Q A Q A Q Akbar Muhammad Shah Jahan Ghana A Babur Figure 7: An extremely misleading question about John Cabot, at least to computer models. The words muslim and mecca lead to three Mughal emperors in the top five guesses from qanta; other models are similarly led awry. with ir-wiki. A promising avenue for future work would be to incorporate Wikipedia data into qanta by transforming sentences to look like quiz bowl questions (Wang et al., 2007) and to select relevant sentences, as not every sentence in a Wikipedia article directly describes its subject. Syntax-specific annotation (Sayeed et al., 2012) may help in this regard. Finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion. Having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships. 8 Conclusion We present qanta, a dependency-tree recursive neural network for factoid question answering that outperforms bag of words and information retrieval baselines. Our model improves upon a contrastive max-margin objective function from previous work to dynamically u"
D14-1070,D07-1002,0,0.141453,"z Kafka Figure 6: A question on the German novelist Thomas Mann that contains no named entities, along with the five top answers as scored by qanta. Each cell in the heatmap corresponds to the score (inner product) between a node in the parse tree and the given answer, and the dependency parse of the sentence is shown on the left. All of our baselines, including irwiki, are wrong, while qanta uses the plot description to make a correct guess. return the answer. Many approaches to this problem rely on hand-crafted pattern matching and answer-type classification to narrow down the search space (Shen, 2007; Bilotti et al., 2010; Wang, 2006). More recent factoid qa systems incorporate the web and social media into their retrieval systems (Bian et al., 2008). In contrast to these approaches, we place the burden of learning answer types and patterns on the model. 7 Future Work While we have shown that dt-rnns are effective models for quiz bowl question answering, other factoid qa tasks are more challenging. Questions like what does the aarp stand for? from trec qa data require additional infrastructure. A more apt comparison would be to IBM’s proprietary Watson system (Lally et al., 2012) for Jeop"
D14-1070,D11-1014,1,0.167867,"argin objective function described below. While we are not interested in obtaining a ranked list of answers,3 we observe better performance by adding the weighted approximaterank pairwise (warp) loss proposed in Weston et al. (2011) to our objective function. where R(n, k) is the dependency relation between node n and child node k. 3.2 Training Our goal is to map questions to their corresponding answer entities. Because there are a limited number of possible answers, we can view this as a multi-class classification task. While a softmax layer over every node in the tree could predict answers (Socher et al., 2011; Iyyer et al., 2014), this method overlooks that most answers are themselves words (features) in other questions (e.g., a question on World 635 Given a sentence paired with its correct answer c, we randomly select j incorrect answers from the set of all incorrect answers and denote this subset as Z. Since c is part of the vocabulary, it has a vector xc ∈ We . An incorrect answer z ∈ Z is also associated with a vector xz ∈ We . We define S to be the set of all nodes in the sentence’s dependency tree, where an individual node s ∈ S is associated with the 2 Of course, questions never contain the"
D14-1070,P13-1045,1,0.470518,"the Holy Roman Empire. The first sentence contains no words or named entities that by themselves are indicative of the answer, while subsequent sentences contain more and more obvious clues. where inputs are typically a single sentence and outputs are either continuous or a limited discrete set. Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces. Introduction Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis (Bengio et al., 2003; Socher et al., 2013a; Socher et al., 2013c). The vector spaces learned by these models cluster words and phrases together based on similarity. For example, a neural network trained for a sentiment analysis task such as restaurant review classification might learn that “tasty” and “delicious” should have similar representations since they are synonymous adjectives. These models have so far only seen success in a limited range of text-based prediction tasks, Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed. We describe a task with high-quality ma"
D14-1070,D13-1170,1,0.042951,"the Holy Roman Empire. The first sentence contains no words or named entities that by themselves are indicative of the answer, while subsequent sentences contain more and more obvious clues. where inputs are typically a single sentence and outputs are either continuous or a limited discrete set. Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces. Introduction Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis (Bengio et al., 2003; Socher et al., 2013a; Socher et al., 2013c). The vector spaces learned by these models cluster words and phrases together based on similarity. For example, a neural network trained for a sentiment analysis task such as restaurant review classification might learn that “tasty” and “delicious” should have similar representations since they are synonymous adjectives. These models have so far only seen success in a limited range of text-based prediction tasks, Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed. We describe a task with high-quality ma"
D14-1070,Q14-1017,1,0.765477,"te (see Section 4.1 for more details). Thus, we have built-in redundancy (the number of “askable” entities is limited), but also built-in diversity, as difficult clues cannot appear in every question without becoming well-known. Dependency-Tree Recursive Neural Networks To compute distributed representations for the individual sentences within quiz bowl questions, we use a dependency-tree rnn (dt-rnn). These representations are then aggregated and fed into a multinomial logistic regression classifier, where class labels are the answers associated with each question instance. In previous work, Socher et al. (2014) use dt-rnns to map text descriptions to images. dt-rnns are robust to similar sentences with slightly different syntax, which is ideal for our problem since answers are often described by many sentences that are similar in meaning but different in structure. Our model improves upon the existing dt-rnn model by jointly learning answer and question representations in the same vector space rather than learning them separately. 3.1 Model Description As in other rnn models, we begin by associating each word w in our vocabulary with a vector representation xw ∈ Rd . These vectors are stored as the"
D14-1070,D07-1003,0,0.0464695,"al., 2012) for Jeopardy, which is limited to single sentences, or to models trained on Yago (Hoffart et al., 2013). We would also like to fairly compare qanta 641 Q A Q A Q A Q A Q Akbar Muhammad Shah Jahan Ghana A Babur Figure 7: An extremely misleading question about John Cabot, at least to computer models. The words muslim and mecca lead to three Mughal emperors in the top five guesses from qanta; other models are similarly led awry. with ir-wiki. A promising avenue for future work would be to incorporate Wikipedia data into qanta by transforming sentences to look like quiz bowl questions (Wang et al., 2007) and to select relevant sentences, as not every sentence in a Wikipedia article directly describes its subject. Syntax-specific annotation (Sayeed et al., 2012) may help in this regard. Finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion. Having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships. 8 Conclusion We present qanta, a dependency-tree recursive neural network for factoid question answering that outpe"
D14-1070,D11-1016,0,0.0348678,"on Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to thi"
D14-1070,C10-1142,0,0.00402061,"ane_addams hull_house jimmy_carter ronald_reagan woodrow_wilson Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowle"
D14-1140,P13-2121,0,0.0244753,"Missing"
D14-1140,P98-2141,0,0.421183,"hown”, which is a poor translation. However, it is still comprehensible, and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one. 8 Related Work Just as mt was revolutionized by statistical learning, we suspect that simultaneous mt will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical mt module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) t"
D14-1140,J03-1002,0,0.0606447,"rectly once it has been fed the appropriate source words as input. There are two edge cases: empty input yields an empty output, while a complete, correct source sentence returns the correct, complete translation. Intermediate cases—where the input is either incomplete or incorrect—require using an alignment. The omniscient translator assumes as input a reference translation r, a partial source language input x1:t and a corresponding partial output y. In addition, the omniscient translator assumes access to an alignment between r and x. In practice, we use the hmm aligner (Vogel et al., 1996; Och and Ney, 2003). We first consider incomplete but correct inputs. Let y = τ (x1:t ) be the translator’s output given a partial source input x1:t with translation y. Then, τ (x1:t ) produces all target words yj if there is a source word xi in the input aligned to those words—i.e., (i, j) ∈ ax,y —and all preceding target words can be translated. (That translations must be contiguous is a natural requirement for human recipients of translations). In the case where yj is unaligned, the closest aligned target word to yj that has a corresponding alignment entry is aligned to xi ; then, if xi is present in the inpu"
D14-1140,P14-2090,0,0.18418,"r recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken context—or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an optimization problem, using a greedy search and dynamic programming to find segmentation strategies that maximize an evaluation measure. However, unlike our work, the direction of translation was from from svo to svo, obviating the need for verb prediction. Simultaneous translation is more straightforward for languages with compatible word orders, such as English and Spanish (F¨ ugen, 2008). To our knowledge, the only attempt to specifically predict verbs or any late-occurring terms (Matsubara et al.,"
D14-1140,P02-1040,0,0.114049,"n our system’s actions, we need to know how to select which actions to take. Eventually, we will formalize this as a learned policy (Section 5) that maps from states to actions. First, however, we need to define a reward that measures how “good” an action is. 3 Objective: What is a good simultaneous translation? Good simultaneous translations must optimize two objectives that are often at odds, i.e., producing translations that are, in the end, accurate, and producing them in pieces that are presented expeditiously. While there are existing automated metrics for assessing translation quality (Papineni et al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann e"
D14-1140,P11-1027,0,0.0210688,"ht language model. This is (na¨ıvely) analogous to how a human translator might use his own “language model” to guess upcoming words to gain some speed by completing, for example, collocations before they are uttered. We use a simple bigram language model for next-word prediction. We use Heafield et al. (2013). For verb prediction, we use a generative model that combines the prior probability of a particular verb v, p(v), with the likelihood of the source context at time t given that verb (namely, p(x1:t |v)), as estimated by a smoothed Kneser-Ney language model (Kneser and Ney, 1995). We use Pauls and Klein (2011). The prior probability p(v) is estimated by simple relative frequency estimation. The context, x1:t , consists of all words observed. We model p(x1:t |v) with verb-specific n-gram language models. The predicted verb v (t) at time t is then: arg max p(v) v 5 t Y p(xi |v, xi−n+1:i−1 ) (2) i=1 One could replace T with a parameter, β, to bias towards different kinds of simultaneous translations. As β → ∞, we recover batch translation. 1346 where xi−n+1:i−1 is the n − 1-gram context. To narrow the search space, we consider only the 100 most frequent final verbs, where a “final verb” is defined as"
D14-1140,2006.amta-papers.18,0,0.0265515,"vie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The former is not simultaneous at all; the latter is mere word-for-word replacement and results in awkward, often unintelligible translations of distant language pairs. Once we have predictions, we have an expanded array of possibilities, however. On one extreme, we can imagine a psychic translator— 3 Using constrained decoding to enforce consistent translation prefixes would complicate our method but is an appealing extension. one that can completely translate an imagined sentence after one word is uttered—as an unobtainable system. On the other extreme is a standard batch translator, which"
D14-1140,quasthoff-etal-2006-corpus,0,0.0437515,"Missing"
D14-1140,P06-2088,0,0.157126,"or translation. However, it is still comprehensible, and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one. 8 Related Work Just as mt was revolutionized by statistical learning, we suspect that simultaneous mt will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical mt module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the t"
D14-1140,shimizu-etal-2014-collection,0,0.0581262,"on and Future Work Creating an effective simultaneous translation system for sov to svo languages requires not only translating partial sentences, but also effectively predicting a sentence’s verb. Both elements of the system require substantial refinement before they are usable in a real-world system. Replacing our idealized translation system is the most challenging and most important next step. Supporting multiple translation hypotheses and incremental decoding (Sankaran 1350 et al., 2010) would improve both the efficiency and effectiveness of our system. Using data from human translators (Shimizu et al., 2014) could also add richer strategies for simultaneous translation: passive constructions, reordering, etc. Verb prediction also can be substantially improved both in its scope in the system and how we predict verbs. Verb-final languages also often place verbs at the end of clauses, and also predicting these verbs would improve simultaneous translation, enabling its effective application to a wider range of sentences. Instead predicting an exact verb early (which is very difficult), predicting a semantically close or a more general verb might yield interpretable translations. A natural next step i"
D14-1140,2006.amta-papers.25,0,0.026828,"lect which actions to take. Eventually, we will formalize this as a learned policy (Section 5) that maps from states to actions. First, however, we need to define a reward that measures how “good” an action is. 3 Objective: What is a good simultaneous translation? Good simultaneous translations must optimize two objectives that are often at odds, i.e., producing translations that are, in the end, accurate, and producing them in pieces that are presented expeditiously. While there are existing automated metrics for assessing translation quality (Papineni et al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The"
D14-1140,P97-1037,0,0.133617,"al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The former is not simultaneous at all; the latter is mere word-for-word replacement and results in awkward, often unintelligible translations of distant language pairs. Once we have predictions, we have an expanded array of possibilities, however. On one extreme, we can imagine a psychic translator— 3 Using constrained decoding to enforce consistent translation prefixes would complicate our method but is an appealing extension. one that can completely translate an imagined sentence after one word is uttered—as an unobtainable system. On the other extreme is a s"
D14-1140,N03-1033,0,0.0339159,":t , consists of all words observed. We model p(x1:t |v) with verb-specific n-gram language models. The predicted verb v (t) at time t is then: arg max p(v) v 5 t Y p(xi |v, xi−n+1:i−1 ) (2) i=1 One could replace T with a parameter, β, to bias towards different kinds of simultaneous translations. As β → ∞, we recover batch translation. 1346 where xi−n+1:i−1 is the n − 1-gram context. To narrow the search space, we consider only the 100 most frequent final verbs, where a “final verb” is defined as the sentence-final sequence of verbs and particles as detected by a German part-of-speech tagger (Toutanova et al., 2003).6 5 Learning a Policy We have a framework (states and actions) for simultaneous machine translation and a metric for assessing simultaneous translations. We now describe the use of reinforcement learning to learn a policy, a mapping from states to actions, to maximize lbleu reward. We use imitation learning (Abbeel and Ng, 2004; Syed et al., 2008): given an optimal sequence of actions, learn a generalized policy that maps states to actions. This can be viewed as a cost-sensitive classification (Langford and Zadrozny, 2005): a state is represented as a feature vector, the loss corresponds to t"
D14-1140,C96-2141,0,0.0530067,"source sentence correctly once it has been fed the appropriate source words as input. There are two edge cases: empty input yields an empty output, while a complete, correct source sentence returns the correct, complete translation. Intermediate cases—where the input is either incomplete or incorrect—require using an alignment. The omniscient translator assumes as input a reference translation r, a partial source language input x1:t and a corresponding partial output y. In addition, the omniscient translator assumes access to an alignment between r and x. In practice, we use the hmm aligner (Vogel et al., 1996; Och and Ney, 2003). We first consider incomplete but correct inputs. Let y = τ (x1:t ) be the translator’s output given a partial source input x1:t with translation y. Then, τ (x1:t ) produces all target words yj if there is a source word xi in the input aligned to those words—i.e., (i, j) ∈ ax,y —and all preceding target words can be translated. (That translations must be contiguous is a natural requirement for human recipients of translations). In the case where yj is unaligned, the closest aligned target word to yj that has a corresponding alignment entry is aligned to xi ; then, if xi is"
D14-1140,2013.mtsummit-papers.11,0,0.0115023,"statistical learning, we suspect that simultaneous mt will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical mt module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken context—or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an o"
D14-1140,W10-1733,0,0.122467,"Missing"
D14-1140,W05-0909,0,\N,Missing
D14-1140,C98-2136,0,\N,Missing
D14-1182,D12-1101,0,0.0504196,"Missing"
D14-1182,D10-1005,1,0.868179,") 2. For each document d ∈ [1, D] (a) Draw topic distribution θd ∼ Dir(α) (b) For each word n ∈ [1, Nd ] i. Draw topic zd,n ∼ Mult(θd ) ii. Draw word wd,n ∼ Mult(φzd,n ) (c) Draw response yd ∼ N (η T z¯d , ρ) where PNd z¯d,k = N1d n=1 I [zd,n = k] where I [x] = 1 if x is true, and 0 otherwise. In SLDA, in addition to the K multinomials {φk }K k=1 , the global latent variables also contain the regression parameter ηk for each topic k. The local latent variables of SLDA resembles LDA’s: the topic proportion vector θd for each document d. Train: For posterior inference during training, following Boyd-Graber and Resnik (2010), we use stochastic EM, which alternates between (1) a Gibbs sampling step to assign a topic to each token, and (2) optimizing the regression parameters. The probability of assigning topic k to token n in the training document d is TR TR TR TR p(zd,n = k |z−d,n , w−d,n , wd,n = v) ∝ N (yd ; µd,n , ρ) · −d,n NTR ,d,k + α −d,n NTR ,d,· + Kα · −d,n NTR ,k,v + β −d,n NTR ,k,· + V β (9) PK −d,n where µd,n = ( k0 =1 ηk0 NTR ,d,k0 + ηk )/NTR,d is the TR mean of the Gaussian generating yd if zd,n = k. Here, NTR,d,k is the number of times topic k is assigned to tokens in the training document d; NTR,k,"
D14-1182,P12-1009,1,0.830051,"1 X S(i, TTE ). |TTR |i∈T In LDA, the global latent variables are topics {φk }K k=1 and the local latent variables for each document d are topic proportions θd . Train: During training, we use collapsed Gibbs sampling to assign each token in the training data with a topic (Steyvers and Griffiths, 2006). The probability of 1753 assigning token n of training document d to topic k is TR TR TR TR p(zd,n = k |z−d,n , w−d,n , wd,n = v) ∝ −d,n NTR ,d,k + α −d,n NTR ,d,· + Kα · −d,n NTR ,k,v + β −d,n NTR ,k,· +Vβ 1240 1160 (5) 2150 Test: Because we lack explicit topic annotations for these data (c.f. Nguyen et al. (2012)), we use perplexity– a widely-used metric to measure the predictive power of topic models on held-old documents. To compute perplexity, we follow the estimating θ method (Wallach et al., 2009, Section 5.1) and evenly split each test document d into wdTE1 and wdTE2 . We first run Gibbs sampling on wdTE1 to estimate the topic proportion θˆdTE of test document d. The probability of assigning topic k TE1 TE 1 ˆ to token n in wdTE1 is p(zd,n = k |z−d,n , w TE1 , φ(i)) ∝ · φˆk,w TE1 (i) where NTE1 ,d,k is the number of tokens in wdTE1 assigned to topic k. At each iteration j in test chain i, we can"
D14-1182,P05-1015,0,0.239662,"Missing"
D14-1182,D09-1026,0,0.0674365,"analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate samples on training data to estimate corpus-level latent variables, and use them to generate samples to estimate document-level latent variables for test"
D15-1006,D08-1089,0,0.0551911,"ference, he said. Unlike previous approaches to simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one. We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system. In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined. Table 4: Example of translation produced by GD and RW. 5.5 Error Analysis This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders. However, our problem is different in several ways. First, while the approaches resemble each other, our motivation is to reduce translation delay. Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed. Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences. Table 4 compares translations b"
D15-1006,D14-1140,1,0.793294,"Missing"
D15-1006,I13-1147,0,0.0122048,"revious approaches to simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one. We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system. In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined. Table 4: Example of translation produced by GD and RW. 5.5 Error Analysis This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders. However, our problem is different in several ways. First, while the approaches resemble each other, our motivation is to reduce translation delay. Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed. Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences. Table 4 compares translations by GD and RW. RW correc"
D15-1006,W14-7008,0,0.0167516,"simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one. We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system. In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined. Table 4: Example of translation produced by GD and RW. 5.5 Error Analysis This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders. However, our problem is different in several ways. First, while the approaches resemble each other, our motivation is to reduce translation delay. Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed. Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences. Table 4 compares translations by GD and RW. RW correctly puts the verb said"
D15-1006,D10-1092,0,0.030681,"y slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum is over all aligned words except punctuation and stopwords. 5.1 Quality of Rewritten Translations After applying the rewriting rules (Section 4), Table 2 shows the percentage of sen"
D15-1006,2011.iwslt-evaluation.18,0,0.0132805,"ow the effect of rewritten references, we compare the following MT systems: • GD: only gold reference translations; • RW: only rewritten reference translations; • RW + GD: both gold and the rewritten references; and • RW- LM + GD: using gold reference translations but using the rewritten references for training the LM and for tuning. For RW + GD and RW- LM + GD, we interpolate the language models of GD and RW. The interpolating weight is tuned with the rewritten sentences. For RW + GD, we combine the translation models (phrase tables and reordering tables) of RW and GD by fill-up combination (Bisazza et al., 2011), where all entries in the tables of RW are preserved and entries from the tables of GD are added if new. Increasing the RP threshold increases interpretation delay but improves the quality of the translation. We set the RP threshold at 0.0, 0.2, 0.4, 0.8 and finally 1.0 (equivalent to batch translation). Figure 3 shows the BLEU/RIBES scores vs. the number of words per segement as we increase the threshold. Rewritten sentences alone do not significantly improve over the baseline. We suspect this is because the transformation rules sometimes generate ungrammatical sentences due to parsing error"
D15-1006,P03-1054,0,0.0117594,"e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum is over all aligned words except punctuation and stopwords. 5.1 Quality of Rewritten Translations After applying the rewriting rules (Section 4), Table 2 shows the percentage of sentences that are candidates and how many rewrites are accepted. The most generalizable rules are passiviz"
D15-1006,N12-1047,0,0.0155365,"ccompanying example sentences.7 Statistics of the dataset are shown in Table 1. The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum is over all aligned words except punctuation and stopwords. 5.1 Qua"
D15-1006,P05-1066,0,0.21838,"Missing"
D15-1006,J14-1002,0,0.0383052,"Missing"
D15-1006,matsubara-etal-2002-bilingual,0,0.537728,"n. These transformations enable us to divide the input into shorter segments, thus reducing translation delay. To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order. Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff. However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010). In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compared to human translations done at the translator’s leisure, allowing for more introspection and precise word choice. We aim to address the data scarcity problem and combine translators’ lexical precision and interpreters’ sy"
D15-1006,J03-1002,0,0.00596878,"stem, we also include the EIJIRO dictionary entries and the accompanying example sentences.7 Statistics of the dataset are shown in Table 1. The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum"
D15-1006,P14-2090,0,0.503832,"system that only uses gold reference translations. 1 Introduction Simultaneous interpretation is challenging because it demands both quality and speed. Conventional batch translation waits until the entire sentence is completed before starting to translate. This merely optimizes translation quality and often introduces undesirable lag between the speaker and the audience. Simultaneous interpretation instead requires a tradeoff between quality and speed. A common strategy is to translate independently translatable segments as soon as possible. Various segmentation methods (Fujita et al., 2013; Oda et al., 2014) reduce translation delay; they are limited, however, by the unavoidable word reordering between languages with drastically different word orders. We show an example of Japanese-English translation in Figure 1. Consider the batch translation: in English, the verb change comes immediately after the subject We, whereas in Japanese it comes at the end 55 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 55–64, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Source: ! We-TOP government-GEN structure and compositi"
D15-1006,P02-1040,0,0.0942046,"rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum is over all aligned words except punctuation and stopwords. 5.1 Quality of Rewritten Translations After applying the rewriting rules (Section 4), Ta"
D15-1006,2013.iwslt-papers.3,0,0.642823,"he the subject and begin translating before observing the final verb. Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction. These transformations enable us to divide the input into shorter segments, thus reducing translation delay. To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order. Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff. However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010). In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compared to human translations done at the translator’s l"
D15-1006,shimizu-etal-2014-collection,0,0.0714722,"other. Consider the monotone translation in Figure 1. By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb. Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction. These transformations enable us to divide the input into shorter segments, thus reducing translation delay. To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order. Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff. However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010). In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction ("
D15-1006,N13-1023,0,0.463868,"responding Japanese sentence. We evaluate our approach using standard machine translation data (the Reuters newsfeed Japanese-English corpus) in a simultaneous translation setting. Our experimental results show that including the rewritten references into the learning of a phrase-based MT system results in a better speed-accuracy tradeoff against both the original and the rewritten reference translations. 2 While we are motivated by real-time interpretation, to simplify our problem, we assume that we have perfect text input. Given this constraint, a typical simultaneous interpretation system (Sridhar et al., 2013; Fujita et al., 2013; Oda et al., 2014) produces partial translations of consecutive segments in the source sentence and concatenates them to produce a complete translation. We define the translation delay of a sentence as the average number of tokens the system has to observe between translation of two consecutive segments (denoted by # words/seg).1 For instance, the minimum delay of 1 word/seg is achieved when we translate immediately upon hearing a word. At test time, when the input is segmented, the delay is the average segment length. During the data preprocessing step of rewriting, we c"
D15-1006,tohyama-matsubara-2006-collection,0,0.187856,"Figure 1. By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb. Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction. These transformations enable us to divide the input into shorter segments, thus reducing translation delay. To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order. Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff. However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010). In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compar"
D15-1006,P03-1010,0,0.0607567,"alignment therefore forms a segment with delay of four words/seg. Alignments of the following words come before the source word aligned to love; hence, they are already translated in the previous segment and we do not double count their delay. In this example, the delay of the original sentence is 2.5 word/seg; after rewriting, it is reduced to 1.7 word/seg. Therefore, we accept the rewritten sentence. However, when the subject phrase is long and the object phrase is short, a swap may not reduce delay. Experiments We evaluate our method on the Reuters JapaneseEnglish corpus of news articles (Utiyama and Isahara, 2003). For training the MT system, we also include the EIJIRO dictionary entries and the accompanying example sentences.7 Statistics of the dataset are shown in Table 1. The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters s"
D15-1006,N09-1028,0,0.191939,"nd the gold reference translation. Boldface indicates the number is significantly larger than others (excluding the gold ref) according to two-sample t-tests with p &lt; 0.001. Effect on Verbs Rewriting training data not only creates lower latency simultaneous translations, but it also improves batch translation. One reason is that SOV to SVO translation often drops the verb because of long range reordering. (We see this for Japanese here, but this is also true for German.) Similar word orders in the source and target results in less reordering and improves phrase-based MT (Collins et al., 2005; Xu et al., 2009). Table 3 shows the number of verbs in the translations of the test sentences produced by GD, RW, RW + GD, as well as the number in the gold reference translation. Both RW and RW + GD produce more verbs (a statistically significant result), although RW + GD captures the most verbs. 61 he also said that the real dangers for the euro lay in the potential for divergences in the domestic policy needs Ref among the various participating nations of the single currency. GD he also for the euro, is a real danger to launch a single currency in many different countries and domestic policies on the need"
D15-1006,P07-2045,0,\N,Missing
D15-1030,W09-2206,0,0.0247196,"f our model against LDA and MRTF and then perform link prediction tasks. We demonstrate improvements in link prediction as measured by predictive link rank and provide both qualitative and quantitative perspectives on the improvements achieved by the model. 2 Sk,i = Nk X p(wk,j |wk,i ), (1) j=1,j6=i where Nk denotes the number of words in topic k. We then select the three words with the highest sum of transition probabilities as the seed words for topic k. In the sampling process (Section 3), seed words are only assigned to their corresponding topics, similar to the use of hard constraints by Andrzejewski and Zhu (2009). Discriminative Links from Topics Figure 1 is a two-document segment of our model, which has the following generative process: 1. For each related-document cluster l ∈ {1, . . . , L} Draw πl ∼ Dir(α0 ) 2. For each topic k ∈ {1, . . . , K} (a) Draw word distribution φk ∼ Dir(β) (b) Draw topic regression parameter ηk ∼ N (0, ν 2 ) 3. For each word v ∈ {1, . . . , V } Draw lexical regression parameter τv ∼ N (0, ν 2 ) 4. For each document d ∈ {1, . . . , D} (a) Draw topic proportions θd ∼ Dir(απld ) (b) For each word td,n in document d i. Draw a topic assignment zd,n ∼ Mult(θd ) ii. Draw a word"
D15-1030,P14-1018,0,0.0714405,"Missing"
D15-1030,W03-1730,0,0.0366584,"tes for test. As the training corpus is generated ranthe indexes of documents which are linked to docdomly, seeding is not applied in this section. The −d,n ument d; πld ,k is estimated based on the maximal results are given in Table 1, where I- denotes that path assumption (Wallach, 2008) the model incorporates user interactions. P −d,n 0 The results confirm that our model outperforms d0 ∈S(ld ) Nd0 ,k + α πl−d,n = , (5) P both LDA and MRTF and that its use of user inter−d,n d ,k 0 d0 ∈S(ld ) Nd0 ,· + Kα actions holds promise. where S(ld ) denotes the cluster which contains 4 We use ICTCLAS (Zhang et al., 2003) for segmentation. document d (Step 1 in the generative process). After stopword and low-frequency word removal, the vocabulary includes 12,257 words, with ∼755 tokens per document and 5,404 links. 3 More details here and throughout this section appear in the supplementary materials. 263 (a) Mentioning 82.44 73.31 (a) Mentioning 60 76.71 (c) Following (b) Retweeting 74.38 82.44 110.99 73.31 76.71 74.38 98.99 101.34 104.65 103.79 100.09 82.13 40 50 40 50 60 RTM 72.89 70 IS-RTM 60 82.44 80 60 70 Lex-IS-RTM 80 90 100 MED-RTM 110 118.55 119.70 120.18 117.19 123.64 114.85 82.13 72.89 70 80 IS-MED-R"
D15-1030,P09-2074,0,0.179937,"Missing"
D15-1030,E14-1056,0,0.066577,"o generally increases as the models grow more sophisticated.6 4.5 In the SD/Avg row of Table 3, we also compute a ratio of standard deviations to mean values. Ratios given by the models with hinge loss are lower than those not using hinge loss. This means that the regression values given by the models with hinge loss are more concentrated around the average value, suggesting that these models can better identify linked pairs, even though the ratio of linked pairs’ average regression value to all pairs’ average value is lower. Quantitative Analysis Topic Quality. Automatic coherence detection (Lau et al., 2014) is an alternative to manual evaluations of topic quality (Chang et al., 2009). In each topic, the top n words’ average pointwise mutual information (PMI)—based on a reference corpus—serves as a measure of topic coherence.7 Topic quality improves with user interactions and max-margin learning (Table 3). PMI drops when lexical terms are added to the link probability function, however. This is consistent with the role of lexical terms in the model; their purpose is to improve link prediction performance, not improve topic quality. 5 Conclusions and Future Work We introduce a new topic model that"
D15-1037,P12-2054,1,0.91825,"Ramage et al. (2009) propose Labeled-LDA, which improves LDA with document labels. It assumes that there is a one-to-one mapping between topics and labels, and it restricts each document’s topics to be sampled only from those allowed by the documents label set. Therefore, Labeled-LDA can be expressed in our model. We define ( 1, if z ∈ md fm (z, w, d) = (8) −∞, else 3.1 3.2 able 1,900,000 100,000,000 1,946,000 Dataset Prior Knowledge Generation Word Correlation Prior Knowledge Previous work proposes two methods to automatically generate prior word correlation knowledge from external sources. Hu and Boyd-Graber (2012) use WordNet 3.0 to obtain synsets for word types, and then if a synset is also in the vocabulary, they add a must-link correlation between the word type and the synset. Xie et al. (2015) use a different method that takes advantage of an existing pretrained word embedding. Each word embedding is a real-valued vector capturing the word’s semantic meaning based on distributional similarity. If the similarity between the embeddings of two word types in the vocabulary exceeds a threshold, they generate a must-link between the two words. In our experiments, we adopt a hybrid method that combines th"
D15-1037,P11-1026,1,0.890009,"times (once for each topic) because we need the summation of P (z = t) for sampling. Therefore, the critical part of speeding up the sampler is finding a sparse representation of the second term. In the following sections, we show that natural, sparse prior knowledge representations are possible. We first present an efficient sparse representation of word correlation prior knowledge and then one for document-label knowledge. 2.3 We now illustrate how we can encode word correlation knowledge as a set of sparse constraints fm (z, w, d) in our model. In previous work (Andrzejewski et al., 2009; Hu et al., 2011; Xie et al., 2015), word correlation prior knowledge is represented as word must-link constraints and cannotlink constraints. A must-link relation between two words indicates that the two words tend to be related to the same topics, i.e. their topic probabilities are correlated. In contrast, a cannot-link relation between two words indicates that these two words are not topically similar, and they should not both be prominent within the same topic. For example, “quarterback” and “fumble” are both related to American football, so they can share a must-link relation. But “fumble” and “bank” imp"
D15-1037,P14-1110,1,0.706166,"ent increases. SC-LDA exhibits greater speedup with 5 For 20NG dataset, it may overfit the data with 500 topics, but here we use it to demonstrate the scalability. 7 314 MCMC inference (Mimno et al., 2012). In this paper, we only focus on single-processor learning, but existing parallelization techniques (Newman et al., 2009) are applicable to our model. At the intersection lies models that improve the scalability of upstream topic model inference. In addition to our SC-LDA, Hu and Boyd-Graber (2012) speed Gibbs sampling in tree-based topic models using SparseLDA’s factorization strategy, and Hu et al. (2014) extend this approach by parallelizing global parameter updates using variational inference. Our work is more general (also encompassing document-based constraints) and is faster. In contrast to these upstream models, Zhu et al. (2013) and Nguyen et al. (2015) improve inference of downstream models. Adding knowledge and metadata to topic models makes the models richer, more understandable, and more domain-specific. A common distinction is upstream (conditioning on metadata) vs. downstream models (conditioning on variables already present in a topic model to predict metadata) (Mimno et al., 200"
D15-1037,D07-1109,1,0.376818,"Missing"
D15-1037,P09-2074,0,0.056563,"Missing"
D15-1037,D11-1024,0,0.0720074,"porate different numbers of word correlations # Word Correlations C0 C100 C500 2.02 2.14 2.30 0.53 0.56 0.58 0.48 0.50 0.53 0.48 0.49 0.52 C1000 2.50 0.62 0.56 0.56 Table 2: SC-LDA runtime (in seconds) in the 1st, 50th, 100th, and 200th iteration with different numbers of correlations. in SC-LDA. SC-LDA runs faster as sampling proceeds as the sparsity increases, but additional correlations slow the model. 3.5 Topic Coherence Topic models are often evaluated using perplexity on held-out test data, but this evaluation is of6 313 ten at odds with human evaluations (Chang et al., 2009). Following Mimno et al. (2011), we employ Topic Coherence—a metric that is consistent with human judgment—to measure a topic model’s quality. Topic t’s coherence is defined (t) (t) P Pm−1 F (vm ,vl )+ as C(t : V (t) ) = M log , (t) m=2 l=1 F (vl ) where F (v) is the document frequency of word type v, F (v, v 0 ) is the co-document frequency of (t) (t) word type v and v 0 , and V (t) = (v1 , ..., vM ) is a list of the M most probable words in topic t. In our experiments, we choose the ten words with highest probability in the topic to compute topic coherence, i.e., M = 10. Mimno et al. (2011) use  = 1, but R¨oder et al. ("
D15-1037,P15-1075,1,0.83317,"tional Gibbs sampling takes too long to finish. For standard LDA, recently introduced fast sampling methods (Yao et al., 2009; Li et al., 2014; Yuan et al., 2015) enable industrial applications of topic modeling to search engines and online advertising, where capturing the “long tail” of infrequently used topics requires large topic spaces. For example, while typical LDA models in academic papers have up to 103 topics, industrial applications with 105 –106 topics are common (Wang et al., 2014). Moreover, scaling topic models to many topics can also reveal the hierarchical structure of topics (Downey et al., 2015). Thus, there is a need for topic models that can both benefit from rich prior information and that can scale to large datasets. However, existing methods for improving scalability focus on topic models without prior information. To rectify this, we propose a factor graph model that encodes a potential function over the hidden topic variables, encouraging topics consistent with prior knowledge. The factor model representation admits an efficient sampling algorithm that takes advantage of the model’s sparsity. We show that our method achieves comparable performance but runs significantly faster"
D15-1037,N15-1074,0,0.485055,"each topic) because we need the summation of P (z = t) for sampling. Therefore, the critical part of speeding up the sampler is finding a sparse representation of the second term. In the following sections, we show that natural, sparse prior knowledge representations are possible. We first present an efficient sparse representation of word correlation prior knowledge and then one for document-label knowledge. 2.3 We now illustrate how we can encode word correlation knowledge as a set of sparse constraints fm (z, w, d) in our model. In previous work (Andrzejewski et al., 2009; Hu et al., 2011; Xie et al., 2015), word correlation prior knowledge is represented as word must-link constraints and cannotlink constraints. A must-link relation between two words indicates that the two words tend to be related to the same topics, i.e. their topic probabilities are correlated. In contrast, a cannot-link relation between two words indicates that these two words are not topically similar, and they should not both be prominent within the same topic. For example, “quarterback” and “fumble” are both related to American football, so they can share a must-link relation. But “fumble” and “bank” imply two different to"
D15-1037,D09-1026,0,0.586854,"et’s return to the question whether Equation 6 is sparse, allowing efficient computation of Equation 7. Fortunately, nu,t and nv,t , which are the 2: 3: 4: 5: 6: 4 311 compute st , rt , qt with SparseLDA, (see Eq. 3) for t ← 0 to T do update st , rt , qt . ∀u ∈ Mw if nu,t &gt; λ end for p(t) = st + rt + qt sample new topic assignment for w from p(t) 2.4 DATASET NIPS NYT-N EWS 20NG Other Types of Prior Knowledge The factor model framework can also handle other types of prior knowledge, such as document labels, sentence labels, and document link relations. We briefly describe document labels here. Ramage et al. (2009) propose Labeled-LDA, which improves LDA with document labels. It assumes that there is a one-to-one mapping between topics and labels, and it restricts each document’s topics to be sampled only from those allowed by the documents label set. Therefore, Labeled-LDA can be expressed in our model. We define ( 1, if z ∈ md fm (z, w, d) = (8) −∞, else 3.1 3.2 able 1,900,000 100,000,000 1,946,000 Dataset Prior Knowledge Generation Word Correlation Prior Knowledge Previous work proposes two methods to automatically generate prior word correlation knowledge from external sources. Hu and Boyd-Graber (2"
D15-1037,N15-1076,1,\N,Missing
D17-1046,P14-1110,1,0.838056,"gorithm cannot break the symmetry quickly. We provide solutions for this problem. Two alternative learning rate methods, i.e., ADADELTA (Zeiler, 2012) and ADAM (Kingma and Ba, 2014), can address this incompatibility with online topic models. When the dataset is small enough, e.g., a corpus with only hundreds of documents, ADAGRAD can still work. GRAD Probabilistic topic models (Blei, 2012) are popular algorithms for uncovering hidden thematic structure in text. They have been widely used to help people understand and navigate document collections (Blei et al., 2003), multilingual collections (Hu et al., 2014), images (Chong et al., 2009), networks (Chang and Blei, 2009; Yang et al., 2016), etc. Probabilistic topic modeling usually requires computing a posterior distribution over thousands or millions of latent variables, which is often intractable. Variational inference (Blei et al., 2016, VI ) approximates posterior distributions. Stochastic variational inference (Hoffman et al., 2013, SVI) is its natural online extension and enables the analysis of large datasets. Online topic models (Hoffman et al., 2010; Bryant and Sudderth, 2012; Paisley et al., 2015) optimize the global parameters of interes"
D17-1046,P16-1065,1,0.83616,"m. Two alternative learning rate methods, i.e., ADADELTA (Zeiler, 2012) and ADAM (Kingma and Ba, 2014), can address this incompatibility with online topic models. When the dataset is small enough, e.g., a corpus with only hundreds of documents, ADAGRAD can still work. GRAD Probabilistic topic models (Blei, 2012) are popular algorithms for uncovering hidden thematic structure in text. They have been widely used to help people understand and navigate document collections (Blei et al., 2003), multilingual collections (Hu et al., 2014), images (Chong et al., 2009), networks (Chang and Blei, 2009; Yang et al., 2016), etc. Probabilistic topic modeling usually requires computing a posterior distribution over thousands or millions of latent variables, which is often intractable. Variational inference (Blei et al., 2016, VI ) approximates posterior distributions. Stochastic variational inference (Hoffman et al., 2013, SVI) is its natural online extension and enables the analysis of large datasets. Online topic models (Hoffman et al., 2010; Bryant and Sudderth, 2012; Paisley et al., 2015) optimize the global parameters of interest using stochastic gradient ascent. At each iteration, they sample data points to"
D17-1153,D15-1006,1,0.859145,"Missing"
D17-1153,2012.eamt-1.60,0,0.0130167,"Missing"
D17-1153,W08-0336,0,0.0133316,"anguage pairs from different language families with different typological properties: German-to-English and (De-En) and Chinese-to-English (Zh-En). We use parallel transcriptions of TED talks for these pairs of languages from the machine translation track of the IWSLT 2014 and 2015 (Cettolo et al., 2014, 2015, 2012). For each language pair, we split its data into four sets for supervised training, bandit training, development and testing (Table 1). For English and German, we tokenize and clean sentences using Moses (Koehn et al., 2007). For Chinese, we use the Stanford Chinese word segmenter (Chang et al., 2008) to segment sentences and tokenize. We remove all sentences with length greater than 50, resulting in an average sentence length of 18. We use IWSLT 2015 data for supervised training and development, IWSLT 2014 data for bandit training and previous years’ development and evaluation data for testing. 5.1 Evaluation Framework For each task, we first use the supervised training set to pre-train a reference NMT model using supervised learning. On the same training set, we also pre-train the critic model with translations sampled from the pre-trained NMT model. Next, we enter a bandit learning mode"
D17-1153,W14-3346,0,0.0136011,"e wish to optimize. To establish the feasibility of driving learning from human feedback without doing a full, costly user study, we begin with a simulation study. The key aspects (Figure 2) of human feedback we capture are: (a) mismatch between training objective and feedbackmaximizing objective, (b) human ratings typically are binned (§ 4.1), (c) individual human ratings have high variance (§4.2), and (d) non-expert ratings can be skewed with respect to expert ratings (§4.3). In our simulated study, we begin by modeling gold standard human ratings using add-onesmoothed sentence-level B LEU (Chen and Cherry, 2014).3 Our evaluation criteria, therefore, is average sentence-B LEU over the run of our algo1467 3 “Smoothing 2” in Chen and Cherry (2014). Granularity Original [0.1, 0.3), [0.3, 0.5), [0.5, 0.7), [0.7, 0.9) and [0.9, 1.0]. Since most sentence-B LEU scores are much closer to zero than to one, many of the larger bins are frequently vacant. Perturbed g=1 g=3 4.2 Variance λ=0 λ=5 Skew ρ=2 ρ=.5 Figure 2: Examples of how our perturbation functions change the “true” feedback distribution (left) to ones that better capture features found in human feedback (right). rithm. However, in any realistic scenar"
D17-1153,D14-1140,1,0.299199,"Missing"
D17-1153,N16-1111,1,0.603668,"Missing"
D17-1153,P07-2045,0,0.0110527,"sh; for ρ < 1, the rater is motivational. 5 Experimental Setup We choose two language pairs from different language families with different typological properties: German-to-English and (De-En) and Chinese-to-English (Zh-En). We use parallel transcriptions of TED talks for these pairs of languages from the machine translation track of the IWSLT 2014 and 2015 (Cettolo et al., 2014, 2015, 2012). For each language pair, we split its data into four sets for supervised training, bandit training, development and testing (Table 1). For English and German, we tokenize and clean sentences using Moses (Koehn et al., 2007). For Chinese, we use the Stanford Chinese word segmenter (Chang et al., 2008) to segment sentences and tokenize. We remove all sentences with length greater than 50, resulting in an average sentence length of 18. We use IWSLT 2015 data for supervised training and development, IWSLT 2014 data for bandit training and previous years’ development and evaluation data for testing. 5.1 Evaluation Framework For each task, we first use the supervised training set to pre-train a reference NMT model using supervised learning. On the same training set, we also pre-train the critic model with translations"
D17-1153,P17-1138,0,0.0675524,"tion (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches to more general structured prediction under bandit feedback (Chang et al., 2015; Sokolov et al., 2016a,b) show the broader efficacy of this framework. Ranzato et al. (2016) describe MIXER for training neural encoder-decoder models, which is a reinforcement learning approach closely related to ours but requires a policy-mixing strategy and only uses a linear critic model. Among work on bandit MT, ours is closest to Kreutzer et al. (2017), which also tackle this problem using neural encoder-decoder models, but we (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simulated rewards. Our results show that bandit feedback can be an effective feedback mechanism for neural machine translation systems. This is despite that errors in human annotations hurt machine learning models in many NLP tasks (Snow et al., 2008). An obvious question is whether we could extend our framework to model individual annotator"
D17-1153,D16-1127,0,0.0324095,"Zh−En 0.25 0.5 0.67 1 (e) Variance ρ 1.5 2 4 (f) Skew Figure 5: Performance gains of NMT models trained with NED-A2C in Per-Sentence B LEU (top row) and in Heldout B LEU (bottom row) under various degrees of granularity, variance, and skew of scores. Performance gains of models trained with un-perturbed scores are within the shaded regions. 2010), myoelectric control (Pilarski et al., 2011), and virtual assistants (Isbell et al., 2001). Recently, this learning framework has been combined with recurrent neural networks to solve machine translation (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches to more general structured prediction under bandit feedback (Chang et al., 2015; Sokolov et al., 2016a,b) show the broader efficacy of this framework. Ranzato et al. (2016) describe MIXER for training neural encoder-decoder models, which is a reinforcement learning approach closely related to ours but requires a policy-mixing strategy and only uses a linear critic model. Among work on bandit MT, ours is closest to Kreutzer et al. (2017), which also tackle this problem using neural"
D17-1153,D15-1166,0,0.747737,"al,jbg}@umiacs.umd.edu Abstract Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoderdecoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors. 1 Introduction Bandit structured prediction is the task of learning to solve complex joint prediction problems (like parsing or machine translation) under a very limited feedback model: a system must produce a single structured output (e.g., translation) and then the world reveals a score that measures how good or bad th"
D17-1153,D08-1027,0,0.16025,"Missing"
D17-1153,P16-1152,0,0.0543786,"nularity, variance, and skew of scores. Performance gains of models trained with un-perturbed scores are within the shaded regions. 2010), myoelectric control (Pilarski et al., 2011), and virtual assistants (Isbell et al., 2001). Recently, this learning framework has been combined with recurrent neural networks to solve machine translation (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches to more general structured prediction under bandit feedback (Chang et al., 2015; Sokolov et al., 2016a,b) show the broader efficacy of this framework. Ranzato et al. (2016) describe MIXER for training neural encoder-decoder models, which is a reinforcement learning approach closely related to ours but requires a policy-mixing strategy and only uses a linear critic model. Among work on bandit MT, ours is closest to Kreutzer et al. (2017), which also tackle this problem using neural encoder-decoder models, but we (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simul"
D17-1153,2015.mtsummit-papers.13,0,0.218913,"imizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors. 1 Introduction Bandit structured prediction is the task of learning to solve complex joint prediction problems (like parsing or machine translation) under a very limited feedback model: a system must produce a single structured output (e.g., translation) and then the world reveals a score that measures how good or bad that output is, but provides neither a “correct” output nor feedback on any other possible output (Chang et al., 2015; Sokolov et al., 2015). Because of the extreme sparsity of this feedback, a common experimental setup is that one pre-trains a good-but-not-great “reference” system based on whatever labeled data is available, and then seeks to improve it over time using this bandit feedback. A common motivation for this problem setting is cost. In the case of translation, bilingual “experts” can read a source sentence and a possible translation, and can much more quickly provide a rating of that translation than they can produce a full translation on their own. Furthermore, one can often collect even less expensive ratings from “n"
D17-1153,D15-1130,0,0.028197,"e (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simulated rewards. Our results show that bandit feedback can be an effective feedback mechanism for neural machine translation systems. This is despite that errors in human annotations hurt machine learning models in many NLP tasks (Snow et al., 2008). An obvious question is whether we could extend our framework to model individual annotator preferences (Passonneau and Carpenter, 2014) or learn personalized models (Mirkin et al., 2015; Rabinovich et al., 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al., 2007; Antos et al., 2010). Another direction is to apply active learning techniques to reduce the sample complexity required to improve the systems or to extend to richer action spaces for problems like simultaneous translation, which requires prediction (Grissom II et al., 2014) and reordering (He et al., 2015) among other strategies to both minimize delay and effectively translate a sentence (He et al., 2016). Acknowledgements Many thanks to Yvette Graham for her help with the WMT human evaluations dat"
D17-1153,Q14-1025,0,0.0176647,"tackle this problem using neural encoder-decoder models, but we (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simulated rewards. Our results show that bandit feedback can be an effective feedback mechanism for neural machine translation systems. This is despite that errors in human annotations hurt machine learning models in many NLP tasks (Snow et al., 2008). An obvious question is whether we could extend our framework to model individual annotator preferences (Passonneau and Carpenter, 2014) or learn personalized models (Mirkin et al., 2015; Rabinovich et al., 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al., 2007; Antos et al., 2010). Another direction is to apply active learning techniques to reduce the sample complexity required to improve the systems or to extend to richer action spaces for problems like simultaneous translation, which requires prediction (Grissom II et al., 2014) and reordering (He et al., 2015) among other strategies to both minimize delay and effectively translate a sentence (He et al., 2016). Acknowledgements Many thanks to Yvette Grah"
D17-1153,K15-1001,0,\N,Missing
D17-1153,E17-1101,0,\N,Missing
D17-1153,2015.iwslt-evaluation.1,0,\N,Missing
D17-1203,D07-1109,1,0.782269,"human topic interpretability into the topic model optimization process in a way that is effective and more straightforward than previous methods (Newman et al., 2011). We take advantage of the human-centered evaluation of Chang et al. (2009), which can be reasonably approximated using an automatic metric based on word associations derived from a large, more general corpus (Lau et al., 2014). We exploit LDA and its Bayesian formulation by bringing word associations into the picture using a prior—specifically, we use external lexical association to create a tree structure and then use tree LDA (Boyd-Graber et al., 2007, tLDA), which derives topics using a given tree prior. We construct tree priors with combinations of two types of word association scores (skip-gram probability (Mikolov et al., 2013) and G 2 likelihood ratio (Dunning, 1993)) and three construction algorithms (two-level, hierarchical clustering with and without leaf duplication). Then tLDA identifies topics with these tree priors in Amazon reviews and the 20NewsGroups datasets. tLDA topics are more coherent compared with “vanilla” LDA topics, while retaining and often slightly improving topics’ extrinsic performance as features for supervised"
D17-1203,J93-1003,0,0.353239,"(2009), which can be reasonably approximated using an automatic metric based on word associations derived from a large, more general corpus (Lau et al., 2014). We exploit LDA and its Bayesian formulation by bringing word associations into the picture using a prior—specifically, we use external lexical association to create a tree structure and then use tree LDA (Boyd-Graber et al., 2007, tLDA), which derives topics using a given tree prior. We construct tree priors with combinations of two types of word association scores (skip-gram probability (Mikolov et al., 2013) and G 2 likelihood ratio (Dunning, 1993)) and three construction algorithms (two-level, hierarchical clustering with and without leaf duplication). Then tLDA identifies topics with these tree priors in Amazon reviews and the 20NewsGroups datasets. tLDA topics are more coherent compared with “vanilla” LDA topics, while retaining and often slightly improving topics’ extrinsic performance as features for supervised classification. Our approach can be viewed as a form of adaptation, and the flexibility of the tree prior approach—amenable to any kind of association score—suggests that there are many directions to pursue beyond the two fl"
D17-1203,P96-1024,0,0.397937,"jbg@umiacs.umd.edu CS , Abstract Models work best when they are optimized taking into account the evaluation criteria that people care about. For topic models, people often care about interpretability, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting extrinsic performance. 1 Introduction Goodman (1996) introduces a key insight for machine learning models in natural language processing: if you know how performance on a problem is evaluated, it makes more sense to optimize using that evaluation metric, rather than others. Goodman applies his insight to parsing algorithms, but this insight has had an even larger impact in machine translation, where the introduction of the fully automatic BLEU metric makes it possible to tune systems using a score correlated with human rankings of MT system performance (Papineni et al., 2002). Chang et al. (2009) provide a similar insight for topic models (Blei"
D17-1203,P16-2062,0,0.232793,"tperform the ones built using the G 2 likelihood ratio. Among the three tree prior construction algorithms, the two-level is the best on the 20NewsGroups corpus. However, there is no such consistent pattern on Amazon reviews. 4.2 Topic Coherence Instead of manually evaluating topic quality using word intrusion (Chang et al., 2009), we use an automatic alternative to compute topic coherence (Lau et al., 2014). For every topic, we extract its top ten words and compute average pairwise PMI on a reference corpus (Wikipedia as of October 8, 2014). We include LDA and the latent concept topic model (Hu and Tsujii, 2016, LCTM) as baselines. LCTM also incorporates prior knowledge from word embeddings. It assumes that latent concepts exist in the embedding space and are Gaussian distributions over word embeddings, and a topic is a multinomial distribution over these concepts. We marginalize over concepts and obtain the probability mass of every word in every topic and compare against LDA and tLDA topics. 4 https://catalog.ldc.upenn.edu/ ldc2011t07. 1903 Topic KLD Christian 0.709 Security 0.720 Middle East 0.765 Sports 1.212 University Research 1.647 Health 1.914 Images 1.995 Hardware 2.127 People 2.512 Model L"
D17-1203,P14-1110,1,0.887693,"a topic. The generative process of tLDA is: 1. For topics k ∈ {1, . . . , K} and internal nodes ni (a) Draw child distribution1 πk,i ∼ Dir(β) 2. For each document d ∈ {1, . . . , D} (a) Draw topic distribution θd ∼ Dir(α) (b) For each token td,n in document d i. Draw topic assignment zd,n ∼ Mult(θd ) ii. Q Draw path yd,n to word wd,n with probability (i,j)∈yd,n πzd,n ,i,j tLDA can perform different tasks using different tree priors. If we encode synonyms in the tree prior, tLDA disambiguates word senses (BoydGraber et al., 2007). With word translation priors, it is a multilingual topic model (Hu et al., 2014). 1 Unlike other tree-based topic models such as Andrzejewski et al. (2009), all Dirichlet hyperparameters are the same for all internal nodes. Regardless of cardinality, all Dirichlet parameters are the same scalar β. match matches tournament Figure 2: A two-level tree example with N = 2. The words in the internal nodes denote concepts and have no effect in tLDA. 3 Tree Prior Construction from Word Association Scores A two-level tree is the most straightforward construction.2 Each internal node, ni , is a concept associated with a word vi in the vocabulary. Then we sort all other words in des"
D17-1203,E14-1056,0,0.453882,"UMIACS University of Maryland College Park, MD resnik@umd.edu We take the logical next step suggested when you bring together the insights of Goodman (1996) and Chang et al. (2009), namely incorporating an approximation of human topic interpretability into the topic model optimization process in a way that is effective and more straightforward than previous methods (Newman et al., 2011). We take advantage of the human-centered evaluation of Chang et al. (2009), which can be reasonably approximated using an automatic metric based on word associations derived from a large, more general corpus (Lau et al., 2014). We exploit LDA and its Bayesian formulation by bringing word associations into the picture using a prior—specifically, we use external lexical association to create a tree structure and then use tree LDA (Boyd-Graber et al., 2007, tLDA), which derives topics using a given tree prior. We construct tree priors with combinations of two types of word association scores (skip-gram probability (Mikolov et al., 2013) and G 2 likelihood ratio (Dunning, 1993)) and three construction algorithms (two-level, hierarchical clustering with and without leaf duplication). Then tLDA identifies topics with the"
D17-1203,P02-1040,0,0.0988489,"topic interpretability without hurting extrinsic performance. 1 Introduction Goodman (1996) introduces a key insight for machine learning models in natural language processing: if you know how performance on a problem is evaluated, it makes more sense to optimize using that evaluation metric, rather than others. Goodman applies his insight to parsing algorithms, but this insight has had an even larger impact in machine translation, where the introduction of the fully automatic BLEU metric makes it possible to tune systems using a score correlated with human rankings of MT system performance (Papineni et al., 2002). Chang et al. (2009) provide a similar insight for topic models (Blei et al., 2003, LDA): if what you care about is the interpretability of topics, the standard objective function for parameter inference (likelihood) is not only poorly correlated with a human-centered measurement of topic coherence, but inversely correlated. Nonetheless, most topic models are still trained using methods that optimize likelihood (McAuliffe and Blei, 2008; Nguyen et al., 2013). Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu We take the logical next step suggested whe"
D18-1134,S17-2097,0,0.0302631,"Missing"
D18-1134,D13-1160,0,0.0691701,"vious work considers improving that base framework itself (Clark and Gardner, 2018; Swayamdipta et al., 2018, inter alia). But retains the assumption of answering individual questions. Aside from the open-domain setup, much of the 1080 recent work on question answering has focused on the sub-problem of reading-comprehension, where the gold answer to each question is assumed to exist in a given single paragraph for the model to read (Hermann et al., 2015; Rajpurkar et al., 2016; Seo et al., 2017). Another line of work on question answering is question answering over structured knowledge-bases (Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014; Gardner and Krishnamurthy, 2017). Although we focus on the more general open-domain setup, QBLink can be adapted to be usable in the readingcomprehension setup as well as the question answering over knowledge-bases setup. Several question answering datasets have been proposed (Berant et al., 2013; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018, inter alia). However, all of them were limited to answering individual questions. Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for t"
D18-1134,D12-1118,1,0.911894,"Missing"
D18-1134,P17-1171,0,0.259362,"ontext. Instead, we investigate sequential question answering, asking multiple related questions. We present QBLink, a new dataset of fully human-authored questions. We extend existing strong question answering frameworks to include previous questions to improve the overall question-answering accuracy in open-domain question answering. The dataset is publicly available at http:// sequential.qanta.org. 1 Introduction The framework of combining information retrieval and neural reading comprehension has been the basis of several systems for answering open-domain questions over unstructured text (Chen et al., 2017; Wang et al., 2018; Clark and Gardner, 2018; Htut et al., 2018). Typically, such systems take one input question at a time, retrieving and ranking multiple paragraphs that potentially contain the answer. A reading comprehension model then produces a ranked list of candidate answer spans from each paragraph. The final answer is then selected from the produced spans. In information-seeking dialogs, e.g., personal assistants, users interact with a question answering system by asking a sequence of related questions, where questions share the same predicate, entities, or at least a topic. Answerin"
D18-1134,D18-1241,0,0.101688,"lmor and Berant (2018) answer complex questions by decomposing each into a sequence of simple questions. Iyyer et al. (2017) adopt a semantic parsing approach to answer questions over semi-structured tables. They construct a dataset of around 6,000 question sequences by asking humans to rewrite a set of 2,000 complex questions into simple sequences. Talmor and Berant (2018) consider the setup of open-domain question answering over unstructured text, but their dataset is constructed synthetically (with human paraphrasing) by combining simple questions with a few rules. In parallel to our work, Choi et al. (2018) and Reddy et al. (2018) introduce sequential question answering datasets (QuAC and CoQA) that focus on the reading comprehension setup (i.e., a single text snippet is pre-specified for answering the given questions). QBLink is entirely naturally occurring (all questions and answers were authored independently from any knowledge sources) and is primarily designed to challenge human players. The idea of our baseline to improving the reading step by incorporating additional relation description spans is similar as Weissenborn et al. (2017) and Mihaylov and Frank (2018), who integrate background"
D18-1134,P18-1078,0,0.224835,"tial question answering, asking multiple related questions. We present QBLink, a new dataset of fully human-authored questions. We extend existing strong question answering frameworks to include previous questions to improve the overall question-answering accuracy in open-domain question answering. The dataset is publicly available at http:// sequential.qanta.org. 1 Introduction The framework of combining information retrieval and neural reading comprehension has been the basis of several systems for answering open-domain questions over unstructured text (Chen et al., 2017; Wang et al., 2018; Clark and Gardner, 2018; Htut et al., 2018). Typically, such systems take one input question at a time, retrieving and ranking multiple paragraphs that potentially contain the answer. A reading comprehension model then produces a ranked list of candidate answer spans from each paragraph. The final answer is then selected from the produced spans. In information-seeking dialogs, e.g., personal assistants, users interact with a question answering system by asking a sequence of related questions, where questions share the same predicate, entities, or at least a topic. Answering each question in isolation is sub-optimal"
D18-1134,P18-1076,0,0.0188237,"w rules. In parallel to our work, Choi et al. (2018) and Reddy et al. (2018) introduce sequential question answering datasets (QuAC and CoQA) that focus on the reading comprehension setup (i.e., a single text snippet is pre-specified for answering the given questions). QBLink is entirely naturally occurring (all questions and answers were authored independently from any knowledge sources) and is primarily designed to challenge human players. The idea of our baseline to improving the reading step by incorporating additional relation description spans is similar as Weissenborn et al. (2017) and Mihaylov and Frank (2018), who integrate background commonsense knowledge into readingcomprehension systems. Both rely on structured knowledge bases to extract information about semantic relations that hold between entities. On the other hand, we extract text spans that mention each pair of entities and encoded them into vector representations of the relations between entities. 7 Conclusions and Future Work We introduce QBLink, a dataset of 56,000 naturally occurring sequential question, answer pairs. The questions are designed primarily to challenge human players in Quiz Bowl tournaments. We use QBLink to evaluate ba"
D18-1134,P16-1105,0,0.0671139,"Missing"
D18-1134,Q17-1008,0,0.0596073,"Missing"
D18-1134,D16-1264,0,0.0605146,"rrect answer. 6 Related Work and Discussion We adopt the open-domain question answering framework (Wang et al., 2018; Chen et al., 2017). Previous work considers improving that base framework itself (Clark and Gardner, 2018; Swayamdipta et al., 2018, inter alia). But retains the assumption of answering individual questions. Aside from the open-domain setup, much of the 1080 recent work on question answering has focused on the sub-problem of reading-comprehension, where the gold answer to each question is assumed to exist in a given single paragraph for the model to read (Hermann et al., 2015; Rajpurkar et al., 2016; Seo et al., 2017). Another line of work on question answering is question answering over structured knowledge-bases (Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014; Gardner and Krishnamurthy, 2017). Although we focus on the more general open-domain setup, QBLink can be adapted to be usable in the readingcomprehension setup as well as the question answering over knowledge-bases setup. Several question answering datasets have been proposed (Berant et al., 2013; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018, inter alia). However, all of them were limit"
D18-1134,P18-2124,0,0.0375069,"for the model to read (Hermann et al., 2015; Rajpurkar et al., 2016; Seo et al., 2017). Another line of work on question answering is question answering over structured knowledge-bases (Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014; Gardner and Krishnamurthy, 2017). Although we focus on the more general open-domain setup, QBLink can be adapted to be usable in the readingcomprehension setup as well as the question answering over knowledge-bases setup. Several question answering datasets have been proposed (Berant et al., 2013; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018, inter alia). However, all of them were limited to answering individual questions. Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for the task. However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases. 2) Their dataset construction was overly synthetic: templates were collected by human annotators given knowledge-base predicates. Further, sequences were constructed synthetically as well by grouping individual questions by predicate or subjects. Both Iyyer et al. (2017) and Talmor and Berant"
D18-1134,N18-4017,0,0.0163055,"asking multiple related questions. We present QBLink, a new dataset of fully human-authored questions. We extend existing strong question answering frameworks to include previous questions to improve the overall question-answering accuracy in open-domain question answering. The dataset is publicly available at http:// sequential.qanta.org. 1 Introduction The framework of combining information retrieval and neural reading comprehension has been the basis of several systems for answering open-domain questions over unstructured text (Chen et al., 2017; Wang et al., 2018; Clark and Gardner, 2018; Htut et al., 2018). Typically, such systems take one input question at a time, retrieving and ranking multiple paragraphs that potentially contain the answer. A reading comprehension model then produces a ranked list of candidate answer spans from each paragraph. The final answer is then selected from the produced spans. In information-seeking dialogs, e.g., personal assistants, users interact with a question answering system by asking a sequence of related questions, where questions share the same predicate, entities, or at least a topic. Answering each question in isolation is sub-optimal as information from"
D18-1134,P17-1167,0,0.0293197,"hler et al., 2017; Rajpurkar et al., 2018, inter alia). However, all of them were limited to answering individual questions. Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for the task. However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases. 2) Their dataset construction was overly synthetic: templates were collected by human annotators given knowledge-base predicates. Further, sequences were constructed synthetically as well by grouping individual questions by predicate or subjects. Both Iyyer et al. (2017) and Talmor and Berant (2018) answer complex questions by decomposing each into a sequence of simple questions. Iyyer et al. (2017) adopt a semantic parsing approach to answer questions over semi-structured tables. They construct a dataset of around 6,000 question sequences by asking humans to rewrite a set of 2,000 complex questions into simple sequences. Talmor and Berant (2018) consider the setup of open-domain question answering over unstructured text, but their dataset is constructed synthetically (with human paraphrasing) by combining simple questions with a few rules. In parallel to our"
D18-1134,P17-1147,0,0.0477179,"assumed to exist in a given single paragraph for the model to read (Hermann et al., 2015; Rajpurkar et al., 2016; Seo et al., 2017). Another line of work on question answering is question answering over structured knowledge-bases (Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014; Gardner and Krishnamurthy, 2017). Although we focus on the more general open-domain setup, QBLink can be adapted to be usable in the readingcomprehension setup as well as the question answering over knowledge-bases setup. Several question answering datasets have been proposed (Berant et al., 2013; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018, inter alia). However, all of them were limited to answering individual questions. Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for the task. However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases. 2) Their dataset construction was overly synthetic: templates were collected by human annotators given knowledge-base predicates. Further, sequences were constructed synthetically as well by grouping individual questions by predicate or subjects"
D18-1134,N18-1059,0,0.0499311,"rkar et al., 2018, inter alia). However, all of them were limited to answering individual questions. Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for the task. However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases. 2) Their dataset construction was overly synthetic: templates were collected by human annotators given knowledge-base predicates. Further, sequences were constructed synthetically as well by grouping individual questions by predicate or subjects. Both Iyyer et al. (2017) and Talmor and Berant (2018) answer complex questions by decomposing each into a sequence of simple questions. Iyyer et al. (2017) adopt a semantic parsing approach to answer questions over semi-structured tables. They construct a dataset of around 6,000 question sequences by asking humans to rewrite a set of 2,000 complex questions into simple sequences. Talmor and Berant (2018) consider the setup of open-domain question answering over unstructured text, but their dataset is constructed synthetically (with human paraphrasing) by combining simple questions with a few rules. In parallel to our work, Choi et al. (2018) and"
D18-1134,W17-2623,0,0.0332612,"a given single paragraph for the model to read (Hermann et al., 2015; Rajpurkar et al., 2016; Seo et al., 2017). Another line of work on question answering is question answering over structured knowledge-bases (Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014; Gardner and Krishnamurthy, 2017). Although we focus on the more general open-domain setup, QBLink can be adapted to be usable in the readingcomprehension setup as well as the question answering over knowledge-bases setup. Several question answering datasets have been proposed (Berant et al., 2013; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018, inter alia). However, all of them were limited to answering individual questions. Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for the task. However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases. 2) Their dataset construction was overly synthetic: templates were collected by human annotators given knowledge-base predicates. Further, sequences were constructed synthetically as well by grouping individual questions by predicate or subjects. Both Iyyer et al. (201"
D18-1134,P14-1090,0,0.0845706,"Missing"
D18-1407,W16-1601,0,0.122618,"Missing"
D18-1407,D15-1075,0,0.24531,"rather than providing explanations of the original prediction, our reduced examples more closely resemble adversarial examples. In Figure 1, the reduced input is meaningless to a human but retains the same model prediction with high confidence. Gradient-based input reduction exposes pathological model behaviors that contradict what one expects based on existing interpretation methods. In Section 2, we construct more of these counterintuitive examples by augmenting input reduction with beam search and experiment with three tasks: SQ UAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering. Input reduction with beam search consistently reduces the input sentence to very short lengths—often only one or two words—without lowering model confidence on its original prediction. The reduced examples appear nonsensical to humans, which we verify with crowdsourced experiments. In Section 3, we draw connections to adversarial examples and confidence calibration; we explain why the observed pathologies are a consequence of the overconfidence of neural models. This elucidates limitations of interpretation me"
D18-1407,D12-1118,1,0.90524,"Missing"
D18-1407,P17-1171,0,0.0140056,"e model changes its prediction. We experi3720 ment with three popular datasets: SQ UAD (Rajpurkar et al., 2016) for reading comprehension, SNLI (Bowman et al., 2015) for textual entailment, and VQA (Antol et al., 2015) for visual question answering. We describe each of these tasks and the model we use below, providing full details in the Supplement. In SQ UAD, each example is a context paragraph and a question. The task is to predict a span in the paragraph as the answer. We reduce only the question while keeping the context paragraph unchanged. The model we use is the D R QA Document Reader (Chen et al., 2017). In SNLI, each example consists of two sentences: a premise and a hypothesis. The task is to predict one of three relationships: entailment, neutral, or contradiction. We reduce only the hypothesis while keeping the premise unchanged. The model we use is Bilateral Multi-Perspective Matching (B I MPM) (Wang et al., 2017). In VQA, each example consists of an image and a natural language question. We reduce only the question while keeping the image unchanged. The model we use is Show, Ask, Attend, and Answer (Kazemi and Elqursh, 2017). During the iterative reduction process, we ensure that the p"
D18-1407,N18-2017,0,0.083614,"ut—cause significant changes in the interpretation. Ghorbani et al. (2017) make a similar observation about secondorder sensitivity, that “the fragility of interpretation is orthogonal to fragility of the prediction”. Previous work studies biases in the annotation process that lead to datasets easier than desired or expected which eventually induce pathological models. We attribute our observed pathologies primarily to the lack of accurate uncertainty estimates in neural models trained with maximum likelihood. SNLI hypotheses contain artifacts that allow training a model without the premises (Gururangan et al., 2018); we apply input reduction at test time to the hypothesis. Similarly, VQA images are surprisingly unimportant for training a model; we reduce the question. The recent SQ UAD 2.0 (Rajpurkar et al., 2018) augments the original reading comprehension task with an uncertainty modeling requirement, the goal being to make the task more realistic and challenging. Section 3.1 explains the pathologies from the overconfidence perspective. One explanation for overconfidence is overfitting: Guo et al. (2017) show that, late in maximum likelihood training, 3725 the model learns to minimize loss by outputtin"
D18-1407,P15-1162,1,0.896952,"Missing"
D18-1407,N18-1170,1,0.844269,"n et al., 2015), but to our knowledge not for NLP. Our input reduction process gradually transforms a valid input into a rubbish example. We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5. These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution. The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output. HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words. Our work and Belinkov and Bisk (2018) both identify cases where noisy user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably high model confidence."
D18-1407,D17-1215,0,0.0558403,"w et al., 2015; Nguyen et al., 2015), but to our knowledge not for NLP. Our input reduction process gradually transforms a valid input into a rubbish example. We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5. These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution. The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output. HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words. Our work and Belinkov and Bisk (2018) both identify cases where noisy user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably hig"
D18-1407,N16-1082,0,0.389222,"he instability of neural network predictions by showing how small perturbations to the input dramatically change the output. A common, non-adversarial form of model interpretation is feature attribution: features that are crucial for predictions are highlighted in a heatmap. One can measure a feature’s importance by input perturbation. Given an input for text classification, a word’s importance can be measured by the difference in model confidence before and after that word is removed from the input—the word is important if confidence decreases significantly. This is the leave-one-out method (Li et al., 2016b). Gradients can also measure feature importance; for example, a feature is influential to the prediction if its gradient is a large positive value. Both perturbation and gradient-based methods can generate heatmaps, implying that the model’s prediction is highly influenced by the highlighted, important words. Instead, we study how the model’s prediction is influenced by the unimportant words. We use input reduction, a process that iteratively removes the unimportant words from the input while maintaining the model’s prediction. Intuitively, the words remaining after input reduction should be"
D18-1407,P11-1015,0,0.175621,"Missing"
D18-1407,D16-1244,0,0.109526,"Missing"
D18-1407,P18-2124,0,0.0185617,"e prediction”. Previous work studies biases in the annotation process that lead to datasets easier than desired or expected which eventually induce pathological models. We attribute our observed pathologies primarily to the lack of accurate uncertainty estimates in neural models trained with maximum likelihood. SNLI hypotheses contain artifacts that allow training a model without the premises (Gururangan et al., 2018); we apply input reduction at test time to the hypothesis. Similarly, VQA images are surprisingly unimportant for training a model; we reduce the question. The recent SQ UAD 2.0 (Rajpurkar et al., 2018) augments the original reading comprehension task with an uncertainty modeling requirement, the goal being to make the task more realistic and challenging. Section 3.1 explains the pathologies from the overconfidence perspective. One explanation for overconfidence is overfitting: Guo et al. (2017) show that, late in maximum likelihood training, 3725 the model learns to minimize loss by outputting low-entropy distributions without improving validation accuracy. To examine if overfitting can explain the input reduction results, we run input reduction using D R QA model checkpoints from every tra"
D18-1407,N16-3020,0,0.549742,"he reduced examples appear nonsensical to humans, which we verify with crowdsourced experiments. In Section 3, we draw connections to adversarial examples and confidence calibration; we explain why the observed pathologies are a consequence of the overconfidence of neural models. This elucidates limitations of interpretation methods that rely on model confidence. In Section 4, we encourage high model uncertainty on reduced examples with entropy regularization. The pathological model behavior under input reduction is mitigated, leading to more reasonable reduced examples. 2 Input Reduction 2.1 Ribeiro et al. (2016) and Li et al. (2016b) define importance by seeing how confidence changes when a feature is removed; a natural approximation is to use the gradient (Baehrens et al., 2010; Simonyan et al., 2014). We formally define these importance metrics in natural language contexts and introduce the efficient gradient-based approximation. For each word in an input sentence, we measure its importance by the change in the confidence of the original prediction when we remove that word from the sentence. We switch the sign so that when the confidence decreases, the importance value is positive. Formally, let x"
D18-1407,P18-1079,0,0.0395479,"to our knowledge not for NLP. Our input reduction process gradually transforms a valid input into a rubbish example. We can often determine which word’s removal causes the transition to occur—for example, removing “Broncos” in Figure 5. These rubbish examples are particularly interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise which are far outside the training data distribution. The robustness of NLP models has been studied extensively (Papernot et al., 2016; Jia and Liang, 2017; Iyyer et al., 2018; Ribeiro et al., 2018), and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output. HotFlip (Ebrahimi et al., 2017) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words. Our work and Belinkov and Bisk (2018) both identify cases where noisy user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input can lead to unreasonably high model confidence. Other failures of inter"
D18-1407,D14-1162,0,\N,Missing
D18-1407,P16-1223,0,\N,Missing
D18-1407,D16-1264,0,\N,Missing
D18-1407,D16-1011,0,\N,Missing
D18-1407,W17-5401,0,\N,Missing
D19-1120,E14-1056,0,0.08912,"Missing"
D19-1120,D09-1092,0,0.023202,"are similar, outperforming LDA and previous MTMs in classification tasks using documents’ topic posteriors as features. It also learns coherent topics on documents with low comparability. 1 Topic models explain document collections at a high level (Boyd-Graber et al., 2017). Multilingual topic models (MTMs) uncover latent topics across languages and reveal commonalities and differences across languages and cultures (Ni et al., 2009; Shi et al., 2016; Guti´errez et al., 2016). Existing models extend latent Dirichlet allocation (Blei et al., 2003, LDA) and learn aligned topics across languages (Mimno et al., 2009). Prior models work well because they implicitly assume—even if not part of the model—parallel or highly comparable data with well-aligned topics. However, this assumption does not always comport with reality. Even documents from the same place and time can discuss very different things across languages: in multicultural London, Hindi tweets focus on a Bollywood actor’s BBC appearance, French blogs fret about Brexit, and English articles focus on Tottenham’s lineup. Generally, corpora have a range of “nonparallelness” (Fung, 2000). In less comparable settings, while some † Now at Facebook Now"
D19-1120,P16-1064,0,0.120625,"ful in important low-resource language scenarios. Our MTM learns weighted topic links and connects cross-lingual topics only when the dominant words defining them are similar, outperforming LDA and previous MTMs in classification tasks using documents’ topic posteriors as features. It also learns coherent topics on documents with low comparability. 1 Topic models explain document collections at a high level (Boyd-Graber et al., 2017). Multilingual topic models (MTMs) uncover latent topics across languages and reveal commonalities and differences across languages and cultures (Ni et al., 2009; Shi et al., 2016; Guti´errez et al., 2016). Existing models extend latent Dirichlet allocation (Blei et al., 2003, LDA) and learn aligned topics across languages (Mimno et al., 2009). Prior models work well because they implicitly assume—even if not part of the model—parallel or highly comparable data with well-aligned topics. However, this assumption does not always comport with reality. Even documents from the same place and time can discuss very different things across languages: in multicultural London, Hindi tweets focus on a Bollywood actor’s BBC appearance, French blogs fret about Brexit, and English a"
D19-1120,L16-1521,0,0.0682902,"Missing"
D19-1120,D15-1037,1,0.927645,"translation dictionary. We validate the MTM in two classification tasks using inferred topic posteriors as features. Our 1243 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1243–1248, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics MTM has higher F1 than other models in both intra- and cross-lingual evaluations, while discovering coherent topics and meaningful topic links. 2 Multilingual Topic Model for Connecting Cross-Lingual Topics Yang et al. (2015) present a flexible framework for adding regularization to topic models. We extend this model to the multilingual setting by adding a potential function that links topics across languages. For simplicity of exposition, we focus on the bilingual case with languages S and T . Unlike Yang et al. (2015) that encode monolingual information only, our potential function encodes multilingual knowledge parameterized by two matrices, ρS→T and ρT →S , that transform topics between the two languages. Cells’ values are between 0 and 1 and a cell ρS→T,kT ,kS close to one is a strong connection of topics kT"
D19-1120,C18-1220,0,0.0365805,"Missing"
D19-1120,P14-1110,1,0.872849,"1 26.7 33.3 15.1 11.4 26.7 26.7 26.5 32.6 24.0 18.2 23.1 23.1 38.1 38.1 MCTA MTAnchor LDA ptLDA MTM MTM+TOP MTM+TF-IDF MTM+TF-IDF+TOP MCTA MTAnchor LDA ptLDA MTM MTM+TOP MTM+TF-IDF MTM+TF-IDF+TOP Cross-Lingual 4.1 English 13.0 20.8 27.8 12.8 MCTA MTAnchor LDA ptLDA MTM MTM+TOP MTM+TF-IDF MTM+TF-IDF+TOP 0 25 50 75 100 Figure 3: The F1 scores on disaster response (upper) and Wikipedia (lower) datasets. Our MTM outperforms all the baselines in intra- and cross-lingual evaluations. which is the proportion of the tokens assigned to topic k in document d. The baselines include polylingual tree LDA (Hu et al., 2014, ptLDA) which encodes the dictionary as a tree prior (Andrzejewski et al., 2009), Multilingual Topic Anchoring (Yuan et al., 2018, MTAnchor), and Multilingual Cultural-common Topic Analysis (Shi et al., 2016, MCTA). We also include LDA, which runs monolingually in each language. We use 20 topics and set hyperparameters α = 0.1 and β = 0.01 (if applicable). Our evaluations are both intra- and crosslingual. The intra-lingual evaluation trains and tests classifiers on the same language, while the cross-lingual evaluation trains classifiers on one language and tests on another. In cross-lingual e"
D19-1605,D18-1241,0,0.284689,"oblem, but existing datasets lack key elements of language understanding such as coreference and ellipsis resolution. We consider sequential question answering: multiple questions are asked one-by-one in a conversation between a questioner and an answerer. Answering these questions is only possible through understanding the conversation history. We introduce the task of question-in-context rewriting: given the context of a conversation’s history, rewrite a context-dependent into a selfcontained question with the same answer. We construct, CANARD, a dataset of 40,527 questions based on Q U AC (Choi et al., 2018) and train Seq2Seq models for incorporating context into standalone questions. 1 Q1: What happened to Anna Vissi in 1983? A1: In May 1983, she marries Nikos Karvelas, a composer Q2: Did they have any children? Did Anna Vissi and Nikos Karvelas have any children together? A2: In November, she gave birth to her daughter Sofia Q3: Did she have any other children? Did Anna Vissi have any other children than her daughter Sofia? A3: I don’t know Introduction Question Answering (QA) is an AI complete problem (Webber, 1992), but existing QA datasets do not rise to the challenge: they lack key NLP prob"
D19-1605,D14-1162,0,0.0824424,"rewrites are the upper-bound for this task. However, annotators (especially crowdworkers) can be inconsistent or disagree. To estimate the human accuracy, we collect 100 pairs of rewritten questions; each pair has two rewrites of the same question (in its given context) by two different workers. We manually verify that all rewrites are valid and then use the pair of rewrites as a hypothesis and a reference. Table 3 shows the BLEU scores produced by the baselines and humans over both the validation and the test sets.3 Although a well-trained standard 2 We initialize the embeddings with GloVE (Pennington et al., 2014) and train with a batch-size of 16 for 200000 steps. We use OpenNMT (Klein et al., 2018) implementation. 3 We use multi-bleu-detok.perl (Sennrich et al., 2017) 5920 Label QUESTION REWRITE Average Length HISTORY Pronoun Ratio Original Pronoun Sub Reference Seq2Seq Figure 2: Human rewrites are longer, have fewer pronouns, and have more proper nouns than the original Q U AC questions. Rewrites are longer and contain more proper nouns than our Pronoun Sub baseline and trained Seq2Seq model. neural sequence-to-sequence improves 2–4 BLEU points over naive baselines, it is still 9 BLEU points below h"
D19-1605,D17-1091,0,0.025021,"nt learning could learn to retain the necessary context to rewrite questions in CQA . However, our dataset could be used to pretrain a question rewriter that can further be refined using reinforcement learning. More broadly, we hope CANARD can drive human-computer collaboration in QA (Feng and Boyd-Graber, 2019). While questions typically vary in difficulty (Sugawara et al., 2018), existing research either introduces new benchmarks of difficult (adversarial) stand-alone questions (Dua et al., 2019; Wallace et al., 2019, inter alia), or models that simplify hard questions through paraphrasing (Dong et al., 2017) or decomposition (Talmor and Berant, 2018). We aim at studying QA models that can ask for human assistance (feedback) when they struggle to answer a question. The reading comprehension setup of CQA provides a controlled environment where the main source of difficulty is interpreting a question in its context. The interactive component of CQA also provides a natural mechanism for improving rewriting. When the computer cannot understand (rewrite) a question because of complicated context, missing world knowledge, or upstream errors (Peskov et al., 2019) in the course of a conversation, it shoul"
D19-1605,D16-1264,0,0.0573826,"y other children? Did Anna Vissi have any other children than her daughter Sofia? A3: I don’t know Introduction Question Answering (QA) is an AI complete problem (Webber, 1992), but existing QA datasets do not rise to the challenge: they lack key NLP problems like anaphora resolution, coreference disambiguation, and ellipsis resolution. The logic needed to answer these types of questions requires deeper NLP understanding that simulates the context in which humans naturally answer questions. Neural techniques question answering have improved (Devlin et al., 2018) machine reading comprehension (Rajpurkar et al., 2016, MRC): computers can take a single question and extract answers from datasets like Wikipedia. However, QA models struggle to generalize when questions do not look like the standalone questions systems in training data: e.g., new genres, languages, or closely-related tasks (Yogatama et al., 2019). Conversational question answering (Reddy et al., 2019, CQA) is a generalization that ask multiple ∗ What happened in 1983? Figure 1: Question-in-context rewriting task. The input to each step is a question to rewrite given the dialog history which consists of the dialog utterances (questions and answ"
D19-1605,N19-1246,0,0.0253811,"in retrieval-based QA (Liu et al., 2019) and in MRC (Buck et al., 2018). A natural question is whether reinforcement learning could learn to retain the necessary context to rewrite questions in CQA . However, our dataset could be used to pretrain a question rewriter that can further be refined using reinforcement learning. More broadly, we hope CANARD can drive human-computer collaboration in QA (Feng and Boyd-Graber, 2019). While questions typically vary in difficulty (Sugawara et al., 2018), existing research either introduces new benchmarks of difficult (adversarial) stand-alone questions (Dua et al., 2019; Wallace et al., 2019, inter alia), or models that simplify hard questions through paraphrasing (Dong et al., 2017) or decomposition (Talmor and Berant, 2018). We aim at studying QA models that can ask for human assistance (feedback) when they struggle to answer a question. The reading comprehension setup of CQA provides a controlled environment where the main source of difficulty is interpreting a question in its context. The interactive component of CQA also provides a natural mechanism for improving rewriting. When the computer cannot understand (rewrite) a question because of complicated"
D19-1605,N19-2013,0,0.0510608,"has used simple concatenation (Elgohary et al., 2018), sequential neural models (Huang et al., 2019), and transformers (Qu et al., 2019a) for modeling the interaction between the conversation history, the question and reference documents. Some of the components in those models, such as relevant history turn selection (Qu et al., 2019b), can be adopted in question rewriting models for our task. An interesting avenue for future work is to incorporate deeper context, either from other modalities (Das et al., 2017) or from other dialog comprehension tasks (Sun et al., 2019). Parallel to our work, Rastogi et al. (2019) and Su et al. (2019) introduce utterance rewriting datasets for dialog state tracking. Rastogi et al. (2019) covers a narrow set of domains and the rewrites of Su et al. (2019) are based on Chinese dialog with two-turn fixed histories. In contrast, CANARD has histories of variable turn lengths, covers wider topics, and is based on CQA. Training question rewriting using reinforcement learning with the task accuracy as reward signal is explored in retrieval-based QA (Liu et al., 2019) and in MRC (Buck et al., 2018). A natural question is whether reinforcement learning could learn to retain the"
D19-1605,D18-1134,1,0.804938,"in any other countries? Did Giannina Braschi have any more works than United States of Banana, La Comedia and Asalto al tiempo? Table 5: Example erroneous rewrites generated by the Seq2Seq models and their corresponding reference rewrites. The dominant source of error is the model tendency to produce short rewrites (Examples 1–3). Related entities (Copa America and Argentina in Example 4) distract the model. The model struggles with listing multiple entities mentioned in different parts of the context (Example 5). 6 Related Work and Discussion Recent work in CQA has used simple concatenation (Elgohary et al., 2018), sequential neural models (Huang et al., 2019), and transformers (Qu et al., 2019a) for modeling the interaction between the conversation history, the question and reference documents. Some of the components in those models, such as relevant history turn selection (Qu et al., 2019b), can be adopted in question rewriting models for our task. An interesting avenue for future work is to incorporate deeper context, either from other modalities (Das et al., 2017) or from other dialog comprehension tasks (Sun et al., 2019). Parallel to our work, Rastogi et al. (2019) and Su et al. (2019) introduce"
D19-1605,W18-1817,0,0.0179861,"n be inconsistent or disagree. To estimate the human accuracy, we collect 100 pairs of rewritten questions; each pair has two rewrites of the same question (in its given context) by two different workers. We manually verify that all rewrites are valid and then use the pair of rewrites as a hypothesis and a reference. Table 3 shows the BLEU scores produced by the baselines and humans over both the validation and the test sets.3 Although a well-trained standard 2 We initialize the embeddings with GloVE (Pennington et al., 2014) and train with a batch-size of 16 for 200000 steps. We use OpenNMT (Klein et al., 2018) implementation. 3 We use multi-bleu-detok.perl (Sennrich et al., 2017) 5920 Label QUESTION REWRITE Average Length HISTORY Pronoun Ratio Original Pronoun Sub Reference Seq2Seq Figure 2: Human rewrites are longer, have fewer pronouns, and have more proper nouns than the original Q U AC questions. Rewrites are longer and contain more proper nouns than our Pronoun Sub baseline and trained Seq2Seq model. neural sequence-to-sequence improves 2–4 BLEU points over naive baselines, it is still 9 BLEU points below human-accuracy. We analyze sources of errors in the following section. 5 biguous than tho"
D19-1605,P17-1099,0,0.0655818,"me as the input the rewrite qm question qm without making any changes. We also try a Pronoun Substitution baseline in which the first pronoun in qm is replaced with the topic entity of the conversation. We use the title of the corresponding Wikipedia article to the original Q U AC conversation as the topic entity. Similar to the Copy baseline, the training data is not used in that baseline. Unlike the previous baselines which do not use our rewrites as training data, the third baseline is a neural sequence-to-sequence (Seq2Seq) model with attention and a copy mechanism (Bahdanau et al., 2015; See et al., 2017). We construct the input sequence by concatenating all utterances in the history H, prepending them to qm , and adding a special separator token between utterances. We use a bidirectional LSTM encoder-decoder model with shared the word embeddings between the encoder and the decoder.2 Since questions are written by humans, a human rewrites are the upper-bound for this task. However, annotators (especially crowdworkers) can be inconsistent or disagree. To estimate the human accuracy, we collect 100 pairs of rewritten questions; each pair has two rewrites of the same question (in its given contex"
D19-1605,E17-3017,0,0.0632101,"Missing"
D19-1605,P19-1003,0,0.0899439,"ion (Elgohary et al., 2018), sequential neural models (Huang et al., 2019), and transformers (Qu et al., 2019a) for modeling the interaction between the conversation history, the question and reference documents. Some of the components in those models, such as relevant history turn selection (Qu et al., 2019b), can be adopted in question rewriting models for our task. An interesting avenue for future work is to incorporate deeper context, either from other modalities (Das et al., 2017) or from other dialog comprehension tasks (Sun et al., 2019). Parallel to our work, Rastogi et al. (2019) and Su et al. (2019) introduce utterance rewriting datasets for dialog state tracking. Rastogi et al. (2019) covers a narrow set of domains and the rewrites of Su et al. (2019) are based on Chinese dialog with two-turn fixed histories. In contrast, CANARD has histories of variable turn lengths, covers wider topics, and is based on CQA. Training question rewriting using reinforcement learning with the task accuracy as reward signal is explored in retrieval-based QA (Liu et al., 2019) and in MRC (Buck et al., 2018). A natural question is whether reinforcement learning could learn to retain the necessary context to"
D19-1605,N18-1059,0,0.0285246,"necessary context to rewrite questions in CQA . However, our dataset could be used to pretrain a question rewriter that can further be refined using reinforcement learning. More broadly, we hope CANARD can drive human-computer collaboration in QA (Feng and Boyd-Graber, 2019). While questions typically vary in difficulty (Sugawara et al., 2018), existing research either introduces new benchmarks of difficult (adversarial) stand-alone questions (Dua et al., 2019; Wallace et al., 2019, inter alia), or models that simplify hard questions through paraphrasing (Dong et al., 2017) or decomposition (Talmor and Berant, 2018). We aim at studying QA models that can ask for human assistance (feedback) when they struggle to answer a question. The reading comprehension setup of CQA provides a controlled environment where the main source of difficulty is interpreting a question in its context. The interactive component of CQA also provides a natural mechanism for improving rewriting. When the computer cannot understand (rewrite) a question because of complicated context, missing world knowledge, or upstream errors (Peskov et al., 2019) in the course of a conversation, it should be able to ask its interlocutor, “can you"
D19-1605,Q19-1029,1,0.848352,"QA (Liu et al., 2019) and in MRC (Buck et al., 2018). A natural question is whether reinforcement learning could learn to retain the necessary context to rewrite questions in CQA . However, our dataset could be used to pretrain a question rewriter that can further be refined using reinforcement learning. More broadly, we hope CANARD can drive human-computer collaboration in QA (Feng and Boyd-Graber, 2019). While questions typically vary in difficulty (Sugawara et al., 2018), existing research either introduces new benchmarks of difficult (adversarial) stand-alone questions (Dua et al., 2019; Wallace et al., 2019, inter alia), or models that simplify hard questions through paraphrasing (Dong et al., 2017) or decomposition (Talmor and Berant, 2018). We aim at studying QA models that can ask for human assistance (feedback) when they struggle to answer a question. The reading comprehension setup of CQA provides a controlled environment where the main source of difficulty is interpreting a question in its context. The interactive component of CQA also provides a natural mechanism for improving rewriting. When the computer cannot understand (rewrite) a question because of complicated context, missing world"
D19-6016,P17-1080,0,0.0332854,"how such comparisons are made: models learn a consistent ordering over all the objects in the comparisons. Probing models have significantly higher accuracy than those baseline models which use dataset artifacts: e.g., memorizing some words are larger than any other word. 1 Introduction Pre-trained word representations or embeddings (Mikolov et al., 2013) such as GloVe (Pennington et al., 2014) underpin modern NLP. To understand what information is encoded, supervised models probe (Adi et al., 2016; Linzen et al., 2016; Conneau et al., 2018) a particular property, for example, part-of-speech (Belinkov et al., 2017), morphology (Peters et al., 2018a), etc. in these representations. With the advent of contextualized word embeddings such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), efforts to understand the information encoded in representations learned by neural model have increased (Peters et al., 2018b; Tenney et al., 2019; Liu et al., 2019). Apart from linguistic properties, what do these representations learn about the world? Commonsense reasoning over language that incorporates world knowledge such as ‘an elephant is heavier than a person’ can help agents make better decisions and u"
D19-6016,P18-1198,0,0.0428506,"Missing"
D19-6016,D14-1162,0,0.0920894,"robe whether pre-trained representations capture comparisons and find they, in fact, have higher accuracy than previous approaches. They also generalize to comparisons involving objects not seen during training. We investigate how such comparisons are made: models learn a consistent ordering over all the objects in the comparisons. Probing models have significantly higher accuracy than those baseline models which use dataset artifacts: e.g., memorizing some words are larger than any other word. 1 Introduction Pre-trained word representations or embeddings (Mikolov et al., 2013) such as GloVe (Pennington et al., 2014) underpin modern NLP. To understand what information is encoded, supervised models probe (Adi et al., 2016; Linzen et al., 2016; Conneau et al., 2018) a particular property, for example, part-of-speech (Belinkov et al., 2017), morphology (Peters et al., 2018a), etc. in these representations. With the advent of contextualized word embeddings such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), efforts to understand the information encoded in representations learned by neural model have increased (Peters et al., 2018b; Tenney et al., 2019; Liu et al., 2019). Apart from linguistic"
D19-6016,N18-1202,0,0.320493,"s learn a consistent ordering over all the objects in the comparisons. Probing models have significantly higher accuracy than those baseline models which use dataset artifacts: e.g., memorizing some words are larger than any other word. 1 Introduction Pre-trained word representations or embeddings (Mikolov et al., 2013) such as GloVe (Pennington et al., 2014) underpin modern NLP. To understand what information is encoded, supervised models probe (Adi et al., 2016; Linzen et al., 2016; Conneau et al., 2018) a particular property, for example, part-of-speech (Belinkov et al., 2017), morphology (Peters et al., 2018a), etc. in these representations. With the advent of contextualized word embeddings such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), efforts to understand the information encoded in representations learned by neural model have increased (Peters et al., 2018b; Tenney et al., 2019; Liu et al., 2019). Apart from linguistic properties, what do these representations learn about the world? Commonsense reasoning over language that incorporates world knowledge such as ‘an elephant is heavier than a person’ can help agents make better decisions and understand ‘complex’ phenomena lik"
D19-6016,P19-1388,0,0.104213,"Missing"
D19-6016,D18-1179,0,0.0611754,"s learn a consistent ordering over all the objects in the comparisons. Probing models have significantly higher accuracy than those baseline models which use dataset artifacts: e.g., memorizing some words are larger than any other word. 1 Introduction Pre-trained word representations or embeddings (Mikolov et al., 2013) such as GloVe (Pennington et al., 2014) underpin modern NLP. To understand what information is encoded, supervised models probe (Adi et al., 2016; Linzen et al., 2016; Conneau et al., 2018) a particular property, for example, part-of-speech (Belinkov et al., 2017), morphology (Peters et al., 2018a), etc. in these representations. With the advent of contextualized word embeddings such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), efforts to understand the information encoded in representations learned by neural model have increased (Peters et al., 2018b; Tenney et al., 2019; Liu et al., 2019). Apart from linguistic properties, what do these representations learn about the world? Commonsense reasoning over language that incorporates world knowledge such as ‘an elephant is heavier than a person’ can help agents make better decisions and understand ‘complex’ phenomena lik"
D19-6016,P17-1025,0,0.327255,"ervised classification task takes a pair of words being compared on a physical attribute such as size or speed, with the system’s objective to decide which is ‘bigger’ or ‘faster’ (§ 2.1). We use a linear or a one-layer fully-connected neural network probing model with only a combination (concatenation or subtraction) of the frozen pre-trained embeddings for the words to be compared as input (§ 2.2). This probing model achieves better accuracy than previous approaches (§ 2.3) which use extra information other than the words (such as the verbs connecting the words) on the Verb Physics dataset (Forbes and Choi, 2017) (§ 3): it encodes physical commonsense comparisons.2 It generalizes to objects not present in the training set (§ 3.1) with higher accuracy than baselines exploiting dataset artifacts (§ 4). We use a ‘simple’ probing model since more complex models make it difficult to disentangle the major contributing factor to results - model or embeddings (as in other probing studies like Liu et al. (2019)). Our other major contribution is analyzing how models compare objects. The output logits for labels (indicating model confidence) order objects consistently across orderings or rankings built around di"
D19-6016,N15-1098,0,0.0479347,"Missing"
D19-6016,P18-2102,0,0.0577361,"ajority labels disagree (both words tend to be ‘bigger’ most of the times), this baseline uses the ratio of frequency of the majority label with the total number of comparisons involving the word to decide. We also compare with the previous state-of-theart approaches on the Verb Physics dataset: Verb-centric Frame Semantics: (Forbes and Choi, 2017, F&C) use probabilistic graphical modeling for joint inference over objects as well as actions/verbs that can imply physical relationship their arguments (for example, ‘x entered y’ implies y is bigger than x). Property Comparisons from Embeddings: (Yang et al., 2018, PCE) use a one-layer neural network over concatenated word embeddings and compare the projection with the embeddings of ‘poles’: words exemplifying a physical relation (‘big’, ‘small’ for size; ‘fast’, ‘slow’ for speed, etc.). Classification is the closest ‘pole’. This use of poles is the main difference with our approach. Apart from these baseline models, we devise additional baselines to test for possible artifacts in the dataset, such as using only one of the words as input to the model, in Section 4. Our Probing Model The probing model is a simple setup to assess if pre-trained represent"
D19-6016,Q16-1037,0,0.0311938,"They also generalize to comparisons involving objects not seen during training. We investigate how such comparisons are made: models learn a consistent ordering over all the objects in the comparisons. Probing models have significantly higher accuracy than those baseline models which use dataset artifacts: e.g., memorizing some words are larger than any other word. 1 Introduction Pre-trained word representations or embeddings (Mikolov et al., 2013) such as GloVe (Pennington et al., 2014) underpin modern NLP. To understand what information is encoded, supervised models probe (Adi et al., 2016; Linzen et al., 2016; Conneau et al., 2018) a particular property, for example, part-of-speech (Belinkov et al., 2017), morphology (Peters et al., 2018a), etc. in these representations. With the advent of contextualized word embeddings such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), efforts to understand the information encoded in representations learned by neural model have increased (Peters et al., 2018b; Tenney et al., 2019; Liu et al., 2019). Apart from linguistic properties, what do these representations learn about the world? Commonsense reasoning over language that incorporates world kn"
D19-6016,N19-1112,0,0.080162,". This probing model achieves better accuracy than previous approaches (§ 2.3) which use extra information other than the words (such as the verbs connecting the words) on the Verb Physics dataset (Forbes and Choi, 2017) (§ 3): it encodes physical commonsense comparisons.2 It generalizes to objects not present in the training set (§ 3.1) with higher accuracy than baselines exploiting dataset artifacts (§ 4). We use a ‘simple’ probing model since more complex models make it difficult to disentangle the major contributing factor to results - model or embeddings (as in other probing studies like Liu et al. (2019)). Our other major contribution is analyzing how models compare objects. The output logits for labels (indicating model confidence) order objects consistently across orderings or rankings built around different objects (§ 4.1.1). Models also learn an ordering over all the objects and use this learned ordering for comparisons (§ 4.1.2). Understanding common sense is important for effective natural language reasoning. One type of common sense is how two objects compare on physical properties such as size and weight: e.g., ‘is a house bigger than a person?’. We probe whether pre-trained represent"
K15-1020,N13-1132,0,0.151338,"8a shows that CS LDA performs poorly on the CrowdFlower-annotated Newsgroups documents described at the beginning of Section 3 (not the synthetic annotations). Error analysis uncovers that CS LDA lumps related classes together in this dataset. This is because annotators could specify up to 3 simultaneous labels for each annotation, so that similar labels (e.g., “talk.politics.misc” and “talk.politics.mideast”) are usually chosen in blocks. Suppose each member of a set of documents with similar topical content is annotated 200 and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et a"
K15-1020,N13-1062,0,0.0800508,"quires labeled training corpora, historically produced by laborious and costly annotation projects. Microtask markets such as Amazon’s Mechanical Turk and Crowdflower have turned crowd labor into a commodity that can be purchased with relatively little overhead. However, crowdsourced judgments can suffer from high error rates. A common solution to this problem is to obtain multiple redundant human judgments, or annotations,1 relying on the observation that, in aggregate, non-experts often rival or exceed experts by averaging over individual error patterns (Surowiecki, 2005; Snow et al., 2008; Jurgens, 2013). A crowdsourcing model harnesses the wisdom of the crowd and infers labels based on the evidence of the available annotations, imperfect 2 Latent Representations that Reflect Labels and Confusion Most crowdsourcing models extend the itemresponse model of Dawid and Skene (1979). The Bayesian version of this model, referred to here as I TEM R ESP, is depicted in Figure 1. In the generative story for this model, a confusion matrix γj is drawn for each human annotator j. Each row γjc of the confusion matrix γj is drawn from 1 In this paper, we call human judgments annotations to distinguish them"
K15-1020,felt-etal-2014-momresp,1,0.775126,"e model to overweight feature evidence and underweight annotation evidence. This imbalance can result in degraded performance in the presence of high quality or many annotations. Leveraging Data Some extensions to I TEM R ESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to I TEM R ESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from class-conditional"
K15-1020,N15-1089,1,0.190077,"rweight annotation evidence. This imbalance can result in degraded performance in the presence of high quality or many annotations. Leveraging Data Some extensions to I TEM R ESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to I TEM R ESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from class-conditional distributions. Felt et al. (2015) identify the"
K15-1020,W13-2323,0,0.0404535,"Missing"
K15-1020,C10-1099,0,0.0240967,"lassconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). CFGROUPS1000 0.7 Accuracy algorithm csLDA 0.6 MomResp LogResp 0.5 ItemResp Majority 0.4 0 2 4 6 Number of annotated instances x 1,000 (a) Original data CFSIMPLEGROUPS 0.95 Accuracy algorithm 0.90 csLDA MomResp 0.85 LogResp ItemResp 0.80 Majority 0.75 0 2 4 6 Number of annotated instances x 1,000 (b) After combining frequently co-annotated label classes Figure 8: An illustrative failure case. CS LDA, lacking a class label prior, prefers to combine label classes that are highly co-annotated. with both label A and B. In this scenario it is apparent that CS LDA will achieve its best fit by infer"
K15-1020,D08-1027,0,0.847468,"Missing"
K15-1020,Q14-1025,0,\N,Missing
K16-1010,matsubara-etal-2002-bilingual,0,0.0105705,"se preferences. This is consistent with our use of case-based features and suggests that further gains are possible using richer syntactic representations. Chow et al. (2015) use N 400 measurements to investigate two competing hypotheses for the initial prediction of an upcoming verb: whether predictions are dependent on all words equally (the Bag-of-words hypothesis), or alternatively, whether prediction is selectively modulated by the final verb’s arguments (the Bag-of-arguments hypothesis). They argue for the latter. The literature on incremental verb prediction is sparse. A key finding of Matsubara et al. (2002) is that Japanese-English simultaneous interpreters, when given access to lecture slides, would refer to them to predict the next phrase. Prediction for Simultaneous Machine Translation The Verbmobil simultaneous translation system (Kay et al., 1992) uses deleted interpolation (Jelinek, 1990) to create a weighted n-gram models to predict dialogue acts—almost identical to predicting the next word (Reithinger et al., 1996). Konieczny and D¨oring (2003) predict verbs with a recurrent neural network, but Matsubara et al. (2000) was the first to use verb predictions as 5 Conclusion Verb prediction"
K16-1010,D14-1140,1,0.825562,"Missing"
K16-1010,N01-1021,0,0.299754,"ction using discriminative classification with shallow features. Both humans and machines predict verbs more accurately as more of a sentence becomes available, and case markers—when available—help humans and sometimes machines predict final verbs. 1 The Importance of Verb Prediction Humans predict future linguistic input before it is observed (Kutas et al., 2011). This predictability has been formalized in information theory (Shannon, 1948)—the more predictable a word is, the lower the entropy—and has explained various linguistic phenomena, such as garden path ambiguity (Den and Inoue, 1997; Hale, 2001). Such instances of linguistic prediction are fundamental to statistical NLP. Auto-complete from search engines has made next-word prediction one of best known NLP applications. 95 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 95–104, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 2 Human Verb Prediction Answer Choice Selection We display the correct verb bunsetsu and three incorrect bunsetsu completions as choices that occur in the data with frequency close to the correct answer in the overall corpus."
K16-1010,P15-1020,0,0.301905,"rb phrase usually comes after the first noun phrase—the main subject—in a sentence, while in verb-final languages such as Japanese or German, it comes very last. Human simultaneous translators must make predictions about the unspoken final verb to incrementally translate the sentence. Minimizing interpretation delay thus requires making constant predictions and deciding when to trust those predictions and commit to translating in real-time. Such prediction can also aid machines. Matsubara et al. (2000) use pattern-matching rules; Grissom II et al. (2014) use a statistical n-gram approach; and Oda et al. (2015) extend the idea of using prediction by predicting entire syntactic constituents for English-Japanese translation. These systems require fast, accurate verb prediction to further improve simultaneous translation systems. We focus on verb prediction in verb-final languages such as Japanese with this motivation in mind. In Section 2, we present what is, to our knowledge, the first study of humans’ ability to incrementally predict the verbs in Japanese. We use these human data as a yardstick to which to compare computational incremental verb prediction. Incorporating some of the key insights from"
K16-1010,N03-1033,0,0.0270402,"owing this study, we train a model on the fifty most common verbs in the training set. In Japanese, due to the small size of the standard test set, we split the data randomly, training on 60,926 verb-final sentences ending in the top fifty verbs and testing on 1,932. Our total feature count is 4,649,055. We use the MeCab (Kudo, 2005) morphological analyzer for segmentation and verb identification. We consider only verb-final sentences. We skip semantically vacuous post-verbal copulas when identifying final verbs. Finding Verbs We identify verbs in the German text with a part-of-speech tagger (Toutanova et al., 2003) and select from the top fifty verbs. We consider the sentence-ending set of verbs to be the final verbs. We train on 76,209 verb-final sentences ending in the top fifty verbs and test on 9,386. In German, to approximate the case information that we extract in Japanese, we test the inclusion of equivalent unigram and bigram features for German articles, the surface forms of which determine the case of the next noun phrase. In Japanese, we omit some special cases of light verbs that combine with other verbs, as well as ambiguous surface forms and copulas.8 Features All features are encoded as b"
N12-1085,P11-2019,0,0.0144114,"syntax, we use a sentence’s syntactic structure to build a probabilistic model that encodes whether a word is opinion bearing as a latent variable. We build a data structure we call a “syntactic relatedness trie” (Section 3) that serves as the skeleton for a graphical model over the sentiment relevance of words (Section 4). This approach allows us to learn features that predict opinion bearing constructions from grammatical structures. Because of a dearth of resources for this fine-grained task, we also develop new crowdsourcing techniques for labeling word-level, syntactically informed sen1 Alm (2011) recently argued that work on sentiment analysis needs to de-emphasize the goal of building systems that are “high-performing” by traditional measures, because the field risks sacrificing “opportunities that may lead to a more thorough understanding of language uses and users” in relation to subjective phenomena. The work we present in this paper therefore focuses on extracting meaningful features as an investment in future work that directly improves retrieval performance. 668 timent (Section 5). We use inference techniques to uncover grammatical patterns that connect opinionexpressing words"
N12-1085,W06-1651,0,0.0171285,"ut the relationships are often indirect. The variability of language prevents a complete enumeration of all intervening items that make the relationships indirect, but examples include negation and intensifiers, which change opinion, and sentiment-neutral words, which fill syntactic or stylistic needs. In this paper, we cope with the variability of expression by using supervised machine learning to generalize across observations and learn which features best enable us to identify opinionated language. Existing work in this area often uses semantic frames and role labeling (Kim and Hovy, 2006; Choi et al., 2006), but resources typically used in these tasks (e.g. FrameNet) are not exhaustive. More general approaches (Ruppenhofer et al., 2008) describe semantic and discourse contexts of opinion sources and targets cannot recognize them. When techniques do identify targets via syntax, they often only use grammar as a feature in an otherwise syntax-agnostic model. Some work of this nature merely identifies targets without providing the syntactic evidence necessary to find domain-relevant opinionated language (Jakob and Gurevych, 2010), relying on lists of opinion keywords. There is also work (Qiu et al.,"
N12-1085,W08-1301,0,0.0313834,"Missing"
N12-1085,P05-1045,0,0.00567292,"corresponding subset of variables Yi from Z. We can then write the relationship as follows: Qm p(Z) ∝ k=1 fk (Yk ) Our goal is to discover the values for the variables that best explain a dataset. While there are many approaches for inference in statistical models, we turn to MCMC methods (Neal, 1993) to discover the underlying structure of the model. More specifically, we seek a posterior distribution over latent variables 671 child2 child3 Figure 3: Graphical model of SRT factors that partition words in a sentence into flow and inert groups; we estimate this posterior using Gibbs sampling (Finkel et al., 2005). The sampler requires an initial state that respects the invariant. Our initial setting is produced by iterating through all labels in the SRT forest and randomly setting them as either flow or inert with uniform probability. A Gibbs sampler samples new variable assignments from the conditional distribution, treating the variable assignments for all other variables fixed. However, the assignment of a single node is highly coupled with its neighbors, so a block sampler is used to propose changes to groups nodes that respect the flow labeling of the overall assignments. This was implemented by"
N12-1085,W09-1904,0,0.0710268,"Missing"
N12-1085,D10-1101,0,0.168769,"yntactically uninformed methods. 2 Background and existing work We call opinion mining “fine-grained” when it retrieves many different {source, target, opinion} triples per document. This is particularly challenging when there are multiple triples even within a sentence. There is considerable work on identifying the source of an opinion. However, it is much harder to find obvious features that tell us whether “virtualization” is the target of an opinion. The most recent target identification techniques use machine learning to determine the presence of a target from known opinionated language (Jakob and Gurevych, 2010). Even when targets are identified we must decide if an opinion is expressed, since not all target mentions will necessarily be accompanied by opinion expressions. Returning to the first example sentence, we could say that the negative opinion about virtualization is expressed by the words “slew” and “issues”. A system that could automatically make this discovery must draw on grammatical relationships between targets and the opinion bearing words. Parsers reveal these relationships, but the relationships are often indirect. The variability of language prevents a complete enumeration of all int"
N12-1085,W06-0301,0,0.191672,"Lloyd Hession, BT Radianz, and New York. These are potential opinion sources. There are also a number of mentioned concepts that could serve as the topic of an opinion in the sentence, or target. These include all the sources, but also “virtualization”, “network access control”, “network”, and so on. The challenging task is to discriminate between these mentions and choose the ones that are relevant to the user. Furthermore, such a system must also indicate the content of the opinion itself. This means that we are actually searching for all triples {source, target, opinion} in this sentence (Kim and Hovy, 2006) and throughout each document in the corpus. In this case, we want to identify that Lloyd Hession is the source of an opinion, “slew of network issues,” about a target, virtualization. Providing such fine-grained annotations would enrich information extraction, question answering, and corpus exploration applications by letting users see who is saying what with what opinion (Wilson et al., 2005; Stoyanov and Cardie, 2006). We motivate the need for a grammatically-focused approach to fine-grained opinion mining and situate it 667 2012 Conference of the North American Chapter of the Association f"
N12-1085,N10-1120,0,0.0130103,"ntexts of opinion sources and targets cannot recognize them. When techniques do identify targets via syntax, they often only use grammar as a feature in an otherwise syntax-agnostic model. Some work of this nature merely identifies targets without providing the syntactic evidence necessary to find domain-relevant opinionated language (Jakob and Gurevych, 2010), relying on lists of opinion keywords. There is also work (Qiu et al., 2011) that uses predefined heuristics over dependency parses to identify both targets and opinion keywords but does not acquire new syntactic heuristics. Other work (Nakagawa et al., 2010) is similar to ours in that it uses factor graph modeling over a dependency parse formalism, but it assumes that opinionated language is known a priori and focuses on polarity classification, while our work tackles the more fundamental problem of identifying the opinionated language itself. Little work has been done to perform target and opinion-expression extraction jointly, especially in a way that extracts features for downstream processing. This dearth persists despite evidence that such information improves sentiment analysis (Moilanen and Pulman, 2007). An advantage of our proposed appro"
N12-1085,J11-1002,0,0.0209207,"al., 2006), but resources typically used in these tasks (e.g. FrameNet) are not exhaustive. More general approaches (Ruppenhofer et al., 2008) describe semantic and discourse contexts of opinion sources and targets cannot recognize them. When techniques do identify targets via syntax, they often only use grammar as a feature in an otherwise syntax-agnostic model. Some work of this nature merely identifies targets without providing the syntactic evidence necessary to find domain-relevant opinionated language (Jakob and Gurevych, 2010), relying on lists of opinion keywords. There is also work (Qiu et al., 2011) that uses predefined heuristics over dependency parses to identify both targets and opinion keywords but does not acquire new syntactic heuristics. Other work (Nakagawa et al., 2010) is similar to ours in that it uses factor graph modeling over a dependency parse formalism, but it assumes that opinionated language is known a priori and focuses on polarity classification, while our work tackles the more fundamental problem of identifying the opinionated language itself. Little work has been done to perform target and opinion-expression extraction jointly, especially in a way that extracts feat"
N12-1085,ruppenhofer-etal-2008-finding,0,0.0268195,"that make the relationships indirect, but examples include negation and intensifiers, which change opinion, and sentiment-neutral words, which fill syntactic or stylistic needs. In this paper, we cope with the variability of expression by using supervised machine learning to generalize across observations and learn which features best enable us to identify opinionated language. Existing work in this area often uses semantic frames and role labeling (Kim and Hovy, 2006; Choi et al., 2006), but resources typically used in these tasks (e.g. FrameNet) are not exhaustive. More general approaches (Ruppenhofer et al., 2008) describe semantic and discourse contexts of opinion sources and targets cannot recognize them. When techniques do identify targets via syntax, they often only use grammar as a feature in an otherwise syntax-agnostic model. Some work of this nature merely identifies targets without providing the syntactic evidence necessary to find domain-relevant opinionated language (Jakob and Gurevych, 2010), relying on lists of opinion keywords. There is also work (Qiu et al., 2011) that uses predefined heuristics over dependency parses to identify both targets and opinion keywords but does not acquire new"
N12-1085,C10-2126,1,0.824664,"s1 — such approaches improve downstream sentiment tasks (Moilanen and Pulman, 2007). There are multiple types of downstream tasks that potentially require the retrieval of {source, target, opinion} relations on a sentence-by-sentence basis. An increasingly significant application area is in the use of large corpora in social science. This area of research requires the exploration and aggregation of data about the relationships between discourses, organizations, and people. For example, the IT business press data that we use in this work belongs to a larger research program (Tsui et al., 2009; Sayeed et al., 2010) of exploring industry opinion leadership. IT business press text is one type of text in which many entities and opinions can appear intermingled with one another in a small amount of text. Another application for fine-grained sentiment relation retrieval of this type is paraphrasing, where attribution of which opinion belongs to which entities may be important for producing useful and accurate output, since source and target identification errors can change the entire meaning of an output text. Unlike previous approaches that ignore syntax, we use a sentence’s syntactic structure to build a p"
N12-1085,W11-1510,1,0.854362,"sing supervised machine learning techniques. Sentiment corpora with sub-sentential annotations, such as the Multi-Perspective Question-Answering (MPQA) corpus (Wilson and Wiebe, 2005) and the J. D. Power and Associates (JDPA) blog post corpus (Kessler et al., 2010), exist, but most of these annotations are at a phrase level. Within a phrase, however, some words may contribute more than others to the statement of an opinion. We developed our own annotations to discover such distinctions3 . We describe these briefly here; more information about the development of the data source can be found in Sayeed et al. (2011). This consists of 33K articles including news bulletins and opinion columns. Our IT concept target list (59 terms) comes from our application. Thus, we construct a trie for each appearance of any of these possible target terms. We consider this list of target terms to be complete, which allows us to focus on discovering opinion-bearing text associated with these targets. 5.2 Crowdsourced annotation process Our process for obtaining gold standard data involves multiple levels of human annotation including on crowdsourcing platforms Hsueh et al. (2009). There are 75K sentences with IT concept m"
N12-1085,W06-1640,0,0.0147613,"Furthermore, such a system must also indicate the content of the opinion itself. This means that we are actually searching for all triples {source, target, opinion} in this sentence (Kim and Hovy, 2006) and throughout each document in the corpus. In this case, we want to identify that Lloyd Hession is the source of an opinion, “slew of network issues,” about a target, virtualization. Providing such fine-grained annotations would enrich information extraction, question answering, and corpus exploration applications by letting users see who is saying what with what opinion (Wilson et al., 2005; Stoyanov and Cardie, 2006). We motivate the need for a grammatically-focused approach to fine-grained opinion mining and situate it 667 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 667–676, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics within the context of existing work in Section 2. We propose a supervised technique for learning opiniontarget relations from dependency graphs in a way that preserves syntactic coherence and semantic compositionality. In addition to being theoretically sound — a la"
N12-1085,W05-0308,0,0.011306,"the score(label|node) = Y f (parentφ , nodeφ |label)g(nodeφ |label) φ∈Φ  h(nodeφ , child1φ , . . . , childnφ |label) . After assignments for the latent variables are sampled, the weights for the factors (which when combined create individual factors f that define the joint) must be learned. This is accomplished via the sample-rank algorithm (Wick et al., 2009). 5 Data source Our goal is to identify opinion-bearing words and targets using supervised machine learning techniques. Sentiment corpora with sub-sentential annotations, such as the Multi-Perspective Question-Answering (MPQA) corpus (Wilson and Wiebe, 2005) and the J. D. Power and Associates (JDPA) blog post corpus (Kessler et al., 2010), exist, but most of these annotations are at a phrase level. Within a phrase, however, some words may contribute more than others to the statement of an opinion. We developed our own annotations to discover such distinctions3 . We describe these briefly here; more information about the development of the data source can be found in Sayeed et al. (2011). This consists of 33K articles including news bulletins and opinion columns. Our IT concept target list (59 terms) comes from our application. Thus, we construct"
N12-1085,H05-1044,0,0.0123051,"elevant to the user. Furthermore, such a system must also indicate the content of the opinion itself. This means that we are actually searching for all triples {source, target, opinion} in this sentence (Kim and Hovy, 2006) and throughout each document in the corpus. In this case, we want to identify that Lloyd Hession is the source of an opinion, “slew of network issues,” about a target, virtualization. Providing such fine-grained annotations would enrich information extraction, question answering, and corpus exploration applications by letting users see who is saying what with what opinion (Wilson et al., 2005; Stoyanov and Cardie, 2006). We motivate the need for a grammatically-focused approach to fine-grained opinion mining and situate it 667 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 667–676, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics within the context of existing work in Section 2. We propose a supervised technique for learning opiniontarget relations from dependency graphs in a way that preserves syntactic coherence and semantic compositionality. In addition to bein"
N13-3009,P11-1026,1,\N,Missing
N13-3009,P12-1009,1,\N,Missing
N15-1076,D10-1005,1,0.0926749,"Missing"
N15-1076,P14-1099,0,0.0895875,"fication. It could be that better feature preprocessing could improve our performance. Related Work Improving the scalability of statistical learning has taken many forms: creating online approximations of large batch algorithms (Hoffman et al., 2013; Zhai et al., 2014) or improving the efficiency of sampling (Yao et al., 2009; Hu and Boyd-Graber, 2012; Li et al., 2014). 753 In contrast, this paper explores a different vein of research that focuses on using efficient representations of summary statistics to estimate statistical models. While this has seen great success in unsupervised models (Cohen and Collins, 2014), it has increasingly also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not ideal for sentiment tasks. Rubin et al. (2012) introduce Dependency LDA which associates individual word tokens with different labels; their model also outperforms linear SVMs on a very large multi-labeled corpus. Latent variable models that consider grammatical structure ("
N15-1076,P12-2054,1,0.686689,"d that a discriminative classifier did not always perform best on the downstream classification task. Zhu et al. (2009) make a comprehensive comparison between MEDLDA, SLDA, and SVM + LDA, and they show that SVM + LDA performs worse than MEDLDA and SLDA on binary classification. It could be that better feature preprocessing could improve our performance. Related Work Improving the scalability of statistical learning has taken many forms: creating online approximations of large batch algorithms (Hoffman et al., 2013; Zhai et al., 2014) or improving the efficiency of sampling (Yao et al., 2009; Hu and Boyd-Graber, 2012; Li et al., 2014). 753 In contrast, this paper explores a different vein of research that focuses on using efficient representations of summary statistics to estimate statistical models. While this has seen great success in unsupervised models (Cohen and Collins, 2014), it has increasingly also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not idea"
N15-1076,P14-1105,1,0.732244,"supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not ideal for sentiment tasks. Rubin et al. (2012) introduce Dependency LDA which associates individual word tokens with different labels; their model also outperforms linear SVMs on a very large multi-labeled corpus. Latent variable models that consider grammatical structure (Sayeed et al., 2012; Socher et al., 2011; Iyyer et al., 2014) could also be improved through efficient inference (Cohen and Collins, 2014). 7 Discussion Supervised anchor word topic modeling provides a general framework for learning better topic representations by taking advantage of both wordcooccurrence and metadata. Our straightforward extension (Equation 2) places each word in a vector space that not only captures co-occurrence with other terms but also the interaction of the word and its sentiment, in contrast to algorithms that only consider raw words. While our experiments focus on binary classification, the same extension is also applicable to m"
N15-1076,E14-1056,0,0.0353433,"d their Topics One important evaluation for topic models is how easy it is for a human reader to understand the topics. In this section, we evaluate topics produced by 752 each model using topic interpretability (Chang et al., 2009). Topic interpretability measures how human users understand topics presented by a topic modeling algorithm. We use an automated approximation of interpretability that uses a reference corpus as a proxy for which words belong together (Newman et al., 2010). Using half a million documents from Wikipedia, we compute the induced normalized pairwise mutual information (Lau et al., 2014, NPMI) on the top ten words in topics as a proxy for interpretability. Figure 6 shows the NPMI scores for each model. Unsurprisingly, unsupervised models (LDA) produce the best topic quality. In contrast, supervised models must balance metadata (i.e., response variable) prediction against capturing word meaning. Consequently, SLDA does slightly worse with respect to topic interpretability. SUP ANCHOR and ANCHOR produce the same topic quality consistently on all datasets. Since SUP ANCHOR and ANCHOR have nearly identical runtime, SUP ANCHOR is better suited for supervised tasks because it impr"
N15-1076,D14-1138,0,0.0515003,"based on per-token inference of topic assignments; this detail is not relevant to our contribution, and in Section 4.2 we use existing techniques to discover documents’ topics. 748 Because the anchor algorithm scales so well compared to traditional probabilistic inference, we now unify the supervised topic models of Section 2 with the anchor algorithm discussed in Section 1. We do ¯ with an additional so by augmenting the matrix Q dimension for each metadata attribute, such as sentiment. We provide the geometric intuition in Figure 1. Picture the anchor words projected down to two dimensions (Lee and Mimno, 2014): each word is a point, and the anchor words are the vertices of a polygon encompassing every point. Every non-anchor word can be approximated by a convex combination of the anchor words (Figure 1, top). Now add an additional dimension as a column to ¯ Q (Figure 2). This column encodes the metadata specific to a word. For example, we have encoded sentiment metadata in a new dimension (Figure 1, bottom). Neutral sentiment words will stay in the plane inhabited by the other words, positive sentiment words will move up, and negative sentiment words will move down. For simplicity, we only show a s"
N15-1076,N10-1012,0,0.0633914,"ic distributions. This result is far better than the twenty hours required by SLDA to train on TRIPADVISOR . 5 Inspecting Anchors and their Topics One important evaluation for topic models is how easy it is for a human reader to understand the topics. In this section, we evaluate topics produced by 752 each model using topic interpretability (Chang et al., 2009). Topic interpretability measures how human users understand topics presented by a topic modeling algorithm. We use an automated approximation of interpretability that uses a reference corpus as a proxy for which words belong together (Newman et al., 2010). Using half a million documents from Wikipedia, we compute the induced normalized pairwise mutual information (Lau et al., 2014, NPMI) on the top ten words in topics as a proxy for interpretability. Figure 6 shows the NPMI scores for each model. Unsurprisingly, unsupervised models (LDA) produce the best topic quality. In contrast, supervised models must balance metadata (i.e., response variable) prediction against capturing word meaning. Consequently, SLDA does slightly worse with respect to topic interpretability. SUP ANCHOR and ANCHOR produce the same topic quality consistently on all datas"
N15-1076,P14-1034,1,0.846902,"at this as a regression: seeing one word with topic k in document d means that prediction of yd should be adjusted by µk . Given a document’s distribution over topics z¯d , the response yd is normally distributed with mean µ ~ > z¯d .2 Typically, the topics are discovered through a process of probabilistic inference, either variational EM (Wang et al., 2009) or Gibbs sampling (BoydGraber and Resnik, 2010). However, these methods scale poorly to large datasets. Variational inference requires dozens of expensive passes over the entire dataset, and Gibbs sampling requires multiple Markov chains (Nguyen et al., 2014b). 2 We are eliding some details in the interest of a more compact presentation. The topics used by a document, z¯d , are based on per-token inference of topic assignments; this detail is not relevant to our contribution, and in Section 4.2 we use existing techniques to discover documents’ topics. 748 Because the anchor algorithm scales so well compared to traditional probabilistic inference, we now unify the supervised topic models of Section 2 with the anchor algorithm discussed in Section 1. We do ¯ with an additional so by augmenting the matrix Q dimension for each metadata attribute, suc"
N15-1076,D14-1182,1,0.782913,"at this as a regression: seeing one word with topic k in document d means that prediction of yd should be adjusted by µk . Given a document’s distribution over topics z¯d , the response yd is normally distributed with mean µ ~ > z¯d .2 Typically, the topics are discovered through a process of probabilistic inference, either variational EM (Wang et al., 2009) or Gibbs sampling (BoydGraber and Resnik, 2010). However, these methods scale poorly to large datasets. Variational inference requires dozens of expensive passes over the entire dataset, and Gibbs sampling requires multiple Markov chains (Nguyen et al., 2014b). 2 We are eliding some details in the interest of a more compact presentation. The topics used by a document, z¯d , are based on per-token inference of topic assignments; this detail is not relevant to our contribution, and in Section 4.2 we use existing techniques to discover documents’ topics. 748 Because the anchor algorithm scales so well compared to traditional probabilistic inference, we now unify the supervised topic models of Section 2 with the anchor algorithm discussed in Section 1. We do ¯ with an additional so by augmenting the matrix Q dimension for each metadata attribute, suc"
N15-1076,N12-1085,1,0.0571089,", it has increasingly also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not ideal for sentiment tasks. Rubin et al. (2012) introduce Dependency LDA which associates individual word tokens with different labels; their model also outperforms linear SVMs on a very large multi-labeled corpus. Latent variable models that consider grammatical structure (Sayeed et al., 2012; Socher et al., 2011; Iyyer et al., 2014) could also be improved through efficient inference (Cohen and Collins, 2014). 7 Discussion Supervised anchor word topic modeling provides a general framework for learning better topic representations by taking advantage of both wordcooccurrence and metadata. Our straightforward extension (Equation 2) places each word in a vector space that not only captures co-occurrence with other terms but also the interaction of the word and its sentiment, in contrast to algorithms that only consider raw words. While our experiments focus on binary classification,"
N15-1076,D11-1014,0,0.0320106,"also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods rely on non-negative matrix factorization. Bag-of-words representations are not ideal for sentiment tasks. Rubin et al. (2012) introduce Dependency LDA which associates individual word tokens with different labels; their model also outperforms linear SVMs on a very large multi-labeled corpus. Latent variable models that consider grammatical structure (Sayeed et al., 2012; Socher et al., 2011; Iyyer et al., 2014) could also be improved through efficient inference (Cohen and Collins, 2014). 7 Discussion Supervised anchor word topic modeling provides a general framework for learning better topic representations by taking advantage of both wordcooccurrence and metadata. Our straightforward extension (Equation 2) places each word in a vector space that not only captures co-occurrence with other terms but also the interaction of the word and its sentiment, in contrast to algorithms that only consider raw words. While our experiments focus on binary classification, the same extension is"
N15-1076,P08-1036,0,0.06719,"nd topic. We show that these new latent representations predict sentiment as accurately as supervised topic models, and we find these representations more quickly without sacrificing interpretability. Topic models were introduced in an unsupervised setting (Blei et al., 2003), aiding in the discovery of topical structure in text: large corpora can be distilled into human-interpretable themes that facilitate quick understanding. In addition to illuminating document collections for humans, topic models have increasingly been used for automatic downstream applications such as sentiment analysis (Titov and McDonald, 2008; Paul and Girju, 2010; Nguyen et al., 2013). Unfortunately, the structure discovered by unsupervised topic models does not necessarily constitute the best set of features for tasks such as sentiment analysis. Consider a topic model trained on Amazon product reviews. A topic model might discover a topic about vampire romance. However, we often want to go deeper, discovering facets of a topic that reflect topic-specific sentiment, e.g., “buffy” and “spike” for positive sentiment vs. “twilight” and “cullen” for negative sentiment. Techniques for discovering such associations, called supervised t"
N15-1076,Q14-1036,1,0.809051,"he fact that the sampling algorithm grows with the number of tokens. 6 We found that a discriminative classifier did not always perform best on the downstream classification task. Zhu et al. (2009) make a comprehensive comparison between MEDLDA, SLDA, and SVM + LDA, and they show that SVM + LDA performs worse than MEDLDA and SLDA on binary classification. It could be that better feature preprocessing could improve our performance. Related Work Improving the scalability of statistical learning has taken many forms: creating online approximations of large batch algorithms (Hoffman et al., 2013; Zhai et al., 2014) or improving the efficiency of sampling (Yao et al., 2009; Hu and Boyd-Graber, 2012; Li et al., 2014). 753 In contrast, this paper explores a different vein of research that focuses on using efficient representations of summary statistics to estimate statistical models. While this has seen great success in unsupervised models (Cohen and Collins, 2014), it has increasingly also been applied to supervised models. Wang and Zhu (2014) show how to use tensor decomposition to estimate the parameters of SLDA instead of sampling to find maximum likelihood estimates. In contrast, anchor-based methods"
N15-1117,D08-1031,0,0.0289439,"demonstrates good performance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before the last five years (Soon et al., 2001; Bengtson and Roth, 2008; Stoyanov et al., 2010). Recently, better results have been obtained with mention-ranking systems (Luo et al., 2004; Haghighi and Klein, 2010; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, on quiz bowl data, our experiments show that binary classifiers can outperform mention-ranking approaches. 7 Embracing Harder Coreference This paper introduces a new, naturally-occuring coreference dataset that is easy to annotate but difficult for computers to solve. We show that active learning allows us to create a dataset that is rich in different types of coreference. We develop an end"
N15-1117,W12-4503,0,0.0262708,"Missing"
N15-1117,P14-1005,0,0.0433592,"Missing"
N15-1117,D12-1118,1,0.860225,"sentence—while quiz bowl contains 1.16 mentions per sentence (Figure 2). Examples of nested mentions can be seen in in Table 1. Since quiz bowl is a game, it makes the task of solving coreference interesting and challenging for an annotator. In the next section, we use the intrinsic fun of this task to create a new annotated coreference dataset. 4 Intelligent Annotation Here we describe our annotation process. Each document is a single quiz bowl question containing an average of 5.2 sentences. While quiz bowl covers all areas of academic knowledge, we focus on questions about literature from Boyd-Graber et al. (2012), as annotation standards are more straightforward. Our webapp (Figure 3) allows users to annotate a question by highlighting a phrase using their mouse and then pressing a number corresponding to the coreference group to which it belongs. Each group is highlighted with a single color in the interface. The webapp displays a single question at a time, and for some questions, users can compare their answers against gold annotations by the authors. We provide annotators the ability to see if their tags match the gold labels for a few documents as we need to provide a mechanism to help them learn"
N15-1117,de-marneffe-etal-2006-generating,0,0.020866,"Missing"
N15-1117,doddington-etal-2004-automatic,0,0.15856,"Klein, 2013). These results motivate us to develop a very simple end-to-end coreference resolution system consisting of a crfbased mention detector and a pairwise classifier. Our system outperforms the Berkeley system when both have been trained on our new dataset. This result motivates further exploration into complex coreference types absent in newswire data, which we discuss at length in Section 7. 2 Newswire’s Limitations for Coreference Newswire text is widely used as training data for coreference resolution systems. The standard datasets used in the muc (MUC-6, 1995; MUC-7, 1997), ace (Doddington et al., 2004), and CoNLL shared tasks (Pradhan et al., 2011) contain only such text. In this section we argue why this monoculture, despite its many past successes, offer diminishing results for advancing the coreference subfield. First, newswire text has sparse references, and those that it has are mainly identity coreferences and appositives. In the CoNLL 2011 shared task (Pradhan et al., 2007) based on OntoNotes 4.0 (Hovy et al., 2006),2 there are 2.1 mentions per sentence; in the next section we present a dataset with 3.7 mentions per sentence.3 In newswire text, most nominal entities (not including pr"
N15-1117,D13-1203,0,0.166578,"Introduction Coreference resolution—adding annotations to an input text where multiple strings refer to the same entity—is a fundamental problem in computational linguistics. It is challenging because it requires the application of syntactic, semantic, and world knowledge (Ng, 2010). For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character. There are a panoply of sophisticated coreference systems, both data-driven (Fernandes et al., 2012; Durrett and Klein, 2013; Durrett and Klein, 2014; Bj¨orkelund and Kuhn, 2014) and rule-based (Pradhan et al., 2011; Lee et al., 2011). Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems. However, because all of these shared tasks contain strictly newswire data,1 it is unclear how existing systems perform on more diverse data. We argue in Section 2 that to truly solve coreference resolution, the research community needs high-quality datasets that contain many challenging cases such as nested coreferences and coreferences that can only be resolved using external knowledge"
N15-1117,N10-1061,0,0.104663,"t amount of external knowledge, but existing models lack information about worlds (both real and imaginary) and thus cannot confidently mark these coreferences. We discuss coreference work that incorporates external resources such as Wikipedia in the next section; our aim is to provide a dataset that benefits more from this type of information than newswire does. 6 Related Work We describe relevant data-driven coreference research in this section, all of which train and evaluate on only newswire text. Despite efforts to build better rule-based (Luo et al., 2004) or hybrid statistical systems (Haghighi and Klein, 2010), data-driven systems currently dominate the field. The 2012 CoNLL shared task led to improved data-driven systems for coreference resolution that finally outperformed both the Stanford system (Lee et al., 2011) and the ims system (Bj¨orkelund and Farkas, 2012), the latter of which was the best available publiclyavailable English coreference system at the time. The recently-released Berkeley coreference system (Durrett and Klein, 2013) is especially striking: it performs well with only a sparse set of carefully-chosen features. Semantic knowledge sources—especially WordNet (Miller, 1995) and W"
N15-1117,N06-2015,0,0.0614004,"Coreference Newswire text is widely used as training data for coreference resolution systems. The standard datasets used in the muc (MUC-6, 1995; MUC-7, 1997), ace (Doddington et al., 2004), and CoNLL shared tasks (Pradhan et al., 2011) contain only such text. In this section we argue why this monoculture, despite its many past successes, offer diminishing results for advancing the coreference subfield. First, newswire text has sparse references, and those that it has are mainly identity coreferences and appositives. In the CoNLL 2011 shared task (Pradhan et al., 2007) based on OntoNotes 4.0 (Hovy et al., 2006),2 there are 2.1 mentions per sentence; in the next section we present a dataset with 3.7 mentions per sentence.3 In newswire text, most nominal entities (not including pronouns) are singletons; in other words, they do not corefer to anything. OntoNotes 4.0 development data contains 25.4K singleton nominal entities (Durrett and Klein, 2013), compared to only 7.6K entities which corefer to something (anaphora). On the other hand, most pronominals are anaphoric, which makes them easy to resolve as pronouns are single token entities. While 2 As our representative for “newswire” data, the English"
N15-1117,D14-1070,1,0.878909,"Missing"
N15-1117,W11-1916,0,0.0213565,"plore the linguistic phenomena that make quiz bowl coreference so hard and draw insights from our analysis that may help to guide the next generation of coreference systems. 11 We use default options, including hyperparameters tuned on OntoNotes 1113 Evaluating the Berkeley System on Quiz Bowl Data A Simple Mention Detector Detecting mentions is done differently by different coreference systems. The Berkeley system does rule-based mention detection to detect every NP span, every pronoun, and every named entity, which leads to many spurious mentions. This process is based on an earlier work of Kummerfeld et al. (2011), which assumes that every maximal projection of a noun or a pronoun is a mention and uses rules to weed out spurious mentions. Instead of using such a rule-based mention detector, our system detects mentions via sequence labeling, as detecting mentions is essentially a problem of detecting start and stop points in spans of text. We solve this sequence tagging problem using the mallet (McCallum, 2002) implementation of conditional random fields (Lafferty et al., 2001). Since our data contain nested mentions, the sequence labels are bio markers (Ratinov and Roth, 2009). The features we use, whi"
N15-1117,W12-2409,0,0.053764,"a hundred documents and an evaluation set of fifty documents10 we sample 250 more 9 These numbers do not include singletons as OntoNotes does not have them tagged, while ours does. 10 These were documents tagged by the quiz bowl com1112 documents from our set of 7,000 quiz bowl questions. We use the Berkeley coreference system (described in the next section) for the training phase. In Figure 4 we show the effectiveness of our iteration procedure. Unlike the result shown by Miller et al. (2012), we find that for our dataset voting sampling beats random sampling, which supports the findings of Laws et al. (2012). Voting sampling works by dividing the seed set into multiple parts and using each to train a model. Then, from the rest of the dataset we select the document that has the most variance in results after predicting using all of the models. Once that document gets tagged, we add it to the seed set, retrain, and repeat the procedure. This process is impractical with instance-level active learning methods, as there are 116,125 mention pairs (instances) for just 400 documents. Even with document-level sampling, the procedure of training on all documents in the seed set and then testing every docum"
N15-1117,W11-1902,0,0.30178,"tity—is a fundamental problem in computational linguistics. It is challenging because it requires the application of syntactic, semantic, and world knowledge (Ng, 2010). For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character. There are a panoply of sophisticated coreference systems, both data-driven (Fernandes et al., 2012; Durrett and Klein, 2013; Durrett and Klein, 2014; Bj¨orkelund and Kuhn, 2014) and rule-based (Pradhan et al., 2011; Lee et al., 2011). Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems. However, because all of these shared tasks contain strictly newswire data,1 it is unclear how existing systems perform on more diverse data. We argue in Section 2 that to truly solve coreference resolution, the research community needs high-quality datasets that contain many challenging cases such as nested coreferences and coreferences that can only be resolved using external knowledge. In contrast, newswire is deliberately written to contain few coreferences, and those coreferences should be"
N15-1117,P04-1018,0,0.275517,"’s Tale Humans solve cases like these using a vast amount of external knowledge, but existing models lack information about worlds (both real and imaginary) and thus cannot confidently mark these coreferences. We discuss coreference work that incorporates external resources such as Wikipedia in the next section; our aim is to provide a dataset that benefits more from this type of information than newswire does. 6 Related Work We describe relevant data-driven coreference research in this section, all of which train and evaluate on only newswire text. Despite efforts to build better rule-based (Luo et al., 2004) or hybrid statistical systems (Haghighi and Klein, 2010), data-driven systems currently dominate the field. The 2012 CoNLL shared task led to improved data-driven systems for coreference resolution that finally outperformed both the Stanford system (Lee et al., 2011) and the ims system (Bj¨orkelund and Farkas, 2012), the latter of which was the best available publiclyavailable English coreference system at the time. The recently-released Berkeley coreference system (Durrett and Klein, 2013) is especially striking: it performs well with only a sparse set of carefully-chosen features. Semantic"
N15-1117,H05-1004,0,0.0608093,"at our lr model outperforms Berkeley by a wide margin when both are trained on the mentions found by our mention detector (crf). For four metrics, the crf mentions actually improve over training on the gold mentions. Why does the lr model outperform Berkeley 13 The muc (Vilain et al., 1995) score is the minimum number of links between mentions to be inserted or deleted when mapping the output to a gold standard key set. bcub (Bagga and Baldwin, 1998) computes the precision and recall for all mentions separately and then combines them to get the final precision and recall of the output. ceafe (Luo, 2005) is an improvement on bcub and does not use entities multiple times to compute scores. Coreference LR CRF Mentions Gold Mentions 75 50 25 0 75 50 25 0 75 50 25 0 BCUB CEAFE Score Berkeley Mentions QB Final (Berkeley trained on QB) MUC F1 P R F1 P R F1 P R Figure 5: All models are trained and evaluated on quiz bowl data via five fold cross validation on F1 , precision, and recall. Berkeley/crf/Gold refers to the mention detection used, lr refers to our logistic regression model and QB Final refers to the Berkeley model trained on quiz bowl data. Our model outperforms the Berkeley model on every"
N15-1117,M95-1025,0,0.71932,"Missing"
N15-1117,P02-1014,0,0.241297,"Missing"
N15-1117,P10-1142,0,0.0243244,"coreferences and named entities. We also successfully integrate active learning into this annotation pipeline to collect documents maximally useful to coreference models. State-of-the-art coreference systems underperform a simple classifier on our new dataset, motivating non-newswire data for future coreference research. 1 Introduction Coreference resolution—adding annotations to an input text where multiple strings refer to the same entity—is a fundamental problem in computational linguistics. It is challenging because it requires the application of syntactic, semantic, and world knowledge (Ng, 2010). For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character. There are a panoply of sophisticated coreference systems, both data-driven (Fernandes et al., 2012; Durrett and Klein, 2013; Durrett and Klein, 2014; Bj¨orkelund and Kuhn, 2014) and rule-based (Pradhan et al., 2011; Lee et al., 2011). Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems. However, because all of these shared tasks contain"
N15-1117,D14-1162,0,0.080163,"arser (De Marneffe et al., 2006). 1114     m1 and m2 concatenated with their partsof-speech same as above except for an n-word window before and after m1 and m2 how many tokens separate m1 and m2 how many sentences separate m1 and m2 the cosine similarity of word2vec (Mikolov et al., 2013) vector representations of m1 and m2 ; we obtain these vectors by averaging the word embeddings for all words in each mention. We use publicly-available 300dimensional embeddings that have been pretrained on 100B tokens from Google News. same as above except with publicly-available 300-dimensional GloVe (Pennington et al., 2014) vector embeddings trained on 840B tokens from the Common Crawl The first four features are standard in coreference literature and similar to some of the surface features used by the Berkeley system, while the word embedding similarity scores increase our F-measure by about 5 points on the quiz bowl data. Since they have been trained on huge corpora, the word embeddings allow us to infuse world knowledge into our model; for instance, the vector for Russian is more similar to Dostoevsky than Hemingway. Figure 5 shows that our logistic regression model (lr) outperforms the Berkeley system on num"
N15-1117,N06-1025,0,0.0158281,"ld. The 2012 CoNLL shared task led to improved data-driven systems for coreference resolution that finally outperformed both the Stanford system (Lee et al., 2011) and the ims system (Bj¨orkelund and Farkas, 2012), the latter of which was the best available publiclyavailable English coreference system at the time. The recently-released Berkeley coreference system (Durrett and Klein, 2013) is especially striking: it performs well with only a sparse set of carefully-chosen features. Semantic knowledge sources—especially WordNet (Miller, 1995) and Wikipedia—have been used in coreference engines (Ponzetto and Strube, 2006). A system by Ratinov and Roth (2012) demonstrates good performance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before th"
N15-1117,W11-1901,0,0.374848,"s refer to the same entity—is a fundamental problem in computational linguistics. It is challenging because it requires the application of syntactic, semantic, and world knowledge (Ng, 2010). For example, in the sentence Monsieur Poirot assured Hastings that he ought to have faith in him, the strings Monsieur Poirot and him refer to the same person, while Hastings and he refer to a different character. There are a panoply of sophisticated coreference systems, both data-driven (Fernandes et al., 2012; Durrett and Klein, 2013; Durrett and Klein, 2014; Bj¨orkelund and Kuhn, 2014) and rule-based (Pradhan et al., 2011; Lee et al., 2011). Recent CoNLL shared tasks provide the opportunity to make a fair comparison between these systems. However, because all of these shared tasks contain strictly newswire data,1 it is unclear how existing systems perform on more diverse data. We argue in Section 2 that to truly solve coreference resolution, the research community needs high-quality datasets that contain many challenging cases such as nested coreferences and coreferences that can only be resolved using external knowledge. In contrast, newswire is deliberately written to contain few coreferences, and those core"
N15-1117,W09-1119,0,0.0602804,"ed on an earlier work of Kummerfeld et al. (2011), which assumes that every maximal projection of a noun or a pronoun is a mention and uses rules to weed out spurious mentions. Instead of using such a rule-based mention detector, our system detects mentions via sequence labeling, as detecting mentions is essentially a problem of detecting start and stop points in spans of text. We solve this sequence tagging problem using the mallet (McCallum, 2002) implementation of conditional random fields (Lafferty et al., 2001). Since our data contain nested mentions, the sequence labels are bio markers (Ratinov and Roth, 2009). The features we use, which are similar to those used in Kummerfeld et al. (2011), are: muc System Train P R F1 Surface Final OntoN OntoN 47.22 50.79 27.97 30.77 35.13 38.32 Surface Final QB QB 60.44 60.21 31.31 33.41 41.2 42.35  Table 3: The top half of the table represents Berkeley models trained on OntoNotes 4.0 data, while the bottom half shows models trained on quiz bowl data. The muc F1 -score of the Berkeley system on OntoNotes text is 66.4, which when compared to these results prove that quiz bowl coreference is significantly different than OntoNotes coreference.     the token it"
N15-1117,D12-1113,0,0.0130123,"proved data-driven systems for coreference resolution that finally outperformed both the Stanford system (Lee et al., 2011) and the ims system (Bj¨orkelund and Farkas, 2012), the latter of which was the best available publiclyavailable English coreference system at the time. The recently-released Berkeley coreference system (Durrett and Klein, 2013) is especially striking: it performs well with only a sparse set of carefully-chosen features. Semantic knowledge sources—especially WordNet (Miller, 1995) and Wikipedia—have been used in coreference engines (Ponzetto and Strube, 2006). A system by Ratinov and Roth (2012) demonstrates good performance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before the last five years (Soon et al., 2001;"
N15-1117,J01-4004,0,0.143123,"ov and Roth (2012) demonstrates good performance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before the last five years (Soon et al., 2001; Bengtson and Roth, 2008; Stoyanov et al., 2010). Recently, better results have been obtained with mention-ranking systems (Luo et al., 2004; Haghighi and Klein, 2010; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, on quiz bowl data, our experiments show that binary classifiers can outperform mention-ranking approaches. 7 Embracing Harder Coreference This paper introduces a new, naturally-occuring coreference dataset that is easy to annotate but difficult for computers to solve. We show that active learning allows us to create a dataset that is rich in different types of coref"
N15-1117,P10-2029,0,0.018002,"ance by using Wikipedia knowledge to strengthen a multi-pass rule based system. In a more recent work, Durrett and Klein (2014) outperform previous systems by building a joint model that matches mentions to Wikipedia entities while doing named entity resolution and coreference resolution simultaneously. We take a different approach by approximating semantic and world knowledge through our word embedding features. Our simple classifier yields a bi1116 nary decision for each mention pair, a method that had been very popular before the last five years (Soon et al., 2001; Bengtson and Roth, 2008; Stoyanov et al., 2010). Recently, better results have been obtained with mention-ranking systems (Luo et al., 2004; Haghighi and Klein, 2010; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). However, on quiz bowl data, our experiments show that binary classifiers can outperform mention-ranking approaches. 7 Embracing Harder Coreference This paper introduces a new, naturally-occuring coreference dataset that is easy to annotate but difficult for computers to solve. We show that active learning allows us to create a dataset that is rich in different types of coreference. We develop an end-to-end coreference syst"
N15-1117,uryupina-2006-coreference,0,0.0707799,"Missing"
N15-1117,M95-1005,0,0.528265,"ar to Dostoevsky than Hemingway. Figure 5 shows that our logistic regression model (lr) outperforms the Berkeley system on numerous metrics when trained and evaluated on the quiz bowl dataset. We use precision, recall, and F1 , metrics applied to muc, bcub, and ceafe measures used for comparing coreference systems.13 We find that our lr model outperforms Berkeley by a wide margin when both are trained on the mentions found by our mention detector (crf). For four metrics, the crf mentions actually improve over training on the gold mentions. Why does the lr model outperform Berkeley 13 The muc (Vilain et al., 1995) score is the minimum number of links between mentions to be inserted or deleted when mapping the output to a gold standard key set. bcub (Bagga and Baldwin, 1998) computes the precision and recall for all mentions separately and then combines them to get the final precision and recall of the output. ceafe (Luo, 2005) is an improvement on bcub and does not use entities multiple times to compute scores. Coreference LR CRF Mentions Gold Mentions 75 50 25 0 75 50 25 0 75 50 25 0 BCUB CEAFE Score Berkeley Mentions QB Final (Berkeley trained on QB) MUC F1 P R F1 P R F1 P R Figure 5: All models are"
N15-1117,W12-4502,0,\N,Missing
N15-1117,N12-1055,0,\N,Missing
N15-1117,Q14-1037,0,\N,Missing
N15-2017,C04-1200,0,0.0795765,"aining data, which is time consuming and needs human expertise. We believe engaging user in the process of document labeling helps reduce annotation time and address user needs. We present an interactive tool for document labeling. We use topic models to help users in this procedure. Our preliminary results show that users can more effectively and efficiently apply labels to documents using topic model information. 1 2 Introduction Many fields depend on texts labeled by human experts; computational linguistics uses such annotation to determine word senses and sentiment (Kelly and Stone, 1975; Kim and Hovy, 2004); social science uses “coding” to scale up and systemetize content analysis (Budge, 2001; Klingemann et al., 2006). In general text classification is a standard tool for managing large document collections. However, these labeled data have to come from somewhere. The process for creating a broadly applicable, consistent, and generalizable label set and then applying them to the dataset is long and difficult, requiring expensive annotators to examine large swaths of the data. Interactive Document Labeling We propose an alternative framework for assigning labels to documents. We use topic models"
N15-2017,2005.mtsummit-papers.11,0,0.0163829,"nts. Third, we want to allow the user to refine and correct labels further. Our existing interface allows the user to delete a label or edit a label. We believe it is also important for users to merge labels if they think the labels are too specific. In addition, we believe a crucially important step is to generate the label set. Giving the user some information about the range of documents can help them generate a better label set. One other option is to suggest labels to users based on topic models (Lau et al., 2010). Fourth, we will explore other corpora such as European Parliament corpus (Koehn, 2005). To our knowledge, there are no true labels for Europarl corpus and using our interactive tool can help users find the categorized information they need. Finally, for evaluating our method, in addition to using the correct labeling and purity score, we will conduct a user experiment with more users involved. Since the task of labeling congress data set requires some political knowledge, we will choose annotators who have some political science background. Acknowledgments We thank the anonymous reviewers for their insightful comments. We thank Dr. Niklas Elmqvist for his advice for revising th"
N15-2017,C10-2069,0,0.0331615,"formation about the overview of documents and help them further for applying labels to documents. Third, we want to allow the user to refine and correct labels further. Our existing interface allows the user to delete a label or edit a label. We believe it is also important for users to merge labels if they think the labels are too specific. In addition, we believe a crucially important step is to generate the label set. Giving the user some information about the range of documents can help them generate a better label set. One other option is to suggest labels to users based on topic models (Lau et al., 2010). Fourth, we will explore other corpora such as European Parliament corpus (Koehn, 2005). To our knowledge, there are no true labels for Europarl corpus and using our interactive tool can help users find the categorized information they need. Finally, for evaluating our method, in addition to using the correct labeling and purity score, we will conduct a user experiment with more users involved. Since the task of labeling congress data set requires some political knowledge, we will choose annotators who have some political science background. Acknowledgments We thank the anonymous reviewers fo"
N15-2017,D11-1136,0,0.0240565,"ature extraction methods for this task. However, providing an initial set of labeled documents for both text classification and supervised topic models still requires lots of time and human effort. Active learning (Settles, 2010), reduces the amount of required labeled data by having a learner which actively queries the label for specific documents and collects a labeled training set. In a user interactive system, the active learner queries document labels from users (Settles, 2010). In other words, the learner suggests some documents to the user and wants the user to assign a label to those. Settles (2011) discusses that having interactive users in annotation process along with active learning, reduces the amount of annotation time while still achieving acceptable performance. In more detail, they presents an interactive learning framework to get user annotations and produce accurate classifiers in less time. The shortcoming of active learning is that they don’t provide any overview information of corpus, like topic model approaches do. Nevertheless, new methods in both analysis and evaluation are needed. Classification algorithms restrict document labels to a predefined label set. Grimmer and"
N15-2017,P11-1026,1,\N,Missing
N16-1107,S12-1051,0,0.0575137,"of text—neither longer than a few sentences—short text similarity (STS) determines how semantically close they are. STS has a broad range of applications: question answering (Yao et al., 2013; Severyn and Moschitti, 2015), text summarization (Dasgupta et al., 2013; Wang et al., 2013), machine translation evaluation (Chan and Ng, 2008; Liu et al., 2011), and grading of student answers in academic tests (Mohler et al., 2011; Ramachandran et al., 2015). STS is typically viewed as a supervised machine learning problem (B¨ar et al., 2012; Lynum et al., 2014; H¨anig et al., 2015). SemEval contests (Agirre et al., 2012; Agirre et al., 2015) have spurred recent progress in STS and have provided valuable training data for these supervised approaches. However, similarity varies across domains, as does the underlying In Section 3, we describe our Bayesian approach that posits that per-domain parameter vectors share a common Gaussian prior that represents the global parameter vector. Importantly, this idea can be extended with little effort to a nested domain hierarchy (domains within domains), which allows us to create a single, unified STS model that generalizes across domains as well as tasks, capturing the n"
N16-1107,S13-1004,0,0.0588055,"Missing"
N16-1107,S14-2010,0,0.045034,"Missing"
N16-1107,S15-2045,0,0.0383182,"Missing"
N16-1107,S15-2026,0,0.0327962,"Missing"
N16-1107,S12-1059,0,0.0699864,"Missing"
N16-1107,P14-1023,0,0.0964249,"Missing"
N16-1107,P08-1007,0,0.036187,"s headlines, tweets), and (2) applications of STS (e.g., QA vs. answer grading). Our goal is to improve performance in a new domain with few in-domain annotations by using many out-of-domain ones (Section 2). Short Text Similarity: The Need for Domain Adaptation Given two snippets of text—neither longer than a few sentences—short text similarity (STS) determines how semantically close they are. STS has a broad range of applications: question answering (Yao et al., 2013; Severyn and Moschitti, 2015), text summarization (Dasgupta et al., 2013; Wang et al., 2013), machine translation evaluation (Chan and Ng, 2008; Liu et al., 2011), and grading of student answers in academic tests (Mohler et al., 2011; Ramachandran et al., 2015). STS is typically viewed as a supervised machine learning problem (B¨ar et al., 2012; Lynum et al., 2014; H¨anig et al., 2015). SemEval contests (Agirre et al., 2012; Agirre et al., 2015) have spurred recent progress in STS and have provided valuable training data for these supervised approaches. However, similarity varies across domains, as does the underlying In Section 3, we describe our Bayesian approach that posits that per-domain parameter vectors share a common Gaussian"
N16-1107,P13-1100,0,0.0256767,"eanings, we consider adaptation to different (1) sources of text (e.g., news headlines, tweets), and (2) applications of STS (e.g., QA vs. answer grading). Our goal is to improve performance in a new domain with few in-domain annotations by using many out-of-domain ones (Section 2). Short Text Similarity: The Need for Domain Adaptation Given two snippets of text—neither longer than a few sentences—short text similarity (STS) determines how semantically close they are. STS has a broad range of applications: question answering (Yao et al., 2013; Severyn and Moschitti, 2015), text summarization (Dasgupta et al., 2013; Wang et al., 2013), machine translation evaluation (Chan and Ng, 2008; Liu et al., 2011), and grading of student answers in academic tests (Mohler et al., 2011; Ramachandran et al., 2015). STS is typically viewed as a supervised machine learning problem (B¨ar et al., 2012; Lynum et al., 2014; H¨anig et al., 2015). SemEval contests (Agirre et al., 2012; Agirre et al., 2015) have spurred recent progress in STS and have provided valuable training data for these supervised approaches. However, similarity varies across domains, as does the underlying In Section 3, we describe our Bayesian approac"
N16-1107,P07-1033,0,0.225461,"Missing"
N16-1107,N09-1068,0,0.03454,"plied to similarity computations for test pairs in domain d) alongside a common, global domainagnostic weight vector w∗ , which has a zero-mean Gaussian prior and serves as the Gaussian prior mean for each wd . Figure 2 shows the model. Both w∗ and wd have hyperpriors identical to w in Figure 1a.4 Each wd depends not just on its domain-specific observations but also on information derived from the global, shared parameter w∗ . The balance between capturing in-domain information and inductive trans4 Results do not improve with individual domain-specific instances of σS and σw , consistent with Finkel and Manning (2009) for dependency parsing and named entity recognition. 929 fer is regulated by Σw ; larger variance allows wd more freedom to reflect the domain. 3.3 Multitask Learning An advantage of hierarchical DA is that it extends easily to arbitrarily nested domains. Our multitask learning model (Figure 3) models topical domains nested within one of three related tasks: STS, SAS, and ASR (Section 2). This model adds a level to the hierarchy of weight vectors: each domain-level wd is now normally distributed around a task-level weight vector (e.g., wSTS ), which in turn has global Gaussian mean w∗ .5 Like"
N16-1107,N13-1092,0,0.058583,"Missing"
N16-1107,S13-1030,0,0.0444927,"Missing"
N16-1107,S15-2046,0,0.0226676,"Missing"
N16-1107,S13-2046,0,0.052268,"Missing"
N16-1107,S13-1013,0,0.0368877,"Missing"
N16-1107,P14-1110,1,0.897165,"Missing"
N16-1107,D11-1035,0,0.0184729,"), and (2) applications of STS (e.g., QA vs. answer grading). Our goal is to improve performance in a new domain with few in-domain annotations by using many out-of-domain ones (Section 2). Short Text Similarity: The Need for Domain Adaptation Given two snippets of text—neither longer than a few sentences—short text similarity (STS) determines how semantically close they are. STS has a broad range of applications: question answering (Yao et al., 2013; Severyn and Moschitti, 2015), text summarization (Dasgupta et al., 2013; Wang et al., 2013), machine translation evaluation (Chan and Ng, 2008; Liu et al., 2011), and grading of student answers in academic tests (Mohler et al., 2011; Ramachandran et al., 2015). STS is typically viewed as a supervised machine learning problem (B¨ar et al., 2012; Lynum et al., 2014; H¨anig et al., 2015). SemEval contests (Agirre et al., 2012; Agirre et al., 2015) have spurred recent progress in STS and have provided valuable training data for these supervised approaches. However, similarity varies across domains, as does the underlying In Section 3, we describe our Bayesian approach that posits that per-domain parameter vectors share a common Gaussian prior that represe"
N16-1107,S14-2078,0,0.0582767,"Missing"
N16-1107,P11-1076,0,0.156827,"l is to improve performance in a new domain with few in-domain annotations by using many out-of-domain ones (Section 2). Short Text Similarity: The Need for Domain Adaptation Given two snippets of text—neither longer than a few sentences—short text similarity (STS) determines how semantically close they are. STS has a broad range of applications: question answering (Yao et al., 2013; Severyn and Moschitti, 2015), text summarization (Dasgupta et al., 2013; Wang et al., 2013), machine translation evaluation (Chan and Ng, 2008; Liu et al., 2011), and grading of student answers in academic tests (Mohler et al., 2011; Ramachandran et al., 2015). STS is typically viewed as a supervised machine learning problem (B¨ar et al., 2012; Lynum et al., 2014; H¨anig et al., 2015). SemEval contests (Agirre et al., 2012; Agirre et al., 2015) have spurred recent progress in STS and have provided valuable training data for these supervised approaches. However, similarity varies across domains, as does the underlying In Section 3, we describe our Bayesian approach that posits that per-domain parameter vectors share a common Gaussian prior that represents the global parameter vector. Importantly, this idea can be extended"
N16-1107,W15-0612,0,0.0649753,"rmance in a new domain with few in-domain annotations by using many out-of-domain ones (Section 2). Short Text Similarity: The Need for Domain Adaptation Given two snippets of text—neither longer than a few sentences—short text similarity (STS) determines how semantically close they are. STS has a broad range of applications: question answering (Yao et al., 2013; Severyn and Moschitti, 2015), text summarization (Dasgupta et al., 2013; Wang et al., 2013), machine translation evaluation (Chan and Ng, 2008; Liu et al., 2011), and grading of student answers in academic tests (Mohler et al., 2011; Ramachandran et al., 2015). STS is typically viewed as a supervised machine learning problem (B¨ar et al., 2012; Lynum et al., 2014; H¨anig et al., 2015). SemEval contests (Agirre et al., 2012; Agirre et al., 2015) have spurred recent progress in STS and have provided valuable training data for these supervised approaches. However, similarity varies across domains, as does the underlying In Section 3, we describe our Bayesian approach that posits that per-domain parameter vectors share a common Gaussian prior that represents the global parameter vector. Importantly, this idea can be extended with little effort to a nes"
N16-1107,P13-2125,0,0.0462092,"Missing"
N16-1107,Q14-1018,1,0.856375,"d domains. 4 Features Any feature-based STS model can serve as the base model for a hierarchical Bayesian adaptation framework. For our experiments, we adopt the feature set of the ridge regression model in Sultan et al. (2015), the best-performing system at SemEval-2015 (Agirre et al., 2015). (1) (1) Input sentences S (1) = (w1 , ..., wn ) and (2) (2) S (2) = (w1 , ..., wm ) (where each w is a token) 930 produce two similarity features. The first is the proportion of content words in S (1) and S (2) (combined) that have a semantically similar word—identified using a monolingual word aligner (Sultan et al., 2014)— in the other sentence. The overall semantic simi(1) (2) larity of a word pair (wi , wj ) ∈ S (1) × S (2) is a weighted sum of lexical and contextual similarities: a paraphrase database (Ganitkevitch et al., 2013, PPDB ) identifies lexically similar words; contextual similarity is the average lexical similarity in (1) de(1) (2) pendencies of wi in S (1) and wj in S (2) , and (2) (1) content words in [-3, 3] windows around wi in (2) S (1) and wj in S (2) . Lexical similarity scores of pairs in PPDB as well as weights of word and contextual similarities are optimized on an alignment dataset (Br"
N16-1107,S15-2027,1,0.88964,"c (wSTS , wSAS , wASR ) and domain-specific (wd ) weight vectors are jointly learned, enabling transfer across domains and tasks. ??w ?w w ~ ? ?, Σw Σw w ?w ~ ? ?, ??w Σw = ???? ?w f ??? f p ?? S A ?STS ∪ ?SAS ?ASR ?? ~ ? 0, ??? S ~ ? w? f, ??2 p = ??????? w? f A ~ ????????? ? Figure 4: A non-hierarchical joint model for STS, SAS and ASR. A common weight vector w is learned for all tasks and domains. 4 Features Any feature-based STS model can serve as the base model for a hierarchical Bayesian adaptation framework. For our experiments, we adopt the feature set of the ridge regression model in Sultan et al. (2015), the best-performing system at SemEval-2015 (Agirre et al., 2015). (1) (1) Input sentences S (1) = (w1 , ..., wn ) and (2) (2) S (2) = (w1 , ..., wm ) (where each w is a token) 930 produce two similarity features. The first is the proportion of content words in S (1) and S (2) (combined) that have a semantically similar word—identified using a monolingual word aligner (Sultan et al., 2014)— in the other sentence. The overall semantic simi(1) (2) larity of a word pair (wi , wj ) ∈ S (1) × S (2) is a weighted sum of lexical and contextual similarities: a paraphrase database (Ganitkevitch et al."
N16-1107,D07-1003,0,0.0610356,"retaining 1,182 student responses to forty questions spread across five assignments and tests.2 Answer Sentence Ranking (ASR) Given a factoid question and a set of candidate answer sentences, ASR orders candidates so that sentences containing 1 2012: MSRpar-test; 2013: SMT; 2014: Deft-forum, OnWN, Tweet-news; 2015: Answers-forums, Answers-students, Belief, Headlines and Images. 2 Assignments: #1, #2, and #3; Exams: #11 and #12. 928 the answer are ranked higher. Text similarity is the foundation of most prior work: a candidate sentence’s relevance is based on its similarity with the question (Wang et al., 2007; Yao et al., 2013; Severyn and Moschitti, 2015). For our ASR experiments, we use factoid questions developed by Wang et al. (2007) from Text REtrieval Conferences (TREC) 8–13. Candidate QA pairs of a question and a candidate were labeled with whether the candidate answers the question. The questions are of different types (e.g., what, where); we retain 2,247 QA pairs under four question types, each with at least 200 answer candidates in the combined development and test sets.3 Each question type represents a unique topical domain—who questions are about persons and how many questions are abou"
N16-1107,P13-1136,0,0.0143424,"aptation to different (1) sources of text (e.g., news headlines, tweets), and (2) applications of STS (e.g., QA vs. answer grading). Our goal is to improve performance in a new domain with few in-domain annotations by using many out-of-domain ones (Section 2). Short Text Similarity: The Need for Domain Adaptation Given two snippets of text—neither longer than a few sentences—short text similarity (STS) determines how semantically close they are. STS has a broad range of applications: question answering (Yao et al., 2013; Severyn and Moschitti, 2015), text summarization (Dasgupta et al., 2013; Wang et al., 2013), machine translation evaluation (Chan and Ng, 2008; Liu et al., 2011), and grading of student answers in academic tests (Mohler et al., 2011; Ramachandran et al., 2015). STS is typically viewed as a supervised machine learning problem (B¨ar et al., 2012; Lynum et al., 2014; H¨anig et al., 2015). SemEval contests (Agirre et al., 2012; Agirre et al., 2015) have spurred recent progress in STS and have provided valuable training data for these supervised approaches. However, similarity varies across domains, as does the underlying In Section 3, we describe our Bayesian approach that posits that p"
N16-1107,N13-1106,0,0.070823,"Missing"
N16-1111,D11-1033,0,0.0209909,"e and Interpretese are different and characterize how they differ, the contribution of our work is not just examining an interesting, important dialect. Our work provides opportunities to improve conventional simultaneous MT systems by exploiting and modeling human tactics. He et al. (2015) use hand-crafted rules to decrease latency; our data-driven approach could yield additional strategies for improving MT systems. Another strategy—given the scarcity and artifacts of interpretation corpus—is to select references that present delay-minimizing features of Interpretese from translation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015). A Feature Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is align"
N16-1111,D14-1018,0,0.128125,"2, the literal translation “as much as possible” is reduced to “very”, and the adjective “Japanese” is omitted. Before we study these characteristics quantitatively in the next section, we visualize Interpretese and Translationese by a word cloud in Figure 1. The size of each word is proportional to the difference between its frequencies in Interpretese and Translationese (Section 3). The word color indicates whether it is more frequent in Interpretese (black) or Translationese (gold). “the” is over-represented in Interpretese, a phenomenon also occurs in Translationese vs. the original text (Eetemadi and Toutanova, 2014). More conjunction words (e.g., “and”, “so”, “or”, “then”) are used in Interpretese, likely for segmentation, whereas “that” is more frequent in Translationese—a sign of clauses. In addition, the pronoun “I” occurs more often in Translationese while “be” and “is” occur more often in Interpretese, which is consistent with our passivization hypothesis. Source (S), translation (T) and interpretation (I) text 1 2 3 4 5 (S) この日本語の待遇表現の特徴ですが英語から日本語へ直訳しただけでは表現できないと いった特徴があります. (T) (One of) the characteristics of honorific Japanese is that it can not be adequately expressed when using a direct transla"
N16-1111,D14-1140,1,0.915717,"Missing"
N16-1111,D15-1006,1,0.897002,"2013), a process we call segmentation. This is different from what is commonly used in speech translation systems (Fujita et al., 2013; Oda et al., 2014), where translations of segments are directly concatenated. Instead, humans try to incorporate new information into the precedent partial translation, e.g., using “which is” to put it in a clause (Table 1, Example 3), or creating a new sentence joined by conjunctions (Table 1, Example 5). Passivization Passivization is useful for interpreting from head-final languages (e.g., Japanese, German) to head-initial languages (e.g., English, French) (He et al., 2015). Because the verb is needed early in the target sentence but only appears at the end of the source sentence, an obvious strategy is to wait for the final verb. However, if the interpreter uses passive voice, they can start translating immediately and append the verb at the end (Table 1, Examples 4– 5). During passivization, the subject is often omitted when obvious from context. Generalization Camayd-Freixas (2011) and AlKhanji et al. (2000) observe that interpreters focus on delivering the gist of a sentence rather than duplicating the nuanced meaning of each word. More frequent words are ch"
N16-1111,J06-4003,0,0.0197397,"ture Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is aligned. We count Ai and Aj (i &lt; j) as an inverted pair if max(Ai ) &gt; min(Aj ). This means that we have to wait until the jth word to translate the ith word. Segmentation: We use the punkt sentence segmenter (Kiss and Strunk, 2006) from NLTK to detect sentences in a text chunk. Passivization: We compute the number of passive verbs normalized by the total number of verbs. We detect passive voice in English by matching the following regular expression: a be verb (be, are, is, was, were etc.) followed by zero to four non-verb words and one verb in its past participle form. We detect passive voice in Japanese by checking that the dictionary form of a verb has the suffix “れる”. Vocabulary To measure variety, we use Vt /N and Vs /N , where Vt and Vs are counts of distinct tokens and stems, and N is the total number of tokens."
N16-1111,N06-1014,0,0.0278821,"ics. He et al. (2015) use hand-crafted rules to decrease latency; our data-driven approach could yield additional strategies for improving MT systems. Another strategy—given the scarcity and artifacts of interpretation corpus—is to select references that present delay-minimizing features of Interpretese from translation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015). A Feature Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is aligned. We count Ai and Aj (i &lt; j) as an inverted pair if max(Ai ) &gt; min(Aj ). This means that we have to wait until the jth word to translate the ith word. Segmentation: We use the punkt sentence segmenter (Kiss and Strunk, 2006) from NLTK to detect sentences in a text c"
N16-1111,ma-2006-champollion,0,0.0490351,"by wavy lines. :::::::: 3 Classification of Translationese and Interpretese We investigate the difference between Translationese and Interpretese by creating a text classifier to distinguish between them and then examining the most useful features. We train our classifier on a bilingual Japanese-English corpus of spoken monologues and their simultaneous interpretations (Matsubara et al., 2002). To obtain a three-way parallel corpus of aligned translation, interpretation, and their shared source text, we first align the interpreted sentences to source sentences by dynamic programming following Ma (2006).5 This step results in 1684 pairs 5 Sentences are defined by sentence boundaries marked in the corpus, thus coherence is preserved during alignment. 973 of text chunks, with 33 tokens per chunk on average. We then collect human translations from Gengo6 for each source text chunk (one translator per monologue). The original corpus has four interpretors per monologue. We use all available interpretation by copying the translation of a text chunk for its additional interpretation. 3.1 Discriminative Features We use logistic regression as our classifier. Its job is to tell, given a chunk of Engli"
N16-1111,matsubara-etal-2002-bilingual,0,0.0712313,"verbs in translation are underlined. Information appearing in translation but omitted in interpretation are in (parentheses). Summarized expressions and their corresponding expression in translation are :::::::: underlined :: by wavy lines. :::::::: 3 Classification of Translationese and Interpretese We investigate the difference between Translationese and Interpretese by creating a text classifier to distinguish between them and then examining the most useful features. We train our classifier on a bilingual Japanese-English corpus of spoken monologues and their simultaneous interpretations (Matsubara et al., 2002). To obtain a three-way parallel corpus of aligned translation, interpretation, and their shared source text, we first align the interpreted sentences to source sentences by dynamic programming following Ma (2006).5 This step results in 1684 pairs 5 Sentences are defined by sentence boundaries marked in the corpus, thus coherence is preserved during alignment. 973 of text chunks, with 33 tokens per chunk on average. We then collect human translations from Gengo6 for each source text chunk (one translator per monologue). The original corpus has four interpretors per monologue. We use all availa"
N16-1111,P14-2090,0,0.0121586,"he next section. Our hypothesis is that tactics used by interpreters roughly fall in two non-exclusive categories: (i) delay minimization, to enable prompt translation by arranging target words in an order similar to the source; (ii) memory footprint minimization, to avoid overloading working memory by reducing communicated information. Segmentation Interpreters often break source sentences into multiple smaller sentences (CamaydFreixas, 2011; Shimizu et al., 2013), a process we call segmentation. This is different from what is commonly used in speech translation systems (Fujita et al., 2013; Oda et al., 2014), where translations of segments are directly concatenated. Instead, humans try to incorporate new information into the precedent partial translation, e.g., using “which is” to put it in a clause (Table 1, Example 3), or creating a new sentence joined by conjunctions (Table 1, Example 5). Passivization Passivization is useful for interpreting from head-final languages (e.g., Japanese, German) to head-initial languages (e.g., English, French) (He et al., 2015). Because the verb is needed early in the target sentence but only appears at the end of the source sentence, an obvious strategy is to w"
N16-1111,P15-1020,0,0.0654508,"ntional simultaneous MT systems by exploiting and modeling human tactics. He et al. (2015) use hand-crafted rules to decrease latency; our data-driven approach could yield additional strategies for improving MT systems. Another strategy—given the scarcity and artifacts of interpretation corpus—is to select references that present delay-minimizing features of Interpretese from translation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015). A Feature Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is aligned. We count Ai and Aj (i &lt; j) as an inverted pair if max(Ai ) &gt; min(Aj ). This means that we have to wait until the jth word to translate the ith word. Segmentation: We use the punkt sentence segm"
N16-1111,2013.iwslt-papers.3,0,0.692172,"ystems. 2 Distinguishing Translationese and Interpretese In this section, we discuss strategies used in Interpretese, which we detect automatically in the next section. Our hypothesis is that tactics used by interpreters roughly fall in two non-exclusive categories: (i) delay minimization, to enable prompt translation by arranging target words in an order similar to the source; (ii) memory footprint minimization, to avoid overloading working memory by reducing communicated information. Segmentation Interpreters often break source sentences into multiple smaller sentences (CamaydFreixas, 2011; Shimizu et al., 2013), a process we call segmentation. This is different from what is commonly used in speech translation systems (Fujita et al., 2013; Oda et al., 2014), where translations of segments are directly concatenated. Instead, humans try to incorporate new information into the precedent partial translation, e.g., using “which is” to put it in a clause (Table 1, Example 3), or creating a new sentence joined by conjunctions (Table 1, Example 5). Passivization Passivization is useful for interpreting from head-final languages (e.g., Japanese, German) to head-initial languages (e.g., English, French) (He et"
N16-1111,shimizu-etal-2014-collection,0,0.0912836,"do human interpreters use? 1 Unlike consecutive interpretation (speakers stop after a complete thought and wait for the interpreter), simultaneous interpretation has the interpreter to translate while listening to speakers. 2 Language produced in the process of translation is often considered a dialect of the target language: “Translationese” (Baker, 1993). Thus, “Interpretese” refers to interpreted language. Hal Daumé III Computer Science and UMIACS University of Maryland hal@cs.umd.edu Most previous work focuses on qualitative analysis (Bendazzoli and Sandrelli, 2005; Camayd-Freixas, 2011; Shimizu et al., 2014) or pattern counting (Tohyama and Matsubara, 2006; Sridhar et al., 2013). In contrast, we use a more systematic approach based on feature selection and statistical tests. In addition, most work ignores translated text, making it hard to isolate strategies applied by interpreters as opposed to general strategies needed for any translation. Shimizu et al. (2014) are the first to take a comparative approach; however, they directly train MT systems on the interpretation corpus without explicitly examining interpretation tactics. While some techniques can be learned implicitly, the model may also l"
N16-1111,tohyama-matsubara-2006-collection,0,0.248117,"utive interpretation (speakers stop after a complete thought and wait for the interpreter), simultaneous interpretation has the interpreter to translate while listening to speakers. 2 Language produced in the process of translation is often considered a dialect of the target language: “Translationese” (Baker, 1993). Thus, “Interpretese” refers to interpreted language. Hal Daumé III Computer Science and UMIACS University of Maryland hal@cs.umd.edu Most previous work focuses on qualitative analysis (Bendazzoli and Sandrelli, 2005; Camayd-Freixas, 2011; Shimizu et al., 2014) or pattern counting (Tohyama and Matsubara, 2006; Sridhar et al., 2013). In contrast, we use a more systematic approach based on feature selection and statistical tests. In addition, most work ignores translated text, making it hard to isolate strategies applied by interpreters as opposed to general strategies needed for any translation. Shimizu et al. (2014) are the first to take a comparative approach; however, they directly train MT systems on the interpretation corpus without explicitly examining interpretation tactics. While some techniques can be learned implicitly, the model may also learn undesirable behavior such as omission and si"
N16-1111,N03-1033,0,0.0322122,"our data-driven approach could yield additional strategies for improving MT systems. Another strategy—given the scarcity and artifacts of interpretation corpus—is to select references that present delay-minimizing features of Interpretese from translation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015). A Feature Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is aligned. We count Ai and Aj (i &lt; j) as an inverted pair if max(Ai ) &gt; min(Aj ). This means that we have to wait until the jth word to translate the ith word. Segmentation: We use the punkt sentence segmenter (Kiss and Strunk, 2006) from NLTK to detect sentences in a text chunk. Passivization: We compute the number of passive verbs normalize"
N16-1180,P13-1035,0,0.618432,"Missing"
N16-1180,P14-1035,0,0.635859,"and other topic model baselines in two crowdsourced evaluations described in Section 4. In Section 5 we show qualitative results and make connections to existing literary scholarship. 2 A Dataset of Character Interactions Our dataset consists of 1,383 fictional works pulled from Project Gutenberg and other Internet sources. Project Gutenberg has a limited selection (outside of science fiction) of mostly classic literature, so we add more contemporary novels from various genres such as mystery, romance, and fantasy to our dataset. To identify character mentions, we run the BookNLP pipeline of Bamman et al. (2014), which includes character name clustering, quoted speaker identification, and coreference resolution.1 For ev1 While this pipeline works reasonably well, it is unreliable for first-person narratives; we leave the necessary improvements 1535 ery detected character mention, we define a span as beginning 100 tokens before the mention and ending 100 tokens after the mention. We do not use sentence or paragraph boundaries because they vary considerably depending on the author (e.g., William Faulkner routinely wrote single sentences longer than many of Hemingway’s paragraphs). All spans in our data"
N16-1180,P08-1090,0,0.0134561,"ric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as"
N16-1180,P09-1068,0,0.015139,"ased, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et"
N16-1180,P15-1077,0,0.0237195,"relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interp"
N16-1180,E12-1065,0,0.296948,"ships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. su"
N16-1180,P10-1015,0,0.202155,"summaries (Bamman et al., 2013). The NUBBI model of Chang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations sho"
N16-1180,P15-1144,0,0.0178133,"; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgments We thank Jonathan Chang and Amit Gruber for providing baseli"
N16-1180,D15-1208,0,0.0777085,"ts even when we reduce the number of topics. 1541 reasonably. Finally, returning to the “extra” meaning of meals discussed in Section 1, food occurs slightly more frequently in positive relationships. 6 Related Work There are two major areas upon which our work builds: computational literary analysis and deep neural networks for natural language processing. Most previous work in computational literary analysis has focused either on characters or events. In the former category, graphical models and classifiers have been proposed for learning character personas from novels (Bamman et al., 2014; Flekova and Gurevych, 2015) and film summaries (Bamman et al., 2013). The NUBBI model of Chang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from"
N16-1180,N15-1004,0,0.0148287,"nal datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgments We thank Jonathan Chang and Amit Grub"
N16-1180,P14-1105,1,0.80738,"ky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Concl"
N16-1180,P15-1162,1,0.0293637,"Missing"
N16-1180,N15-1185,0,0.0223603,"ang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of charact"
N16-1180,P14-1132,0,0.0147958,"ations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgmen"
N16-1180,P15-1107,0,0.00991822,"characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et"
N16-1180,D14-1162,0,0.127245,"Missing"
N16-1180,D15-1088,0,0.0242198,"of the RMN is a set of relationship descriptors (topics) and—for each relationship in the dataset—a trajectory, or a sequence of probability distributions over these descriptors (document-topic assignments). However, the RMN uses recent advances in deep learning to achieve better control over descriptor coherence and trajectory smoothness (Section 4). 3.1 Formalizing the Problem Assume we have two characters c1 and c2 in book b. We define Sc1 ,c2 as a sequence of token spans where each span st ∈ Sc1 ,c2 is itself a set of tokens to character name clustering, which are further expanded upon in Vala et al. (2015), for future work. 2 Code and span data available at http://github.com/ miyyer/rmn. Each input to the RMN is a tuple that contains identifiers for a book and two characters, as well as the spans corresponding to their relationship: (b, c1 , c2 , Sc1 ,c2 ). Given one such input, our objective is to reconstruct Sc1 ,c2 using a linear combination of relationship descriptors from R as shown in Figure 2; we now describe this process formally. r t = RT dt : reconstruction of input span R: descriptor matrix dt dt = ↵ · softmax(Wd · [ht ; dt 1: previous state (1 ↵) · dt 1 ])+ 1: distribution over desc"
N16-1180,Q14-1017,0,\N,Missing
N18-1099,P14-1110,1,0.877669,"ncreases, more low-probability and irrelevant words appear the topic, which lowers CNPMI scores. However, MTA stays stable or increases with increasing cardinality. Thus, MTA fails to fulfill a critical property of topic model evaluation. Finally, MTA requires a comprehensive multilingual dictionary, which may be unavailable for lowresource languages. Additionally, most languages often only have one dictionary, which makes it problematic to use the same resource (a language’s single multilingual dictionary) for training and evaluating models that use a dictionary to build multilingual topics (Hu et al., 2014). Given these concerns, we continue the paper’s focus on CNPMI as a data-driven alternative to MTA. However, for many applications MTA may suffice as a simple, adequate evaluation metric. 6 Model-Level Evaluation While the previous section looked at individual topics, we also care about how well CNPMI characterizes the quality of models through an average of a model’s constituent topics. 6.1 Training Knowledge Adding more knowledge to multilingual topic models improves topics (Hu et al., 2014), so an effective evaluation should reflect this improvement as knowlege is added to the model. For po"
N18-1099,C12-1089,0,0.154564,"Missing"
N18-1099,2005.mtsummit-papers.11,0,0.0393457,"Missing"
N18-1099,N16-1132,0,0.114728,"cific “views” of a document share the document-topic distribution θd . The generative story for the document-links model is: 1 2 3 4 5 6 7 8 for each topic k and each language ` do Draw a distribution over words φ`k ∼ Dirichlet(β);   Alternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vuli´c et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model. Monolingual Evaluation Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014) is normalized pointwise mutual information (Bouma, 2009, NPMI). NPMI compares the joint probability of words appearing together Pr(wi , wj ) to their probability assuming independence Pr(wi ) Pr(wj ), normalized by the joint probability: Pr(w ,w ) NPMI (wi , wj )"
N18-1099,N16-1053,0,0.107928,"el/comparable language-specific documents, 1090 Proceedings of NAACL-HLT 2018, pages 1090–1100 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics d(`) , and the language-specific “views” of a document share the document-topic distribution θd . The generative story for the document-links model is: 1 2 3 4 5 6 7 8 for each topic k and each language ` do Draw a distribution over words φ`k ∼ Dirichlet(β);   Alternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vuli´c et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model. Monolingual Evaluation Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014) is normalized pointwise mutual information (Boum"
N18-1099,N16-1057,0,0.0756798,"pair do not appear in the reference corpus, the cooccurrence score is zero. Similar to monolingual settings, CNPMI for a bilingual topic k is the average of the NPMI scores of all C 2 bilingual word pairs, CNPMI (`1 , `2 , k) = PC i,j NPMI (wi,`1 , wj,`2 ) . C2 (4) It is straightforward to generalize CNPMI from a language pair to multiple languages by averaging CNPMI (`i , `j , k) over all language pairs (`i , `j ). 3 Adapting to Low-Resource Languages CNPMI needs a reference corpus for co-occurrence statistics. Wikipedia, which has good coverage of topics and vocabularies is a common choice (Lau and Baldwin, 2016). Unfortunately, Wikipedia is often unavailable or not large enough for lowresource languages. It only covers 282 languages,1 and only 249 languages have more than 1,000 pages: many of pages are short or unlinked to 1 a high-resource language. Since CNPMI requires comparable documents, the usable reference corpus is defined by paired documents. Another option for a parallel reference corpus is the Bible (Resnik et al., 1999), which is available in most world languages;2 however, it is small and archaic. It is good at evaluating topics such as family and religion, but not “modern” topics like b"
N18-1099,Q16-1004,0,0.203877,"Missing"
N18-1099,E14-1056,0,0.698881,"ang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vuli´c et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model. Monolingual Evaluation Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014) is normalized pointwise mutual information (Bouma, 2009, NPMI). NPMI compares the joint probability of words appearing together Pr(wi , wj ) to their probability assuming independence Pr(wi ) Pr(wj ), normalized by the joint probability: Pr(w ,w ) NPMI (wi , wj ) = j log Pr(wi ) iPr(w j) log Pr(wi , wj ) . (1) The word probabilities are calculated from a reference corpus, R, typically a large corpus such as Wikipedia that can provide meaningful cooccurrence patterns that are independent of the target dataset. The quality of topic k is the average NPMI of all word pairs (wi , wj ) in the topic"
N18-1099,D16-1229,0,0.0191501,"ern words are. The word era features are the earliest usage year 3 for each word in a topic. We use both the mean and standard deviation as features. Meaning Drift (DRIFT). The meaning of a word can expand and drift over time. For example, in the Bible, “web” appears in Isaiah 59:5: They hatch cockatrice’ eggs, and weave the spider’s web. 3 https://oxforddictionaries.com/ The word “web” could be evaluated correctly in an animal topic. For modern topics, however, Bible fails to capture modern meanings of “web”, as in Topic 5 (Figure 1). To address this meaning drift, we use a method similar to Hamilton et al. (2016). For each English word, we calculate the context vector from Bible and from Wikipedia with a window size of five and calculate the cosine similarity between them as word similarity. Similar context vectors mean that the usage in the Bible is consistent with Wikipedia. We calculate word similarities for all the English topic words in a topic and use the average and standard deviation as features. 3.2 Example In Figure 3, Topic 1 is coherent while Topic 8 is not. From left to right, we incrementally add new feature sets, and show how the estimated topic coherence scores (dashed lines) approach"
N18-1099,D09-1092,0,0.158757,"hes to multilingual settings. 2.1 Multilingual Topic Modeling Probabilistic topic models associate each document in a corpus with a distribution over latent topics, while each topic is associated with a distribution over words in the vocabulary. The most widely used topic model, latent Dirichlet allocation (Blei et al., 2003, LDA), can be extended to connect languages. These extensions require additional knowledge to link languages together. One common encoding of multilingual knowledge is document links (indicators that documents are parallel or comparable), used in polylingual topic models (Mimno et al., 2009; Ni et al., 2009). In these models, each document d indexes a tuple of parallel/comparable language-specific documents, 1090 Proceedings of NAACL-HLT 2018, pages 1090–1100 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics d(`) , and the language-specific “views” of a document share the document-topic distribution θd . The generative story for the document-links model is: 1 2 3 4 5 6 7 8 for each topic k and each language ` do Draw a distribution over words φ`k ∼ Dirichlet(β);   Alternatively, word translations (Jagarlamudi and Daumé III, 2010), conce"
N18-1099,D11-1024,0,0.0584405,"and not adaptable to low-resource languages. The approaches here could help improve the evaluation of all multilingual representation learning algorithms (Schnabel et al., 2015). ● ● ● ● 0.015 ● ● 0.010 ● ● ● ● ● ● ● ● Bible ● ● 8 ● ● ● ● ● ● 0.4 0.6 0.8 ● ● 0.005 ● 0.2 1.0 Proportion of Reference Corpus Figure 10: CNPMI is stable once the number of reference documents is large enough (around five thousand documents). 7 Related Work Topic Coherence Many coherence metrics based on co-occurrence statistics have been proposed besides NPMI. Similar metrics—such as asymmetrical word pair metrics (Mimno et al., 2011) and combinations of existing measurements (Lau et al., 2014; Röder et al., 2015)— correlate well with human judgments. NPMI has been the current gold standard for evaluation and improvements of monolingual topic models (Pecina, 2010; Newman et al., 2011). External Tasks Another approach is to use a model for predictive tasks: the better the results are on external tasks, the better a topic model is assumed to be. A common task is held-out likelihood (Wallach et al., 2009b; Jagarlamudi and Daumé III, 2010; Fukumasu et al., 2012), but as Chang et al. (2009) show, this does not always reflect hu"
N18-1099,D15-1036,0,0.0242104,"● 0.10 ● CNPMI 0.05 ● ● ● Wikipedia ● Representation Learning Topic models are one example of a broad class of techniques of learning representations of documents (Bengio et al., 2013). Other approaches learn respresentations at the word (Klementiev et al., 2012; Vyas and Carpuat, 2016), paragraph (Mogadala and Rettinger, 2016), or corpus level (Søgaard et al., 2015). However, neural representation learning approaches are often data hungry and not adaptable to low-resource languages. The approaches here could help improve the evaluation of all multilingual representation learning algorithms (Schnabel et al., 2015). ● ● ● ● 0.015 ● ● 0.010 ● ● ● ● ● ● ● ● Bible ● ● 8 ● ● ● ● ● ● 0.4 0.6 0.8 ● ● 0.005 ● 0.2 1.0 Proportion of Reference Corpus Figure 10: CNPMI is stable once the number of reference documents is large enough (around five thousand documents). 7 Related Work Topic Coherence Many coherence metrics based on co-occurrence statistics have been proposed besides NPMI. Similar metrics—such as asymmetrical word pair metrics (Mimno et al., 2011) and combinations of existing measurements (Lau et al., 2014; Röder et al., 2015)— correlate well with human judgments. NPMI has been the current gold standard"
N18-1099,P15-1165,0,0.0941828,"Missing"
N18-1099,tiedemann-2012-parallel,0,0.060771,"Missing"
N18-1099,N16-1083,0,0.0548686,"Missing"
N18-1099,N10-1012,0,0.618154,"tion Topic models provide a high-level view of the main themes of a document collection (Boyd-Graber et al., 2017). Document collections, however, are often not in a single language, driving the development of multilingual topic models. These models discover topics that are consistent across languages, providing useful tools for multilingual text analysis (Vuli´c et al., 2015), such as detecting cultural differences (Gutiérrez et al., 2016) and bilingual dictionary extraction (Liu et al., 2015). Monolingual topic models can be evaluated through likelihood (Wallach et al., 2009b) or coherence (Newman et al., 2010), but topic model evaluation is not well understood in multilingual settings. Our contributions are two-fold. We introduce an improved intrinsic evaluation metric for multilingual topic models, called Crosslingual Normalized Pointwise Mutual Information (CNPMI, Section 2). We explore the behaviors of CNPMI at both the model and topic levels with six language pairs and varying model specifications. This metric Evaluating Multilingual Coherence A multilingual topic contains one topic for each language. For a multilingual topic to be meaningful to humans (Figure 1), the meanings should be consist"
N18-1099,D09-1026,0,0.0124036,"els are often used as a feature extraction technique for downstream machine learning Bible AM TL 0.607 0.796 RO SV 0.631 0.797 ZH TR 0.907 0.911 Train RO + SV ZH + TR RO + SV + ZH + TR 0.677 0.875 AM + TL 0.912 0.959 RO + SV 0.918 0.862 0.707 0.924 ZH + TR 0.919 0.848 AM + TL 0.951 0.898 0.694 0.918 AM + TL + ZH + TR 0.931 0.878 RO + SV + AM + TL 0.939 0.887 Table 4: At the model level, the estimator improves correlations between CNPMI and downstream classification for all languages except for Turkish. applications, and topic model evaluations should reflect whether these features are useful (Ramage et al., 2009). For each model, we apply a document classifier trained on the model parameters to test whether CNPMI is consistent with classification accuracy. Specifically, we want our classifier to transfer information from training on one language to testing on another (Smet et al., 2011; Heyman et al., 2016). We train a classifier on one language’s documents, where each document’s feature vector is the document-topic distribution θd . We apply this to TED Talks, where each document is labeled with multiple categories. We choose the most frequent seven categories across the corpus as labels,5 and only h"
N18-1099,N16-1142,0,0.0434465,"Missing"
N18-1099,D17-1203,1,0.855381,"s, each document d indexes a tuple of parallel/comparable language-specific documents, 1090 Proceedings of NAACL-HLT 2018, pages 1090–1100 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics d(`) , and the language-specific “views” of a document share the document-topic distribution θd . The generative story for the document-links model is: 1 2 3 4 5 6 7 8 for each topic k and each language ` do Draw a distribution over words φ`k ∼ Dirichlet(β);   Alternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges. Since the polylingual topic model is the most common approach for building multilingual topic models (Vuli´c et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model. Monolingual Evaluation Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010). The most successful (Lau et al., 2014)"
N18-2120,N16-1147,0,0.0917693,"Missing"
P11-1026,D10-1005,1,0.311877,"ords, ignoring potential relations between words. This problem with vanilla LDA can be solved by encoding constraints, which will “guide” different words into the same topic. Constraints can be added to vanilla LDA by replacing the multinomial distribution over words for each topic with a collection of tree-structured multinomial distributions drawn from a prior as depicted in Figure 1. By encoding word distributions as a tree, we can preserve conjugacy and relatively simple inference while encouraging correlations between related concepts (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Boyd-Graber and Resnik, 2010). Each topic has a top-level distribution over words and constraints, and each constraint in each topic has second-level distribution over the words in the constraint. Critically, the perconstraint distribution over words is engineered to be non-sparse and close to uniform. The top level distribution encodes which constraints (and unconstrained words) to include; the lower-level distribution forces the probabilities to be correlated for each of the constraints. In LDA, a document’s token is produced in the generative process by choosing a topic z and sampling a word from the multinomial distri"
P11-1026,D07-1109,1,0.585613,"used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions. 1 There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). Introduction Probabilistic topic models, as exemplified by probabilistic latent semantic indexing (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003) are unsupervised statistical techniques to discover the thematic topics that permeate a large corpus of text documents. Topic models have had considerable application beyond natural language processing in computer vision (Rob et al., 2005), biology (Shringarpure and Xing, 2008), and psychology (Landauer et al., 2006) in addition to their canonical application to text. For"
P11-1026,W10-0720,0,0.0150756,"on of the resulting topics, we found that 30 additional iterations of inference was acceptable; this is used in later experiments. 7 Getting Humans in the Loop To move beyond using simulated users adding the same words regardless of what topics were discovered by the model, we needed to expose the model to human users. We solicited approximately 200 judgments from Mechanical Turk, a popular crowdsourcing platform that has been used to gather linguistic annotations (Snow et al., 2008), measure topic quality (Chang et al., 2009), and supplement traditional inference techniques for topic models (Chang, 2010). After presenting our interface for collecting judgments, we examine the results from these ITM sessions both quantitatively and qualitatively. 7.1 Interface for soliciting refinements Figure 5 shows the interface used in the Mechanical Turk tests. The left side of the screen shows the current topics in a scrollable list, with the top 30 words displayed for each topic. Users create constraints by clicking on words from the topic word lists. The word lists use a color-coding scheme to help the users keep track of which words they are currently grouping into constraints. The right side of the s"
P11-1026,P09-2074,0,0.0495581,"Missing"
P11-1026,W06-2915,0,0.00763751,"by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions. 1 There has been a thriving cottage industry adding more and more information to topic models to correct these shortcomings; either by modeling perspective (Paul and Girju, 2010; Lin et al., 2006), syntax (Wallach, 2006; Gruber et al., 2007), or authorship (Rosen-Zvi et al., 2004; Dietz et al., 2007). Similarly, there has been an effort to inject human knowledge into topic models (Boyd-Graber et al., 2007; Andrzejewski et al., 2009; Petterson et al., 2010). Introduction Probabilistic topic models, as exemplified by probabilistic latent semantic indexing (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003) are unsupervised statistical techniques to discover the thematic topics that permeate a large corpus of text documents. Topic models have had considerable applica"
P11-1026,N10-1012,0,0.0953205,"ons between documents and topics. In LDA both of these outputs are multinomial distributions; typically they are presented to users in summary form by listing the elements with highest probability. For an example of topics discovered from a 20-topic model of New York Times editorials, see Table 1. When presented with poor topics learned from data, users can offer a number of complaints:2 these documents should have similar topics but don’t (Daum´e III, 2009); this topic should have syntactic coherence (Gruber et al., 2007; Boyd-Graber and Blei, 2008); this topic doesn’t make any sense at all (Newman et al., 2010); this topic shouldn’t be associated with this document but is (Ramage et al., 2009); these words shouldn’t be the in same topic but are (Andrzejewski et al., 2009); or these words should be in the same topic but aren’t (Andrzejewski et al., 2009). Many of these complaints can be addressed by using “must-link” constraints on topics, retaining Andrzejewski et al’s (2009) terminology borrowed from the database literature. A “must-link” constraint is a group of words whose probability must be correlated in the topic. For example, Figure 1 shows an example constraint: {plant, factory}. After this"
P11-1026,D09-1026,0,0.634791,"ibutions; typically they are presented to users in summary form by listing the elements with highest probability. For an example of topics discovered from a 20-topic model of New York Times editorials, see Table 1. When presented with poor topics learned from data, users can offer a number of complaints:2 these documents should have similar topics but don’t (Daum´e III, 2009); this topic should have syntactic coherence (Gruber et al., 2007; Boyd-Graber and Blei, 2008); this topic doesn’t make any sense at all (Newman et al., 2010); this topic shouldn’t be associated with this document but is (Ramage et al., 2009); these words shouldn’t be the in same topic but are (Andrzejewski et al., 2009); or these words should be in the same topic but aren’t (Andrzejewski et al., 2009). Many of these complaints can be addressed by using “must-link” constraints on topics, retaining Andrzejewski et al’s (2009) terminology borrowed from the database literature. A “must-link” constraint is a group of words whose probability must be correlated in the topic. For example, Figure 1 shows an example constraint: {plant, factory}. After this constraint is added, the probabilities of “plant” and “factory” in each topic are li"
P11-1026,D08-1027,0,0.0372575,"Missing"
P11-1026,N12-1085,1,\N,Missing
P11-1026,S12-1025,0,\N,Missing
P11-1026,P11-1154,0,\N,Missing
P11-1026,N09-1031,0,\N,Missing
P11-1026,W11-1513,0,\N,Missing
P11-1026,D12-1087,0,\N,Missing
P11-1026,D11-1024,0,\N,Missing
P11-1026,D10-1124,0,\N,Missing
P11-1026,P06-4018,0,\N,Missing
P11-1026,P10-1117,0,\N,Missing
P11-1026,N07-1018,0,\N,Missing
P11-1026,D08-1038,0,\N,Missing
P11-1026,P12-2054,1,\N,Missing
P11-1026,W02-0109,0,\N,Missing
P12-1009,D08-1035,0,0.207999,"Missing"
P12-1009,D10-1124,0,0.0783457,"Missing"
P12-1009,P03-1071,0,0.207722,"Missing"
P12-1009,W06-2914,0,0.03896,"Missing"
P12-1009,J97-1003,0,0.819786,"Missing"
P12-1009,E06-1035,0,0.0389378,"Missing"
P12-1009,P10-1117,0,0.0601251,"Missing"
P12-1009,J91-1002,0,0.112383,"Missing"
P12-1009,H05-1122,0,0.0466397,"Missing"
P12-1009,J02-1002,0,0.0809409,"Missing"
P12-1009,P06-1003,0,0.138311,"Missing"
P12-1009,W06-1639,0,0.264531,"Missing"
P12-2023,D11-1033,0,0.0420528,"hrase entirely. In a food related context, the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) e"
P12-2023,D10-1005,1,0.141903,"conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations, as evidenced by significant performance gains. This method presents several advantages over existing approaches. We can construct a topic model once on the training data, and use it infer topics on any test set to adapt the translation model. We can also incorporate large quantities of additional data (whether parallel or not) in the source language to infer better topics without relying on collection or genre annotations. Multilingual topic models (Boyd-Graber and Resnik, 2010) would provide a technique to use data from multiple languages to ensure consistent topics. Acknowledgments Vladimir Eidelman is supported by a National Defense Science and Engineering Graduate Fellowship. This work was also supported in part by NSF grant #1018625, ARL Cooperative Agreement W911NF-09-2-0072, and by the BOLT and GALE programs of the Defense Advanced Research Projects Agency, Contracts HR0011-12-C-0015 and HR0011-06-2-001, respectively. Any opinions, findings, conclusions, or recommendations expressed are the authors’ and do not necessarily reflect those of the sponsors. Referen"
P12-2023,P96-1041,0,0.243101,"and non-HK Hansards portions of the NIST training corpora with LTM only. Table 1 summarizes the data statistics. For both settings, the data were lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). The Chinese data were segmented using the Stanford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4 By having as many topics as genres/collections and setting p(zn |di ) to 1 for every sentence in the collection and 0 to everything else. Corpus Sentences FBIS NIST 269K 1.6M Tokens En Zh 10.3M 7.9M 44.4M 40.4M Model Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a C"
P12-2023,P11-2080,0,0.531052,"(Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on provenance (i.e., the origin of the text) by removing the separate mapping step, directly optimizing the weight of the genre and collection features by computing a separate word translation table for each feature, estimated from only those sentences that comprise that genre or collection. The common thread throughout prior work is the concept of a domain. A domain is typically a hard constraint that is externally imposed and hand labeled, such as genre or corpus collection. For example, a sentence either comes from newswire, or weblog, but not both."
P12-2023,P10-4002,1,0.181997,"Missing"
P12-2023,W12-3160,1,0.806631,"the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4 By having as many topics as genres/collections and setting p(zn |di ) to 1 for every sentence in the collection and 0 to everything else. Corpus Sentences FBIS NIST 269K 1.6M Tokens En Zh 10.3M 7.9M 44.4M 40.4M Model Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a Chinese stoplist and setting the per-document Dirichlet parameter α = 0.01. This setting of was chosen to encourage sparse topic assignments, which make induced subdomains consistent within a document. Results Results for both settings are shown in Table 2. GTM models the latent topics at the document level, while LTM models each sentence as a separate document. To evaluate the effect topic granularity would have on translation, we varied the number of latent topics in each model to be 5, 10,"
P12-2023,D10-1044,0,0.354617,"ood related context, the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notio"
P12-2023,N03-1017,0,0.141356,"robabilities directly as features in the translation model, and interpolating them log-linearly with our other features, thus allowing us to discriminatively optimize their weights on an arbitrary objective function. Incorporating these features into our hierarchical phrase-based translation system significantly improved translation performance, by up to 1 BLEU and 3 TER over a strong Chinese to English baseline. 2 Model Description Lexical Weighting Lexical weighting features estimate the quality of a phrase pair by combining the lexical translation probabilities of the words in the phrase2 (Koehn et al., 2003). Lexical conditional probabilities p(e|f ) are obtained with maximum likelihood estimates from relative frequencies 2 For hierarchical systems, these correspond to translation rules. 116 P c(f, e)/ e c(f, e) . Phrase pair probabilities p(e|f ) are computed from these as described in Koehn et al. (2003). Chiang et al. (2011) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features, fs (e|f ), one for each domain s, which compute a new word translation table ps (e|f ) estimated from P only those sentences which belo"
P12-2023,D09-1074,0,0.482162,"tion, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on provenance (i.e., the origin of the text) by removing the separate mapping step, directly optimizing the weight of the genre and collection"
P12-2023,J03-1002,0,0.00397406,"ights toward having topic information be useful, not toward a specific distribution. 3 Experiments Setup To evaluate our approach, we performed experiments on Chinese to English MT in two settings. First, we use the FBIS corpus as our training bitext. Since FBIS has document delineations, we compare local topic modeling (LTM) with modeling at the document level (GTM). The second setting uses the non-UN and non-HK Hansards portions of the NIST training corpora with LTM only. Table 1 summarizes the data statistics. For both settings, the data were lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). The Chinese data were segmented using the Stanford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4 By having as many topics as genres/collections and setting p(zn |di ) to 1 for every sentence in the collection and 0 to everything else. Corpus Sentences FBIS"
P12-2023,P02-1040,0,0.107979,"nford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4 By having as many topics as genres/collections and setting p(zn |di ) to 1 for every sentence in the collection and 0 to everything else. Corpus Sentences FBIS NIST 269K 1.6M Tokens En Zh 10.3M 7.9M 44.4M 40.4M Model Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a Chinese stoplist and setting the per-document Dirichlet parameter α = 0.01. This setting of was chosen to encourage sparse topic assignments, which make induced subdomains consistent within a document. Results Results for both settings are shown in Table 2. GTM models the latent topics at the document level, while LTM models each sentence as a separate document. To evaluate"
P12-2023,D08-1090,0,0.0126658,"the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on p"
P12-2023,P06-2124,0,0.396164,"nslation model. Unsupervised modeling of the training data produces naturally occurring subcorpora, generalizing beyond corpus and genre. Depending on the model used to select subcorpora, we can bias our translation toward any arbitrary distinction. This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation. In this work, we consider the underlying latent topics of the documents (Blei et al., 2003). Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment. In our case, by building a topic distribution for the source side of the training data, we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership. This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics. We accomplish this by introducing topic dependent lexical probabilities directly as features in the translation model, and interpolating them log-linearly with our other features, thus allowing us to discriminatively optimize"
P12-2054,W99-0901,0,0.116075,"interactive settings (Hu et al., 2011), efficient inference would improve perceived latency. S PARSE LDA (Yao et al., 2009) is an efficient Gibbs sampling algorithm for LDA based on a refactorization of the conditional topic distribution (reviewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we provide a factorization for tree-based models within a broadly applicable inference framework that empirically improves the efficiency of inference (Section 5). 2 Topic Modeling with Tree-Based Priors Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used treestructured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al., 2007). In both cases, the tree came from WordNet (Miller, 1990), but the tree could also come from domain experts (Andrzejewski et al., 2009). Organizing words in this way induces correlations that are mathematically impossible to represent with a symmetric Dirichlet prior. To see how correlations can occur, consider the generative process. Start with a rooted tree structure that contains internal nodes and leaf nodes. This skeleton is a prior"
P12-2054,D07-1109,1,0.915494,"algorithm for LDA based on a refactorization of the conditional topic distribution (reviewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we provide a factorization for tree-based models within a broadly applicable inference framework that empirically improves the efficiency of inference (Section 5). 2 Topic Modeling with Tree-Based Priors Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used treestructured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al., 2007). In both cases, the tree came from WordNet (Miller, 1990), but the tree could also come from domain experts (Andrzejewski et al., 2009). Organizing words in this way induces correlations that are mathematically impossible to represent with a symmetric Dirichlet prior. To see how correlations can occur, consider the generative process. Start with a rooted tree structure that contains internal nodes and leaf nodes. This skeleton is a prior that generates K topics. Like vanilla LDA, these topics are distributions over words. Unlike vanilla LDA, their structure correlates words. Internal nodes ha"
P12-2054,P11-1026,1,0.834134,"(Blei et al., 2003), discover latent themes present in text collections. “Topics” discovered by topic models are multinomial probability distributions over words that evince thematic coherence. Topic models are used in computational biology, computer vision, music, and, of course, text analysis. One of LDA’s virtues is that it is a simple model that assumes a symmetric Dirichlet prior over its word distributions. Recent work argues for structured distributions that constrain clusters (Andrzejewski et al., 2009), span languages (Jagarlamudi and Daum´e III, 2010), or incorporate human feedback (Hu et al., 2011) to improve the quality and flexibility of topic modeling. These models all use different tree-based prior distributions (Section 2). These approaches are appealing because they preserve conjugacy, making inference using Gibbs sampling (Heinrich, 2004) straightforward. While straightforward, inference isn’t cheap. Particularly Jordan Boyd-Graber iSchool and UMIACS University of Maryland, College Park jbg@umiacs.umd.edu for interactive settings (Hu et al., 2011), efficient inference would improve perceived latency. S PARSE LDA (Yao et al., 2009) is an efficient Gibbs sampling algorithm for LDA"
P14-1034,E09-1013,0,0.0312876,"els. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al., 2003), which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood. An exciting vein of new research"
P14-1034,N09-1009,0,0.0188528,": The number of documents in the train, development, and test folds in our three datasets. and maximizing log probability of the posterior (ignoring constant terms) (Rennie, 2003). 3.1 L2 Regularization The simplest form of regularization we can add is L2 regularization. This is similar to assuming that probability of a word given a topic comes from a Gaussian distribution. While the distribution over topics is typically Dirichlet, Dirichlet distributions have been replaced by logistic normals in topic modeling applications (Blei and Lafferty, 2005) and for probabilistic grammars of language (Cohen and Smith, 2009). Augmenting the anchor objective with an L2 penalty yields   X ¯ s ,·  ¯ i,· || Ci,· =argminCi,· DKL Q Ci,k Q k sk ∈S + λkCi,· − µi,· k22 , (5) where regularization weight λ balances the importance of a high-fidelity reconstruction against the regularization, which encourages the anchor coefficients to be close to the vector µ. When the mean vector µ is zero, this encourages the topic coefficients to be zero. In Section 4.3, we use a non-zero mean µ to encode an informed prior to encourage topics to discover specific concepts. 3.2 Beta Regularization The more common prior for topic models"
P14-1034,N13-1015,0,0.0548633,"Missing"
P14-1034,P07-1033,0,0.0515445,"Missing"
P14-1034,P12-2023,1,0.800938,"th probabilistic models. We examine Arora et al.’s anchor words algorithm for topic modeling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a"
P14-1034,N10-1012,0,0.161459,"(Hu et al., 2013). In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors. Another shortcoming is that these models have not been scrutinized using standard NLP evaluations. Because these approaches emerged from the theory community, anchor’s evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously unseen data—as measured by held-out likelihood (Blei et al., 2003)—and are more interpretable (Chang et al., 2009; Newman et al., 2010). We also show that our extension to the anchor method enables new applications: for 359 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 359–369, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics K V M Q ¯ Q ¯ i,· Q A C S λ number of topics vocabulary size document frequency: minimum documents an anchor word candidate must appear in word co-occurrence matrix Qi,j = p(w1 = i, w2 = j) conditional distribution of Q ¯ i,j = p(w1 = j |w2 = i) Q ¯ row i of Q topic matrix, of size V × K Aj,k = p(w = j |z = k) anc"
P14-1034,N09-1068,0,0.0260817,"For example, Arora et al. (2012b)’s approach for inference in topic models assume that each topic has a unique “anchor” word (thus, we call this approach anchor). This approach is fast and effective; because it only uses word co-occurrence information, it can scale to much larger datasets than MCMC or EM alternatives. We review the anchor method in Section 2. Despite their advantages, these techniques are not a panacea. They do not accommodate the rich priors that modelers have come to expect. Priors can improve performance (Wallach et al., 2009), provide domain adaptation (Daum´e III, 2007; Finkel and Manning, 2009), and guide models to reflect users’ needs (Hu et al., 2013). In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors. Another shortcoming is that these models have not been scrutinized using standard NLP evaluations. Because these approaches emerged from the theory community, anchor’s evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously unseen data—as measured by held-out likelihood (Blei et al., 2003)—and ar"
P14-1034,D08-1038,0,0.028051,"lop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors. 1 Introduction Topic models are of practical and theoretical interest. Practically, they have been used to understand political perspective (Paul and Girju, 2010), improve machine translation (Eidelman et al., 2012), reveal literary trends (Jockers, 2013), and understand scientific discourse (Hall et al., 2008). Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Modern topic models are formulated as a latent variable model. Like hidden Markov models (Rabiner, 1989, HMM), each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an admixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC (Griffiths and Steyvers, 2004) or variational EM (Blei et al"
P14-1034,E14-1056,0,0.0677137,"Missing"
P14-1034,P11-1026,1,\N,Missing
P14-1105,D10-1111,0,0.0321643,"campaign cycle. They use an HMM -based model, defining the states as a set of fine-grained political ideologies, and rely on a closed set of lexical bigram features associated with each ideology, inferred from a manually labeled ideological books corpus. Although it takes elements of discourse structure into account (capturing the“burstiness” of ideological terminology usage), their model explicitly ignores intrasentential contextual influences of the kind seen in Figure 1. Other approaches on the document level use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently,"
P14-1105,J92-4003,0,0.211075,"th liberal, Republicans with conservative) lends confidence that this dataset contains a rich mix of ideological 1115 1 Available at http://cs.umd.edu/˜miyyer/ibc statements. However, the raw Convote dataset contains a low percentage of sentences with explicit ideological bias.2 We therefore use the features in Yano et al. (2010), which correlate with political bias, to select sentences to annotate that have a higher likelihood of containing bias. Their features come from the Linguistic Inquiry and Word Count lexicon (LIWC) (Pennebaker et al., 2001), as well as from lists of “sticky bigrams” (Brown et al., 1992) strongly associated with one party or another (e.g., “illegal aliens” implies conservative, “universal healthcare” implies liberal). We first extract the subset of sentences that contains any words in the LIWC categories of Negative Emotion, Positive Emotion, Causation, Anger, and Kill verbs.3 After computing a list of the top 100 sticky bigrams for each category, ranked by loglikelihood ratio, and selecting another subset from the original data that included only sentences containing at least one sticky bigram, we take the union of the two subsets. Finally, we balance the resulting dataset s"
P14-1105,P13-1162,0,0.282011,"; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently, Recasens et al. (2013) detect biased words in sentences using indicator features for bias cues such as hedges and factive verbs in addition to standard bag-of-words and part-of-speech features. They show that this type of linguistic information dramatically improves performance over several standard baselines. Greene and Resnik (2009) also emphasize the connection between syntactic and semantic relationships in their work on “implicit sentiment”, 1120 n 1 Most conservative n-grams Salt, Mexico, housework, speculated, consensus, lawyer, pharmaceuticals, ruthless, deadly, Clinton, redistribution 3 prize individual li"
P14-1105,N12-1085,1,0.308395,"ences, and selected typical partisan unigrams to represent the “biased” class. DUALIST labels 11,555 sentences as politically biased, 5,434 of which come from conservative authors and 6,121 of which come from liberal authors. 3.2.1 Annotating the IBC For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence’s author, where position is either liberal or conservative.5 We used the Crowdflower crowdsourcing platform (crowdflower.com), which has previously been used for subsentential sentiment annotation (Sayeed et al., 2012), to obtain human annotations of the filtered IBC dataset for political bias on both the sentence and phrase level. While members of the Crowdflower workforce are certainly not experts in political science, our simple task and the ubiquity of political bias allows us to acquire useful annotations. Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al., 2013a). Because of the expense of labeling every node in a sentence, we only label one path in each sentence. The process for selecting paths is as follows: first, if any paths contain o"
P14-1105,D11-1136,0,0.00970619,"also substantial parliamentary boilerplate language. 3 While Kill verbs are not a category in LIWC, Yano et al. (2010) adopted it from Greene and Resnik (2009) and showed it to be a useful predictor of political bias. It includes words such as “slaughter” and “starve”. 4 This difference can be mainly attributed to a historical topics in the IBC (e.g., the Crusades, American Civil War). In Convote, every sentence is part of a debate about 2005 political policy. bias, instead of the more general task of classifying sentences as “neutral” or “biased”, we filter the dataset further using DUALIST (Settles, 2011), an active learning tool, to reduce the proportion of neutral sentences in our dataset. To train the DUALIST classifier, we manually assigned class labels of “neutral” or “biased” to 200 sentences, and selected typical partisan unigrams to represent the “biased” class. DUALIST labels 11,555 sentences as politically biased, 5,434 of which come from conservative authors and 6,121 of which come from liberal authors. 3.2.1 Annotating the IBC For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence’s auth"
P14-1105,D13-1010,0,0.0532505,"the liberal-to-conservative switch. In D, negation confuses our model. the sake of simplicity. For example, Gentzkow and Shapiro (2010) derive a “slant index” to rate the ideological leaning of newspapers. A newspaper’s slant index is governed by the frequency of use of partisan collocations of 2-3 tokens. Similarly, authors have relied on simple models of language when leveraging inferred ideological positions. E.g., Gerrish and Blei (2011) predict the voting patterns of Congress members based on bagof-words representations of bills and inferred political leanings of those members. Recently, Sim et al. (2013) have proposed a model to infer mixtures of ideological positions in documents, applied to understanding the evolution of ideological rhetoric used by political candidates during the campaign cycle. They use an HMM -based model, defining the states as a set of fine-grained political ideologies, and rely on a closed set of lexical bigram features associated with each ideology, inferred from a manually labeled ideological books corpus. Although it takes elements of discourse structure into account (capturing the“burstiness” of ideological terminology usage), their model explicitly ignores intras"
P14-1105,N09-1057,1,0.545424,"sentences in the IBC, most of which have no noticeable political bias. Therefore we use the filtering procedure outlined in Section 3.1.1 to obtain a subset of 55,932 sentences. Compared to our final Convote dataset, an even larger percentage of the IBC sentences exhibit no noticeable political bias.4 Because our goal is to distinguish between liberal and conservative 2 Many sentences in Convote are variations on “I think this is a good/bad bill”, and there is also substantial parliamentary boilerplate language. 3 While Kill verbs are not a category in LIWC, Yano et al. (2010) adopted it from Greene and Resnik (2009) and showed it to be a useful predictor of political bias. It includes words such as “slaughter” and “starve”. 4 This difference can be mainly attributed to a historical topics in the IBC (e.g., the Crusades, American Civil War). In Convote, every sentence is part of a debate about 2005 political policy. bias, instead of the more general task of classifying sentences as “neutral” or “biased”, we filter the dataset further using DUALIST (Settles, 2011), an active learning tool, to reduce the proportion of neutral sentences in our dataset. To train the DUALIST classifier, we manually assigned cl"
P14-1105,D11-1014,0,0.786987,"e sentence in Figure 1 includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here"
P14-1105,D13-1137,0,0.00652278,"data; the discrepancy between categorical predictions and annotations is measured 1114 through the cross-entropy loss. We optimize the model parameters to minimize the cross-entropy loss over all sentences in the corpus. The crossentropy loss of a single sentence is the sum over the true labels yi in the sentence, `(ˆ ys ) = k X yp ∗ log(ˆ yp ). (4) p=1 This induces a supervised objective function over all sentences: a regularized sum over all node losses normalized by the number of nodes N in the training set, C= 1 N N X `(predi ) + i λ kθk2 . 2 (5) We use L - BFGS with parameter averaging (Hashimoto et al., 2013) to optimize the model parameters θ = (WL , WR , Wcat , We , b1 , b2 ). The gradient of the objective, shown in Eq. (6), is computed using backpropagation through structure (Goller and Kuchler, 1996), N ∂C 1 X ∂`(ˆ yi ) = + λθ. ∂θ N ∂θ (6) i 2.2 Initialization When initializing our model, we have two choices: we can initialize all of our parameters randomly or provide the model some prior knowledge. As we see in Section 4, these choices have a significant effect on final performance. Random The most straightforward choice is to initialize the word embedding matrix We and composition matrices W"
P14-1105,P13-1088,0,0.0090081,"ove further when given additional phrase-level annotations. RNNs are quantitatively more effective than existing methods that use syntactic and semantic features separately, and we also illustrate how our model correctly identifies ideological bias in complex syntactic constructions. 2 Recursive Neural Networks Recursive neural networks (RNNs) are machine learning models that capture syntactic and semantic composition. They have achieved state-of-the-art performance on a variety of sentence-level NLP tasks, including sentiment analysis, paraphrase detection, and parsing (Socher et al., 2011a; Hermann and Blunsom, 2013). RNN models represent a shift from previous research on ideological bias detection in that they do not rely on hand-made lexicons, dictionaries, or rule sets. In this section, we describe a supervised RNN model for bias detection and highlight differences from previous work in training procedure and initialization. 2.1 ological bias becomes identifiable only at higher levels of sentence trees (as verified by our annotation, Figure 4), models relying primarily on wordlevel distributional statistics are not desirable for our problem. The basic idea behind the standard RNN model is that each wor"
P14-1105,P13-1045,0,0.411506,"includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here, d = 6) are composed"
P14-1105,D13-1170,0,0.148607,"includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here, d = 6) are composed"
P14-1105,W06-1639,0,0.0602891,"wo terms are highly correlated (e.g., a member of the Republican party likely agrees with conservative stances on most issues), they are not identical. For example, a moderate Republican might agree with the liberal position on increased gun control but take conservative positions on other issues. To avoid conflating partisanship and ideology we create a new dataset annotated for ideological bias on the sentence and phrase level. In this section we describe our initial dataset (Convote) and explain the procedure we followed for creating our new dataset (IBC).1 3.1 Convote The Convote dataset (Thomas et al., 2006) consists of US Congressional floor debate transcripts from 2005 in which all speakers have been labeled with their political party (Democrat, Republican, or independent). We propagate party labels down from the speaker to all of their individual sentences and map from party label to ideology label (Democrat → liberal, Republican → conservative). This is an expedient choice; in future work we plan to make use of work in political science characterizing candidates’ ideological positions empirically based on their behavior (Carroll et al., 2009). While the Convote dataset has seen widespread use"
P14-1105,J04-3002,0,0.0126291,"s on the document level use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently, Recasens et al. (2013) detect biased words in sentences using indicator features for bias cues such as hedges and factive verbs in addition to standard bag-of-words and part-of-speech features. They show that this type of linguistic information dramatically improves performance over several standard baselines. Greene and Resnik (2009) also emphasize the connection between syntactic and semantic relationships in their work on “implicit sentiment”, 1120 n 1 Most conservative n-grams Salt, M"
P14-1105,H05-1044,0,0.0342694,"“be used as an instrument to achieve charitable or social ends” reflects a liberal ideology, which the model predicts correctly. However, our model is unable to detect the polarity switch when this phrase is negated with “should not”. Since many different issues are discussed in the IBC, it is likely that our dataset has too few examples of some of these issues for the model to adequately learn the appropriate ideological positions, and more training data would resolve many of these errors. 6 Related Work A growing NLP subfield detects private states such as opinions, sentiment, and beliefs (Wilson et al., 2005; Pang and Lee, 2008) from text. In general, work in this category tends to combine traditional surface lexical modeling (e.g., bag-of-words) with hand-designed syntactic features or lexicons. Here we review the most salient literature related to the present paper. 6.1 Automatic Ideology Detection Most previous work on ideology detection ignores the syntactic structure of the language in use in favor of familiar bag-of-words representations for 1119 A B Thus , the harsh made worse by of free-market the implementing ideology conditions for farmers caused by a number of factors , , have created"
P14-1105,W10-0723,1,0.935118,"al., 2009). While the Convote dataset has seen widespread use for document-level political classification, we are unaware of similar efforts at the sentence level. 3.1.1 Biased Sentence Selection The strong correlation between US political parties and political ideologies (Democrats with liberal, Republicans with conservative) lends confidence that this dataset contains a rich mix of ideological 1115 1 Available at http://cs.umd.edu/˜miyyer/ibc statements. However, the raw Convote dataset contains a low percentage of sentences with explicit ideological bias.2 We therefore use the features in Yano et al. (2010), which correlate with political bias, to select sentences to annotate that have a higher likelihood of containing bias. Their features come from the Linguistic Inquiry and Word Count lexicon (LIWC) (Pennebaker et al., 2001), as well as from lists of “sticky bigrams” (Brown et al., 1992) strongly associated with one party or another (e.g., “illegal aliens” implies conservative, “universal healthcare” implies liberal). We first extract the subset of sentences that contains any words in the LIWC categories of Negative Emotion, Positive Emotion, Causation, Anger, and Kill verbs.3 After computing"
P14-1110,D10-1005,1,0.84948,"2; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review these models for discovering topics in multilingual datasets and discuss how they can improve SMT. However, no models combine multiple bridges between languages. In Section 3, we create a model—the polylingual tree-based topic models (ptLDA)—that uses information from both external dictionaries and document alignments simultaneously. In Section 4, we derive both MCMC and variational inference for this new topic model. In Section 5, we evaluate our model on the task of SMT using aligned datasets. We show that ptLDA offers bett"
P14-1110,W07-0717,0,0.00835741,"th the same genre (e.g., sports, business) from a similar style (e.g., newswire, blog-posts). These are called domains. Translations within one domain are better than translations across domains since they vary dramatically in their word choices and style. A correct translation in one domain may be inappropriate in another domain. For example, “潜水” in a newspaper usually means “underwater diving”. On social media, it means a non-contributing “lurker”. Domain Adaptation for SMT Training a SMT system using diverse data requires domain adaptation. Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al., 2009) to model domain information. Chiang et al. (2011) combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain. However, these approaches treat domains as hand-labeled, constant, and known a priori. This setup is at best expensive and at worst infeasible for large data. Topic models provide a solution where domains can be automatically induced from raw data: treat each topic as a domain.1 1 Henceforth we will use the term “topic” and “domain” interchangeably: “topic” to refer"
P14-1110,2012.iwslt-papers.17,0,0.08445,"work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. Vladimir Eidelman FiscalNote Inc. Washington DC Jordan Boyd-Graber iSchool and UMIACS University of Maryland vlad@fiscalnote.com jbg@umiacs.umd.edu As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik"
P14-1110,P12-2054,1,0.917701,"π)p(wdn |ydn )] Markov Chain Monte Carlo Inference We use a collapsed Gibbs sampler for tree-based topic models to sample the path ydn and topic assignment zdn for word wdn , p(zdn = k, ydn = s|¬zdn , ¬ydn , w; α, β) j i→j |k i→j 0 ) , where Ω(s) represents the word that path s leads to, Nk|d is the number of tokens assigned to topic k in document d and Ni→j|k is the number of times edge i → j in the tree assigned to topic k, excluding the topic assignment zdn and its path ydn of current token wdn . In practice, we sample the latent variables using efficient sparse updates (Yao et al., 2009; Hu and Boyd-Graber, 2012). 4.2 (6) where H[•] represents the entropy of a distribution. Optimizing L using coordinate descent provides the following updates: N +α P k|d k0 (Nk0 |d +α) ∝ I [Ω(s) = wdn ] · Q N +βi→j · i→j∈s P 0 (Ni→j|k0 +β + H[q(θ)] + H[q(π)] + H[q(z, y)], Variational Bayesian Inference Variational Bayesian inference approximates the posterior distribution with a simplified variational distribution q over the latent variables: document topic proportions θ, transition probabilities π, topic assignments z, and path assignments y. Variational distributions typically assume a mean-field distribution over th"
P14-1110,D07-1109,1,0.478983,"mation can induce topics from multilingual corpora. For instance, orthographic similarity connects words with the same meaning in related languages (BoydGraber and Blei, 2009), and dictionaries are a more general source of information on which words share meaning (Boyd-Graber and Resnik, 2010). These two approaches are not mutually exclusive, however; they reveal different connections across languages. In the next section, we combine these two approaches into a polylingual tree-based topic model. 3 Polylingual Tree-based Topic Models In this section, we bring existing tree-based topic models (Boyd-Graber et al., 2007, tLDA) and polylingual topic models (Mimno et al., 2009, pLDA) together and create the polylingual treebased topic model (ptLDA) that incorporates both word-level correlations and document-level alignment information. Word-level Correlations Tree-based topic models incorporate the correlations between words by encouraging words that appear together in a concept to have similar probabilities given a topic. These concepts can come from WordNet (BoydGraber and Resnik, 2010), domain experts (Andrzejewski et al., 2009), or user constrains (Hu et al., 2013). When we gather concepts from bilingual r"
P14-1110,N03-1017,0,0.300258,"ociation for Computational Linguistics 2.1 Statistical Machine Translation 2.2 Statistical machine translation casts machine translation as a probabilistic process (Koehn, 2009). For a parallel corpus of aligned source and target sentences (F, E), a phrase f¯ ∈ F is translated to a phrase e¯ ∈ E according to a distribution pw (¯ e|f¯). One popular method to estimate the probability pw (¯ e|f¯) is via lexical weighting features. Lexical Weighting In phrase-based SMT, lexical weighting features estimate the phrase pair quality by combining lexical translation probabilities of words in a phrase (Koehn et al., 2003). Lexical conditional probabilities p(e|f ) are maximum likelihood estimates from relative lexical freP quencies c(f, e)/ e c(f, e) , where c(f, e) is the count of observing lexical pair (f, e) in the training dataset. The phrase pair probabilities pw (¯ e|f¯) are the normalized product of lexical probabilities of the aligned word pairs within that phrase pair (Koehn et al., 2003). In Section 2.2, we create topic-specific lexical weighting features. Cross-Domain SMT A SMT system is usually trained on documents with the same genre (e.g., sports, business) from a similar style (e.g., newswire, b"
P14-1110,P96-1041,0,0.193917,"e domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms— Gibbs sampling (gibbs), variational inference (variational) and hybrid approach (variationalhybrid)—on the effects of SMT performance. I"
P14-1110,P11-2080,0,0.035923,"se are called domains. Translations within one domain are better than translations across domains since they vary dramatically in their word choices and style. A correct translation in one domain may be inappropriate in another domain. For example, “潜水” in a newspaper usually means “underwater diving”. On social media, it means a non-contributing “lurker”. Domain Adaptation for SMT Training a SMT system using diverse data requires domain adaptation. Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al., 2009) to model domain information. Chiang et al. (2011) combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain. However, these approaches treat domains as hand-labeled, constant, and known a priori. This setup is at best expensive and at worst infeasible for large data. Topic models provide a solution where domains can be automatically induced from raw data: treat each topic as a domain.1 1 Henceforth we will use the term “topic” and “domain” interchangeably: “topic” to refer to the concept in topic models and “domain” to refer to SMT corpora. Inducing Domains with Top"
P14-1110,P10-4002,1,0.702076,"the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms— Gibbs sampling (gibbs), variational inference (variational) and hybrid approach (variationalhybrid)—on the effects of SMT performance. In all experiments, we set the per"
P14-1110,P12-2023,1,0.852876,"on rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. Vladimir Eidelman FiscalNote Inc. Washington DC Jordan Boyd-Graber iSchool and UMIACS University of Maryland vlad@fiscalnote.com jbg@umiacs.umd.edu As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (B"
P14-1110,W04-3250,0,0.0433168,"on three test sets is mostly significant with p = 0.01, except the results on MT03 using variational and variational-hybrid inferences. Polylingual topic models pLDA and tree-based topic models tLDA-dict are consistently better than LDA, suggesting that incorporating additional bilingual knowledge improves topic models. These improvements are not redundant: our new ptLDA-dict model, which has aspects of both models yields the best performance among these approaches—up to a 1.2 BLEU point gain (higher is better), and -2.6 TER improvement (lower is better). The BLEU improvement is significant (Koehn, 2004) at p = 0.01,6 except on MT03 with variational and variationalhybrid inference. While ptLDA-align performs better than baseline SMT and LDA, it is worse than ptLDA-dict, possibly because of errors in the word alignments, making the tree priors less effective. Scalability While gibbs has better translation scores than variational and variational-hybrid, it is less scalable to larger datasets. With 1.6M NIST 6 Because we have multiple runs of each topic model (and thus different translation models), we select the run closest to the average BLEU for the translation significance test. training sen"
P14-1110,P09-5002,0,0.152188,"unannotated document collections. Topic models discover—without any supervision—the primary themes presented in a dataset: the namesake topics. Topic models have two primary applications: to aid human exploration of corpora (Chang et al., 2009) or serve as a low-dimensional representation for downstream applications. We focus on the second application, which has been fruitful for computer vision (Li Fei-Fei and Perona, 2005), computational biology (Perina et al., 2010), and information retrieval (Kataria et al., 2011). In particular, we use topic models to aid statistical machine translation (Koehn, 2009, SMT). Modern machine translation systems use millions of examples of translations to learn translation rules. These systems work best when the training corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. Vladimir Eidelman FiscalNote Inc. Washington DC Jordan Boyd-Graber iSchool and UMIACS University of Maryland vlad@fiscalnote.com jbg@umiacs.umd.edu As we review in Section 2, topic models are a promising solution for automatically discovering domains in ma"
P14-1110,D09-1074,0,0.0119481,"rom a similar style (e.g., newswire, blog-posts). These are called domains. Translations within one domain are better than translations across domains since they vary dramatically in their word choices and style. A correct translation in one domain may be inappropriate in another domain. For example, “潜水” in a newspaper usually means “underwater diving”. On social media, it means a non-contributing “lurker”. Domain Adaptation for SMT Training a SMT system using diverse data requires domain adaptation. Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al., 2009) to model domain information. Chiang et al. (2011) combine these approaches by directly optimizing genre and collection features by computing separate translation tables for each domain. However, these approaches treat domains as hand-labeled, constant, and known a priori. This setup is at best expensive and at worst infeasible for large data. Topic models provide a solution where domains can be automatically induced from raw data: treat each topic as a domain.1 1 Henceforth we will use the term “topic” and “domain” interchangeably: “topic” to refer to the concept in topic models and “domain”"
P14-1110,D09-1092,0,0.758506,"e-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review these models for discovering topics in multilingual datasets and discuss how they can improve SMT. However, no models combine multiple bridges between languages. In Section 3, we create a model—the polylingual tree-based topic models (ptLDA)—that uses information from both external dictionaries and document alignments simultaneously. In Section 4, we derive both MCMC and variational inference for this new topic model. In Section 5, we evaluate our model on the task of SMT using ali"
P14-1110,J03-1002,0,0.0389007,"012). 5 Experiments We evaluate our new topic model, ptLDA, and existing topic models—LDA, pLDA, and tLDA—on their ability to induce domains for machine translation and the resulting performance of the translations on standard machine translation metrics. Dataset and SMT Pipeline We use the NIST MT Chinese-English parallel corpus (NIST), excluding non-UN and non-HK Hansards portions as our training dataset. It contains 1.6M sentence pairs, with 40.4M Chinese tokens and 44.4M English tokens. We replicate the SMT pipeline of Eidelman et al. (2012): word segmentation (Tseng et al., 2005), align (Och and Ney, 2003), and symmetrize (Koehn et al., 2003) the data. We train a modified KneserNey trigram language model on English (Chen and Goodman, 1996). We use CDEC (Dyer et al., 2010) for decoding, and MIRA (Crammer et al., 2006) for parameter training. To optimize SMT system, we tune the parameters on NIST MT06, and report results on three test sets: MT02, MT03 and MT05.2 Topic Models Configuration We compare our polylingual tree-based topic model (ptLDA) against tree-based topic models (tLDA), polylingual topic models (pLDA) and vanilla topic models (LDA).3 We also examine different inference algorithms—"
P14-1110,P02-1040,0,0.0925535,"word alignments between Chinese and English using GIZA ++ (Och and Ney, 2003). We then remove the word pairs appearing more than 50K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs (align). We apply both trees to tLDA and ptLDA, denoted as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDAalign. However, tLDA-align and ptLDA-align do worse than tLDA-dict and ptLDA-dict, so we omit tLDA-align in the results. Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics—BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). We report the results on three different test sets (Figure 2), and all SMT results are averaged over five runs. We refer to the SMT model without domain adaptation as baseline.5 LDA marginally improves machine translation (less than half a BLEU point). 2 The NIST datasets contain 878, 919, 1082 and 1664 sentences for MT02, MT03, MT05 and MT06 respectively. 3 For Gibbs sampling, we use implementations available in Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum, 2002) for LDA and pLDA. 4 This is a two-level tree structure. However, one could build a more"
P14-1110,P06-2124,0,0.0274967,"modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review these models for discovering topics in multilingual datasets and discuss how they can improve SMT. However, no models combine multiple bridges between languages. In Section 3, we create a model—the polylingual tree-based topic models (ptLDA)—that uses information from both external dictionaries and document alignments simultaneously. In Section 4, we derive both MCMC and variational inference for this new topic model. In Section 5, we evaluate our model on the task of SMT using aligned datasets. We show that ptLDA offers better domain adaptation than other topic model"
P14-1110,2006.amta-papers.25,0,0.0413634,"and English using GIZA ++ (Och and Ney, 2003). We then remove the word pairs appearing more than 50K times or fewer than 500 times and construct a second prior tree with about 2500 word pairs (align). We apply both trees to tLDA and ptLDA, denoted as tLDA-dict, tLDA-align, ptLDA-dict, and ptLDAalign. However, tLDA-align and ptLDA-align do worse than tLDA-dict and ptLDA-dict, so we omit tLDA-align in the results. Domain Adaptation using Topic Models We examine the effectiveness of using topic models for domain adaptation on standard SMT evaluation metrics—BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). We report the results on three different test sets (Figure 2), and all SMT results are averaged over five runs. We refer to the SMT model without domain adaptation as baseline.5 LDA marginally improves machine translation (less than half a BLEU point). 2 The NIST datasets contain 878, 919, 1082 and 1664 sentences for MT02, MT03, MT05 and MT06 respectively. 3 For Gibbs sampling, we use implementations available in Hu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum, 2002) for LDA and pLDA. 4 This is a two-level tree structure. However, one could build a more sophisticated tree prior with"
P14-1110,P12-1048,0,0.0563285,"raining corpus has consistent genre, register, and topic. Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. Vladimir Eidelman FiscalNote Inc. Washington DC Jordan Boyd-Graber iSchool and UMIACS University of Maryland vlad@fiscalnote.com jbg@umiacs.umd.edu As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word"
P14-1110,P12-1079,0,0.064938,"Systems that are robust to systematic variation in the training set are said to exhibit domain adaptation. † indicates equal contributions. Vladimir Eidelman FiscalNote Inc. Washington DC Jordan Boyd-Graber iSchool and UMIACS University of Maryland vlad@fiscalnote.com jbg@umiacs.umd.edu As we review in Section 2, topic models are a promising solution for automatically discovering domains in machine translation corpora. However, past work either relies solely on monolingual source-side models (Eidelman et al., 2012; Hasler et al., 2012; Su et al., 2012), or limited modeling of the target side (Xiao et al., 2012). In contrast, machine translation uses inherently multilingual data: an SMT system must translate a phrase or sentence from a source language to a different target language, so existing applications of topic models (Eidelman et al., 2012) are wilfully ignoring available information on the target side that could aid domain discovery. This is not for a lack of multilingual topic models. Topic models bridge the chasm between languages using document connections (Mimno et al., 2009), dictionaries (Boyd-Graber and Resnik, 2010), and word alignments (Zhao and Xing, 2006). In Section 2, we review th"
P14-1110,I05-3027,0,\N,Missing
P14-1110,P11-1026,1,\N,Missing
P14-1110,W12-3160,1,\N,Missing
P15-1139,P12-2054,1,0.835429,"+λ · ψˆk,j new , (6) PJk −d,n 0 j 0 =1 ηk,j Nd,k,j 0 where µa,k,j = ( + ηk,j )/Nd,k,· for an existing frame j, and for a newly created frame j new , we have µa,k,j new = P −d,n ( Jj 0k=1 ηk,j 0 Nd,k,j 0 +ηk,j new )/Nd,k,· , where ηk,j new is drawn from the Gaussian prior N (0, γ). Here, the estimated global probability of choosing a frame j of issue k is ψˆk,j . Sampling Issue Topics In the generative process of HIPTM, the topic φk of issue k (1) generates tokens in the bill text and (2) provides the Dirichlet priors of the issue’s frames. Rather than collapsing multinomials and factorizing (Hu and Boyd-Graber, 2012), we follow Ahmed et al. (2013) and sample ˜ k + βφ? ) φˆk ∼ Dir(mk + n (7) k where mk ≡ (Mk,1 , Mk,2 , · · · , Mk,V ) is the token count vector from the bill text assigned to each ˜k,1 , N ˜k,2 , · · · , N ˜k,V ) ˜ k ≡ (N issue. The vector n denotes the token counts propagated from words assigned to topics that are associated with frames of issue k, approximated using minimal or maximal path assumptions (Cowans, 2006; Wallach, 2008). Sampling Frame Proportions Following the direct assignment method described in Teh et al. (2006), we sample the global frame proportion as ψˆk ≡ (ψˆk,1 , ψˆk,2 ,"
P15-1139,D14-1182,1,0.825569,"th stratified sampling, which preserves the ratio of the two classes in both the training and test sets. We report performance using AUC - ROC (Lusted, 1971) using SVMlight (Joachims, 1999).9 After preprocessing, our vocabulary contains 5,349 unique word types. Membership from Votes and Text. First, given the votes and text of all the legislators, we run HIPTM for 1,000 iterations with a burn-in period of 500 iterations. After burning in, we keep the sampled state of the model after every fifty iterations. The feature values are obtained by averaging over the ten stored models as suggested in Nguyen et al. (2014). Each legislator a is represented by a vector concatenating: • K-dimensional ideal point vector estimated from both votes and text ua,k • K-dimensional vector, estimating the ideal point using only text ηkT ψˆa,k • B probabilities estimating a’s votes on B bills P ˆb,k ua,k + yb ) Φ(xb K ϑ k=1 Predicting Tea Party Membership To quantitatively evaluate the effectiveness of HIPTM in capturing “Tea Partiness”, we predict Tea Party Caucus membership of legislators given their votes and text. This examines (1) how effective the baseline features extracted from the votes and text are in predicting"
P15-1139,N15-1076,1,\N,Missing
P15-1159,P13-1025,1,0.844867,"I think he can still be effective to help me take TUN while you take ROM. HOL should fall next year, and then MUN and KIE shortly thereafter. Politeness I wonder if you shouldn’t try to support Italy into MAR ... What do you think? Subjectivity I’m just curious what you think. Talkativeness Table 1: Summary of the linguistic cues we consider. likely to be aware that the cooperation has no future. (More argumentation and discourse features will be discussed in the following sections.) Politeness. Pragmatic information can also be informative of the relation between two individuals; for example Danescu-Niculescu-Mizil et al. (2013) show that differences in levels of politeness can echo differences in status and power. We measure the politeness of each message using the Stanford Politeness classifier and find that friendships that end in betrayal show a slight imbalance between the level of politeness used by the two individuals (one-sample t-test on the imbalance, p = 0.09) and that in those cases the future victim is the one that is less polite. Subjectivity. We explored phrases expressing opinion, accusation, suspicion, and speculation taken from an automatically collected lexicon (Riloff and Wiebe, 2003), but did not"
P15-1159,P12-2034,0,0.0235038,"2011) is formed from an incomplete sample of a course of a relationship. In contrast, a game of Diplomacy is shorter than almost any marriage and we have a complete account of all interactions throughout the entire relationship. Furthermore, this work focuses on the unilateral and asymmetric act of betrayal, rather than on the question of whether a relation will last. Playing Diplomacy online is less tangible than a romantic relationship, but understanding trust and deception in online interactions (Riegelsberger et 1657 al., 2003; Newman et al., 2003; Hancock et al., 2007; Ott et al., 2011; Feng et al., 2012) is particularly important because the Internet marketplace is a growing driver of economic growth (Boyd, 2003). Diplomacy offers a setting in which deception occurs spontaneously in the context of complex relationships. 7 References Conclusions Despite people’s best effort to hide it, the intention to betray can leak through the language one uses. Detecting it is not a task that we expect to be solvable with high accuracy, as that would entail a reliable “recipe” for avoiding betrayal in relationships; in this unrealistic scenario, betrayals would be unlikely to exist. While the effects we fi"
P15-1159,P11-1032,0,0.063176,"0; Ireland et al., 2011) is formed from an incomplete sample of a course of a relationship. In contrast, a game of Diplomacy is shorter than almost any marriage and we have a complete account of all interactions throughout the entire relationship. Furthermore, this work focuses on the unilateral and asymmetric act of betrayal, rather than on the question of whether a relation will last. Playing Diplomacy online is less tangible than a romantic relationship, but understanding trust and deception in online interactions (Riegelsberger et 1657 al., 2003; Newman et al., 2003; Hancock et al., 2007; Ott et al., 2011; Feng et al., 2012) is particularly important because the Internet marketplace is a growing driver of economic growth (Boyd, 2003). Diplomacy offers a setting in which deception occurs spontaneously in the context of complex relationships. 7 References Conclusions Despite people’s best effort to hide it, the intention to betray can leak through the language one uses. Detecting it is not a task that we expect to be solvable with high accuracy, as that would entail a reliable “recipe” for avoiding betrayal in relationships; in this unrealistic scenario, betrayals would be unlikely to exist. Whi"
P15-1159,prasad-etal-2008-penn,0,0.00800499,"is the source of this imbalance (Figure 3a, right), we find that that it is the eventual betrayer that uses significantly more positive sentiment than the control counterpart in the matched friendship (twosample t-test, p = 0.001). This is somewhat surprising, and we speculate that this is the betrayer overcompensating for his forthcoming actions. Argumentation and Discourse. Structured discourse and well-made arguments are essential in persuasion (Cialdini, 2000; Anand et al., 2011). To capture discourse complexity, we measure the average number of explicit discourse connectors per sentence (Prasad et al., 2008).7 These markers belong to four coarse classes: comparison, contingency, expansive, and temporal. To capture planning, we group temporal markers that refer to the future (e.g.,“next”, “thereafter”) in a separate category. To quantify the level of argumentation, we calculate average number of claim and premise markers per sentence, as identified by Stab and Gurevych (2014). We also measure the number of request sentences in each message, as identified by the heuristics in the Stanford Politeness classifier (Danescu-Niculescu-Mizil et al., 2013). The structure of the discourse offers clues to wh"
P15-1159,D09-1035,0,0.0306782,"ive research to understand what makes for effective relationships. Jung et al. (2012) show that a balanced working relationship is more likely to lead to better performance on tasks like pair programming. Imai and Gelfand (2010) show that understanding cultural norms improves negotiations. While these data are elicited in the lab, our “found” data are inexpensive because Diplomacy games are fun and inherently anonymized. Romance is a popular and more real-world phenomenon that helps us understand how relationships form and dissolve. The research that tells us how language shapes early dating (Ranganath et al., 2009) and whether an existing relationship will continue (Slatcher and Pennebaker, 2006; Gottman and Levenson, 2000; Ireland et al., 2011) is formed from an incomplete sample of a course of a relationship. In contrast, a game of Diplomacy is shorter than almost any marriage and we have a complete account of all interactions throughout the entire relationship. Furthermore, this work focuses on the unilateral and asymmetric act of betrayal, rather than on the question of whether a relation will last. Playing Diplomacy online is less tangible than a romantic relationship, but understanding trust and d"
P15-1159,W03-1014,0,0.0265118,"anescu-Niculescu-Mizil et al. (2013) show that differences in levels of politeness can echo differences in status and power. We measure the politeness of each message using the Stanford Politeness classifier and find that friendships that end in betrayal show a slight imbalance between the level of politeness used by the two individuals (one-sample t-test on the imbalance, p = 0.09) and that in those cases the future victim is the one that is less polite. Subjectivity. We explored phrases expressing opinion, accusation, suspicion, and speculation taken from an automatically collected lexicon (Riloff and Wiebe, 2003), but did not find significant differences between betrayals and control friendships. Talkativeness. Another conversational aspect is the amount of communication flowing between the players, in each direction. To quantify this, we simply use the number of messages sent, the average number of sentences per message, and the average number of words per sentence. Abnormal communication patterns can indicate a relationship breakdown. For example, friendships that dissolve are characterized by an imbalance in the number of messages exchanged between the two players (one-sample t-test, p < 0.001). Th"
P15-1159,D13-1170,0,0.00450021,"was a pity, as I was willing to give you the lion’s share of centers in the west. [...] If you voiced your concerns I would have supported you in most of the western centers. Unfortunately now you have jumped out of the pan into the fire. Sentiment. Changes in the sentiment expressed in conversation can reflect emotional responses, social affect, as well as the status of the relationship as a whole (Gottman and Levenson, 2000; Wang and Cardie, 2014). We quantify the proportion of exchanged sentences that transmit positive, neutral and negative sentiment using the Stanford Sentiment Analyzer (Socher et al., 2013).6 Example sentences with these features, as well as all other features we consider, can be found in Table 1. We find that an imbalance in the amount of positive sentiment expressed by the two individuals is a subtle sign that the relation will end in betrayal (Figure 3a, left; one-sample t-test on the imbalance, p = 0.008). When looking closer at who is the source of this imbalance (Figure 3a, right), we find that that it is the eventual betrayer that uses significantly more positive sentiment than the control counterpart in the matched friendship (twosample t-test, p = 0.001). This is somewh"
P15-1159,D14-1006,0,0.0167723,"scourse. Structured discourse and well-made arguments are essential in persuasion (Cialdini, 2000; Anand et al., 2011). To capture discourse complexity, we measure the average number of explicit discourse connectors per sentence (Prasad et al., 2008).7 These markers belong to four coarse classes: comparison, contingency, expansive, and temporal. To capture planning, we group temporal markers that refer to the future (e.g.,“next”, “thereafter”) in a separate category. To quantify the level of argumentation, we calculate average number of claim and premise markers per sentence, as identified by Stab and Gurevych (2014). We also measure the number of request sentences in each message, as identified by the heuristics in the Stanford Politeness classifier (Danescu-Niculescu-Mizil et al., 2013). The structure of the discourse offers clues to whether the friendship will last. For example, Figure 3b shows that in friendships doomed to end in betrayal, the victim uses planning discourse markers significantly more often than the betrayer (onesample t-test on the imbalance, p = 0.03), who is 6 We collapse the few examples classified as extreme positive and extreme negative examples into positive and negative, respec"
P15-1159,P14-2113,0,0.161013,"ng is a typical reaction of a player after having been betrayed by a friend: Well that move was sour. I’m guessing France put you up to it, citing my large growth. This was a pity, as I was willing to give you the lion’s share of centers in the west. [...] If you voiced your concerns I would have supported you in most of the western centers. Unfortunately now you have jumped out of the pan into the fire. Sentiment. Changes in the sentiment expressed in conversation can reflect emotional responses, social affect, as well as the status of the relationship as a whole (Gottman and Levenson, 2000; Wang and Cardie, 2014). We quantify the proportion of exchanged sentences that transmit positive, neutral and negative sentiment using the Stanford Sentiment Analyzer (Socher et al., 2013).6 Example sentences with these features, as well as all other features we consider, can be found in Table 1. We find that an imbalance in the amount of positive sentiment expressed by the two individuals is a subtle sign that the relation will end in betrayal (Figure 3a, left; one-sample t-test on the imbalance, p = 0.008). When looking closer at who is the source of this imbalance (Figure 3a, right), we find that that it is the"
P15-1162,S14-2098,0,0.0113706,"Missing"
P15-1162,D10-1115,0,0.0107131,"ible”. However, we find no significant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been success"
P15-1162,D14-1070,1,0.582478,"OW, g averages word embeddings1 1 X z = g(w ∈ X) = vw . (1) |X| w∈X Feeding z to a softmax layer induces estimated probabilities for each output label yˆ = softmax(Ws · z + b), (2) where the softmax function is exp q softmax(q) = Pk j=1 exp qj (3) Ws is a k × d matrix for a dataset with k output labels, and b is a bias term. We train the NBOW model to minimize crossentropy error, which for a single training instance with ground-truth label y is `(ˆ y) = k X yp log(ˆ yp ). (4) p=1 1 Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014). Before we describe our deep extension of the model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed further in Section 4. NBOW 2.2 Considering Syntax for Composition Given a sentence like “You’ll be more entertained getting hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficien"
P15-1162,D12-1118,1,0.276157,"Missing"
P15-1162,D14-1082,0,0.0813431,"ide of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for classification on newspaper reviews. Another potentially interesting application is to add gated units to a DAN,as has been done for recurrent and recursive neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Sutskever et al., 2014; Tai et al., 2015), to drop useless words rather than randomly-selected ones. 8 Conclusion In this paper, we introduce the deep averaging network, which feeds an unweighted average of word"
P15-1162,D14-1179,0,0.0113963,"Missing"
P15-1162,D08-1094,0,0.0145513,"Missing"
P15-1162,D11-1129,0,0.0213482,"n the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and para"
P15-1162,W13-3209,0,0.0832105,"state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we sho"
P15-1162,P14-1105,1,0.666888,"rameters selected on the SST also work well for the IMDB task. 4.1.5 Results The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN - MC model. It’s also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive (Sayeed et al., 2012; Iyyer et al., 2014b), this result is promising for large or messy datasets where fine-grained annotation is infeasible. 4.1.6 Timing Experiments DAN s require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of Table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minutes on a single core o"
P15-1162,W13-3214,0,0.0468386,"Missing"
P15-1162,P14-1062,0,0.482625,"instantiation of NBOW, g averages word embeddings1 1 X z = g(w ∈ X) = vw . (1) |X| w∈X Feeding z to a softmax layer induces estimated probabilities for each output label yˆ = softmax(Ws · z + b), (2) where the softmax function is exp q softmax(q) = Pk j=1 exp qj (3) Ws is a k × d matrix for a dataset with k output labels, and b is a bias term. We train the NBOW model to minimize crossentropy error, which for a single training instance with ground-truth label y is `(ˆ y) = k X yp log(ˆ yp ). (4) p=1 1 Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014). Before we describe our deep extension of the model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed further in Section 4. NBOW 2.2 Considering Syntax for Composition Given a sentence like “You’ll be more entertained getting hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficien"
P15-1162,D13-1166,0,0.0216649,"gnificant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (So"
P15-1162,D14-1181,0,0.0448337,"an NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we show in our question-answering experiments. Kim (2014) shows that some of these issues can be avoided by using a convolutional network instead of a RecNN, but the computational complexity increases even further (see Section 4 for runtime comparisons). What contributes most to the power of syntactic 1682 RecNN softmax z3 = f (W   DAN softmax c1 + b) z2 softmax z2 = f (W  softmax  c2 + b) z1 z1 = f (W h2 = f (W2 · h1 + b2 ) h1 = f (W1 · av + b1 )   c3 + b) c4 av = 4 P i=1 Predator c1 is c2 a c3 masterpiece c4 Predator c1 is c2 a c3 ci 4 masterpiece c4 Figure 1: On the left, a RecNN is given an input sentence for sentiment classification. Soft"
P15-1162,P14-1140,0,0.010475,"stette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a"
P15-1162,P11-1015,0,0.19269,"och on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN ) and the convolutional neural network multichannel (Kim, 2014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vec"
P15-1162,P08-1028,0,0.0096376,"sentiment. Intuitively, after the embeddings are fine-tuned during DAN training, we might expect a decrease in the norms of stopwords and an increase in the 1688 norms of sentiment-rich words like “awesome” or “horrible”. However, we find no significant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN ,"
P15-1162,P05-1015,0,0.56308,"— — — — — 89.4 92.6 — 89.2 — — 431 — — — 2,452 — Table 1: DANs achieve comparable sentiment accuracies to syntactic functions (bottom third of table) but require much less training time (measured as time of a single epoch on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines in"
P15-1162,D14-1162,0,0.123646,"paces of size |V |2 . 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW- RAND and DAN - RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-available 300-d GloVe vectors trained over the Common Crawl (Pennington et al., 2014). The DAN - ROOT model only has access to sentence-level labels for SST experiments, while all other models are trained on labeled phrases (if they exist) in addition to sentences. We train all NBOW and DAN models using AdaGrad (Duchi et al., 2011). We apply DANs to documents by averaging the embeddings for all of a document’s tokens and then feeding that average through multiple layers as before. Since the representations computed by DAN s are always d-dimensional vectors regardless of the input size, they are efficient with respect to both memory and computational cost. We find that the hype"
P15-1162,N12-1085,1,0.265487,"find that the hyperparameters selected on the SST also work well for the IMDB task. 4.1.5 Results The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN - MC model. It’s also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive (Sayeed et al., 2012; Iyyer et al., 2014b), this result is promising for large or messy datasets where fine-grained annotation is infeasible. 4.1.6 Timing Experiments DAN s require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of Table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minute"
P15-1162,D11-1014,0,0.0224104,"oth fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN ) and the convolutional neural network multichannel (Kim, 2014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3 PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. 1684 the word-represe"
P15-1162,P13-1045,0,0.0214218,"NNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their r"
P15-1162,D13-1170,0,0.156033,"NNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their r"
P15-1162,P15-1150,0,0.409406,"hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficiency in the process. In subsequent sections, we argue that this complexity is not matched by a corresponding gain in performance. Recursive neural networks (RecNNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW model"
P15-1162,D14-1004,0,0.00882624,"Missing"
P15-1162,P12-2018,0,0.150115,"014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3 PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural Baselines We also compare to non-neural baselines, specifically the bigram na¨ıve Bayes (BINB) and na¨ıve Bayes support vector machine (NBSVM - BI) models introduced by Wang and Manning (2012), both of which are memory-intensive due to huge feature spaces of size |V |2 . 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW- RAND and DAN - RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-ava"
P16-1065,D10-1005,1,0.838871,"Downstream models, on the contrary, generates topics and supervisory data simultaneously, which turns unsupervised topic models to (semi-)supervised ones. Supervisory data, like labels of documents and links between documents, can be generated from either a maximum likelihood estimation approach (McAuliffe and Blei, 2008; Chang and 6 We omit the comparison of WSBM with other models, because this has been done by Aicher et al. (2014). In addition, WSBM is a probabilistic method while SCC is deterministic. They are not comparable quantitatively, so we compare them qualitatively. 693 Blei, 2010; Boyd-Graber and Resnik, 2010) or a maximum entropy discrimination approach (Zhu et al., 2012; Yang et al., 2015). In block detection literature, stochastic block model (Holland et al., 1983; Wang and Wong, 1987, SBM) is one of the most basic generative models dealing with binary-weighted edges. SBM assumes that each node belongs to only one block and each link exists with a probability that depends on the block assignments of its connecting nodes. It has been generalized for degreecorrection (Karrer and Newman, 2011), bipartite structure (Larremore et al., 2014), and categorial values (Guimer`a and Sales-Pardo, 2013), as"
P16-1065,D07-1109,1,0.777842,"blocks in the C ORA dataset identified by WSBM , designated Blocks 1 and 2. Some statistics are given in Table 4. The two blocks are very sparsely connected, but comparatively quite densely connected inside either block. The two blocks’ topic distributions also reveal their differences: abstracts in Block 1 mainly focus on learning theory (learn, algorithm, bound, result, etc.) and MCMC (markov, chain, distribution, converge, etc.). Abstracts in Block 2, however, have higher 6 Related Work Topic models are widely used in information retrieval (Wei and Croft, 2006), word sense disambiguation (Boyd-Graber et al., 2007), dialogue segmentation (Purver et al., 2006), and collaborative filtering (Marlin, 2003). Topic models can be extended in either upstream or downstream way. Upstream models generate topics conditioned on supervisory information (Daum´e III, 2009; Mimno and McCallum, 2012; Li and Perona, 2005). Downstream models, on the contrary, generates topics and supervisory data simultaneously, which turns unsupervised topic models to (semi-)supervised ones. Supervisory data, like labels of documents and links between documents, can be generated from either a maximum likelihood estimation approach (McAuli"
P16-1065,P09-2074,0,0.0549227,"Missing"
P16-1065,W04-3243,0,0.0598077,"tion gets higher. Finally, only the NN topic dominates the two documents when LBH - RTM is applied (Figure 7(e)). LCH - RTM gives the highest proportion to the NN topic (Figure 7(b)). However, the NN topic 5.3 Topic Quality Results We use an automatic coherence detection method (Lau et al., 2014) to evaluate topic quality. Specifically, for each topic, we pick out the top n words and compute the average association score of each pair of words, based on the held-out documents in development and test sets. We choose n = 25 and use Fisher’s exact test (Upton, 1992, FET) and log likelihood ratio (Moore, 2004, LLR) as the association measures (Table 3). The main advantage of these measures is that they are robust even when the reference corpus is not large. Coherence improves with WSBM and maxmargin learning, but drops a little when adding lexical weights except the FET score on the W E B KB dataset, because lexical weights are intended to improve link prediction performance, not topic quality. Topic quality of LBH-RTM is also better than that of LCH-RTM, suggesting that WSBM benefits topic quality more than SCC. 692 Parallel Computing Optimization-2 Algorithm Bound Knowledge Base Belief Network O"
P16-1065,P14-1110,1,0.874468,"rvedi et al., 2012). edges in the experiment, it would be straightforward to adapt to model both directed/undirected and binary/nonnegative real weight edges. We are also interested in modeling changing topics and vocabularies (Blei and Lafferty, 2006; Zhai and Boyd-Graber, 2013). In the spirit of treating links probabilistically, we plan to explore application of the model in suggesting links that do not exist but should, for example in discovering missed citations, marking social dynamics (Nguyen et al., 2014), and identifying topically related content in multilingual networks of documents (Hu et al., 2014). 7 References Acknowledgment This research has been supported in part, under subcontract to Raytheon BBN Technologies, by DARPA award HR0011-15-C-0113. Boyd-Graber is also supported by NSF grants IIS/1320538, IIS /1409287, and NCSE /1422492. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors. Conclusions and Future Work Christopher Aicher, Abigail Z. Jacobs, and Aaron Clauset. 2014. Learning latent block structure in weighted networks. Journal of Complex Networks. We introduce LBH-RTM, a discr"
P16-1065,P06-1003,0,0.103916,"Missing"
P16-1065,E14-1056,0,0.0324056,"(Table 2), except LCH-RTM, which is consistent with our PLR results. In Figure 7, we also show the proportions of topics that dominate the two documents according to the various models. There are multiple topics dominating K and A according to RTM (Figure 7(a)). As the model gets more sophisticated, the NN topic proportion gets higher. Finally, only the NN topic dominates the two documents when LBH - RTM is applied (Figure 7(e)). LCH - RTM gives the highest proportion to the NN topic (Figure 7(b)). However, the NN topic 5.3 Topic Quality Results We use an automatic coherence detection method (Lau et al., 2014) to evaluate topic quality. Specifically, for each topic, we pick out the top n words and compute the average association score of each pair of words, based on the held-out documents in development and test sets. We choose n = 25 and use Fisher’s exact test (Upton, 1992, FET) and log likelihood ratio (Moore, 2004, LLR) as the association measures (Table 3). The main advantage of these measures is that they are robust even when the reference corpus is not large. Coherence improves with WSBM and maxmargin learning, but drops a little when adding lexical weights except the FET score on the W E B"
P16-1065,D15-1030,1,0.886976,"rs only pairwise document relationships, failing to capture network structure at the level of groups or blocks of documents. We propose a new joint model that makes fuller use of the rich link structure within a document network. Specifically, our model embeds the weighted stochastic block model (Aicher et al., 2014, WSBM) to identify blocks in which documents are densely connected. WSBM basically categorizes each item in a network probabilistically as belonging to one of L blocks, by reviewing its connections with each block. Our model can be viewed as a principled probabilistic extension of Yang et al. (2015), who identify blocks in a document network deterministically as strongly connected components (SCC). Like them, we assign a distinct Dirichlet prior to each block to capture its topical commonalities. Jointly, a linear regression model with a discriminative, max-margin objective function (Zhu et al., 2012; Zhu et al., 2014) is trained to reconstruct the links, taking into account the features of documents’ topic and word distributions (Nguyen et al., 2013), block assignments, and inter-block link rates. We validate our approach on a scientific paper abstract dataset and collection of webpages"
P16-1110,J04-3001,0,0.0360418,"otes (mean) best worst 60 40 20 0 LA LR TA TR auto Condition Figure 6: Best and worst votes for document labels. Error bars are standard error from bootstrap sample. ALTO ( TA ) gets the most best votes and the fewest worst votes. with or without active learning selection—they can generate label sets that improve upon automatic labels and labels assigned without the topic model overview. 7 Related Work Text classification—a ubiquitous machine learning tool for automatically labeling text (Zhang, 2010)— is a well-trodden area of NLP research. The difficulty is often creating the training data (Hwa, 2004; Osborne and Baldridge, 2004); coding theory is an entire subfield of social science devoted to creating, formulating, and applying labels to text data (Saldana, 2012; Musialek et al., 2016). Crowdsourcing (Snow et al., 2008) and active learning (Settles, 2012), can decrease the cost of annotation but only after a label set exists. ALTO’s corpus overviews aid text understanding, building on traditional interfaces for gaining both local and global information (Hearst and Pedersen, 1996). More elaborate interfaces (Eisenstein et al., 2012; Chaney and Blei, 2012; Roberts et al., 2014) provide ri"
P16-1110,P14-1105,1,0.82071,"Missing"
P16-1110,C04-1200,0,0.0383514,"c Overviews, an interactive system to help humans annotate documents: topic models provide a global overview of what labels to create and active learning directs them to the right documents to label. Our forty-annotator user study shows that while active learning alone is best in extremely resource limited conditions, topic models (even by themselves) lead to better label sets, and ALTO’s combination is best overall. 1 Introduction Many fields depend on texts labeled by human experts; computational linguistics uses such annotation to determine word senses and sentiment (Kelly and Stone, 1975; Kim and Hovy, 2004); while social science uses “coding” to scale up and systemetize content analysis (Budge, 2001; Klingemann et al., 2006). Classification takes these labeled data as a training set and labels new data automatically. Creating a broadly applicable and consistent label set that generalizes well is time-consuming and difficult, requiring expensive annotators to examine We create a single interface—ALTO (Active Learning with Topic Overviews)—to address both global and local challenges using two machine learning tools: topic models and active learning (we review both in Section 2). Topic models addre"
P16-1110,P11-1154,0,0.227853,"l interfaces for gaining both local and global information (Hearst and Pedersen, 1996). More elaborate interfaces (Eisenstein et al., 2012; Chaney and Blei, 2012; Roberts et al., 2014) provide richer information given a fixed topic model. Alternatively, because topic models are imperfect (Boyd-Graber et al., 2014), refining underlying topic models may also improve users’ understanding of a corpus (Choo et al., 2013; Hoque and Carenini, 2015). Summarizing document collections through discovered topics can happen through raw topics labeled manually by users (Talley et al., 2011), automatically (Lau et al., 2011), or by learning a mapping from labels to topics (Ramage et al., 2009). When there is not a direct correspondence between topics and labels, classifiers learn a mapping (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2015). Because we want topics to be consistent between users, we use a classifier with static topics in ALTO. Combining our interface with dynamic topics could improve overall labeling, perhaps at the cost of introducing confusion as topics change during the labeling process. 8 Conclusion and Future Work We introduce ALTO, an interactive framework that combines both ac"
P16-1110,E14-1056,0,0.0155198,"We use the 112th Congress; after filtering,6 this dataset has 5558 documents. We use this dataset in both the synthetic experiments (Section 5) and the user study (Section 6). The 20 Newsgroups corpus has 19, 997 documents grouped in twenty news groups that are further grouped into six more general topics. Examples are talk.politics.guns and sci.electronics, which belong to the general topics of politics and science. We use this dataset in synthetic experiments (Section 5). 4.2 Machine Learning Techniques Topic Modeling To choose the number of topics (K), we calculate average topic coherence (Lau et al., 2014) on US Congressional Bills, between ten and forty topics and choose K = 19, as it has the maximum coherence score. For consistency, we use the same number of topics (K = 19) for 20 Newsgroups corpus. After filtering words based on TF - IDF, we use Mallet (McCallum, 2002) with default options to learn topics. Features and Classification A logistic regression predicts labels for documents and provides the classification uncertainty for active learning. To make classification and active learning updates efficient, we use incremental learning (Carpenter, 2008, LingPipe). We update classification p"
P16-1110,P00-1016,0,0.231574,"Missing"
P16-1110,N15-1076,1,0.851688,"model. Alternatively, because topic models are imperfect (Boyd-Graber et al., 2014), refining underlying topic models may also improve users’ understanding of a corpus (Choo et al., 2013; Hoque and Carenini, 2015). Summarizing document collections through discovered topics can happen through raw topics labeled manually by users (Talley et al., 2011), automatically (Lau et al., 2011), or by learning a mapping from labels to topics (Ramage et al., 2009). When there is not a direct correspondence between topics and labels, classifiers learn a mapping (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2015). Because we want topics to be consistent between users, we use a classifier with static topics in ALTO. Combining our interface with dynamic topics could improve overall labeling, perhaps at the cost of introducing confusion as topics change during the labeling process. 8 Conclusion and Future Work We introduce ALTO, an interactive framework that combines both active learning selections with topic model overviews to both help users induce a label set and assign labels to documents. We show that users can more effectively and efficiently induce a label set and create training data using ALTO i"
P16-1110,D08-1027,0,0.044472,"Missing"
P16-1110,D07-1051,0,0.0804637,"Missing"
P16-1110,N04-1012,0,0.0547544,"best worst 60 40 20 0 LA LR TA TR auto Condition Figure 6: Best and worst votes for document labels. Error bars are standard error from bootstrap sample. ALTO ( TA ) gets the most best votes and the fewest worst votes. with or without active learning selection—they can generate label sets that improve upon automatic labels and labels assigned without the topic model overview. 7 Related Work Text classification—a ubiquitous machine learning tool for automatically labeling text (Zhang, 2010)— is a well-trodden area of NLP research. The difficulty is often creating the training data (Hwa, 2004; Osborne and Baldridge, 2004); coding theory is an entire subfield of social science devoted to creating, formulating, and applying labels to text data (Saldana, 2012; Musialek et al., 2016). Crowdsourcing (Snow et al., 2008) and active learning (Settles, 2012), can decrease the cost of annotation but only after a label set exists. ALTO’s corpus overviews aid text understanding, building on traditional interfaces for gaining both local and global information (Hearst and Pedersen, 1996). More elaborate interfaces (Eisenstein et al., 2012; Chaney and Blei, 2012; Roberts et al., 2014) provide richer information given a fixed"
P16-1110,D09-1026,0,0.0614508,"and Pedersen, 1996). More elaborate interfaces (Eisenstein et al., 2012; Chaney and Blei, 2012; Roberts et al., 2014) provide richer information given a fixed topic model. Alternatively, because topic models are imperfect (Boyd-Graber et al., 2014), refining underlying topic models may also improve users’ understanding of a corpus (Choo et al., 2013; Hoque and Carenini, 2015). Summarizing document collections through discovered topics can happen through raw topics labeled manually by users (Talley et al., 2011), automatically (Lau et al., 2011), or by learning a mapping from labels to topics (Ramage et al., 2009). When there is not a direct correspondence between topics and labels, classifiers learn a mapping (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2015). Because we want topics to be consistent between users, we use a classifier with static topics in ALTO. Combining our interface with dynamic topics could improve overall labeling, perhaps at the cost of introducing confusion as topics change during the labeling process. 8 Conclusion and Future Work We introduce ALTO, an interactive framework that combines both active learning selections with topic model overviews to both help users"
P16-1110,P14-2103,0,\N,Missing
P16-1110,P11-1026,1,\N,Missing
P16-1177,D14-1110,0,0.0254243,"e-art model for this task is a semi-supervised approach (Rothe and Sch¨utze, 2015). This model use resources like WordNet 1886 to compute embeddings for different senses of words. Given a pair of target words and their context (neighboring words and sentences), this model represents each target word as the average of its sense embeddings weighted by cosine similarity to the context. The cosine similarity between the representations of words in a pair is then used to determine their semantic similarity. Also, the Skip-gram model (Mikolov et al., 2013a) is extended in (Neelakantan et al., 2014; Chen et al., 2014) to learn contextual word pair similarity in an unsupervised way. Table 2 shows the performance of different models on the SCWS dataset. SAE, CSAE-LC, CSAE-LGC show the performance of our pairwise autoencoders without context, with local context, and with local and global context, respectively. In case of CSAE-LGC, we concatenate local and global context to create context vectors. CSAELGC performs significantly better than the baselines, including the semi-supervised approach in Rothe and Sch¨utze (2015). It is also interesting that SAE (without any context information) outperforms the pre-tra"
P16-1177,P15-2114,0,0.12848,"een their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various forms. For example, the context of a current sentence in a document could be either its neighboring sentences (Lin et al., 2015; Wang and Cho,"
P16-1177,N10-1145,0,0.131137,"and h2n , as additional features: hsub = |h1n − h2n | hdot = h1n h2n , (10) where hsub and hdot capture the element-wise difference and similarity (in terms of the sign of elements in each dimension) between h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a wor"
P16-1177,P12-1092,0,0.157429,"mantic retrieval tasks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, co"
P16-1177,P15-1162,1,0.379487,"Missing"
P16-1177,D14-1218,0,0.0343004,"Missing"
P16-1177,D15-1106,0,0.119472,"e, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various forms. For example, the context of a current sentence in a document could be either its neighboring sentences (Lin et al., 2015; Wang and Cho, 2015), topics associated with the sentence (Mikolov and Zweig, 2012; Le and Mikolov, 2014), the document that contains the sentence (Huang et al., 2012), as well as their combinations (Ji et al., 2016). It is important to integrate context into neural networks because these models are often trained with only local information about their individual inputs. For example, recurrent and recursive neural networks only use local information about previously seen words in a sentence to predict the next word or composition.1 On the other hand, context information (such as topical infor"
P16-1177,D14-1113,0,0.0149496,"ection 3). The state-of-the-art model for this task is a semi-supervised approach (Rothe and Sch¨utze, 2015). This model use resources like WordNet 1886 to compute embeddings for different senses of words. Given a pair of target words and their context (neighboring words and sentences), this model represents each target word as the average of its sense embeddings weighted by cosine similarity to the context. The cosine similarity between the representations of words in a pair is then used to determine their semantic similarity. Also, the Skip-gram model (Mikolov et al., 2013a) is extended in (Neelakantan et al., 2014; Chen et al., 2014) to learn contextual word pair similarity in an unsupervised way. Table 2 shows the performance of different models on the SCWS dataset. SAE, CSAE-LC, CSAE-LGC show the performance of our pairwise autoencoders without context, with local context, and with local and global context, respectively. In case of CSAE-LGC, we concatenate local and global context to create context vectors. CSAELGC performs significantly better than the baselines, including the semi-supervised approach in Rothe and Sch¨utze (2015). It is also interesting that SAE (without any context information) out"
P16-1177,Q15-1022,0,0.00399514,"performances initially improve. The CSAE-LGC model that uses both local and global context benefits more from greater number of hidden layers than CSAE-LC that only uses local context. We attribute this to the use of global context in CSAE-LGC that leads to more accurate representations of words in their context. We also note that with just a single hidden layer, CSAE-LGC largely improves the performance as compared to CSAE-LC. 6 Related Work Representation learning models have been effective in many tasks such as language modeling (Bengio et al., 2003; Mikolov et al., 2013b), topic modeling (Nguyen et al., 2015), paraphrase detection (Socher et al., 2011), and ranking tasks (Yih et al., 2013). We briefly review works that use context information for text representation. Huang et al. (2012) presented an RNN model that uses document-level context information to construct more accurate word representations. In particular, given a sequence of words, the approach uses other words in the document as external (global) knowledge to predict the next word in the sequence. Other approaches have also modeled context at the document level (Lin et al., 2015; Wang and Cho, 2015; Ji et al., 2016). Ji et al. (2016) p"
P16-1177,D14-1162,0,0.108443,"Missing"
P16-1177,P15-1173,0,0.0245374,"Missing"
P16-1177,D13-1044,0,0.01333,"h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context from Huang et al. (2012); “qAns” a TREC QA dataset with ground-truth labels for semantically relevant quest"
P16-1177,P14-1068,0,0.0298506,"ks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various"
P16-1177,D12-1087,0,0.00457205,"ncoder network. Each decoder layer tries to recover the input of its corresponding encoder layer. As such, the weights are initially symmetric and the decoder weights do need to be learned. After the training is complete, the hidden layer hn contains a context-sensitive representation of the inputs x and cx . 2.3 Context Information Context is task and data dependent. For example, a sentence or document that contains a target word forms the word’s context. When context information is not readily available, we use topic models to determine such context for individual inputs (Blei et al., 2003; Stevens et al., 2012). In particular, we use Non-Negative Matrix Factorization (NMF) (Lin, 2007): Given a training set with n instances, i.e., X ∈ Rv×n , where v is the size of a global vocabulary and the scalar k is the number of topics in the dataset, we learn the topic matrix D ∈ Rv×k and context matrix C ∈ Rk×n using the following sparse coding algorithm: min kX − DCk2F + µkCk1 , D,C s.t. (8) D ≥ 0, C ≥ 0, where each column in C is a sparse representation of an input over all topics and will be used as global context information in our model. We obtain context vectors for test instances by transforming them ac"
P16-1177,D07-1003,0,0.0108977,"quation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context from Huang et al. (2012); “qAns” a TREC QA dataset with ground-truth labels for semantically relevant questions and (single-sentence) answers from Wang et al. (2007); and “qSim” a community QA dataset crawled from Stack Exchange with ground-truth labels for semantically equivalent questions from Dos Santos et al. (2015). Table 1 shows statistics of these datasets. To enable direct comparison with previous work, we use the same training, development, and test data provided by Dos Santos et al. (2015) and Wang et al. (2007) for qSim and qAns respectively and the entire data of SCWS (in unsupervised setting). We consider local and global context for target words in SCWS. The local context of a target word is its ten neighboring words (five before and five af"
P16-1177,N13-1106,0,0.0321806,"Missing"
P16-1177,P13-1171,0,0.0498837,"ment-wise difference and similarity (in terms of the sign of elements in each dimension) between h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context"
P17-1083,P14-1034,1,0.83967,"anchors mirrors that of the underlying topics, we can use the same reasoning as Arora et al. (2012a) to assert that we can provably recover the topic-word matrix A with all of the same theoretical guarantees of complexity and robustness. Furthermore, we runtime analysis given by Arora et al. (2013) applies to tandem anchors. If desired, we can also add further robustness and extensibility to tandem anchors by adding regularization to Equation 6. Regularization allows us to add something which is mathematically similar to priors, and has been shown to improve the vanilla anchor word algorithm (Nguyen et al., 2014). We leave the question of the best regularization for tandem anchors as future work, and focus our efforts on solving the problem of interactive topic modeling. i∈Gk This construction, while perhaps not as simple as the previous two, is robust to words which have cooccurrences which are not unique to a single topic. Harmonic Mean Leveraging the intuition that we should use a combination function which is both centralizing (like vector average) and ignores large outliers (like element-wise min), the final combination function is the element-wise harmonic mean. Thus, for each anchor facet ! −1"
P17-1083,P08-1036,0,0.0251704,"andem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive. Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012). Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster. 896 Proc"
P17-1083,P12-2054,1,0.853657,"nately, the runtime of tandem anchors is amenable to interactive topic modeling. On 20 NEWS, interactive updates take a median time of 2.13 seconds. This result was obtained using a single core of an AMD Phemon II X6 1090T processor. Furthermore, larger datasets typically have a sublinear increase in distinct word types, so we can expect to see similar run times, even on much larger datasets. Compared to other interactive topic modeling algorithms, tandem anchors has a very attractive run time. For example, using an optimized version of the sampler for the Interactive Topic Model described by Hu and Boyd-Graber (2012), and the recommended 30 iterations of sampling, the Interactive Topic Model updates with a median time of 24.8 seconds (Hu and Boyd-Graber, 2012), which is well beyond our desired update time for interactive use and an order of magnitude slower than tandem anchors. Another promising interactive topic modeling approach is Utopian (Choo et al., 2013), which uses non-negative factorization, albeit without the benefit of anchor words. Utopian is much slower than tandem anchors. Even on the small InfoVisVAST dataset which contains only 515 documents, Utopian takes 48 seconds to converge. While the"
P17-1083,D14-1138,0,0.13713,"able 1). The anchor word “backpack” may seem strange. However, this dataset contains nothing about regular backpacks; thus, “backpack” is unique to camera bags. Bizarre, low-to-mid frequency words are often anchors because anchor words must be unique to a topic; intuitive or high-frequency words cannot be anchors if they have probability in any other topic. The anchor selection strategy can mitigate this problem to some degree. For example, rather than selecting anchors using an approximate convex hull in high-dimensional space, we can find an exact convex hull in a low-dimensional embedding (Lee and Mimno, 2014). This strategy will produce more salient topics but still makes it difficult for users to manually choose unique anchor words for interactive topic modeling. If we instead ask users to give us representative 897 codes conditional probabilities so that Si,j is equal to p(wi |wj ). For the additional K rows, we invent a cooccurrence pattern that can effectively explain the other words’ conditional probabilities. This modification is similar in spirit to supervised anchor words (Nguyen et al., 2015). This supervised extension of the anchor words algorithm adds columns corresponding to conditiona"
P17-1083,N10-1012,0,0.0150831,", we use variation of information (VI). This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels (Meil˘a, 2003). Since we are measuring the amount of information lost, lower variation of information is better. Based on these clustering metrics, tandem anchors can yield superior topics to those created using single word anchors (Figure 1). As with accuracy, this is true regardless of which combination function we use. Furthermore, harmonic mean produces the least confusion between classes.5 The final evaluation is topic coherence by Newman et al. (2010), which measures whether the topics make sense, and correlates with human judgments of topic quality. Given V , the set of the n most probable words of a topic, coherence is Experimental Results Our first evaluation is a classification task to predict documents’ newsgroup membership. Thus, we do not aim for state-of-the-art accuracy,2 but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than Gram-Schmidt anchors. After randomly splitting the data into test and training sets we learn topics from the test data using both the title-based tandem anchors"
P17-1083,N15-1076,1,0.851927,"hull in high-dimensional space, we can find an exact convex hull in a low-dimensional embedding (Lee and Mimno, 2014). This strategy will produce more salient topics but still makes it difficult for users to manually choose unique anchor words for interactive topic modeling. If we instead ask users to give us representative 897 codes conditional probabilities so that Si,j is equal to p(wi |wj ). For the additional K rows, we invent a cooccurrence pattern that can effectively explain the other words’ conditional probabilities. This modification is similar in spirit to supervised anchor words (Nguyen et al., 2015). This supervised extension of the anchor words algorithm adds columns corresponding to conditional probabilities of metadata values after having seen a particular word. By extending the vector-space representation of each word, anchor words corresponding to metadata values can be found. In contrast, our extension does not add dimensions to the representation, but simply places additional points corresponding to pseudoword words in the vectorspace representation. words for this topic, we would expect combinations of words like “camera” and “bag.” However, with single word anchors we must choos"
P17-1083,P11-1026,1,\N,Missing
P18-2105,W14-3342,0,0.0134729,"tial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017). QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexibility make it an attractive candidate for our proposed task. There are two main modules to QuEst++: a feature extractor and a learning module. The feature extractor produces an intermediate representation of the source and translation in Ratio of non-specific words: Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology (Sridhar et al., 2013), either t"
P18-2105,W16-2387,0,0.0129747,"mation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017). QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexibility make it an attractive candidate for our proposed task. There are two main modules to QuEst++: a feature extractor and a learning module. The feature extractor produces an intermediate representation of the source and translation in Ratio of non-specific words: Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology (Sridhar et al., 2013), either to prevent redundancy or to ease cogniti"
P18-2105,Q17-1015,0,0.0235044,"Missing"
P18-2105,P02-1040,0,0.102145,"words: Sridhar et al. (2013) propose that interpreters regularly use pauses to gain more time to think and as a cognitive strategy to manage memory constraints. An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output. In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017). QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexi"
P18-2105,C04-1046,0,0.19602,"ian (EN-IT) interpretation data attempting to answer these questions. 2 Quality Estimation for Interpretation Ratio of pauses/hesitations/incomplete words: Sridhar et al. (2013) propose that interpreters regularly use pauses to gain more time to think and as a cognitive strategy to manage memory constraints. An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output. In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017"
P18-2105,2013.iwslt-papers.3,1,0.710879,"on metrics available for MT including WER (Su et al.), BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Denkowski and Lavie, 2014), all of which compare the similarity between reference translations and translations. Interpreter output is fundamentally different from any reference that we may use in evaluation because interpreters employ a range of economizing strategies such as segmentation, omission, generalization, and reformulation (Riccardi, 2005). As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (Shimizu et al., 2013). To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction word distinctions, and thus should be better equipped to deal with the disparity between MT and SI. Better handling of these divergences 5 Interpreter Quality Experiments To evaluate the quality of our QE system, we use the Pearson’s r correlation between the predicted and true METEOR for each language pair (Graham, 2015). As a baseline, we train QuEst++ on the out-of-the-box feature set (Section 2). We use k-fold cross-validation individually on EN-JA, EN-FR, and EN-IT s"
P18-2105,W12-3102,0,0.103844,"Missing"
P18-2105,P15-4020,0,0.315887,"t of information passed to the interpreter, maximizing the quality of interpreter output. As a concrete method for estimating interpreter performance, we turn to existing work on QE for machine translation (MT) systems (Specia et al., 2010, 2015), which takes in the source sentence and MT-generated outputs and estimates a measure of quality. In doing so, we arrive at two natural research questions: 3 The default, out-of-the-box, sentence-level feature set for QuEst++ includes seventeen features such as number of tokens in source/target utterances, average token length, n-gram frequency, etc. (Specia et al., 2015). While this feature set is effective for evaluation of MT output, SI output is inherently different—full of pauses, hesitations, paraphrases, re-orderings and repetitions. In the following sections, we describe our methods to adapt QE to handle these phenomena. 1. Do existing methods for performing QE on MT output also allow for accurate estimation of interpreter performance, despite the inherent differences between MT and SI? 2. What unique aspects of the problem of interpreter performance estimation, such as the availability of prosody and other linguistic cues, can be exploited to further"
P18-2105,W14-3348,0,0.0113805,"are assessed for accuracy on the number of omissions, additions and the inaccurate renditions of lexical items and longer phrases (Altman, 1994), but recovery of content and correct terminology are highly valued. While no large corpus exists that has been manually annotated with these measures, they align with the phenomena that MT evaluation tries to solve. One important design decision is which evaluation metric to target in our QE system. There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Denkowski and Lavie, 2014), all of which compare the similarity between reference translations and translations. Interpreter output is fundamentally different from any reference that we may use in evaluation because interpreters employ a range of economizing strategies such as segmentation, omission, generalization, and reformulation (Riccardi, 2005). As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (Shimizu et al., 2013). To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction wo"
P18-2105,N13-1023,0,0.0360684,"the availability of prosody and other linguistic cues, can be exploited to further improve the accuracy of our predictions? 3.1 Interpretation-specific Features To adapt QE to interpreter output, we augment the baseline feature set with four additional types of features that may indicate a struggling interpreter. The remainder of the paper describes methods and experiments on English-Japanese (ENJA), English-French (EN-FR), and English-Italian (EN-IT) interpretation data attempting to answer these questions. 2 Quality Estimation for Interpretation Ratio of pauses/hesitations/incomplete words: Sridhar et al. (2013) propose that interpreters regularly use pauses to gain more time to think and as a cognitive strategy to manage memory constraints. An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output. In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely"
P18-2105,2016.tc-1.5,0,0.199189,"e METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.1 1 Figure 1: Simultaneous interpretation scenarios We examine the task of estimating simultaneous interpreter performance: automatically predicting when interpreters are interpreting smoothly and when they are struggling. This has several immediate potential applications, one of which being in Computer-Assisted Interpretation (CAI). CAI is quickly gaining traction in the interpreting community, with software products such as InterpretBank (Fantinouli, 2016) deployed in interpreting booths to provide live and interactive terminology support. Figure 1(b) shows how this might work; both the interpreter and the CAI system receive the source message and the system displays assistive information in the form of terminology and informational support. While this might improve the quality of interpreter output, there is a danger that these systems will provide too much information and increase the cognitive load imposed upon the interpreter (Fantinouli, 2018). Intuitively, the ideal level of support depends on current interpreter performance. The system c"
P18-2105,P15-1174,0,0.119286,"n (Riccardi, 2005). As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (Shimizu et al., 2013). To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction word distinctions, and thus should be better equipped to deal with the disparity between MT and SI. Better handling of these divergences 5 Interpreter Quality Experiments To evaluate the quality of our QE system, we use the Pearson’s r correlation between the predicted and true METEOR for each language pair (Graham, 2015). As a baseline, we train QuEst++ on the out-of-the-box feature set (Section 2). We use k-fold cross-validation individually on EN-JA, EN-FR, and EN-IT source-interpreter language pairs with a held-out development set and test set for each fold. For each experiment setting, we run the experiment for each fold (ten iterations for each set) and evaluate average Pearson’s r correlation on the development set. In our baseline setting, we extract features based on the default QuEst++ sentence-level feature set (baseline). We ablate baseline features through cross-validation and remove features rela"
P18-2105,J07-1003,0,0.0301372,"ated for pauses and partial renditions of words. Quality Estimation Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017). QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE. Its effectiveness and flexibility make it an attractive candidate for our proposed task. There are two main modules to QuEst++: a feature extractor and a learning module. The feature extractor produces an intermediate representation of the source and translation in Ratio of non-specific words: Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology (Sridhar et"
P18-2105,N16-1111,1,0.899657,"Missing"
P18-3018,P17-1147,0,0.103606,"break them. This annotation process yields a challenge set, which despite being easy for trivia players to answer, systematically stumps automated question answering systems. Diagnosing model errors on the evaluation data provides actionable insights to explore in developing robust and generalizable question answering systems. 1 Jordan Boyd-Graber University of Maryland, College Park jbg@umiacs.umd.edu Introduction Proponents of modern machine learning systems have claimed human parity on difficult tasks such as question answering.1 Datasets such as SQuAD and TriviaQA (Rajpurkar et al., 2016; Joshi et al., 2017) have certainly advanced the state of the art, but are they providing the right examples to measure how well machines can answer questions? Many of the existing question answering datasets are written and evaluated with humans in mind, not computers. Though the way computers solve NLP tasks is fundamentally different than humans. They train on hundreds of thousands of questions, rather than looking at small groups of them in isolation. This allows models to pick up on superficial patterns that may occur in data crawled from the internet (Chen et al., 2016) or 2 A Model-Driven Annotation Proces"
P18-3018,Q16-1037,0,0.0319752,"o the correct answer are given. The system must then triangulate the correct answer by “filling in the blank”. For example, in the first question of Table 2, the place of death and the brother of the entity are given. The training data contains a clue about the place of death (The Battle of Thames) reading “though Related Work Creating evaluation datasets to get fine-grained analysis of particular linguistics features or model attributes has been explored in past work. The LAMBADA dataset tests a model’s ability to understand the broad contexts present in book passages (Paperno et al., 2016). Linzen et al. (2016) create a dataset to evaluate if language models can learn subject-verb number agreement. The most closely 131 Set Training Challenge Training Challenge Question Name this sociological phenomenon, the taking of one’s own life. Name this self-inflicted method of death. Clinton played the saxophone on The Arsenio Hall Show He was edited to appear in the film “Contact”. . . For ten points, name this American president who played the saxophone on an appearance on the Arsenio Hall Show. Answer Suicide Arthur Miller Bill Clinton Rationale Paraphrase Entity Type Distractor Don Cheadle Table 1: Snippe"
P18-3018,P16-1144,0,0.0630919,"Missing"
P18-3018,P16-1223,0,0.0281761,"nd TriviaQA (Rajpurkar et al., 2016; Joshi et al., 2017) have certainly advanced the state of the art, but are they providing the right examples to measure how well machines can answer questions? Many of the existing question answering datasets are written and evaluated with humans in mind, not computers. Though the way computers solve NLP tasks is fundamentally different than humans. They train on hundreds of thousands of questions, rather than looking at small groups of them in isolation. This allows models to pick up on superficial patterns that may occur in data crawled from the internet (Chen et al., 2016) or 2 A Model-Driven Annotation Process This section introduces our framework for tailoring questions to challenge computers, the surrounding community of trivia enthusiasts that create thousands of questions annually, and how we expose QA algorithms to this community to help them craft questions that challenge computers. 1 https://rajpurkar.github.io/ SQuAD-explorer/ 2 www.qanta.org 127 Proceedings of ACL 2018, Student Research Workshop, pages 127–133 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics The protagonist of this opera describes the future d"
P18-3018,D16-1264,0,0.028687,"ained models and try to break them. This annotation process yields a challenge set, which despite being easy for trivia players to answer, systematically stumps automated question answering systems. Diagnosing model errors on the evaluation data provides actionable insights to explore in developing robust and generalizable question answering systems. 1 Jordan Boyd-Graber University of Maryland, College Park jbg@umiacs.umd.edu Introduction Proponents of modern machine learning systems have claimed human parity on difficult tasks such as question answering.1 Datasets such as SQuAD and TriviaQA (Rajpurkar et al., 2016; Joshi et al., 2017) have certainly advanced the state of the art, but are they providing the right examples to measure how well machines can answer questions? Many of the existing question answering datasets are written and evaluated with humans in mind, not computers. Though the way computers solve NLP tasks is fundamentally different than humans. They train on hundreds of thousands of questions, rather than looking at small groups of them in isolation. This allows models to pick up on superficial patterns that may occur in data crawled from the internet (Chen et al., 2016) or 2 A Model-Dri"
P18-3018,W17-5401,0,0.0473402,"Missing"
P18-3018,N18-2017,0,0.0554355,"Missing"
P18-3018,P15-1162,1,0.844248,"Missing"
P18-3018,N18-1170,0,0.022646,"lenging examples for it. An analysis of the resulting data can reveal unknown model limitations and provide insight into improving a system. related work to ours is Ettinger et al. (2017) who also consider using humans as adversaries. Our work differs in that we use model interpretation methods to facilitate breaking a specific system. Other methods have found very simple input modifications can break neural models. For example, adding character level noise drastically reduces machine translation quality (Belinkov and Bisk, 2018), while paraphrases can fool natural language inference systems (Iyyer et al., 2018). Jia and Liang (2017) placed distracting sentences at the end of paragraphs and caused QA systems to incorrectly pick up on the misleading information. These types of input modifications can evaluate one specific type of phenomenon and are complementary to our approach. 7 Acknowledgments We thank all of the Quiz Bowl players and writers who helped make this work possible. We also thank the anonymous reviewers and members of the UMD “Feet Thinking” group for helpful comments. JBG is supported by NSF Grant IIS1652666. Any opinions, findings, conclusions, or recommendations expressed here are th"
P18-3018,D17-1215,0,0.144355,"hem? 2.2 Figure 1: An example Quiz Bowl question. The question becomes progressively easier to answer later on; thus, more knowledgeable players can answer after hearing fewer clues. 2.1 Adversarial Question Writing One approach to evaluate models beyond a typical test set is through adversarial examples (Szegedy et al., 2013) and other types of intentionally difficult inputs. However, language data is hard to modify (e.g., replacing word tokens) without changing the meaning of the input. Past work side-steps this difficulty by modifying examples in a simple enough manner to preserve meaning (Jia and Liang, 2017; Belinkov and Bisk, 2018). Though it is hard to generate complex examples that expose richer phenomena through automatic means. Instead, we propose to use human adversaries in a process we call adversarial writing. In this setting, question writers are tasked with generating challenge questions that break existing QA systems but are still answerable by humans. To facilitate this breaking process, we expose model predictions and interpretation methods to question writers through a user interface. This allows writers to see what changes should be made to confuse the system and visualize the res"
P18-3018,D14-1162,0,\N,Missing
P18-3018,N15-1117,1,\N,Missing
P18-3018,N18-2007,0,\N,Missing
P18-3018,P18-1079,0,\N,Missing
P18-3018,C18-1198,0,\N,Missing
P18-3018,D18-1546,0,\N,Missing
P18-3018,D18-1009,0,\N,Missing
P18-3018,Q19-1004,0,\N,Missing
P18-3018,Q19-1026,0,\N,Missing
P18-3018,N16-3020,0,\N,Missing
P18-3018,W18-5416,1,\N,Missing
P19-1076,W10-0720,0,0.197639,"ll this task allow us to evaluate and compare topic models themselves, but it will also allow us to determine the effectiveness of automated metrics. Because local topic quality is subjective, directly asking annotators to judge assignment quality can result in poor inter-annotator agreement. Instead, we prefer to ask users to perform a task which illuminates the underlying quality indirectly. This parallels the reliance on the word intrusion task to rate topic coherence and topic intrusion to rate document coherence (Chang et al., 2009). We call this proposed task ‘topic-word matching’. Like Chang (2010), we show the annotator a We note the similarity between the topic-word matching task and the task of constructing lexical chains (Hirst et al., 1998). While the relationship between topic modeling and lexical chains has been explored (Chiru et al., 2014; Joty et al., 2010), our task is unique in that it asks users to consider a single word in isolation, rather than to consider any relationship between words in a chain. topics. In contrast to LDA and Anchor Words, which run in minutes and seconds respectively, CopulaLDA takes days to run using the original authors’ implementation. Our attempts"
P19-1076,D14-1182,1,0.822328,"el quality. By correlating the human evaluation with existing, global 1 http://jmcauley.ucsd.edu/data/amazon/ https://radimrehurek.com/gensim 3 Unfortunately, CopulaLDA does not scale beyond 100 2 790 Figure 3: Example of the topic-word matching task. Users are asked to select the topic which best explains the underlined token (“Olympic”). topic models have a few coherent—albeit less significant—topics, while large topic models have many significant topics. Since each model includes non-determinism, we train five instances of each dataset, model, and topic cardinality and average our results (Nguyen et al., 2014, “Multiple Final”). In the interest of reproducibility, the data, the scripts for importing and preprocessing the data, and the code for training and evaluating these topic models are available.4 3.2 short snippet from the data with a single token underlined along with five topic summaries (i.e., the 10 most probable words in the topic-word distribution). We then ask the user to select the topic which best fits the underlined token (Figure 3). One of the five options is the topic that the model actually assigns to the underlined token. The intuition is that the annotator will agree more often"
P19-1076,D10-1038,0,0.0259086,"nter-annotator agreement. Instead, we prefer to ask users to perform a task which illuminates the underlying quality indirectly. This parallels the reliance on the word intrusion task to rate topic coherence and topic intrusion to rate document coherence (Chang et al., 2009). We call this proposed task ‘topic-word matching’. Like Chang (2010), we show the annotator a We note the similarity between the topic-word matching task and the task of constructing lexical chains (Hirst et al., 1998). While the relationship between topic modeling and lexical chains has been explored (Chiru et al., 2014; Joty et al., 2010), our task is unique in that it asks users to consider a single word in isolation, rather than to consider any relationship between words in a chain. topics. In contrast to LDA and Anchor Words, which run in minutes and seconds respectively, CopulaLDA takes days to run using the original authors’ implementation. Our attempts to run it with 150 and 200 topics never finished and were finally killed due to excessive memory consumption on 32GB systems. 4 https://github.com/jefflund/ankura 791 For each of our 39 trained models (i.e., for each model type, dataset, and topic cardinality), we randomly"
P19-1076,D09-1026,0,0.117573,"Missing"
P19-1076,D18-1095,1,0.818883,"istency, should be adopted alongside global metrics such as topic coherence. 1 Introduction Topic models such as Latent Dirichlet Allocation (Blei et al., 2003, LDA) automatically discover topics in a collection of documents, giving users a glimpse into themes present in the documents. LDA jointly derives a set of topics (a distribution over words) and token-topic assignments (a distribution over the topics for each token). While the topics by themselves are valuable, the token-topic assignments are also useful as features for document classification (Ramage et al., 2009; Nguyen et al., 2015; Lund et al., 2018) and, in principle, for topic-based document segmentation. 788 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 788–796 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 A dance1 break1 by P.Diddy1 is also featured2 in both settings4 of the video2 , intercut1 with scenes2 of Mario3 Winans1 playing1 the drums1 . Music1 Film2 Gaming3 Prior work in automated metrics to evaluate topic model quality primarily deals with global evaluations (i.e. evaluations of the topic-word distributions that represent topics)"
P19-1076,D11-1024,0,0.0570006,"nts. 2.1 Coherence While human evaluation of topic coherence is useful, automated evaluations are easier to deploy. Consequently, Newman et al. (2010) proposed a variety of automated evaluations of topic coherence and correlated these metrics with human evaluations using the topic-word intrusion task mentioned above and showed that an evaluation based on aggregating pointwise mutual information (PMI) scores across the most likely terms in a topic distribution correlates well with human evaluations. In fact, there are multiple metrics referred to as ‘coherence’, including Newman et al. (2010); Mimno et al. (2011) and Lau et al. (2014), as well as some more recent exploration of coherence (R¨oder et al., 2015; Lau and Baldwin, 2016). All of these ‘coherence’ metrics are measures of global topic quality, since they consider only the global topic-word distributions. For consistency with Arora et al. (2013), we use the Mimno et al. (2011) formulation of coherence in our evaluations, and use this automated evaluation as a proxy for human evaluations using topic-intrusion tasks. Because automated evaluation is known to correlate with human evaluations of global topic quality, we do not investigate global to"
P19-1076,N10-1012,0,0.2829,"ley4, courtni.byun} @byu.edu Jordan Boyd-Graber Computer Science, iSchool, LSC , and UMIACS University of Maryland jbg@umiacs.umd.edu Kevin Seppi Computer Science Brigham Young University kseppi@byu.edu Abstract Given the breadth of topic model variants and implementations, the question of algorithm selection and model evaluation can be as daunting as it is important. When the model is used for a downstream evaluation task (e.g., document classification), these questions can often be answered by maximizing downstream task performance. In other cases, automated metrics such as topic coherence (Newman et al., 2010) can help assess topic model quality. Generally speaking, these metrics evaluate topic models globally, meaning that the metrics evaluate characteristics of the topics (word distributions) themselves, ignoring the topic assignments of individual tokens. In the context of human interaction, this means that models produce global topic-word distributions that typically make sense to users and serve to give a good high-level overview of the general themes and trends in the data. However, the local topic assignments can be bewildering. For example, Figure 1 shows typical topic assignments using LDA"
P19-1076,N15-1076,1,0.935489,"c, which we call consistency, should be adopted alongside global metrics such as topic coherence. 1 Introduction Topic models such as Latent Dirichlet Allocation (Blei et al., 2003, LDA) automatically discover topics in a collection of documents, giving users a glimpse into themes present in the documents. LDA jointly derives a set of topics (a distribution over words) and token-topic assignments (a distribution over the topics for each token). While the topics by themselves are valuable, the token-topic assignments are also useful as features for document classification (Ramage et al., 2009; Nguyen et al., 2015; Lund et al., 2018) and, in principle, for topic-based document segmentation. 788 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 788–796 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 A dance1 break1 by P.Diddy1 is also featured2 in both settings4 of the video2 , intercut1 with scenes2 of Mario3 Winans1 playing1 the drums1 . Music1 Film2 Gaming3 Prior work in automated metrics to evaluate topic model quality primarily deals with global evaluations (i.e. evaluations of the topic-word distributions th"
P19-1307,D18-1214,0,0.129085,"Missing"
P19-1307,P19-1489,1,0.893449,"Missing"
P19-1307,P19-1070,0,0.250817,"nal mappings can be improved with refinement steps (Artetxe et al., 2017; Conneau et al., 2018). After learning an initial mapping W0 from the seed dictionary D, we build a synthetic dictionary D1 by translating each word with W0 . We then use the new dictionary D1 to learn a new mapping W1 and repeat the process. 3 We only report accuracy for one run, because these CLWE methods are deterministic. 3183 Relaxed CSLS Loss (RCSLS). Joulin et al. (2018) optimize CSLS scores between translation pairs instead of Equation (1). RCSLS has state-ofthe-art supervised word translation accuracies on MUSE (Glavas et al., 2019). For the ease of optimization, RCSLS does not enforce the orthogonal constraint. Nevertheless, Iterative Normalization also improves its accuracy (Table 1), showing it can help linear non-orthogonal mappings too. 5.2 Training Details We use the implementation from MUSE for Procrustes analysis and refinement (Conneau et al., 2018). We use five refinement steps. For RCSLS, we use the same hyperparameter selection strategy as Joulin et al. (2018)—we choose learning rate from {1, 10, 25, 50} and number of epochs from {10, 20} by validation. As recommended by Joulin et al. (2018), we turn off the"
P19-1307,P15-1119,0,0.149864,"Missing"
P19-1307,D18-1043,0,0.20201,"Missing"
P19-1307,Q19-1007,0,0.103718,"Missing"
P19-1307,D18-1330,0,0.332381,"analysis (Schönemann, 1966) to find the orthogonal transformation that minimizes Equation 1, the total distance between translation pairs. Post-hoc Refinement. Orthogonal mappings can be improved with refinement steps (Artetxe et al., 2017; Conneau et al., 2018). After learning an initial mapping W0 from the seed dictionary D, we build a synthetic dictionary D1 by translating each word with W0 . We then use the new dictionary D1 to learn a new mapping W1 and repeat the process. 3 We only report accuracy for one run, because these CLWE methods are deterministic. 3183 Relaxed CSLS Loss (RCSLS). Joulin et al. (2018) optimize CSLS scores between translation pairs instead of Equation (1). RCSLS has state-ofthe-art supervised word translation accuracies on MUSE (Glavas et al., 2019). For the ease of optimization, RCSLS does not enforce the orthogonal constraint. Nevertheless, Iterative Normalization also improves its accuracy (Table 1), showing it can help linear non-orthogonal mappings too. 5.2 Training Details We use the implementation from MUSE for Procrustes analysis and refinement (Conneau et al., 2018). We use five refinement steps. For RCSLS, we use the same hyperparameter selection strategy as Jouli"
P19-1307,C12-1089,0,0.387047,"Missing"
P19-1307,D14-1162,0,0.0904738,"Missing"
P19-1307,N15-1104,0,\N,Missing
P19-1307,N16-1156,0,\N,Missing
P19-1307,Q17-1010,0,\N,Missing
P19-1307,D16-1250,0,\N,Missing
P19-1307,P17-1042,0,\N,Missing
P19-1307,D18-1027,0,\N,Missing
P19-1307,D18-1042,0,\N,Missing
P19-1489,W04-3230,0,\N,Missing
P19-1489,W02-1001,0,\N,Missing
P19-1489,D09-1092,0,\N,Missing
P19-1489,H93-1061,0,\N,Missing
P19-1489,D11-1024,0,\N,Missing
P19-1489,N10-1012,0,\N,Missing
P19-1489,W15-1806,0,\N,Missing
P19-1489,P15-1162,1,\N,Missing
P19-1489,E14-1056,0,\N,Missing
P19-1489,goldhahn-etal-2012-building,0,\N,Missing
P19-1489,C12-1089,0,\N,Missing
P19-1489,N15-1128,0,\N,Missing
P19-1489,N15-1104,0,\N,Missing
P19-1489,Q17-1010,0,\N,Missing
P19-1489,W16-1620,0,\N,Missing
P19-1489,P16-1024,0,\N,Missing
P19-1489,D16-1250,0,\N,Missing
P19-1489,L16-1521,0,\N,Missing
P19-1489,P17-1179,0,\N,Missing
P19-1489,P17-1042,0,\N,Missing
P19-1489,D18-1043,0,\N,Missing
P19-1489,E14-1049,0,\N,Missing
P19-1489,D15-1243,0,\N,Missing
P19-1489,2016.gwc-1.30,0,\N,Missing
P19-1554,D15-1075,0,0.101816,"seem, this kind of argument can be misleading—it is important to understand what exactly these results do and do not imply. A low accuracy from a partial-input baseline only means that the model failed to confirm a specific exploitable pattern in the part of the input that the model can see. This does not mean, however, that the dataset is free of artifacts—the full input might still contain very trivial patterns. To illustrate how the failures of partial-input baselines might shadow more trivial patterns that are only visible in the full input, we construct two variants of the SNLI dataset (Bowman et al., 2015). The datasets are constructed to contain trivial patterns that partial-input baselines cannot exploit, i.e., the patterns are only visible in the full input. As a result, a full-input can achieve perfect accuracy whereas partial-input models fail. 3.1 Label as Premise In SNLI, each example consists of a pair of sentences: a premise and a hypothesis. The goal is to classify the semantic relationship between the premise and the hypothesis—either entailment, neutral, or contradiction. Our first SNLI variant is an extreme example of artifacts that cannot be detected by a hypothesisonly baseline."
P19-1554,P16-1223,0,0.0536768,"Missing"
P19-1554,N19-1423,0,0.0700658,"Missing"
P19-1554,P18-2103,0,0.0578655,"cial patterns, the resulting dataset may still contain artifacts. In particular, a much stronger model (BERT) that sees the full-input easily solves the dataset. This demonstrates that using partial-input baselines as adversaries may lead to datasets that are just difficult enough to fool the baselines but not difficult enough to ensure that no model can cheat. Adversarial Evaluation Instead of validating a dataset, one can alternatively probe the model directly. For example, models can be stress tested using adversarial examples (Jia and Liang, 2017; Wallace et al., 2019) and challenge sets (Glockner et al., 2018; Naik et al., 2018). These tests can reveal strikingly simple model limitations, e.g., basic paraphrases can fool textual entailment and visual question answering systems (Iyyer et al., 2018; Ribeiro et al., 2018), while common typos drastically degrade neural machine translation quality (Belinkov and Bisk, 2018). Interpretations Another technique for probing models is to use interpretation methods. Interpretations, however, have a problem of faithfulness (Rudin, 2018): they approximate (often locally) a complex model with a simpler, interpretable model (often a linear model). Since interpret"
P19-1554,N18-2017,0,0.383257,"016; Jia and Liang, 2017; Goyal et al., 2017). One key method is to use partialinput baselines, i.e., models that intentionally ignore portions of the input. Example use cases include hypothesis-only models for natural language inference (Gururangan et al., 2018), question-only models for visual question answering (Goyal et al., 2017), and paragraph-only models for reading comprehension (Kaushik and Lipton, 2018). A successful partial-input baseline indicates that a dataset contains artifacts which make it easier than expected. On the other hand, examples where this baseline fails are “hard” (Gururangan et al., 2018), and the failure of partial-input baselines is considered a verdict of a dataset’s difficulty (Zellers et al., 2018; Kaushik and Lipton, 2018). These partial-input analyses are valuable and indeed reveal dataset issues; however, they do not tell the whole story. Just as being free of one ailment is not the same as a clean bill of health, a baseline’s failure only indicates that a dataset is not broken in one specific way. There is no reason that artifacts only infect part of the input—models can exploit patterns that are only visible in the full input. After reviewing partial-input baselines"
P19-1554,N18-1170,0,0.038692,"tial-input baselines as adversaries may lead to datasets that are just difficult enough to fool the baselines but not difficult enough to ensure that no model can cheat. Adversarial Evaluation Instead of validating a dataset, one can alternatively probe the model directly. For example, models can be stress tested using adversarial examples (Jia and Liang, 2017; Wallace et al., 2019) and challenge sets (Glockner et al., 2018; Naik et al., 2018). These tests can reveal strikingly simple model limitations, e.g., basic paraphrases can fool textual entailment and visual question answering systems (Iyyer et al., 2018; Ribeiro et al., 2018), while common typos drastically degrade neural machine translation quality (Belinkov and Bisk, 2018). Interpretations Another technique for probing models is to use interpretation methods. Interpretations, however, have a problem of faithfulness (Rudin, 2018): they approximate (often locally) a complex model with a simpler, interpretable model (often a linear model). Since interpretations are inherently an approximation, they can never be completely faithful—there are cases where the original model and the simple model behave differently (Ghorbani et al., 2019). These 5"
P19-1554,D17-1215,0,0.261383,"se artifacts corrupt the intention of the datasets to train and evaluate models for natural language understanding. Importantly, a human inspection of individual examples cannot catch artifacts because they are only visible in aggregate on the dataset level. However, machine learning algorithms, which detect and exploit recurring patterns in large datasets by design, can just as easily use artifacts as real linguistic clues. As a result, models trained on these datasets can achieve high test accuracy by exploiting artifacts but fail to generalize, e.g., they fail under adversarial evaluation (Jia and Liang, 2017; Ribeiro et al., 2018). Jordan Boyd-Graber Computer Science, iSchool, UMIACS , and LSC University of Maryland jbg@umiacs.umd.edu The identification of dataset artifacts has changed model evaluation and dataset construction (Chen et al., 2016; Jia and Liang, 2017; Goyal et al., 2017). One key method is to use partialinput baselines, i.e., models that intentionally ignore portions of the input. Example use cases include hypothesis-only models for natural language inference (Gururangan et al., 2018), question-only models for visual question answering (Goyal et al., 2017), and paragraph-only mode"
P19-1554,D18-1546,0,0.114529,"n Boyd-Graber Computer Science, iSchool, UMIACS , and LSC University of Maryland jbg@umiacs.umd.edu The identification of dataset artifacts has changed model evaluation and dataset construction (Chen et al., 2016; Jia and Liang, 2017; Goyal et al., 2017). One key method is to use partialinput baselines, i.e., models that intentionally ignore portions of the input. Example use cases include hypothesis-only models for natural language inference (Gururangan et al., 2018), question-only models for visual question answering (Goyal et al., 2017), and paragraph-only models for reading comprehension (Kaushik and Lipton, 2018). A successful partial-input baseline indicates that a dataset contains artifacts which make it easier than expected. On the other hand, examples where this baseline fails are “hard” (Gururangan et al., 2018), and the failure of partial-input baselines is considered a verdict of a dataset’s difficulty (Zellers et al., 2018; Kaushik and Lipton, 2018). These partial-input analyses are valuable and indeed reveal dataset issues; however, they do not tell the whole story. Just as being free of one ailment is not the same as a clean bill of health, a baseline’s failure only indicates that a dataset"
P19-1554,D18-1009,0,0.304885,"ionally ignore portions of the input. Example use cases include hypothesis-only models for natural language inference (Gururangan et al., 2018), question-only models for visual question answering (Goyal et al., 2017), and paragraph-only models for reading comprehension (Kaushik and Lipton, 2018). A successful partial-input baseline indicates that a dataset contains artifacts which make it easier than expected. On the other hand, examples where this baseline fails are “hard” (Gururangan et al., 2018), and the failure of partial-input baselines is considered a verdict of a dataset’s difficulty (Zellers et al., 2018; Kaushik and Lipton, 2018). These partial-input analyses are valuable and indeed reveal dataset issues; however, they do not tell the whole story. Just as being free of one ailment is not the same as a clean bill of health, a baseline’s failure only indicates that a dataset is not broken in one specific way. There is no reason that artifacts only infect part of the input—models can exploit patterns that are only visible in the full input. After reviewing partial-input baselines (Section 2), we construct variants of a natural language inference dataset to highlight the potential pitfalls of pa"
P19-1554,C18-1198,0,0.0945145,"lting dataset may still contain artifacts. In particular, a much stronger model (BERT) that sees the full-input easily solves the dataset. This demonstrates that using partial-input baselines as adversaries may lead to datasets that are just difficult enough to fool the baselines but not difficult enough to ensure that no model can cheat. Adversarial Evaluation Instead of validating a dataset, one can alternatively probe the model directly. For example, models can be stress tested using adversarial examples (Jia and Liang, 2017; Wallace et al., 2019) and challenge sets (Glockner et al., 2018; Naik et al., 2018). These tests can reveal strikingly simple model limitations, e.g., basic paraphrases can fool textual entailment and visual question answering systems (Iyyer et al., 2018; Ribeiro et al., 2018), while common typos drastically degrade neural machine translation quality (Belinkov and Bisk, 2018). Interpretations Another technique for probing models is to use interpretation methods. Interpretations, however, have a problem of faithfulness (Rudin, 2018): they approximate (often locally) a complex model with a simpler, interpretable model (often a linear model). Since interpretations are inherentl"
P19-1554,S18-2023,0,0.105697,"Missing"
P19-1554,P18-1079,0,0.184751,"the intention of the datasets to train and evaluate models for natural language understanding. Importantly, a human inspection of individual examples cannot catch artifacts because they are only visible in aggregate on the dataset level. However, machine learning algorithms, which detect and exploit recurring patterns in large datasets by design, can just as easily use artifacts as real linguistic clues. As a result, models trained on these datasets can achieve high test accuracy by exploiting artifacts but fail to generalize, e.g., they fail under adversarial evaluation (Jia and Liang, 2017; Ribeiro et al., 2018). Jordan Boyd-Graber Computer Science, iSchool, UMIACS , and LSC University of Maryland jbg@umiacs.umd.edu The identification of dataset artifacts has changed model evaluation and dataset construction (Chen et al., 2016; Jia and Liang, 2017; Goyal et al., 2017). One key method is to use partialinput baselines, i.e., models that intentionally ignore portions of the input. Example use cases include hypothesis-only models for natural language inference (Gururangan et al., 2018), question-only models for visual question answering (Goyal et al., 2017), and paragraph-only models for reading comprehe"
P19-1554,N19-1197,0,0.0176922,"f the input to use. 2. Reduce all examples in the training set and the test set. 3. Train a new model from scratch on the partialinput training set. 4. Test the model on the partial-input test set. High accuracy from a partial-input model implies the original dataset is solvable (to some extent) in the wrong ways, i.e., using unintended patterns. Partial-input baselines have identified artifacts in many datasets, e.g., SNLI (Gururangan et al., 2018; Poliak et al., 2018), VQA (Goyal et al., 2017), EmbodiedQA (Anand et al., 2018), visual dialogue (Massiceti et al., 2018), and visual navigation (Thomason et al., 2019). 3 How Partial-input Baselines Fail If a partial-input baseline fails, e.g., it gets close to chance accuracy, one might conclude that a dataset is difficult. For example, partial-input baselines are used to identify the “hard” examples in SNLI (Gururangan et al., 2018), verify that SQuAD is well constructed (Kaushik and Lipton, 2018), and that SWAG is challenging (Zellers et al., 2018). Reasonable as it might seem, this kind of argument can be misleading—it is important to understand what exactly these results do and do not imply. A low accuracy from a partial-input baseline only means that"
P19-1554,W18-5416,1,0.888171,"Missing"
P19-1554,Q19-1029,1,0.800071,"n be easily fooled if they rely on superficial patterns, the resulting dataset may still contain artifacts. In particular, a much stronger model (BERT) that sees the full-input easily solves the dataset. This demonstrates that using partial-input baselines as adversaries may lead to datasets that are just difficult enough to fool the baselines but not difficult enough to ensure that no model can cheat. Adversarial Evaluation Instead of validating a dataset, one can alternatively probe the model directly. For example, models can be stress tested using adversarial examples (Jia and Liang, 2017; Wallace et al., 2019) and challenge sets (Glockner et al., 2018; Naik et al., 2018). These tests can reveal strikingly simple model limitations, e.g., basic paraphrases can fool textual entailment and visual question answering systems (Iyyer et al., 2018; Ribeiro et al., 2018), while common typos drastically degrade neural machine translation quality (Belinkov and Bisk, 2018). Interpretations Another technique for probing models is to use interpretation methods. Interpretations, however, have a problem of faithfulness (Rudin, 2018): they approximate (often locally) a complex model with a simpler, interpretable mod"
Q14-1036,P12-2017,0,0.0252599,"Missing"
Q14-1036,N10-1081,1,0.0610669,"parametric counterparts (Yao et al., 2009). A common approach to address this computational bottleneck is through variational inference (Wainwright and Jordan, 2008). One of the advantages of variational inference is that it can be easily parallelized (Nallapati et al., 2007) or transformed into an online algorithm (Hoffman et al., 2010), which often converges in fewer iterations than batch variational inference. Past variational inference techniques for adaptor grammars assume a preprocessing step that looks at all available data to establish the support of these nonparametric distributions (Cohen et al., 2010). Thus, these past approaches are not directly amenable to online inference. Markov chain Monte Carlo (MCMC) inference, an alternative to variational inference, does not have this disadvantage. MCMC is easier to implement, and it discovers the support of nonparametric models during inference rather than assuming it a priori. We apply stochastic hybrid inference (Mimno et al., 2012) to adaptor grammars to get the best of both worlds. We interleave MCMC inference inside variational inference. This preserves the scalability of variational inference while adding the sparse statistics and improved"
Q14-1036,I05-3017,0,0.0375005,"unigram and collocation grammars introduced in Johnson and Goldwater (2009) as listed in Table 1. Figure 2 illustrates the word segmentation accuracy in terms of word token F1 -scores on brent against the number of inside-outside function calls for all three approaches using unigram and collocation grammars. In both cases, our ONLINE approach converges faster than MCMC and VARIATIONAL approaches, yet yields comparable or better performance when seeing more data. In addition to the brent corpus, we also evaluate three approaches on three other Chinese datasets compiled by Xue et al. (2005) and Emerson (2005):8 • Chinese Treebank 7.0 (ctb7): 162k sentences, 57k distinct words, 4.5k distinct characters; our model under κ = {0.7, 0.9} and τ = {64, 256}. 8 We use all punctuation as natural delimiters (i.e., words cannot cross punctuation). 9 Their results are not directly comparable: they use different subsets and assume different preprocessing. 10 Note that this is only an approximation to the true held-out likelihood, since it is impossible to enumerate all the possible parse trees and hence compute the likelihood for a given sentence under the model. 11 We train all models with 5 topics with setti"
Q14-1036,D10-1028,1,0.82192,"former German chancellor) first appeared in minibatch 300, was successfully picked up by our model, and became one of the top ranked words in the topic. 6 Conclusion Probabilistic modeling is a useful tool in understanding unstructured data or data where the structure is latent, like language. However, developing these models is often a difficult process, requiring significant machine learning expertise. Adaptor grammars offer a flexible and quick way to prototype and test new models. Despite expensive inference, they have been used for topic modeling (Johnson, 2010), discovering perspective (Hardisty et al., 2010), segmentation (Johnson and Goldwater, 2009), and grammar induction (Cohen et al., 2010). We have presented a new online, hybrid inference scheme for adaptor grammars. Unlike previous approaches, it does not require extensive preprocessing. It is also able to faster discover useful structure in text; with further development, these algorithms could further speed the development and application of new nonparametric models to large datasets. Acknowledgments We would like to thank the anonymous reviewers, Kristina Toutanova, Mark Johnson, and Ke Wu for insightful discussions. This work was suppor"
Q14-1036,N09-1036,0,0.37781,"s (Mochihashi et al., 2009). We impose a reward term for longer phrases in addition to f˜ and sort all adapted productions in TNGa using the ranking score Λ(a ⇒ za,i ) = f˜(l) (a ⇒ za,i ) · log( · |s |+ 1), where |s |is the number of yields in production a ⇒ za,i . Because  decreases each minibatch, the reward for long phrases diminishes. This is similar to an annealed version of Cohen et al. (2010)—where the reward for long phrases is fixed, see also Mochihashi et al. (2009). After sorting, we remove all but the top Ka adapted productions. Rederiving Adapted Productions For MCMC inference, Johnson and Goldwater (2009) observe that atoms already associated with a yield may have trees that do not explain their yield well. They propose table label resampling to rederive yields. In our approach this is equivalent to “mutating” some derivations in a TNG . After pruning rules every u minibatches, we perform table label resampling for adapted nonterminals from general to specific (i.e., a topological sort). This provides better expected counts n(r, •) for rules used in phrasestructure subtrees. Empirically, we find table label resampling only marginally improves the wordsegmentation result. Initialization Our inf"
Q14-1036,N07-1018,0,0.0145467,"iational distribution over trees φ using the empirical distribution σ d , i.e., φd,i ∝ I[σd,j = td,i , ∀σd,j ∈ σ d ]. (5) This leads to a sparse approximation of variational distribution φ.3 Previous inference strategies (Johnson et al., 2006; B¨orschinger and Johnson, 2012) for adaptor grammars have used sampling. The adaptor grammar inference methods use an approximate PCFG to emulate the marginalized Pitman-Yor distributions 3 In our experiments, we use ten samples. 468 at each nonterminal. Given this approximate PCFG, we can then sample a derivation z for string x from the possible trees (Johnson et al., 2007). Sampling requires a derived PCFG G 0 that approximates the distribution over tree derivations conditioned on a yield. It includes the original PCFG rules R = {c → β} that define the base distribution and the new adapted productions R0 = {c ⇒ z, z ∈ TNG c }. Under G 0 , the probability θ 0 of adapted production c ⇒ z is   Eq [log πc,i ], if TNGc (i) = z 0 log θc⇒z = Eq [log πc,Kc ] + Eq [log θc⇒z ], (6)   otherwise where Kc is the truncation level of TNGc and πc,Kc represents the left-over stick weights in the stickbreaking process for adaptor c ∈ M . θc⇒z represents the probability of g"
Q14-1036,P10-1117,0,0.388833,"ximation to the true held-out likelihood, since it is impossible to enumerate all the possible parse trees and hence compute the likelihood for a given sentence under the model. 11 We train all models with 5 topics with settings: TNG refinement interval u = 100, truncation size KTopic = 3k, and the mini-batch size B = 50. We observe a similar behavior under κ ∈ {0.7, 0.9} and τ ∈ {64, 256}. 473 ⌧ : 512 512 128 128  pmi coherence pmi 1 1 1 10 0 10 101 0 10 0000 00 1 1 10 10 10 10 0 10 100 0000 1 1 10 10 10 1 10 1000 0000 0 1 Topic models often can be replicated using a carefully crafted PCFG (Johnson, 2010). These powerful extensions can capture topical collocations and sticky topics; these embelishments could further improve NLP applications of simple unigram topic models such as word sense disambiguation (Boyd-Graber and Blei, 2007), part of speech ⌧: 3232 1 5.2 Infinite Vocabulary Topic Modeling ⌧: 0.6 0.8 We compare our inference method against other approaches on F1 score. While other unsupervised word segmentation systems are available (Mochihashi et al. (2009), inter alia),9 our focus is on a direct comparison of inference techniques for adaptor grammar, which achieve competitive (if not"
Q14-1036,P09-1012,0,0.0240337,"ucts and expands all TNG s on the fly. To prevent the TNG from growing unwieldy, we prune TNG after every u minibatches. As a result, we need to impose an ordering over all the parse trees in the TNG . The underlying PYGEM distribution implicitly places an ranking over all the atoms according to their corresponding sufficient statistics (Kurihara et al., 2007), as shown in Equation 9. It measures the “usefulness” of every adapted production throughout inference process. In addition to accumulated sufficient statistics, Cohen et al. (2010) add a secondary term to discourage short constituents (Mochihashi et al., 2009). We impose a reward term for longer phrases in addition to f˜ and sort all adapted productions in TNGa using the ranking score Λ(a ⇒ za,i ) = f˜(l) (a ⇒ za,i ) · log( · |s |+ 1), where |s |is the number of yields in production a ⇒ za,i . Because  decreases each minibatch, the reward for long phrases diminishes. This is similar to an annealed version of Cohen et al. (2010)—where the reward for long phrases is fixed, see also Mochihashi et al. (2009). After sorting, we remove all but the top Ka adapted productions. Rederiving Adapted Productions For MCMC inference, Johnson and Goldwater (2009"
Q14-1036,P12-1046,0,0.0132947,"unsupervised models of grammar productions. This flexibility comes at the cost of expensive inference. We address the difficulty of inference through an online algorithm which uses a hybrid of Markov chain Monte Carlo and variational inference. We show that this inference strategy improves scalability without sacrificing performance on unsupervised word segmentation and topic modeling tasks. 1 Introduction Nonparametric Bayesian models are effective tools to discover latent structure in data (M¨uller and Quintana, 2004). These models have had great success in text analysis, especially syntax (Shindo et al., 2012). Nonparametric distributions provide support over a countably infinite long-tailed distributions common in natural language (Goldwater et al., 2011). We focus on adaptor grammars (Johnson et al., 2006), syntactic nonparametric models based on probabilistic context-free grammars. Adaptor grammars weaken the strong statistical independence assumptions PCFGs make (Section 2). The weaker statistical independence assumptions that adaptor grammars make come at the cost of expensive inference. Adaptor grammars are not alone in this trade-off. For example, nonparametric extensions of topic models (Te"
Q14-1036,P14-1004,1,\N,Missing
Q17-1001,P14-2103,0,0.39814,", conditions}—the algorithm generated the label health care in the united states, but users preferred the less specific labels health and medical research. Similarly, for Topic 48—{league, team, baseball, players, contract}—the algorithm generated the label major league baseball on fox; users preferred simpler labels, such as baseball. Automatic labeling algorithms thus can be improved to focus on general, shorter labels. Interestingly, simple textual labels have been shown to be more efficient but less effective than topic keywords (i.e., word lists) for an automatic document retrieval task (Aletras and Stevenson, 2014), highlighting the extra information present in the word lists. Our findings show that users are also able to effectively interpret the word list information, as that visualization was both efficient and effective for the task of topic labeling compared to the other more complex visualizations. Although we use WordNet to verify that users prefer more general labels, this is not a panacea, because WordNet does not capture all of the generalization users want in labels. In many cases, users use terms that synthesize relationships beyond trivial WordNet relationships, such as locations or entitie"
Q17-1001,A00-2018,0,0.0796342,"apply automatic labeling and by providing training data for improving automatic labeling. While automatic labels falter compared to human labels in general, they do quite well when the underlying topics are of high quality. Thus, one reasonable strategy would be to use automatic labels for a portion of topics, but to use human validation to either first improve the remainder of the topics (Hu et al., 2014) or to provide labels (as in this study) for lower quality topics. Moreover, our labels provide training data that may be useful for automatic labeling techniques using featurebased models (Charniak, 2000)—combining information from Wikipedia, WordNet, syntax, and the underlying topics—to reproduce the types of labels and sentences created (and favored) by users. Finally, our study focuses on comparing individual topic visualization techniques. An open question that we do not address is whether this generalizes to understanding entire topic models. In other words, simple word list visualizations are useful for quick and high-quality topic summarization, but does this mean that a collection of word lists— one per topic—will also be optimal when displaying the entire model? Future work should loo"
Q17-1001,C10-2069,0,0.0254893,"and the overall model. Most existing topic presentations use simple word lists (Chaney and Blei, 2012; Eisenstein et al., 2012). Although a variety of alternative topic visualization techniques exist (Sievert and Shirley, 2014; Yi et al., 2005), there has been no systematic assessment to compare them. Beyond exploring different visualization techniques, another means of making topics easier for users to understand is to provide descriptive labels to complement a topic’s set of words (Aletras et al., 2014). Unfortunately, manual labeling is slow and, while automatic labeling approaches exist (Lau et al., 2010; Mei et al., 2007; Lau et al., 2011), their effectiveness is not guaranteed for all tasks. Comprehensible Topic Models Needed A central challenge of the “big data” era is to help users make sense of large text collections (Hotho et al., 2005). A common approach to summarizing the main themes in a corpus is to use topic models (Blei, 2012), which are data-driven statistical models that To better understand these problems, we use labeling to evaluate topic model visualizations. Our study compares the impact of four commonly used topic visualization techniques on the labels that users create whe"
Q17-1001,P11-1154,0,0.788953,"g topic presentations use simple word lists (Chaney and Blei, 2012; Eisenstein et al., 2012). Although a variety of alternative topic visualization techniques exist (Sievert and Shirley, 2014; Yi et al., 2005), there has been no systematic assessment to compare them. Beyond exploring different visualization techniques, another means of making topics easier for users to understand is to provide descriptive labels to complement a topic’s set of words (Aletras et al., 2014). Unfortunately, manual labeling is slow and, while automatic labeling approaches exist (Lau et al., 2010; Mei et al., 2007; Lau et al., 2011), their effectiveness is not guaranteed for all tasks. Comprehensible Topic Models Needed A central challenge of the “big data” era is to help users make sense of large text collections (Hotho et al., 2005). A common approach to summarizing the main themes in a corpus is to use topic models (Blei, 2012), which are data-driven statistical models that To better understand these problems, we use labeling to evaluate topic model visualizations. Our study compares the impact of four commonly used topic visualization techniques on the labels that users create when interpreting a topic (Figure 1): wo"
Q17-1001,E14-1056,0,0.0664323,"nerated labels and sentences: the user labels for the topic shown in Figure 1 include government, iraq war, politics, bush administration, and war on terror. Examples of sentences include “President Bush’s military plan in Iraq” and “World news involving the US president and Iraq”.7 To interpret the results, it is useful to also understand the quality of the generated topics, which varies throughout the model and may impact a user’s ability to generate good labels. We measure topic quality using topic coherence, an automatic measure that correlates with how much sense a topic makes to a user (Lau et al., 2014).8 The average topic coherence for the model is 0.09 (SD = 0.05). Figure 4 shows the three best (top) and three worst topics (bottom) according to their observed coherence: the coherence metric distinguishes obvious topics from inscrutable ones. Section 4.3 shows that users cre7 The complete set of labels and sentences are available at https://github.com/alisonmsmith/Papers/ tree/master/TopicRepresentations. 8 We use a reference corpus of 23 million Wikipedia articles for computing normalized pointwise mutual information needed for computing the observed coherence. Technique Cardinality # task"
Q17-1001,N10-1012,0,0.127735,"ck to the original documents to support directed exploration. The topic distributions can also be used to present other documents related to a given document. Clustering is hard because there are multiple reasonable objectives that are impossible to satisfy simultaneously (Kleinberg, 2003). Topic modeling evaluation has focused on perplexity, which measures how well a model can predict words in unseen documents (Wallach et al., 2009b; Jelinek et al., 1977). However, Chang et al. (2009) argue that evaluations optimizing for perplexity encourage complexity at the cost of human interpretability. Newman et al. (2010a) build on this insight, noting that “one indicator of usefulness is the ease by which one could think of a short label to describe the topic.” Unlike previous interpretability studies, here we examine the connection between a topic’s visual representation (not just its content) and its interpretability. Recent work has focused on automatic generation of labels for topics. Lau et al. (2011) use Wikipedia articles to automatically label topics. The assumption is that for each topic there will be a Wikipedia article title that offers a good representation of the topic. Aletras et al. (2014) use"
Q17-1001,N13-3009,1,0.908077,"Rodrigues et al., 2011, GIB). The individual 3 topics are displayed using a network graph visualization, and related topics are displayed within a treemap (Shneiderman, 1992) layout. The result is a visualization where related words cluster within topics and related topics cluster in the overall layout. TopicFlow (Smith et al., 2015) visualizes how a model changes over time using a Sankey diagram (Riehmann et al., 2005). The individual topics are represented both as word lists in the model overview and as word list with bars when viewing a single topic or comparing between two topics. Argviz (Nguyen et al., 2013) captures temporal shifts in topics during a debate or a conversation. The individual topics are presented as word lists in the model overview and using word list with bars for the selected topics. Klein et al. (2015) use a dustand-magnet visualization (Yi et al., 2005) to visualize the force of topics on newspaper issues. The temporal trajectories of several newspapers are displayed as dust trails in the visualization. The individual topics are displayed as word clouds. In contrast to these visualizations which support viewing the underlying topics on demand, Termite (Chuang et al., 2012) use"
Q17-1001,W14-3110,0,0.177863,"ger but expose multi-word expressions that simpler visualizations obscure. Automatic labels lag behind user-created labels, but our dataset of manually labeled topics highlights linguistic patterns (e.g., hypernyms, phrases) that can be used to improve automatic topic labeling algorithms. 1 A key challenge for topic modeling, however, is how to promote end-user understanding of individual topics and the overall model. Most existing topic presentations use simple word lists (Chaney and Blei, 2012; Eisenstein et al., 2012). Although a variety of alternative topic visualization techniques exist (Sievert and Shirley, 2014; Yi et al., 2005), there has been no systematic assessment to compare them. Beyond exploring different visualization techniques, another means of making topics easier for users to understand is to provide descriptive labels to complement a topic’s set of words (Aletras et al., 2014). Unfortunately, manual labeling is slow and, while automatic labeling approaches exist (Lau et al., 2010; Mei et al., 2007; Lau et al., 2011), their effectiveness is not guaranteed for all tasks. Comprehensible Topic Models Needed A central challenge of the “big data” era is to help users make sense of large text"
Q17-1001,W14-3112,1,0.910049,"Other tools provide additional information within topic model overviews, such as the relationship between topics or temporal changes in the model. However, they still require the user to understand individual topics. LDAVis (Sievert and Shirley, 2014) includes information about the relationship between topics in the model. Multi-dimensional scaling projects the model’s topics as circles onto a two-dimensional plane based on their inter-topic distances; the circles are sized by their overall prevalence. The individual topics, however, are then visualized on demand using a word list with bars. Smith et al. (2014) visualize a topic model using a nested network graph layout called group-in-abox (Rodrigues et al., 2011, GIB). The individual 3 topics are displayed using a network graph visualization, and related topics are displayed within a treemap (Shneiderman, 1992) layout. The result is a visualization where related words cluster within topics and related topics cluster in the overall layout. TopicFlow (Smith et al., 2015) visualizes how a model changes over time using a Sankey diagram (Riehmann et al., 2005). The individual topics are represented both as word lists in the model overview and as word l"
Q17-1001,P11-1026,1,\N,Missing
Q19-1029,P16-1223,0,0.0844846,"Missing"
Q19-1029,D14-1179,0,0.017362,"Missing"
Q19-1029,N18-2017,0,0.05148,"ti-hop reasoning to entity type distractors, exposing open challenges in robust question answering. 1 Introduction Proponents of machine learning claim human parity on tasks like reading comprehension (Yu et al., 2018) and commonsense inference (Devlin et al., 2018). Despite these successes, many evaluations neglect that computers solve natural language processing (NLP) tasks in a fundamentally different way than humans. Models can succeed without developing ‘‘true’’ language understanding, instead learning superficial patterns from crawled (Chen et al., 2016) or manually annotated data sets (Gururangan et al., 2018; Kaushik and Lipton, 2018). Thus, 387 Transactions of the Association for Computational Linguistics, vol. 7, pp. 387–401, 2019. https://doi.org/10.1162/tacl a 00279. Action Editor: Marco Baroni. Submission batch: 2/2019; Revision batch: 4/2019; Published 7/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  Figure 1: Adversarial evaluation in NLP typically focuses on a specific phenomenon (e.g., word replacements) and then generates the corresponding examples (top). Consequently, adversarial examples are limited to the diversity of what the underly"
Q19-1029,N19-1246,0,0.042705,"ts. Multiple authors agree that identifying oft-repeated ‘‘stock’’ clues was the interface’s most useful feature. As one author explained, ‘‘There were clues which I did not think were stock clues but were later revealed to be.’’ In particular, the author’s question about the 6 https://www.qbwiki.com/wiki/Jordan Brownstein. 396 a pool of talented authors who (2) write text with specific goals. Future research can look to craft adversarially authored data sets for other NLP tasks that meet these criteria. Other work puts an adversary in the data annotation or postprocessing loop. For instance, Dua et al. (2019) and Zhang et al. (2018) filter out easy questions using a baseline QA model, and Zellers et al. (2018) use stylistic classifiers to filter language inference examples. Rather than filtering out easy questions, we use human adversaries to generate hard ones. Similar to our work, Ettinger et al. (2017) use human adversaries. We extend their setting by providing humans with model interpretations to facilitate adversarial writing. Moreover, we have a readymade audience of question writers to generate adversarial questions. The collaborative adversarial writing process reflects the complementary a"
Q19-1029,N16-1111,1,0.891778,"Missing"
Q19-1029,P18-2006,0,0.133868,"es (Szegedy et al., 2013) often reveal model failures better than traditional test sets. However, automatic adversarial generation is tricky for NLP (e.g., by replacing words) without changing an example’s meaning or invalidating it. Recent work sidesteps this by focusing on simple transformations that preserve meaning. For instance, Ribeiro et al. (2018) generate adversarial perturbations such as replacing What has → What’s. Other minor perturbations such as typos (Belinkov and Bisk, 2018), adding distractor sentences (Jia and Liang, 2017; Mudrakarta et al., 2018), or character replacements (Ebrahimi et al., 2018) preserve meaning while degrading model performance. Generative models can discover more adversarial perturbations but require post hoc human verification of the examples. For example, neural paraphrase or language models can generate syntax modifications (Iyyer et al., 2018), plausible captions (Zellers et al., 2018), or NLI premises (Zhao et al., 2018). These methods improve examplelevel diversity but mainly target a specific phenomenon, (e.g., rewriting question syntax). Furthermore, existing adversarial perturbations are restricted to sentences—not the paragraph inputs of Quizbowl and othe"
Q19-1029,W17-5401,0,0.0615611,"Missing"
Q19-1029,P15-1162,1,0.886283,"Missing"
Q19-1029,N18-1170,0,0.0341504,"zbowl tournaments (Jennings, 2006). We award prizes for questions read at live human–computer matches (Section 5.3). The question authors are familiar with the standard format of Quizbowl questions (Lujan and Teitler, 2003). The questions follow a common paragraph structure, are well edited for grammar, and finish with a simple ‘‘give-away’’ clue. These constraints benefit the adversarial writing process as it is very clear what constitutes a difficult but valid question. Thus, our examples go beyond surface level ‘‘breaks’’ such as character noise (Belinkov and Bisk, 2018) or syntax changes (Iyyer et al., 2018). Rather, questions are difficult because of their semantic content (examples in Section 6). The authors interact with either the IR or RNN model through a user interface1 (Figure 3). An author writes their question in the upper right and the model’s top five predictions (Machine Guesses) appear in the upper left. If the top prediction is the right answer, the interface indicates where in the question the model is first correct. The goal is to cause the model to be incorrect or to delay the correct answer position as much as possible.2 The words of the current question are highlighted using th"
Q19-1029,P18-2103,0,0.0209214,"model. The author gives up and submits their relatively non-adversarial question. 7.1 8 Related Work New data sets often allow for a finer-grained analysis of a linguistic phenomenon, task, or genre. The LAMBADA data set (Paperno et al., 2016) tests a model’s understanding of the broad contexts present in book passages, whereas the Natural Questions corpus (Kwiatkowski et al., 2019) combs Wikipedia for answers to questions that users trust search engines to answer (OeldorfHirsch et al., 2014). Other work focuses on natural language inference, where challenge examples highlight model failures (Glockner et al., 2018; Naik et al., 2018; Wang et al., 2019). Our work is unique in that we use human adversaries to expose model weaknesses, which provides a diverse set of phenomena (from paraphrases to multi-hop reasoning) that models cannot solve. Interviews With Adversarial Authors We also interview the adversarial authors who attended our live events. Multiple authors agree that identifying oft-repeated ‘‘stock’’ clues was the interface’s most useful feature. As one author explained, ‘‘There were clues which I did not think were stock clues but were later revealed to be.’’ In particular, the author’s questio"
Q19-1029,D17-1215,0,0.0636841,"Missing"
Q19-1029,D18-1546,0,0.0575845,"y type distractors, exposing open challenges in robust question answering. 1 Introduction Proponents of machine learning claim human parity on tasks like reading comprehension (Yu et al., 2018) and commonsense inference (Devlin et al., 2018). Despite these successes, many evaluations neglect that computers solve natural language processing (NLP) tasks in a fundamentally different way than humans. Models can succeed without developing ‘‘true’’ language understanding, instead learning superficial patterns from crawled (Chen et al., 2016) or manually annotated data sets (Gururangan et al., 2018; Kaushik and Lipton, 2018). Thus, 387 Transactions of the Association for Computational Linguistics, vol. 7, pp. 387–401, 2019. https://doi.org/10.1162/tacl a 00279. Action Editor: Marco Baroni. Submission batch: 2/2019; Revision batch: 4/2019; Published 7/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  Figure 1: Adversarial evaluation in NLP typically focuses on a specific phenomenon (e.g., word replacements) and then generates the corresponding examples (top). Consequently, adversarial examples are limited to the diversity of what the underlying generative model or per"
Q19-1029,P16-1144,0,0.0616878,"Missing"
Q19-1029,Q19-1026,0,0.0244527,"lways this brittle. In Figure C.1, the interpretation fails to aid an adversarial attack against the RNN model. At each step, the author uses the highlighted words as a guide to edit targeted portions of the question yet fails to trick the model. The author gives up and submits their relatively non-adversarial question. 7.1 8 Related Work New data sets often allow for a finer-grained analysis of a linguistic phenomenon, task, or genre. The LAMBADA data set (Paperno et al., 2016) tests a model’s understanding of the broad contexts present in book passages, whereas the Natural Questions corpus (Kwiatkowski et al., 2019) combs Wikipedia for answers to questions that users trust search engines to answer (OeldorfHirsch et al., 2014). Other work focuses on natural language inference, where challenge examples highlight model failures (Glockner et al., 2018; Naik et al., 2018; Wang et al., 2019). Our work is unique in that we use human adversaries to expose model weaknesses, which provides a diverse set of phenomena (from paraphrases to multi-hop reasoning) that models cannot solve. Interviews With Adversarial Authors We also interview the adversarial authors who attended our live events. Multiple authors agree th"
Q19-1029,D14-1162,0,0.0825656,"em but are still answerable by humans. We expose model predictions and 388 retrieval (IR) system. The IR model is the baseline from a NIPS 2017 shared task on Quizbowl (BoydGraber et al., 2018) based on ElasticSearch (Gormley and Tong, 2015). In the second round, authors attack either the IR model or a neural QA model. The neural model is a bidirectional recurrent neural network (RNN) using the gated recurrent unit architecture (Cho et al., 2014). The model treats Quizbowl as classification and predicts the answer entity from a sequence of words represented as 300dimensional GloVe embeddings (Pennington et al., 2014). Both models in this round are trained using an expanded data set of approximately 110,000 Quizbowl questions. We expanded the second round data set to incorporate more diverse answers (25,000 entities vs. 11,000 in round one). Figure 2: An example Quizbowl question. The question becomes progressively easier (for humans) to answer later on; thus, more knowledgeable players can answer after hearing fewer clues. Our adversarial writing process ensures that the clues also challenge computers. The answers to Quizbowl questions are typically well-known entities. In the QA community (Hirschman and"
Q19-1029,N16-3020,0,0.0289062,"knowledgeable players can answer after hearing fewer clues. Our adversarial writing process ensures that the clues also challenge computers. The answers to Quizbowl questions are typically well-known entities. In the QA community (Hirschman and Gaizauskas, 2001), this is called ‘‘factoid’’ QA: The entities come from a relatively closed set of possible answers. 3.1 3.3 To help write adversarial questions, we expose what the model is thinking to the authors. We interpret models using saliency heat maps: Each word of the question is highlighted based on its importance to the model’s prediction (Ribeiro et al., 2016). For the neural model, word importance is the decrease in prediction probability when a word is removed (Li et al., 2016; Wallace et al., 2018). We focus on gradient-based approximations (Simonyan et al., 2014; Montavon et al., 2018) for their computational efficiency. To interpret a model prediction on an input sequence of n words w = w1 , w2 , . . . wn , we approximate the classifier f with a linear function of wi derived from the first-order Taylor expansion. The importance of wi , with embedding v i , is the derivative of f with respect to the one-hot vector: Known Exploits of Quizbowl"
Q19-1029,P18-1079,0,0.0961186,"Missing"
Q19-1029,C18-1198,0,0.0270935,"up and submits their relatively non-adversarial question. 7.1 8 Related Work New data sets often allow for a finer-grained analysis of a linguistic phenomenon, task, or genre. The LAMBADA data set (Paperno et al., 2016) tests a model’s understanding of the broad contexts present in book passages, whereas the Natural Questions corpus (Kwiatkowski et al., 2019) combs Wikipedia for answers to questions that users trust search engines to answer (OeldorfHirsch et al., 2014). Other work focuses on natural language inference, where challenge examples highlight model failures (Glockner et al., 2018; Naik et al., 2018; Wang et al., 2019). Our work is unique in that we use human adversaries to expose model weaknesses, which provides a diverse set of phenomena (from paraphrases to multi-hop reasoning) that models cannot solve. Interviews With Adversarial Authors We also interview the adversarial authors who attended our live events. Multiple authors agree that identifying oft-repeated ‘‘stock’’ clues was the interface’s most useful feature. As one author explained, ‘‘There were clues which I did not think were stock clues but were later revealed to be.’’ In particular, the author’s question about the 6 https"
Q19-1029,W18-5416,1,0.940385,"e answers to Quizbowl questions are typically well-known entities. In the QA community (Hirschman and Gaizauskas, 2001), this is called ‘‘factoid’’ QA: The entities come from a relatively closed set of possible answers. 3.1 3.3 To help write adversarial questions, we expose what the model is thinking to the authors. We interpret models using saliency heat maps: Each word of the question is highlighted based on its importance to the model’s prediction (Ribeiro et al., 2016). For the neural model, word importance is the decrease in prediction probability when a word is removed (Li et al., 2016; Wallace et al., 2018). We focus on gradient-based approximations (Simonyan et al., 2014; Montavon et al., 2018) for their computational efficiency. To interpret a model prediction on an input sequence of n words w = w1 , w2 , . . . wn , we approximate the classifier f with a linear function of wi derived from the first-order Taylor expansion. The importance of wi , with embedding v i , is the derivative of f with respect to the one-hot vector: Known Exploits of Quizbowl Questions Like most QA data sets, Quizbowl questions are written for humans. Unfortunately, the heuristics that question authors use to select c"
Q19-1029,D18-1316,0,0.0417842,"Missing"
S07-1060,J04-3004,0,0.0277589,"wsi ) = Generative Model of WSD X Word Sense Disambiguation (WSD) is the problem of labeling text with the appropriate semantic labels automatically. Although WSD is claimed to be an essential step in information retrieval and machine translation, it has not seen effective practical application because the dearth of labeled data has prevented the use of established supervised statistical methods that have been successfully applied to other natural language problems. Unsupervised methods have been developed for WSD, but despite modest success have not always been well understood statistically (Abney, 2004). Unsupervised methods are particularly appealing because they do not require expensive senseannotated data and can use the ever-increasing amount of raw text freely available. This paper expands on an effective unsupervised method for WSD and embeds it into a topic model, thus allowing an algorithm trained on a single, monolithic corpora to instead hand-pick relevant documents in choosing nj ∈Nw wnss(wsi , nj ) , wsj ∈Sw wnss(wsj , nj ) γ(w, nj ) P (1) where wnss(s, c) = max α(a, s). a∈Sc (2) One can view finding the appropriate sense as a search in two types of space. In determining how good"
S07-1060,O97-1002,0,0.0399842,"Missing"
S07-1060,P04-1036,0,0.188096,"McCarthy et al.’s predominant sense method to create an unsupervised method of word sense disambiguation that uses automatically derived topics using Latent Dirichlet allocation. Using topicspecific synset similarity measures, we create predictions for each word in each document using only word frequency information. It is hoped that this procedure can improve upon the method for larger numbers of topics by providing more relevant training corpora for the individual topics. This method is evaluated on SemEval-2007 Task 1 and Task 17. 1 1.1 The Intersection of Syntactic and Semantic Similarity McCarthy et al. (2004) outlined a method for learning a word’s most-used sense given an untagged corpus that ranks each sense wsi using a distributional syntactic similarity γ and a W ORD N ET-derived semantic similarity α. This process for a word w uses its distributional neighbors Nw , the possible senses of not only the word in question, Sw , and also those of the distributionally similar words, Snj . Thus, P (wsi ) = Generative Model of WSD X Word Sense Disambiguation (WSD) is the problem of labeling text with the appropriate semantic labels automatically. Although WSD is claimed to be an essential step in info"
S07-1060,H93-1061,0,0.137209,"of the terms like “foul ball,” “pop fly,” and “grounder” and the other sense in the contexts where insect has a higher probability in the topic. 2 Nouns .467 .478 .456 .420 Task Topic Expansion Document Expansion English Translation SensEval 2 SensEval 3 PUTOP 0.30 0.15 0.17 0.39 0.33 Table 2: Performance results on Task 1 SemCor As an initial evaluation, we learned LDA topics on the British National corpus with paragraphs as the underlying “document” (this allowed for a more uniform document length). These documents were then used to infer topic probabilities for each of the words in SemCor (Miller et al., 1993), and the model described in the previous section was run to determine the most likely synset. The results of this procedure are shown in Table 1. Accuracy is determined as the percentage of words for which the most likely sense was the one tagged in the corpus. While the method does roughly recreate McCarthy et al.’s result for a single topic, it only offers a one percent improvement over McCarthy et al. on five topics and then falls below McCarthy for all greater numbers of topics tried. Thus, for all subsequent experiments we used a five topic model trained on the BNC. 2.2 All .393 .397 .38"
S07-1060,S07-1001,0,\N,Missing
S07-1060,S07-1016,0,\N,Missing
S16-2012,P06-1117,0,0.380816,"Missing"
S16-2012,P14-1097,1,0.452225,"nguistic intuition used in its construction. Its great strength is also its downfall: adding new verbs, new senses, and new classes requires trained linguists - at least, to preserve the integrity of the resource. According to Levin’s hypothesis, knowing the set of allowable syntactic patterns for a verb sense is sufficient to make meaningful semantic classifications. Large-scale corpora provide an extremely comprehensive picture of the possible syntactic realizations for any particular verb. With enough data in the training set, even infrequent verbs have sufficient data to support learning. Kawahara et al. (2014) showed that, using a Dirichlet Process Mixture Model (DPMM), a VerbNet-like clustering of verb senses can be built from counts of syntactic features. We develop a model to extend VerbNet, using a large corpus with machine-annotated dependencies. We build on prior work by adding partial supervision from VerbNet, treating VerbNet classes as additional latent variables. The resulting clusters are more similar to the evaluation set, and each cluster in the DPMM predicts its VerbNet class distribution naturally. Because the technique is data-driven, it is easily adaptable to domainspecific corpora"
S16-2012,P03-1009,0,0.14761,"Missing"
S16-2012,P09-1033,0,0.0585909,"Missing"
S16-2012,D09-1026,0,0.0381471,"the verb sense induction and the clustering of verb senses, the features can be optimized for the distinct tasks. According to (Kawahara et al., 2014), the best features for inducing verb classes are joint slot:token pairs. For the verb clustering task, slot features which ignore the lexical items were the most effective. This aligns with Levin’s hypothesis of diathesis alternations - the syntactic contexts are sufficient for the clustering. In this paper, we re-create the second stage clustering with the same features, but add supervision. Supervised Topic Modeling (Mimno and McCallum, 2008; Ramage et al., 2009) builds on the Bayesian framework by adding, for each item, a Dirichlet Process Mixture Models The DPMM used in Kawahara et al. (2014) is shown in Figure 1. The clusters are drawn from a Dirichlet Process with hyperparameter α and base distribution G. The Dirichlet process prior creates a clustering effect described by the Chinese Restaurant Process. Each cluster is chosen proportionally to the number of elements it already 103 prediction about a variable of interest, which is observed at least some of the time. This encourages the topics to be useful at predicting a supervised signal, as well"
S16-2012,N07-1069,1,0.770948,"Missing"
S16-2012,N06-5006,0,\N,Missing
W10-0730,N09-1057,1,0.929182,"e) and captures how much “action” takes place in a sentence. Such notions of Transitivity are not apparent from surface features alone; identical syntactic constructions can have vastly different Transitivity. This well-established linguistic theory, however, is not useful for real-world applications without a Transitivity-annotated corpus. Given such a substantive corpus, conventional machine learning techniques could help determine the Transitivity of verbs within sentences. Transitivity has been found to play a role in what is called “syntactic framing,” which expresses implicit sentiment (Greene and Resnik, 2009). 1 We use capital “T” to differentiate from conventional syntactic transitivity throughout the paper. Table 1 shows the subset of the Hopper-Thompson dimensions of Transitivity used in this study. We excluded noun-specific aspects as we felt that these were well covered by existing natural language processing (NLP) approaches (e.g. whether the object / subject is person, abstract entity, or abstract concept is handled well by existing named entity recognition systems) and also excluded aspects which we felt had significant overlap with the dimensions we were investigating (e.g. affirmation an"
W10-0730,P03-1054,0,0.00337267,"and that raters were less confident about their answers, prompting more hedging and a flat distribution. 3.4 Predicting Transitivity We also performed an set of initial experiments to investigate our ability to predict Transitivity values for held out data. We extracted three sets of features from the sentences: lexical features, syntactic features, and features derived from WordNet (Miller, 1990). Lexical Features A feature was created for each word in a sentence after being stemmed using the Porter stemmer (Porter, 1980). Syntactic Features We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. “running”), or whether the sentence is passive. If any of these constructions appear in the sentence, we generate a corresponding feature. These represent features identified by Greene and Resnik (2009). WordNet Features For each word in the sentence, we extracted all the possible senses for each word. If any possible sense was a hyponym (i.e. an instance of) one of: artifact, living thing, abstract entity, location, or food, we added a feature corresponding to that top level s"
W14-2008,J95-2003,0,0.866656,"en defined as what the sentence is about (Reinhart, 1981). • The topic can be defined as prominent characters such as the protagonist (Francik, 1985). • The topic is often associated with old information (Gundel et al., 1993). • The subject position is considered to be a topical position (Chafe, 1976). • Repeated mentions are topical (Kameyama, 1994). • Psycholinguistic experiments define a discourse topic as a referent that has already been mentioned in the preceding discourse as a pronoun/the topic of a cleft (Arnold, 1999) or realized in subject position (Cowles, 2003). • Centering theory (Grosz et al., 1995; Brennan, 1995) formalizes the topic as a backward-looking center that is a single entity mentioned in the last sentence and in the most salient grammatical position (the grammatical subject is the most salient, and followed by the object and oblique object). • Giv´on (1983) suggests that all discourse entities are topical but that topicality is defined by a gradient/continuous property. Giv´on shows that three measures of topicality – recency (the distance between the referent and the referring expression), persistence (how long the referent would remain in the subsequent discourse), and pot"
W14-2008,J94-2003,0,0.366563,"ate with the types of reference expressions. Note that these scales measure topicality of the referring expression, but not the referent per se. Saliency and referring expressions Various factors have been proposed to influence referent salience (Arnold, 1998; Arnold, 2010). These factors include giveness (Chafe, 1976; Gundel et al., 1993), grammatical position (Brennan, 1995; Stevenson et al., 1994), order of mention (J¨arvikivi et al., 2005; Kaiser and Trueswell, 2008), recency (Giv´on, 1983; Arnold, 1998), syntactic focus and syntactic topic (Cowles et al., 2007; Foraker and McElree, 2007; Walker et al., 1994), parallelism (Chambers and Smyth, 1998; Arnold, 1998), thematic role (Stevenson et al., 1994; Arnold, 2001; Rohde et al., 2007), coherence relation (Kehler, 2002; Rohde et al., 2007) and topicality (Ariel, 1990; Arnold, 1998; Arnold, 1999). Psycholinguistic experiments (Arnold, 1998; Arnold, 2001; Kaiser, 2006) show that determining the salient referent is a complex process which is affected by various sources of information, and that these multiple factors have different strengths of influence. Among the numerous factors influencing the salience of a referent, this study focuses on topicalit"
W14-3112,N10-1012,0,0.163137,"within the topics and related topics are clustered within the overall layout. 3 Relationship Metrics We compute the term and topic relationship information required by the GIB layout as term co-occurrence and topic covariance, respectively. Term co-occurrence is a corpus-level statistic that can be computed independently from the LDA algorithm. The results of the LDA algorithm are required to compute the topic covariance. 3.1 Corpus-Level Term Co-Occurrence Prior work has shown that Pointwise Mutual Information (PMI) is the most consistent scoring method for evaluating topic model coherence (Newman et al., 2010). PMI is a statistical technique for measuring the association between two observations. For our purposes, PMI is used to measure the correlation between each term pair within each topic on the document level3 . The PMI is calculated for every possible term pair in the ingested data set using Equation 1. The visualization uses only the PMI for the term pairs for the top terms for each topic, which is a small subset of the calculated PMI values. Computing the PMI is trivial compared to the LDA calculation, and computing the values for all pairs allows the job to be run in parallel, as opposed t"
W14-3112,P11-1026,1,\N,Missing
W15-1212,W14-3207,0,0.411088,"recent surge of interest in finding accessible, cost effective, non-intrusive methods to detect depression and other mental disorders. Continuing a line of thought pioneered by Pennebaker and colleagues (Pennebaker and King, 1999; Rude et al., 2004, and others), researchers have been developing methods for identifying relevant signal in people’s language use, which could potentially provide inexpensive early detection of individuals who might require a specialist’s evaluation, on the basis of their naturally occurring linguistic behavior, e.g. (Neuman et al., 2012; De Choudhury et al., 2013; Coppersmith et al., 2014). Critical mass for a community of interest on these topics has been building within the computational linguistics research community (Resnik et al., 2014). To date, however, the language analysis methods used in this domain have tended to be fairly simple, typically including words or n-grams, manually defined word categories (e.g., Pennebaker’s LIWC lexicon, Pennebaker and King (1999)), and “vanilla” topic models (Blei et al., 2003, latent Dirichlet allocation (LDA)). This stands in contrast to other domains of computational social science in which more sophisticated models have been develop"
W15-1212,W15-1204,0,0.102468,"Missing"
W15-1212,N09-1031,0,0.0192038,". To assess the ability of the models/features and how they compare to baseline methods, we trained a linear support vector regression (SVR) model on the union of the Twitter train and dev sets, evaluated on the test set. We chose regression over classification despite having binary labels in our data in order to more easily evaluate precision at various levels of recall, which can be done simply by thresholding the predicted value at different points in order to obtain different recall levels. In addition, SVR has been shown to be an adequate choice to other similar text regression problems (Kogan et al., 2009), and in future analyses the use of the linear kernel will allow us to further see the contributions of each feature from the weights assigned by the regression model. We follow standard practice in using unigram features and LIWC categories as baseline feature sets, and we also use topic posteriors from a 50topic LDA model built on the Twitter training data.5 5 Not to be confused with the LDA model built using the stream-of-consciousness dataset in Section 3.1, which was used 104 As shown in Table 7, we evaluated alternative models/feature sets by fixing the percentage of recalled (correctly"
W15-1212,N15-1076,1,0.743604,"latent Dirichlet allocation (LDA)). This stands in contrast to other domains of computational social science in which more sophisticated models have been developed for some time, including opinion analysis (Titov and McDonald, 2008), analysis of the scientific literature (Blei and Lafferty, 2007), and computational political science (Grimmer, 2010). In this paper, we take steps toward employing more sophisticated models in the analysis of linguistic signal for detecting depression, providing promising results using supervised LDA (Blei and McAuliffe, 2007) and supervised anchor topic models (Nguyen et al., 2015), and beginning some initial exploration of a new supervised nested LDA model (SNLDA). 2 Data Our primary experimental dataset is the Twitter collection created by Coppersmith et al. (2014) 99 Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 99–107, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics and used in the CLPsych Hackathon (Coppersmith, 2015). The raw set contains roughly 3 million tweets from about 2,000 twitter users, of which roughly 600 self-identify as having been"
W15-1212,D13-1133,1,0.743591,"ving a regression value of 1 and those from the control subset as having a regression value of -1. In building some of our models, we also use a collection of 6,459 stream-of-consciousness essays collected between 1997 and 2008 by Pennebaker and King (1999), who asked students to think about their thoughts, sensations, and feelings in the moment and “write your thoughts as they come to you”. As discussed in Section 3.1, running LDA on this dataset provides informative priors for S LDA’s learning process on the Twitter training data. The student essays average approximately 780 words each, and Resnik et al. (2013) showed that unsupervised topic models based on this dataset can produce very clean, interpretable topical categories, a number of which were viewed by a clinician as relevant in the assessment of depression, including, for example, “vegetative” symptoms (particularly related to sleep and energy level), somatic symptoms (physical discomfort, e.g. headache, itching, digestive problems), and situational factors such as homesickness. For uniformity, we preprocessed the stream-ofconsciousness dataset with the same tools as the Twitter set.1 We created a shared vocabulary for our models by taking t"
W15-1212,W15-1207,1,0.72997,"Missing"
W15-1212,W14-3214,0,0.369338,"Missing"
W15-1212,P11-1026,1,\N,Missing
W16-0107,D12-1118,1,0.835636,"Missing"
W16-0107,N12-1094,0,0.0275354,"paintings. For example, in a question about The Holy Trinity by 46 Related Work Our work is specifically related to previous work on visual question answering and more generally to multimodal applications of vision and language. Visual QA has previously focused on content questions (Antol et al., 2015; Ren et al., 2015; Andreas et al., 2015), while we focus on identity questions. Relatedly, Zhu et al. (2015) find semantic links between images and text via an attention model. We use coreference to connect text and image regions, similar to Kong et al. (2014). However, not all text is “visual” (Dodge et al., 2012) and not all image regions can be described textually (Berg et al., 2012). While we focus on meaning, structure of text (Elsner et al., 2014) can also be inferred from images. Socher et al. (2014) match sentences to images; however, our dataset is unique in that the text is intentionally oblique (rather than direct descriptions) and our images—paintings—are more varied visually. Aside from QA, images have been successfully used to generate captions (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Vinyals et al., 2014; Xu et al., 2015; Chen and Zitnick, 2014). While we use vision to aid an NLP ta"
W16-0107,E14-1055,0,0.0273154,"question answering and more generally to multimodal applications of vision and language. Visual QA has previously focused on content questions (Antol et al., 2015; Ren et al., 2015; Andreas et al., 2015), while we focus on identity questions. Relatedly, Zhu et al. (2015) find semantic links between images and text via an attention model. We use coreference to connect text and image regions, similar to Kong et al. (2014). However, not all text is “visual” (Dodge et al., 2012) and not all image regions can be described textually (Berg et al., 2012). While we focus on meaning, structure of text (Elsner et al., 2014) can also be inferred from images. Socher et al. (2014) match sentences to images; however, our dataset is unique in that the text is intentionally oblique (rather than direct descriptions) and our images—paintings—are more varied visually. Aside from QA, images have been successfully used to generate captions (Karpathy and Fei-Fei, 2014; Mao et al., 2014; Vinyals et al., 2014; Xu et al., 2015; Chen and Zitnick, 2014). While we use vision to aid an NLP task, others have gone in the opposite direction, inducing correspondences between words and video clips (Yu and Siskind, 2013), words and acti"
W16-0107,P15-1162,1,0.826101,"Missing"
W16-0107,P13-1006,0,0.0582621,"Missing"
W16-0107,N15-1117,1,\N,Missing
W16-0108,D14-1070,1,0.865767,"Missing"
W18-5416,D15-1075,0,0.0218545,"h changes the neighbors at every layer. Consequently, conformity is a better uncertainty metric for both regular examples and adversarial ones, making it suitable for generating interpretations. 3.2 We consider six common text classification tasks: binary sentiment analysis using Stanford Sentiment Treebank (Socher et al., 2013, SST) and Customer Reviews (Hu and Liu, 2004, CR), topic classification using TREC (Li and Roth, 2002), opinion polarity (Wiebe et al., 2005, MPQA), and subjectivity/objectivity (Pang and Lee, 2004, SUBJ). Additionally, we consider natural language inference with SNLI (Bowman et al., 2015). We experiment with B I LSTM and CNN models. CNN Our CNN architecture resembles Kim (2014). We use convolutional filters of size three, four, and five, with max-pooling over time (Collobert and Weston, 2008). The filters are followed by three fully-connected layers. We fine-tune G LOV E embeddings (Pennington et al., 2014) of each word. For D K NN, we use the activations from the convolution layer and the three fullyconnected layers. Handling Sequences B I LSTM Our architecture uses a bidirectional LSTM (Graves and Schmidhuber, 2005), with the final hidden state forming the fixed-size represe"
W18-5416,D17-1070,0,0.0141973,"K NN, we use the final activations of the three recurrent layers and the two fully-connected layers. The D K NN algorithm requires fixed-size vector representations. To reach a fixed-size representation for text classification, we can take the final hidden state of a recurrent neural network or use a form of max pooling across time (Collobert and Weston, 2008). We consider deep architectures of these two forms, using each of the layers’ representations as the features. 3.3 SNLI Classifier Unlike other tasks with a single input sentence, SNLI has two inputs, a premise and hypothesis. Following Conneau et al. (2017), we use the same model to encode the two inputs, generating representations u for the premise and v for the hypothesis. We concatenate the two representations along with their dot-product and elementwise absolute difference, arriving at a final representation [u; v; u ∗ v; |u − v|]. This vector passes through two fully-connected layers for classification. For D K NN, we use the activations of the two fully-connected layers. Conformity leave-one-out Using conformity, we generate interpretations through a modified version of leave-one-out (Li et al., 2016). After removing a word, rather than ob"
W18-5416,D18-1407,1,0.912058,"es. The class with the highest probability is selected as the prediction y, with its probability serving as the model confidence. To create an interpretation, each input word is assigned an importance value, g(wi |x, y), which indicates the word’s contribution to the prediction. A saliency map (or heat map) visually highlights words in a sentence. 2.1 2.3 Interpreting neural networks can have unexpected negative results. Ghorbani et al. (2017) and Kindermans et al. (2017) show how a lack of model robustness and stability can cause egregious interpretation failures in computer vision settings. Feng et al. (2018) extend this to NLP and draw connections between interpretation failures and adversarial examples (Szegedy et al., 2014). To counteract this, new interpretation methods alone are not enough—models must be improved. For instance, Feng et al. (2018) argues that interpretation methods should not rely on prediction confidence as it does not reflect a model’s uncertainty. Following this, we improve interpretations by replacing neural network confidence with a robust uncertainty estimate using D K NN (Papernot and McDaniel, 2018). This algorithm achieves comparable accuracy on image classification t"
W18-5416,W16-1601,0,0.0344523,"Missing"
W18-5416,N18-2017,0,0.404852,"cion: D K NN generates interpretations with higher precision because conformity is robust to irrelevant changes. 5.2 Analyzing Dataset Annotation Artifacts Through D K NN, we get a new uncertainty measurement, conformity, that measures how a test example’s representation is positioned relative to the training data representations. In this section, we use conformity leave-one-out to interpret a model trained on SNLI. This dataset is known to contain annotation artifacts and we demonstrate that our interpretation method can help identify when models exploit these dataset biases. Recent studies (Gururangan et al., 2018; Poliak et al., 2018) identified annotation artifacts in the SNLI dataset. These works identified that superficial patterns exist in the input which strongly correlate with certain labels, making it possible for models to “game” the task: obtain high accuracy without true understanding. For instance, the hypothesis of an entailment example is often 4 https://sites.google.com/view/ language-dknn/ 140 2015). Most importantly for interpretation, the change in confidence often will not properly reflect whether discriminative input features have been removed (Feng et al., 2018). cause they contain"
W18-5416,D13-1170,0,0.00271939,"f every layer in the model, and therefore, a prediction only receives high conformity if it largely agrees with neighboring examples at all representation levels. This mechanism defends against adversarial examples (Szegedy et al., 2014), as it is difficult to construct a perturbation which changes the neighbors at every layer. Consequently, conformity is a better uncertainty metric for both regular examples and adversarial ones, making it suitable for generating interpretations. 3.2 We consider six common text classification tasks: binary sentiment analysis using Stanford Sentiment Treebank (Socher et al., 2013, SST) and Customer Reviews (Hu and Liu, 2004, CR), topic classification using TREC (Li and Roth, 2002), opinion polarity (Wiebe et al., 2005, MPQA), and subjectivity/objectivity (Pang and Lee, 2004, SUBJ). Additionally, we consider natural language inference with SNLI (Bowman et al., 2015). We experiment with B I LSTM and CNN models. CNN Our CNN architecture resembles Kim (2014). We use convolutional filters of size three, four, and five, with max-pooling over time (Collobert and Weston, 2008). The filters are followed by three fully-connected layers. We fine-tune G LOV E embeddings (Penningt"
W18-5416,D14-1181,0,0.00739498,"oth regular examples and adversarial ones, making it suitable for generating interpretations. 3.2 We consider six common text classification tasks: binary sentiment analysis using Stanford Sentiment Treebank (Socher et al., 2013, SST) and Customer Reviews (Hu and Liu, 2004, CR), topic classification using TREC (Li and Roth, 2002), opinion polarity (Wiebe et al., 2005, MPQA), and subjectivity/objectivity (Pang and Lee, 2004, SUBJ). Additionally, we consider natural language inference with SNLI (Bowman et al., 2015). We experiment with B I LSTM and CNN models. CNN Our CNN architecture resembles Kim (2014). We use convolutional filters of size three, four, and five, with max-pooling over time (Collobert and Weston, 2008). The filters are followed by three fully-connected layers. We fine-tune G LOV E embeddings (Pennington et al., 2014) of each word. For D K NN, we use the activations from the convolution layer and the three fullyconnected layers. Handling Sequences B I LSTM Our architecture uses a bidirectional LSTM (Graves and Schmidhuber, 2005), with the final hidden state forming the fixed-size representation. We use three LSTM layers, followed by two fully-connected layers. We fine-tune G L"
W18-5416,P18-3018,1,0.880761,"Missing"
W18-5416,C02-1150,0,0.0571626,"es with neighboring examples at all representation levels. This mechanism defends against adversarial examples (Szegedy et al., 2014), as it is difficult to construct a perturbation which changes the neighbors at every layer. Consequently, conformity is a better uncertainty metric for both regular examples and adversarial ones, making it suitable for generating interpretations. 3.2 We consider six common text classification tasks: binary sentiment analysis using Stanford Sentiment Treebank (Socher et al., 2013, SST) and Customer Reviews (Hu and Liu, 2004, CR), topic classification using TREC (Li and Roth, 2002), opinion polarity (Wiebe et al., 2005, MPQA), and subjectivity/objectivity (Pang and Lee, 2004, SUBJ). Additionally, we consider natural language inference with SNLI (Bowman et al., 2015). We experiment with B I LSTM and CNN models. CNN Our CNN architecture resembles Kim (2014). We use convolutional filters of size three, four, and five, with max-pooling over time (Collobert and Weston, 2008). The filters are followed by three fully-connected layers. We fine-tune G LOV E embeddings (Pennington et al., 2014) of each word. For D K NN, we use the activations from the convolution layer and the th"
W18-5416,P04-1035,0,0.00592388,"sarial examples (Szegedy et al., 2014), as it is difficult to construct a perturbation which changes the neighbors at every layer. Consequently, conformity is a better uncertainty metric for both regular examples and adversarial ones, making it suitable for generating interpretations. 3.2 We consider six common text classification tasks: binary sentiment analysis using Stanford Sentiment Treebank (Socher et al., 2013, SST) and Customer Reviews (Hu and Liu, 2004, CR), topic classification using TREC (Li and Roth, 2002), opinion polarity (Wiebe et al., 2005, MPQA), and subjectivity/objectivity (Pang and Lee, 2004, SUBJ). Additionally, we consider natural language inference with SNLI (Bowman et al., 2015). We experiment with B I LSTM and CNN models. CNN Our CNN architecture resembles Kim (2014). We use convolutional filters of size three, four, and five, with max-pooling over time (Collobert and Weston, 2008). The filters are followed by three fully-connected layers. We fine-tune G LOV E embeddings (Pennington et al., 2014) of each word. For D K NN, we use the activations from the convolution layer and the three fullyconnected layers. Handling Sequences B I LSTM Our architecture uses a bidirectional LS"
W18-5416,D14-1162,0,0.0813897,"l., 2013, SST) and Customer Reviews (Hu and Liu, 2004, CR), topic classification using TREC (Li and Roth, 2002), opinion polarity (Wiebe et al., 2005, MPQA), and subjectivity/objectivity (Pang and Lee, 2004, SUBJ). Additionally, we consider natural language inference with SNLI (Bowman et al., 2015). We experiment with B I LSTM and CNN models. CNN Our CNN architecture resembles Kim (2014). We use convolutional filters of size three, four, and five, with max-pooling over time (Collobert and Weston, 2008). The filters are followed by three fully-connected layers. We fine-tune G LOV E embeddings (Pennington et al., 2014) of each word. For D K NN, we use the activations from the convolution layer and the three fullyconnected layers. Handling Sequences B I LSTM Our architecture uses a bidirectional LSTM (Graves and Schmidhuber, 2005), with the final hidden state forming the fixed-size representation. We use three LSTM layers, followed by two fully-connected layers. We fine-tune G LOV E embeddings of each word. For D K NN, we use the final activations of the three recurrent layers and the two fully-connected layers. The D K NN algorithm requires fixed-size vector representations. To reach a fixed-size representa"
W18-5416,S18-2023,0,0.0475848,"Missing"
W18-5416,N16-3020,0,0.480269,"eutral tall first competition sad favorite 1.09 2.14 2.33 1.39 1.69 2.61 2.99 5.56 1.79 3.89 1.00 1.64 2.53 1.92 1.42 1.00 2.34 5.74 3.74 3.62 nobody sleeping Contradiction no tv cat 6.5 Trust in Model Predictions Model confidence is important for real-world applications: it signals how much one should trust a neural network’s predictions. Unfortunately, users may be misled when a model outputs highly confident predictions on rubbish examples (Goodfellow et al., 2015; Nguyen et al., 2015) or adversarial examples (Szegedy et al., 2014). Recent work decides when to trust a neural network model (Ribeiro et al., 2016; Doshi-Velez and Kim, 2017; Jiang et al., 2018). For instance, analyzing local linear model approximations (Ribeiro et al., 2016) or flagging rare network activations using kernel density estimation (Jiang et al., 2018). The D K NN conformity score is a trust metric that helps defend against image adversarial examples (Papernot and McDaniel, 2018). Future work should study if this robustness extends to interpretations. Table 4: The top SNLI artifacts identified by Gururangan et al. (2018) are shown on the left. For each word, we compute the average importance rank over the validation set usin"
