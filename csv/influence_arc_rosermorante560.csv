2007.sigdial-1.49,W07-2449,1,0.864632,"Missing"
2007.sigdial-1.49,W06-1322,0,0.0304827,"dence, has the role of accepting the previous contribution. A diff culty with this model is that its grounding criterion says that “the contributor and the partners mutually believe that the partners have understood what the contributor meant”. So the grounding process is conceived in terms of mutual beliefs. However, the central problem of grounding is precisely how mutual beliefs are established. Work based on this model includes its extension to human–computer interaction by Brennan and collaborators (Brennan, 1998; Cahn and S. E. Brennan, 1999), Li et al.’s model for multimodal grounding (Li et al., 2006), and Paek and Horvitz’s formal theory of grounding (Paek and Horvitz, 2000). In his inf uential computational model of grounding, Traum (1994) has introduced separate grounding acts which are used to provide communicative feedback and thereby create mutual beliefs. For this approach to work, Traum assumes that feedback acts are always correctly perceived and understood, therefore a dialogue participant does not need feedback about his feedback acts. This is an unwarranted assumption, however. Like any dialogue utterance, an utterance which expresses feedback can suffer from the addressee temp"
2007.sigdial-1.49,A00-2001,0,0.0346248,"s an unwarranted assumption, however. Like any dialogue utterance, an utterance which expresses feedback can suffer from the addressee temporarily being disturbed by the phone, or by an aircraft f ying over, or by noise on a communication channel; hence a speaker who performs a grounding act can never be sure that his act was performed successfully until he has received some form of feedback. A limitation and somewhat confusing aspect of this model is that it discusses the grounding of utterances, rather than the grounding of information conveyed by utterances through their semantic content. (Matheson et al., 2000) use elements of Traum’s model in their treatment of grounding from the Information State Update perspective. They represent grounded and ungrounded discourse units in the information state, and change their status from ungrounded to grounded through grounding acts. The dialogue act Acknowledgement is the only grounding act implemented; its main effect is to merge the information in the acknowledged discourse unit into the grounded information. They do not deal with cases of misunderstandings or cases where the user asks for acknowledgement. The model keeps only the last two utterances in the"
2020.cl-1.5,W17-1806,0,0.433077,"Missing"
2020.cl-1.5,J12-2006,0,0.0414998,"Missing"
2020.cl-1.5,L16-1597,0,0.441304,"Missing"
2020.cl-1.5,basile-etal-2012-developing,0,0.063294,"n order to create a larger training corpus? What are the problems that arise? In order to answer the questions we first review all existing corpora and characterize them in terms of several factors: type of information about negation that they contain, type of information about negation that is lacking, and type of application they would be suitable for. Available corpora that contain a representation of negation can be divided into two types (Fancellu et al. 2017): (i) those that represent negation in a logical form, using quantifiers, predicates, and relations (e.g., Groningen Meaning Bank [Basile et al. 2012], DeepBank [Flickinger, Zhang, and Kordoni 2012]); and (ii) those that use a string-level, where the negation operator and the elements (scope, event, focus) are defined as spans of text (e.g., BioScope [Vincze et al. 2008], ConanDoyle-neg [Morante and Daelemans 2012]). It should be noted that we focus on corpora that deal with string-level negation. The rest of the article is organized as follows: In Section 2 previous overviews that focus on negation are presented; in Section 3 the criteria used to review the existing corpora annotated with negation are described; in Sections 4, 5, and 6 th"
2020.cl-1.5,P11-1059,0,0.308502,"Missing"
2020.cl-1.5,W11-0418,0,0.0912085,"Missing"
2020.cl-1.5,W16-4011,0,0.0347185,"Missing"
2020.cl-1.5,W16-5113,0,0.449604,"Missing"
2020.cl-1.5,W16-2921,0,0.120361,"rphological negation, as in the UAM Spanish Treebank corpus. Unlike this one, annotations on the event and on how negation affects the polarity of the words within its scope were included. It was annotated by two senior researchers with in-depth experience in corpus annotation who supervised the whole process and two trained annotators who carried out the annotation task. The kappa coefficient for inter-annotator agreement was 0.97 for negation cues, 0.95 for negated events, and 0.94 for scopes.20 A detailed discussion of the main sources of disagreements can be found in Jim´enez-Zafra et al. (2016). The guidelines of the Bioscope corpus were taken into account, but after a thorough analysis of negation in Spanish, a typology of negation patterns in Spanish (Marti et al. 2016) was defined. As in Bioscope, NegDDI-DrugBank, and UAM Spanish Treebank, negation markers were included within the scope. Moreover, the subject was also included within the scope when the word directly affected by negation is the verb of the sentence. The event was also included within the scope of negation as in the ConanDoyle-neg corpus. The SFU ReviewSP -NEG is in XML format. It is publicly available and can be d"
2020.cl-1.5,W10-3110,0,0.317365,"Missing"
2020.cl-1.5,P07-2009,0,0.0651237,"Missing"
2020.cl-1.5,dalianis-velupillai-2010-certain,0,0.368624,"with these phenomena. In addition, a summary of studies in the field of sentiment analysis that have modeled negation and modality are shown. Some of the conclusions drawn by Morante and Sporleder are that although work on the treatment of negation and modality has been carried out in recent years, there is still much to do. Most research has been carried out on the English language and on specific domains and genres (biomedical, reviews, newswire, etc.). At the time of this overview only corpora annotated with negation for English had been developed, with the exception of one Swedish corpus (Dalianis and Velupillai 2010). Therefore, the authors indicate that it would be interesting to look at different languages and also distinct domains and genres, due to the fact that extrapropositional meaning is susceptible to domain and genre effects. Another interesting conclusion drawn from this study is that it would be a good idea to study which aspects of extra-propositional meaning need to be modeled for which applications, and the appropriate modeling of modality and negation. In relation to the modeling of negation, we can reference one survey about the role of negation in sentiment analysis (Wiegand et al. 2010)"
2020.cl-1.5,W17-1810,0,0.468104,"Missing"
2020.cl-1.5,P16-1047,0,0.438779,"Missing"
2020.cl-1.5,W17-1804,0,0.0200247,"needs to be large for a system to be able to learn. This motivates our main research questions: Is it possible to merge the existing negation corpora in order to create a larger training corpus? What are the problems that arise? In order to answer the questions we first review all existing corpora and characterize them in terms of several factors: type of information about negation that they contain, type of information about negation that is lacking, and type of application they would be suitable for. Available corpora that contain a representation of negation can be divided into two types (Fancellu et al. 2017): (i) those that represent negation in a logical form, using quantifiers, predicates, and relations (e.g., Groningen Meaning Bank [Basile et al. 2012], DeepBank [Flickinger, Zhang, and Kordoni 2012]); and (ii) those that use a string-level, where the negation operator and the elements (scope, event, focus) are defined as spans of text (e.g., BioScope [Vincze et al. 2008], ConanDoyle-neg [Morante and Daelemans 2012]). It should be noted that we focus on corpora that deal with string-level negation. The rest of the article is organized as follows: In Section 2 previous overviews that focus on ne"
2020.cl-1.5,P10-2013,0,0.0210337,"be found in Appendix A. 4. English Corpora As we already indicated, our analysis focuses on corpora with string-level annotations. We are aware of two corpora that do not follow this annotation approach: Groningen Meaning Bank (Basile et al. 2012) and DeepBank (Flickinger, Zhang, and Kordoni 2012). The Groningen Meaning Bank9 corpus is a collection of semantically annotated English texts with formal meaning representations rather than shallow semantics. It is composed of newswire texts from Voice of America, country descriptions from the CIA Factbook, a collection of texts from the open ANC (Ide et al. 2010), and Aesop’s fables. It was automatically annotated using C&C tools and Boxer (Curran, Clark, and Bos 2007) and then manually corrected. The DeepBank corpus10 contains rich syntactic and semantic annotations for the 25 Wall Street Journal sections included in the Penn Treebank (Taylor, Marcus, and Santorini 2003). The annotations are for the most part produced by manual disambiguation of parses licensed by the English Resource Grammar (Flickinger 2000). It is available in a variety of representation formats. To the best of our knowledge, the following are corpora that contain texts in English"
2020.cl-1.5,W16-5006,1,0.864594,"Missing"
2020.cl-1.5,W09-1401,0,0.0394055,"-Zafra et al. 2017) and was incorporated in sentiment analysis systems. Some systems use rules to detect negation, without evaluating their impact (Das and Chen 2001; Polanyi and Zaenen 2006; Kennedy and Inkpen 2006; Jia, Yu, and Meng 2009), whereas other systems use a lexicon of negation cues and predict the scope with machine learning algorithms (Councill, McDonald, and Velikovich 2010a; Lapponi, Read, and 2012; Cruz, Taboada, and Mitkov 2016b). Most systems are tested on the SFU Review corpus. Several shared tasks have addressed negation processing for English: the BioNLP’09 Shared Task 3 (Kim et al. 2009), the i2b2 NLP Challenge (Uzuner et al. 2011), the *SEM 2012 Shared Task (Morante and Blanco 2012), and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al. 2014). Although most of the work on processing negation focused on English texts, recently, negation in Spanish texts has attracted the attention of researchers. Costumero et al. (2014), Stricker, Iacobacci, and Cotik (2015), and Cotik et al. (2016b) develop systems for the identification of negation in clinical texts by adapting the NegEx algorithm (Chapman et al. 2001b). Regarding product reviews, there are some works that tr"
2020.cl-1.5,konstantinova-etal-2012-review,0,0.411283,"Missing"
2020.cl-1.5,R11-2022,0,0.0568866,"Missing"
2020.cl-1.5,P12-1043,0,0.0116146,"e criteria that have been applied to review the corpora: • 3 4 5 6 7 Language: The language(s) of the texts included in the corpus. This characteristic should always be specified in the description of any corpus, as it conditions its use. https://catalog.ldc.upenn.edu/. http://catalog.elra.info/en-us/. http://lremap.elra.info/. http://www.meta-share.org/. http://linguistic.linkeddata.es/retele-share/sparql-editor/. 194 Jim´enez-Zafra et al. Corpora Annotated with Negation: An Overview • Domain: Field to which the texts belong. Although cross-domain methodologies are being used for many tasks (Li et al. 2012; Szarvas et al. 2012; Bollegala, Mu, and Goulermas 2016), the domain of a corpus partly determines its area of application since different areas have different vocabularies. • Availability: Accessibility of the corpora. We indicate whether the corpus is publicly available and we provide the links for obtaining the data when possible. Corpora annotation is time-consuming and expensive, so it is not only necessary that corpora exist, but also that they be publicly available for the research community to use. • Guidelines: We study the guidelines used for the annotation showing similarities and"
2020.cl-1.5,P18-2085,0,0.145864,"e been developed based on lists of negations and stop words (Mitchell et al. 2004; Harkema et al. 2009; Mykowiecka, Marciniak, and Kup´sc´ 2009; Uzuner, Zhang, and Sibanda 2009; Sohn, Wu, and Chute 2012). The first system was the 217 Computational Linguistics Volume 46, Number 1 NegEx algorithm (Chapman et al. 2001a), which was then improved resulting in systems such as ConText (Harkema et al. 2009), DEEPEN (Mehrabi et al. 2015), and NegMiner (Elazhary 2017); (ii) machine learning techniques (Agarwal and Yu 2010; Li et al. 2010; Cruz D´ıaz et al. 2012; Velldal et al. 2012; Cotik et al. 2016b; Li and Lu 2018); and (iii) deep learning approaches (Fancellu, Lopez, and Webber 2016; Qian et al. 2016; Ren, Fei, and Peng 2018; Lazib et al. 2018). Although the interest in processing negation has only increased, negation resolvers are not yet a standard component of the natural language processing pipeline. Recently, a tool for detecting negation cues and scopes in English natural language texts has been released (Enger, Velldal, and Øvrelid 2017). Later on, with the developments in opinion mining, negation was studied as a marker of polarity change (Das and Chen 2001; Wilson, Wiebe, and Hoffmann 2005; Po"
2020.cl-1.5,C10-1076,0,0.0340169,"f approaches have been applied to processing negation: (i) rule-based systems have been developed based on lists of negations and stop words (Mitchell et al. 2004; Harkema et al. 2009; Mykowiecka, Marciniak, and Kup´sc´ 2009; Uzuner, Zhang, and Sibanda 2009; Sohn, Wu, and Chute 2012). The first system was the 217 Computational Linguistics Volume 46, Number 1 NegEx algorithm (Chapman et al. 2001a), which was then improved resulting in systems such as ConText (Harkema et al. 2009), DEEPEN (Mehrabi et al. 2015), and NegMiner (Elazhary 2017); (ii) machine learning techniques (Agarwal and Yu 2010; Li et al. 2010; Cruz D´ıaz et al. 2012; Velldal et al. 2012; Cotik et al. 2016b; Li and Lu 2018); and (iii) deep learning approaches (Fancellu, Lopez, and Webber 2016; Qian et al. 2016; Ren, Fei, and Peng 2018; Lazib et al. 2018). Although the interest in processing negation has only increased, negation resolvers are not yet a standard component of the natural language processing pipeline. Recently, a tool for detecting negation cues and scopes in English natural language texts has been released (Enger, Velldal, and Øvrelid 2017). Later on, with the developments in opinion mining, negation was studied as a"
2020.cl-1.5,W17-1807,0,0.217148,"Missing"
2020.cl-1.5,morante-2010-descriptive,1,0.740721,"ctic negation, if a syntactically independent negation marker is used to express negation (e.g., no [‘no/not’], nunca [‘never’]). – Lexical negation, if the cue is a word whose meaning has a negative component (e.g., negar [‘deny’], desistir [‘desist’]). – Morphological negation, if a morpheme is used to express negation (e.g., i- in ilegal [‘illegal’], in in incoherente [‘incoherent’]). It is also known as affixal negation. • – Negation components: Components of negation that have been annotated: Cues: lexical items that modify the truth value of the propositions that are within their scope (Morante 2010), that is, they are words that express negation. Negation cues can be adverbs (e.g., I have never been to Los Angeles), pronouns (e.g., His decisions have nothing 195 Computational Linguistics – – – Volume 46, Number 1 to do with me), verbs (e.g., The magazine desisted from published false stories about the celebrity), and words with negative prefixes (e.g., What you’ve done is illegal). They may consist of a single token (e.g., I do not like the food of this restaurant), a sequence of two or more contiguous tokens (e.g., He has not even tried it), or two or more non-contiguous tokens (e.g., I"
2020.cl-1.5,S12-1035,1,0.941155,"ed example from the PropBank Focus (PB-FOC) corpus. Figure 7 Annotated example from the ConanDoyle-neg corpus. annotation guidelines13 are based on those of the BioScope corpus, but there are some differences. The most important differences are that in the ConanDoyle-neg corpus the cue is not considered to be part of the scope, the scope can be discontinuous, and all the arguments of the event being negated are considered to be within the scope, including the subject, which is kept out of the scope in the BioScope corpus. The ConanDoyle-neg corpus was prepared with the aim of using it at the *SEM 2012 Shared Task14 (Morante and Blanco 2012), which was dedicated to resolving the scope and focus of negation. It is in CoNLL format (Farkas et al. 2010) and can be downloaded at http://www.clips.ua.ac.be/sem2012-st-neg/data.html. In Figure 7 it can be seen how Example (4.6) is represented in the corpus. The content of the columns is as follows: chapter name (1), sentence number within chapter (2), token number within sentence (3), token (4), lemma (5), POS tag (6), parse tree information (7). If the sentence has no negations, column (8) has a “***” value and there are no more columns, but if the"
2020.cl-1.5,morante-daelemans-2012-conandoyle,1,0.865378,"Missing"
2020.cl-1.5,D08-1075,1,0.882464,"Missing"
2020.cl-1.5,J12-2001,1,0.876091,"processing systems that have been developed using the corpora; in Sections 8 and 9 the corpora are analyzed showing features of interest, applications for which they can be used, and problems found for the development of negation processing systems; and finally, conclusions are drawn in Section 10. 2. Related Work To the best of our knowledge, there are currently no extensive reviews of corpora annotated with negation, but there are overviews that focus on the role of negation. An interesting overview on how modality and negation have been modeled in computational linguistics was presented by Morante and Sporleder (2012). The authors emphasize that most research in NLP has focused on propositional aspects of meaning, but extra-propositional aspects, such as negation and modality, are also important to understanding language. They also observe a growing interest in the computational treatment of these phenomena, evidenced by several annotations projects. In this overview, 1 http://www.mrtuit.com/. 2 https://www.meaningcloud.com/es/productos/analisis-de-sentimiento. 192 Jim´enez-Zafra et al. Corpora Annotated with Negation: An Overview modality and negation are defined in detail with some examples. Moreover, de"
2020.cl-1.5,padro-stanilovsky-2012-freeling,0,0.0970361,"Missing"
2020.cl-1.5,J05-1004,0,0.534363,"Missing"
2020.cl-1.5,W02-1011,0,0.0249526,"Missing"
2020.cl-1.5,D16-1078,0,0.0851334,"ema et al. 2009; Mykowiecka, Marciniak, and Kup´sc´ 2009; Uzuner, Zhang, and Sibanda 2009; Sohn, Wu, and Chute 2012). The first system was the 217 Computational Linguistics Volume 46, Number 1 NegEx algorithm (Chapman et al. 2001a), which was then improved resulting in systems such as ConText (Harkema et al. 2009), DEEPEN (Mehrabi et al. 2015), and NegMiner (Elazhary 2017); (ii) machine learning techniques (Agarwal and Yu 2010; Li et al. 2010; Cruz D´ıaz et al. 2012; Velldal et al. 2012; Cotik et al. 2016b; Li and Lu 2018); and (iii) deep learning approaches (Fancellu, Lopez, and Webber 2016; Qian et al. 2016; Ren, Fei, and Peng 2018; Lazib et al. 2018). Although the interest in processing negation has only increased, negation resolvers are not yet a standard component of the natural language processing pipeline. Recently, a tool for detecting negation cues and scopes in English natural language texts has been released (Enger, Velldal, and Øvrelid 2017). Later on, with the developments in opinion mining, negation was studied as a marker of polarity change (Das and Chen 2001; Wilson, Wiebe, and Hoffmann 2005; Polanyi and Zaenen 2006; Taboada et al. 2011; Jim´enez-Zafra et al. 2017) and was incorpor"
2020.cl-1.5,S12-1039,0,0.18091,"t have been developed using the corpora; in Sections 8 and 9 the corpora are analyzed showing features of interest, applications for which they can be used, and problems found for the development of negation processing systems; and finally, conclusions are drawn in Section 10. 2. Related Work To the best of our knowledge, there are currently no extensive reviews of corpora annotated with negation, but there are overviews that focus on the role of negation. An interesting overview on how modality and negation have been modeled in computational linguistics was presented by Morante and Sporleder (2012). The authors emphasize that most research in NLP has focused on propositional aspects of meaning, but extra-propositional aspects, such as negation and modality, are also important to understanding language. They also observe a growing interest in the computational treatment of these phenomena, evidenced by several annotations projects. In this overview, 1 http://www.mrtuit.com/. 2 https://www.meaningcloud.com/es/productos/analisis-de-sentimiento. 192 Jim´enez-Zafra et al. Corpora Annotated with Negation: An Overview modality and negation are defined in detail with some examples. Moreover, de"
2020.cl-1.5,D13-1170,0,0.0243104,"Missing"
2020.cl-1.5,Q14-1012,0,0.0524926,"Missing"
2020.cl-1.5,W08-0606,0,0.798813,"Missing"
2020.cl-1.5,J12-2004,0,0.0525602,"Missing"
2020.cl-1.5,taboada-etal-2006-methods,0,0.0932782,"Missing"
2020.cl-1.5,J11-2001,0,0.821603,"ted into applications to detect negations. Work on negation started in 2001 with the aim of processing clinical records (Chapman et al. 2001a; Mutalik, Deshpande, and Nadkarni 2001; Goldin and Chapman 2003). Some rule-based systems were developed based on lists of negations and stop words (Mitchell et al. 2004; Harkema et al. 2009; Mykowiecka, Marciniak, and Kup´sc´ 2009; Uzuner, Zhang, and Sibanda 2009; Sohn, Wu, and Chute 2012). With the surge of opinion mining, negation was studied as a marker of polarity change (Das and Chen 2001; Wilson, Wiebe, and Hoffmann 2005; Polanyi and Zaenen 2006; Taboada et al. 2011; Jim´enez-Zafra et al. 2017). Only with the release of the BioScope corpus (Vincze et al. 2008) did the work on negation receive a boost. But even so, despite the existence of several publications that focus on negation, it is difficult to find a negation processor for languages other than English. For English, some systems are available for processing clinical documents (NegEx [Chapman et al. 2001b], ConText [Harkema et al. 2009], Deepen [Mehrabi et al. 2015]) and, recently, a tool for detecting negation cues and scopes in natural language texts has been published (Enger, Velldal, and Øvreli"
2020.cl-1.5,J12-2005,0,0.051479,"sing negation: (i) rule-based systems have been developed based on lists of negations and stop words (Mitchell et al. 2004; Harkema et al. 2009; Mykowiecka, Marciniak, and Kup´sc´ 2009; Uzuner, Zhang, and Sibanda 2009; Sohn, Wu, and Chute 2012). The first system was the 217 Computational Linguistics Volume 46, Number 1 NegEx algorithm (Chapman et al. 2001a), which was then improved resulting in systems such as ConText (Harkema et al. 2009), DEEPEN (Mehrabi et al. 2015), and NegMiner (Elazhary 2017); (ii) machine learning techniques (Agarwal and Yu 2010; Li et al. 2010; Cruz D´ıaz et al. 2012; Velldal et al. 2012; Cotik et al. 2016b; Li and Lu 2018); and (iii) deep learning approaches (Fancellu, Lopez, and Webber 2016; Qian et al. 2016; Ren, Fei, and Peng 2018; Lazib et al. 2018). Although the interest in processing negation has only increased, negation resolvers are not yet a standard component of the natural language processing pipeline. Recently, a tool for detecting negation cues and scopes in English natural language texts has been released (Enger, Velldal, and Øvrelid 2017). Later on, with the developments in opinion mining, negation was studied as a marker of polarity change (Das and Chen 2001;"
2020.cl-1.5,W10-3105,0,0.0835586,"Missing"
2020.cl-1.5,W10-3111,0,0.135406,"and Velupillai 2010). Therefore, the authors indicate that it would be interesting to look at different languages and also distinct domains and genres, due to the fact that extrapropositional meaning is susceptible to domain and genre effects. Another interesting conclusion drawn from this study is that it would be a good idea to study which aspects of extra-propositional meaning need to be modeled for which applications, and the appropriate modeling of modality and negation. In relation to the modeling of negation, we can reference one survey about the role of negation in sentiment analysis (Wiegand et al. 2010). In this survey, several papers with novel approaches to modeling negation in sentiment analysis are presented. Sentiment analysis focuses on the automatic detection and classification of opinions expressed in texts; and negation can affect the polarity of a word (usually positive, negative, or neutral) because it can change, increment, or reduce the polarity value, hence the importance of dealing with this phenomenon in this area. The authors study the level of representation used for sentiment analysis, negation word detection, and scope of negation. In relation to the representation of neg"
2020.cl-1.5,H05-1044,0,0.358241,"Missing"
2020.cl-1.5,D15-1187,0,0.528925,"Missing"
2020.cl-1.5,E99-1043,0,\N,Missing
2020.law-1.2,W06-2920,0,0.350882,"Missing"
2020.law-1.2,W13-5501,0,0.0299137,"r to create an entire web of data (Bizer et al., 2011). Several projects were already performed to create linked linguistic data. Since 2012, six series of Linked Data in Linguistics (LDL) workshops have been hosted to gather and discuss contributions promoting linking linguistic data. After the second workshop, one of the presented efforts was the Linked Linguistic Open Data (LLOD) cloud.1 While creating the LLOD cloud, a lot of linguistic data was already available, which were mostly represented using RDF. One example is WordNet, which is also part of the Semantic Web (Gangemi et al., 2003; Chiarcos et al., 2013). Furthermore, Chiarcos proposed to represent linguistic corpora into OWL and RDF to interlink all of the different resources of corpora. In this way, the annotations of the corpora could be linked using terminological resources like standardized annotation formats (Chiarcos, 2012). Existing approaches that aim to represent the structure and content of natural language and its annotations include the Ontology Lexicalisation and the Ontologies of Linguistic Annotation (OLiA) (Villegas and Bel, 2015; Buitelaar et al., 2011; Bosque-Gil et al., 2015). OLiA focuses on annotations of linguistic corp"
2020.law-1.2,L16-1619,0,0.019727,"d the challenges, as well as the opportunity, of event corpora interoperability. Existing event annotation standards range from annotations about the participants, timing/temporal order, factuality, and entity coreference relation, making a standardized description of an event difficult. This is where nanopublications as a fine-grained and provenance-aware data format could provide transparency and interoperability for computational linguistic research. For our case study, we have chosen to focus on integrating annotations from two corpora, FactBank (Saurí and Pustejovsky, 2009) and PARC 3.0 (Pareti, 2016). In PARC, attribution relations and their components (Cue, Content, Source) are annotated, while in FactBank events and their factuality values (i.e. the certainty of the occurrence of an event) are annotated. It is interesting to combine the two annotation layers in order to extract, for example, which events occur in the content of attribution relations and what are their factuality values. Figure 1 presents the same sentence annotated in PARC and FactBank. Figure 1: An example sentence with its annotations in FactBank and PARC. Our main contributions thereby are (1) providing a fine-graine"
2020.law-1.2,L18-1178,1,0.874971,"Missing"
2020.lrec-1.611,C16-1324,0,0.0459725,"Missing"
2020.lrec-1.611,W18-2501,0,0.0126978,"ocurator.org/es Sentences 23,467 Tokens 528,727 Table 1: The Vaccination Corpus. To ensure future accessibility of the web documents, we made use of their archived versions in the Internet Archive (http://archive.org). We took the most recent snapshot in the Archive at the time of collecting, and retrieved the meta data and texts from this snapshot. All texts in the corpus have been automatically preprocessed with Stanford CoreNLP (Manning et al., 2014)4 for tokenization, sentence splitting, part-of-speech tagging, lemmatization and dependency parsing. In addition, we used the AllenNLP tools (Gardner et al., 2018)5 for semantic role labeling, which is a reimplementation of a deep BiLSTM model (He et al., 2017) and uses the PropBank representation (Palmer et al., 2005) of predicate-argument structures in sentences. The preprocessing information has not been used to support the manual annotation process, but we it will be used in our future research to extract perspectives. Manual annotations were performed with the open source annotation tool eHOST,6 which also provides options to calculate pairwise inter annotator agreement (IAA) in terms of F-Score, which is the weighted harmonic mean of precision and"
2020.lrec-1.611,J17-1004,0,0.0528841,"Missing"
2020.lrec-1.611,P17-1044,0,0.0266272,"sibility of the web documents, we made use of their archived versions in the Internet Archive (http://archive.org). We took the most recent snapshot in the Archive at the time of collecting, and retrieved the meta data and texts from this snapshot. All texts in the corpus have been automatically preprocessed with Stanford CoreNLP (Manning et al., 2014)4 for tokenization, sentence splitting, part-of-speech tagging, lemmatization and dependency parsing. In addition, we used the AllenNLP tools (Gardner et al., 2018)5 for semantic role labeling, which is a reimplementation of a deep BiLSTM model (He et al., 2017) and uses the PropBank representation (Palmer et al., 2005) of predicate-argument structures in sentences. The preprocessing information has not been used to support the manual annotation process, but we it will be used in our future research to extract perspectives. Manual annotations were performed with the open source annotation tool eHOST,6 which also provides options to calculate pairwise inter annotator agreement (IAA) in terms of F-Score, which is the weighted harmonic mean of precision and recall calculated as 2*((precison*recall)/precision+recall). The IAA scores presented in this pap"
2020.lrec-1.611,P14-5010,0,0.00275018,"he ControCurator dataset.3 The data was filtered using a set of keywords (e.g. vaccin, inoculation) to make sure we only included relevant documents. 3 http://controcurator.org/es Sentences 23,467 Tokens 528,727 Table 1: The Vaccination Corpus. To ensure future accessibility of the web documents, we made use of their archived versions in the Internet Archive (http://archive.org). We took the most recent snapshot in the Archive at the time of collecting, and retrieved the meta data and texts from this snapshot. All texts in the corpus have been automatically preprocessed with Stanford CoreNLP (Manning et al., 2014)4 for tokenization, sentence splitting, part-of-speech tagging, lemmatization and dependency parsing. In addition, we used the AllenNLP tools (Gardner et al., 2018)5 for semantic role labeling, which is a reimplementation of a deep BiLSTM model (He et al., 2017) and uses the PropBank representation (Palmer et al., 2005) of predicate-argument structures in sentences. The preprocessing information has not been used to support the manual annotation process, but we it will be used in our future research to extract perspectives. Manual annotations were performed with the open source annotation tool"
2020.lrec-1.611,L18-1524,0,0.0491706,"Missing"
2020.lrec-1.611,J05-1004,0,0.252949,"hived versions in the Internet Archive (http://archive.org). We took the most recent snapshot in the Archive at the time of collecting, and retrieved the meta data and texts from this snapshot. All texts in the corpus have been automatically preprocessed with Stanford CoreNLP (Manning et al., 2014)4 for tokenization, sentence splitting, part-of-speech tagging, lemmatization and dependency parsing. In addition, we used the AllenNLP tools (Gardner et al., 2018)5 for semantic role labeling, which is a reimplementation of a deep BiLSTM model (He et al., 2017) and uses the PropBank representation (Palmer et al., 2005) of predicate-argument structures in sentences. The preprocessing information has not been used to support the manual annotation process, but we it will be used in our future research to extract perspectives. Manual annotations were performed with the open source annotation tool eHOST,6 which also provides options to calculate pairwise inter annotator agreement (IAA) in terms of F-Score, which is the weighted harmonic mean of precision and recall calculated as 2*((precison*recall)/precision+recall). The IAA scores presented in this paper have been calculated with eHost, with lenient span match"
2020.lrec-1.611,C10-2117,0,0.0973323,"Missing"
2020.lrec-1.611,W06-0305,0,0.167196,"Missing"
2020.lrec-1.611,prasad-etal-2008-penn,0,0.308652,"Missing"
2020.lrec-1.611,W08-1203,0,0.106262,"Missing"
2020.lrec-1.611,sauri-etal-2014-newsome,0,0.016614,"per articles. Their schema focuses on two functional components of private states, i.e. the experiencers holding attitudes and the opinion targets towards which the attitudes are expressed. For this task we concentrate on the identification of opinion expressions and targets leaving the identification of holders for later work. Additionally, we do not annotate opinions on all topics, but only those on persons, groups of persons and institutions (called Person+ for the remainder of this section). This choice is motivated by the fact that the identification of opinion targets is difficult (see (Sauri et al., 2014)) which we aim to resolve by predefining possible opinion targets in the text. Besides, many different stakeholders participate in a debate like the vaccination debate and their attitude is not only expressed by giving opinions about the topic of vaccination, but also by criticizing and praising each other. As with most annotations of subjective content (Reidsma and op den Akker, 2008), our guidelines are not specified in extreme detail and the coding relies on the often subjective interpretation of the annotators. There are no fixed rules about how particular words should be annotated and sen"
2020.lrec-1.611,J17-3005,0,0.261612,"Missing"
2020.lrec-1.611,P10-1059,0,0.0539595,"Missing"
2020.lrec-1.611,W18-5207,1,0.848424,"be interpreted in different ways; (iii) it should also be stressed that claim-like statements that are not directly related to the topic need to be marked, as they are relevant; and (iv) the guidelines should be more restrictive. For example, the analysis of the errors related to attributability lead to the conclusion that claims should be attributable to a source and that the source should express a high level of commitment to the claim. A general conclusion is that agreeing on what a claim is still remains a difficult endeavor for human annotators. For more details about the annotation see Torsi and Morante (2018). 6. For this task a subset of the corpus is annotated. We selected 210 texts about children’s vaccinations thus excluding texts about, for example, the vaccination of animals and travellers. The documents were treated as follows: • Person+ entities were annotated to create a fixed set of possible opinion targets. 26,996 person+ entities were identified in 210 documents. Table 7 shows the most frequent entities. • The 210 documents were annotated with opinions by 2 different annotators. They found that only 168 of the documents indeed contained opinions. These 168 documents contain 23,000 Pers"
2020.lrec-1.611,L16-1187,1,0.702653,"Missing"
2020.lrec-1.703,2016.lilt-14.6,0,0.262147,"enses. In the second part of their research, Ruppenhofer and Rehbein (2012) use the annotated corpus as training data for a maximum entropy classifier. For all modals, apart from must, their system outperformed the baseline (using the most frequent sense of the relevant modal as default). While our goal is not to train a classifier, the results from Ruppenhofer and Rehbein (2012) are a good indication that their modal auxiliary classification scheme is meaningful and useful for future computational linguistic tasks. Building upon insights from Ruppenhofer and Rehbein (2012) and R¨omer (2004), Moon et al. (2016) conduct a similar modal annotation task, testing two hypotheses: (i) “uses of can, could and would are more difficult to classify than other modal auxiliaries, predicting lower IAA” (Moon et al., 2016). (ii) “Results would be comparable to those of previous studies when can and could are classified according to traditional taxonomic labels”. In their annotation scheme, the three difficult modal auxiliaries can have three senses, based on the most common readings as reported by (R¨omer, 2004). The other modals (should, ought to, may, might, must, have to and will) were allocated two senses 573"
2020.lrec-1.703,2020.lrec-1.611,1,0.804188,"Missing"
2020.lrec-1.703,ruppenhofer-rehbein-2012-yes,0,0.0827436,"dal annotation schemes devised for news texts be applied to the domain of the vaccination debate? Are cases of annotator disagreements dependent on the characteristics of the vaccination debate? In Section 2. we present related work. Section 3. describes the annotation task with pair-wise inter-annotator agreement and specific cases of disagreement. Section 4. provides an error analysis of disagreement followed by the description of the adjudicated corpus in Section 5. We discuss our findings in Section 6. Finally, in Section 7. we put forward some conclusions and future work. 2. Related Work Ruppenhofer and Rehbein (2012) research modality in the English news domain. They annotated the modal auxiliaries can, could, may, might, must, shall, should, and the semimodal verb ought to with six modal senses, as informed by previous research conducted by Kratzer (1991). The first three senses of Table 1 are equivalent to the epistemic, deontic and dynamic senses presented by (Palmer, 1986). The classification scheme used by Ruppenhofer and Rehbein (2012) largely overlaps with other modal sense taxonomies: • Epistemic: given the state of knowledge, the speaker is compelled to come to a particular conclusion. • Deontic:"
2020.lrec-1.703,W15-2705,0,0.124656,"(2012), where ‘ability’ relates to dynamic modality, ‘possibility’ to epistemic modality, and ‘permission’ to deontic modality. The categories of ‘obligation/advice’ and ‘inference/deduction’ can arguably be likened to deontic and epistemic modality, respectively. The remaining three modal senses presented by R¨omer (2004) – hypothetical meaning, prediction and volition – are not addressed by Ruppenhofer and Rehbein (2012), likely because will and would are not included in their annotation task and that shall was treated the same as should. Based on the work by Ruppenhofer and Rehbein (2012), Zhou et al. (2015) conducted further research with two main aims: (i) overcoming unequal modal sense distribution through the creation of an additional, balanced corpus, and (ii) to train the same maximum entropy classifier as used in Ruppenhofer and Rehbein (2012), but with richer features. Zhou et al. (2015) extended the MPQA corpus annotated by Ruppenhofer and Rehbein (2012) with sentences from MASC (and other resources)2 in order to balance the unequal distribution of modal senses. MASC3 is a subcorpus of the Open American National Corpus (OANC) and contains multiple genres of American English written text"
2020.lrec-1.853,J12-2006,0,0.0109648,"iwatts Marketing Group, 31 December 2017, accessed 20 February 2019. URL: https: //www.internetworldstats.com/stats7.htm 2 In the examples we mark in bold negation cues and enclose negation scopes between square brackets. 2. Related Work Negation is a well-studied phenomenon from a theoretical perspective (Horn, 2010; Horn, 1989). However, its computational treatment has not been extensively studied for 6902 languages other than English. Its automatic detection and treatment is relevant in a wide range of applications, such as information extraction (Savova et al., 2010), machine translation (Baker et al., 2012) or sentiment analysis (Liu, 2015), where it is crucial to detect when a fragment of text expresses a different meaning due to the presence of negation. The first attempts to process negation in English were mostly rule-based and focused on finding negated terms in clinical texts (Chapman et al., 2001; Mutalik et al., 2001; Goldin and Chapman, 2003; Auerbuch et al., 2004; Elkin et al., 2005; Boytcheva et al., 2005; Goryachev et al., ; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning"
2020.lrec-1.853,W16-2921,0,0.101754,"t al., 2005; Goryachev et al., ; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning sequence labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001;"
2020.lrec-1.853,W10-3110,0,0.0459921,"the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia et al., 2009). Other systems employ a lexicon of negation cues and predict the scope with CRFs using as features lowercased token strings, token PoS tags, token-wise distances from explicit negation cues and dependency syntax information (Councill et al., 2010), or a rich set of lexical and syntactic features, together with cue-dependant information (Lapponi et al., 2012a). Cruz et al. (2016) use SVM and lexical, syntactic and dependency features. They test the system in the SFU Review corpus (Konstantinova et al., 2012). Work on processing negation in Spanish is relatively recent. Costumero et al. (2014), Stricker et al. (2015) and Cotik et al. (2016) develop systems for the identification of negation in clinical texts by adapting the NegEx algorithm (Chapman et al., 2001). Regarding product reviews, there are some works that treat negation as a su"
2020.lrec-1.853,W10-3001,0,0.113629,"Missing"
2020.lrec-1.853,W09-1401,0,0.0283826,"ce labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia et al., 2009). Other systems employ a lexicon of negation cues and predict the scope with CRFs using as features lowercased token strings, toke"
2020.lrec-1.853,konstantinova-etal-2012-review,0,0.0425358,"Missing"
2020.lrec-1.853,S12-1042,0,0.167483,"y et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia et al., 2009). Other systems employ a lexicon of negation cues and predict the scope with CRFs using as features lowercased token strings, token PoS tags, token-wise distances from explicit negation cues and dependency syntax information (Councill et al., 2010), or a rich set of lexical and syntactic features, together with cue-dependant information (Lapponi et al., 2012a). Cruz et al. (2016) use SVM and lexical, syntactic and dependency features. They test the system in the SFU Review corpus (Konstantinova et al., 2012). Work on processing negation in Spanish is relatively recent. Costumero et al. (2014), Stricker et al. (2015) and Cotik et al. (2016) develop systems for the identification of negation in clinical texts by adapting the NegEx algorithm (Chapman et al., 2001). Regarding product reviews, there are some works that treat negation as a subtask of sentiment analysis (Taboada et al., 2011; Vilares et al., 2013; Vilares et al., 2015; Jim´enez-Zafra et"
2020.lrec-1.853,C10-1076,0,0.0355942,"al., 2004; Elkin et al., 2005; Boytcheva et al., 2005; Goryachev et al., ; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning sequence labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate th"
2020.lrec-1.853,S12-1035,1,0.907207,"ed systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia et al., 2009). Other systems employ a lexicon of negation cues and predict the scope with CRFs using as features lowercased token strings, token PoS tags, token-wise distances from explicit negation cues and dependency syntax information (Councill et al., 2010), or a rich set of lexical and"
2020.lrec-1.853,D08-1075,1,0.429593,"u, 2015), where it is crucial to detect when a fragment of text expresses a different meaning due to the presence of negation. The first attempts to process negation in English were mostly rule-based and focused on finding negated terms in clinical texts (Chapman et al., 2001; Mutalik et al., 2001; Goldin and Chapman, 2003; Auerbuch et al., 2004; Elkin et al., 2005; Boytcheva et al., 2005; Goryachev et al., ; Sanchez-Graillet and Poesio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning sequence labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared"
2020.lrec-1.853,padro-stanilovsky-2012-freeling,0,0.0931654,"Missing"
2020.lrec-1.853,D16-1078,0,0.0191586,"sio, 2007; Huang and Lowe, 2007; Rokach et al., 2008). The task of detecting negation scopes was introduced in 2008 as a machine learning sequence labelling task (Morante et al., 2008). Subsequently, three main types of approaches have been applied to processing negation: (i) rule-based systems, in an attempt to improve the NegEx algorithm, such as ConText (Harkema et al., 2009), DEEPEN (Mehrabi et al., 2015), and NegMiner (Elazhary, 2017); (ii) machine learning techniques (Agarwal and Yu, 2010; Li et al., 2010; Cruz D´ıaz et al., 2012; Cotik et al., 2016); and (iii) deep learning approaches (Qian et al., 2016; Ren et al., 2018; Lazib et al., 2018). Several shared tasks have addressed negation processing: the BioNLP’09 Shared Task 3 (Kim et al., 2009), the CoNLL2010 shared task (Farkas et al., 2010), the i2b2 NLP Challenge (Uzuner et al., 2011), the *SEM 2012 Shared Task (Morante and Blanco, 2012) and the ShARe/CLEF eHealth Evaluation Lab 2014 Task 2 (Mowery et al., 2014). Previous work has incorporated negation processing in sentiment analysis systems. Some systems use rules, but do not evaluate the processing of negation (Das and Chen, 2001; Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006; Jia"
2020.lrec-1.853,W15-2914,0,0.0414316,"Missing"
2020.lrec-1.853,J11-2001,0,0.0452492,"d syntactic features, together with cue-dependant information (Lapponi et al., 2012a). Cruz et al. (2016) use SVM and lexical, syntactic and dependency features. They test the system in the SFU Review corpus (Konstantinova et al., 2012). Work on processing negation in Spanish is relatively recent. Costumero et al. (2014), Stricker et al. (2015) and Cotik et al. (2016) develop systems for the identification of negation in clinical texts by adapting the NegEx algorithm (Chapman et al., 2001). Regarding product reviews, there are some works that treat negation as a subtask of sentiment analysis (Taboada et al., 2011; Vilares et al., 2013; Vilares et al., 2015; Jim´enez-Zafra et al., 2015; Miranda et al., 2016; Amores et al., 2016; Jim´enez-Zafra et al., 2019c). The first systems that detect negation cues were developed in the framework of the 2018 and 2019 editions of NEGES, the Workshop on Negation in Spanish (Jim´enezZafra et al., 2019a; Jim´enez-Zafra et al., 2019b) and were trained on the SFU ReviewSP -NEG corpus (Jim´enez-Zafra et al., 2018). Participants (Fabregat et al., 2018; Loharja et al., 2018; Giudice, 2019; Beltr´an and Gonz´alez, 2019; Dom´ınguez-Mas et al., 2019; Fabregat et al., 2019) add"
2020.lrec-1.853,taboada-etal-2006-methods,0,0.0889954,"Missing"
2021.argmining-1.5,N19-1423,0,0.0168085,"ently more connected to sociocultural aspects and topic-specific differences than related tasks such as sentiment analysis. This paper is organized as follows. Section 2 discusses earlier work on stance detection, and specifically generalizability across topics. Section 3 presents the reproduction results. Section 4 adds additional, topic-specific analyses of the classification performance and a bag-of-words-based model to find topic-(in)dependent features. This is followed by our conclusions in Section 5. tion has seen a performance increase due to pretrained Transformer models such as BERT (Devlin et al., 2019). Reimers et al. (2019) reported .20 point F1 improvement over an LSTM baseline with a pre-trained BERT model. Combining multiple stance detection datasets in fine-tuning such a pretrained Transformer again led to a performance increase, though this model lacks robustness against slight test set manipulations (Schiller et al., 2021). 2 Recent work has specifically worked on identifying stances on topics not seen in training. Reimers et al. (2019) train their model on detecting stances and arguments for unseen topics. In their approach however, they treat all topics and stances on these topics"
2021.argmining-1.5,N16-1138,0,0.0304868,"ets on “dimensions in the sociocultural field” (Du Bois, 2007, p. 163). Current work focuses mostly on debates deemed controversial in the U.S. sociopolitical domain, such as abortion and gun control. 2.2 Prior work Early work on stance detection focused on parliamentary debates and longer texts (Thomas et al., 2006). Since Mohammad et al. (2016)’s stance detection shared task, Twitter has attracted a lot of attention in NLP work on stance detection (Zhu et al., 2019; Darwish et al., 2020; Hossain et al., 2020). Others addressed stance detection in the news domain, with (fake) news headlines (Ferreira and Vlachos, 2016; Hanselowski et al., 2018), disinformation (Hardalov et al., 2021) and user comments on news websites (Bošnjak and Karan, 2019). Feature-based approaches have largely been replaced by end-to-end neural models. Stance detec2 We would like to note that the stance expressed in a text unit does not have to be the stance of an author, e.g. in cases where someone is writing a piece in which they express or quote someone else’s opinion. 47 Generalization to new topics against abortion). Some of these studies attempt to leverage these vocabularies to generalize across similar topics. Recent work has"
2021.argmining-1.5,2021.naacl-main.379,0,0.0336439,"pic-specific manner. Topic-specific features were more informative for SVM models than more topic-independent features. In more recent work, Wei and Mao (2019) instead specifically focus on how generalizable certain topics are for transferring knowledge to new topics on stance detection. Some Twitter discussion topics seem to share a latent, underlying topic (e.g. both feminism and abortion have the latent topic of equality). In a (latent) topic-enhanced multi-layer perceptron (MLP) model with RNN representation of the tweet, the model indeed uses shared vocabulary between the related topics. Allaway et al. (2021) notice that earlier work, when considering training on some topics and testing on others, incorporates topic-relatedness. Unlike these other studies however, Allaway et al. (2021, p. 4756) “do not assume a relationship between training and test topics” as a fairer test of robustness. Results they present do show that stance detection is related to topic, but their efforts go to finding topic-invariant stance representations, which improves the generalizability of their model. Their consideration of topic similarity shows that topic difference is very relevant to stance detection. ALDayel and"
2021.argmining-1.5,P13-1166,1,0.85288,"Missing"
2021.argmining-1.5,2021.eacl-main.29,0,0.184586,"sformer models show promising results on cross-topic argument classification (Reimers et al., 2019; Schiller et al., 2021). In this paper, we investigate the ability of crosstopic stance detection approaches to generalize to different debate topics. Our question is: To what extent is stance detection topic-independent and generalizable across topics? Our contributions are threefold. We first complete a reproduction of state-of-the-art cross-topic stance detection work (Reimers et al., 2019), as reproduction has repeatedly shown to be relevant for NLP (Fokkens et al., 2013; Cohen et al., 2018; Belz et al., 2021). The reproduction is largely successful: we obtain similar numeric results. Secondly, we investigate the topic-specific performance of this model, and conclude that BERT’s performance fluctuates on different topics. Additionally, we find that a bag-of-words-based SVM model can rival its performance for some topics. Thirdly, we relate this to the nature of the stance detection modelling Cross-topic stance detection is the task to automatically detect stances (pro, against, or neutral) on unseen topics. We successfully reproduce state-of-the-art cross-topic stance detection work (Reimers et al."
2021.argmining-1.5,C18-1158,0,0.0351058,"Missing"
2021.argmining-1.5,W19-3707,0,0.0148243,"l in the U.S. sociopolitical domain, such as abortion and gun control. 2.2 Prior work Early work on stance detection focused on parliamentary debates and longer texts (Thomas et al., 2006). Since Mohammad et al. (2016)’s stance detection shared task, Twitter has attracted a lot of attention in NLP work on stance detection (Zhu et al., 2019; Darwish et al., 2020; Hossain et al., 2020). Others addressed stance detection in the news domain, with (fake) news headlines (Ferreira and Vlachos, 2016; Hanselowski et al., 2018), disinformation (Hardalov et al., 2021) and user comments on news websites (Bošnjak and Karan, 2019). Feature-based approaches have largely been replaced by end-to-end neural models. Stance detec2 We would like to note that the stance expressed in a text unit does not have to be the stance of an author, e.g. in cases where someone is writing a piece in which they express or quote someone else’s opinion. 47 Generalization to new topics against abortion). Some of these studies attempt to leverage these vocabularies to generalize across similar topics. Recent work has looked into generalizing stance detection across datasets, task definitions, and domains (Schiller et al., 2021), in which topic"
2021.argmining-1.5,2020.nlpcovid19-2.11,0,0.0278334,"omenon of actors communicating their evaluation of targets, by which they place themselves and their targets on “dimensions in the sociocultural field” (Du Bois, 2007, p. 163). Current work focuses mostly on debates deemed controversial in the U.S. sociopolitical domain, such as abortion and gun control. 2.2 Prior work Early work on stance detection focused on parliamentary debates and longer texts (Thomas et al., 2006). Since Mohammad et al. (2016)’s stance detection shared task, Twitter has attracted a lot of attention in NLP work on stance detection (Zhu et al., 2019; Darwish et al., 2020; Hossain et al., 2020). Others addressed stance detection in the news domain, with (fake) news headlines (Ferreira and Vlachos, 2016; Hanselowski et al., 2018), disinformation (Hardalov et al., 2021) and user comments on news websites (Bošnjak and Karan, 2019). Feature-based approaches have largely been replaced by end-to-end neural models. Stance detec2 We would like to note that the stance expressed in a text unit does not have to be the stance of an author, e.g. in cases where someone is writing a piece in which they express or quote someone else’s opinion. 47 Generalization to new topics against abortion). Some"
2021.argmining-1.5,L18-1025,0,0.279716,"es, pre-trained Transformer models show promising results on cross-topic argument classification (Reimers et al., 2019; Schiller et al., 2021). In this paper, we investigate the ability of crosstopic stance detection approaches to generalize to different debate topics. Our question is: To what extent is stance detection topic-independent and generalizable across topics? Our contributions are threefold. We first complete a reproduction of state-of-the-art cross-topic stance detection work (Reimers et al., 2019), as reproduction has repeatedly shown to be relevant for NLP (Fokkens et al., 2013; Cohen et al., 2018; Belz et al., 2021). The reproduction is largely successful: we obtain similar numeric results. Secondly, we investigate the topic-specific performance of this model, and conclude that BERT’s performance fluctuates on different topics. Additionally, we find that a bag-of-words-based SVM model can rival its performance for some topics. Thirdly, we relate this to the nature of the stance detection modelling Cross-topic stance detection is the task to automatically detect stances (pro, against, or neutral) on unseen topics. We successfully reproduce state-of-the-art cross-topic stance detection"
2021.argmining-1.5,P10-2047,0,0.047433,"e topics and testing on others, incorporates topic-relatedness. Unlike these other studies however, Allaway et al. (2021, p. 4756) “do not assume a relationship between training and test topics” as a fairer test of robustness. Results they present do show that stance detection is related to topic, but their efforts go to finding topic-invariant stance representations, which improves the generalizability of their model. Their consideration of topic similarity shows that topic difference is very relevant to stance detection. ALDayel and Magdy (2021) describe in their survey how several studies (Klebanov et al., 2010; Zhu et al., 2019; Darwish et al., 2020) show that texts pro or against an issue use different vocabularies (e.g. using ‘pro-life’ when expressing a stance 2.1 2.3 Background Definition of Stance Detection Stance detection is a long-established task in computational linguistics. Küçük and Can (2020) identify its most commonly used task definition: “For an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.” (Küçük and Can"
2021.argmining-1.5,S16-1003,0,0.0730956,"Missing"
2021.argmining-1.5,D18-1402,0,0.194885,"mance on different topics, and addressing topic-specific vocabulary and context, is a future avenue for cross-topic stance detection. 1 Introduction (Online) debate has long been studied and modelled by computational linguistics with argument mining tasks such as stance detection. Stance detection is the task of automatically identifying the stance (agreeing, disagreeing, and/or neutral) of a text towards a debated topic or issue (Küçük and Can, 2020; Schiller et al., 2021).1 Its use-cases increasingly relate to online information environments and societal challenges, such as argument search (Stab et al., 2018), fake news identification (Hanselowski et al., 2018), or diversifying stances in a news recommender (Reuver et al., 2021). Cross-topic stance detection models should thus be able to deal with the quickly changing landscape of (online) public debate, where new topics and issues appear all the time. As Schlangen (2021) described in his recent paper on natural language 1 There is a wide array of datasets, definitions, and operationalizations of stance detection and classification, and recently Schiller et al. (2021) gave a great overview in their Section 2, as do Küçük and Can (2020) in their su"
2021.argmining-1.5,P19-1054,0,0.108146,"on dimensions in the socio-cultural field. This also comes with very topic-specific word use (Somasundaran and Wiebe, 2009; Wei and Mao, 2019). For instance, an against abortion argument might be expressed indirectly with a ‘pro-life’ expression, and someone aware of the socio-cultural context of this debate will be able to recognize this. Knowledge from other debate topics such as gun control may not be useful, since the debate strategies might change per topic. Despite these fundamental challenges, pre-trained Transformer models show promising results on cross-topic argument classification (Reimers et al., 2019; Schiller et al., 2021). In this paper, we investigate the ability of crosstopic stance detection approaches to generalize to different debate topics. Our question is: To what extent is stance detection topic-independent and generalizable across topics? Our contributions are threefold. We first complete a reproduction of state-of-the-art cross-topic stance detection work (Reimers et al., 2019), as reproduction has repeatedly shown to be relevant for NLP (Fokkens et al., 2013; Cohen et al., 2018; Belz et al., 2021). The reproduction is largely successful: we obtain similar numeric results. Sec"
2021.argmining-1.5,W06-1639,0,0.128417,"2 to 4, e.g. by adding ‘comment’ and ‘query’ next to ‘for’ and ‘against’ (Schiller et al., 2021). Küçük and Can (2020) emphasize that this computational definition is built upon the linguistic phenomenon of actors communicating their evaluation of targets, by which they place themselves and their targets on “dimensions in the sociocultural field” (Du Bois, 2007, p. 163). Current work focuses mostly on debates deemed controversial in the U.S. sociopolitical domain, such as abortion and gun control. 2.2 Prior work Early work on stance detection focused on parliamentary debates and longer texts (Thomas et al., 2006). Since Mohammad et al. (2016)’s stance detection shared task, Twitter has attracted a lot of attention in NLP work on stance detection (Zhu et al., 2019; Darwish et al., 2020; Hossain et al., 2020). Others addressed stance detection in the news domain, with (fake) news headlines (Ferreira and Vlachos, 2016; Hanselowski et al., 2018), disinformation (Hardalov et al., 2021) and user comments on news websites (Bošnjak and Karan, 2019). Feature-based approaches have largely been replaced by end-to-end neural models. Stance detec2 We would like to note that the stance expressed in a text unit does"
2021.argmining-1.5,2021.hackashop-1.7,1,0.895114,"ce detection. 1 Introduction (Online) debate has long been studied and modelled by computational linguistics with argument mining tasks such as stance detection. Stance detection is the task of automatically identifying the stance (agreeing, disagreeing, and/or neutral) of a text towards a debated topic or issue (Küçük and Can, 2020; Schiller et al., 2021).1 Its use-cases increasingly relate to online information environments and societal challenges, such as argument search (Stab et al., 2018), fake news identification (Hanselowski et al., 2018), or diversifying stances in a news recommender (Reuver et al., 2021). Cross-topic stance detection models should thus be able to deal with the quickly changing landscape of (online) public debate, where new topics and issues appear all the time. As Schlangen (2021) described in his recent paper on natural language 1 There is a wide array of datasets, definitions, and operationalizations of stance detection and classification, and recently Schiller et al. (2021) gave a great overview in their Section 2, as do Küçük and Can (2020) in their survey. 46 Proceedings of The 8th Workshop on Argument Mining, pages 46–56 Punta Cana, Dominican Republic, November 10–11, 2"
2021.argmining-1.5,2021.starsem-1.25,0,0.0333352,"tance detection over non-Transformer models? We argue this claim needs an asterisk: this cross-topic approach does not work as well for all topics. Different topics show specific vocabularies and socio-cultural contexts, and especially these specific contexts BERT cannot navigate. BERT models still make similar mistakes on gun control as the LSTM-based models in Stab et al. (2018). These findings lead us to two take-aways. Firstly, we hypothesize that models like BERT rely more on topic-specific features for stance detection than topic-independent lexical words related to argumentation. Thorn Jakobsen et al. (2021) also recently found this, and connected BERT’s crosstopic stance detection performance to its focus on spurious topic-specific lexical features (""gun"", ""criminal"") rather than words related to argumentation. They also conclude a fair real-world evaluation of cross-topic stance detection means reporting the worst performing cross-topic pair rather than average performance over topics. Secondly, we also think it is necessary to analyze the context of topics, and its relation to other debate topics within and outside the dataset. Most topics in stance detection studies are currently U.S. socio-p"
2021.argmining-1.5,2021.acl-short.85,0,0.171511,"ically identifying the stance (agreeing, disagreeing, and/or neutral) of a text towards a debated topic or issue (Küçük and Can, 2020; Schiller et al., 2021).1 Its use-cases increasingly relate to online information environments and societal challenges, such as argument search (Stab et al., 2018), fake news identification (Hanselowski et al., 2018), or diversifying stances in a news recommender (Reuver et al., 2021). Cross-topic stance detection models should thus be able to deal with the quickly changing landscape of (online) public debate, where new topics and issues appear all the time. As Schlangen (2021) described in his recent paper on natural language 1 There is a wide array of datasets, definitions, and operationalizations of stance detection and classification, and recently Schiller et al. (2021) gave a great overview in their Section 2, as do Küçük and Can (2020) in their survey. 46 Proceedings of The 8th Workshop on Argument Mining, pages 46–56 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics task, which is inherently more connected to sociocultural aspects and topic-specific differences than related tasks such as sentiment analysis."
2021.argmining-1.5,P18-2095,0,0.0197785,"y chosen from online lists of controversial topics on discussion websites (Stab et al., 2018, p. 3666). Specifically, these topics are abortion, cloning, death penalty, gun control, marijuana legalization, minimum wage, nuclear energy and school uniforms. The stance classes (pro, con, and no argument) were annotated by two argument mining experts Reproduction Experiments Reimers et al. (2019) apply their approach of crosstopic claim classification to two datasets: the UKP Sentential Argument Mining Corpus (Stab et al., 2018) (‘the UKP dataset’) and the IBM Debater: Evidence Sentences dataset (Shnarch et al., 2018) (‘the IBM dataset’). We focus on the UKP Dataset, since the IBM Debater dataset has no ‘pro’ and ‘con’ class, but rather ‘evidence’ and ‘no evidence’ (and our focus is on stance detection). As a second step after stance classification, the authors also attempt to cluster similar arguments within the same topic in a cross-topic training setting. We do not replicate this component, but instead dive deeper into the classification results. We adopt the definition of reproduction by Belz et al. (2021): repeating the experiments as described in the earlier study, with the exact same data and softwa"
2021.argmining-1.5,P09-1026,0,0.34733,"denuniv.nl Abstract processing (NLP) methodology, generalization is a main goal of computational linguistics. A computational model (e.g. a stance detection model) should learn task capabilities beyond one set of datapoints, in our case: beyond one debate topic. Cross-topic stance detection is especially challenging because generalization to a new discussion topic is not trivial. Expressing stances is inherently socio-cultural behavior (Du Bois, 2007), where social actors place themselves and targets on dimensions in the socio-cultural field. This also comes with very topic-specific word use (Somasundaran and Wiebe, 2009; Wei and Mao, 2019). For instance, an against abortion argument might be expressed indirectly with a ‘pro-life’ expression, and someone aware of the socio-cultural context of this debate will be able to recognize this. Knowledge from other debate topics such as gun control may not be useful, since the debate strategies might change per topic. Despite these fundamental challenges, pre-trained Transformer models show promising results on cross-topic argument classification (Reimers et al., 2019; Schiller et al., 2021). In this paper, we investigate the ability of crosstopic stance detection app"
2021.latechclfl-1.1,P14-2131,0,0.217883,"imply replaced with its corresponding 50-dimensional embedding vector. The authors of (Wu et al., 2015) compared three strategies for deriving distributed word representations in a corpus of English clinical texts. They base their method on earlier work (Guo et al., 2014), in which three different approaches for utilizing the embedding features are compared. In Wu et al. (2015) it is shown that the binarized word embedding features caused the largest improvement of the performance of the CRF system, while Guo et al. (2014) concluded that a distributional prototype approach performed the best. Bansal et al. (2014) perform agglomerative hierarchical clustering of the embedding vectors to take into account all dimensions of a vector simultaneously. Daˇrena and Süss (2020) use a neural network that is allowed to update the word vectors during training, since their Czech corpus from which a word embedding model was build, was rather small. Related work Historical text mining is an incipient field (Piotrowski, 2012). Regarding the Dutch language, historical text mining studies are limited. Stylometric methods have been applied in authorship verification tasks on medieval and early modern texts (Kestemont et"
2021.latechclfl-1.1,2021.findings-acl.243,0,0.01192,"ents, and Section 6 the results. Finally, in Section 7 we put forward some conclusions. 2 els) have recently become a popular way to describe a corpus of texts. Well known implementations are word2vec and fasttext (Bojanowski et al., 2017). Regarding the Dutch language, they have been used for relation evaluation and dialect identification (Tulkens et al., 2016), and to detect semantic change in Dutch newspapers (Wevers and Koolen, 2020; Wevers, 2019) and parliamentary debates (Lange, van and Futselaar, 2019). Word embeddings have been used as features in classification tasks (see for example Beelen et al. (2021)), but to the best of our knowledge, this has never been done for the historic Dutch language. Studies with corpora in other languages show that there are multiple ways to implement the word embeddings as features of a classifier. In Huang et al. (2015), every one hot encoding word representation is simply replaced with its corresponding 50-dimensional embedding vector. The authors of (Wu et al., 2015) compared three strategies for deriving distributed word representations in a corpus of English clinical texts. They base their method on earlier work (Guo et al., 2014), in which three different"
2021.latechclfl-1.1,Q17-1010,0,0.241591,"ermore, it tells us something about how the preferences for certain media develop over time. The broader context of this study is to explore whether the use of computational methods can help historians to analyse the data concerning the mediascape of early modern chroniclers.1 Section 2 presents related work, Section 3 describes the data, Section 4 the methods, Section 5 the experiments, and Section 6 the results. Finally, in Section 7 we put forward some conclusions. 2 els) have recently become a popular way to describe a corpus of texts. Well known implementations are word2vec and fasttext (Bojanowski et al., 2017). Regarding the Dutch language, they have been used for relation evaluation and dialect identification (Tulkens et al., 2016), and to detect semantic change in Dutch newspapers (Wevers and Koolen, 2020; Wevers, 2019) and parliamentary debates (Lange, van and Futselaar, 2019). Word embeddings have been used as features in classification tasks (see for example Beelen et al. (2021)), but to the best of our knowledge, this has never been done for the historic Dutch language. Studies with corpora in other languages show that there are multiple ways to implement the word embeddings as features of a"
2021.latechclfl-1.1,D14-1012,0,0.0341772,"tasks (see for example Beelen et al. (2021)), but to the best of our knowledge, this has never been done for the historic Dutch language. Studies with corpora in other languages show that there are multiple ways to implement the word embeddings as features of a classifier. In Huang et al. (2015), every one hot encoding word representation is simply replaced with its corresponding 50-dimensional embedding vector. The authors of (Wu et al., 2015) compared three strategies for deriving distributed word representations in a corpus of English clinical texts. They base their method on earlier work (Guo et al., 2014), in which three different approaches for utilizing the embedding features are compared. In Wu et al. (2015) it is shown that the binarized word embedding features caused the largest improvement of the performance of the CRF system, while Guo et al. (2014) concluded that a distributional prototype approach performed the best. Bansal et al. (2014) perform agglomerative hierarchical clustering of the embedding vectors to take into account all dimensions of a vector simultaneously. Daˇrena and Süss (2020) use a neural network that is allowed to update the word vectors during training, since their"
2021.latechclfl-1.1,L16-1652,0,0.0254642,"s to explore whether the use of computational methods can help historians to analyse the data concerning the mediascape of early modern chroniclers.1 Section 2 presents related work, Section 3 describes the data, Section 4 the methods, Section 5 the experiments, and Section 6 the results. Finally, in Section 7 we put forward some conclusions. 2 els) have recently become a popular way to describe a corpus of texts. Well known implementations are word2vec and fasttext (Bojanowski et al., 2017). Regarding the Dutch language, they have been used for relation evaluation and dialect identification (Tulkens et al., 2016), and to detect semantic change in Dutch newspapers (Wevers and Koolen, 2020; Wevers, 2019) and parliamentary debates (Lange, van and Futselaar, 2019). Word embeddings have been used as features in classification tasks (see for example Beelen et al. (2021)), but to the best of our knowledge, this has never been done for the historic Dutch language. Studies with corpora in other languages show that there are multiple ways to implement the word embeddings as features of a classifier. In Huang et al. (2015), every one hot encoding word representation is simply replaced with its corresponding 50-d"
2021.latechclfl-1.1,W18-3917,0,0.0286283,"Missing"
2021.latechclfl-1.1,W19-4712,0,0.0280133,"rning the mediascape of early modern chroniclers.1 Section 2 presents related work, Section 3 describes the data, Section 4 the methods, Section 5 the experiments, and Section 6 the results. Finally, in Section 7 we put forward some conclusions. 2 els) have recently become a popular way to describe a corpus of texts. Well known implementations are word2vec and fasttext (Bojanowski et al., 2017). Regarding the Dutch language, they have been used for relation evaluation and dialect identification (Tulkens et al., 2016), and to detect semantic change in Dutch newspapers (Wevers and Koolen, 2020; Wevers, 2019) and parliamentary debates (Lange, van and Futselaar, 2019). Word embeddings have been used as features in classification tasks (see for example Beelen et al. (2021)), but to the best of our knowledge, this has never been done for the historic Dutch language. Studies with corpora in other languages show that there are multiple ways to implement the word embeddings as features of a classifier. In Huang et al. (2015), every one hot encoding word representation is simply replaced with its corresponding 50-dimensional embedding vector. The authors of (Wu et al., 2015) compared three strategies for"
C18-1078,P11-1059,0,0.0277004,"pendent negation marker is used to express negation (i.e. no [‘no/not’], nunca [‘never’]). – Lexical negation, if a word is used whose meaning has a negative component (i.e. negar [‘deny’], desistir [‘desist’]). – Morphological negation, if a morpheme is used to express the negation (i.e. i- in ilegal [‘illegal’], in in incoherente [‘incoherent’]). It is also known as affixal negation. • Scope: the part of the sentence affected by the negation cue (Vincze et al., 2008). The scope can be continuous or discontinuous. • Focus: the part of the scope that is most prominently or explicitly negated (Blanco and Moldovan, 2011). • Negated event: the event that is directly negated by the negation cue, usually a verb, a noun or an adjective (Kim et al., 2008). This is just a list of the main aspects that have been annotated for negation. However, each language has specific linguistic resources to express negation and specific negation structures, which should also be reflected in the information annotated in corpora. As we will show in Section 4, most existing annotation schemes for Spanish do not account for the complexity of the linguistic structures used to express negation that are present in texts. This happens m"
C18-1078,W17-1808,1,0.835932,"Missing"
C18-1078,W16-5006,1,0.723098,"Missing"
C18-1078,konstantinova-etal-2012-review,0,0.0530509,"Missing"
C18-1078,W17-1807,0,0.145006,"Missing"
C18-1078,morante-daelemans-2012-conandoyle,1,0.886629,"egated events and 0.94 for scopes.11 A detailed discussion of the main sources of disagreements can be found in (Jim´enez-Zafra et al., 2016). The guidelines of the Bioscope corpus were taken into account but after a thorough analysis of negation in Spanish, a typology of Spanish negation patterns was defined (Mart´ı et al., 2016). As in Bioscope, NegDDI-DrugBank and UAM Spanish Treebank, negation markers were included within the scope. Moreover, the subject was also included within the scope when the word directly affected by negation is the verb of the sentence, as in ConanDoyle-neg corpus (Morante and Daelemans, 2012). The event was also included in the scope of negation as in ConanDoyle-neg corpus. The SFU ReviewSP -NEG is publicly available and can be downloaded at http://sinai.ujaen. es/sfu-review-sp-neg-2/. 3.4 UHU-HUVR The UHU-HUVR (Cruz D´ıaz et al., 2017) is the first Spanish corpus in which affixal negation is annotated. It is composed of 604 clinical reports from the Virgen del Roc´ıo Hospital in Seville (Spain). 276 of this clinical documents correspond to radiology reports and 328 to the personal history of anamnesis reports written in free text. In this corpus, all types of negation were annota"
C18-1078,W08-0606,0,0.0931952,"Missing"
C18-1078,taboada-etal-2006-methods,0,0.0817691,"Missing"
C18-1078,W10-3105,0,0.0612608,"Missing"
C18-1191,W12-3808,0,0.105199,"e approaches to the annotation of focus and different definitions of the corresponding task. Blanco and Moldovan (2012) introduce the concept of granularity of focus and add fine-grained foci on top of the course-grained foci in PB - FOC; whereas coarse-grained focus includes all words belonging to a semantic role, fine-grained focus comprises fewer words within the semantic role (e.g. We didn’t get [an offer for more than $40]FOCUS ), allowing for more specific implicit positive interpretations (“we got something, but not an offer for more than $40” versus “we got an offer for $40 or less”). Anand and Martell (2012) evaluated the annotations of PB - FOC, arguing that positive interpretations resulting from scalar implicatures and neg-raising predicates should be separated from those (indirectly) resulting from focus, and reannotated the corpus by using an alternative annotation approach that relies on relevant questions under discussion (QUDs). Another criticism on PB - FOC was raised by Blanco and Sarabi (2016), who point out that the guidelines required the annotators to choose one semantic role as the focus, prioritizing “the one that yields the 2254 most meaningful implicit [positive] information” in"
C18-1191,P11-1059,0,0.167156,"ent on our test set and the ablation tests on feature combinations against the baseline. In Section 5, we redefine the task as a classification task and apply an error analysis to understand system performance against the baseline and the role of the different features in the classification task. Our error analysis leads to a discussion reported in Section 6 and plans for future work. Section 7 summarizes our conclusions. 2 Related Work The task of scoring implicit positive meanings from negated statements was preceded by the task of detecting the focus of negation. This task was pioneered by Blanco and Moldovan (2011), who argued that the scope and focus of negation are crucial for a correct interpretation of negated statements. Following Huddleston and Pullum (2002), they defined scope as “the part of the meaning that is negated” and focus as “the part of the scope that is most prominently or explicitly negated.” More specifically, according to Blanco and Moldovan (2011), the focus of negation gives rise to implicit positive meaning. To capture this information, they created the PB - FOC corpus, which contains annotations for the focus of negation in the 3,993 verbal negations in PropBank (Palmer et al.,"
C18-1191,N12-1050,0,0.0227298,"s using a set of features derived from gold syntactic annotation and semantic role labels. Whereas both CLaCs NegFocus and FOC - DET use only information from within the sentence, Zou et al. (2015) argue that contextual discourse information plays a critical role in negation focus identification and propose a word-topic graph model that uses this contextual information from both lexical and topical perspectives. They report an accuracy of 69.39 on PB - FOC. PB - FOC has given rise to various alternative approaches to the annotation of focus and different definitions of the corresponding task. Blanco and Moldovan (2012) introduce the concept of granularity of focus and add fine-grained foci on top of the course-grained foci in PB - FOC; whereas coarse-grained focus includes all words belonging to a semantic role, fine-grained focus comprises fewer words within the semantic role (e.g. We didn’t get [an offer for more than $40]FOCUS ), allowing for more specific implicit positive interpretations (“we got something, but not an offer for more than $40” versus “we got an offer for $40 or less”). Anand and Martell (2012) evaluated the annotations of PB - FOC, arguing that positive interpretations resulting from sc"
C18-1191,N16-1169,0,0.0802587,"ecting implicit positive meaning from negated statements. Such a system could support a range of Natural Language Processing (NLP) technologies that require deep understanding of language, such as Recognizing Textual Entailment (RTE) and Question Answering (QA). The research that we reproduce This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2253 Proceedings of the 27th International Conference on Computational Linguistics, pages 2253–2264 Santa Fe, New Mexico, USA, August 20-26, 2018. is that of Blanco and Sarabi (2016), who propose an interesting methodology to automatically generate positive interpretations from negated statements and score them according to their likelihood. We reflect on the definition of the task and the contribution of the different features to the performance of their system. Our findings show that the features they propose are not able to improve upon our baseline because of class imbalance. Based on an error analysis that we perform on the output of our baseline, we discuss future lines of research that take levels of informativeness into account, which we expect to be also applicab"
C18-1191,N06-2015,0,0.0102531,"their likelihood (see Section 3 for more details). Similar to the distinction between fine-grained and coarse-grained foci, Sarabi and Blanco (2016) propose to extract and score more fine-grained positive interpretations by manipulating syntactic dependencies instead of semantic roles. Sanders and Blanco (2016) further extend this approach of generating and scoring implicit positive interpretations by applying it to modal constructions. 3 Dataset and Task The dataset created by Blanco and Sarabi (2016) contains 1,888 positive interpretations generated from 600 negated verbs in OntoNotes 5.0 (Hovy et al., 2006). These positive interpretations are automatically generated by first converting the negated statement into its positive counterpart by (1) removing the negation mark, (2) removing auxiliaries, expanding contractions and fixing third-person singular and past tense, and (3) rewriting negatively-oriented polarity-sensitive items (e.g. anyone becomes someone). From this positive counterpart, positive interpretations are generated by rewriting each semantic role or the (originally negated) verb. For example, ARG0-ARG4 are rewritten as someone / some people / something, and ARGM-TMP is rewritten as"
C18-1191,S12-1035,1,0.826675,"d.” More specifically, according to Blanco and Moldovan (2011), the focus of negation gives rise to implicit positive meaning. To capture this information, they created the PB - FOC corpus, which contains annotations for the focus of negation in the 3,993 verbal negations in PropBank (Palmer et al., 2005). Candidates for focus annotation were the verb itself or any of its semantic roles and annotators were instructed to choose only one. The PB - FOC corpus was used in the first edition of the *SEM Shared Task, which was dedicated to resolving the scope (Task 1) and focus (Task 2) of negation (Morante and Blanco, 2012). Only one team (Rosenberg and Bergler, 2012) participated in the Focus Detection task. Their system, called CLaCs NegFocus, is rule-based and consists of three components: the identification of explicit negation cues (not, nor and never), the detection of the syntactic scope of negation, and the detection of the focus of negation using a set of four syntactic heuristics (e.g. “if an adverb directly precedes the negated verb and is connected through an advmod dependency relation to the negated verb, this adverb is annotated as the focus”). They report an F-measure of 0.584. Blanco and Moldovan"
C18-1191,J05-1004,0,0.0411398,"Moldovan (2011), who argued that the scope and focus of negation are crucial for a correct interpretation of negated statements. Following Huddleston and Pullum (2002), they defined scope as “the part of the meaning that is negated” and focus as “the part of the scope that is most prominently or explicitly negated.” More specifically, according to Blanco and Moldovan (2011), the focus of negation gives rise to implicit positive meaning. To capture this information, they created the PB - FOC corpus, which contains annotations for the focus of negation in the 3,993 verbal negations in PropBank (Palmer et al., 2005). Candidates for focus annotation were the verb itself or any of its semantic roles and annotators were instructed to choose only one. The PB - FOC corpus was used in the first edition of the *SEM Shared Task, which was dedicated to resolving the scope (Task 1) and focus (Task 2) of negation (Morante and Blanco, 2012). Only one team (Rosenberg and Bergler, 2012) participated in the Focus Detection task. Their system, called CLaCs NegFocus, is rule-based and consists of three components: the identification of explicit negation cues (not, nor and never), the detection of the syntactic scope of n"
C18-1191,W11-1901,0,0.0519411,"Missing"
C18-1191,S12-1039,0,0.0193194,"and Moldovan (2011), the focus of negation gives rise to implicit positive meaning. To capture this information, they created the PB - FOC corpus, which contains annotations for the focus of negation in the 3,993 verbal negations in PropBank (Palmer et al., 2005). Candidates for focus annotation were the verb itself or any of its semantic roles and annotators were instructed to choose only one. The PB - FOC corpus was used in the first edition of the *SEM Shared Task, which was dedicated to resolving the scope (Task 1) and focus (Task 2) of negation (Morante and Blanco, 2012). Only one team (Rosenberg and Bergler, 2012) participated in the Focus Detection task. Their system, called CLaCs NegFocus, is rule-based and consists of three components: the identification of explicit negation cues (not, nor and never), the detection of the syntactic scope of negation, and the detection of the focus of negation using a set of four syntactic heuristics (e.g. “if an adverb directly precedes the negated verb and is connected through an advmod dependency relation to the negated verb, this adverb is annotated as the focus”). They report an F-measure of 0.584. Blanco and Moldovan (2013) report an F-measure of 0.641 with the"
C18-1191,D16-1118,0,0.0135375,"aningful implicit [positive] information” in case of multiple candidates, but that it is not specified what “most meaningful” means. Therefore, they designed a new annotation task where several positive interpretations per negation (automatically generated by manipulating semantic roles) are scored according to their likelihood (see Section 3 for more details). Similar to the distinction between fine-grained and coarse-grained foci, Sarabi and Blanco (2016) propose to extract and score more fine-grained positive interpretations by manipulating syntactic dependencies instead of semantic roles. Sanders and Blanco (2016) further extend this approach of generating and scoring implicit positive interpretations by applying it to modal constructions. 3 Dataset and Task The dataset created by Blanco and Sarabi (2016) contains 1,888 positive interpretations generated from 600 negated verbs in OntoNotes 5.0 (Hovy et al., 2006). These positive interpretations are automatically generated by first converting the negated statement into its positive counterpart by (1) removing the negation mark, (2) removing auxiliaries, expanding contractions and fixing third-person singular and past tense, and (3) rewriting negatively-"
C18-1191,D16-1119,0,0.0142649,"rabi (2016), who point out that the guidelines required the annotators to choose one semantic role as the focus, prioritizing “the one that yields the 2254 most meaningful implicit [positive] information” in case of multiple candidates, but that it is not specified what “most meaningful” means. Therefore, they designed a new annotation task where several positive interpretations per negation (automatically generated by manipulating semantic roles) are scored according to their likelihood (see Section 3 for more details). Similar to the distinction between fine-grained and coarse-grained foci, Sarabi and Blanco (2016) propose to extract and score more fine-grained positive interpretations by manipulating syntactic dependencies instead of semantic roles. Sanders and Blanco (2016) further extend this approach of generating and scoring implicit positive interpretations by applying it to modal constructions. 3 Dataset and Task The dataset created by Blanco and Sarabi (2016) contains 1,888 positive interpretations generated from 600 negated verbs in OntoNotes 5.0 (Hovy et al., 2006). These positive interpretations are automatically generated by first converting the negated statement into its positive counterpar"
C18-1191,D15-1187,0,0.0139022,"of the focus of negation using a set of four syntactic heuristics (e.g. “if an adverb directly precedes the negated verb and is connected through an advmod dependency relation to the negated verb, this adverb is annotated as the focus”). They report an F-measure of 0.584. Blanco and Moldovan (2013) report an F-measure of 0.641 with their system FOC - DET, which is trained with bagging over standard C4.5 decision trees using a set of features derived from gold syntactic annotation and semantic role labels. Whereas both CLaCs NegFocus and FOC - DET use only information from within the sentence, Zou et al. (2015) argue that contextual discourse information plays a critical role in negation focus identification and propose a word-topic graph model that uses this contextual information from both lexical and topical perspectives. They report an accuracy of 69.39 on PB - FOC. PB - FOC has given rise to various alternative approaches to the annotation of focus and different definitions of the corresponding task. Blanco and Moldovan (2012) introduce the concept of granularity of focus and add fine-grained foci on top of the course-grained foci in PB - FOC; whereas coarse-grained focus includes all words bel"
D08-1075,W06-2920,0,0.0134564,"chez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2 . Additionally, we converted the annotation about scope of negation into a token-per-token representation. Table 1 shows an example sentence of the corpus that results from converting and processing the BioScope representation. Following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of tokens, each one starting on a new line. A token consists of the following 10 fields: 1. ABSTRACT ID: number of the GENIA abstract. 2. SENTENCE ID: sentence counter starting at 1 for each new abstract. 3. TOKEN ID: token counter, starting at 1 for each new sentence. 4. FORM: word form or punctuation symbol. 5. LEMMA: lemma of word form. 6. POS TAG: Penn Treebank part-of-speech tags described in (Santorini, 1990). 7. CHUNK TAG: IOB (Inside, Outside, Begin) tags produced by the GE"
D08-1075,E99-1043,0,0.0342535,"the non-existence or uncertainty of something are annotated, in contrast to other corpora where only sentences of interest in the domain are annotated. A second characteristic is that the annotation is extended to the biggest syntactic unit possible so that scopes have the maximal length. In (2) below, negation signal no scopes over primary impairment of glucocorticoid metabolism instead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics. 1 The part used in our experiments are the biological paper abstracts from the GENIA corpus (Collier et al., 1999). This part consists of 11,872 sentences in 1,273 abstracts. We automatically discarded five sentences due to annotation errors. The total number of words used is 313,222, 1,739 of which are negation signals that belong to the different types described in (Sanchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2 . Additionally, we converted"
D08-1075,W08-2128,1,0.867313,"Missing"
D08-1075,D07-1096,0,0.0131332,"Missing"
D08-1075,W95-0107,0,0.0128028,"have been obtained by converting the xml files of BioScope. Each token can have one or more NEG SCOPE tags, depending on the number of negation signals in the sentence. 4 Task description We approach the scope finding task as a classification task that consists of classifying the tokens of a sentence as being a negation signal or not, and as being inside or outside the scope of the negation signal(s). This happens as many times as there are 718 negation signals in the sentence. Our conception of the task is inspired by Ramshaw and Marcus’ representation of text chunking as a tagging problem (Ramshaw and Marcus, 1995) . The information that can be used to train the system appears in columns 1 to 8 of Table 1. The information to be predicted by the system is contained in columns 9 and 10. As far as we know, approaching the negation scope finding task as a token per token classification task is novel, whereas at the same time it conforms to the well established standards of the recent CoNLL Shared Tasks3 on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) and semantic role labeling (Surdeanu et al., 2008). By setting up the task in this way we show that the negation scope finding task can be"
D08-1075,W08-2121,0,0.0271576,"Missing"
D08-1075,W08-0606,0,0.263472,"Missing"
D08-1075,H05-1059,0,0.0188566,"ucocorticoid metabolism instead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics. 1 The part used in our experiments are the biological paper abstracts from the GENIA corpus (Collier et al., 1999). This part consists of 11,872 sentences in 1,273 abstracts. We automatically discarded five sentences due to annotation errors. The total number of words used is 313,222, 1,739 of which are negation signals that belong to the different types described in (Sanchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2 . Additionally, we converted the annotation about scope of negation into a token-per-token representation. Table 1 shows an example sentence of the corpus that results from converting and processing the BioScope representation. Following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sentences are separated by a blank line and fields are separated by a single"
D12-1053,U10-1012,0,0.0347457,"Missing"
D12-1053,W03-1310,0,0.0193458,"Missing"
D12-1053,D08-1068,0,0.0172336,"n terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employs kernel based methods grounded in statistical learning theory, and 2) it employs a Prolog like language for defining and using background knowledge. As Prolog is a programming language, this is more flexible that the formalism used by Markov Logic. 581 3 Methodology In learning from examples, or interpretations (De Raedt et al., 2008), the instances are sampled identically and independently from some unknown but fixed distribution. They can be represented as pairs z = (x, y), in which x represents the inp"
D12-1053,W08-2125,0,0.0105785,"al articles. In this case areas of text are classified in terms of the rhetorical categories Introduction, Methods, Results and Discussion (IMRAD) (Agarwal and Yu, 2009) or richer categories, such as problem-setting or insight (Mizuta et al., 2006). There exists a wide range of statistical relational learning systems (Getoor and Taskar, 2007; De Raedt et al., 2008), and many of these systems are in principle useful for natural language processing. The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). With respect to Markov Logic, two distinguishing features of kLog are that 1) it employs kernel based methods grounded in statistical learning theory, and 2) it employs a Prolog like language for defining and using background knowledge. As Prolog is a programming language, this is more flexible that the formalism used by Markov Logic. 581 3 Methodology In learning from examples, or interpretations (De Raedt et al., 2008), the instances are sampled identically and independently from some unknown but fixed distribution. They can be represent"
D12-1053,D07-1111,0,0.0298776,"Missing"
D12-1053,J07-1005,0,\N,Missing
J12-2001,dalianis-velupillai-2010-certain,0,0.0252184,"/rgai/bioscope. Last accessed on 8 December 2011. 237 Computational Linguistics Volume 38, Number 2 levels of certainty are deﬁned: low confidence or considerable speculation, high confidence or slight speculation, and no expression of uncertainty or speculation. Information about negation is encoded in the LEXICAL POLARITY dimension, which identiﬁes negated events. Negation is deﬁned here as “the absence or non-existence of an entity or a process.” For languages other than English there are much fewer resources. A corpus of 6,740 sentences from the Stockholm Electronic Patient Record Corpus (Dalianis and Velupillai 2010) has been annotated with certain and uncertain expressions as well as speculative and negation cues, with the purpose of creating a resource for the development of automatic detection of speculative language in Swedish clinical text. The categories used are: certain, uncertain, and undefined at sentence level, and negation, speculative words, and undefined speculative words at token level. Inter-annotator agreement for certain sentences and negation are high, but for the rest of the classes results are lower. 5. Detection of Speculative Sentences Initial work on processing speculation focuses"
J12-2001,P08-1118,0,0.0290135,"Missing"
J12-2001,W09-3012,0,0.0577069,"nd the predicate 245 Computational Linguistics Volume 38, Number 2 that is within their scope (target). They describe two modality taggers that identify modality cues and modality targets, a string-based tagger and a structure-based tagger, and compare their performances. The string-based tagger takes as input text tagged with PoS and marks as modality cues words or phrases that match exactly cues from a modality lexicon. More information about the modality taggers and their application in machine translation can be found in the article by Baker et al. included in this special issue. Finally, Diab et al. (2009) model belief categorization as a sequence labeling task, which allows them to treat cue detection and scope recognition in a uniﬁed fashion. Diab et al. distinguish three belief categories. For committed belief the writer indicates clearly that he or she believes a proposition. In the case of non-committed belief the writer identiﬁes the proposition as something in which he or she could believe but about which the belief is not strong. This category is further subdivided into weak belief, which is often indicated by modals, such as may, and reported speech. The ﬁnal category, not applicable,"
J12-2001,W08-0607,0,0.0495649,"lection procedure that allows a reduction of the number of keyword candidates from 2,407 to 253. The system is trained on the data set of Medlock and Briscoe and evaluated on four newly annotated biomedical full articles8 and on radiology reports. The best results of the system are achieved by performing automatic and manual feature selection consecutively and by adding external dictionaries. The ﬁnal results on biomedical articles are 85.29 BEP and 85.08 F1 score. The results for the external corpus of radiology reports are lower, at 82.07 F1 score. A different type of system is presented by Kilicoglu and Bergler (2008), who apply a linguistically motivated approach to the same classiﬁcation task by using knowledge from existing lexical resources and incorporating syntactic patterns, including unhedgers, lexical cues, and patterns that strongly suggest non-speculation. Additionally, hedge cues are weighted by automatically assigning an information gain measure to them and by assigning weights semi-automatically based on their types and centrality to hedging. The hypothesis behind this approach is that “a more linguistically oriented approach can enhance recognition of speculative language.” The results are e"
J12-2001,W09-1418,0,0.0226542,"Missing"
J12-2001,W09-1401,0,0.0763386,"Missing"
J12-2001,C04-1200,0,0.0171358,"polarity of an expression (Kennedy and Inkpen 2006; Polanyi and Zaenen 2006), or by introducing speciﬁc negation features (Wilson, Wiebe, and Hoffman 2005; Wilson, Wiebe, and Hwa 2006; Wilson 2008). It was found that these more sophisticated models typically lead to a signiﬁcant improvement over a simple bag-of-words model with negation preﬁxes. This improvement can to a large extent be directly attributed to the better modeling of negation (Wilson, Wiebe, and Hoffman 2009). Whereas modeling negation in opinion mining frequently involves determining the polarity of opinions (Hu and Liu 2004; Kim and Hovy 2004; Wilson, Wiebe, and Hoffman 2005; Wilson 2008), some researchers have also used negation models to 14 The three terms are used sometimes interchangeably and sometimes reserved for somewhat different contexts. We follow here the deﬁnitions of Pang and Lee (2008) who use “opinion mining” and “sentiment analysis” as largely synonymous terms and “subjectivity analysis” as a cover term for both. 251 Computational Linguistics Volume 38, Number 2 determine the strength of opinions (Popescu and Etzioni 2005; Wilson, Wiebe, and Hwa 2006). Choi and Cardie (2010) found that performing both tasks jointly"
J12-2001,W04-3103,0,0.63478,"Missing"
J12-2001,matsuyoshi-etal-2010-annotating,0,0.0713208,"SSP is underlined. 234 Morante and Sporleder Modality and Negation for the same event are found in Example (16). Discriminatory co-predication tests are provided for the annotators to determine the factuality of events. The interannotator agreement reported for assigning factuality values is κcohen 0.81. (15) John knows whether Mary came. (16) a. John does not know whether Mary came. b. John does not know that Mary came. c. John knows that Paul said that Mary came. A corpus of 50,108 event mentions in blogs and Web posts in Japanese has been annotated with information about extended modality (Matsuyoshi et al. 2010). The annotation scheme of extended modality is based on four desiderata: information should be assigned to the event mention; the modality system has to be language independent; polarity should be divided into two classes: POLARITY ON THE ACTUALITY of the event and SUBJECTIVE POLARITY from the perspective of the source’s evaluation; and the annotation labels should not be too ﬁne-grained. In Example (17) the polarity on actuality is negative for the events STUDY and PASS because they did not occur, but the subjective polarity for the PASS event is positive. Extended modality is characterized"
J12-2001,P07-1125,0,0.755745,"peculative is not. The annotation work by Wilbur, Rzhetsky, and Shatkay (2006) is motivated by the need to identify and characterize parts of scientiﬁc documents where reliable information can be found. They deﬁne ﬁve dimensions to characterize scientiﬁc sentences: FOCUS (scientific versus general), POLARITY (positive versus negative statement), LEVEL OF CERTAINTY in the range 0–3, STRENGTH of evidence, and DIRECTION / TREND (increase or decrease in certain measurement). A corpus5 of six articles from the functional genomics literature has been annotated at the sentence level for speculation (Medlock and Briscoe 2007). Sentences are annotated as being speculative or not. Of the 1,157 sentences, 380 were found to be speculative. An inter-annotator agreement of 0.93 κcohen is reported. BioInfer (Pyysalo et al. 2007) is a corpus of 1,100 sentences from abstracts of biomedical research articles annotated with protein, gene, and RNA relationships. The annotation scheme captures information about the absence of a relation. Statements expressing absence of a relation such as not affected by or independent of are annotated using a predicate NOT, as in this example: not:NOT(affect:AFFECT(deletion of SIR3, silencing"
J12-2001,morante-2010-descriptive,1,0.823267,"cal opacity]. b. This result [suggests that the valency of Bi in the material is smaller than +3]. Because the Genia Event and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al. (2010). Their study shows that the scopes of BioScope are not directly useful to detect the certainty status of the events in Genia, and that the BioScope annotation is more easily adaptable to non-biomedical applications. A description of negation cues and their scope in biomedical texts, based on the cues that occur in the BioScope corpus, can be found in Morante (2010), where information is provided relative to the ambiguity of the negation cue and to the type of scope, as well as examples. The description shows that the scope depends mostly on the PoS of the cue and on the syntactic features of the clause. The NaCTeM team has annotated events in biomedical texts with meta-knowledge that includes polarity and modality (Thompson et al. 2008). The modality categorization scheme covers epistemic modality and speculation and contains information about the following dimensions: KNOWLEDGE TYPE, LEVEL OF CERTAINTY, and POINT OF VIEW. Four types of knowledge are de"
J12-2001,W09-1304,1,0.924573,"presentation of text chunking as a tagging problem and by the standard CoNLL representation format (Buchholz and Marsi 2006). By setting up the task in this way they show that the task can be modeled as a sequence labeling problem, and by conforming to the existing CoNLL standards they show that scope resolution could be integrated in a joint learning setting with dependency parsing and semantic role labeling. Their system is a memory-based scope ﬁnder that tackles the task in two phases: cue identiﬁcation and scope resolution, which are modeled as consecutive token level classiﬁcation tasks. Morante and Daelemans (2009b) present another scope resolution system that uses a different architecture, can deal with multiword negation cues, and is tested on the three subcorpora of the BioScope corpus. For resolving the scope, three classiﬁers (kNN, SVM, CRF++) predict whether a token is the ﬁrst token in the scope sequence, the last, or neither. A fourth classiﬁer is a metalearner that uses the predictions of the 246 Morante and Sporleder Modality and Negation three classiﬁers to predict the scope classes. The system is evaluated on three corpora using as measure the percentage of fully correct scopes (PCS), which"
J12-2001,W09-1105,1,0.898782,"presentation of text chunking as a tagging problem and by the standard CoNLL representation format (Buchholz and Marsi 2006). By setting up the task in this way they show that the task can be modeled as a sequence labeling problem, and by conforming to the existing CoNLL standards they show that scope resolution could be integrated in a joint learning setting with dependency parsing and semantic role labeling. Their system is a memory-based scope ﬁnder that tackles the task in two phases: cue identiﬁcation and scope resolution, which are modeled as consecutive token level classiﬁcation tasks. Morante and Daelemans (2009b) present another scope resolution system that uses a different architecture, can deal with multiword negation cues, and is tested on the three subcorpora of the BioScope corpus. For resolving the scope, three classiﬁers (kNN, SVM, CRF++) predict whether a token is the ﬁrst token in the scope sequence, the last, or neither. A fourth classiﬁer is a metalearner that uses the predictions of the 246 Morante and Sporleder Modality and Negation three classiﬁers to predict the scope classes. The system is evaluated on three corpora using as measure the percentage of fully correct scopes (PCS), which"
J12-2001,D08-1075,1,0.923051,"Missing"
J12-2001,W10-3006,1,0.910251,"Missing"
J12-2001,W10-3112,0,0.195715,"Missing"
J12-2001,C10-1155,0,0.0417093,"Missing"
J12-2001,D09-1145,0,0.0667183,"Missing"
J12-2001,W02-1011,0,0.0197905,"Missing"
J12-2001,P09-1077,0,0.0330926,"Missing"
J12-2001,H05-1043,0,0.0225137,"Missing"
J12-2001,C10-2117,0,0.0342491,"Missing"
J12-2001,W06-0305,0,0.0595174,"Missing"
J12-2001,prasad-etal-2008-penn,0,0.057837,"Missing"
J12-2001,W95-0107,0,0.0504303,"Missing"
J12-2001,D08-1002,0,0.0155781,", which varies depending on whether the negated object is an event, an entity, or a state. For events, the negation is assumed to scope over the whole predicate–argument structure. For entities and for states realized by nominalizations the negation is assumed to scope over the whole NP. Implicit negations are detected by searching for antonymy chains in WordNet. de Marneffe, Rafferty, and Manning (2008) also make use of negation detection to discover contradictions. They do so rather implicitly, however, by using a number of features which check for explicit negation, polarity, and antonymy. Ritter et al. (2008) present a contradiction detection system that uses the T EXT R UNNER system (Banko et al. 2007) to extract relations of the form R(x,y) (e.g., was born in(Mozart,Salzburg)). They then inspect potential contradictions (i.e., relations which overlap in one variable but not in the other) and ﬁlter out non-contradictions by looking, for example, for synonyms and meronyms. In the context of contrast detection in discourse processing, negation detection is rarely used as an explicit step. An exception is Kim et al. (2006), who are concerned with discovering contrastive information about protein int"
J12-2001,N07-2036,0,0.0423535,"Missing"
J12-2001,S10-1008,1,0.792855,"ieve P?). The annotation guidelines to annotate the modalities are deﬁned in Baker et al. (2009). 3 Web site of the modality lexicon: http://www.umiacs.umd.edu/∼bonnie/ModalityLexicon.txt. Last accessed on 8 December 2011. 235 Computational Linguistics Volume 38, Number 2 The scope of negation has been annotated on a corpus of Conan Doyle stories (Morante, Schrauwen, and Daelemans 2011)4 (The Hound of the Baskervilles and The Adventure of Wisteria Lodge), which have also been annotated with coreference and semantic roles for the SemEval Task Linking Events and Their Participants in Discourse (Ruppenhofer et al. 2010). As for negation, the corpus is annotated with negation cues and their scope in a way similar to the BioScope corpus (Vincze et al. 2008) described subsequently, and in addition negated events are also marked, if they occur in factual statements. Blanco and Moldovan (2011) take a different approach by annotating the focus, “that part of the scope that is most prominently or explicitly negated,” in the 3,993 verbal negations signaled with MNEG in the PropBank corpus. According to the authors, the annotation of the focus allows the derivation of the implicit positive meaning of negated statemen"
J12-2001,W10-3113,0,0.0406808,"Missing"
J12-2001,W10-1103,0,0.0291232,"Missing"
J12-2001,N06-1005,0,0.0848299,"Missing"
J12-2001,W10-2102,0,0.0554897,"Missing"
J12-2001,P08-1033,0,0.187289,"on interact with mood and tense markers, and also with each other. Finally, discourse factors also add to the complexity of these phenomena. Incorporating information about modality and negation has been shown to be useful for a number of applications such as recognizing textual entailment (de Marneffe et al. 2006; Snow, Vanderwende, and Menezes 2006; Hickl and Bensley 2007), machine translation (Baker et al. 2010), trustworthiness detection (Su, Huang, and Chen 2010), classiﬁcation of citations (Di Marco, Kroon, and Mercer 2006), clinical and biomedical text processing (Friedman et al. 1994; Szarvas 2008), and identiﬁcation of text structure (Grabar and Hamon 2009). This overview is organized as follows: Sections 2 and 3 deﬁne modality and negation, respectively. Section 4 gives details of linguistic resources annotated with various aspects of negation and modality. We also discuss properties of the different annotation schemes that have been proposed. Having discussed the linguistic basis as well as the available resources, the remainder of the article then provides an overview of automated methods for dealing with modality and negation. Most of the work in this area has been carried out at t"
J12-2001,W10-3002,0,0.0240341,"words in Wikipedia: http://en.wikipedia.org/wiki/Weasel word. Last accessed on 8 December 2011. 10 Wikipedia instructions about weasel words are available at http://simple.wikipedia.org/wiki/ Wikipedia:Avoid weasel words. Last accessed on 8 December 2011. 240 Morante and Sporleder Modality and Negation top-ranked systems for Wikipedia data follow a bag-of-words approach. None of the top-ranked systems uses features derived from syntactic parsing. The best system for Wikipedia data (Georgescul 2010) implements an SVM and obtains an F1 score of 60.2, whereas the best system for biological data (Tang et al. 2010) incorporates conditional random ﬁelds (CRF) and obtains an F1 score of 86.4. As a follow-up of the CoNLL Shared Task, Velldal (2011) proposes to handle the hedge detection task as a simple disambiguation problem, restricted to the words that have previously been observed as hedge cues. This reduces the number of examples that need to be considered and the relevant feature space. Velldal develops a largemargin SVM classiﬁer based on simple sequence-oriented n-gram features collected for PoS-tags, lemmas, and surface forms. This system produces better results (86.64 F1 ) than the best system of"
J12-2001,W08-0606,0,0.781475,"Missing"
J12-2001,J94-2004,0,0.337443,"Missing"
J12-2001,J04-3002,0,0.124068,"Missing"
J12-2001,W10-3111,0,0.0846081,"ity is also not always entirely straightforward. For example, whereas negation can change the polarity of an expression from positive to negative (e.g., good vs. not good in Examples (32a) vs. (32b)) it can also shift negative polarity to neutral or even positive polarity (32c). (32) a. This is a good camera. b. This is not a good camera. c. This is by no means a bad camera. In this section, we discuss some approaches that make explicit use of negation in the context of sentiment analysis. For a recent general overview of work on sentiment analysis, we refer the reader to Pang and Lee (2008). Wiegand et al. (2010) present a survey of the role of negation in sentiment analysis. They indicate that it is necessary to perform ﬁne-grained linguistic analysis in order to extract features for machine learning or rule-based opinion analysis systems. The features allow the incorporation of information about linguistic phenomena such as negation (Wiegand et al. 2010, page 60). Early approaches made use of negation in a bag-of-words model by preﬁxing a word x with a negation marker if a negation word was detected immediately preceding x (Pang, Lee, and Vaithyanathan 2002). Thus x and NOT x were treated as two com"
J12-2001,H05-2018,0,0.0588463,"Missing"
J12-2001,H05-1044,0,0.0184724,"Missing"
J12-2001,J09-3003,0,0.0235573,"Missing"
J12-2001,D10-1070,0,0.0565376,"mpetitive performance. Øvrelid et al. report that the errors of their system are mostly of two classes: (i) failing to recognize phrase and clause boundaries, as in Example (28a), and (ii) not dealing successfully with 248 Morante and Sporleder Modality and Negation relatively superﬁcial properties of the text as in Example (28b). The scope boundaries produced by the system are marked with ‘’. (28) a. ... [the reverse complement mR of m will be considered to be ...]. b. This [might affect the results] if there is a systematic bias on the composition of a protein interaction set. Finally, Zhu et al. (2010) approach the scope learning problem via simpliﬁed shallow semantic parsing. The cue is regarded as the predicate and its scope is mapped into several constituents as the arguments of the cue. The system resolves the scope of negation and modality cues in the standard two phase approach. For cue identiﬁcation they apply an SVM that uses features from the surrounding words and from the structure of the syntax tree. The scope resolution task is different than in previous systems. The task is addressed in three consecutive phases: (1) argument pruning, consisting of collecting as argument candida"
J12-2001,baker-etal-2010-modality,0,\N,Missing
J12-2001,W06-2920,0,\N,Missing
J12-2001,W10-3004,0,\N,Missing
J12-2001,H05-2017,0,\N,Missing
J12-2001,W07-1428,0,\N,Missing
J12-2001,W10-3001,0,\N,Missing
J12-2001,W07-1401,0,\N,Missing
J12-2001,E99-1043,0,\N,Missing
J12-2001,W10-3110,0,\N,Missing
J12-2001,P11-1059,0,\N,Missing
J12-2001,P09-2044,0,\N,Missing
J12-2001,sauri-etal-2006-slinket,0,\N,Missing
J12-2001,W10-2900,0,\N,Missing
J12-2001,P10-2050,0,\N,Missing
L16-1187,P11-1059,0,0.0216625,"ent to annotate the event, as a representative of the whole proposition, as the target of factuality. This is because factuality cues can target specific relations within a proposition. To clarify, consider the following example, taken from FactBank: 1182 12 13 TimeBank/FactBank – APW19980227.476-S1 GEN AUTHOR denotes a non-explicit generic source. We call this phenomenon perspective scope, referring to those specific propositional relations associated with an event (or entity) that are affected by a perspective cue. It is strongly related to the scope and focus of negation as investigated by Blanco and Moldovan (2011), but to our knowledge its annotation has not been investigated before in the context of factuality (or sentiment). We believe that perspective scope is an important and innovative aspect of our annotation scheme, and our formal model GRaSP allows for the representation of separate factuality assignments for the event and its relations. In the near future, we will work out the details with respect to its annotation. 4.4. Opinion Layer The final annotation layer that we have included in our scheme is that of opinion or sentiment. As our annotations are largely based on Wiebe et al. (2005) and T"
L16-1187,N15-1146,0,0.0316723,"of factuality. In the factuality layer, each event identified in the event layer is to be annotated as the target of a factuality relation. In the opinion layer, annotators have no clear pre-defined targets; instead, they need to look for cues and understand the text in more detail. The first thing to look for are attributional cues, since they are fairly easy to recognize and some of them will already have been identified in the attribution layer. An example of such an attributional cue is support in our example sentence repeated below, which expresses a positive attitude of Mbeki. Following Deng and Wiebe (2015), we aim to identify the specific entities and events that are the target of the opinion. In this case, there are two targets: the entity denoted by Mugabe and the event expressed by elections. 10. Investors and Western diplomats have saide1 they might interprete2 {Mbeki’s}SENT- SOURCE {supporte3 }SENT- CUE for {Mugabe}SENT- TARGET or the {electionse4 }SENT- TARGET as a sign that Africa is not intent on revitalizinge5 its economies through good governmente6 and expanded international tradee7 . The other two types of cues are also present in our example. An example of a factual opinion cue is e"
L16-1187,W09-3012,0,0.198309,"events and their causes (e.g. conspiracy theories on 9/11). Textual data always provide specific perspectives of the author and quoted sources on the information they contain. Mining information from texts thus implies dealing with these perspectives. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives, each with its own approaches and goals. Targeted aspects of perspectives include, for example, attribution (Prasad et al., 2007; Pareti, 2012), factuality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009), and opinion (Wiebe et al., 2005). Coordinating initiatives such as the Unified Linguistic Annotation project1 have tried to technically combine such annotations into a unique annotation model, but they lack an overarching framework for the various layers of annotation from different resources. Furthermore, annotation initiatives such as those proposed by Prasad et al. (2007) and Pareti (2012) have attempted to tackle the annotation of perspectives in a unified approach, but with different levels of success.2 In our approach, the notion of perspectives lies at the semantic-pragmatic interface"
L16-1187,pareti-2012-database,0,0.1945,"ortion, vaccinations, etc.), and interpretative frames on events and their causes (e.g. conspiracy theories on 9/11). Textual data always provide specific perspectives of the author and quoted sources on the information they contain. Mining information from texts thus implies dealing with these perspectives. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives, each with its own approaches and goals. Targeted aspects of perspectives include, for example, attribution (Prasad et al., 2007; Pareti, 2012), factuality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009), and opinion (Wiebe et al., 2005). Coordinating initiatives such as the Unified Linguistic Annotation project1 have tried to technically combine such annotations into a unique annotation model, but they lack an overarching framework for the various layers of annotation from different resources. Furthermore, annotation initiatives such as those proposed by Prasad et al. (2007) and Pareti (2012) have attempted to tackle the annotation of perspectives in a unified approach, but with different levels of success.2 In our approach, the n"
L16-1187,S15-1009,0,0.0145949,"uting the description of the single ‘real’ event in this sentence expressed by demonstrations. In other words, only one event in this sentence should be put on a timeline. In this framework, syntax is only used to decide on the span of the event. The two main corpora for factuality (or belief) are FactBank (Saur´ı and Pustejovsky, 2009) and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). Although both corpora address the same phenomenon (i.e. the commitment of a source towards the truth of some event/proposition), the annotations are quite different (Werner et al., 2015; Prabhakaran et al., 2015). The LU Corpus so far has only addressed the problem from the perspective of the speaker/writer, in contrast to FactBank, which has fully annotated nested sources. Furthermore, the LU corpus ignores negation (the polarity axis of factuality in FactBank) and does not distinguish between POSSI BLE and PROBABLE (the certainty axis of factuality in FactBank). Finally, there is a subtle difference in the targets. Whereas FactBank has assigned factuality values to events, the LU corpus has assigned belief tags to the head words of propositions, disregarding event-denoting noun phrases (e.g. the col"
L16-1187,pustejovsky-etal-2010-iso,0,0.0225563,"d approach. Section 4 describes the four layers that we have currently defined for the an1177 notation of perspectives: events, attribution, factuality, and opinion. Finally, we conclude and summarize our work and give an outlook on future work in Section 5. 2. Related Work In our annotations, events play an important role because we consider them to be the basic semantic elements that may give rise to or be involved in perspectives. A wellknown specification language for events is TimeML (Pustejovsky et al., 2003a), which has been consolidated as an international cross-language ISO standard (Pustejovsky et al., 2010) and has been used as the annotation language for the TempEval shared task series (Verhagen et al., 2009). Its reference corpus is TimeBank (Pustejovsky et al., 2003b). TimeML defines an event as something that can be said to obtain or hold true, to happen or to occur. TimeML adopts a surface-based annotation of texts and morpho-syntactic information plays a key role for detecting all possible mentions of an event. According to TimeML, both demonstrations and taken (place) in Example 1 are to be annotated as valid event mentions. 1. Several pro-Iraq demonstrations have taken place in the last"
L16-1187,P15-5003,0,0.0165306,"e represented as: PARTY −−−−−−−−−→ MARY OrganizedBy Note that both Examples 3 and 4 mention the O RGA NIZED B Y relation between Mary and his birthday party; however, whereas in the former it functions merely as additional information on the target, in the latter it is the target of the perspective. The schematic representation in Figure 1 illustrates the differences between these targets. Not all attitude dimensions can take the same types of targets. For example, opinion can target entities, events or propositional relations, but factuality can only target events or propositional relations (Rambow and Wiebe, 2015). Whereas the source and target are usually expressed by a single linguistic unit (e.g. an NP or a clause), the attitude may be expressed either by a single linguistic cue or a combination of cues.6 For example, the commitment of a source towards the factual nature of an event or proposition may be expressed by a combination of polarity (e.g. not, never) and modality cues (e.g. could, maybe). In turn, one cue can express multiple attitude dimensions. For example, a verb like hope expresses positive sentiment and uncertainty towards the target at the same time. 5 If the author of a document is"
L16-1187,P10-1059,0,0.0384465,"), but to our knowledge its annotation has not been investigated before in the context of factuality (or sentiment). We believe that perspective scope is an important and innovative aspect of our annotation scheme, and our formal model GRaSP allows for the representation of separate factuality assignments for the event and its relations. In the near future, we will work out the details with respect to its annotation. 4.4. Opinion Layer The final annotation layer that we have included in our scheme is that of opinion or sentiment. As our annotations are largely based on Wiebe et al. (2005) and Toprak et al. (2010), the three main elements are defined as follows: • Source: The entity that has a positive or negative attitude towards some target. • Cue: A linguistic cue that, possibly in combination with other cues, expresses the positive or negative attitude of the source towards the target. We regard cues as belonging to one of the following categories: – Attributional cue: contributes a source while directly expressing the positive or negative attitude of the source towards the embedded target; – Indirect cue: signals the positive or negative attitude of the source by the choice of words; – Factual opi"
L16-1187,W15-1304,0,0.0122515,"merely helps constituting the description of the single ‘real’ event in this sentence expressed by demonstrations. In other words, only one event in this sentence should be put on a timeline. In this framework, syntax is only used to decide on the span of the event. The two main corpora for factuality (or belief) are FactBank (Saur´ı and Pustejovsky, 2009) and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). Although both corpora address the same phenomenon (i.e. the commitment of a source towards the truth of some event/proposition), the annotations are quite different (Werner et al., 2015; Prabhakaran et al., 2015). The LU Corpus so far has only addressed the problem from the perspective of the speaker/writer, in contrast to FactBank, which has fully annotated nested sources. Furthermore, the LU corpus ignores negation (the polarity axis of factuality in FactBank) and does not distinguish between POSSI BLE and PROBABLE (the certainty axis of factuality in FactBank). Finally, there is a subtle difference in the targets. Whereas FactBank has assigned factuality values to events, the LU corpus has assigned belief tags to the head words of propositions, disregarding event-denoting"
L18-1051,P98-1013,0,0.335983,"Missing"
L18-1051,P14-2082,0,0.264893,"irst. However, differences in results are not statistically significant after performing the McNemar’s test (p&gt;0.05). 4.2. Temporal Relation and Classification The TR task is addressed by means of 3 multi-class CRF classifiers, one for each pair of temporal entities (e-dct, et, and e-e pairs), which predict the 14 TimeML temporal values. Similarly to existing systems, we target only intrasentence relations for e-t and e-e pairs, given that the number of cross-sentence relations in the training data is low. The classifiers are trained with the gold data set only, plus additional relations from Bethard et al. (2014). Different pairs have been normalized with respect to the directionality of the relation in order to reduce the variability of the temporal values. This resulted in the following ordering of pairs: i) relations involving an event and a timex, including the DCT, have been represented as e-t/e-dct pairs; ii) relations involving event pairs have been normalized according to the linear order of presentation of the events in the sentences. In this task the system uses predicted event triggers, which in the learning model are represented with the morphosyntactic and lexical-semantic features used i"
L18-1051,S16-1165,0,0.0203764,"mpaign (UzZaman et al., 2013) is the latest campaign on open-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the development and evaluation of end-to-end TP systems based on the TimeML mark-up language (Pustejovsky et al., 2003). After the TempEval-3 evaluation a new dataset has been released, the TimeBank-Dense corpus (Cassidy et al., 2014). This corpus has been developed to address one of the major shortcomings of the TempEval-3 dataset, namely lack 1 TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), Clinical TempEval (Bethard et al., 2016; Bethard et al., 2017), Q-A TempEval (Llorens et al., 2015). 2 For an extended list of available TimeBanks see (Caselli and Sprugnoli, 2017). of connectivity between all possible e-e and e-t pairs, by completing the transitive closure. The outcome of this approach is a very dense temporal graph consisting of 12,713 annotated TRs, with a 6.3 ratio of relations to events and timexes, which is much higher than the 0.8 ratio of the TempEval-3 data, where 11,098 TRs were manually annotated. Density has been obtained by forcing the annotators to always provide an answer. Additionally, the set of TR"
L18-1051,S13-2002,0,0.256652,"l-3 that targeted either the event extraction and classification subtask only (Task B in the TempEval-3 guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextual features: con"
L18-1051,Q14-1022,0,0.0305623,"Missing"
L18-1051,S13-2012,0,0.377252,"n the TempEval-3 guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextual features: context windows of token, lemma, POS; and tokens polarity, among others. • Semantic featur"
L18-1051,D17-1190,0,0.0313767,"Missing"
L18-1051,W07-1409,0,0.0201448,"Missing"
L18-1051,S13-2004,0,0.416906,"guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextual features: context windows of token, lemma, POS; and tokens polarity, among others. • Semantic features, limited to semantic"
L18-1051,S13-2014,0,0.348341,"traction and classification subtask only (Task B in the TempEval-3 guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextual features: context windows of token, lemma, POS; and tokens pola"
L18-1051,S13-2011,0,0.346604,"systems from TempEval-3 that targeted either the event extraction and classification subtask only (Task B in the TempEval-3 guidelines) or the end-to-end temporal relation identification and classification subtask (Task C in the TempEval-3 guidelines, which includes Task B as well). In total we review 6 unique systems (5 for event detection and classification only and 4 for the full TP). 3.1. Event detection and classification The event detection and classification task is addressed by all systems using supervised discrete machine learning classifiers such as Conditional Random Fields (CRFs) (Kolya et al., 2013; Bethard, 2013), Logistic Regression (Kolomiyets and Moens, 2013), and Maximum Entropy (Chambers, 2013; Jung and Stent, 2013). Most of the systems (4 out of 5) adopted the same learning model also for event classification. Overall, 17 features are represented in the learning models, which can be aggregated in 5 groups: https://github.com/cltl/TimeMLEventTrigger 340 • Basic morpho-syntactic features: token, lemma, stem, parts-of-speech (POSs), token’s affix and/or suffix, among others. • Syntactic features: constituency/dependency syntax relations; governing verb lemma, verb chunks. • Contextu"
L18-1051,lopez-de-lacalle-etal-2014-predicate,0,0.0127694,"on and classification; ii) we intend to establish a relation between the errors made by the systems, including ours, and the features used. CRF4TimeML has been designed taking as reference efficient existing systems that incorporate discrete classifiers. In particular, all classifiers we developed share with previous systems basic morpho-syntactic features, such as token, lemma, POS, and dependency relations. However, we have added lexical semantic information by using not only WordNet synsets, but also VerbNet classes and FrameNet frames, obtained from the alignments in the Predicate Matrix (Lacalle et al., 2014). The pre-processing of data is performed with state-of-the-art tools, such as the Stanford CoreNLP (Manning et al., 2014) and the NewsReader NLP pipelines (Agerri et al., 2014). 4 4.1. Event Detection and Classification The event detection and classification task is performed by 4 different classifiers share the basic morpho-syntactic and 4 The timex detection and normalization task is performed using a state-of-the-art system (Bethard, 2013), available at https://bitbucket.org/qwaider/textpro-en. 341 • Adding the automatically generated training data to the manually created training data onl"
L18-1051,S15-2134,0,0.0133911,"n-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the development and evaluation of end-to-end TP systems based on the TimeML mark-up language (Pustejovsky et al., 2003). After the TempEval-3 evaluation a new dataset has been released, the TimeBank-Dense corpus (Cassidy et al., 2014). This corpus has been developed to address one of the major shortcomings of the TempEval-3 dataset, namely lack 1 TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), Clinical TempEval (Bethard et al., 2016; Bethard et al., 2017), Q-A TempEval (Llorens et al., 2015). 2 For an extended list of available TimeBanks see (Caselli and Sprugnoli, 2017). of connectivity between all possible e-e and e-t pairs, by completing the transitive closure. The outcome of this approach is a very dense temporal graph consisting of 12,713 annotated TRs, with a 6.3 ratio of relations to events and timexes, which is much higher than the 0.8 ratio of the TempEval-3 data, where 11,098 TRs were manually annotated. Density has been obtained by forcing the annotators to always provide an answer. Additionally, the set of TRs has been simplified with respect to the one used in TempEv"
L18-1051,P14-5010,0,0.00247954,"he features used. CRF4TimeML has been designed taking as reference efficient existing systems that incorporate discrete classifiers. In particular, all classifiers we developed share with previous systems basic morpho-syntactic features, such as token, lemma, POS, and dependency relations. However, we have added lexical semantic information by using not only WordNet synsets, but also VerbNet classes and FrameNet frames, obtained from the alignments in the Predicate Matrix (Lacalle et al., 2014). The pre-processing of data is performed with state-of-the-art tools, such as the Stanford CoreNLP (Manning et al., 2014) and the NewsReader NLP pipelines (Agerri et al., 2014). 4 4.1. Event Detection and Classification The event detection and classification task is performed by 4 different classifiers share the basic morpho-syntactic and 4 The timex detection and normalization task is performed using a state-of-the-art system (Bethard, 2013), available at https://bitbucket.org/qwaider/textpro-en. 341 • Adding the automatically generated training data to the manually created training data only for the Event Trigger and Class classifiers. lexico-semantic features: one for the identification of event triggers, and"
L18-1051,I17-1085,0,0.0280445,"Missing"
L18-1051,C14-1198,0,0.0542149,"Missing"
L18-1051,C16-1265,0,0.0274861,"Missing"
L18-1051,W17-0222,0,0.0607951,"Missing"
L18-1051,S13-2001,0,0.20146,"timex (et). Temporally aware Natural Language Processing (NLP) systems are crucial not only to generate timelines and storylines (Vossen et al., 2015), but also in decision support systems, summarization and textual entailment applications, question answering systems, and document archiving, among others. Since the release of the TimeBank corpus (Pustejovsky et al., 2003) there has been a renewed interest in the area of TP, which has resulted in the celebration of several evaluation campaigns1 and in the creation of corpora and tools in languages other than English. 2 The TempEval-3 campaign (UzZaman et al., 2013) is the latest campaign on open-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the development and evaluation of end-to-end TP systems based on the TimeML mark-up language (Pustejovsky et al., 2003). After the TempEval-3 evaluation a new dataset has been released, the TimeBank-Dense corpus (Cassidy et al., 2014). This corpus has been developed to address one of the major shortcomings of the TempEval-3 dataset, namely lack 1 TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), Clinical TempEval (Bethard et al., 2016; Bethard"
L18-1051,S07-1014,0,0.0405623,"n the creation of corpora and tools in languages other than English. 2 The TempEval-3 campaign (UzZaman et al., 2013) is the latest campaign on open-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the development and evaluation of end-to-end TP systems based on the TimeML mark-up language (Pustejovsky et al., 2003). After the TempEval-3 evaluation a new dataset has been released, the TimeBank-Dense corpus (Cassidy et al., 2014). This corpus has been developed to address one of the major shortcomings of the TempEval-3 dataset, namely lack 1 TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), Clinical TempEval (Bethard et al., 2016; Bethard et al., 2017), Q-A TempEval (Llorens et al., 2015). 2 For an extended list of available TimeBanks see (Caselli and Sprugnoli, 2017). of connectivity between all possible e-e and e-t pairs, by completing the transitive closure. The outcome of this approach is a very dense temporal graph consisting of 12,713 annotated TRs, with a 6.3 ratio of relations to events and timexes, which is much higher than the 0.8 ratio of the TempEval-3 data, where 11,098 TRs were manually annotated. Density has been obta"
L18-1051,W15-4507,1,0.644984,"meaningfully interpreted by using models of time, which allow to connect events on a timeline (temporal anchoring) and to understand complex sequences of events (temporal ordering). Temporal relations (TRs) provide a model and a set of properties to account for the connections between pairs of entities. Temporal Processing (TP) is a task consisting in automatically identifying and classifying basic entities and their relations, such as event-event (e-e), and event-timex (et). Temporally aware Natural Language Processing (NLP) systems are crucial not only to generate timelines and storylines (Vossen et al., 2015), but also in decision support systems, summarization and textual entailment applications, question answering systems, and document archiving, among others. Since the release of the TimeBank corpus (Pustejovsky et al., 2003) there has been a renewed interest in the area of TP, which has resulted in the celebration of several evaluation campaigns1 and in the creation of corpora and tools in languages other than English. 2 The TempEval-3 campaign (UzZaman et al., 2013) is the latest campaign on open-domain TP in English. The major contribution of TempEval-3 is the release of a platform for the d"
L18-1178,W14-2907,0,0.0577576,"Missing"
L18-1178,W13-2322,0,0.0318646,"Missing"
L18-1178,bejan-harabagiu-2008-linguistic,0,0.0598379,"Missing"
L18-1178,calzolari-etal-2012-lre,0,0.0293628,"ts, after which we focus on a subset: PropBank/NomBank (PB/NB), FactBank (FB) and TempEval-3 (TE3). We analyse their interoperability at the level of text in Section 5 and at the level of annotations in Section 6. Finally, Section 7 concludes with our lessons learned and proposes some best-practice guidelines. 2 Related Work There is a range of initiatives collecting and indexing metadata of language resources at the corpus-level to support researchers in finding the right one for their task or application. These include OLAC (Simons and Bird, 2003), Language Grid (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Reposito"
L18-1178,W11-0402,0,0.0274906,"d (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Repositories of linguistic annotation terminology, such as GOLD (Farrar and Langendoen, 2003), ISOcat (Windhouwer and Wright, 2012) and its successor CCR,3 make it possible to overcome the heterogeneity of annotation schemes by acting as an interlingua that allows mapping annotations from one scheme to another, thus addressing conceptual interoperability (Chiarcos, 2012a). However, far from all corpora that we use today follow the principles mentioned above. This may be because they were created in a time where these standards simply did not yet exist. For more recently cre"
L18-1178,cybulska-vossen-2014-using,1,0.834358,"ata is not yet publicly available, but the LDC kindly provided us with a list of training data filenames. 2010) which was built to encode event structures with relations like SUBEVENT or REASON, and intra- and crossdocument event coreference. ECB 1.0 consists of 482 documents from Google News clustered into 43 topics. A first extension to ECB was released by Lee et al. (2012), who revised and completed the original annotations and added entity coreference relations following the OntoNotes annotation guidelines for coreference (Pradhan et al., 2007). We included a second extension called ECB+ (Cybulska and Vossen, 2014), which contains another corpus consisting of 502 documents, completely (re)annotated according to new guidelines. The corpus is annotated with event classes (based on TimeML), locations and times (based on ACE and TimeML), and intra- and cross-document coreference. 3.4 Other Annotation Standards The full-text annotations in FrameNet (ICSI Berkeley, 2017) capture the frame semantic structures as defined in its lexical database (Fillmore et al., 2003). The documents come from different sources, including PropBank, the AQUAINT Program7 and the Lexical Understanding (LU) Annotation Corpus (Diab e"
L18-1178,W09-3012,0,0.0322581,"2014), which contains another corpus consisting of 502 documents, completely (re)annotated according to new guidelines. The corpus is annotated with event classes (based on TimeML), locations and times (based on ACE and TimeML), and intra- and cross-document coreference. 3.4 Other Annotation Standards The full-text annotations in FrameNet (ICSI Berkeley, 2017) capture the frame semantic structures as defined in its lexical database (Fillmore et al., 2003). The documents come from different sources, including PropBank, the AQUAINT Program7 and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). The latter contains annotations of dialog acts, event coreference, event relations and entity relations, but is best known for its annotations of committed belief, i.e. the strength of the author’s beliefs and the degree of commitment to their utterance (similar to FactBank). The EventStatus Corpus (Huang et al., 2017) annotated approximately 3,000 English and 1,500 Spanish news articles with temporal and aspectual properties of major societal events, that is, whether an event has already happened, is currently happening or may happen in the future. Its English documents were sourced from En"
L18-1178,doddington-etal-2004-automatic,0,0.217079,"Missing"
L18-1178,N06-2015,0,0.142213,"Missing"
L18-1178,D16-1005,0,0.0450954,"Missing"
L18-1178,W07-1501,0,0.0291678,"the right one for their task or application. These include OLAC (Simons and Bird, 2003), Language Grid (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Repositories of linguistic annotation terminology, such as GOLD (Farrar and Langendoen, 2003), ISOcat (Windhouwer and Wright, 2012) and its successor CCR,3 make it possible to overcome the heterogeneity of annotation schemes by acting as an interlingua that allows mapping annotations from one scheme to another, thus addressing conceptual interoperability (Chiarcos, 2012a). However, far from all corpora that we use today follow the principles mentioned above. This may be"
L18-1178,hinrichs-krauwer-2014-clarin,0,0.0171782,"their interoperability at the level of text in Section 5 and at the level of annotations in Section 6. Finally, Section 7 concludes with our lessons learned and proposes some best-practice guidelines. 2 Related Work There is a range of initiatives collecting and indexing metadata of language resources at the corpus-level to support researchers in finding the right one for their task or application. These include OLAC (Simons and Bird, 2003), Language Grid (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Repositories of linguistic annotation terminology, such as GOLD (Farrar and Langendoen, 2003), ISOcat (Windhouwer and Wrig"
L18-1178,D12-1045,0,0.0195649,"datasets of the TempEval shared tasks, from which we selected the TempEval3 dataset (UzZaman et al., 2013). The TimeML specifications were followed to annotate events in EventCorefBank (ECB) (Bejan and Harabagiu, 6 At the moment, the ERE data is not yet publicly available, but the LDC kindly provided us with a list of training data filenames. 2010) which was built to encode event structures with relations like SUBEVENT or REASON, and intra- and crossdocument event coreference. ECB 1.0 consists of 482 documents from Google News clustered into 43 topics. A first extension to ECB was released by Lee et al. (2012), who revised and completed the original annotations and added entity coreference relations following the OntoNotes annotation guidelines for coreference (Pradhan et al., 2007). We included a second extension called ECB+ (Cybulska and Vossen, 2014), which contains another corpus consisting of 502 documents, completely (re)annotated according to new guidelines. The corpus is annotated with event classes (based on TimeML), locations and times (based on ACE and TimeML), and intra- and cross-document coreference. 3.4 Other Annotation Standards The full-text annotations in FrameNet (ICSI Berkeley,"
L18-1178,P14-5010,0,0.00299599,"representing a token and information about its document id, sentence id, token id, token text, lemma and POS. If possible, we used the gold sentence splitting, tokenization, lemmatization and POS tagging. If that was not available (e.g. TE3 only has POS tags for most but not all events, PB/NB only has lemmas for events), we used the Stanford CoreNLP pipeline to retrieve the POS tags and the lemma of all the tokens in the datasets. 12 http://www.nltk.org/howto/propbank.html https://github.com/propbank/propbank-release 14 http://timeml.org/timeMLdocs/TimeML1.2.1.xsd 13 We used Stanford CoreNLP (Manning et al., 2014) in order to split the documents into sentences. 1105 PB/NB FB TE-3 Verb 114,574 6,377 5,835 Noun 109,793 2,498 2,451 Adjective 250 202 Preposition 45 10 Number 46 - Adverb 21 - Particle 6 - Determiner 2 - Oth/Unknown 1 2,632 Table 2: Distribution of POS tags across annotated single-token events 6.2 Conceptual Interoperability The event annotations in both FB and TE3 are based on the TimeML 1.2.1 Annotation Guidelines (Saur´ı et al., 2006), which define an event as a situation that happens or occurs. The guidelines further specify in which cases an event should or should not be annotated as a"
L18-1178,J93-2004,0,0.0609057,"Missing"
L18-1178,W04-2705,0,0.199006,"Missing"
L18-1178,W16-5706,0,0.0364542,"Missing"
L18-1178,J05-1004,0,0.236,"Missing"
L18-1178,W14-3004,0,0.0142939,"w of) that actually clearly illustrate the extent of the problems. Some studies indirectly discuss conceptual interoperability by comparing annotation schemes. For example, Aguilar et al. (2014) compare the Events, Entities and Relations represented in ACE, ERE, TAC-KBP Slot-filling, and FrameNet. Werner et al. (2015) compare the factuality/committed belief annotations in FactBank and the Language Understanding (LU) corpus. The differences between the representations of semantic propositions in PropBank, VerbNet and FactBank have been extensively described and even leveraged to build SemLink (Palmer et al., 2014). Close to our work is that of Pustejovsky et al. (2005), who discuss the issues involved in creating a Unified Linguistic Annotation (ULA) by merging the annotation schemes of PropBank, NomBank, TimeBank, the Discourse Treebank and Coreference Annotation. However, their work remains on theoretical ground by limiting their discussion to overlapping and conflicting annotations in example sentences. Our approach is unique in the sense that we provide empirical evidence by discussing the overlap of the actual annotations for the complete resources when aligning them on the same texts, as well as"
L18-1178,piperidis-2012-meta,0,0.0287955,"mpEval-3 (TE3). We analyse their interoperability at the level of text in Section 5 and at the level of annotations in Section 6. Finally, Section 7 concludes with our lessons learned and proposes some best-practice guidelines. 2 Related Work There is a range of initiatives collecting and indexing metadata of language resources at the corpus-level to support researchers in finding the right one for their task or application. These include OLAC (Simons and Bird, 2003), Language Grid (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Repositories of linguistic annotation terminology, such as GOLD (Farrar and Langendoe"
L18-1178,W05-0302,0,0.259717,": a proposition is formed by a predicate with its arguments; events are expressed by predicates describing situations that happen/occur (Saur´ı et al., 2006); predicates can be of “propositional” type (representing an event, state, etc.) (Meyers, 2007). There is, however, little agreement on the degree of meaning overlap and relatedness. For the sake of clarity, we will use in this article the term event to refer to all three of the overlapping and interrelated notions. Many existing event corpora contain annotations of different aspects of events that are often applied to the same documents (Pustejovsky et al., 2005), providing an interesting use case for analysing interoperability. Furthermore, event annotations involve a wide range of properties and phenomena which makes it ultimately rewarding to achieve interoperability and combine these annotations. The contributions of this paper are the following: • a comprehensive overview of interoperability issues 1101 across event corpora that result from differences in document naming conventions, textual content and structural/conceptual representations of annotations; • a method for aligning diverse language resource corpora to identify divergent and overlap"
L18-1178,W15-0812,0,0.0194146,"contain event annotations, but a selection of its documents was used in TimeBank, which does (Section 3.3). ACE was followed by Light ERE, which was designed as a lighter-weight version of ACE with the goal of making annotation easier and more consistent. Modifications to ACE for Light ERE included a reduced inventory of entity and relation types, a slightly modified and reduced event ontology, and the addition of event coreference. In turn, Light ERE has transitioned to the more complex Rich ERE, with the latter enabling a more comprehensive treatment of phenomena such as event coreference (Song et al., 2015).6 3.3 TimeML TimeML (Pustejovsky et al., 2003a) is a specification language for events and temporal expressions, designed to capture their attributes, to link them (event time-stamping) and to determine the temporal order between events. It has been applied in several corpora, including the AQUAINT TimeML Corpus (Brandeis University, 2008) and TimeBank 1.2 (Pustejovsky et al., 2006). The documents in TimeBank come from PropBank and the ACE-2 corpus. In turn, data from TimeBank and AQUAINT TimeML was used to build FactBank 1.0 (Saur´ı and Pustejovsky, 2009), adding a representation of factuali"
L18-1178,strassel-etal-2008-linguistic,0,0.0435113,"n, 145K of P2.5 data and 200K of Web data taken from other sources. PropBank’s representation of semantic roles is also used in the Abstract Meaning Representation (AMR) corpus (Knight et al., 2014), which represents the semantics of English sentences as single rooted, directed graphs with the aim of abstracting away from syntactic idiosyncrasies. It uses a variety of sources for its data, including WSJ news. 3.2 Automatic Content Extraction (ACE) & Entities, Relations and Events (ERE) The key content extraction tasks of the Automatic Content Extraction (ACE) program (Doddington et al., 2004; Strassel et al., 2008), which ran between 1999 and 2008, were defined as the automatic detection and characterization of real-world Entities, Relations, and Events. However, the program mostly focused on entities and relations between them. Event annotations are available only in the ACE 2005 Multilingual Training Corpus (Walker et al., 2006), where annotators tagged the extent, trigger, polarity, tense, genericity, modality, participants and attributes for a constrained set of event (sub)types. This data has been reused in several other corpora, including OntoNotes and Datasets for Generic Relation Extraction (reA"
L18-1178,S13-2001,0,0.239601,"es, to link them (event time-stamping) and to determine the temporal order between events. It has been applied in several corpora, including the AQUAINT TimeML Corpus (Brandeis University, 2008) and TimeBank 1.2 (Pustejovsky et al., 2006). The documents in TimeBank come from PropBank and the ACE-2 corpus. In turn, data from TimeBank and AQUAINT TimeML was used to build FactBank 1.0 (Saur´ı and Pustejovsky, 2009), adding a representation of factuality interpretation to the event annotations, and the evaluation datasets of the TempEval shared tasks, from which we selected the TempEval3 dataset (UzZaman et al., 2013). The TimeML specifications were followed to annotate events in EventCorefBank (ECB) (Bejan and Harabagiu, 6 At the moment, the ERE data is not yet publicly available, but the LDC kindly provided us with a list of training data filenames. 2010) which was built to encode event structures with relations like SUBEVENT or REASON, and intra- and crossdocument event coreference. ECB 1.0 consists of 482 documents from Google News clustered into 43 topics. A first extension to ECB was released by Lee et al. (2012), who revised and completed the original annotations and added entity coreference relatio"
L18-1178,W15-1304,0,0.0208891,"ot yet exist. For more recently created corpora, however, there is presumably a plethora of reasons. We hypothesize that one of them is that whereas working groups such as the Open Linguistics Working Group (OWLG)4 actively promote resource interoperability, there seem to be few examples (that we know of) that actually clearly illustrate the extent of the problems. Some studies indirectly discuss conceptual interoperability by comparing annotation schemes. For example, Aguilar et al. (2014) compare the Events, Entities and Relations represented in ACE, ERE, TAC-KBP Slot-filling, and FrameNet. Werner et al. (2015) compare the factuality/committed belief annotations in FactBank and the Language Understanding (LU) corpus. The differences between the representations of semantic propositions in PropBank, VerbNet and FactBank have been extensively described and even leveraged to build SemLink (Palmer et al., 2014). Close to our work is that of Pustejovsky et al. (2005), who discuss the issues involved in creating a Unified Linguistic Annotation (ULA) by merging the annotation schemes of PropBank, NomBank, TimeBank, the Discourse Treebank and Coreference Annotation. However, their work remains on theoretical"
morante-2008-semantic,W99-0623,0,\N,Missing
morante-2008-semantic,S07-1008,0,\N,Missing
morante-2008-semantic,W06-2933,0,\N,Missing
morante-2008-semantic,W06-2920,0,\N,Missing
morante-2008-semantic,C04-1186,0,\N,Missing
morante-2008-semantic,W05-0620,0,\N,Missing
morante-2008-semantic,N06-2033,0,\N,Missing
morante-2008-semantic,P05-1073,0,\N,Missing
morante-2008-semantic,W05-1518,0,\N,Missing
morante-2008-semantic,J02-3001,0,\N,Missing
morante-2008-semantic,J05-1004,0,\N,Missing
morante-2008-semantic,D07-1096,0,\N,Missing
morante-2008-semantic,W04-2412,0,\N,Missing
morante-2008-semantic,D07-1121,0,\N,Missing
morante-2010-descriptive,W08-0606,0,\N,Missing
morante-2010-descriptive,W09-1105,1,\N,Missing
morante-2010-descriptive,E99-1043,0,\N,Missing
morante-daelemans-2012-conandoyle,burchardt-etal-2006-salto,0,\N,Missing
morante-daelemans-2012-conandoyle,W08-0606,0,\N,Missing
morante-daelemans-2012-conandoyle,S10-1008,1,\N,Missing
morante-daelemans-2012-conandoyle,W10-3110,0,\N,Missing
morante-daelemans-2012-conandoyle,P11-1059,0,\N,Missing
morante-daelemans-2012-conandoyle,erk-pado-2004-powerful,0,\N,Missing
R09-1051,W06-2924,1,0.87021,"training and processing efficiency to the number of class labels [4]. Memory-based algorithms have been previously applied to processing semantic and syntactic dependencies separately. As for semantic role labeling, [10] describes a memory-based semantic role labeling system for Spanish based on gold standard dependency syntax; [11] report on a semantic role labeling system for English based on syntactic dependencies produced by the MaltParser system of Nivre et al. [13]. As for dependency parsing, MaltParser uses memorybased learning as one of its optional local classifiers. Canisius et al. [2] present another type of memorybased dependency parser, extended later in [3] to a constraint satisfaction-based dependency parser. The latter parser combines local memory-based classification with a global optimization method based on soft weighted constraint-satisfaction inference, where the local classifiers estimate syntactic relations between pairs of words, the direction of the relation from children to parents, and the relations that parents have with children. Our current joint system adopts a similar strategy, but uses ranking rather than weighted constraint satisfaction inference. 1"
R09-1051,D07-1121,0,0.0469762,"-based algorithms have been previously applied to processing semantic and syntactic dependencies separately. As for semantic role labeling, [10] describes a memory-based semantic role labeling system for Spanish based on gold standard dependency syntax; [11] report on a semantic role labeling system for English based on syntactic dependencies produced by the MaltParser system of Nivre et al. [13]. As for dependency parsing, MaltParser uses memorybased learning as one of its optional local classifiers. Canisius et al. [2] present another type of memorybased dependency parser, extended later in [3] to a constraint satisfaction-based dependency parser. The latter parser combines local memory-based classification with a global optimization method based on soft weighted constraint-satisfaction inference, where the local classifiers estimate syntactic relations between pairs of words, the direction of the relation from children to parents, and the relations that parents have with children. Our current joint system adopts a similar strategy, but uses ranking rather than weighted constraint satisfaction inference. 1 http://ufal.mff.cuni.cz/conll2009-st/ 275 International Conference RANLP 2009"
R09-1051,W09-1202,0,0.0423312,"t A is the argument with the A0 role of predicate B. Thus, we merge the class labels of the two 276 tasks into single labels, and present the classifiers with examples with these labels. Further on the system, as we describe in the next section, we do make use of the compositionality of the labels, as in the end we have to produce syntactic dependency graphs and semantic role assignments separately. Apart from our system, three more joint systems participated in the CoNLL Shared Task 2009. The system described by [9] extends the Eisner parser to accomodate semantic dependencies. The system of [5] decomposes the joint learning task in four subtasks: semantic dependency identification and labeling, and syntactic dependency identification and labeling. A pipeline approach is set up in order to use the output of one task as input of another, and features not available at a certain step are incorporated iteratively. The system described by [7] is based on an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for both types of dependency structures. Fig. 1: Architecture of the joint system for dependency parsing and semantic role labe"
R09-1051,N09-1037,0,0.032308,"ion performance as compared to learning the letter-phoneme task and the stress marker assignment task separately. Buchholz [1] transposes this idea to shallow parsing, and shows that POS tagging and base phrase chunking could be learnd as a single task without any significant performance loss. Wang et al. [17] jointly learn Chinese word segmentation, named entity recognition, and part-of-speech tagging, outperforming a pipeline architecture baseline. Recently, Finkel and Manning show that joint learning of parsing and named entity recognition produce mildly improved performance for both tasks [6]. The merging of two tasks will typically lead to an increase in the number of class labels, and generally a more complex class space. In the worst case, the number of classes in the new class space is the product of the number of classes in the original tasks. In practice, if two combined tasks are to some extent related, the increase will tend to be limited, as class labels from the original tasks will tend to correlate. For instance, the POS tag for “determiner” will typically co-occur with the chunk marker for “beginning of noun phrase”, and less so, or not at all with other chunk markers."
R09-1051,W09-1205,0,0.0965718,"ency graphs and semantic role assignments separately. Apart from our system, three more joint systems participated in the CoNLL Shared Task 2009. The system described by [9] extends the Eisner parser to accomodate semantic dependencies. The system of [5] decomposes the joint learning task in four subtasks: semantic dependency identification and labeling, and syntactic dependency identification and labeling. A pipeline approach is set up in order to use the output of one task as input of another, and features not available at a certain step are incorporated iteratively. The system described by [7] is based on an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for both types of dependency structures. Fig. 1: Architecture of the joint system for dependency parsing and semantic role labeling 3 System description In this section we describe the joint system, and compare it to the isolated version of the system. The joint system operates in three phases (see Figure 1): a classification phase in which three memory-based classifiers predict different aspects of joint syntactic and semantic labeling; a ranking phase in which the outpu"
R09-1051,W09-1201,0,0.0653145,"do not signify the same, the “subject” dependency relation, for example, often co-occurs with the “A0” label that denotes the agent role in the PropBank annotation scheme [14]. Overlaps such as these naturally suggest the possibility of jointly learning the two labeling tasks as if they were one. In this paper we present a system that performs dependency parsing and semantic role labeling jointly, Antal van den Bosch Tilburg Centre for Creative Computing Tilburg University P.O. Box 90153 NL-5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl which we submitted to the CoNLL Shared Task 2009 [8]. The task combines the identification and labeling of syntactic dependencies and semantic roles for seven languages. Details about the task setting and the data sets used can be found in the web page of the task1 . Additionally, we present a comparison of the joint system with another version of the system (“isolated” system) that processes semantic and syntactic dependencies separately. In this way, we are able to evaluate whether and where the joint learning approach is more efficient and successful than the isolated approach. As far as we know, this is the first time that such a comparison"
R09-1051,W09-1212,0,0.0661215,"is the modifier in a subject dependency relation with its head B, as well as that A is the argument with the A0 role of predicate B. Thus, we merge the class labels of the two 276 tasks into single labels, and present the classifiers with examples with these labels. Further on the system, as we describe in the next section, we do make use of the compositionality of the labels, as in the end we have to produce syntactic dependency graphs and semantic role assignments separately. Apart from our system, three more joint systems participated in the CoNLL Shared Task 2009. The system described by [9] extends the Eisner parser to accomodate semantic dependencies. The system of [5] decomposes the joint learning task in four subtasks: semantic dependency identification and labeling, and syntactic dependency identification and labeling. A pipeline approach is set up in order to use the output of one task as input of another, and features not available at a certain step are incorporated iteratively. The system described by [7] is based on an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for both types of dependency structures. Fig."
R09-1051,morante-2008-semantic,1,0.900396,"mber of labels increases, and the average number of examples per label decreases. This does not rule out the application of a machine learning classifier to the joint task, but the classifier should not be too sensitive to a fragmented class space with many labels. This is the main reason our system relies on local memory-based classifiers: they are largely insensitive in terms of training and processing efficiency to the number of class labels [4]. Memory-based algorithms have been previously applied to processing semantic and syntactic dependencies separately. As for semantic role labeling, [10] describes a memory-based semantic role labeling system for Spanish based on gold standard dependency syntax; [11] report on a semantic role labeling system for English based on syntactic dependencies produced by the MaltParser system of Nivre et al. [13]. As for dependency parsing, MaltParser uses memorybased learning as one of its optional local classifiers. Canisius et al. [2] present another type of memorybased dependency parser, extended later in [3] to a constraint satisfaction-based dependency parser. The latter parser combines local memory-based classification with a global optimizatio"
R09-1051,W08-2128,1,0.901491,"cation of a machine learning classifier to the joint task, but the classifier should not be too sensitive to a fragmented class space with many labels. This is the main reason our system relies on local memory-based classifiers: they are largely insensitive in terms of training and processing efficiency to the number of class labels [4]. Memory-based algorithms have been previously applied to processing semantic and syntactic dependencies separately. As for semantic role labeling, [10] describes a memory-based semantic role labeling system for Spanish based on gold standard dependency syntax; [11] report on a semantic role labeling system for English based on syntactic dependencies produced by the MaltParser system of Nivre et al. [13]. As for dependency parsing, MaltParser uses memorybased learning as one of its optional local classifiers. Canisius et al. [2] present another type of memorybased dependency parser, extended later in [3] to a constraint satisfaction-based dependency parser. The latter parser combines local memory-based classification with a global optimization method based on soft weighted constraint-satisfaction inference, where the local classifiers estimate syntactic"
R09-1051,W09-1203,1,0.864082,"vel. Two classifiers consider pairs of words, and predict the identity or the presence, respectively, of a joint semantic and syntactic dependency between them. The third classifier focus on single words only, and predicts the relations one word has with other words, without making reference to these other words. The hyperparameters of the classifiers were optimized on English, by training on the full training set and testing on the development set; these optimized settings were then used for the other six languages as well. The hyperparameters and features used per classifier can be found in [12]. Classifier 1: Pairwise semantic and syntactic dependencies. Classifier 1 predicts the merged semantic and syntactic dependencies that hold between two tokens. Instances represent combinations of pairs of tokens within a sentence. Each token is combined with all other tokens in the sentence. The class predicted is a joint < dependencyrelation &gt;:< semanticrole &gt; label, or NONE if no relation is present between the tokens. The amount of occurring classes for all seven languages is shown in Table 2. C1 C2 Cat 111 111 Chi 309 1209 Cze 395 1221 Eng 551 1957 Ger 152 300 Jap 103 505 Spa 124 124 Tabl"
R09-1051,J05-1004,0,0.144165,"ntences, while semantic role assignments center around individual predicates. Yet, the spaces overlap; in a dependency graph verbal predicates will tend to have dependency relations with the same modifiers that have a semantic role as argument of that predicate. In general, even though the labels are different, syntactic dependencies between two words often co-occur with the existence of certain semantic roles. Although they do not signify the same, the “subject” dependency relation, for example, often co-occurs with the “A0” label that denotes the agent role in the PropBank annotation scheme [14]. Overlaps such as these naturally suggest the possibility of jointly learning the two labeling tasks as if they were one. In this paper we present a system that performs dependency parsing and semantic role labeling jointly, Antal van den Bosch Tilburg Centre for Creative Computing Tilburg University P.O. Box 90153 NL-5000 LE Tilburg, The Netherlands Antal.vdnBosch@uvt.nl which we submitted to the CoNLL Shared Task 2009 [8]. The task combines the identification and labeling of syntactic dependencies and semantic roles for seven languages. Details about the task setting and the data sets used"
S07-1038,W05-0620,0,0.0566946,"Missing"
S07-1038,J02-3001,0,0.111893,"Missing"
S07-1038,W05-0628,0,0.0549315,"Missing"
S07-1038,S07-1008,0,0.0449497,"Missing"
S07-1038,W05-0637,0,0.536187,"Missing"
S07-1038,W04-3212,0,0.0602564,"Missing"
S07-1038,W04-2412,0,\N,Missing
S07-1039,W96-0102,0,0.0707357,"ous way to model their lexical semantics was by utilizing WordNet3.0 (Fellbaum, 1998) (WN). One of the systems followed this route. We also entered a second system, which did not rely on WN but instead made use of automatically 2 System Description The development of the system consists of a preprocessing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, c Prague, June 2007. 2007 Association for Computatio"
S07-1039,J93-2004,0,0.0321182,"ing phase to extract the features, and the classification phase. 2.1 Preprocessing Each sentence is preprocessed automatically in the following steps. First, the sentence is tokenized with a rule-based tokenizer. Next a part-of-speech tagger and text chunker that use the memory-based tagger MBT (Daelemans et al., 1996) produces partof-speech tags and NP chunk labels for each token. Then a memory-based shallow parser predicts grammatical relations between verbs and NP chunks such as subject, object or modifier (Buchholz, 2002). The tagger, chunker and parser were all trained on the WSJ Corpus (Marcus et al., 1993). We also use a memory-based lemmatizer (Van den Bosch et al., 1996) trained on Celex (Baayen et al., 1993) to predict the lemma of each word. 187 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187–190, c Prague, June 2007. 2007 Association for Computational Linguistics The features extracted are of three types: semantic, lexical, and morpho-syntactic. The features that apply to the entities in a relation (e1,e2) are extracted for term 1 (t1) and term 2 (t2) of the relation, where t1 is the first term in the relation name, and t2 is the second term."
S10-1008,S07-1018,1,0.754355,"rd word senses (i.e., frames) for the target words and the participants had to perform role recognition/labeling and null instantiation linking, and a NI only task, in which the test set was already annotated with gold standard semantic argument structures and the participants only had to recognize definite null instantiations and find links to antecedents in the wider context (NI linking). However, it turned out that the basic semantic role labeling task was already quite challenging for our data set. Previous shared tasks have shown that frame-semantic SRL of running text is a hard problem (Baker et al., 2007), partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available. In our case the difficulty was increased because our data came from a new genre and domain (i.e., crime fiction, see Section 3.2). Hence, we decided to add standard SRL, i.e., role recognition and labeling, as a third task (SRL only). This task did not involve NI linking. The theory of null complementation used here is the one adopted by FrameNet, which derives from the work of Fillmore (1986).3 Briefly, omissions of core arguments of predicates are categoriz"
S10-1008,erk-pado-2004-powerful,0,0.102782,"Missing"
S10-1008,J02-3001,0,0.0764846,"scourse entities or events. In the shared task we looked at one particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications, such as information extraction, question answering or text summarization. 1 Introduction Semantic role labeling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. However, semantic role labeling as it is currently defined misses a lot of information due to the fact that it is viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded. This view of SRL as a sentence-internal task is partly due to the fact that large-scale manual annotation 1 http://framenet.icsi.berkeley.edu/ http://verbs.colorado.edu/˜mpalm"
S10-1008,P86-1004,1,0.741997,"fer and Caroline Sporleder Roser Morante Computational Linguistics CNTS Saarland University University of Antwerp {josefr,csporled}@coli.uni-sb.de Roser.Morante@ua.ac.be Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu Collin Baker ICSI Berkeley, CA 94704 collin@icsi.berkeley.edu projects such as FrameNet1 and PropBank2 typically present their annotations lexicographically by lemma rather than by source text. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore, 1977). In early work, Palmer et al. (1986) discussed filling null complements from context by using knowledge about individual predicates and tendencies of referential chaining across sentences. But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semantic relations between the predicates involved. Two notable exceptions are Fillmore and Baker (2001) and Burchardt et al. (2005). Fillmore and Baker (2001) analyse a short newspaper article and discuss how frame semantics could benefit discourse processing but without making concrete suggestions"
S10-1008,W09-2417,1,0.499792,"Missing"
S10-1008,H86-1011,1,\N,Missing
S12-1035,P11-1059,1,0.233104,"he GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus provides focus annotation on top of PropBank. It targets exclusively verbal negations marked with MNEG in PropBank and selects as focus the semantic role containing the most likely focus. The motivation behind their approach, annotation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For e"
S12-1035,H92-1022,0,0.0728387,"ntic role containing the most likely focus. The motivation behind their approach, annotation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5 November 2009 release available from Brown University. 6 http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7 The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. 269 1 role 2 roles 3 roles All A1 AM-NEG AM-TMP AM-MNR A2 A0"
S12-1035,P05-1022,0,0.0141545,"ocessed at the University of Oslo. Tokenization was obtained by the PTBcompliant tokenizer that is part of the LinGO English Resource Grammar. 4 4 268 Training 65,450 3644 848 23.27 984 30 887 616 http://moin.delph-in.net/ Apart from the gold annotations, the corpus was provided to participants with additional annotations: 3.2 Semantic roles focus belongs to • Lemmatization using the GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus provides focus annotation on top of PropBank. It targets exclusively verbal negations marked with MNEG in PropBank and selects as focus the semantic role containing the most likely focus. The motivation"
S12-1035,A00-2018,0,0.00565266,"ons 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5 November 2009 release available from Brown University. 6 http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7 The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. 269 1 role 2 roles 3 roles All A1 AM-NEG AM-TMP AM-MNR A2 A0 None AM-ADV C-A1 AM-PNC AM-LOC A4 R-A1 Other Train 2,210 89 3 2,302 980 592 161 127 112 94 88 78 46 33 25 11 10 40 Devel 515 15 0 530 222 138 35 27 28 23 19 23 6 8 4 2 2 8 Test 672 38 2 712 309 172"
S12-1035,de-marneffe-etal-2006-generating,0,0.00947387,"Missing"
S12-1035,W10-3001,0,0.147806,"Missing"
S12-1035,P05-1045,0,0.00178046,"tation guidelines and examples can be found in the aforementioned paper. We gathered all negations from sections 02–21, 23 and 24 and discarded negations for which the focus or PropBank annotations were not sound, leaving 3,544 instances.7 For each verbal negation, PBFOC provides the current sentence, and the previous and next sentences as context. For each sentence, along with the gold focus annotations, PB-FOC contains the following additional annotations: • Token number; • POS tags using the Brill tagger (Brill, 1992); • Named Entities using the Stanford named entity recognizer recognizer (Finkel et al., 2005); • Chunks using the chunker by Phan (2006); • Syntactic tree using the Charniak parser (Charniak, 2000); • Dependency tree derived from the syntactic tree (de Marneffe et al., 2006); ErgTokenization, http://moin.delph-in.net/ ReppTop 5 November 2009 release available from Brown University. 6 http://www.lsi.upc.edu/˜srlconll/ srlconll-1.1.tgz 7 The original focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. 269 1 role 2 roles 3 roles All A1 AM-NEG AM-TMP AM-MNR A2 A0 None AM-ADV C-A1 AM-PNC AM-LOC A4 R-A1 Other Train 2,210 89 3 2,302 980 592 161 127 112 94 88"
S12-1035,W09-1105,1,0.708954,"ince clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC corpus). Focus annotation is restricted to verbal negations annotated with MNEG in PropBank, and all the words belonging to a semantic role are selected as focus"
S12-1035,morante-daelemans-2012-conandoyle,1,0.60973,"Missing"
S12-1035,J12-2001,1,0.718204,"of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a partial manner. The interest in automatically processi"
S12-1035,J08-2005,0,0.00730407,"inal focus annotation targeted the 3,993 negations marked with MNEG in the whole PropBank. 269 1 role 2 roles 3 roles All A1 AM-NEG AM-TMP AM-MNR A2 A0 None AM-ADV C-A1 AM-PNC AM-LOC A4 R-A1 Other Train 2,210 89 3 2,302 980 592 161 127 112 94 88 78 46 33 25 11 10 40 Devel 515 15 0 530 222 138 35 27 28 23 19 23 6 8 4 2 2 8 Test 672 38 2 712 309 172 46 38 36 31 35 26 16 12 10 5 2 16 Table 2: Basic numeric analysis for PB-FOC. The first 4 rows indicate the number of unique roles each negation belongs to, the rest indicate the counts for each role. • Semantic roles using the labeler described by (Punyakanok et al., 2008); and • Verbal negation, indicates with ‘N’ if that token correspond to a verbal negation for which focus must be predicted. Figure 2 provides a sample of PB-FOC. Knowing that the original focus annotations were done on top of PropBank and that focus corresponds to a single role, semantic role information is key to predict the focus. In Table 2, we show some basic numeric analysis regarding focus annotation and the automatically obtained semantic role labels. Most instances of focus belong to a single role in the three splits and the most common role focus belongs to is A1, followed by AM-NEG,"
S12-1035,D07-1002,0,0.00978739,"cs (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and com"
S12-1035,P10-1071,0,0.0153104,"dicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a"
S12-1035,P03-1002,0,0.0130037,"d Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is"
S12-1035,W08-2121,0,0.0715168,"Missing"
S12-1035,H05-1059,0,0.00829332,"presented in Table 1. More information about the annotation guidelines is provided by Morante et al. (2011) and Morante and Daelemans (2012), including inter-annotator agreement. The corpus was preprocessed at the University of Oslo. Tokenization was obtained by the PTBcompliant tokenizer that is part of the LinGO English Resource Grammar. 4 4 268 Training 65,450 3644 848 23.27 984 30 887 616 http://moin.delph-in.net/ Apart from the gold annotations, the corpus was provided to participants with additional annotations: 3.2 Semantic roles focus belongs to • Lemmatization using the GENIA tagger (Tsuruoka and Tsujii, 2005), version 3.0.1, with the ’-nt’ command line option. GENIA PoS tags are complemented with TnT PoS tags for increased compatibility with the original PTB. • Parsing with the Charniak and Johnson (2005) reranking parser.5 For compatibility with PTB conventions, the top-level nodes in parse trees (‘S1’), were removed. The conversion of PTB-style syntax trees into CoNLL-style format was performed using the CoNLL 2005 Shared Task software.6 PB-FOC: Focus Annotation We have adapted the only previous annotation effort targeting focus of negation for PB-FOC (Blanco and Moldovan, 2011). This corpus pro"
S12-1035,J12-2005,0,0.419944,"charge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC corpus). Focus annotation is restricted to verbal negations annotated with MNEG in PropBank, and all the words belonging to a semantic role are selected as focus. An annotated example"
S12-1035,W08-0606,0,0.689454,"rt of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002). Although negation is a very relevant and complex semantic aspect of language, current proposals to annotate meaning either dismiss negation or only treat it in a partial manner. The interest in automatically processing negation originated in the medical domain (Chapman et al., 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and f"
S12-1035,W10-3111,0,0.13446,"negation or only treat it in a partial manner. The interest in automatically processing negation originated in the medical domain (Chapman et al., 2001), since clinical reports and discharge summaries must be reliably interpreted and indexed. The annotation of negation and hedge cues and their scope in the BioScope corpus (Vincze et al., 2008) represented a pioneering effort. This corpus boosted research on scope resolution, especially since it was used in the CoNLL 2010 Shared Task (CoNLL ST 2010) on hedge detection (Farkas et al., 2010). Negation has also been studied in sentiment analysis (Wiegand et al., 2010) as a means to determine the polarity of sentiments and opinions. Whereas several scope detectors have been developed using BioScope (Morante and Daelemans, 2009; Velldal et al., 2012), there is a lack of corpora and tools to process negation in general domain texts. This is why we have prepared new corpora for scope and focus detection. Scope is annotated in Conan Doyle stories (CD-SCO corpus). For each negation, the cue, its scope and the negated event, if any, are marked as shown in example (1a). Focus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank (PB-FOC"
S12-1035,N09-2004,0,0.00938611,"rence on Lexical and Computational Semantics (*SEM) each year hosts a shared task on semantic related topics. In its first edition held in 2012, the shared task was dedicated to resolving the scope and focus of negation. This paper presents the specifications, datasets and evaluation criteria of the task. An overview of participating systems is provided and their results are summarized. 1 Introduction Semantic representation of text has received considerable attention these past years. While early shallow approaches have been proven useful for several natural language processing applications (Wu and Fung, 2009; Surdeanu et al., 2003; Shen and Lapata, 2007), the field is moving towards analyzing and processing complex linguistic phenomena, such as metaphor (Shutova, 2010) or modality and negation (Morante and Sporleder, 2012). The *SEM 2012 Shared Task is devoted to negation, specifically, to resolving its scope and focus. Negation is a grammatical category that comprises devices used to reverse the truth value of propositions. Broadly speaking, scope is the part of the meaning that is negated and focus the part of the scope that is most prominently or explicitly negated (Huddleston and Pullum, 2002"
S15-2133,P09-1068,0,0.0302034,"ent levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output"
S15-2133,chambers-jurafsky-2010-database,0,0.0132799,"ation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output of a state of the art system,"
S15-2133,I11-1012,0,0.0308787,"nteractions between the participants involved in an event individually; • Event relations: in an entity based representation, event mentions with more than one entity as their participants will be repeated in the final representation (both at in-document at crossdocument levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have"
S15-2133,R13-1021,1,0.8536,"the participants involved in an event individually; • Event relations: in an entity based representation, event mentions with more than one entity as their participants will be repeated in the final representation (both at in-document at crossdocument levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 W"
S15-2133,lopez-de-lacalle-etal-2014-predicate,0,0.0274096,"Missing"
S15-2133,S10-1063,0,0.124598,"SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output of a state of the art system, TIPSem (Llorens et al., 2010), for event detection and temporal relations; • SPINOZA VU 2 is entirely based on data from the NWR pipeline including the temporal (TLINKs) and causal relation (CLINKs) layers. The final output is based on a dedicated rulebased module, the TimeLine (TML) module. We will describe in the following paragraphs how each subtask has been tackled with respect to each version of the system. Entity identification Entity identification relies on the entity detection and disambiguation layer (NERD) of the NWR pipeline. Each detected entity is associated with a URI (a unique identifier), either from DBpe"
S15-2133,S13-2001,0,0.0340615,"extraction. 1 Introduction This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines. In this task, a timeline is defined as a set of chronologically anchored and ordered events extracted from a corpus spanning over a (large) period of time with respect to a target entity. Cross-document timeline extraction benefits from previous works and evaluation campaigns in Temporal Processing, such as the TempEval evaluation campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) and aims at promoting research in temporal processing by tackling the following issues: cross-document and cross-temporal event detection and ordering; event coreference (indocument and cross-document); and entity-based temporal processing. The SPINOZA VU system is based on the NewsReader (NWR) NLP pipeline (Agerri et al., 2013; Beloki et al., 2014), which has been developed within the context of the NWR project1 and provides multi-layer annotations over raw texts from tokenization up to temporal relations. The goal of the NWR project is to build structured event indexes from large volumes of"
S15-2133,S07-1014,0,0.171512,"towards a more complex task such as storyline extraction. 1 Introduction This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines. In this task, a timeline is defined as a set of chronologically anchored and ordered events extracted from a corpus spanning over a (large) period of time with respect to a target entity. Cross-document timeline extraction benefits from previous works and evaluation campaigns in Temporal Processing, such as the TempEval evaluation campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) and aims at promoting research in temporal processing by tackling the following issues: cross-document and cross-temporal event detection and ordering; event coreference (indocument and cross-document); and entity-based temporal processing. The SPINOZA VU system is based on the NewsReader (NWR) NLP pipeline (Agerri et al., 2013; Beloki et al., 2014), which has been developed within the context of the NWR project1 and provides multi-layer annotations over raw texts from tokenization up to temporal relations. The goal of the NWR project is to build"
S15-2133,S10-1010,1,\N,Missing
S16-1193,agerri-etal-2014-ixa,0,0.0317586,"have used the CRF++ tool with default settings for the regularization algorithm (L2)2 for all tasks. The final output is obtained by converting the output of 7 different classifiers into the task representation format, i.e. anafora xml files. In the following subsections, we describe the preprocessing steps, which is common for all subtasks, and the specific system for each subtask. 1 For obtaining scripts and trained models contact the authors. 2 https://taku910.github.io/crfpp/#links 1242 2.1 Preprocessing All text files have been preprocessed by using two different tools: the IXA-pipeline (Agerri et al., 2014)3 and the Stanford CoreNLP tool (Manning et al., 2014). From the IXA pipeline, we used the tokenization, offset and sentence splitting modules. We then passed the tokenized data to the Stanford CoreNLP tool in order to extract additional basic annotation layers such as lemmatization, part-ofspeech tagging, and dependency parsing. The preprocessing step outputs the texts in a tab separated column format. After preprocessing the text files, we merged the preprocessed text with the gold annotations, which were exported from the anafora xml files into a tabcolumn separated files. 2.2 Span Detectio"
S16-1193,S16-1165,0,0.0418554,", 2010; UzZaman et al., 2013) in the framework of several shared tasks where systems were challenged to extract the relevant components of a document timeline: temporal expressions, The setting of the 2015 and 2016 SemEval Clinical TempEval Tasks is similar to previous TempEval campaigns, with the two main differences: i.) the domain , i.e. (colon) cancer clinical notes; and ii.) the annotation scheme, i.e. the THYME annotation scheme (Styler IV et al., 2014), an extended version of TimeML (Pustejovsky et al., 2003a). Similarly to the previous edition, the SemEval 2016 Clinical TempEval task (Bethard et al., 2016) consists of the following six subtasks: temporal expression detection (TS) and attribute classification (TA), event detection (ES) and attribute classification (EA), temporal relation detection and classification of an event with respect to the Document Creation Time (DR), and, finally, narrative container relation identification (CR). Systems are evaluated in two phases: Phase 1, which addressed all six subtasks from raw data, and Phase 2, where target entities, such as events and temporal expressions (including their attributes), were given and the systems were evaluated 1241 Proceedings of"
S16-1193,S13-2002,0,0.0396354,"Missing"
S16-1193,S10-1063,0,0.0201236,"ije Univeristeit Amsterdam De Boelelaan 1105 1081 HV Amsterdam, The Netherlands {t.caselli,r.morantevallejo}@vu.nl Abstract event mentions, and temporal relations. Several evaluations have shown the capabilities and limits of both the annotated resources and the systems. For instance, the best system in TempEval-3 (Bethard, 2013) reports 0.398 F1 on Temporal Relation Detection and Classification from raw text. The development of temporally annotated corpora has boosted research in languages other than English such as Italian (Caselli et al., 2014), French (Arnulphy et al., 2015), and Spanish (Llorens et al., 2010), among others. Recently, interest in temporal processing has moved forward in two directions: cross-document timeline extraction (Minard et al., 2015) and domain adaptation (Sun et al., 2013; Bethard et al., 2015). This paper describes VUACLTL, the system the CLTL Lab submitted to the SemEval 2016 Task Clinical TempEval. The system is based on a purely data-driven approach based on a cascade of seven CRF classifiers which use generic features and little domain knowledge. The challenge consisted in six subtasks related to temporal processing clinical notes from raw text (event and temporal exp"
S16-1193,P14-5010,0,0.00497734,"e regularization algorithm (L2)2 for all tasks. The final output is obtained by converting the output of 7 different classifiers into the task representation format, i.e. anafora xml files. In the following subsections, we describe the preprocessing steps, which is common for all subtasks, and the specific system for each subtask. 1 For obtaining scripts and trained models contact the authors. 2 https://taku910.github.io/crfpp/#links 1242 2.1 Preprocessing All text files have been preprocessed by using two different tools: the IXA-pipeline (Agerri et al., 2014)3 and the Stanford CoreNLP tool (Manning et al., 2014). From the IXA pipeline, we used the tokenization, offset and sentence splitting modules. We then passed the tokenized data to the Stanford CoreNLP tool in order to extract additional basic annotation layers such as lemmatization, part-ofspeech tagging, and dependency parsing. The preprocessing step outputs the texts in a tab separated column format. After preprocessing the text files, we merged the preprocessed text with the gold annotations, which were exported from the anafora xml files into a tabcolumn separated files. 2.2 Span Detection (ES, TS) and Attributes Classification (EA, TA) Task"
S16-1193,S13-2001,0,0.0392726,"or all the subtasks. 1 Introduction Temporal Processing is becoming more and more important for improving access to content. The availability of timelines (either event-centric or entity-centric) can help improving more complex semantically-focused tasks such as Question Answering, Text Summarization, and Textual Entailment, among others. Furthermore, timelines can be further exploited for monitoring the development in time of different phenomena, e.g. the opinions in debates. Temporal Processing research has mainly focused on the newswire domain (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) in the framework of several shared tasks where systems were challenged to extract the relevant components of a document timeline: temporal expressions, The setting of the 2015 and 2016 SemEval Clinical TempEval Tasks is similar to previous TempEval campaigns, with the two main differences: i.) the domain , i.e. (colon) cancer clinical notes; and ii.) the annotation scheme, i.e. the THYME annotation scheme (Styler IV et al., 2014), an extended version of TimeML (Pustejovsky et al., 2003a). Similarly to the previous edition, the SemEval 2016 Clinical TempEval task (Bethard et al., 2016) consist"
S16-1193,P15-4022,0,0.0428954,"Missing"
S16-1193,S07-1014,0,0.0768811,"e results, which are not equally competitive for all the subtasks. 1 Introduction Temporal Processing is becoming more and more important for improving access to content. The availability of timelines (either event-centric or entity-centric) can help improving more complex semantically-focused tasks such as Question Answering, Text Summarization, and Textual Entailment, among others. Furthermore, timelines can be further exploited for monitoring the development in time of different phenomena, e.g. the opinions in debates. Temporal Processing research has mainly focused on the newswire domain (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) in the framework of several shared tasks where systems were challenged to extract the relevant components of a document timeline: temporal expressions, The setting of the 2015 and 2016 SemEval Clinical TempEval Tasks is similar to previous TempEval campaigns, with the two main differences: i.) the domain , i.e. (colon) cancer clinical notes; and ii.) the annotation scheme, i.e. the THYME annotation scheme (Styler IV et al., 2014), an extended version of TimeML (Pustejovsky et al., 2003a). Similarly to the previous edition, the SemEval 2016 Clinica"
S16-1193,S10-1010,1,\N,Missing
S16-1193,S15-2132,0,\N,Missing
S16-1193,S15-2136,0,\N,Missing
S16-1193,Q14-1012,0,\N,Missing
W07-2449,bunt-2006-dimensions,0,0.161577,"e no longer simulated, but generated automatically. 2 DIT In Dynamic Interpretation Theory (DIT) (Bunt, 2000), a dialogue is modelled as a sequence of utterances expressing sets of dialogue acts. These are semantic units, operating on the information states of the participants. Formally, a dialogue act in DIT consists of a semantic content and a communicative function, the latter specifying how the information state of the addressee is to be updated with the former upon understanding the corresponding utterance. Communicative functions are organised in a taxonomy1 consisting of 10 dimensions (Bunt, 2006) that reflect different aspects of communication speakers may address in their dialogue behaviour. In each utterance, several dialogue acts can be performed, each dialogue act from a different dimension. The overview below shows a layered structure in which the dimensions are given in boldface italic. So, besides the task/domain dimension, 1 See web page http://let.uvt.nl/general/people/bunt/docs/ditschema2.html. the taxonomy provides for several dialogue control dimensions, organised into the layers of feedback, interaction management (IM) and social obligations management (SOM). • Task/domai"
W07-2449,W06-1306,1,0.767489,"stem (9). These beliefs are also considered by the system to be mutually believed. Such beliefs about mutual beliefs are placed in the common ground of the cognitive context. Utterance U4 is represented by a dialogue act in the Social Obligations Management dimension, with the communicative function T HANKING. Updating the context with this dialogue act creates a so-called reactive pressure, set in the Social Context. The system releases this pressure in utterance S5 by means of a T HANKING -D OWNPLAY. 5 Dialogue act generation A more recent feature of DISCUS is that a dialogue act generator (Keizer and Bunt, 2006) can be connected to the simulator and can take care of generating the system’s actions. In this case, system utterances no longer need to be simulated through the GUI. The dialogue act generator also follows the multidimensional organisation of the taxonomy in that it consists of several agents, each dedicated to the generation of dialogue acts from a particular dimension. As illustrated in Figure 3, the dialogue act agents monitor and write to different parts of the context model. Dialogue act candidates produced by these agents are recorded in the so-called ‘dialogue future’ as part of the"
W08-1129,W07-2302,0,0.0688957,"irstname.lastname@ua.ac.be Abstract In this paper we describe our machine learning approach to the generation of referring expressions. As our algorithm we use memory-based learning. Our results show that in case of predicting the TYPE of the expression, having one general classifier gives the best results. On the contrary, when predicting the full set of properties of an expression, a combined set of specialized classifiers for each subdomain gives the best performance. 1 Introduction In this paper we describe the systems with which we participated in the GREC task of the REG 2008 challenge (Belz and Varges, 2007). The GREC task concerns predicting which expression is appropriate to refer to a particular discourse referent in a certain position in a text, given a set of alternative referring expressions for selection. The organizers provided the GREC corpus that consists of 2000 texts collected from Wikipedia, from 5 different subdomains (people, cities, countries, mountains and rivers) . One of the main goals of the task is to discover what kind of information is useful in the input to make the decision between candidate referring expressions. We experimented with a pool of features and several machin"
W08-2128,C04-1186,0,0.0628602,"Missing"
W08-2128,S07-1038,1,0.801742,"n memory has been argued to provide a key advantage over abstracting methods in NLP that ignore exceptions and subregularities (Daelemans et al., 1999). Memory-based algorithms have been previously applied to semantic role labeling. Van den Bosch et al. (2004) participated in the CoNLL2004 shared task with a system that extended the basic memory-based learning method with class n-grams, iterative classifier stacking, and automatic output post-processing. Tjong Kim Sang et al. (2005) participated in the CoNLL2005 shared task with a system that incorporates spelling error correction techniques. Morante and Busser (2007) participated in the SemEval-2007 competition with a semantic role labeler for Spanish based on gold standard constituent syntax. These systems use different types of constituent syntax (shallow parsing, full parsing). We are aware of two systems that perform semantic role labeling based on dependency syntax previous to the CoNLL-2008 shared task. Hacioglu (2004) converts the data from the CoNLL-2004 shared task into dependency trees and uses support vector machines. Morante (2008) describes a memorybased semantic role labeling system for Spanish based on gold standard dependency syntax. We de"
W08-2128,W08-2121,0,0.101009,"Missing"
W08-2128,W06-2933,0,0.0284678,"account for relative differences in discriminative power of the features. 2.1 Syntactic dependencies The MaltParser 0.41 (Nivre, 2006; Nivre et al., 2007) is an inductive dependency parser that uses four essential components: a deterministic algorithm for building labeled projective dependency graphs; history-based feature models for predicting the next parser action; support vector machines for mapping histories to parser actions; and graph transformations for recovering nonprojective structures. The learner type used was support vector machines, with the same parameter options reported by (Nivre et al., 2006). The parser algorithm used was Nivre, with the options and model (eng.par) for English as specified on http://w3.msi.vxu.se/users/jha/conll07/. The tagset.pos, tagset.cpos and tagset.dep were extracted from the training corpus. 2.2 Semantic dependencies The semantics task consists of finding the predicates, assigning a PropBank or a NomBank frame to them and extracting their semantic role dependencies. Because of lack of resources, we did not have time to develop a word sense disambiguation system. So, predicates were assigned the frame ‘.01’ by default. The system handles the semantic role l"
W08-2128,W05-0637,0,0.208478,"Missing"
W08-2128,W04-2414,1,0.883024,"Missing"
W08-2128,morante-2008-semantic,1,\N,Missing
W09-1105,W06-2920,0,0.00604904,"0.01 13.55 877 4.98 4.84 Papers 9 2670 60935 5566 26.24 11.27 27.67 29.55 17.00 0.03 12.70 389 8.81 7.61 Abstracts 1273 11871 282243 14506 26.43 3.17 30.49 35.93 19.76 10.63 13.45 1848 9.43 8.06 6.33 5.69 8.55 97.64 2.35 81.77 18.22 85.70 14.29 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format. Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of a sequence of tokens, each one starting on a new line. 4 Table 1: Statistics about the subcorpora in the BioScope corpus and the negation scopes (“Av”. stands for average). The BioScope corpus consists of three parts: clinical free-texts (radiology reports), biological full papers and biological paper abstracts from the GENIA corpus (Collier et al., 1999). Table 1 shows statistics about the corpora. Negation signals are represented by one or more tokens. Only one negation"
W09-1105,E99-1043,0,0.589616,"Missing"
W09-1105,W08-2128,1,0.881647,"Missing"
W09-1105,W08-0606,0,0.667574,"Missing"
W09-1105,W05-0637,0,0.0461433,"Missing"
W09-1105,H05-1059,0,0.0194498,"Missing"
W09-1203,burchardt-etal-2006-salsa,0,0.0646198,"Missing"
W09-1203,D07-1121,0,0.17029,"Missing"
W09-1203,kawahara-etal-2002-construction,0,0.0175209,"performing the identification and labeling of syntactic and semantic dependencies in multiple languages. Dependencies are truly jointly learned, i.e. as if they were a single task. The system works in two phases: a classification phase in which three classifiers predict different types of information, and a ranking phase in which the output of the classifiers is combined. 1 Introduction In this paper we present the machine learning system submitted to the CoNLL Shared Task 2009 (Hajiˇc et al., 2009). The task is an extension to multiple languages (Burchardt et al., 2006; Hajiˇc et al., 2006; Kawahara et al., 2002; Palmer and Xue, 2009; Surdeanu et al., 2008; Taul´e et al., 2008) of the CoNLL Shared Task 2008, combining the identification and labeling of syntactic dependencies and semantic roles. Our system is a joint-learning system tested in the “closed” challenge, i.e. without making use of external resources. Our system operates in two phases: a classification phase in which three memory-based classifiers predict different types of information, and a ranking phase in which the output of the classifiers is combined by ranking the predictions. Semantic and syntactic dependencies are jointly learned a"
W09-1203,W08-2128,1,0.860736,"Missing"
W09-1203,W08-2121,0,0.11314,"Missing"
W09-1203,taule-etal-2008-ancora,0,0.0626567,"Missing"
W09-1304,W06-2920,0,0.0108606,"73.28 26.71 76.55 23.44 82.45 17.54 Table 1: Statistics about the subcorpora in the BioScope corpus and the hedge scopes (“Av”. stands for average). The texts have been processed with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format. Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of a se2 Web page: www.inf.u-szeged.hu/rgai/bioscope. quence of tokens, each one starting on a new line. 30 4 Finding the scope of hedge cues We model this task in the same way that we modelled the task for finding the scope of negation (Morante and Daelemans, 2009), i.e., as two consecutive classification tasks: a first one that consists of classifying the tokens of a sentence as being at the beginning of a hedge signal, inside or outside. This allows the system to find mult"
W09-1304,E99-1043,0,0.694642,"Missing"
W09-1304,W08-0607,0,0.618012,"Available at http://www.benmedlock.co.uk/hedgeclassif.html. Medlock and Briscoe (2007), whereas with a lemma representation the system achieves a peak performance of 0.8 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or nonspeculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same clasification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigning an information gain measure and by assigning weights semi– automatically depending on their types and centrality to hedging. The system achieves results of 0.85 BEP. As mentioned earlier, we are not aware of research that has focused on learning the scope of hedge signals inside or outside of the biomedical domain, which makes a direct comparison with the approaches des"
W09-1304,W04-3103,0,0.669823,"Missing"
W09-1304,P07-1125,0,0.61545,"in patient documents into controlled vocabulary terms”. The system uses a semantic grammar that consists of rules that specify well-formed semantic patterns. The extracted findings are assigned one of five types of modality information: no, low certainty, moderate certainty, high certainty and cannot evaluate. Di Marco and Mercer (2005) use hedging information to classify citations. They observe that citations appear to occur in sentences marked with hedging cues. Work on hedging in the machine learning field has as a goal to classify sentences into speculative or definite (non speculative). Medlock and Briscoe (2007) provide a definition of what they consider to be hedge instances and define hedge classification as a weakly supervised machine learning task. The method they use to derive a learning model from a seed corpus is based on iteratively predicting labels for unlabeled training samples. They report experiments with SVMs on a dataset that they make publicly available1 . The experiments achieve a recall/precision break even point (BEP) of 0.76. They apply a bag-of-words (BOG) approach to sample representation. Medlock (2008) presents an extension of this work by experimenting with more features (par"
W09-1304,W09-1105,1,0.771533,"= “X7.5.2”&gt; might &lt;/cue&gt; be involved &lt;xcope id=“X7.5.1”&gt;in terminal granulocyte differentiation &lt;cue type= “speculation” ref=“X7.5.1” &gt;or&lt;/cue&gt; in regulating granulocyte functionality &lt;/xcope&gt;&lt;/xcope&gt;&lt;/xcope&gt;. Proceedings of the Workshop on BioNLP, pages 28–36, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Contrary to current practice to only detect modality, our system also determines the part of the sentence that is hedged. We are not aware of other systems that perform this task. The system is based on a similar system that finds the scope of negation cues (Morante and Daelemans, 2009). We show that the system performs well for this task and that the same scope finding approach can be applied to both negation and hedging. To investigate the robustness of the approach, the system is tested on three subcorpora of the BioScope corpus that represent different text types. Although the system was developed and tested on biomedical text, the same approach can also be applied to text from other domains. The paper is organised as follows. In Section 2, we summarise related work. In Section 3, we describe the corpus on which the system has been developed. In Section 4, we introduce t"
W09-1304,W08-0606,0,0.74984,"adjectives, adverbs, and nouns. Additionally, it includes also a variety of non–lexical cues. Light et 29 al. (2004) analyse the use of speculative language in MEDLINE abstacts. They studied the expression of levels of belief (hypothesis, tentative conclusions, hedges, and speculations) and annotated a corpus of abstracts in order to check if the distinction between high speculative, low speculative and definite sentences could be made reliably. They found that the speculative vs. definite distinction was reliable, but the distinction between low and high speculative was not. Thompson et al. (2008) report on a list of words and phrases that express modality in biomedical texts and put forward a categorisation scheme. The list and the scheme are validated by annotating 202 MEDLINE abstracts. Some NLP applications incorporate modality information. Friedman et al. (1994) develop a medical text processor “that translates clinical information in patient documents into controlled vocabulary terms”. The system uses a semantic grammar that consists of rules that specify well-formed semantic patterns. The extracted findings are assigned one of five types of modality information: no, low certaint"
W09-1304,P08-1033,0,0.696896,"ly available1 . The experiments achieve a recall/precision break even point (BEP) of 0.76. They apply a bag-of-words (BOG) approach to sample representation. Medlock (2008) presents an extension of this work by experimenting with more features (part-of-speech (PoS), lemmas, and bigrams). Experiments show that the PoS representation does not yield significant improvement over the results in 1 Available at http://www.benmedlock.co.uk/hedgeclassif.html. Medlock and Briscoe (2007), whereas with a lemma representation the system achieves a peak performance of 0.8 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or nonspeculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same clasification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are"
W09-1304,H05-1059,0,0.01795,"Missing"
W09-1408,W06-2920,0,0.00903165,"syntactic information as features for the machine learner. GDep is a a dependency parser for biomedical text trained on the Tsujii Lab’s GENIA treebank. The dependency parser predicts for every word the partof-speech tag, the lemma, the syntactic head, and the dependency relation. In addition to these regular dependency tags it also provides information about the IOB-style chunks and named entities. The classifiers use the output of GDep in addition to some frequency measures as features. We represent the data into a columns format, following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), in which sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of tokens, each one starting on a new line. 4.1 Phase 1: Entity Detection In the first phase, a memory based classifier predicts for every word in the corpus whether it is an entity or not and the type of entity. In this setting, entity refers to what in the shared task definition are events and entities other than proteins. Classes are defined in the IOB-style3 in order to find entities that span over multiple words. Figure 1 shows a simplified version of a sentence in wh"
W09-1408,W06-0602,0,0.0655271,"Missing"
W09-1408,E99-1043,0,0.0900074,"Missing"
W09-1408,W09-1105,1,0.814887,"of the shared task definition: rich semantics, a text-bound approach, and decomposition of linguistic phenomena. Memory-based algorithms have been successfully applied in language processing to a wide range of linguistic tasks, from phonology to semantic analysis. Our goal was to investigate the performance of a memory–based approach to the event extraction task, using only the information available in the training corpus and modelling the task applying an approach similar to the one that has been applied to tasks like semantic role labeling (Morante et al., 2008) or negation scope detection (Morante and Daelemans, 2009). In Section 2 we briefly describe the task. Section 3 reviews some related work. Section 4 presents the system, and Section 5 the results. Finally, some conclusions are put forward in Section 6. 2 Task description The BioNLP Shared Task 2009 on event extraction consists of recognising bio-molecular events in biomedical texts, focusing on molecular events involving proteins and genes. An event is defined as a relation that holds between multiple entities that fulfil different roles. Events can participate in one type Proceedings of the Workshop on BioNLP: Shared Task, pages 59–67, c Boulder, C"
W09-1408,W08-2128,1,0.887388,"Missing"
W09-1408,D07-1111,0,0.0154539,"e detected. In the second phase, event participants and arguments are identified. In the third phase, postprocessing heuristics select the best frame for each event. Parameterisation of the classifiers used in Phases 1 and 2 was performed by experimenting with sets of parameters on the development set. We experimented with manually selected parameters and with parameters selected by a genetic algorithm, but the parameters found by the genetic algorithm did not yield better results than the manually selected parameters As a first step, we preprocess the corpora with the GDep dependency parser (Sagae and Tsujii, 2007) so that we can use part-of-speech tags and syntactic information as features for the machine learner. GDep is a a dependency parser for biomedical text trained on the Tsujii Lab’s GENIA treebank. The dependency parser predicts for every word the partof-speech tag, the lemma, the syntactic head, and the dependency relation. In addition to these regular dependency tags it also provides information about the IOB-style chunks and named entities. The classifiers use the output of GDep in addition to some frequency measures as features. We represent the data into a columns format, following the sta"
W09-1408,C08-1096,0,0.0129636,"Official results of Task 2. Approximate Span Matching/Approximate Recursive Matching. Results obtained on the development set are a little bit higher. For Task1 an overall F1 of 34.78 and for Task 2 33.54. For most event types precision and recall are unbalanced, the system scores higher in recall. Further research should focus on increasing precision because the system is predicting false positives. It would be possible to add a step in order to filter out the false positives by comparing word sequences with event patterns derived from the corpus, which is an approach taken in the system by Sasaki et al. (2008) . In the case of Binding events, both precision and recall are low. There are two explanations for this. In the first place, the first classifier misses almost half of the binding events. As an example, for the sentence in (8.1), the gold standard identifies as binding event the multiwords binds as a homodimer and form heterodimers, whereas the system identifies two binding events for the same sentence, binds and homodimer, none of which is correct because the correct one is the multiword unit. For the sentence in (8.2), the gold standard identifies as binding events bind, form homo-, and het"
W09-2417,S07-1018,1,0.851873,"nce resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank,"
W09-2417,W04-2412,0,0.0812467,"Missing"
W09-2417,W05-0620,0,0.313707,"Missing"
W09-2417,J02-3001,0,0.0966796,"s sentence boundaries. Specifically, the task aims at linking locally uninstantiated roles to their coreferents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inf"
W09-2417,W04-0803,0,0.0306698,"so from co-reference resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (B"
W09-2417,S07-1008,0,0.0681195,"Missing"
W09-2417,C08-1084,1,0.879566,"Missing"
W09-2417,P86-1004,1,0.770233,"he fact that large-scale manual annotation projects such as FrameNet1 and PropBank2 typically present their annotations lexicographically by lemma rather than by source text. Furthermore, in the case of FrameNet, the annotation effort did not start out with the goal of exhaustive corpus annotation but instead focused on isolated instances of the target words sampled from a very large corpus, which did not allow for a view of the data as ‘full-text annotation’. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore, 1977). In early work, Palmer et al. (1986) discussed filling null complements from context by using knowledge about individual predicates and ten1 http://framenet.icsi.berkeley.edu/ http://verbs.colorado.edu/˜mpalmer/ projects/ace.html 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106–111, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics dencies of referential chaining across sentences. But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semanti"
W09-2417,D07-1002,0,0.0290019,"in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering. The reason for this is that SRL has traditionally been viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic information. This view of SRL as a sentence-"
W09-2417,P03-1002,0,0.0620561,"years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering. The reason for this is that SRL has traditionally been viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic"
W09-2417,W08-2121,0,0.0907887,"Missing"
W09-2417,S07-1017,1,\N,Missing
W09-2417,H86-1011,1,\N,Missing
W09-2417,erk-pado-2004-powerful,0,\N,Missing
W10-1916,D07-1111,0,0.0177396,"POS from event to ROOT and from PA to ROOT. • Normalised distance in number of tokens between event an potential argument in the string of words. We use an IB1 memory–based algorithm as implemented in TiMBL (version 6.1.2) 1 (Daelemans et al., 2009), a memory-based classifier based on the k-nearest neighbor rule. The IB1 algorithm was parameterised by using Jeffrey divergence as the similarity metric, gain ratio for feature weighting, using 5 k-nearest neighbors, and weighting System description We perform two preprocessing steps. First, we extract the text and parse it with the GDep parser (Sagae and Tsujii, 2007) and then we convert the corpus from xml into CoNLL format. Table 1 shows a preprocessed sentence. The system performs argument identification and semantic role assignment in a single step, assuming gold 1 TiMBL: http://ilk.uvt.nl/timbl 126 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 126–127, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 WORD Lrp binds to two regions in the dadAX promoter region of Escherichia coli to repress and activate transcription directly"
W10-1916,C08-1096,0,\N,Missing
W10-3006,W08-0607,0,0.634068,"ension of this work by experimenting with more features (part-of-speech, lemmas, and bigrams). With a lemma representation the system achieves a peak performance of 0.80 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or non-speculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same classification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigning an information gain measure and by assigning weights semi–automatically depending on their types and centrality to hedging. The system achieves results of 0.85 BEP. As for Task 2, previous work (Morante and ¨ ur and Radev, 2009) has Daelemans, 2009; Ozg¨ focused on finding the scope of hedge cues in the BioScope corpus (Vincze et al., 2008). Both systems approach the t"
W10-3006,W04-3103,0,0.181568,"Hedging has been broadly treated from a theoretical perspective. The term hedging is originally due to Lakoff (1972). Palmer (1986) defines a term related to hedging, epistemic modality, which expresses the speaker’s degree of commitment to the truth of a proposition. Hyland (1998) focuses specifically on scientific texts. He proposes a pragmatic classification of hedge expressions based on an exhaustive analysis of a corpus. The catalogue of hedging cues includes modal auxiliaries, epistemic lexical verbs, epistemic adjectives, adverbs, nouns, and a variety of non–lexical cues. Light et al. (2004) analyse the use of speculative language in MEDLINE abstracts. Some NLP applications incorporate modality information (Friedman et al., 1994; Di Marco and Mercer, 2005). As for annotated corpora, Thompson et al. (2008) report on a list of words and phrases that express modality in biomedical texts and put forward a categorisation scheme. Additionally, the BioScope corpus (Vincze et al., 2008) consists of a collection of clinical free-texts, biological full papers, and biological abstracts annotated with negation and speculation cues and their scope. Although only a few pieces of research have"
W10-3006,P07-1125,0,0.51731,"putational Linguistics model from a seed corpus is based on iteratively predicting labels for unlabeled training samples. They report experiments with SVMs on a dataset that they make publicly available3 . The experiments achieve a recall/precision break even point (BEP) of 0.76. They apply a bag-of-words approach to sample representation. Medlock (2008) presents an extension of this work by experimenting with more features (part-of-speech, lemmas, and bigrams). With a lemma representation the system achieves a peak performance of 0.80 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or non-speculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same classification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigni"
W10-3006,W06-2920,0,0.00400713,"m the training data. 3 the xml format to the CoNLL format is a source of error propagation, we convert the gold CoNLL files into xml format and we run the scorer provided by the task organisers. The results obtained are listed in Table 2. Task 1 Task 2 WIKI BIO-ART BIO-ABS BIO-ART BIO-ABS F1 100.00 100.00 100.00 99.10 99.66 Preprocessing Table 2: Evaluation of the conversion from xml to CoNLL format. As a first step, we preprocess the data in order to extract features for the machine learners. We convert the xml files into a token-per-token representation, following the standard CoNLL format (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of a sequence of tokens, each one starting on a new line. The WIKI data are processed with the Memory Based Shallow Parser (MBSP) (Daelemans and van den Bosch, 2005) in order to obtain lemmas, part-of-speech (PoS) tags, and syntactic chunks, and with the MaltParser (Nivre, 2006) in order to obtain dependency trees. The BIO data are processed with the GDep parser (Sagae and Tsujii, 2007) in order to get the same information. # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20"
W10-3006,W09-1304,1,0.908589,"ces and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigning an information gain measure and by assigning weights semi–automatically depending on their types and centrality to hedging. The system achieves results of 0.85 BEP. As for Task 2, previous work (Morante and ¨ ur and Radev, 2009) has Daelemans, 2009; Ozg¨ focused on finding the scope of hedge cues in the BioScope corpus (Vincze et al., 2008). Both systems approach the task in two steps, identifying the hedge cues and finding their scope. The main difference between the two systems is that Morante and Daelemans (2009) perform the sec¨ ond phase with a machine learner, whereas Ozgur and Radev (2009) perform the second phase with a rule-based system that exploits syntactic information. The approach to resolving the scopes of hedge cues that we present in this paper is similar to the approach followed in Morante and Daelemans (2009) in that the task is modelled in the same way. A difference between the two systems is that this system uses only one classifier to solve Task 2, whereas the system described in Morante and Daelemans (2009) used three classifiers and a metof classifying the tokens of a sentence as"
W10-3006,D09-1145,0,0.874751,"ocedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same classification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues are weighted by automatically assigning an information gain measure and by assigning weights semi–automatically depending on their types and centrality to hedging. The system achieves results of 0.85 BEP. As for Task 2, previous work (Morante and ¨ ur and Radev, 2009) has Daelemans, 2009; Ozg¨ focused on finding the scope of hedge cues in the BioScope corpus (Vincze et al., 2008). Both systems approach the task in two steps, identifying the hedge cues and finding their scope. The main difference between the two systems is that Morante and Daelemans (2009) perform the sec¨ ond phase with a machine learner, whereas Ozgur and Radev (2009) perform the second phase with a rule-based system that exploits syntactic information. The approach to resolving the scopes of hedge cues that we present in this paper is similar to the approach followed in Morante and Daele"
W10-3006,D07-1111,0,0.0074119,"he xml files into a token-per-token representation, following the standard CoNLL format (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of a sequence of tokens, each one starting on a new line. The WIKI data are processed with the Memory Based Shallow Parser (MBSP) (Daelemans and van den Bosch, 2005) in order to obtain lemmas, part-of-speech (PoS) tags, and syntactic chunks, and with the MaltParser (Nivre, 2006) in order to obtain dependency trees. The BIO data are processed with the GDep parser (Sagae and Tsujii, 2007) in order to get the same information. # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 WORD The structural evidence lends strong support to the inferred domain pair , resulting in a high confidence set of domain pairs . LEMMA The structural evidence lend strong support to the inferred domain pair , result in a high confidence set of domain pair . PoS CHUNK NE D DT B-NP O 3 JJ I-NP O 3 NN I-NP O 4 VBZ B-VP O 0 JJ B-NP O 6 NN I-NP O 4 TO B-PP O 6 DT B-NP O 11 JJ I-NP O 11 NN I-NP O 11 NN I-NP O 7 , O O 4 VBG B-VP O 4 IN B-PP O 13 DT B-NP O 18 JJ I-NP O 18 NN I-NP O 18 NN I-NP O 14 IN"
W10-3006,W10-3001,0,0.359779,"Missing"
W10-3006,P08-1033,0,0.163583,"010 Association for Computational Linguistics model from a seed corpus is based on iteratively predicting labels for unlabeled training samples. They report experiments with SVMs on a dataset that they make publicly available3 . The experiments achieve a recall/precision break even point (BEP) of 0.76. They apply a bag-of-words approach to sample representation. Medlock (2008) presents an extension of this work by experimenting with more features (part-of-speech, lemmas, and bigrams). With a lemma representation the system achieves a peak performance of 0.80 BEP, and with bigrams of 0.82 BEP. Szarvas (2008) follows Medlock and Briscoe (2007) in classifying sentences as being speculative or non-speculative. Szarvas develops a MaxEnt system that incorporates bigrams and trigrams in the feature representation and performs a complex feature selection procedure in order to reduce the number of keyword candidates. It achieves up to 0.85 BEP and 85.08 F1 by using an external dictionary. Kilicoglu and Bergler (2008) apply a linguistically motivated approach to the same classification task by using knowledge from existing lexical resources and incorporating syntactic patterns. Additionally, hedge cues ar"
W10-3006,W08-0606,0,0.608347,"Missing"
W11-0141,baker-etal-2010-modality,0,0.0143565,"e scope finding task that would be different based on observations from a corpus from a different domain (Section 4). 1 2 Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by moda"
W11-0141,W10-3110,0,0.0702159,"f the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; Ozg¨ and Radev, 2009; Agarwal and Yu, 2010; Li et al., 2010). Councill et al. (2010) process scopes of negation cues in a different corpus of product reviews, but this corpus is not publicly available. The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text (Farkas et al., 2010) boosted research on the topic. It consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences. Participating systems would, for example, produce the tagged sentence in (2)3 , in which propose, suggest and possible are identified as hedge cues and their scope is marked in agreement with the gold standard. We"
W11-0141,P09-2044,0,0.0140353,"hold in the world. Negation is a linguistic resource used to express negative polarity. Although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for a number of applications, such as biomedical text processing (Di Marco and Mercer, 2005; Chapman et al., 2001), opinion mining and sentiment analysis (Wilson et al., 2005), recognizing textual entailment (Snow et al., 2006), and automatic style checking (Ganter and Strube, 2009). In general, the treatment of modality and negation is very relevant for computational applications that process factuality (Saur´ı, 2008). For example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1 , which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: The atovaquone/proguanil combination has not been widely used yet in West Africa so it is unlikely that the patient was initially infected with an atovaquone-resistant strain. (1) So far two main tasks ha"
W11-0141,P10-1041,0,0.0329917,"on 3), and we point out aspects of the scope finding task that would be different based on observations from a corpus from a different domain (Section 4). 1 2 Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level w"
W11-0141,D10-1070,0,0.0114771,"to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; Ozg¨ and Radev, 2009; Agarwal and Yu, 2010; Li et al., 2010). Councill et al. (2010) process scopes of negation cues in a different corpus of product reviews, but this corpus is not publicly available. The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text (Farkas et al., 2010) boosted research on the topic. It consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences. Participating systems would, for example, produce the tagged sentence in (2)3 , in which propose, suggest and possible are identified as hedge cues and their scope is marked in agreement wit"
W11-0141,P07-1125,0,0.110652,"n observations from a corpus from a different domain (Section 4). 1 2 Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classific"
W11-0141,W09-1304,1,0.845103,"), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; Ozg¨ and Radev, 2009; Agarwal and Yu, 2010; Li et al., 2010). Councill et al. (2010) process scopes of negation cues in a different corpus of product reviews, but this corpus is not publicly available. The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text (Farkas et al., 2010) boosted research on the topic. It consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences. Participating systems would, for example, produce the tagged sentence in (2)3 , in which propose, suggest and possible are ident"
W11-0141,D08-1075,1,0.870073,"nd Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; Ozg¨ and Radev, 2009; Agarwal and Yu, 2010; Li et al., 2010). Councill et al. (2010) process scopes of negation cues in a different corpus of product reviews, but this corpus is not publicly available. The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language text (Farkas et al., 2010) boosted research on the topic. It consisted of identifying sentences containing uncertainty and recognizing speculative text spans inside sentences. Participating systems would, fo"
W11-0141,W10-3006,1,0.87553,"Missing"
W11-0141,D09-1145,0,0.0282011,"Missing"
W11-0141,W06-0305,0,0.0308548,"und in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues. It was first modelled as a classification problem by Morante et al. (2008). ¨ ur Later on several systems have been"
W11-0141,S10-1008,1,0.784109,"is not the case that tRNASec and tRNAPyl have usual secondary structures 515 4 Annotating scopes in a different domain The existing scope labelers have been trained on biomedical texts. However, it is reasonable to expect that texts from other domains contain different phenomena that would affect the systems performance. We are currently analysing negations and their scopes in a complete different corpus, The Hound of the Baskervilles (HB) by Conan Doyle. This corpus has been annotated with coreference and semantic roles for the SemEval Task Linking Events and Their Participants in Discourse (Ruppenhofer et al., 2010), and will be further annotated with negation and modality cues. Phenomena in this corpus show that whereas the scope of cues can be determined in a similar way as it is determined in biomedical texts, identifying 352 negation cues in certain contexts, which is the first part of the scope finding task, is not only a matter of lexical lookup: − Not all negative affixes are negation cues. For example the affix un- in unspoken does not negate its root morpheme. Unspoken does not mean ‘not spoken’, but ‘understood without the need for words’. Consequently, in (16) unspoken is not a negation cue. ("
W11-0141,N06-1005,0,0.0149348,"nformation as a counterfact, a fact that does not hold in the world. Negation is a linguistic resource used to express negative polarity. Although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for a number of applications, such as biomedical text processing (Di Marco and Mercer, 2005; Chapman et al., 2001), opinion mining and sentiment analysis (Wilson et al., 2005), recognizing textual entailment (Snow et al., 2006), and automatic style checking (Ganter and Strube, 2009). In general, the treatment of modality and negation is very relevant for computational applications that process factuality (Saur´ı, 2008). For example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1 , which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: The atovaquone/proguanil combination has not been widely used yet in West Africa so it is unlikely that the patient was initially infected with an a"
W11-0141,P10-1059,0,0.0318531,"that would be different based on observations from a corpus from a different domain (Section 4). 1 2 Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010] A cue is the lexical marker that expresses negation or modality. 350 2 Achievements in scope processing In the last years, several corpora have been annotated with information related to modality and polarity, which have made it possible to develop machine learning systems. Annotation has been performed at different levels: word (Hassan and Radev, 2010), expression (Baker et al., 2010; Toprak et al., 2010), sentence (Medlock and Briscoe, 2007), event (Saur´ı and Pustejovsky, 2009), discourse relation (Prasad et al., 2006), text (Amancio et al., 2010), and scope of negation and modality cues (Vincze et al., 2008). Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. BioScope is a freely available resource, that consists of three parts of medical and biological texts annotated with negation and hedge cues and their scope. The scope processing task is concerned with determining at a sentence level which tokens are affected by modality and negation cues"
W11-0141,W10-3105,0,0.165055,"Missing"
W11-0141,W08-0606,0,0.123564,"Missing"
W11-0141,H05-2018,0,0.0316481,"the world, whereas negative polarity is used to put information as a counterfact, a fact that does not hold in the world. Negation is a linguistic resource used to express negative polarity. Although the treatment of these topics in computational linguistics is relatively new compared to other areas like machine translation, parsing or semantic role labeling, incorporating information about modality and polarity has been shown to be useful for a number of applications, such as biomedical text processing (Di Marco and Mercer, 2005; Chapman et al., 2001), opinion mining and sentiment analysis (Wilson et al., 2005), recognizing textual entailment (Snow et al., 2006), and automatic style checking (Ganter and Strube, 2009). In general, the treatment of modality and negation is very relevant for computational applications that process factuality (Saur´ı, 2008). For example, information extraction systems may be confronted with fragments of texts like the one presented in (1)1 , which contains two negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being expressed: The atovaquone/proguanil combination has not been widely used yet in West Africa so it is unlike"
W11-0141,W10-2313,0,\N,Missing
W16-2819,bartalesi-lenzi-etal-2012-cat,0,0.0180456,". 169 sentences (n) 150 Round 1 139 91 100 Round 2 83 42 47 50 42 43 32 14 0 1 Task 1: Pilot annotation 2 3 4 annotators (n) 5 Figure 2: Distribution of annotations. This section reports on a pilot annotation experiment targeted at the first subtask. Five expert annotators were asked to identify those sentences in the editorial article that were COMMENTED UPON in the comment. A set of eight editorial articles (152 unique sentences, including titles) and a total of 62 comments were provided. In total, this came down to 1,186 sentences to be annotated. We used the Content Annotation Tool (CAT) (Lenzi et al., 2012) for the annotations. The experiment was performed in two rounds. First, simple instructions were given to the annotators to explore the data and task. For the second round, the instructions were refined by adding two simple rules: exclude titles (they are part of the meta-data), and include cases where a proposition is simply ‘mentioned’ rather than functioning as part of the argumentation. For example, the fact that the closing of Sweet Briar College is repeated in the comment below without its factual status beA deeper analysis of the annotated data and the annotation distribution shows the"
W16-2819,W10-0214,0,0.0249339,"itly mentioned or at least supposed opponent, as for instance in the rebutting of possible objections.” Therefore, these (implicit) interactions between participants should be given a central role when performing argument mining. In recent years, several studies have addressed the annotation and automatic classification of agreement and disagreement in online debates. The main difference between them is the annotation unit they have targeted, i.e. the textual units that are in (dis)agreement. Some studies focused on global (dis)agreement, i.e. the overall stance towards the main debate topic (Somasundaran and Wiebe, 2010). Other studies focused on local (dis)agreement, comparing pairs of posts (Walker 1 http://argmining2016.arg.tech/index. php/home/call-for-papers/unshared-task 160 Proceedings of the 3rd Workshop on Argument Mining, pages 160–165, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ernment to reduce teen pregnancies?) and article title describing the author’s stance (e.g. Publicly Funded Birth Control Is Crucial); and ii) Discussions (i.e. collections of comments from different users) about these editorial articles (Variant D). The remainder of this paper is st"
W16-2819,L16-1187,1,0.846655,"Missing"
W16-2819,walker-etal-2012-corpus,0,0.0611907,"Missing"
W16-2819,W14-2617,0,0.0324304,"Missing"
W16-2819,W12-3710,0,0.0423338,"Missing"
W16-2819,andreas-etal-2012-annotating,0,\N,Missing
W16-3207,P98-1013,0,0.142307,"Missing"
W16-3207,P09-1068,0,0.0338899,"Missing"
W16-3207,W16-3210,1,0.896283,"Missing"
W16-3207,D08-1048,0,0.0244628,"Missing"
W16-3207,Q14-1006,0,0.6518,"e used our categorization to manually annotate sentences containing negations in the Flickr30k corpus, with an agreement score of κ=0.67. With this paper, we hope to open up a broader discussion of subjective language in image descriptions. 1 (1) a. Queen Elizabeth isn’t wearing a dress b. ?? Queen Elizabeth isn’t wearing jeans Introduction Descriptions of images are typically collected from untrained workers via crowdsourcing platforms, such as Mechanical Turk1 . The workers are explicitly instructed to describe only what they can see in the image, in an attempt to control content selection (Young et al., 2014; Chen et al., 2015). However, workers are still free to project their world view when writing the descriptions and they make linguistic choices, such as using negation structures (van Miltenburg, 2016). In this paper we study the use of negations in image descriptions. A negation is a word that communicates that something is not the case. Negations are often used when there is a mismatch between what speakers expect to be the case and what is actually the case (see e.g. (Leech, 1983; Beukeboom et al., 2010)). For example, if Queen Elizabeth of England were to appear in public wearing jeans in"
W16-3207,C98-1013,0,\N,Missing
W16-5007,D08-1103,0,0.0364349,"hort, are psychologically salient and have a strong associative bond between them resulting from their frequent co-occurrence (Fellbaum, 1998a). ‘Indirect antonyms’, then, result from similarity relations defined for the members of these direct antonym pairs. For example, moist and humid are classified as semantically similar to wet, and are therefore indirect antonyms of the lexeme dry. See Figure 2 for a schematic representation of these similarity and antonymy relations in WordNet. However, these resources do not further characterize the relations between the members of an antonymous pair. Mohammad et al. (2008) point out that WordNet does not encode the degree of antonymy between words; in this paper we aim to show that it is not so much the degree that should be encoded (we think that the distinction between direct and indirect antonyms already covers this for the most part), but semantic categories that enable distinguishing between, for example, clear:unclear and appear:disappear. watery parched damp moist arid wet dry humid anhydrous sere soggy dried-up similarity antonymy Figure 2: Similarity and antonymy relations in WordNet, from (Gross and Miller, 1990). 1 See (Clark, 1976; Lehrer, 1985; Sch"
W16-5007,W10-3111,0,0.0329481,"tinguish between facts and counterfacts. Therefore, they focus exclusively on negations that turn an event into a negated event, disregarding any expression that does not meet this criterion. As a consequence, affixal negations are only annotated if the affix negates the event or property expressed by its base. For other tasks, however, it may be relevant to include other kinds of affixal negations. In the context of sentiment analysis it all depends on whether or not the affixal derivative or its base is opinionated; words like flawless or disqualify should be included in a polarity lexicon (Wiegand et al., 2010), whereas words like untie or backless would be irrelevant. In the context of question answering, however, knowing what the word backless entails is essential to know the answer to the question does the dress have a closed back? 2.2 Regular antonyms In the previous subsection we have argued that, depending on its goal, the task to be solved may require a certain subset of affixal negations. On the other hand, the full set of affixal negations may still not 50 be sufficient if the task requires taking all sorts of opposites into account. That is, regular antonyms might have to be considered in"
W17-1808,W08-0606,0,0.211426,"rocessing (Morante and Blanco, 2012) is an emergent task in Natural Language Processing (NLP). Initially, negation processing systems were developed to detect negation in clinical texts in order to extract accurate information about the patients. The systems were rule-based (Chapman et al., 2001) because of the lack of annotated corpora to train machine learning systems. Currently, annotated corpora are still scarce due to the legal and ethical requirements that have to be fulfilled. In addition, most of the corpora only contain documents written in English. An example is the BioScope corpus (Szarvas et al., 2008) which contains 1,954 radiology reports from the Computational Medicine Center in Cincinnati, annotated for negations and uncertainty along with the scopes of each phenomenon. For Spanish clinical reports, to the best of our knowledge, there are only a few attempts to anno53 Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR), pages 53–58, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics 2 Annotating Negation in Spanish Clinical Reports refer to these words as markers. In the examples that follow, negated events are marked withi"
W17-1808,taboada-etal-2006-methods,0,0.296915,"Missing"
W17-1808,W16-5006,0,0.346606,"Missing"
W17-1808,bartalesi-lenzi-etal-2012-cat,0,0.0292297,"e 100 anamnesis documents were annotated by the two annotators in order to train them, and test and improve the guidelines. These documents are not included in the final corpus. During all the annotation process, the annotators were not allowed to communicate with each other. The problematic cases that they encountered were discussed with the expert linguist and author of the guidelines. As work in progress, another author who is expert annotator is currently solving the disagreement cases, acting as adjudicator, in order to generate the gold-standard corpus. The annotation tool used was CAT (Lenzi et al., 2012). Two markables were defined (negation marker and negated event) plus a negation relation between the marker and the event. The annotation task consisted on annotating the events that are affected by contextual negation, as well as the words that express negation. We will The annotation guidelines follow closely the Thyme corpus guidelines (Styler IV et al., 2014) with some adaptations. We defined as clinical event any event that is relevant to elaborate the clinical chronology of a patient such as a diagnosis, tumors, habits, medical tests, or events related to the functional evaluation of th"
W17-1808,S12-1035,1,0.892994,"f anamnesis and radiology reports has been annotated by two domain expert annotators with negation markers and negated events. The Dice coefficient for inter-annotator agreement is higher than 0.94 for negation markers and higher than 0.72 for negated events. The corpus will be publicly released when the annotation process is finished, constituting the first corpus annotated with negation for Spanish clinical reports available for the NLP community. 1 Introduction In this paper we present the UHU-HUVR corpus of Spanish clinical reports annotated with negation information. Negation processing (Morante and Blanco, 2012) is an emergent task in Natural Language Processing (NLP). Initially, negation processing systems were developed to detect negation in clinical texts in order to extract accurate information about the patients. The systems were rule-based (Chapman et al., 2001) because of the lack of annotated corpora to train machine learning systems. Currently, annotated corpora are still scarce due to the legal and ethical requirements that have to be fulfilled. In addition, most of the corpora only contain documents written in English. An example is the BioScope corpus (Szarvas et al., 2008) which contains"
W17-1808,Q14-1012,0,\N,Missing
W18-5207,C16-1324,0,0.411351,"(SaintDizier, 2016). Stab and Gurevych created a corpus of 402 argumentative essays selected from essayforum.com and annotated it with the following argument components: major claims, claims, and premises (Stab and Gurevych, 2017). They model the microstructure of arguments as a connected tree structure where the major claim is the root node which represents the author’s standpoint. The major claim is expected to be contained either in the introduction or the conclusion of the essay. The rest of the essay contains claims (the cores of the arguments) and premises, which support the claims. Al-Khatib et al. (2016) consider that in the editorial genre, the author generally does not only aim at persuading the audience, but she also wants to spread information about the topic. The author defends a thesis that conveys a stance on a controversial subject providing different kinds of evidence. They constructed a corpus by extracting 100 editorials from each of the following websites: aljazeera.com, foxnews.com and theguardian.com. They introduce an annotation task which consisted 1. These are child who can’t be vaccinated. Children who have cancer. Children who are immunocompromised. Children who are truly a"
W18-5207,J17-1004,0,0.355339,"nmatches). IAA was calculated with lenient matching in order not to penalize disagreements due to details such as punctuation. • Statistics: the segment contains the results of a quantitative study or data analyses. • Anecdote: the segment expresses a personal experience, a specific instance, a concrete example. • Other: the segment is not classifiable with any of the above classes. These two argumentation schemes were adopted in the first pilot study presented in this work because the documents composing the Vax Corpus present characteristics of both argumentative essays and news editorials. Habernal and Gurevych (2017) created a corpus of user-generated Web content collecting documents of different registers, such as articles, comments on articles, blog posts, forum posts, etc. Their scheme is based on the Toulmin model (Toulmin, 2003) and it is characterized by the following components argument components: 3.1 • Claim: the conclusion that the author is trying to establish. Annotating Argument Components: Pilot Studies First Pilot Study We observed that some documents in the Vax Corpus present characteristics of argumentative essays. In particular, arguments are expressed in a hierarchical structure where t"
W18-5207,L16-1156,0,0.0672632,"Missing"
W18-5207,J17-3005,0,0.44757,"fers, data from the Web such as social media, on-line newspapers, forums, or blogs is often the subject of exploration (Lippi and Torroni, 2016). The availability of such data and the advancements in computational linguistics fostered the rise of a new research field called argumentation mining (AM) (Peldszus and Stede, 2013a), whose goal is to automatically extract argument components from text, generating structured data for computational models of argument. Thus far, most corpora annotated with argumentation information are composed by a certain type of texts, such as argumentative essays (Stab and Gurevych, 2017) and news editorials (Al Khatib 1 By “argumentation schemes” we mean annotation schemes that have been used to annotate argumentation components. 47 Proceedings of the 5th Workshop on Argument Mining, pages 47–56 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics contain domain-specific features, it should be applicable to other on-line debates. We will test this hypothesis in future work. Section 2 presents related work. In Section 3 we introduce the pilot annotation studies and we discuss the results and main sources of disagreement. In Section 4 we describ"
