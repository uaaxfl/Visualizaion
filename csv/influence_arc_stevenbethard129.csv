2020.acl-main.414,D17-1042,0,0.024517,"iven question and candidate answers are covered by the retrieved justifications. Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC. When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets. 1 Introduction Explainability in machine learning (ML) remains a critical unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved s"
2020.acl-main.414,D19-5310,0,0.25542,"of interpretability via evidence text selection. QA approaches that focus on interpretability can be broadly classified into three main categories: supervised, which require annotated justifications at training time, latent, which extract justification sentences through latent variable methods driven by answer quality, and, lastly, unsupervised ones, which use unsupervised algorithms for evidence extraction. In the first class of supervised approaches, a supervised classifier is normally trained to identify correct justification sentences driven by a query (Nie et al., 2019; Tu et al., 2019; Banerjee, 2019). Many systems tend to utilize a multi-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforceme"
2020.acl-main.414,D18-1454,0,0.0625691,"Missing"
2020.acl-main.414,P17-1171,0,0.198061,"tems have relied on structured knowledge base (KB) QA. For example, several previous works have used ConceptNet (Speer et al., 2017) to keep the QA process interpretable (Khashabi et al., 2018b; Sydorova et al., 2019). However, the construction of such structured knowledge bases is expensive, and may need frequent updates. Instead, in this work we focus on justification selection from textual (or unstructured) KBs, which are inexpensive to build and can be applied in several domains. In the same category of unsupervised approaches, conventional information retrieval (IR) methods such as BM25 (Chen et al., 2017) have also been widely used to retrieve independent individual sentences. As shown by (Khot et al., 2019a; Qi et al., 2019), and our table 2, these techniques do not work well for complex multi-hop questions, which require knowledge aggregation from multiple related justifications. Some unsupervised methods extract groups of justification sentences (Chen et al., 2019; Yadav et al., 2019b) but these methods are exponentially expensive in the retrieval step. Contrary to all of these, AIR proposes a simpler and more efficient method for chaining justification sentences. As shown in fig. 2, the pr"
2020.acl-main.414,N19-1405,0,0.0543589,"cal unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required"
2020.acl-main.414,P17-1020,0,0.0221913,"ti-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforcement learning (Geva and Berant, 2018; Choi et al., 2017) and PageRank (Surdeanu et al., 2008) have been widely used to select justification sentences without explicit training data. While these directions do not require annotated justifications, they tend to need large amounts of question/correct answer pairs to facilitate the identification of latent justifications. strategy that relies on explicit terms from the query and the previously retrieved justification. Lastly, none of the previous iterative retrieval approaches address the problem of semantic drift, whereas AIR accounts for drift by controlling the query reformulation as explained in sec"
2020.acl-main.414,N19-1423,0,0.0311486,"(QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the given question (Yadav et al., 2019b). Figure 1 shows one such multi-hop example from a MCQA dataset. In this paper we introduce a simple alignmentbased iterati"
2020.acl-main.414,N19-1246,0,0.0428887,"Missing"
2020.acl-main.414,P19-1222,0,0.0917786,"poses a simpler and more efficient method for chaining justification sentences. As shown in fig. 2, the proposed QA approach consists of two components: (a) an unsupervised, iterative component that retrieves chains of justification sentences given a query; and (b) an answer classification component that classifies a candidate answer as correct or not, given the original question and the previously retrieved justifications. We detail these components in the next two sub-sections. Recently, many supervised iterative justification retrieval approaches for QA have been proposed (Qi et al., 2019; Feldman and El-Yaniv, 2019; Banerjee, 2019; Das et al., 2018). While these were shown to achieve good evidence selection performance for complex questions when compared to earlier approaches that relied on just the original query (Chen et al., 2017; Yang et al., 2018), they all require supervision. As opposed to all these iterative-retrieval methods and previously discussed directions, our proposed approach AIR is completely unsupervised, i.e., it does not require annotated justifications. Further, unlike many of the supervised iterative approaches (Feldman and El-Yaniv, 2019; Sun et al., 2019a) that perform query refo"
2020.acl-main.414,C18-1014,0,0.0155935,"s tend to utilize a multi-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforcement learning (Geva and Berant, 2018; Choi et al., 2017) and PageRank (Surdeanu et al., 2008) have been widely used to select justification sentences without explicit training data. While these directions do not require annotated justifications, they tend to need large amounts of question/correct answer pairs to facilitate the identification of latent justifications. strategy that relies on explicit terms from the query and the previously retrieved justification. Lastly, none of the previous iterative retrieval approaches address the problem of semantic drift, whereas AIR accounts for drift by controlling the query reformulation"
2020.acl-main.414,N18-1023,0,0.135584,"uction Explainability in machine learning (ML) remains a critical unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from"
2020.acl-main.414,D19-1281,0,0.0649489,"hat slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candid"
2020.acl-main.414,P18-1161,0,0.0211601,"extraction. In the first class of supervised approaches, a supervised classifier is normally trained to identify correct justification sentences driven by a query (Nie et al., 2019; Tu et al., 2019; Banerjee, 2019). Many systems tend to utilize a multi-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforcement learning (Geva and Berant, 2018; Choi et al., 2017) and PageRank (Surdeanu et al., 2008) have been widely used to select justification sentences without explicit training data. While these directions do not require annotated justifications, they tend to need large amounts of question/correct answer pairs to facilitate the identification of latent justifications. strategy that relies on explicit t"
2020.acl-main.414,D19-1242,0,0.250002,"on of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the"
2020.acl-main.414,2021.ccl-1.108,0,0.271141,"Missing"
2020.acl-main.414,Q19-1014,0,0.29854,"on of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the"
2020.acl-main.414,P18-1160,0,0.0348701,", which require annotated justifications at training time, latent, which extract justification sentences through latent variable methods driven by answer quality, and, lastly, unsupervised ones, which use unsupervised algorithms for evidence extraction. In the first class of supervised approaches, a supervised classifier is normally trained to identify correct justification sentences driven by a query (Nie et al., 2019; Tu et al., 2019; Banerjee, 2019). Many systems tend to utilize a multi-task learning setting to learn both answer extraction and justification selection with the same network (Min et al., 2018; Gravina et al., 2018). Although these approaches have achieved impressive performance, they rely on annotated justification sentences, which may not be always available. Few approaches have used distant supervision methods (Lin et al., 2018; Wang et al., 2019b) to create noisy training data for evidence retrieval but these usually underperform due to noisy labels. In the latent approaches for selecting justifica4515 tions, reinforcement learning (Geva and Berant, 2018; Choi et al., 2017) and PageRank (Surdeanu et al., 2008) have been widely used to select justification sentences without expl"
2020.acl-main.414,N19-1270,0,0.0749582,"Missing"
2020.acl-main.414,D19-1258,0,0.165544,"Missing"
2020.acl-main.414,D14-1162,0,0.0835913,"the Association for Computational Linguistics, pages 4514–4525 c July 5 - 10, 2020. 2020 Association for Computational Linguistics AIR also conditionally expands the query using the justifications retrieved in the previous steps. In particular, our key contributions are: (1) We develop a simple, fast, and unsupervised iterative evidence retrieval method, which achieves state-of-the-art results on justification selection on two multi-hop QA datasets: MultiRC (Khashabi et al., 2018a) and QASC (Khot et al., 2019a). Notably, our simple unsupervised approach that relies solely on GloVe embeddings (Pennington et al., 2014) outperforms three transformer-based supervised state-of-the-art methods: BERT (Devlin et al., 2019), XLnet (Yang et al., 2019) and RoBERTa (Liu et al., 2019) on the justification selection task. Further, when the retrieved justifications are fed into a QA component based on RoBERTa (Liu et al., 2019), we obtain the best QA performance on the development sets of both MultiRC and QASC.2 (2) AIR can be trivially extended to capture parallel evidence chains by running multiple instances of AIR in parallel starting from different initial evidence sentences. We show that aggregating multiple parall"
2020.acl-main.414,N19-1403,0,0.0392251,"Missing"
2020.acl-main.414,D19-1261,0,0.159536,"evidence retrieval on both the datasets, which emphasizes the interpretability of AIR. (3) We demonstrate that AIR’s iterative process that focuses on missing information is more robust to semantic drift. We show that even the supervised RoBERTa-based retriever trained to retrieve evidences iteratively, suffers substantial drops in performance with retrieval from consecutive hops. 2 Related Work Our work falls under the revitalized direction that focuses on the interpretability of QA systems, where the machine’s inference process is explained to the end user in natural language evidence text (Qi et al., 2019; Yang et al., 2018; Wang et al., 2019b; Yadav et al., 2019b; Bauer et al., 2018). Several 2 In settings where external labeled resources are not used. Question: Exposure to oxygen and water can cause iron to (A) decrease strength (B) melt (C) uncontrollable burning (D) thermal expansion (E) turn orange on the surface (F) vibrate (G) extremes of temperature (H) levitate Gold justification sentences: 1. when a metal rusts , that metal becomes orange on the surface 2. Iron rusts in the presence of oxygen and water. Parallel evidence chain 1: 1. Dissolved oxygen in water usually causes the oxidat"
2020.acl-main.414,P19-1488,0,0.0303909,"swer pairs to facilitate the identification of latent justifications. strategy that relies on explicit terms from the query and the previously retrieved justification. Lastly, none of the previous iterative retrieval approaches address the problem of semantic drift, whereas AIR accounts for drift by controlling the query reformulation as explained in section 3.1. In unsupervised approaches, many QA systems have relied on structured knowledge base (KB) QA. For example, several previous works have used ConceptNet (Speer et al., 2017) to keep the QA process interpretable (Khashabi et al., 2018b; Sydorova et al., 2019). However, the construction of such structured knowledge bases is expensive, and may need frequent updates. Instead, in this work we focus on justification selection from textual (or unstructured) KBs, which are inexpensive to build and can be applied in several domains. In the same category of unsupervised approaches, conventional information retrieval (IR) methods such as BM25 (Chen et al., 2017) have also been widely used to retrieve independent individual sentences. As shown by (Khot et al., 2019a; Qi et al., 2019), and our table 2, these techniques do not work well for complex multi-hop q"
2020.acl-main.414,N19-1302,0,0.11548,"Missing"
2020.acl-main.414,K19-1065,0,0.384987,"ets, which emphasizes the interpretability of AIR. (3) We demonstrate that AIR’s iterative process that focuses on missing information is more robust to semantic drift. We show that even the supervised RoBERTa-based retriever trained to retrieve evidences iteratively, suffers substantial drops in performance with retrieval from consecutive hops. 2 Related Work Our work falls under the revitalized direction that focuses on the interpretability of QA systems, where the machine’s inference process is explained to the end user in natural language evidence text (Qi et al., 2019; Yang et al., 2018; Wang et al., 2019b; Yadav et al., 2019b; Bauer et al., 2018). Several 2 In settings where external labeled resources are not used. Question: Exposure to oxygen and water can cause iron to (A) decrease strength (B) melt (C) uncontrollable burning (D) thermal expansion (E) turn orange on the surface (F) vibrate (G) extremes of temperature (H) levitate Gold justification sentences: 1. when a metal rusts , that metal becomes orange on the surface 2. Iron rusts in the presence of oxygen and water. Parallel evidence chain 1: 1. Dissolved oxygen in water usually causes the oxidation of iron. 2. When iron combines wit"
2020.acl-main.414,Q18-1021,0,0.0396203,"machine learning (ML) remains a critical unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge b"
2020.acl-main.414,N19-1274,1,0.552747,"talov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the given question (Yadav et al., 2019b). Figure 1 shows one such multi-hop example from a MCQA dataset. In this paper we introduce a simple alignmentbased iterative retriever (AIR)1 , which retrieves high-quality evidence sentences from unstructured knowledge bases. We demonstrate that these evidence sentences are useful not only to explain the required reasoning steps that answer a question, but they also considerably improve the performance of the QA system itself. Unlike several previous works that depend on supervised methods for the retrieval of justification sentences (deployed mostly in settings that rely on small sets of"
2020.acl-main.414,D19-1260,1,0.7612,"talov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is especially critical for multi-hop, multiple choice QA (MCQA) where: (a) the answer text may not come from an actual knowledge base passage, and (b) reasoning is required to link the candidate answers to the given question (Yadav et al., 2019b). Figure 1 shows one such multi-hop example from a MCQA dataset. In this paper we introduce a simple alignmentbased iterative retriever (AIR)1 , which retrieves high-quality evidence sentences from unstructured knowledge bases. We demonstrate that these evidence sentences are useful not only to explain the required reasoning steps that answer a question, but they also considerably improve the performance of the QA system itself. Unlike several previous works that depend on supervised methods for the retrieval of justification sentences (deployed mostly in settings that rely on small sets of"
2020.acl-main.414,D18-1259,0,0.511364,"assification component, we achieve state-of-the-art QA performance on these two datasets. 1 Introduction Explainability in machine learning (ML) remains a critical unsolved challenge that slows the adoption of ML in real-world applications (Biran and Cotton, 2017; Gilpin et al., 2018; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017). Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results on these datasets (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019), we argue that many of these directions lack a human-understandable explanation of their inference process, which is necessary to transition these approaches into realworld applications. This is espec"
2020.acl-main.414,P08-1082,1,\N,Missing
2020.acl-main.414,D19-5309,0,\N,Missing
2020.acl-main.429,konstantinova-etal-2012-review,0,0.161468,"Missing"
2020.acl-main.429,D19-1445,0,0.0186893,"oposed way as both average negation sensitivity and cross-dataset consistency improved over the pretrained model and the control task. Evidence for the large versions of the models was weaker, suggesting that they may be representing negation knowledge in other ways. Other works have explored the effects of finetuning on attention without testing for specific linguistic knowledge. Serrano and Smith (2019), Jain and Wallace (2019) and Wiegreffe and Pinter (2019) found many redundancies in the attention of sequence-to-sequence models, suggesting that attention may encode knowledge in many ways. Kovaleva et al. (2019) found that removal of attention heads in transformers does not necessarily damage downstream performance. Our results suggest an explanation for this finding: knowledge sensitivity spreads broadly, so recovering from a small number of missing heads should be easy. Htut et al. (2019) investigated the role of gram4736 Fine-Tune Pretrain Control Negation Precision F1 Recall mean τ ± sd sig mean τ ± sd sig mean τ ± sd sig 0.440 0.438 ± 0.020 0.469 ± 0.025 10/10 10/10 0.418 0.406 ± 0.034 0.519 ± 0.020 10/10 10/10 0.415 0.409 ± 0.026 0.516 ± 0.020 10/10 10/10 Table 4: Kendall rank correlation (τ )"
2020.acl-main.429,2021.ccl-1.108,0,0.125107,"Missing"
2020.acl-main.429,morante-daelemans-2012-conandoyle,0,0.170596,"-base attention-based classifiers and baselines on the negation scope detection task in terms of precision (P), recall (R) and F1 . The best fixed offset and attention head according to their F1 score are reported. we would expect it to perform reasonably well regardless of changes in text genre or style of negation annotation. Several studies show that generalization ability to a different dataset is not always guaranteed despite a good test performance on the same dataset (Weber et al., 2018; McCoy et al., 2019). We thus consider two different corpora annotated for negation: ConanDoyle-neg (Morante and Daelemans, 2012) and SFU Review (Konstantinova et al., 2012)2 . These datasets differ in genre (Sherlock Holmes stories vs. movie, book, and consumer product reviews) and in annotation schema (e.g., they have different rules for what sentences are considered to contain negation, and how to deal with coordination structure). To see whether the same attention heads are performing well at negation scope detection across the two corpora, we measure kendall rank correlation: τ= X 2 sgn(xi − xj )sgn(yi − yj ) n(n − 1) i&lt;j where xi is the performance of attention head i on the Conan Dolye dataset and yi is the perfo"
2020.acl-main.429,N18-1202,0,0.0468418,"ention mechanism, i.e., how much to care about other words when computing the next version of the current word. Clark et al. (2019) inspected pretrained transformers and found several syntactic properties encoded in an intuitive way, where the maximum attention from a dependent is on its syntactic head. But only the pretrained models were considered, not what happened to these intuitive encodings after fine-tuning to a downstream task. Introduction As large-scale pre-trained language models such as BERT and ELMo have achieved high performance in a variety of natural language processing tasks (Peters et al., 2018a; Radford et al., 2018; Devlin et al., 2019), a growing body of research is devoted to understanding what linguistic properties these language models have acquired. Recent work uses probes, which are supervised models trained to predict linguistic properties including morphology (Belinkov et al., 2017), syntax (Hewitt and Manning, 2019) and semantics (Peters et al., 2018b), etc. (See Belinkov and Glass (2019) for a complete survey.) A good probing performance is considered We argue that if some interpretable encoding of linguistic knowledge is a good explanation of a model, rather than showin"
2020.acl-main.748,S15-2070,0,0.379939,"Missing"
2020.acl-main.748,P19-2055,0,0.27499,"Yepes, 2017; Festag and Spreckelsen, 2017; Lee et al., 2017; Tutubalina et al., 2018; Niu et al., 2019) and learning to rank (Leaman et al., 2013; Liu and Xu, 2017; Li et al., 2017; Nguyen et al., 2018; Murty et al., 2018). Most classification-based approaches using deep neural networks have shown strong performance. They differ in using different architectures, such as Gated Recurrent Units (GRU) with attention mechanisms (Tutubalina et al., 2018), multi-task learning with auxiliary tasks to generate attention weights (Niu et al., 2019), or pre-trained transformer networks (Li et al., 2019; Miftahutdinov and Tutubalina, 2019); different sources for training word embeddings, such as Google News (Limsopatham and Collier, 2016) or concept definitions from the Unified Medical Language System (UMLS) Metathesaurus (Festag and Spreckelsen, 2017); and different input representations, such as using character embeddings (Niu et al., 2019). All classification approaches share the disadvantage that the output space must be the same size as the number of concepts to be predicted, and thus the output space tends to be small such as 2,200 concepts in (Limsopatham and Collier, 2016) and around 22,500 concepts in (Weissenbacher et"
2020.acl-main.748,P18-1010,0,0.489731,"ogy and the concept types that must be predicted do not all appear in the training data. We propose an architecture (shown in Figure 1) that is able to consider both morphological and semantic information. We first apply a candidate generator to generate a list of candidate concepts, and then use a BERT-based list-wise classifier to rank the candidate concepts. This two-step architecture allows unlikely concept candidates to be filtered out prior to the final classification, a necessary step when dealing with ontologies with millions of concepts. In contrast to previous list-wise classifiers (Murty et al., 2018) which only take the concept mention as input, our BERT-based list-wise classifier takes both the concept mention and the candidate concept name as input, and is thus able to handle concepts that never appear in the training data. We further enhance this list-wise approach with a semantic type regularizer that allows our ranker to leverage semantic type information from 8452 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8452–8464 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Candidate Generator (multi-class BERT classifier"
2020.acl-main.748,S14-2007,0,0.0254062,"Missing"
2020.acl-main.748,W19-3203,0,0.103301,"r concept normalization and achieved the best performance in the ShARe/CLEF eHealth 2013 shared task. List-wise learning-to-rank approaches are both computationally more efficient than pairwise learning-to-rank (Cao et al., 2007) and empirically outperform both point-wise and pair-wise approaches (Xia et al., 2008). There are two implementations of list-wise classifiers using neural networks for concept normalization: Murty et al. (2018) treat the selection of the best candidate concept as a flat classification problem, losing the ability to handle concepts not seen during training; Ji et al. (2019) take a generate-and-rank approach similar to ours, but they do not leverage resources such as synonyms or semantic type information from UMLS in their BERT-based ranker. 3 3.1 The main idea of the two-step approach is that we first use a simple and fast system with high recall to generate candidates, and then a more precise system with more discriminative input to rank the candidates. 3.2 Candidate generator We implement two kinds of candidate generators: a BERT-based multi-class classifier when the number of concepts in the ontology is small, and a Lucenebased1 dictionary look-up when there"
2020.acl-main.748,W09-1309,0,0.0627382,"Missing"
2020.alw-1.4,N19-1408,0,0.0241357,"sets, making it easy for BERT to learn the core linguistic phenomenon despite differences in domains. 5.2 training procedure of Liu et al. (2019a). Since our multi-label datasets do not share an annotation scheme, we train the multi-label classifiers on only one dataset at a time (i.e., they are not also multi-domain). Table 3 shows the results of these experiments2 . We find that in most cases training individual binary classifiers (Single) is better than a jointly-learned multi-label classifier (Joint). This is somewhat surprising as the latter is the standard approach with neural networks (Adhikari et al., 2019). Curious if the problem was some low-frequency classes, we tried training a multi-label model on just the three most frequent classes of the Wikipedia comments dataset (Joint top-3 classes), toxic, obscene, and insult. That slightly improved performance on those three classes, but of course at the cost of the classes now being ignored. Adding the staged training procedure (Joint→Single) on top of this classifier only decreased performance. This suggests that class imbalance may be part of the problem, but is not the full explanation. Note that we are the first to report all individual label F"
2020.alw-1.4,W18-5105,0,0.0623066,"Missing"
2020.alw-1.4,S19-2007,0,0.139659,"of Arizona Kevin Coe University of Utah Steven Bethard University of Arizona yotam@email.arizona.edu kevin.coe@utah.edu bethard@email.arizona.edu Abstract language appears in many places online, from microblogs like Twitter, to comments on online newspapers, to edit histories of resources like Wikipedia. Uncivil language detection is thus a multi-label and multi-domain language processing problem. While there has been much research in natural language processing methods for identifying such incivility, especially in the subarea of abusive language (Wiegand et al., 2019; Zampieri et al., 2019; Basile et al., 2019; Sadeque et al., 2019; van Aken et al., 2018, etc.), the multi-label and multi-domain nature of incivility detection is understudied. We thus consider incivility detection on several datasets that (1) require the classification of incivility into several not-mutually-exclusive fine-grained categories, and (2) cover multiple genres of online interactions. Our contributions are: Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models t"
2020.alw-1.4,N19-1423,0,0.0424943,"s, as they have been annotated for multiple forms of incivility. They do not share annotation schemes, so our multi-label experiments consider each multi-label dataset separately. 4 Patti (2019) also explores a recurrent neural network). They do not consider multi-label problems, or modern pre-trained neural networks like BERT, which were more successful in recent shared tasks on abusive language (Zampieri et al., 2019). They also evaluate on several datasets that have been identified as problematic by Wiegand et al. (2019) due to their use of topic-biased sampling. 5 Experiments We use BERT (Devlin et al., 2019) as the starting point for all experiments. BERT is a pre-trained transformer-based neural network that has shown impressive performance on a wide variety of NLP tasks. We follow the standard approach for finetuning BERT for text classification, placing a fully connected layer over BERT’s [CLS] output. We use n sigmoids on this layer rather than a softmax activation, since we are performing multi-label classification. BERT is then fine-tuned as usual, with hyperparameters like learning rate, maximum sequence length, number of epochs, training batch size tuned on the development set. We explore"
2020.alw-1.4,N19-1060,0,0.0855598,"@email.arizona.edu Yotam Shmargad University of Arizona Kevin Coe University of Utah Steven Bethard University of Arizona yotam@email.arizona.edu kevin.coe@utah.edu bethard@email.arizona.edu Abstract language appears in many places online, from microblogs like Twitter, to comments on online newspapers, to edit histories of resources like Wikipedia. Uncivil language detection is thus a multi-label and multi-domain language processing problem. While there has been much research in natural language processing methods for identifying such incivility, especially in the subarea of abusive language (Wiegand et al., 2019; Zampieri et al., 2019; Basile et al., 2019; Sadeque et al., 2019; van Aken et al., 2018, etc.), the multi-label and multi-domain nature of incivility detection is understudied. We thus consider incivility detection on several datasets that (1) require the classification of incivility into several not-mutually-exclusive fine-grained categories, and (2) cover multiple genres of online interactions. Our contributions are: Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edit"
2020.alw-1.4,W18-5117,0,0.0229628,"Missing"
2020.alw-1.4,N18-1095,0,0.0283182,"multi-label, multi-domain corpora we consider. Sadeque et al. (2019) considered the local news comments corpus, training recurrent neural network models, and focusing on only the top two most frequent labels for this dataset. They achieved 0.48 F1 for name-calling and 0.53 F1 for vulgarity. van Aken et al. (2018) presented multiple approaches to the Wikipedia comments dataset. They developed an ensemble of logistic regression, recurrent neural networks, and convolutional neural networks, achieving an AUC score of 0.983. There are a few recent works in cross-domain abusive language detection. Wiegand et al. (2018); ˇ Karan and Snajder (2018); Pamungkas and Patti (2019) all explore training models on one abusive language dataset and testing on another. They focus on binary predictions and bag-of-words support vector machine classifiers (though Pamungkas and learning rate: 8e-6, 2e-5, 4e-5, 8e-5 maximum sequence length: 128, 256, 512 number of epochs: 2, 3, 4, 5, 6, 8 training batch size: 16, 32, 64, 128 5.1 Multi-domain models We consider three methods for training classifiers for prediction in multiple domains: Single One classifier is fine-tuned for each domain. Joint One classifier is fine-tuned on t"
2020.alw-1.4,C16-1038,0,0.0237819,"re in the remaining dataset. When we evaluate this best model on the test data, we achieve a new state-of-the-art on the local news comments corpus, 0.56 F1 . We are the first to report results on the local politics Tweets and Russian troll Tweets domains, as Sadeque et al. (2019) did not evaluate on these. These results did not replicate the findings of Liu et al. (2019a) when applied to our incivility datasets; the extra fine-tuning for each domain was unhelpful, and simply combining all the data was the best. This probably argues for exploring other approaches for domain adapatation, e.g., Kim et al. (2016), but it may also simply suggest that Coe et al. (2014)’s annotators were consistent across datasets, making it easy for BERT to learn the core linguistic phenomenon despite differences in domains. 5.2 training procedure of Liu et al. (2019a). Since our multi-label datasets do not share an annotation scheme, we train the multi-label classifiers on only one dataset at a time (i.e., they are not also multi-domain). Table 3 shows the results of these experiments2 . We find that in most cases training individual binary classifiers (Single) is better than a jointly-learned multi-label classifier (J"
2020.alw-1.4,S19-2010,0,0.0328345,"Missing"
2020.alw-1.4,W19-8904,0,0.0564414,"Missing"
2020.alw-1.4,2021.ccl-1.108,0,0.0711535,"Missing"
2020.alw-1.4,P19-2051,0,0.0167547,"eque et al. (2019) considered the local news comments corpus, training recurrent neural network models, and focusing on only the top two most frequent labels for this dataset. They achieved 0.48 F1 for name-calling and 0.53 F1 for vulgarity. van Aken et al. (2018) presented multiple approaches to the Wikipedia comments dataset. They developed an ensemble of logistic regression, recurrent neural networks, and convolutional neural networks, achieving an AUC score of 0.983. There are a few recent works in cross-domain abusive language detection. Wiegand et al. (2018); ˇ Karan and Snajder (2018); Pamungkas and Patti (2019) all explore training models on one abusive language dataset and testing on another. They focus on binary predictions and bag-of-words support vector machine classifiers (though Pamungkas and learning rate: 8e-6, 2e-5, 4e-5, 8e-5 maximum sequence length: 128, 256, 512 number of epochs: 2, 3, 4, 5, 6, 8 training batch size: 16, 32, 64, 128 5.1 Multi-domain models We consider three methods for training classifiers for prediction in multiple domains: Single One classifier is fine-tuned for each domain. Joint One classifier is fine-tuned on the combined training data from all the domains. Joint→Si"
2020.alw-1.4,S19-1031,1,0.847324,"University of Utah Steven Bethard University of Arizona yotam@email.arizona.edu kevin.coe@utah.edu bethard@email.arizona.edu Abstract language appears in many places online, from microblogs like Twitter, to comments on online newspapers, to edit histories of resources like Wikipedia. Uncivil language detection is thus a multi-label and multi-domain language processing problem. While there has been much research in natural language processing methods for identifying such incivility, especially in the subarea of abusive language (Wiegand et al., 2019; Zampieri et al., 2019; Basile et al., 2019; Sadeque et al., 2019; van Aken et al., 2018, etc.), the multi-label and multi-domain nature of incivility detection is understudied. We thus consider incivility detection on several datasets that (1) require the classification of incivility into several not-mutually-exclusive fine-grained categories, and (2) cover multiple genres of online interactions. Our contributions are: Incivility is a problem on social media, and it comes in many forms (name-calling, vulgarity, threats, etc.) and domains (microblog posts, online news comments, Wikipedia edits, etc.). Training machine learning models to detect such incivili"
2020.bea-1.11,W13-1704,0,0.0318459,"systems have been developed for a wide range of topics, including mechanical systems (Di Eugenio et al., 2002), qualitative physics (Litman and Silliman, 2004), learning a new language (Wang and Seneff, 2007), and introductory computer science (Fossati, 2008). As we focus on assisting students in writing thesis methodology sections, the most relevant prior work focuses on analysis of essays. ETS Criterion (Attali, 2004) uses features like n-gram frequency and syntactic analysis to provide feedback for grammatical errors, discourse structure, and undesirable stylistic features. The SAT system (Andersen et al., 2013) combines lexical and grammatical properties with a perceptron learner to provide detailed sentence-by-sentence feedback about possible lexical and grammatical errors. Revision Assistant (Woods et al., 2017) uses logistic regression over lexical and grammatical features to provide feedback on how individual sentences influence rubricspecific formative scores. All of these systems aim at general types of feedback, not the specific feedback needed for methodology sections. Other related work touches on issues of logical organization. Barzilay and Lapata (2008) propose training sentence ordering"
2020.bea-1.11,J08-1001,0,0.0196609,"ble stylistic features. The SAT system (Andersen et al., 2013) combines lexical and grammatical properties with a perceptron learner to provide detailed sentence-by-sentence feedback about possible lexical and grammatical errors. Revision Assistant (Woods et al., 2017) uses logistic regression over lexical and grammatical features to provide feedback on how individual sentences influence rubricspecific formative scores. All of these systems aim at general types of feedback, not the specific feedback needed for methodology sections. Other related work touches on issues of logical organization. Barzilay and Lapata (2008) propose training sentence ordering models to differentiate between the original order of a well-written text and a permuted sentence order. Cui et al. (2018) continue in this paradigm, training an encoderdecoder network to read a series of sentences and 3 Data A collection was created using the ColTyPi1 site. This site includes Spanish-language theses within the Information Technologies subject area. The graduate level is composed of Doctoral and Master theses. The Undergraduate level is composed of Bachelor and Advanced College-level Technician (TSU) theses. All theses and research proposals"
2020.bea-1.11,W99-0411,0,0.034815,"lexical heuristics with sequence alignment models to score the organization of an essay. However, they provide only an overall score, and do not integrate this into any intelligent tutoring system. A final major difference between our work and prior work is that all the work above focused on the English language, while we provide feedback for Spanish-language theses. Background There is a long history of natural language processing research on interactive systems that assist student writing. Essay scoring has been a popular topic, with techniques ranging from syntactic and discourse analysis (Burstein and Chodorow, 1999), to list-wise learning-to-rank (Chen and He, 2013), to recurrent neural networks (Taghipour and Ng, 2016). Yet the goal of such work is very different from ours, as we aim not to assign an overall score, but rather to provide detailed feedback on aspects of a good methodology that are present or absent from the draft. Intelligent tutoring systems have been developed for a wide range of topics, including mechanical systems (Di Eugenio et al., 2002), qualitative physics (Litman and Silliman, 2004), learning a new language (Wang and Seneff, 2007), and introductory computer science (Fossati, 2008"
2020.bea-1.11,D13-1180,0,0.0143871,"e organization of an essay. However, they provide only an overall score, and do not integrate this into any intelligent tutoring system. A final major difference between our work and prior work is that all the work above focused on the English language, while we provide feedback for Spanish-language theses. Background There is a long history of natural language processing research on interactive systems that assist student writing. Essay scoring has been a popular topic, with techniques ranging from syntactic and discourse analysis (Burstein and Chodorow, 1999), to list-wise learning-to-rank (Chen and He, 2013), to recurrent neural networks (Taghipour and Ng, 2016). Yet the goal of such work is very different from ours, as we aim not to assign an overall score, but rather to provide detailed feedback on aspects of a good methodology that are present or absent from the draft. Intelligent tutoring systems have been developed for a wide range of topics, including mechanical systems (Di Eugenio et al., 2002), qualitative physics (Litman and Silliman, 2004), learning a new language (Wang and Seneff, 2007), and introductory computer science (Fossati, 2008). As we focus on assisting students in writing the"
2020.bea-1.11,D18-1465,0,0.0134713,"ntence feedback about possible lexical and grammatical errors. Revision Assistant (Woods et al., 2017) uses logistic regression over lexical and grammatical features to provide feedback on how individual sentences influence rubricspecific formative scores. All of these systems aim at general types of feedback, not the specific feedback needed for methodology sections. Other related work touches on issues of logical organization. Barzilay and Lapata (2008) propose training sentence ordering models to differentiate between the original order of a well-written text and a permuted sentence order. Cui et al. (2018) continue in this paradigm, training an encoderdecoder network to read a series of sentences and 3 Data A collection was created using the ColTyPi1 site. This site includes Spanish-language theses within the Information Technologies subject area. The graduate level is composed of Doctoral and Master theses. The Undergraduate level is composed of Bachelor and Advanced College-level Technician (TSU) theses. All theses and research proposals in the collection have been reviewed at some point by a review committee. 3.1 Guidelines A four-page guide was provided to the annotators with the instructio"
2020.bea-1.11,D16-1193,0,0.0125565,"only an overall score, and do not integrate this into any intelligent tutoring system. A final major difference between our work and prior work is that all the work above focused on the English language, while we provide feedback for Spanish-language theses. Background There is a long history of natural language processing research on interactive systems that assist student writing. Essay scoring has been a popular topic, with techniques ranging from syntactic and discourse analysis (Burstein and Chodorow, 1999), to list-wise learning-to-rank (Chen and He, 2013), to recurrent neural networks (Taghipour and Ng, 2016). Yet the goal of such work is very different from ours, as we aim not to assign an overall score, but rather to provide detailed feedback on aspects of a good methodology that are present or absent from the draft. Intelligent tutoring systems have been developed for a wide range of topics, including mechanical systems (Di Eugenio et al., 2002), qualitative physics (Litman and Silliman, 2004), learning a new language (Wang and Seneff, 2007), and introductory computer science (Fossati, 2008). As we focus on assisting students in writing thesis methodology sections, the most relevant prior work"
2020.bea-1.11,N07-1059,0,0.0083759,"ging from syntactic and discourse analysis (Burstein and Chodorow, 1999), to list-wise learning-to-rank (Chen and He, 2013), to recurrent neural networks (Taghipour and Ng, 2016). Yet the goal of such work is very different from ours, as we aim not to assign an overall score, but rather to provide detailed feedback on aspects of a good methodology that are present or absent from the draft. Intelligent tutoring systems have been developed for a wide range of topics, including mechanical systems (Di Eugenio et al., 2002), qualitative physics (Litman and Silliman, 2004), learning a new language (Wang and Seneff, 2007), and introductory computer science (Fossati, 2008). As we focus on assisting students in writing thesis methodology sections, the most relevant prior work focuses on analysis of essays. ETS Criterion (Attali, 2004) uses features like n-gram frequency and syntactic analysis to provide feedback for grammatical errors, discourse structure, and undesirable stylistic features. The SAT system (Andersen et al., 2013) combines lexical and grammatical properties with a perceptron learner to provide detailed sentence-by-sentence feedback about possible lexical and grammatical errors. Revision Assistant"
2020.bea-1.11,N19-1423,0,0.0165039,"Missing"
2020.bea-1.11,W02-2116,0,0.20453,"Missing"
2020.bea-1.11,P08-3006,0,0.00928083,"odorow, 1999), to list-wise learning-to-rank (Chen and He, 2013), to recurrent neural networks (Taghipour and Ng, 2016). Yet the goal of such work is very different from ours, as we aim not to assign an overall score, but rather to provide detailed feedback on aspects of a good methodology that are present or absent from the draft. Intelligent tutoring systems have been developed for a wide range of topics, including mechanical systems (Di Eugenio et al., 2002), qualitative physics (Litman and Silliman, 2004), learning a new language (Wang and Seneff, 2007), and introductory computer science (Fossati, 2008). As we focus on assisting students in writing thesis methodology sections, the most relevant prior work focuses on analysis of essays. ETS Criterion (Attali, 2004) uses features like n-gram frequency and syntactic analysis to provide feedback for grammatical errors, discourse structure, and undesirable stylistic features. The SAT system (Andersen et al., 2013) combines lexical and grammatical properties with a perceptron learner to provide detailed sentence-by-sentence feedback about possible lexical and grammatical errors. Revision Assistant (Woods et al., 2017) uses logistic regression over"
2020.bea-1.11,N04-3002,0,0.114291,"scoring has been a popular topic, with techniques ranging from syntactic and discourse analysis (Burstein and Chodorow, 1999), to list-wise learning-to-rank (Chen and He, 2013), to recurrent neural networks (Taghipour and Ng, 2016). Yet the goal of such work is very different from ours, as we aim not to assign an overall score, but rather to provide detailed feedback on aspects of a good methodology that are present or absent from the draft. Intelligent tutoring systems have been developed for a wide range of topics, including mechanical systems (Di Eugenio et al., 2002), qualitative physics (Litman and Silliman, 2004), learning a new language (Wang and Seneff, 2007), and introductory computer science (Fossati, 2008). As we focus on assisting students in writing thesis methodology sections, the most relevant prior work focuses on analysis of essays. ETS Criterion (Attali, 2004) uses features like n-gram frequency and syntactic analysis to provide feedback for grammatical errors, discourse structure, and undesirable stylistic features. The SAT system (Andersen et al., 2013) combines lexical and grammatical properties with a perceptron learner to provide detailed sentence-by-sentence feedback about possible l"
2020.bea-1.11,D10-1023,0,0.0646653,"Missing"
2020.bionlp-1.7,E17-2118,1,0.84358,"cal Temporal Relation Extraction Chen Lin1 , Timothy Miller1* , Dmitriy Dligach2 , Farig Sadeque1 , Steven Bethard3 and Guergana Savova1 * Co-first author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that"
2020.bionlp-1.7,W18-5607,0,0.0120221,"author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by recent work in Green AI"
2020.bionlp-1.7,E17-1108,0,0.355691,"Missing"
2020.bionlp-1.7,S15-2136,1,0.899432,"ed to the document creation time (DCT) as Document Time Relations (DocTimeRel), with possible values of BEFORE, AFTER, OVERLAP, and BEFORE OVERLAP (Styler IV et al., 2014). At a finer level, a narrative container (Pustejovsky and Stubbs, 2011) can temporally subsume an event as a contains relation. The THYME corpus (Styler IV et al., 2014) consists of EMR clinical text and is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). It was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). While the performance of DocTimeRel models has reached above 0.8 F1 on the THYME corpus, 70 Proceedings of the BioNLP 2020 workshop, pages 70–75 c Online, July 9, 2020 2020 Association for Computational Linguistics the conventional token BERT inserts at the start of every input sequence and its embedding is viewed as the representation of the entire sequence. The concatenated embedding is passed to a linear classifier to predict the CONTAINS, CONTAINED-BY, or NONE relation, rˆij , as in eq. (1). P (rˆij |x, Ei , Ej )=sof tmax(W L [G : ei : ej ] + b) (1) where W L ∈ R3dz ×lr , dz"
2020.bionlp-1.7,W18-5619,1,0.808752,"Savova1 * Co-first author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by r"
2020.bionlp-1.7,W17-2341,1,0.77707,"Extraction Chen Lin1 , Timothy Miller1* , Dmitriy Dligach2 , Farig Sadeque1 , Steven Bethard3 and Guergana Savova1 * Co-first author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful"
2020.bionlp-1.7,S17-2093,1,0.888897,"Missing"
2020.bionlp-1.7,W19-1908,1,0.405446,"bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by recent work in Green AI (Schwartz et al., 2019; Strubell et al., 2019), and one-pass encodings for multiple relations extraction (Wang et al., 2019), we propose a one-pass encoding mechanism"
2020.bionlp-1.7,2021.ccl-1.108,0,0.151401,"Missing"
2020.bionlp-1.7,W11-0419,0,0.4223,"he THYME corpus and is much “greener” in computational cost. 1 Introduction The analysis of many medical phenomena (e.g., disease progression, longitudinal effects of medications, treatment regimen and outcomes) heavily depends on temporal relation extraction from the clinical free text embedded in the Electronic Medical Records (EMRs). At a coarse level, a clinical event can be linked to the document creation time (DCT) as Document Time Relations (DocTimeRel), with possible values of BEFORE, AFTER, OVERLAP, and BEFORE OVERLAP (Styler IV et al., 2014). At a finer level, a narrative container (Pustejovsky and Stubbs, 2011) can temporally subsume an event as a contains relation. The THYME corpus (Styler IV et al., 2014) consists of EMR clinical text and is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). It was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). While the performance of DocTimeRel models has reached above 0.8 F1 on the THYME corpus, 70 Proceedings of the BioNLP 2020 workshop, pages 70–75 c Online, July 9, 2020 2020 Association for Computational Lingu"
2020.bionlp-1.7,P19-1355,0,0.0207776,"the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by recent work in Green AI (Schwartz et al., 2019; Strubell et al., 2019), and one-pass encodings for multiple relations extraction (Wang et al., 2019), we propose a one-pass encoding mechanism for the CONTAINS relation extraction task, which can significantly increase the efficiency and scalability. The architecture is shown in Figure 1. The three novel modifications to the original one-pass relational model of Wang et al. (2019) are: (1) Unlike Wang et al. (2019), our model operates in the relation extraction setting, meaning it must distinguish between relations and nonrelations, as well as classifying by relation type. (2) We introduce a pooled embedding for re"
2020.bionlp-1.7,P17-2035,0,0.0158578,"arig Sadeque1 , Steven Bethard3 and Guergana Savova1 * Co-first author Boston Children’s Hospital and Harvard Medical School 2 Loyola University Chicago 3 University of Arizona 1 {first.last}@childrens.harvard.edu 2 ddligach@luc.edu 3 bethard@email.arizona.edu the CONTAINS task remains a challenge for both Abstract 1 conventional learning approaches (Sun et al., 2013; Bethard et al., 2015, 2016, 2017) and neural models (structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long ShortTerm memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018; Galvan et al., 2018)). The difficulty is that the limited labeled data is insufficient for training deep neural models for complex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for eac"
2020.bionlp-1.7,P19-1132,0,0.349241,"lex linguistic phenomena. Some recent work (Lin et al., 2019) has used massive pre-trained language models (BERT; Devlin et al., 2018) and their variations (Lee et al., 2019) for this task and significantly increased the CONTAINS score by taking advantage of the rich BERT representations. However, that approach has an input representation that is highly wasteful – the same sentence must be processed multiple times, once for each candidate relation pair. Inspired by recent work in Green AI (Schwartz et al., 2019; Strubell et al., 2019), and one-pass encodings for multiple relations extraction (Wang et al., 2019), we propose a one-pass encoding mechanism for the CONTAINS relation extraction task, which can significantly increase the efficiency and scalability. The architecture is shown in Figure 1. The three novel modifications to the original one-pass relational model of Wang et al. (2019) are: (1) Unlike Wang et al. (2019), our model operates in the relation extraction setting, meaning it must distinguish between relations and nonrelations, as well as classifying by relation type. (2) We introduce a pooled embedding for relational classification across long distances. Wang et al. (2019) focused on s"
2020.coling-main.81,D13-1078,1,0.793701,"ollection (ensuring that the test set was not used during the development of the grammar). Synchronous grammars allow the construction of two simultaneous trees, one in a source language and one in a target language. In our case, the source is the natural language text in English and the target is a formal grammar of geometry operators. Each of these operators defines a function to produce a geometry from others. For example, the B ETWEEN operator takes two geolocations and calculates the region between them. We run our synchronous grammar with the extended CYK+ parsing algorithm described in Bethard (2013). The grammar contains 219 rules in total, 70 of which are lexical, e.g. [C ARDINAL] → northwest k N W . To illustrate our parsing process, we use as an example the following description: “. . . located between the towns of Adrano and S.Maria di Licodia, 32 kilometres (20 mi) northwest of Catania”. The text is first pre-processed, cleaning some unnecessary tokens and normalizing each geolocation to a SHP Index format: “. . . between the towns of SHP 001 and SHP 002, 32 kilometres northwest of SHP 003”. The Index is mapped to the geometry of its corresponding geolocation (e.g. 001 → relation/39"
2020.coling-main.81,W16-1721,0,0.023214,"and Wikipedia, are frequently used as reference knowledge bases. The problem is commonly separated into two steps. First, location references are extracted from the text following a Named Entity Recognition strategy (Karagoz et al., 2016; Magge et al., 2018). Secondly, the references are disambiguated and linked to the reference knowledge base (Turton, 2008; Weissenbacher et al., 2015; Gritta et al., 2018a). As pointed out by Gritta et al. (2018b), the amount of annotated data for toponym resolution is not large, but a few datasets are available for different domains such as historical texts (DeLozier et al., 2016), social media (Wallgr¨un et al., 2014), scientific literature (Weissenbacher et al., 2019) and Wikipedia (Gritta et al., 2018b). Thanks to these works, it is possible to link locations mentioned in text to unambiguous geographic respresentations, in the form of coordinates. However, they do not address the compositional nature of the geographical descriptions. On the other hand, although not necesarily related to toponyms, previous works have studied the structure of (geo)spatial natural language expressions. However, many of these works limited their 3 Appendix A.1 includes examples of WGS84"
2020.coling-main.81,P18-1119,0,0.0928564,"e toponyms. We present an approach that automates most of the process by combining Wikipedia and OpenStreetMap. As a result, we have gathered a collection of 360,187 uncurated complex geolocation descriptions, from which we have manually curated 1,000 examples intended to be used as a test set. To accompany the data, we define a new geoparsing evaluation framework along with a scoring methodology and a set of baselines. 1 Introduction Geoparsing, or toponym resolution, is the task of identifying geographical entities, and attaching them to their corresponding reference in a coordinate system (Gritta et al., 2018b). In its traditional setting it Figure 1: An illustrative example of complex geographical description parsing. Given a text describing a target geolocation the goal is to aproximate its geometry using as reference the geometries of the geolocations that appear in the description (Adrano, S.Maria di Licodia and Catania). In this example, the target geolocation is Biancavilla, but in general, there may not be a name for the target geolocation. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 936"
2020.coling-main.81,P80-1024,0,0.712501,"Missing"
2020.coling-main.81,S13-2044,1,0.751733,"ph. For simplicity, we only show the links of the second sentence but whole paragraph is included in the dataset. attention to the role of specific elements, such as prepositions (Zwarts, 2005; Kracht, 2008; Radke et al., 2019), or particular grammatical structures (Stock and Yousaf, 2018). Some other works defined the task as a tagging problem, each specifying a different set of labels to tag the components of the text. These works were focused on the relations between the elements of the geospatial expressions in a syntactically inspired manner (Kordjamshidi et al., 2011; Mani et al., 2010; Kolomiyets et al., 2013; Pustejovsky et al., 2015). As pointed out by Aflaki et al. (2018), the manual annotation of compositional (geo)spatial expressions is very challenging and, in consequence, many of these schemas are limited to a reduced set of elements or define complex tag structures. Moreover, the goal of these works is to obtain a tag structure on top of the text but they do not provide an explicit way to intepret them to obtain a single geographical object, i.e. it is not possible to translate these structures into a set of coordinates. Our work takes a different approach by not imposing any annotation st"
2020.coling-main.81,S19-2155,0,0.039657,"Missing"
2020.louhi-1.12,araki-etal-2014-detecting,0,0.0186208,"cal, non-identical, or mereologically (part-whole) related. In keeping with our primary goal of enabling timeline extraction, we implemented CON-SUB as a TLINK since it conveys true temporal containment. CON-SUB, however, differs from other TLINKs, which were constrained by proximity and lexical cues, as discussed in section 2. The fact that CON-SUB also represents structural information allowed us to treat it like a coreference/bridging relation in terms of permissible textual evidence for link creation: namely, semantic scripts. These may be defined as: “a stereotypical sequence of events” (Araki et al., 2014) or “prototypical schematic sequences of events” (Chambers and Jurafsky, 2008). We can expect a surgery, for example, to consist of certain, typical subevents (incisions, subprocedures, anesthesia administration, etc.), which therefore enables annotators to look throughout the whole document for lexical items with meanings that fit those subevents. The concept of semantic scripts is what facilitates attainable long-distance coreference /bridging linking, and therefore, long-distance CON-SUB linking. This is obviously not the case for nonsubevent CONTAINS relations. 4.2 Inter-annotator Agreemen"
2020.louhi-1.12,S17-2093,1,0.860183,"3 School of Information, University of Arizona, Tucson, AZ 4 Department of Computer Science, Loyola University Chicago, Chicago, IL 1 {first.last}@childrens.harvard.edu 2 {first.last}@colorado.edu 3 bethard@arizona.edu 4 ddligach@luc.edu 017 018 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 leading US medical center. This dataset has previously undergone a variety of annotation efforts, most notably temporal annotation (Styler IV et al., 2014). It has been part of several SemEval shared tasks such as Clinical TempEval (Bethard et al., 2017) where state-of-the-art results have been established. Our goal was to utilize this THYME corpus to enable the extraction of more extensive patient timelines by manually creating cross-document links that built off the pre-existing single file annotations. (Wright-Bettner et al., 2019) discuss that a subset of the THYME temporal annotations contributed to incompatible temporal inferences, thus reducing their ability to support meaningful temporal reasoning. Accuracy and informativeness of temporal relation gold annotations are essential for their effectiveness in training a system for temporal"
2020.louhi-1.12,P08-1090,0,0.0789589,"g with our primary goal of enabling timeline extraction, we implemented CON-SUB as a TLINK since it conveys true temporal containment. CON-SUB, however, differs from other TLINKs, which were constrained by proximity and lexical cues, as discussed in section 2. The fact that CON-SUB also represents structural information allowed us to treat it like a coreference/bridging relation in terms of permissible textual evidence for link creation: namely, semantic scripts. These may be defined as: “a stereotypical sequence of events” (Araki et al., 2014) or “prototypical schematic sequences of events” (Chambers and Jurafsky, 2008). We can expect a surgery, for example, to consist of certain, typical subevents (incisions, subprocedures, anesthesia administration, etc.), which therefore enables annotators to look throughout the whole document for lexical items with meanings that fit those subevents. The concept of semantic scripts is what facilitates attainable long-distance coreference /bridging linking, and therefore, long-distance CON-SUB linking. This is obviously not the case for nonsubevent CONTAINS relations. 4.2 Inter-annotator Agreement While the gold intra-document CON-SUB relations enabled high cross-document"
2020.louhi-1.12,N19-1423,0,0.0715314,"(different narratives have different goals, which in turn influences meaning interpretation). This is discussed in detail in Section 4. We empirically found it essential to take changes in discourse context into account and suggest the same would be true for any annotation project that is interested in temporal reasoning, particularly those dealing with longer timelines (i.e., beyond the single-document level). Recent developments in natural language processing establish neural approaches and more specifically transformer-based methods as the state of the art. Pre-trained models such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), Xlnet (Yang et al., 2019), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and SpanBERT (Joshi et al., 2020) report significant gains on multiple tasks. Thus, we demonstrate the learnability of the refined temporal relations in the context of these recent methodological developments. 2 prepositions and adjectives (e.g., during, subsequent to, prior to), chronological narrative progression, and so forth. Additionally, the notes had been separately annotated for intra-document coreference (IDENTICAL) and bridging (SET-SUBSET, WHOLEP"
2020.louhi-1.12,W13-1203,0,0.191253,"r. ii. cancer CONTAINS tumor iii. cancer CONTAINS adenocarcinoma The merged THYME and coreference annotations2 for (1) and (2) were as follows:      January 17, 2009 CONTAINS CT CT CONTAINS metastases February 20, 2009 CONTAINS resected metastases OVERLAPS resected metastases IDENTICAL metastases Both sets of intra-document links are pragmatically appropriate. Discourse contexts can expand or reduce the level of granularity at which a sense is interpreted (Recasens et al., 2011; also see Hobbs, 1985). In Note A, the text supports a coarse-grained interpretation of adenocarcinoma, or what Hovy et al., 2013 term a “wide” reading; it refers generally to the patient’s cancer. Note B, however, requires a fine-grained (“narrow”) interpretation – adenocarcinoma here refers specifically to the new, inoperable tumor and is contrasted with the original, resected tumor. The quandary for the cross-document task lies in whether to link adenocarcinoma in A as IDENTICAL to adenocarcinoma in B. An IDENTICAL relation entails logical impossibilities: assuming we also link cancerA as IDENT to cancerB, the combined within- and crossdocument relations now say the recurrent adenocarcinoma temporally contains itself"
2020.louhi-1.12,2020.tacl-1.5,0,0.153352,"take changes in discourse context into account and suggest the same would be true for any annotation project that is interested in temporal reasoning, particularly those dealing with longer timelines (i.e., beyond the single-document level). Recent developments in natural language processing establish neural approaches and more specifically transformer-based methods as the state of the art. Pre-trained models such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), Xlnet (Yang et al., 2019), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and SpanBERT (Joshi et al., 2020) report significant gains on multiple tasks. Thus, we demonstrate the learnability of the refined temporal relations in the context of these recent methodological developments. 2 prepositions and adjectives (e.g., during, subsequent to, prior to), chronological narrative progression, and so forth. Additionally, the notes had been separately annotated for intra-document coreference (IDENTICAL) and bridging (SET-SUBSET, WHOLEPART) relations, which were later merged with the temporal annotations (Wright-Bettner et al., 2019). Temporal relations alone are insufficient for timeline extraction; core"
2020.louhi-1.12,P14-1094,0,0.029063,"these pre-existing within-note annotations by manually adding coreference and bridging links across each set of three notes. In the process, we discovered a subset of the original CONTAINS relations contributed to temporally-conflicting information, which led to the addition of two new TLINKs: NOTED-ON and CON-SUB (Wright-Bettner et al., 2019). We discuss below how these updates contribute to more accurate and comprehensive temporal relations which facilitated cross-document linking. As such, this is one of the few studies in clinical NLP for cross-document temporal relation annotations (see Raghavan et al., 2014 and Wright-Bettner et al., 2019; also see Song et al., 2018 for general domain cross-document temporal annotation discussions). Dataset The 594 notes that make up the colon cancer part of the THYME corpus are grouped into sets, each set pertaining to a single patient and consisting of three notes written at different times during the patient’s course of care. These notes had been previously annotated for five different intradocument temporal relations (BEFORE, OVERLAP, BEGINS-ON, ENDS-ON and CONTAINS), a subset of the ISO-TimeML temporal link (TLINK) types (Pustejovsky et al., 2010, Styler IV"
2020.louhi-1.12,L18-1558,0,0.035104,"Missing"
2020.louhi-1.12,W19-1908,1,0.922329,"e refinements of the THYME+ annotations. Splitting CONTAINS into CONTAINS and CON-SUB relations and OVERLAP into OVERLAP and NOTEDON relations leads to better learnability: CONTAINS goes from 0.664 F1 on THYME to 0.748 F1 on THYME+, and OVERLAP goes from 0.179 on THYME to 0.416 on THYME+. The best results for the new categories of CON-SUB and NOTED-ON are 0.072 F1 and 0.744 F1 respectively – results that establish baselines for these two new temporal relations. The performance on all types of relations for THYME+ is 0.625 F1 compared to 0.548 for THYME (Table 2, Overall column, rows 1 and 2). Lin et al., 2019 report 0.684 F1 for THYME CONTAINS, however the result is achieved when training on and evaluating for only the CONTAINS links, and augmenting the training data with automatically generated CONTAINS relations. Thus, it is not a fair comparison to use for the results reported in Table 2. Of the models beyond BioBERT that we explored, BART-large was the most successful. The result with BART-large was 0.748 F1 (Table 2, CONTAINS column, row 3). In general, certain pre-trained models, like BioBERT and BART, yield better results than the other models. BioBERT is pre-trained on biomedical text and"
2020.louhi-1.12,W16-5706,1,0.878583,"Missing"
2020.louhi-1.12,pustejovsky-etal-2010-iso,0,0.037864,"otations (see Raghavan et al., 2014 and Wright-Bettner et al., 2019; also see Song et al., 2018 for general domain cross-document temporal annotation discussions). Dataset The 594 notes that make up the colon cancer part of the THYME corpus are grouped into sets, each set pertaining to a single patient and consisting of three notes written at different times during the patient’s course of care. These notes had been previously annotated for five different intradocument temporal relations (BEFORE, OVERLAP, BEGINS-ON, ENDS-ON and CONTAINS), a subset of the ISO-TimeML temporal link (TLINK) types (Pustejovsky et al., 2010, Styler IV et al., 2014)1. To keep annotation manageable and circumvent massively inferential temporal linking, the THYME guidelines constrained TLINK creation to events within the same sentence or adjacent sentences, and specifically prohibited TLINKing across sections (these are clinically-delineated sections separated from each other by numerical section IDs – History of Present Illness is section 20103, Vital Signs is section 20110, etc.) Linguistic evidence for creating these TLINKs included local cues such as temporal 3 Refined Temporal Relation: NOTEDON The THYME guidelines specified t"
2020.louhi-1.12,D19-6201,1,0.888309,"Missing"
2020.semeval-1.240,D19-5024,0,0.0120952,"31 teams with 0.556 F1 in the TC task. Our code is available at https://github.com/amenra99/SemEval2020_Task11. 1 Introduction Propaganda is purposeful information aiming to influence an audience to persuade an ideological or political agenda. Propagandistic messages use psychological and rhetorical techniques to hide their intention and can be delivered through materials such as articles, books, images, videos, etc. There have been multiple studies addressing propaganda detection using machine learning systems (Rashkin et al., 2017; Volkova and Jang, 2018; Barr´on-Cedeno et al., 2019; Da San Martino et al., 2019b). More recently, Da San Martino et al. (2019a) organized the NLP4IF-2019 shared task on Fine-Grained Propaganda Detection extended from their previous work, which has a similar framework to this shared task. In SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles, the organizers provided a Propaganda Techniques Corpus (PTC) manually annotated by six professional annotators in 446 news articles from 48 news outlets (Da San Martino et al., 2019b). This shared task is divided into two sub-tasks. The first sub-task is Span Identification (SI) to identify the begin offset and"
2020.semeval-1.240,D19-1565,0,0.0245618,"Missing"
2020.semeval-1.240,2020.semeval-1.186,0,0.0782165,"Missing"
2020.semeval-1.240,D17-1317,0,0.0310354,"r system ranked 20th out of 36 teams with 0.398 F1 in the SI task and 14th out of 31 teams with 0.556 F1 in the TC task. Our code is available at https://github.com/amenra99/SemEval2020_Task11. 1 Introduction Propaganda is purposeful information aiming to influence an audience to persuade an ideological or political agenda. Propagandistic messages use psychological and rhetorical techniques to hide their intention and can be delivered through materials such as articles, books, images, videos, etc. There have been multiple studies addressing propaganda detection using machine learning systems (Rashkin et al., 2017; Volkova and Jang, 2018; Barr´on-Cedeno et al., 2019; Da San Martino et al., 2019b). More recently, Da San Martino et al. (2019a) organized the NLP4IF-2019 shared task on Fine-Grained Propaganda Detection extended from their previous work, which has a similar framework to this shared task. In SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles, the organizers provided a Propaganda Techniques Corpus (PTC) manually annotated by six professional annotators in 446 news articles from 48 news outlets (Da San Martino et al., 2019b). This shared task is divided into two sub-task"
2021.adaptnlp-1.11,S17-2093,1,0.874823,"nding to, for example, decisions to not use a drug. DocTimeRel helps us distinguish mentions of drugs that are current from those that predate the current time period, or are being speculated about for future use. ALINK can model drug start, stop, and continuation events, which can help to distinguish whether a missing mention in the middle of a record corresponds to a stop and restart, or an incidentally omitted mention. Figure 1 shows an example instance of a drug mention to be classified for all three tasks. The THYME dataset (Styler IV et al., 2014), released as part of Clinical TempEval (Bethard et al., 2017), contains all three of these annotation types, on 1200 notes of patients with colon and brain cancer. We train all models on the colon cancer section (details on data are in Section 4). While our bigger 106 project is specific to drug mentions, the problem is not limited to drug mentions, so we train and evaluate on all annotated events in the THYME corpus. We also assume that events are given, to allow a straightforward metric of how many events we “get right” when combining all property predictions. In the real world, events will have to be automatically detected, so our metric will be an u"
2021.adaptnlp-1.11,N19-1423,0,0.430519,"ancer corpus and an internal corpus, and show that performance of the combined systems is unacceptable despite good performance of individual systems. Although domain adaptation shows improvements on each individual system, the model selection problem is a barrier to improving overall pipeline performance. 1 Introduction Advances in machine learning methods and the release of annotated datasets of clinical texts (Uzuner et al., 2011; Styler IV et al., 2014) in the past decade has led to an increase of available clinical NLP systems for interesting tasks. Recent advances in pre-trained models (Devlin et al., 2019; Liu et al., 2019) have made ever more accurate clinical NLP systems possible. Unsupervised domain adaptation algorithms (e.g., Ziser and Reichart (2019)) have made it possible to reduce performance degradation when applying trained models to new domains. The great promise of these developments is that these methods can be combined into pipelines that allow for sophisticated information extraction capabilities for downstream clinical use cases. Rather than building one-off datasets for each complex downstream task that arises, standard NLP components could potentially be used as “Lego”-style"
2021.adaptnlp-1.11,W06-1673,0,0.116081,"s to new domains. The great promise of these developments is that these methods can be combined into pipelines that allow for sophisticated information extraction capabilities for downstream clinical use cases. Rather than building one-off datasets for each complex downstream task that arises, standard NLP components could potentially be used as “Lego”-style building blocks that allow for flexibly approaching new tasks as they arise. However, the existence of the building blocks alone does not solve this problem. Combining individual components into NLP pipelines can lead to cascading errors (Finkel et al., 2006). The true error rate for structured extraction tasks is potentially as high as the sum of the component tasks’ errors. For example, if the goal is to extract normalized concepts with assertion status, the concept error can come from normalization error, negation detection error, uncertainty detection error, etc, and the errors may not be correlated. These problems are exacerbated in the common case where individual components are trained on data from different domains, and tested on data from yet another domain. In this work, we quantitatively examine the issues described above in the context"
2021.adaptnlp-1.11,2020.acl-main.740,0,0.0448906,"Missing"
2021.adaptnlp-1.11,D19-1433,0,0.0224521,"examples. Since large pre-trained transformer models have arrived, they have been shown to be quite robust to out-of-distribution examples (Hendrycks et al., 2020), including on clinical tasks (Lin et al., 2020), where it was shown that adding domain adaptation layers on top of BERT was no better than BERT itself for negation detection. One of the few effective methods for improving the out-of-distribution performance of pre-trained transformer models has been to continue to pre-train the language modeling objective on the target domain data, before any fine-tuning is done on the source data (Han and Eisenstein, 2019; Gururangan et al., 2020). In this work, we focus on that method, since this is currently the most promising direction for adapting large pre-trained transformers. Specifically, to use 107 this method, we run additional masked language model training steps on the target training data from the RoBERTa-base checkpoint, before fine-tuning on the labeled colon cancer data, and then testing on target test data. We tune the learning rate for the language model pre-training on target development set data, optimizing for perplexity. 4 Evaluation For the three tasks of interest, we evaluate indomain ("
2021.adaptnlp-1.11,2020.acl-main.244,0,0.0345826,"Missing"
2021.adaptnlp-1.11,2020.bionlp-1.17,0,0.0366554,"ven though we use a metric, accuracy, that is forgiving to the worst-performing individual model. 2 Background It is both formally and empirically understood that classifiers can suffer performance loss when the test data is drawn from a different distribution than the training data (sometimes called domain shift). This presents a difficult challenge in clinical NLP because data-sharing limitations make it difficult to create large and diverse training corpora. As a result, domain adaptation approaches have been applied to multiple tasks in clinical NLP (Miller et al., 2017; Liu et al., 2018; Hur et al., 2020). Recent work in the general domain has made use of transfer learning, which can attack the problem of domain shift, but by a different mechanism than domain adaptation; by training on massive corpora, large pre-trained models both learn general features, and are able to learn from smaller new datasets without overfitting. The most prominent of these models are based on the transformer architecture (Vaswani et al., 2017). BERT (Devlin et al., 2019) uses a transformer encoder, and has shown that pre-training with massive amounts of text on a language modeling task, then fine-tuning on a supervi"
2021.adaptnlp-1.11,W18-2315,0,0.0185871,"This is the case even though we use a metric, accuracy, that is forgiving to the worst-performing individual model. 2 Background It is both formally and empirically understood that classifiers can suffer performance loss when the test data is drawn from a different distribution than the training data (sometimes called domain shift). This presents a difficult challenge in clinical NLP because data-sharing limitations make it difficult to create large and diverse training corpora. As a result, domain adaptation approaches have been applied to multiple tasks in clinical NLP (Miller et al., 2017; Liu et al., 2018; Hur et al., 2020). Recent work in the general domain has made use of transfer learning, which can attack the problem of domain shift, but by a different mechanism than domain adaptation; by training on massive corpora, large pre-trained models both learn general features, and are able to learn from smaller new datasets without overfitting. The most prominent of these models are based on the transformer architecture (Vaswani et al., 2017). BERT (Devlin et al., 2019) uses a transformer encoder, and has shown that pre-training with massive amounts of text on a language modeling task, then fine-"
2021.adaptnlp-1.11,2021.ccl-1.108,0,0.0694764,"Missing"
2021.adaptnlp-1.11,W17-2320,1,0.790952,"om multiple systems. This is the case even though we use a metric, accuracy, that is forgiving to the worst-performing individual model. 2 Background It is both formally and empirically understood that classifiers can suffer performance loss when the test data is drawn from a different distribution than the training data (sometimes called domain shift). This presents a difficult challenge in clinical NLP because data-sharing limitations make it difficult to create large and diverse training corpora. As a result, domain adaptation approaches have been applied to multiple tasks in clinical NLP (Miller et al., 2017; Liu et al., 2018; Hur et al., 2020). Recent work in the general domain has made use of transfer learning, which can attack the problem of domain shift, but by a different mechanism than domain adaptation; by training on massive corpora, large pre-trained models both learn general features, and are able to learn from smaller new datasets without overfitting. The most prominent of these models are based on the transformer architecture (Vaswani et al., 2017). BERT (Devlin et al., 2019) uses a transformer encoder, and has shown that pre-training with massive amounts of text on a language modelin"
2021.adaptnlp-1.11,P19-1591,0,0.0187895,"s. Although domain adaptation shows improvements on each individual system, the model selection problem is a barrier to improving overall pipeline performance. 1 Introduction Advances in machine learning methods and the release of annotated datasets of clinical texts (Uzuner et al., 2011; Styler IV et al., 2014) in the past decade has led to an increase of available clinical NLP systems for interesting tasks. Recent advances in pre-trained models (Devlin et al., 2019; Liu et al., 2019) have made ever more accurate clinical NLP systems possible. Unsupervised domain adaptation algorithms (e.g., Ziser and Reichart (2019)) have made it possible to reduce performance degradation when applying trained models to new domains. The great promise of these developments is that these methods can be combined into pipelines that allow for sophisticated information extraction capabilities for downstream clinical use cases. Rather than building one-off datasets for each complex downstream task that arises, standard NLP components could potentially be used as “Lego”-style building blocks that allow for flexibly approaching new tasks as they arise. However, the existence of the building blocks alone does not solve this probl"
2021.bionlp-1.2,S15-2070,0,0.0272233,"se pre-selected negative samples are informative for the whole training process (Hermans et al., 2017; Sung et al., 2020). The code for our proposed framework is available at https://github.com/dongfang91/ Triplet-Search-ConNorm. 2 Related work Earlier work on concept normalization focuses on how to use morphological information to conduct lexical look-up and string matching (Kang et al., 2013; D’Souza and Ng, 2015; Leaman et al., 2015; Leal et al., 2015; Kate, 2016; Lee et al., 2016; Jonnagaddala et al., 2016). They rely heavily on handcrafted rules and domain knowledge, e.g., D’Souza and Ng (2015) define 10 types of rules at different priority levels to measure morphological similarity between mentions and candidate concepts in the ontologies. The lack of lexical overlap between concept mention and concept in domains like social media, makes rule-based approaches that rely on lexical matching less applicable. Inspired by Schroff et al. (2015), we propose a triplet network with online hard triplet mining for concept normalization. Our framework sets up concept normalization as a one-step process, calculating similarity between vector representations of the mention and of all concepts in"
2021.bionlp-1.2,P15-2049,0,0.0251209,"Missing"
2021.bionlp-1.2,W03-0428,0,0.10043,"Missing"
2021.bionlp-1.2,P16-1096,0,0.110499,"he 2013 ShARe/CLEF (Suominen et al., 2013), chemical and disease normalization in BioCreative V Chemical Disease Relation (CDR) Task (Wei et al., 2015), and medical concept normalization in 2019 n2c2 shared task (Henry et al., 2020), and to the availability of annotated data (Do˘gan et al., 2014; Luo et al., 2019). Existing approaches can be divided into three categories: rule-based approaches using string-matching or dictionary look-up (Leal et al., 2015; D’Souza and Ng, 2015; Lee et al., 2016), which rely heavily on handcrafted rules and domain knowledge; supervised multi-class classifiers (Limsopatham and Collier, 2016; Lee et al., 2017; Tutubalina et al., 2018; Niu et al., 2019; Li et al., 2019), which cannot generalize to concept types not present in their training data; and two-step frameworks based on a nontrained candidate generator and a supervised candidate ranker (Leaman et al., 2013; Li et al., 2017; Liu and Xu, 2017; Nguyen et al., 2018; Murty et al., 2018; Mondal et al., 2019; Ji et al., 2020; Xu et al., 2020), which require complex pipelines and fail if the candidate generator does not find the gold truth concept. Concept normalization, the task of linking textual mentions of concepts to concept"
2021.bionlp-1.2,P16-1000,0,0.177578,"Missing"
2021.bionlp-1.2,D19-1410,0,0.107726,"oncept texts denoting concept c. Concept text may come from an ontology, t ∈ O(c), where O(c) is the synonyms of the concept c from the ontology O, or from an annotated corpus, t ∈ D(c), where D(c) is the mentions of the concept c in an annotated corpus D. T (c) will allow the generation of tuples (t, c) such as (MI,C0027051) and (Myocardial Infarction,C0027051). Note that, for a Sim(V (ti ), V (tip )) > Sim(V (ti ), V (tin )) (2) The triplet network architecture has been adopted in learning representations for images (Schroff et al., 2015; Gordo et al., 2016) and text (Neculoiu et al., 2016; Reimers and Gurevych, 2019). It consists of three instances of the same sub-network (with shared parameters). When fed a (ti , tip , tin ) triplet of texts, the sub-network outputs vector representations for each text, which are then fed into a triplet loss. We adopt PubMed-BERT (Gu et al., 13 2020) as the sub-network, where the representation for the concept text is an average pooling of the representations for all sub-word tokens2 . This architecture is shown in Figure 1. The inputs to our model are only the mentions or synonyms. We leave the resolution of ambiguous mentions, which will require exploration of contextu"
2021.bionlp-1.2,P19-2055,0,0.0186066,"ies like the Unified Medical Language System (UMLS) Metathesaurus1 are common. Unlike prior work, our simple and efficient model requires neither negative sampling before the training nor a candidate generator during inference. Our work makes the following contributions: Supervised approaches for concept normalization have improved with the availability of annotated data and deep learning techniques. When the number of concepts to be predicted is small, classification-based approaches (Limsopatham and Collier, 2016; Lee et al., 2017; Tutubalina et al., 2018; Niu et al., 2019; Li et al., 2019; Miftahutdinov and Tutubalina, 2019) are often adopted, with the size of the classifier’s output space equal to the number of concepts. Approaches differ in neural architectures, such as character-level convolution neural networks (CNN) with multi-task learning (Niu et al., 2019) and pre-trained transformer networks (Li et al., 2019; Miftahutdinov and Tutubalina, 2019). However, classification approaches struggle when the annotated training data does not contain examples of all concepts – common when there are many concepts in the ontology – since the output space of the classifier will not include concepts absent from the train"
2021.bionlp-1.2,2020.acl-main.760,0,0.173637,"epresentations of the mention and of all concepts in the ontology. Online hard triplet mining allows such a vector space model to generate triplets of (mention, true concept, false concept) within a mini-batch, leading to efficient training and fast convergence (Schroff et al., 2015). In contrast with previous vector space models where mention and candidate 1 https://www.nlm.nih.gov/research/ umls/knowledge_sources/metathesaurus/ index.html 12 concepts are mapped to vectors via TF-IDF (Leaman et al., 2013), TreeLSTMs (Liu and Xu, 2017), CNNs (Nguyen et al., 2018; Mondal et al., 2019) or ELMO (Schumacher et al., 2020), we generate vector representations with BERT (Devlin et al., 2019), since it can encode both surface and semantic information (Ma et al., 2019). There are a few similar works to our vector space model, CNN-triplet (Mondal et al., 2019), BIOSYN (Sung et al., 2020), RoBERTa-Node2Vec (Pattisapu et al., 2020), and TTI (Henry et al., 2020). CNN-triplet is a two-step approach, requiring a generator to generate candidates for training the triplet network, and requiring various embedding resources as input to CNN-based encoder. BIOSYN, RoBERTa-Node2Vec, and TTI are onestep approaches. BIOSYN require"
2021.bionlp-1.2,W19-1912,0,0.212957,"approaches using string-matching or dictionary look-up (Leal et al., 2015; D’Souza and Ng, 2015; Lee et al., 2016), which rely heavily on handcrafted rules and domain knowledge; supervised multi-class classifiers (Limsopatham and Collier, 2016; Lee et al., 2017; Tutubalina et al., 2018; Niu et al., 2019; Li et al., 2019), which cannot generalize to concept types not present in their training data; and two-step frameworks based on a nontrained candidate generator and a supervised candidate ranker (Leaman et al., 2013; Li et al., 2017; Liu and Xu, 2017; Nguyen et al., 2018; Murty et al., 2018; Mondal et al., 2019; Ji et al., 2020; Xu et al., 2020), which require complex pipelines and fail if the candidate generator does not find the gold truth concept. Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is critical for mining and analyzing biomedical texts. We propose a vector-space model for concept normalization, where mentions and concepts are encoded via transformer networks that are trained via a triplet objective with online hard triplet mining. The transformer networks refine existing pre-trained models, and the online triplet mining makes trainin"
2021.bionlp-1.2,2020.acl-main.335,0,0.233213,"rank (Li et al., 2017), pair-wise learning to rank (Leaman et al., 2013; Liu and Xu, 2017; Nguyen et al., 2018; Mondal et al., 2019), and list-wise learning to rank (Murty et al., 2018; Ji et al., 2020; Xu et al., 2020). These learning to rank approaches also have drawbacks. Firstly, if the candidate generator fails to produce the gold truth concept, the candidate ranker will also fail. Secondly, the training of candidate ranker requires negative sampling beforehand, and it is unclear if these pre-selected negative samples are informative for the whole training process (Hermans et al., 2017; Sung et al., 2020). The code for our proposed framework is available at https://github.com/dongfang91/ Triplet-Search-ConNorm. 2 Related work Earlier work on concept normalization focuses on how to use morphological information to conduct lexical look-up and string matching (Kang et al., 2013; D’Souza and Ng, 2015; Leaman et al., 2015; Leal et al., 2015; Kate, 2016; Lee et al., 2016; Jonnagaddala et al., 2016). They rely heavily on handcrafted rules and domain knowledge, e.g., D’Souza and Ng (2015) define 10 types of rules at different priority levels to measure morphological similarity between mentions and can"
2021.bionlp-1.2,P18-1010,0,0.163095,"tegories: rule-based approaches using string-matching or dictionary look-up (Leal et al., 2015; D’Souza and Ng, 2015; Lee et al., 2016), which rely heavily on handcrafted rules and domain knowledge; supervised multi-class classifiers (Limsopatham and Collier, 2016; Lee et al., 2017; Tutubalina et al., 2018; Niu et al., 2019; Li et al., 2019), which cannot generalize to concept types not present in their training data; and two-step frameworks based on a nontrained candidate generator and a supervised candidate ranker (Leaman et al., 2013; Li et al., 2017; Liu and Xu, 2017; Nguyen et al., 2018; Murty et al., 2018; Mondal et al., 2019; Ji et al., 2020; Xu et al., 2020), which require complex pipelines and fail if the candidate generator does not find the gold truth concept. Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is critical for mining and analyzing biomedical texts. We propose a vector-space model for concept normalization, where mentions and concepts are encoded via transformer networks that are trained via a triplet objective with online hard triplet mining. The transformer networks refine existing pre-trained models, and the online triplet"
2021.bionlp-1.2,W16-1617,0,0.0275921,"here T (c) is all the concept texts denoting concept c. Concept text may come from an ontology, t ∈ O(c), where O(c) is the synonyms of the concept c from the ontology O, or from an annotated corpus, t ∈ D(c), where D(c) is the mentions of the concept c in an annotated corpus D. T (c) will allow the generation of tuples (t, c) such as (MI,C0027051) and (Myocardial Infarction,C0027051). Note that, for a Sim(V (ti ), V (tip )) > Sim(V (ti ), V (tin )) (2) The triplet network architecture has been adopted in learning representations for images (Schroff et al., 2015; Gordo et al., 2016) and text (Neculoiu et al., 2016; Reimers and Gurevych, 2019). It consists of three instances of the same sub-network (with shared parameters). When fed a (ti , tip , tin ) triplet of texts, the sub-network outputs vector representations for each text, which are then fed into a triplet loss. We adopt PubMed-BERT (Gu et al., 13 2020) as the sub-network, where the representation for the concept text is an average pooling of the representations for all sub-word tokens2 . This architecture is shown in Figure 1. The inputs to our model are only the mentions or synonyms. We leave the resolution of ambiguous mentions, which will re"
2021.bionlp-1.2,2020.acl-main.748,1,0.926713,"ictionary look-up (Leal et al., 2015; D’Souza and Ng, 2015; Lee et al., 2016), which rely heavily on handcrafted rules and domain knowledge; supervised multi-class classifiers (Limsopatham and Collier, 2016; Lee et al., 2017; Tutubalina et al., 2018; Niu et al., 2019; Li et al., 2019), which cannot generalize to concept types not present in their training data; and two-step frameworks based on a nontrained candidate generator and a supervised candidate ranker (Leaman et al., 2013; Li et al., 2017; Liu and Xu, 2017; Nguyen et al., 2018; Murty et al., 2018; Mondal et al., 2019; Ji et al., 2020; Xu et al., 2020), which require complex pipelines and fail if the candidate generator does not find the gold truth concept. Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is critical for mining and analyzing biomedical texts. We propose a vector-space model for concept normalization, where mentions and concepts are encoded via transformer networks that are trained via a triplet objective with online hard triplet mining. The transformer networks refine existing pre-trained models, and the online triplet mining makes training efficient even with hundreds of t"
2021.bionlp-1.21,W19-1909,0,0.0260304,"Missing"
2021.bionlp-1.21,D19-1371,0,0.0358033,"Missing"
2021.bionlp-1.21,S15-2136,1,0.718023,"ilot data, MASCC score appears promising in determining suitability for outpatient management of NF in gynecologic oncology patients. Prospective study is ongoing to confirm safety and determine impact on cost. Figure 2: Sample instances for DocTimeRel(1), TLINK:event-time(2), TLINK:event-event(3), Negation (4), and PubMedQA (5). 2.3 Labeled Fine-tuning Data The following sections describe the labeled datasets that are used as fine-tuning tasks. Figure 2 shows examples of how we format inputs for these tasks (more details below). THYME The THYME corpus (Styler IV et al., 2014) is widely used (Bethard et al., 2015, 2016, 2017) for clinical temporal relation discovery. There are two types of temporal relations defined in it: (1) The document time relations (DocTimeRel), which link a clinical event (EVENT) to the document creation time (DCT) with possible values of BEFORE, AFTER, OVERLAP, and BEFORE_OVERLAP, and (2) pairwise temporal relations (TLINK) between two events (EVENT) or an event and a time expression (TIMEX3) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). Recently, the TLINK annotations of (2) were refined with values of BEFORE, BEGINS-ON, CONTAINS, CONS"
2021.bionlp-1.21,E17-2118,1,0.933747,"e Beth Israel Deaconess Medical Center between 2001 and 2012. We process the MIMIC-III corpus with the sentence detection, tokenization, and temporal modules of Apache cTAKES (Savova et al., 2010)2 to identify all entities (events and time expressions) in the corpus. Events are recognized by cTAKES event annotator. Event types include diseases/disorders, signs/symptoms, medications, anatomical sites, and procedures. Time expressions are recognized by cTAKES timex annotator. Time classes includes: date, time, duration, quantifier, prepostesp, and set (Styler IV et al., 2014). Special XML tags (Dligach et al., 2017) are inserted into the text sequence to mark the position of identified entities. Time expressions are replaced by their time class (Lin et al., 2017, 2018) for better generalizability. All special XML-tags and time class tokens are added into the PubMedBERT vocabulary so that they can be recognized. The top line of Figure 1 shows a sample sentence from the MIMIC-III corpus. The entities of this sentence are identified by Apache cTAKES. The bottom line of Figure 1 shows the entities marked by XML tags and the temporal expression replaced by its class. We process the MIMIC corpus sentence by se"
2021.bionlp-1.21,2020.tacl-1.5,0,0.30533,"ent amounts of labeled data. Many clinical NLP tasks are centered around entities: clinical named entity recognition aims to detect clinical entities (Wu et al., 2017; Pradhan et al., 2014; Elhadad et al., 2015), clinical negation extraction decides if a certain clinical entity is negated (Chapman et al., 2001; Harkema et al., 2009; Mehrabi et al., 2015), clinical relation discovery extracts relations among clinical entities (Lv et al., 2016; Leeuwenberg and Moens, 2017), etc. Though various masking strategies have been employed during pretraining – masking contiguous spans of text (SpanBERT, Joshi et al., 2020; BART, Lewis et al., 2019), varying masking ratios (Raffel et al., 2019), building additional neural models to predict which words to mask (Gu et al., 2020b), incorporating knowledge graphs (Zhang et al., 2019), masking entities for a named entity recognition task (Ziyadi et al., 2020) – none of the masking techniques so far have investigated and focused on clinical entities. Besides transformer-based models, there are other efforts (Beam et al., 2019; Chen et al., 2020) to characterize the biomedical/clinical entities at the word embedding level. There are also other statistical methods appl"
2021.bionlp-1.21,E17-1108,0,0.0406111,"Missing"
2021.bionlp-1.21,S15-2051,1,0.741722,"represents one specialty in medicine – intensive care. Pretraining is agnostic to downstream tasks: it learns representations for all words using a selfsupervised data-rich task. Yet, not all words are important for downstream fine-tuning tasks. Numerous pretrained words are not even used in the fine-tuning step, while important words crucial for the downstream task are not well represented due to insufficient amounts of labeled data. Many clinical NLP tasks are centered around entities: clinical named entity recognition aims to detect clinical entities (Wu et al., 2017; Pradhan et al., 2014; Elhadad et al., 2015), clinical negation extraction decides if a certain clinical entity is negated (Chapman et al., 2001; Harkema et al., 2009; Mehrabi et al., 2015), clinical relation discovery extracts relations among clinical entities (Lv et al., 2016; Leeuwenberg and Moens, 2017), etc. Though various masking strategies have been employed during pretraining – masking contiguous spans of text (SpanBERT, Joshi et al., 2020; BART, Lewis et al., 2019), varying masking ratios (Raffel et al., 2019), building additional neural models to predict which words to mask (Gu et al., 2020b), incorporating knowledge graphs (Z"
2021.bionlp-1.21,J81-4005,0,0.637309,"Missing"
2021.bionlp-1.21,2020.emnlp-main.566,0,0.222855,"ppears to provide a vocabare pre-trained on large general domain corpora, ulary that is helpful to the clinical domain. Howmany efforts have been made to continue pre- ever, the language of biomedical literature is diftaining general-domain language models on clini- ferent from the language of the clinical documents cal/biomedical corpora to derive domain-specific found in electronic medical records (EMRs). In language models (Lee et al., 2020; Alsentzer et al., general, a clinical document is written by physi2019; Beltagy et al., 2019). cians who have very limited time to express the Yet, as Gu et al. (2020a) pointed out, in special- numerous details of a patient-physician encounter. ized domains such as biomedicine, continued pre- Many nonstandard expressions, abbreviations, astraining from generic language models is inferior sumptions and domain knowledge are used in clinito domain-specific pretraining from scratch. Con- cal notes which makes the text hard to understand tinued pre-training from a generic model would outside of the clinical community and presents 191 Transformer-based neural language models have led to breakthroughs for a variety of natural language processing (NLP) tasks. Howe"
2021.bionlp-1.21,D19-1259,0,0.369594,"oad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection (Wu et al., 2014), document time relation (DocTimeRel) classification (Lin et al., 2020b), and temporal relation extraction (Wright-Bettner et al., 2020). We also evaluate our models on the PubMedQA(Jin et al., 2019) dataset to measure the models’ performance on a nonentity-centric task in the biomedical domain. The language addressed in this work is English. Proceedings of the BioNLP 2021 workshop, pages 191–201 June 11, 2021. ©2021 Association for Computational Linguistics challenges for automated systems. Pretraining a language model specific to the clinical domain requires large amounts of unlabeled clinical text on par with what the generic models are trained on. Unfortunately, such data are not available to the community. The only available such corpus is MIMIC III used to train ClinicalBERT (Alsent"
2021.bionlp-1.21,2020.acl-main.703,0,0.0877052,"Missing"
2021.bionlp-1.21,W18-5619,1,0.903048,"Missing"
2021.bionlp-1.21,W17-2341,1,0.747659,"les of Apache cTAKES (Savova et al., 2010)2 to identify all entities (events and time expressions) in the corpus. Events are recognized by cTAKES event annotator. Event types include diseases/disorders, signs/symptoms, medications, anatomical sites, and procedures. Time expressions are recognized by cTAKES timex annotator. Time classes includes: date, time, duration, quantifier, prepostesp, and set (Styler IV et al., 2014). Special XML tags (Dligach et al., 2017) are inserted into the text sequence to mark the position of identified entities. Time expressions are replaced by their time class (Lin et al., 2017, 2018) for better generalizability. All special XML-tags and time class tokens are added into the PubMedBERT vocabulary so that they can be recognized. The top line of Figure 1 shows a sample sentence from the MIMIC-III corpus. The entities of this sentence are identified by Apache cTAKES. The bottom line of Figure 1 shows the entities marked by XML tags and the temporal expression replaced by its class. We process the MIMIC corpus sentence by sentence, and discard sentences that have fewer than two entities. The resulting set (MIMIC-BIG) has 15.6 million sentences, 728.6 million words (the b"
2021.bionlp-1.21,W19-1908,1,0.8231,"stejovsky and Stubbs, 2011). Recently, the TLINK annotations of (2) were refined with values of BEFORE, BEGINS-ON, CONTAINS, CONSUB, ENDS-ON, NOTED-ON, OVERLAP, with the revised corpus known as the THYME+ corpus (Wright-Bettner et al., 2020). For the DocTimeRel task, we mark all events in THYME+ corpus with XML tags (“&lt;e>”, “&lt;/e>”) and extract 10 tokens from each side of the event as the contextual information. The DocTimeRel 193 labels are predicted using the special [CLS] embedding and a softmax function. For the TLINK task, we use the THYME+ annotation and the same window-based processing (Lin et al., 2019; Wright-Bettner et al., 2020) for generating relational candidates. The two entities involved in a relation candidate are marked by XML tags following the style of Dligach et al. (2017). Time expressions are represented by their time classes. The TLINK labels are predicted using the special [CLS] embedding and a softmax function. Cross-domain Negation We use the same corpora as Miller et al. (2017); Lin et al. (2020a): (1) 2010 i2b2/VA NLP Challenge Corpus (i2b2: Uzuner et al., 2011), (2) the Multi-source Integrated Platform for Answering Clinical Questions Corpus (MiPACQ: Albright et al., 20"
2021.bionlp-1.21,2020.bionlp-1.7,1,0.888738,"ain data. We propose a methodology to produce a model focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection (Wu et al., 2014), document time relation (DocTimeRel) classification (Lin et al., 2020b), and temporal relation extraction (Wright-Bettner et al., 2020). We also evaluate our models on the PubMedQA(Jin et al., 2019) dataset to measure the models’ performance on a nonentity-centric task in the biomedical domain. The language addressed in this work is English. Proceedings of the BioNLP 2021 workshop, pages 191–201 June 11, 2021. ©2021 Association for Computational Linguistics challenges for automated systems. Pretraining a language model specific to the clinical domain requires large amounts of unlabeled clinical text on par with what the generic models are trained on. Unfortunat"
2021.bionlp-1.21,2021.ccl-1.108,0,0.0941984,"Missing"
2021.bionlp-1.21,W17-2320,1,0.823998,"Missing"
2021.bionlp-1.21,P19-3007,0,0.0287133,"verall 0.773 0.781 Table 5: Effect of masking strategy (Rand or Entity) on cross-domain negation detection. Performance is in terms of F1. PubMedBERT RandMask EntityBERT Accuracy 0.760 0.738 0.750 Table 7: Performance of models on PubMedQA. Model RandMask EntityBERT within-sentence cross-sentence 4,021 4,156 757 768 total 4,778 4,924 Table 8: Correctly predicted TLINK counts by EntityBERT and RandMask before temporal closure. centric task like the TLINK extraction task, these entities can be better utilized for reasoning relations which they are part of. In Figure 5 we visualize with BertViz (Vig, 2019) Model Domain after before bfr/ovlp overlap overall the attention weights of head zero from the last RandMask same 0.88 0.92 0.78 0.94 0.92 layer of the fine-tuned RandMask and EntityBERT EntityBERT same 0.88 0.92 0.79 0.94 0.92 models on the TLINK task for a relation that EntiRandMask cross 0.65 0.65 0.34 0.74 0.69 EntityBERT cross 0.64 0.66 0.40 0.77 0.72 tyBERT correctly predicted but RandMask missed. The context is he has had steroid &lt;e> injection Table 6: Effect of masking strategy (Rand or Entity) &lt;/e> &lt;t> date &lt;/t>. A plausible explanation is that for in-domain (same) and cross-domain s"
2021.bionlp-1.21,W19-5006,0,0.018031,"performance on a nonentity-centric task in the biomedical domain. The language addressed in this work is English. Proceedings of the BioNLP 2021 workshop, pages 191–201 June 11, 2021. ©2021 Association for Computational Linguistics challenges for automated systems. Pretraining a language model specific to the clinical domain requires large amounts of unlabeled clinical text on par with what the generic models are trained on. Unfortunately, such data are not available to the community. The only available such corpus is MIMIC III used to train ClinicalBERT (Alsentzer et al., 2019) and BlueBERT (Peng et al., 2019), but it is magnitudes smaller and represents one specialty in medicine – intensive care. Pretraining is agnostic to downstream tasks: it learns representations for all words using a selfsupervised data-rich task. Yet, not all words are important for downstream fine-tuning tasks. Numerous pretrained words are not even used in the fine-tuning step, while important words crucial for the downstream task are not well represented due to insufficient amounts of labeled data. Many clinical NLP tasks are centered around entities: clinical named entity recognition aims to detect clinical entities (Wu e"
2021.bionlp-1.21,S14-2007,1,0.875764,"Missing"
2021.bionlp-1.21,2020.louhi-1.12,1,0.852197,"focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection (Wu et al., 2014), document time relation (DocTimeRel) classification (Lin et al., 2020b), and temporal relation extraction (Wright-Bettner et al., 2020). We also evaluate our models on the PubMedQA(Jin et al., 2019) dataset to measure the models’ performance on a nonentity-centric task in the biomedical domain. The language addressed in this work is English. Proceedings of the BioNLP 2021 workshop, pages 191–201 June 11, 2021. ©2021 Association for Computational Linguistics challenges for automated systems. Pretraining a language model specific to the clinical domain requires large amounts of unlabeled clinical text on par with what the generic models are trained on. Unfortunately, such data are not available to the community. The only availa"
2021.bionlp-1.21,W11-0419,0,0.0145713,"t inputs for these tasks (more details below). THYME The THYME corpus (Styler IV et al., 2014) is widely used (Bethard et al., 2015, 2016, 2017) for clinical temporal relation discovery. There are two types of temporal relations defined in it: (1) The document time relations (DocTimeRel), which link a clinical event (EVENT) to the document creation time (DCT) with possible values of BEFORE, AFTER, OVERLAP, and BEFORE_OVERLAP, and (2) pairwise temporal relations (TLINK) between two events (EVENT) or an event and a time expression (TIMEX3) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). Recently, the TLINK annotations of (2) were refined with values of BEFORE, BEGINS-ON, CONTAINS, CONSUB, ENDS-ON, NOTED-ON, OVERLAP, with the revised corpus known as the THYME+ corpus (Wright-Bettner et al., 2020). For the DocTimeRel task, we mark all events in THYME+ corpus with XML tags (“&lt;e>”, “&lt;/e>”) and extract 10 tokens from each side of the event as the contextual information. The DocTimeRel 193 labels are predicted using the special [CLS] embedding and a softmax function. For the TLINK task, we use the THYME+ annotation and the same window-based processing (Lin et al., 2019; Wright-Be"
2021.bionlp-1.21,D17-1035,0,0.0482038,"Missing"
2021.bionlp-1.21,P19-1139,0,0.143485,"), clinical negation extraction decides if a certain clinical entity is negated (Chapman et al., 2001; Harkema et al., 2009; Mehrabi et al., 2015), clinical relation discovery extracts relations among clinical entities (Lv et al., 2016; Leeuwenberg and Moens, 2017), etc. Though various masking strategies have been employed during pretraining – masking contiguous spans of text (SpanBERT, Joshi et al., 2020; BART, Lewis et al., 2019), varying masking ratios (Raffel et al., 2019), building additional neural models to predict which words to mask (Gu et al., 2020b), incorporating knowledge graphs (Zhang et al., 2019), masking entities for a named entity recognition task (Ziyadi et al., 2020) – none of the masking techniques so far have investigated and focused on clinical entities. Besides transformer-based models, there are other efforts (Beam et al., 2019; Chen et al., 2020) to characterize the biomedical/clinical entities at the word embedding level. There are also other statistical methods applied to the downstream tasks. We do not include these efforts in our discussion because the focus of our paper is the investigation of a novel entity-based masking strategy in a transformer-based setting. entity-"
2021.conll-1.6,D19-1006,0,0.0556452,"Missing"
2021.conll-1.6,2020.tacl-1.3,0,0.017275,"judgments, reading speeds, neural responses). Evaluating pretrained language models on such tasks can thus provide insights on the linguistic biases acquired by the models. The linguistic properties studied include subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018), filler-gap depedencies (Wilcox et al., 2018), garden-path effects (Futrell et al., 2019), other types of syntactic awareness (Marvin and Linzen, 2018; Hu et al., 2020), and variations of grammatical judgment based on availability of contexts (Lau et al., 2020). Most works focus on syntactic knowledge (though see Ettinger (2020)’s work on semantic and pragmatic aspect). Our work contributes to this line of research. While most previous studies probe syntactic properties, we investigate a semantic property: telicity (whether an event has reached an end point or not). Telicity encoding is covert in English and needs to be inferred from information such as inherent telicity of the main verb, the argument structure, the quantity of noun phrases, and other constituents (Dowty, 1986; Verkuyl, 2013). For instance, the description “John read the book” might allow both telic (he finished reading the book) or atelic (he did no"
2021.conll-1.6,C18-1152,0,0.0575291,"Missing"
2021.conll-1.6,P17-1080,0,0.0291877,"s do. 1 Introduction Large pretrained-language models (ELMo: Peters et al., 2018a, BERT: Devlin et al., 2019, RoBERTa Liu et al., 2019, etc.) keep achieving new states of the art in a variety of NLP tasks, leading to a growing interest in exploring what has been acquired by the pretraining objectives. Many recent works utilize probes: shallow, usually supervised classifiers that try to determine which linguistic phenomena are predictable from the pretrained representations. The linguistic properties studied include syntactic relationships (Hewitt and Manning, 2019), morphological information (Belinkov et al., 2017), and semantic knowledge and entailment (Peters et al., 2018b; Goodwin et al., 2020), and the representations studied include sentence embeddings (Adi et al., 2017; 72 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 72–81 November 10–11, 2021. ©2021 Association for Computational Linguistics which linguistic structures influence interpretation preference, both for humans and pretrained language models. We test telicity preference using an adverbial phrase task frequently used in semantics that is also well-aligned with the masking task used during mo"
2021.conll-1.6,N19-1004,0,0.0312362,"Missing"
2021.conll-1.6,W19-4828,0,0.0334059,"Missing"
2021.conll-1.6,P18-1198,0,0.0575475,"Missing"
2021.conll-1.6,2020.acl-main.177,0,0.0357727,"Missing"
2021.conll-1.6,N18-1108,0,0.0198505,"eads instead of supervised training (Clark et al., 2019; Kovaleva et al., 2019; Zhao and Bethard, 2020). Another approach to understanding pretrained language models is to test their behavior on psycholinguistic tasks. Stimuli in psycholinguistic tasks are typically designed to reveal linguistic bias in human behaviors (e.g., grammatical judgments, reading speeds, neural responses). Evaluating pretrained language models on such tasks can thus provide insights on the linguistic biases acquired by the models. The linguistic properties studied include subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018), filler-gap depedencies (Wilcox et al., 2018), garden-path effects (Futrell et al., 2019), other types of syntactic awareness (Marvin and Linzen, 2018; Hu et al., 2020), and variations of grammatical judgment based on availability of contexts (Lau et al., 2020). Most works focus on syntactic knowledge (though see Ettinger (2020)’s work on semantic and pragmatic aspect). Our work contributes to this line of research. While most previous studies probe syntactic properties, we investigate a semantic property: telicity (whether an event has reached an end point or not). Telicity encoding is cover"
2021.conll-1.6,D18-1151,0,0.0140865,"anguage models is to test their behavior on psycholinguistic tasks. Stimuli in psycholinguistic tasks are typically designed to reveal linguistic bias in human behaviors (e.g., grammatical judgments, reading speeds, neural responses). Evaluating pretrained language models on such tasks can thus provide insights on the linguistic biases acquired by the models. The linguistic properties studied include subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018), filler-gap depedencies (Wilcox et al., 2018), garden-path effects (Futrell et al., 2019), other types of syntactic awareness (Marvin and Linzen, 2018; Hu et al., 2020), and variations of grammatical judgment based on availability of contexts (Lau et al., 2020). Most works focus on syntactic knowledge (though see Ettinger (2020)’s work on semantic and pragmatic aspect). Our work contributes to this line of research. While most previous studies probe syntactic properties, we investigate a semantic property: telicity (whether an event has reached an end point or not). Telicity encoding is covert in English and needs to be inferred from information such as inherent telicity of the main verb, the argument structure, the quantity of noun phrases"
2021.conll-1.6,N19-1419,0,0.020583,"s often rely more heavily on temporal units than humans do. 1 Introduction Large pretrained-language models (ELMo: Peters et al., 2018a, BERT: Devlin et al., 2019, RoBERTa Liu et al., 2019, etc.) keep achieving new states of the art in a variety of NLP tasks, leading to a growing interest in exploring what has been acquired by the pretraining objectives. Many recent works utilize probes: shallow, usually supervised classifiers that try to determine which linguistic phenomena are predictable from the pretrained representations. The linguistic properties studied include syntactic relationships (Hewitt and Manning, 2019), morphological information (Belinkov et al., 2017), and semantic knowledge and entailment (Peters et al., 2018b; Goodwin et al., 2020), and the representations studied include sentence embeddings (Adi et al., 2017; 72 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 72–81 November 10–11, 2021. ©2021 Association for Computational Linguistics which linguistic structures influence interpretation preference, both for humans and pretrained language models. We test telicity preference using an adverbial phrase task frequently used in semantics that is als"
2021.conll-1.6,N18-1202,0,0.0224305,"and determine what factors influence them, we design an English test and a novel-word test that include a variety of linguistic cues (noun phrase quantity, resultative structure, contextual information, temporal units) that bias toward certain interpretations. We find that humans’ choice of telicity interpretation is reliably influenced by theoretically-motivated cues, transformer models (BERT and RoBERTa) are influenced by some (though not all) of the cues, and transformer models often rely more heavily on temporal units than humans do. 1 Introduction Large pretrained-language models (ELMo: Peters et al., 2018a, BERT: Devlin et al., 2019, RoBERTa Liu et al., 2019, etc.) keep achieving new states of the art in a variety of NLP tasks, leading to a growing interest in exploring what has been acquired by the pretraining objectives. Many recent works utilize probes: shallow, usually supervised classifiers that try to determine which linguistic phenomena are predictable from the pretrained representations. The linguistic properties studied include syntactic relationships (Hewitt and Manning, 2019), morphological information (Belinkov et al., 2017), and semantic knowledge and entailment (Peters et al., 20"
2021.conll-1.6,2020.acl-main.158,0,0.0136196,"their behavior on psycholinguistic tasks. Stimuli in psycholinguistic tasks are typically designed to reveal linguistic bias in human behaviors (e.g., grammatical judgments, reading speeds, neural responses). Evaluating pretrained language models on such tasks can thus provide insights on the linguistic biases acquired by the models. The linguistic properties studied include subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018), filler-gap depedencies (Wilcox et al., 2018), garden-path effects (Futrell et al., 2019), other types of syntactic awareness (Marvin and Linzen, 2018; Hu et al., 2020), and variations of grammatical judgment based on availability of contexts (Lau et al., 2020). Most works focus on syntactic knowledge (though see Ettinger (2020)’s work on semantic and pragmatic aspect). Our work contributes to this line of research. While most previous studies probe syntactic properties, we investigate a semantic property: telicity (whether an event has reached an end point or not). Telicity encoding is covert in English and needs to be inferred from information such as inherent telicity of the main verb, the argument structure, the quantity of noun phrases, and other consti"
2021.conll-1.6,D18-1179,0,0.0180579,"and determine what factors influence them, we design an English test and a novel-word test that include a variety of linguistic cues (noun phrase quantity, resultative structure, contextual information, temporal units) that bias toward certain interpretations. We find that humans’ choice of telicity interpretation is reliably influenced by theoretically-motivated cues, transformer models (BERT and RoBERTa) are influenced by some (though not all) of the cues, and transformer models often rely more heavily on temporal units than humans do. 1 Introduction Large pretrained-language models (ELMo: Peters et al., 2018a, BERT: Devlin et al., 2019, RoBERTa Liu et al., 2019, etc.) keep achieving new states of the art in a variety of NLP tasks, leading to a growing interest in exploring what has been acquired by the pretraining objectives. Many recent works utilize probes: shallow, usually supervised classifiers that try to determine which linguistic phenomena are predictable from the pretrained representations. The linguistic properties studied include syntactic relationships (Hewitt and Manning, 2019), morphological information (Belinkov et al., 2017), and semantic knowledge and entailment (Peters et al., 20"
2021.conll-1.6,S19-1026,0,0.0540851,"Missing"
2021.conll-1.6,J00-4004,0,0.183188,"The property also plays an important role in temporal inference. For example, when a punctual temporal adverbial phrase modifies an atelic event (e.g., ‘John ran at 8 am’), the timestamp is inferred as the inception time of the event, but when it modifies a telic event (e.g., ‘John arrived at 8 am’), the timestamp is inferred as the endpoint of the event. English encodes telicity with both overt grammatical aspect (e.g., progressives or perfectives) and covert situational (lexical) aspect. We focus on the situational aspect of telicity because it is considered the fundamental aspectual class (Siegel and McKeown, 2000) and its coding is non-transparent, i.e., it must be constructed by the human or model. 2.1 Time Adverbial Test One common way to probe telicity preference is to use time adverbial judgments as telicity of events select specific adverbial phrases (Dowty, 1986; Vendler, 1957; Rothstein, 2008). Specifically, readers are presented with sentences (e.g., “John read the book”) and asked whether they prefer ‘for’ or ‘in’ when adding a time adverbial (e.g., “John read the book for/in 20 minutes”). Here, we consider a preference for “for” as a preference for an atelic interpretation (e.g., John read th"
2021.conll-1.6,D19-1445,0,0.0609226,"Missing"
2021.conll-1.6,W18-5423,0,0.0177543,", 2019; Kovaleva et al., 2019; Zhao and Bethard, 2020). Another approach to understanding pretrained language models is to test their behavior on psycholinguistic tasks. Stimuli in psycholinguistic tasks are typically designed to reveal linguistic bias in human behaviors (e.g., grammatical judgments, reading speeds, neural responses). Evaluating pretrained language models on such tasks can thus provide insights on the linguistic biases acquired by the models. The linguistic properties studied include subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018), filler-gap depedencies (Wilcox et al., 2018), garden-path effects (Futrell et al., 2019), other types of syntactic awareness (Marvin and Linzen, 2018; Hu et al., 2020), and variations of grammatical judgment based on availability of contexts (Lau et al., 2020). Most works focus on syntactic knowledge (though see Ettinger (2020)’s work on semantic and pragmatic aspect). Our work contributes to this line of research. While most previous studies probe syntactic properties, we investigate a semantic property: telicity (whether an event has reached an end point or not). Telicity encoding is covert in English and needs to be inferred from inf"
2021.conll-1.6,2020.tacl-1.20,0,0.027639,"esigned to reveal linguistic bias in human behaviors (e.g., grammatical judgments, reading speeds, neural responses). Evaluating pretrained language models on such tasks can thus provide insights on the linguistic biases acquired by the models. The linguistic properties studied include subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018), filler-gap depedencies (Wilcox et al., 2018), garden-path effects (Futrell et al., 2019), other types of syntactic awareness (Marvin and Linzen, 2018; Hu et al., 2020), and variations of grammatical judgment based on availability of contexts (Lau et al., 2020). Most works focus on syntactic knowledge (though see Ettinger (2020)’s work on semantic and pragmatic aspect). Our work contributes to this line of research. While most previous studies probe syntactic properties, we investigate a semantic property: telicity (whether an event has reached an end point or not). Telicity encoding is covert in English and needs to be inferred from information such as inherent telicity of the main verb, the argument structure, the quantity of noun phrases, and other constituents (Dowty, 1986; Verkuyl, 2013). For instance, the description “John read the book” might"
2021.conll-1.6,2020.acl-main.429,1,0.855444,"Missing"
2021.conll-1.6,Q16-1037,0,0.0337821,"alysis of attention heads instead of supervised training (Clark et al., 2019; Kovaleva et al., 2019; Zhao and Bethard, 2020). Another approach to understanding pretrained language models is to test their behavior on psycholinguistic tasks. Stimuli in psycholinguistic tasks are typically designed to reveal linguistic bias in human behaviors (e.g., grammatical judgments, reading speeds, neural responses). Evaluating pretrained language models on such tasks can thus provide insights on the linguistic biases acquired by the models. The linguistic properties studied include subject-verb agreement (Linzen et al., 2016; Gulordava et al., 2018), filler-gap depedencies (Wilcox et al., 2018), garden-path effects (Futrell et al., 2019), other types of syntactic awareness (Marvin and Linzen, 2018; Hu et al., 2020), and variations of grammatical judgment based on availability of contexts (Lau et al., 2020). Most works focus on syntactic knowledge (though see Ettinger (2020)’s work on semantic and pragmatic aspect). Our work contributes to this line of research. While most previous studies probe syntactic properties, we investigate a semantic property: telicity (whether an event has reached an end point or not). T"
2021.law-1.11,W13-2322,0,0.0363222,"Missing"
2021.law-1.11,2020.lrec-1.497,0,0.0528326,"Missing"
2021.naacl-main.363,D19-5310,0,0.0151878,"rall execution flow of our approaches. Second, we show that the candidate QA system in Figure 2. The four key components evidence chain from WAIR assist reranker method of the system are explained below. to learn compositional and aggregative reasoning. 1. Initial evidence sentence retrieval: In the first Other recent works have proposed supervised step, we retrieve candidate evidence sentences (or iterative and multi-task approaches for evidence retrieval (Feldman and El-Yaniv, 2019; Qi et al., justification) given a query. We propose a simple unsupervised approach, which, however, has 2019; Banerjee, 2019). But, these supervised chain retrieval approaches are expensive in their run- been designed to bridge the “lexical chasm” inherent between multi-hop questions and their antime and do not scale well on large KB based QA swers (Berger et al., 2000). We call our algorithm datasets. On the contrary, our retrieval approach does not require any labeling data and is faster be- weighted alignment-based information retrieval (WAIR). WAIR operates in two steps, by combincause of its unsupervised nature. Further, our joint ing ideas from embedding based-alignment (Yadav approach is much simpler, perform"
2021.naacl-main.363,C10-2007,0,0.0531478,"Missing"
2021.naacl-main.363,W19-4828,0,0.0511309,"Missing"
2021.naacl-main.363,N19-1423,0,0.0157628,"ticularly focused on a) evaluating attention scores on linking terms that approximate multi-hop compositionality and, b) complementary knowledge aggregation necessary for multi-hop QA. Importance of Evidence Retrieval for Question Answering Several neural QA methods have achieved high performance without relying on evidence texts. Many of these approaches utilize external labeled training data (Raffel et al., 2019; Pan et al., 2019), which limits their portability to other domains. Others rely on pretraining, which tends to be computationally expensive but can be used as starting checkpoints (Devlin et al., 2019; Liu et al., 2019). More importantly, many of these directions lack explanation of their selected answers to the end user. In contrast, QA methods that incorporate an evidence retrieval module can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Chen et al., 2017; Qi et al., 2019). Our work is inspired by these directions but mostly focuses on jointly retrieving+reranking clusters of evidence sentences that leads to substantial"
2021.naacl-main.363,D19-1006,0,0.081497,"retrieval approach does not require any labeling data and is faster be- weighted alignment-based information retrieval (WAIR). WAIR operates in two steps, by combincause of its unsupervised nature. Further, our joint ing ideas from embedding based-alignment (Yadav approach is much simpler, performs well and scales et al., 2019a) and pseudo-relevance feedback (Bernon large KB based QA such as QASC. In this work, we focus on analyzing the multi- hard, 2010) approaches. hop evidence reasoning via attention (Clark et al., In its first step, WAIR uses a query that con2019) and learned embeddings (Ethayarajh, 2019) sists of the non-stop words of the original ques4573 g Joi Sent 0 - RNA is a small molecule that can squeeze ..… membrane Sent 1 - RNA synthesis in eukaryotic cells …. of evidence sentences {Sent 0,Sent 1} {Sent 0,Sent 2} . .. {Sent 8,Sent 9} K=2 {Sent 0, Sent 1, Sent 2} {Sent 0, Sent 1, Sent 3} .... .... {Sent 7,Sent 8, Sent 9} K=3 RoBERTa reranker Answer Selector QASC (MCQA) MultiRC (Classification) Dataset WAIR BM25 Alignment QASC (top 2) MultiRC (top 3) 78.85 55.92 61.42 39.86 63.40 52.98 Gold Evidence 80.81 63.95 Table 1: The coverage of question (+candidate answer) terms in the sentence"
2021.naacl-main.363,P19-1222,0,0.0191434,"oposed Approach stantially improved resulting in state-of-the-art performance and thus outperforming all the previous We summarize the overall execution flow of our approaches. Second, we show that the candidate QA system in Figure 2. The four key components evidence chain from WAIR assist reranker method of the system are explained below. to learn compositional and aggregative reasoning. 1. Initial evidence sentence retrieval: In the first Other recent works have proposed supervised step, we retrieve candidate evidence sentences (or iterative and multi-task approaches for evidence retrieval (Feldman and El-Yaniv, 2019; Qi et al., justification) given a query. We propose a simple unsupervised approach, which, however, has 2019; Banerjee, 2019). But, these supervised chain retrieval approaches are expensive in their run- been designed to bridge the “lexical chasm” inherent between multi-hop questions and their antime and do not scale well on large KB based QA swers (Berger et al., 2000). We call our algorithm datasets. On the contrary, our retrieval approach does not require any labeling data and is faster be- weighted alignment-based information retrieval (WAIR). WAIR operates in two steps, by combincause o"
2021.naacl-main.363,D19-1107,0,0.0267738,"Missing"
2021.naacl-main.363,P17-1147,0,0.0315705,"sed information retrieval techniques, e.g., BM25 (Robertson et al., 2009), tf-idf (Ramos et al., 2003; Manning et al., 2008), or alignment-based methods (Kim et al., 2017), have (1) We introduce a simple, unsupervised and fast evidence retrieval approach -WAIR for multi-hop QA that generates complete and associated candidate evidence chains. To show the multi-hop rea1 soning approximated within WAIR candidate eviCodes - https://github.com/vikas95/WAIR_ dence chains, We present several attention weights interpretability 4572 been widely used to retrieve evidence texts for open-domain QA tasks (Joshi et al., 2017; Dunn et al., 2017). Although these approaches have been strong benchmarks for decades, they usually do not perform well on recent complex reasoning-based QA tasks (Yang et al., 2018; Khot et al., 2019a). More recently, supervised neural network (NN) based retrieval methods have achieved strong results on complex questions (Karpukhin et al., 2020; Nie et al., 2019; Tu et al., 2019). However, these approaches require annotated data for initial retrieval and suffer from the same disadvantages at the reranking stage as the other methods that retrieve+rerank individual evidence sentences, i.e., t"
2021.naacl-main.363,N18-1023,0,0.105211,"g et al., 2019; Yang et al., 2018). Our work is also focused on improv1 Introduction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4571–4581 June 6–11, 2021. ©2021 Association for Computational Linguistics facts within the QA pipeline. The motivation behind our work is s"
2021.naacl-main.363,D19-1281,0,0.102092,"ainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4571–4581 June 6–11, 2021. ©2021 Association for Computational Linguistics facts within the QA pipeline. The motivation behind our work is simple: jointly handling evidence sentences gives access to the complete information together"
2021.naacl-main.363,2020.emnlp-main.574,0,0.031376,"Missing"
2021.naacl-main.363,N18-2017,0,0.058,"Missing"
2021.naacl-main.363,2021.ccl-1.108,0,0.04565,"Missing"
2021.naacl-main.363,W18-1703,0,0.0637151,"Missing"
2021.naacl-main.363,D17-1215,0,0.0141551,"important for faithfulness and explainability of neural QA methods (DeYoung et al., 2019; Yang et al., 2018). Our work is also focused on improv1 Introduction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4571–4581 June 6–11, 2021. ©2021 Association for Computational L"
2021.naacl-main.363,D19-1258,0,0.0523276,"Missing"
2021.naacl-main.363,D19-5804,0,0.0162984,"shown attention based analysis on pretrained transformer language models (Rogers et al., 2020) on various NLP tasks including QA (van Aken et al., 2019). Our novel analyses are particularly focused on a) evaluating attention scores on linking terms that approximate multi-hop compositionality and, b) complementary knowledge aggregation necessary for multi-hop QA. Importance of Evidence Retrieval for Question Answering Several neural QA methods have achieved high performance without relying on evidence texts. Many of these approaches utilize external labeled training data (Raffel et al., 2019; Pan et al., 2019), which limits their portability to other domains. Others rely on pretraining, which tends to be computationally expensive but can be used as starting checkpoints (Devlin et al., 2019; Liu et al., 2019). More importantly, many of these directions lack explanation of their selected answers to the end user. In contrast, QA methods that incorporate an evidence retrieval module can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Ch"
2021.naacl-main.363,D19-1261,0,0.0153823,"eir portability to other domains. Others rely on pretraining, which tends to be computationally expensive but can be used as starting checkpoints (Devlin et al., 2019; Liu et al., 2019). More importantly, many of these directions lack explanation of their selected answers to the end user. In contrast, QA methods that incorporate an evidence retrieval module can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Chen et al., 2017; Qi et al., 2019). Our work is inspired by these directions but mostly focuses on jointly retrieving+reranking clusters of evidence sentences that leads to substantial QA performance improvements. Jointly retrieving evidence sentences: Recently, several works have proposed retrieval of evidence chains that has led to stronger evidence retrieval performance (Yadav et al., 2019b; Khot et al., 2019a). Our WAIR approach aligns in the same direction and particularly utilizes coverage and associativity that leads to higher performance. Importantly, our work focuses on highlighting the benefits of feeding evidence ch"
2021.naacl-main.363,2020.tacl-1.54,0,0.0742247,"ese approaches require annotated data for initial retrieval and suffer from the same disadvantages at the reranking stage as the other methods that retrieve+rerank individual evidence sentences, i.e., the retrieval algorithm is not aware of what information has already been retrieved and what is missing, or how individual facts need to be combined for explaining the multi-hop reasoning (Khot et al., 2019b). Our proposed joint retrieval and reranking approach mitigates both these limitations. analyses. Several works have shown attention based analysis on pretrained transformer language models (Rogers et al., 2020) on various NLP tasks including QA (van Aken et al., 2019). Our novel analyses are particularly focused on a) evaluating attention scores on linking terms that approximate multi-hop compositionality and, b) complementary knowledge aggregation necessary for multi-hop QA. Importance of Evidence Retrieval for Question Answering Several neural QA methods have achieved high performance without relying on evidence texts. Many of these approaches utilize external labeled training data (Raffel et al., 2019; Pan et al., 2019), which limits their portability to other domains. Others rely on pretraining,"
2021.naacl-main.363,K19-1065,0,0.300058,"ch other. Both the gold evidence are also found in sentences from step-1 and step2. (Geva et al., 2019). Thus, justifying the underlying knowledge or evidence text has been deemed very important for faithfulness and explainability of neural QA methods (DeYoung et al., 2019; Yang et al., 2018). Our work is also focused on improv1 Introduction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Con"
2021.naacl-main.363,Q18-1021,0,0.0243297,"duction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et al., 2019). Specifically, black-box neural Welbl et al., 2018; Khot et al., 2019a). For such QA methods have shown to rely on spurious signals compositional aggregation, we emphasize on the confirming unfaithful or non-explainable behavior importance of jointly handeling the set of evidence 4571 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4571–4581 June 6–11, 2021. ©2021 Association for Computational Linguistics facts within the QA pipeline. The motivation behind our work is simple: jointly handling evidence sentences gives access to the complete in"
2021.naacl-main.363,N19-1274,1,0.913614,"can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Chen et al., 2017; Qi et al., 2019). Our work is inspired by these directions but mostly focuses on jointly retrieving+reranking clusters of evidence sentences that leads to substantial QA performance improvements. Jointly retrieving evidence sentences: Recently, several works have proposed retrieval of evidence chains that has led to stronger evidence retrieval performance (Yadav et al., 2019b; Khot et al., 2019a). Our WAIR approach aligns in the same direction and particularly utilizes coverage and associativity that leads to higher performance. Importantly, our work focuses on highlighting the benefits of feeding evidence chains to transformer based reranking methods. First, the evidence retrieval performance of the same reranker is sub- 3 Proposed Approach stantially improved resulting in state-of-the-art performance and thus outperforming all the previous We summarize the overall execution flow of our approaches. Second, we show that the candidate QA system in Figure 2. The fo"
2021.naacl-main.363,N19-1270,0,0.0505889,"Missing"
2021.naacl-main.363,D19-1260,1,0.896253,"can provide these evidence texts as human-readable explanations. Further, several works have demonstrated that retrieve and read approaches (similar to ours) tend to achieve higher performance than the former QA methods (Chen et al., 2017; Qi et al., 2019). Our work is inspired by these directions but mostly focuses on jointly retrieving+reranking clusters of evidence sentences that leads to substantial QA performance improvements. Jointly retrieving evidence sentences: Recently, several works have proposed retrieval of evidence chains that has led to stronger evidence retrieval performance (Yadav et al., 2019b; Khot et al., 2019a). Our WAIR approach aligns in the same direction and particularly utilizes coverage and associativity that leads to higher performance. Importantly, our work focuses on highlighting the benefits of feeding evidence chains to transformer based reranking methods. First, the evidence retrieval performance of the same reranker is sub- 3 Proposed Approach stantially improved resulting in state-of-the-art performance and thus outperforming all the previous We summarize the overall execution flow of our approaches. Second, we show that the candidate QA system in Figure 2. The fo"
2021.naacl-main.363,N19-1302,0,0.0242364,"Missing"
2021.naacl-main.363,2020.acl-main.414,1,0.823917,"Missing"
2021.naacl-main.363,D18-1259,0,0.112588,"ear membrane are called eukaryotic 2. Eukaryotic cells have three different RNA polymerases. Figure 1: An example question from the QASC dataset with evidece sentences retrieved by BM25 and two steps of WAIR. The evidence retrieved in step-2 of WAIR contain information missed by sentences in step1 and are associated with each other. Both the gold evidence are also found in sentences from step-1 and step2. (Geva et al., 2019). Thus, justifying the underlying knowledge or evidence text has been deemed very important for faithfulness and explainability of neural QA methods (DeYoung et al., 2019; Yang et al., 2018). Our work is also focused on improv1 Introduction ing the explainability of QA methods by the means of evidence (or justification) sentence retrieval. Recent advances in question answering (QA) have achieved excellent performance on several benchEvidence retrieval for multi-hop QA is a chalmark datasets (Wang et al., 2019a), even when rely- lenging task as it requires compositional infering on partial (Gururangan et al., 2018), incorrect ence based aggregation of multiple evidence sen(Jia and Liang, 2017) or no supporting knowledge tences (Yang et al., 2018; Khashabi et al., 2018; (Raffel et"
2021.naacl-main.97,N19-1423,0,0.0124938,"ally suffer from the compositionality generalization problem, meaning that they tend to fail when the number of reasoning steps are much larger in the evaluation set than in the training set (Hupkes et al., 2020; Hahn, 2020; Clark et al., 2020). Newell (1994) categorized cognitive processes based on their time scales: unconscious activities take around 50 ms, whereas conscious actions can vary from 100 ms to hours. Importantly, Newell 1 Introduction (1994) argued that conscious actions are sequences of simple conscious/unconscious actions. ExtrapoLarge pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., lating from cognitive science to natural language processing (NLP), in this paper we ask the question: 2019) have been successfully used in multi-hop can we design an interpretable multi-hop reasoning reasoning problems (Banerjee et al., 2020; Asai et al., 2019; Yadav et al., 2019). Usually, these pre- system that sequentially applies neural networks trained on simpler tasks? Further, motivated by the trained language models solve multi-hop reasoning problems in a discriminative end-to-end manner: finding from cognitive science that people might use internal monologue"
2021.naacl-main.97,2020.acl-main.495,0,0.0198096,"015), Dynamic Memory Networks (DMN) (Kumar et al., 2016) and Compositional Attention Networks (MAC) (Hudson and Manning, 2018). Another direction are the neural modular networks, where what components to use are determined dynamically for each question (Gupta et al., 2019; Jiang and Bansal, 2019). However, it is hard to prove the components are actually fulfilling the designed functionality after training due to the distributed nature of the intermediate representations. In contrast, we explicitly evaluate the performance of each component of EVR after training, achieving better faithfulness (Subramanian et al., 2020). Formal Theorem Prover: Neural components have been used to augment formal theorem proving in several ways. Polu and Sutskever (2020) apply a Seq2Seq neural network for mathematical theorem proving by training the neural network to generate the proof at each step. Some works seek to use distributed representations to augment the rule-based backward chaining (Weber et al., 2019; Dong et al., 2018). However, these works still highly rely on the formal representations and they do not generate the natural language subgoals at each step. Problem Solver and Cognitive Architectures: Our work is also"
2021.naacl-main.97,P19-1618,0,0.0283695,"nality after training due to the distributed nature of the intermediate representations. In contrast, we explicitly evaluate the performance of each component of EVR after training, achieving better faithfulness (Subramanian et al., 2020). Formal Theorem Prover: Neural components have been used to augment formal theorem proving in several ways. Polu and Sutskever (2020) apply a Seq2Seq neural network for mathematical theorem proving by training the neural network to generate the proof at each step. Some works seek to use distributed representations to augment the rule-based backward chaining (Weber et al., 2019; Dong et al., 2018). However, these works still highly rely on the formal representations and they do not generate the natural language subgoals at each step. Problem Solver and Cognitive Architectures: Our work is also largely inspired by cognitive arTables 7 and 8 show that EVR1 yields stable perfor- chitectures such as ACT-R (Anderson et al., 1997) mance and proofs when trained with considerably and SOAR (Laird, 2012), which originate from less data (10k and 30k examples). In the lowest Newell’s GPS. These cognitive architectures emdata configuration (10k), EVR outperforms PR con- ploy sym"
2021.naacl-main.97,2020.tacl-1.13,0,0.0243725,"ernal monologue in problem solving/reasoning is mixed. Studies have been shown that internal monologue might not be crucial to visual reasoning (Phillips, 1999), whereas in verbal reasoning tasks, some subjects indeed rely more on the internal monologue than the visual imagery (Bacon et al., 2003). However, there is still not a wide concensus on the form/grammar of the internal monologue. Question Decomposition: Our work is also different from several existing works about question decomposition, where the strategy of decomposition is largely reflected by the question itself (Min et al., 2019; Wolfson et al., 2020). In contrast, the expressions of our questions are already simple, and don’t reflect the decomposition strategies. 6 6.1 Discussion and Future Work Discussion of the Current Method Does EVR solve the problems raised in Section 2.4? We believe our neural GPS at least partially solves the issues mentioned in Section 2.4. First, EVR decomposes a hard problem into several simple ones, thus resembling the human thinking process more. In addition, this modular strategy also enables EVR to suffer less from the compositionality generalization problem (as shown in Section 4). Second, during each step"
2021.naacl-main.97,D19-1260,1,0.834229,"me scales: unconscious activities take around 50 ms, whereas conscious actions can vary from 100 ms to hours. Importantly, Newell 1 Introduction (1994) argued that conscious actions are sequences of simple conscious/unconscious actions. ExtrapoLarge pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., lating from cognitive science to natural language processing (NLP), in this paper we ask the question: 2019) have been successfully used in multi-hop can we design an interpretable multi-hop reasoning reasoning problems (Banerjee et al., 2020; Asai et al., 2019; Yadav et al., 2019). Usually, these pre- system that sequentially applies neural networks trained on simpler tasks? Further, motivated by the trained language models solve multi-hop reasoning problems in a discriminative end-to-end manner: finding from cognitive science that people might use internal monologues to guide their reasoning, these models take the question and all the relevant we want to explore whether it is possible to use evidence as the input, and produce the final answer to the question. This raises two problems. First, natural language to guide this sequential process. In this paper, we propose"
2021.semeval-1.42,L16-1599,1,0.752609,"e train set is never distributed to the participants. data, we used the i2b2 2010 Challenge Dataset, a de-identified dataset of notes from Partners HealthCare. The evaluation dataset for this task consisted of de-identified intensive care unit progress notes from the MIMIC III corpus (Johnson et al., 2016). Time expression recognition has been a key component of previous temporal language related competitions, like TempEval 2010 (Pustejovsky and Verhagen, 2009) and TempEval 2013 (UzZaman et al., 2013). For this task, we followed the Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016) used in in SemEval 2018 Task 6 (Laparra et al., 2018). As in negation detection, previous works have also oberved a significant performance degradation on domain shift (Xu et al., 2019). For time expression recognition, we provided a sequence tagging model, fine-tuned on deidentified clinical notes from the Mayo Clinic, which were available to the task organizers, but are difficult to gain access to due to the complex data use agreements necessary. (Models were approved to be distributed, as the data is deidentified.) The development data was the annotated news portion of the SemEval 2018 Tas"
2021.semeval-1.42,P07-1033,0,0.462609,"Missing"
2021.semeval-1.42,N19-1423,0,0.0139312,"y, generate 5 additional training examples using 5 new words with same entity type; 4) train the model on the resulting dataset. The same method was used by UArizona-2, but, in this case, they fixed some errors in the manual annotations. KISNLP-1 and KISNLP-2 used the development labeled data as a fine-tuning resource, which was complemented by a data augmentation process. They did not use the unlabeled test data, nor any other resource. YNU-HPCC-1 and YNU-HPCC-2 also used the labeled portion of the development set. They finetuned 4 popular transformer-based pre-trained models: RoBERTa, BERT (Devlin et al., 2019), DistilBERt (Sanh et al., 2020) and ALBERT (Lan et al., 2020). The final prediction was given by hard voting strategy, integrating the results of the 4 models along with Source-Trained. Observations: Self-learning (5 submissions) and data augmentation (4 submissions) were the most commonly followed approaches. 2 submissions extended a self-learning technique with manually created heuristics. Only 3 submissions proposed ensemble methods. In this task, the development set was more frequently exploited and 4 submissions made use of the labeled data to continue fine-tuning the provided model. The"
2021.semeval-1.42,C16-1038,0,0.0297124,"velop an accurate system for a target domain when annotations exist for a related domain but cannot be distributed. Instead of annotated training data, participants are given a model trained on the annotations. Then, given unlabeled target domain data, they are asked to make predictions. This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016; Ziser and Reichart, 2017; Saito et al., 2017; Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum´e III, 2007; Xia et al., 2013; Kim et al., 2016; Peng and Dredze, 2017). Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition. These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem. Both of these tasks have previously been run as shared tasks, and had at least two different domains of data available, and we had access to experienced annotators for both tasks, allowi"
2021.semeval-1.42,2021.ccl-1.108,0,0.0322749,"Missing"
2021.semeval-1.42,W17-2612,0,0.0242437,"system for a target domain when annotations exist for a related domain but cannot be distributed. Instead of annotated training data, participants are given a model trained on the annotations. Then, given unlabeled target domain data, they are asked to make predictions. This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016; Ziser and Reichart, 2017; Saito et al., 2017; Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum´e III, 2007; Xia et al., 2013; Kim et al., 2016; Peng and Dredze, 2017). Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition. These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem. Both of these tasks have previously been run as shared tasks, and had at least two different domains of data available, and we had access to experienced annotators for both tasks, allowing us to annotate data i"
2021.semeval-1.42,W09-2418,0,0.126969,"Missing"
2021.semeval-1.42,P18-1096,0,0.0266295,"cipants to develop semantic annotation systems in the face of data sharing constraints. A participant’s goal is to develop an accurate system for a target domain when annotations exist for a related domain but cannot be distributed. Instead of annotated training data, participants are given a model trained on the annotations. Then, given unlabeled target domain data, they are asked to make predictions. This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016; Ziser and Reichart, 2017; Saito et al., 2017; Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum´e III, 2007; Xia et al., 2013; Kim et al., 2016; Peng and Dredze, 2017). Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition. These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem. Both of these tasks have previously been run as shared tasks, and had at le"
2021.semeval-1.42,S13-2001,0,0.0356015,"cuments Time entities 278 99 47 17 18,020 2,231 1,900 Table 2: Size of the time expression recognition datasets. The train set is never distributed to the participants. data, we used the i2b2 2010 Challenge Dataset, a de-identified dataset of notes from Partners HealthCare. The evaluation dataset for this task consisted of de-identified intensive care unit progress notes from the MIMIC III corpus (Johnson et al., 2016). Time expression recognition has been a key component of previous temporal language related competitions, like TempEval 2010 (Pustejovsky and Verhagen, 2009) and TempEval 2013 (UzZaman et al., 2013). For this task, we followed the Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016) used in in SemEval 2018 Task 6 (Laparra et al., 2018). As in negation detection, previous works have also oberved a significant performance degradation on domain shift (Xu et al., 2019). For time expression recognition, we provided a sequence tagging model, fine-tuned on deidentified clinical notes from the Mayo Clinic, which were available to the task organizers, but are difficult to gain access to due to the complex data use agreements necessary. (Models were approved to be"
2021.semeval-1.42,S19-1008,1,0.844233,"k consisted of de-identified intensive care unit progress notes from the MIMIC III corpus (Johnson et al., 2016). Time expression recognition has been a key component of previous temporal language related competitions, like TempEval 2010 (Pustejovsky and Verhagen, 2009) and TempEval 2013 (UzZaman et al., 2013). For this task, we followed the Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016) used in in SemEval 2018 Task 6 (Laparra et al., 2018). As in negation detection, previous works have also oberved a significant performance degradation on domain shift (Xu et al., 2019). For time expression recognition, we provided a sequence tagging model, fine-tuned on deidentified clinical notes from the Mayo Clinic, which were available to the task organizers, but are difficult to gain access to due to the complex data use agreements necessary. (Models were approved to be distributed, as the data is deidentified.) The development data was the annotated news portion of the SemEval 2018 Task 6 data whose source text is from the freely available TimeBank. For evaluation, we used a set of annotated documents extracted from food security warning systems. The main impact of th"
2021.semeval-1.42,K17-1040,0,0.0221133,"task presents a new framework that asks participants to develop semantic annotation systems in the face of data sharing constraints. A participant’s goal is to develop an accurate system for a target domain when annotations exist for a related domain but cannot be distributed. Instead of annotated training data, participants are given a model trained on the annotations. Then, given unlabeled target domain data, they are asked to make predictions. This is a challenging setting, and much previous work on domain adaptation does not apply, as it assumes access to source data (Ganin et al., 2016; Ziser and Reichart, 2017; Saito et al., 2017; Ruder and Plank, 2018), or assumes that labeled target domain data is available (Daum´e III, 2007; Xia et al., 2013; Kim et al., 2016; Peng and Dredze, 2017). Two different semantic tasks in English were created to explore this framework: negation detection and time expression recognition. These represent two common types of classification tasks: negation detection is typically formulated as predicting an attribute of a word or span given its context, and time expression recognition is typically formulated as a named entity tagging problem. Both of these tasks have previo"
2021.semeval-1.56,L16-1599,1,0.813674,"ntext. This is a binary sentence classification task. For example, given the event diarrhea and the sentence Has no diarrhea and no new lumps or masses, the goal is to predict that diarrhea is negated by its context. The goal of time expression recognition sub-task (Laparra et al., 2018) is to recognize time expressions in the target domain. This is a named entity recognition (NER) task. The number of entity types (inside–outside–beginning format) is 65. Entity types in this task are formally defined time entity types from the Semantically Compositional Annotation of Time Expressions (SCATE) (Bethard and Parker, 2016) annotation schema. For example, in 2021-02-19, 2021 will be labeled as Year, 02 will be labeled as Month-Of-Year and 19 will be labeled as Day-Of-Month. We investigate self-training, active learning, and data augmentation techniques on negation detection and time expression recognition under the SFDA setting. Our contributions are: 1. We demonstrate that simple self-training over a small portion of the target domain data can effectively improve the performance of the negation detection model. 2. We demonstrate that active learning with data augmentation can significantly improve time expressi"
2021.semeval-1.56,2021.semeval-1.42,1,0.80112,"Missing"
2021.semeval-1.56,2021.ccl-1.108,0,0.0239287,"Missing"
2021.semeval-1.56,P95-1026,0,0.0948987,"n is whether the target event is negated and the model output for time expression detection is the labels for each input tokens. 1. We select the k instances where M is most uncertain, manually annotate them, and add them to L. (Details in section 2.2.1.) 2. We augment each manually annotated instance with n new examples and add them to L. (Details in section 2.2.2.) 3. We re-initialize M to M0 and fine-tune on L. We repeat this process i times. Note that the training set L is built cumulatively, and M is reinitialized on each iteration. 2.2.1 Active Learning We employ a simple self-training (Yarowsky, 1995) approach that fine-tunes the model with its own predictions on the unlabeled dataset. We start with the pre-trained source-domain model, M . Then, for each self-training iteration: 1. We initialize an empty training set, L. 2. We use M to label the target domain data. 3. If an instance is labeled with a probability above a threshold τ , we add it to L with the predicted label as its pseudo label. 4. We fine-tune M on L. When the source-domain model predictions are the same for two consecutive iterations or the number of iterations of self-training is greater than the predefined maximum number"
2021.wnut-1.36,W10-3004,0,0.0386795,"ords, which Wikipedia defines as “words or phrases aimed at creating an impression that something specific and meaningful has been said, when in fact only a vague or ambiguous claim has been communicated” (Wikipedia, 2021c), and hedges, which Farkas et al. (2010) define as phrases “indicating that authors do not or cannot back up their opinions/statements with facts. The best models for detecting weasel words in both the CoNLL-2010 ACL shared task on weasel words (Farkas et al., 2010), and a multilingual weasel word corpus (Aleksandrova et al., 2019) use bag-ofwords classification approaches (Georgescul, 2010; Aleksandrova et al., 2019). Submissions to the yearly Wiki Workshop at The Web Conference have also considered identifying all types of bias with a single model, using approaches including combining user metadata and machine learning models. Recently, Hube and Fetahu (2018) achieved an f1 score of 0.69 using a generated bias word list and other hand-picked features, including part of speech tags and a context window around each bias word. Figure 1: Both types of puffery warnings: an articlelevel warning (top) and a sentence-level warning (bottom). 3 Data Collection Puffery in Wikipedia is no"
2021.wnut-1.36,2021.ccl-1.108,0,0.0799706,"Missing"
2021.wnut-1.36,R19-1006,0,0.0185303,"rior research on identifying problematic edits has considered weasel words, which Wikipedia defines as “words or phrases aimed at creating an impression that something specific and meaningful has been said, when in fact only a vague or ambiguous claim has been communicated” (Wikipedia, 2021c), and hedges, which Farkas et al. (2010) define as phrases “indicating that authors do not or cannot back up their opinions/statements with facts. The best models for detecting weasel words in both the CoNLL-2010 ACL shared task on weasel words (Farkas et al., 2010), and a multilingual weasel word corpus (Aleksandrova et al., 2019) use bag-ofwords classification approaches (Georgescul, 2010; Aleksandrova et al., 2019). Submissions to the yearly Wiki Workshop at The Web Conference have also considered identifying all types of bias with a single model, using approaches including combining user metadata and machine learning models. Recently, Hube and Fetahu (2018) achieved an f1 score of 0.69 using a generated bias word list and other hand-picked features, including part of speech tags and a context window around each bias word. Figure 1: Both types of puffery warnings: an articlelevel warning (top) and a sentence-level wa"
2021.wnut-1.36,W10-3001,0,0.0105648,"unintentionally or unknowingly. Senior editors and volunteer administrators spend significant amounts of time welcoming new editors and reverting these bad edits, but many bad edits on low-traffic pages are still left up for months, years, or, in the case of some pages, over a decade. Prior research on identifying problematic edits has considered weasel words, which Wikipedia defines as “words or phrases aimed at creating an impression that something specific and meaningful has been said, when in fact only a vague or ambiguous claim has been communicated” (Wikipedia, 2021c), and hedges, which Farkas et al. (2010) define as phrases “indicating that authors do not or cannot back up their opinions/statements with facts. The best models for detecting weasel words in both the CoNLL-2010 ACL shared task on weasel words (Farkas et al., 2010), and a multilingual weasel word corpus (Aleksandrova et al., 2019) use bag-ofwords classification approaches (Georgescul, 2010; Aleksandrova et al., 2019). Submissions to the yearly Wiki Workshop at The Web Conference have also considered identifying all types of bias with a single model, using approaches including combining user metadata and machine learning models. Rec"
bethard-etal-2008-building,kingsbury-palmer-2002-treebank,0,\N,Missing
bethard-etal-2008-building,J93-2004,0,\N,Missing
bethard-etal-2008-building,W04-2703,0,\N,Missing
bethard-etal-2008-building,S07-1014,0,\N,Missing
bethard-etal-2008-building,W06-1618,1,\N,Missing
bethard-etal-2008-building,S07-1025,1,\N,Missing
bethard-etal-2008-building,S07-1085,0,\N,Missing
bethard-etal-2008-building,S07-1108,0,\N,Missing
bethard-etal-2008-building,P06-1095,0,\N,Missing
bethard-etal-2008-building,W03-1210,0,\N,Missing
bethard-etal-2008-building,P00-1043,0,\N,Missing
bethard-etal-2008-building,S07-1003,0,\N,Missing
bethard-etal-2012-annotating,S07-1014,0,\N,Missing
bethard-etal-2012-annotating,P12-1010,1,\N,Missing
bethard-etal-2012-annotating,S10-1010,0,\N,Missing
bethard-etal-2012-annotating,J11-4004,0,\N,Missing
bethard-etal-2012-annotating,P09-1025,0,\N,Missing
bethard-etal-2012-annotating,W11-0419,0,\N,Missing
bethard-etal-2014-cleartk,S13-2002,1,\N,Missing
bethard-etal-2014-cleartk,S13-2101,1,\N,Missing
C14-1116,P11-1030,1,0.860458,"Missing"
C14-1116,E09-1039,0,0.300969,"styles of text. Obtaining such corpora is a challenging task since most authorship attribution studies focus on a single domain. We have found two datasets that meet our criteria, one having both cross-topic and cross-genre flavor, and the other having only cross-topic flavor. The first corpus contains communication samples from 21 authors in six genres (Email, Essay, Blog, Chat, Phone Interview, and Discussion) on six topics (Catholic Church, Gay Marriage, War in Iraq, Legalization of Marijuana, Privacy Rights, and Sex Discrimination), which we call dataset 1. This dataset was obtained from Goldstein-Stewart et al. (2009). Using this dataset, it is possible to see how the performance of cross-topic AA changes across different genres. Another corpus is composed of texts published in The Guardian daily newspaper written by 13 authors in one genre on four topics (dataset 2) due Stamatatos et al. (2013). It contains opinion articles (comments) about World, U.K., Culture, and Politics. Table 1 shows some statistics about the datasets. Corpus #authors #genres #topics Dataset 1 Dataset 2 21 13 6 1 6 4 avg #docs/author 36 64 avg #sentences/doc 31.7 53 avg #words/doc 600 1034 Table 1: Some statistics about dataset 1 an"
C16-1121,P98-1013,0,0.229651,"its arguments in a given sentence. Intuitively, it aims at answering the questions of “Who did What to Whom, and How, When and Where?” in text. For example, the processing of the sentence “He bought tons of roses yesterday” should result in the identification of a “buying” event corresponding to the predicate “bought” with three arguments including “he” as the Agent (A0), “tons of roses” as the Thing being bought (A1), and “yesterday” as the Time (AM-TMP) arguments. Traditional SRL systems have concentrated on supervised learning from several manually-built semantic corpora, (e.g., FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005)). One important limitation of supervised approaches is that they depend heavily on the accuracy, coverage and labeling scheme of the labeled corpus. When the training and the testing data are in different domains, the linguistic patterns and their distributions in the testing domain are different from the ones observed in the training data, resulting in a considerable performance drop. Developing more manually-built semantic corpora is expensive and requires huge human efforts. Thus, exploiting large unlabeled datasets by semi-supervised or unsupervised appr"
C16-1121,C10-3009,0,0.0456503,"Missing"
C16-1121,D15-1112,0,0.0236176,"Missing"
C16-1121,J12-1005,0,0.0294812,"Missing"
C16-1121,N15-1121,0,0.0288347,"Missing"
C16-1121,W04-2405,0,0.103629,"Missing"
C16-1121,J05-1004,0,0.0289786,". Intuitively, it aims at answering the questions of “Who did What to Whom, and How, When and Where?” in text. For example, the processing of the sentence “He bought tons of roses yesterday” should result in the identification of a “buying” event corresponding to the predicate “bought” with three arguments including “he” as the Agent (A0), “tons of roses” as the Thing being bought (A1), and “yesterday” as the Time (AM-TMP) arguments. Traditional SRL systems have concentrated on supervised learning from several manually-built semantic corpora, (e.g., FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005)). One important limitation of supervised approaches is that they depend heavily on the accuracy, coverage and labeling scheme of the labeled corpus. When the training and the testing data are in different domains, the linguistic patterns and their distributions in the testing domain are different from the ones observed in the training data, resulting in a considerable performance drop. Developing more manually-built semantic corpora is expensive and requires huge human efforts. Thus, exploiting large unlabeled datasets by semi-supervised or unsupervised approaches is a promising solution. Our"
C16-1121,J08-2005,0,0.0188295,"step. It is common among the state-of-the-art systems to train a global reranker on top of the local classifiers to improve performance (Toutanova et al., 2005; Bj¨orkelund et al., 2010; Roth and Lapata, 2016). SRL models have also been trained using graphical models (T¨ackstr¨om et al., 2015) and neural networks (Collobert et al., 2011; FitzGerald et al., 2015). Some systems have applied a set of structural constraints to the argument classification sub-task, such as avoiding overlapping arguments and repeated core roles, and enforced these constraints with integer linear programming (ILP) (Punyakanok et al., 2008) or a dynamic program (T¨ackstr¨om et al., 2015). Regarding leveraging unlabeled data, semi-supervised methods have been proposed to reduce human annotation efforts. He and Gildea (2006) investigate the possibility of a weakly supervised approach by using self-training and co-training for unseen frames of SRL. They separate the headword and path as the two views for co-training, but could not show a clear performance improvement. The sources of the problem appeared to be the big gap in performance between the headword and path feature sets and the complexity of the task. Some other works show"
C16-1121,P16-1113,0,0.0724166,"eriment is presented in Section 5 and Section 6 and finally we conclude in Section 7. 2 Related Work In traditional supervised approaches, SRL is modeled as a pipeline of predicate identification, predicate disambiguation, argument identification, and argument classification steps. Hand-engineered linguisticallymotivated feature templates represent the semantic structure employed to train classifiers for each step. It is common among the state-of-the-art systems to train a global reranker on top of the local classifiers to improve performance (Toutanova et al., 2005; Bj¨orkelund et al., 2010; Roth and Lapata, 2016). SRL models have also been trained using graphical models (T¨ackstr¨om et al., 2015) and neural networks (Collobert et al., 2011; FitzGerald et al., 2015). Some systems have applied a set of structural constraints to the argument classification sub-task, such as avoiding overlapping arguments and repeated core roles, and enforced these constraints with integer linear programming (ILP) (Punyakanok et al., 2008) or a dynamic program (T¨ackstr¨om et al., 2015). Regarding leveraging unlabeled data, semi-supervised methods have been proposed to reduce human annotation efforts. He and Gildea (2006)"
C16-1121,D14-1045,0,0.222562,"essing. Such representations are typically learned from a large corpus using neural networks (e.g., Weston et al. (2008)), probabilistic graphical models (e.g., Deschacht et al. (2012)) or term-cooccurrence statistics (e.g., Turney and Pantel (2010)) by capturing the contexts in which the words appear. Often words from the vocabulary or phrases are mapped to vectors of real numbers in a low dimensional continuous space resulting in so-called word embeddings. Deschacht et al. (2012) employ distributed representations for each argument candidate as extra features when training a supervised SRL. Roth and Woodsend (2014) propose to use the compositional representations such as interaction of predicate and argument, dependency path and the full argument span to improve a state-of-the-art SRL system. 3 A Semantic Role Labeling System for Semi-Supervised Approaches In this section, we introduce a semantic role labeling system designed for semi-supervised settings. The system has a simple training strategy with local classifiers for different steps in SRL pipeline. Instead of training a global reranker on top of the local classifiers to improve performance as in other common pipeline-based state-of-the-art system"
C16-1121,W11-3906,0,0.0176956,"ed to reduce human annotation efforts. He and Gildea (2006) investigate the possibility of a weakly supervised approach by using self-training and co-training for unseen frames of SRL. They separate the headword and path as the two views for co-training, but could not show a clear performance improvement. The sources of the problem appeared to be the big gap in performance between the headword and path feature sets and the complexity of the task. Some other works show slight improvements of using co-training for SRL when there is a limited number of labeled data (Lee et al., 2007; Samad Zadeh Kaljahi and Baba, 2011). F¨urstenau and Lapata (2012) find novel instances for classifier training based on their similarity to manually labeled seed instances. This strategy is formalized via a graph alignment problem. Recently, there has been interest in distributional word representations for natural language processing. Such representations are typically learned from a large corpus using neural networks (e.g., Weston et al. (2008)), probabilistic graphical models (e.g., Deschacht et al. (2012)) or term-cooccurrence statistics (e.g., Turney and Pantel (2010)) by capturing the contexts in which the words appear. O"
C16-1121,D11-1012,0,0.0932483,"s work, each word wi is assigned probabilities P AC (p, wi , Lj ) to receive Lj ∈ L as semantic label. We employ the features proposed by Bj¨orkelund et al. (2010) as the basic feature set. All of the local classifiers are trained using L2-regularized logistic regression. For multiclass problems, we use the one-vs-rest strategy. At inference time, the local classifier predictions are merged using integer linear programming (ILP). In most of the prior work, ILP was only used for AC inference. However, this approach limits the interaction of AI and AC when making decisions. In another approach, Srikumar and Roth (2011) introduce a simple approach to joint inference over AI and AC allowing the two argument sub-tasks to support each other. Their local AC classifier has an empty label which indicates that the candidate is, in fact, not an argument. This forces AC module to learn also the argument identification and is in contrast with our approach in which the tasks of AI and AC classifiers are completely separated leading to a simpler AC learning. Their inference is formularized as an ILP problem that mazimizes the sum of local prediction scores over AI and AC. The authors then enforce consistency constraints"
C16-1121,Q15-1003,0,0.022731,"Missing"
C16-1121,P05-1073,0,0.153986,"y in Section 3 and Section 4 respectively. Our experiment is presented in Section 5 and Section 6 and finally we conclude in Section 7. 2 Related Work In traditional supervised approaches, SRL is modeled as a pipeline of predicate identification, predicate disambiguation, argument identification, and argument classification steps. Hand-engineered linguisticallymotivated feature templates represent the semantic structure employed to train classifiers for each step. It is common among the state-of-the-art systems to train a global reranker on top of the local classifiers to improve performance (Toutanova et al., 2005; Bj¨orkelund et al., 2010; Roth and Lapata, 2016). SRL models have also been trained using graphical models (T¨ackstr¨om et al., 2015) and neural networks (Collobert et al., 2011; FitzGerald et al., 2015). Some systems have applied a set of structural constraints to the argument classification sub-task, such as avoiding overlapping arguments and repeated core roles, and enforced these constraints with integer linear programming (ILP) (Punyakanok et al., 2008) or a dynamic program (T¨ackstr¨om et al., 2015). Regarding leveraging unlabeled data, semi-supervised methods have been proposed to red"
C16-1121,C00-2137,0,0.0586598,"Missing"
C16-1121,C98-1013,0,\N,Missing
C18-1182,W15-4319,0,0.0484532,"Missing"
C18-1182,benikova-etal-2014-nosta,0,0.0226218,"k on NER (Grishman and Sundheim, 1996)1 , many shared tasks and datasets for NER have been created. CoNLL 2002 (Tjong Kim Sang, 2002)2 and CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003)3 were created from newswire articles in four different languages (Spanish, Dutch, English, and German) and focused on 4 entities - PER (person), LOC (location), ORG (organization) and MISC (miscellaneous including all other types of entities). NER shared tasks have also been organized for a variety of other languages, including Indian languages (Rajeev Sangal and Singh, 2008), Arabic (Shaalan, 2014), German (Benikova et al., 2014), and slavic languages (Piskorski et al., 2017). The named entity types vary widely by source of dataset and language. For example, Rajeev Sangal and Singh (2008)’s southeast Asian language data has named entity types person, designation, temporal expressions, abbreviations, object number, brand, etc. Benikova et al. (2014)’s data, which is based on German wikipedia and online news, has named entity types similar to that of CoNLL 2002 and 2003: PERson, ORGanization, LOCation and OTHer. The shared task4 or1 Shared task: Shared task: 3 Shared task: 4 Shared task: 2 https://www-nlpir.nist.gov/rel"
C18-1182,D16-1153,0,0.0301821,"Missing"
C18-1182,A97-1029,0,0.389933,"Missing"
C18-1182,W13-2024,0,0.164254,"Missing"
C18-1182,W02-2004,0,0.221944,"Missing"
C18-1182,W16-6101,0,0.034025,"Missing"
C18-1182,W99-0613,0,0.696318,"Missing"
C18-1182,J81-4005,0,0.728263,"Missing"
C18-1182,W02-2007,0,0.248529,"Missing"
C18-1182,W16-3002,0,0.0453761,"Missing"
C18-1182,D17-2017,0,0.0494666,"Missing"
C18-1182,C96-1079,0,0.943272,"tems can yield further improvements. 1 Introduction Named entity recognition is the task of identifying named entities like person, location, organization, drug, time, clinical procedure, biological protein, etc. in text. NER systems are often used as the first step in question answering, information retrieval, co-reference resolution, topic modeling, etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering. The first NER task was organized by Grishman and Sundheim (1996) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks (Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002; Piskorski et al., 2017; Segura Bedmar et al., 2013; Bossy et al., 2013; Uzuner et al., 2011). Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies. These systems were followed by NER systems based on feature-engineering and machine learning (Nadeau and Sekine, 2007). Starting with Collobert et al. (2011), neural network NER systems with minimal feature engineering have become popular. Such models"
C18-1182,W04-1213,0,0.125025,"entity types similar to that of CoNLL 2002 and 2003: PERson, ORGanization, LOCation and OTHer. The shared task4 or1 Shared task: Shared task: 3 Shared task: 4 Shared task: 2 https://www-nlpir.nist.gov/related_projects/muc/ https://www.clips.uantwerpen.be/conll2002/ner/ https://www.clips.uantwerpen.be/conll2003/ner/ http://bsnlp.cs.helsinki.fi/shared_task.html 2146 ganized by Piskorski et al. (2017) covering 7 slavic languages (Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian) also has person, location, organization and miscellaneous as named entity types. In the biomedical domain, Kim et al. (2004) organized a BioNER task on MedLine abstracts, focusing on protien, DNA, RNA and cell attribute entity types. Uzuner et al. (2007) presented a clinical note de-identification task that required NER to locate personal patient data phrases to be anonymized. The 2010 I2B2 NER task5 (Uzuner et al., 2011), which considered clinical data, focused on clinical problem, test and treatment entity types. Segura Bedmar et al. (2013) organized a Drug NER shared task6 as part of SemEval 2013 Task 9, which focused on drug, brand, group and drug n (unapproved or new drugs) entity types. (Krallinger et al., 20"
C18-1182,C16-1087,0,0.0422495,"Missing"
C18-1182,N16-1030,0,0.35713,"Missing"
C18-1182,D15-1098,0,0.0253698,"italization of the first character), dictionaries and lexicons. Later work replaced these manually constructed feature vectors with word embeddings (Collobert et al., 2011), which are representations of words in n-dimensional space, typically learned over large collections of unlabeled data through an unsupervised process such as the skip-gram model (Mikolov et al., 2013). Studies have shown the importance of such pre-trained word embeddings for neural network based NER systems (Habibi et al., 2017), and similarly for pre-trained character embeddings in character-based languages like Chinese (Li et al., 2015; Yin et al., 2016). Modern neural architectures for NER can be broadly classified into categories depending upon their representation of the words in a sentence. For example, representations may be based on words, characters, other sub-word units or any combination of these. 6.4.1 Word level architectures In this architecture, the words of a sentence are given as input to Recurrent Neural Networks (RNN) and each word is represented by its word embedding, as shown in Figure 1. The first word-level NN model was proposed by Collobert et al. (2011)10 . The architecture was similar to the one show"
C18-1182,W16-3920,0,0.0612196,"Missing"
C18-1182,P16-1101,0,0.131802,"Missing"
C18-1182,W02-2019,0,0.22835,"Missing"
C18-1182,J93-2004,0,0.0663492,"Missing"
C18-1182,W17-4114,0,0.0325174,"Missing"
C18-1182,W14-1609,0,0.120468,"Missing"
C18-1182,W17-1412,0,0.104406,"ure, biological protein, etc. in text. NER systems are often used as the first step in question answering, information retrieval, co-reference resolution, topic modeling, etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering. The first NER task was organized by Grishman and Sundheim (1996) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks (Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002; Piskorski et al., 2017; Segura Bedmar et al., 2013; Bossy et al., 2013; Uzuner et al., 2011). Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies. These systems were followed by NER systems based on feature-engineering and machine learning (Nadeau and Sekine, 2007). Starting with Collobert et al. (2011), neural network NER systems with minimal feature engineering have become popular. Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent. Various neural arch"
C18-1182,P16-2067,0,0.0516903,"Missing"
C18-1182,W13-3516,0,0.0747787,"Missing"
C18-1182,S13-2058,0,0.036132,"Missing"
C18-1182,W15-3904,0,0.0522087,"Missing"
C18-1182,S13-2056,0,0.112025,"Missing"
C18-1182,J14-2008,0,0.155791,"ch covered a variety of supervised, semi-supervised and unsupervised NER systems, highlighted common features used by NER systems during that time, and explained NER evaluation metrics that are still in use today. Sharnagat (2014) presented a more recent NER survey that also included supervised, semi-supervised, and unsupervised NER systems, and included a few introductory neural network NER systems. There have also been surveys focused on NER systems for specific domains and languages, including biomedical NER, (Leaman and Gonzalez, 2008), Chinese clinical NER (Lei et al., 2013), Arabic NER (Shaalan, 2014; Etaiwi et al., 2017), and NER for Indian languages (Patil et al., 2016). The existing surveys primarily cover feature-engineered machine learning models (including supervised, semi-supervised, and unsupervised systems), and mostly focus on a single language or a single domain. There is not yet, to our knowledge, a comprehensive survey of modern neural network NER systems, nor is there a survey that compares feature engineered and neural network systems in both multi-lingual (CoNLL 2002 and CoNLL 2003) and multi-domain (e.g., news and medical) settings. 3 Methodology To identify articles for"
C18-1182,W03-1507,0,0.092227,"also been organized on social media data, e.g., Twitter, where the performance of classic NER systems degrades due to issues like variability in orthography and presence of grammatically incomplete sentences (Baldwin et al., 2015). Entity types on Twitter are also more variable (person, company, facility, band, sportsteam, movie, TV show, etc.) as they are based on user behavior on Twitter. Though most named entity annotations are flat, some datasets include more complex structures. Ohta et al. (2002) constructed a dataset of nested named entities, where one named entity can contain another. Strassel et al. (2003) highlighted both entity and entity head phrases. And discontinuous entities are common in chemical and clinical NER datasets (Krallinger et al., 2015). Eltyeb and Salim (2014) presented an survey of various NER systems developed for such NER datasets with a focus on chemical NER. 5 NER evaluation metrics Grishman and Sundheim (1996) scored NER performance based on type, whether the predicted label was correct regardless of entity boundaries, and text, whether the predicted entity boundaries were correct regardless of the label. For each score category, precision was defined as the number of e"
C18-1182,W02-2029,0,0.283119,"Missing"
C18-1182,W03-0419,0,0.548632,"Missing"
C18-1182,W02-2024,0,0.800265,"nical procedure, biological protein, etc. in text. NER systems are often used as the first step in question answering, information retrieval, co-reference resolution, topic modeling, etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering. The first NER task was organized by Grishman and Sundheim (1996) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks (Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002; Piskorski et al., 2017; Segura Bedmar et al., 2013; Bossy et al., 2013; Uzuner et al., 2011). Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies. These systems were followed by NER systems based on feature-engineering and machine learning (Nadeau and Sekine, 2007). Starting with Collobert et al. (2011), neural network NER systems with minimal feature engineering have become popular. Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independ"
C18-1182,S18-2021,1,0.887695,"Missing"
C18-1182,D16-1100,0,0.0242944,"e first character), dictionaries and lexicons. Later work replaced these manually constructed feature vectors with word embeddings (Collobert et al., 2011), which are representations of words in n-dimensional space, typically learned over large collections of unlabeled data through an unsupervised process such as the skip-gram model (Mikolov et al., 2013). Studies have shown the importance of such pre-trained word embeddings for neural network based NER systems (Habibi et al., 2017), and similarly for pre-trained character embeddings in character-based languages like Chinese (Li et al., 2015; Yin et al., 2016). Modern neural architectures for NER can be broadly classified into categories depending upon their representation of the words in a sentence. For example, representations may be based on words, characters, other sub-word units or any combination of these. 6.4.1 Word level architectures In this architecture, the words of a sentence are given as input to Recurrent Neural Networks (RNN) and each word is represented by its word embedding, as shown in Figure 1. The first word-level NN model was proposed by Collobert et al. (2011)10 . The architecture was similar to the one shown in Figure 1, but"
C18-1182,P02-1060,0,0.423794,"Missing"
D13-1078,N12-1049,0,0.147469,"Missing"
D13-1078,llorens-etal-2012-timen,0,0.179768,"Missing"
D13-1078,pustejovsky-etal-2010-iso,0,0.0598975,"Missing"
D13-1078,S13-2001,0,0.206466,"Missing"
D13-1078,S10-1010,0,\N,Missing
D15-1111,S13-1004,0,0.0531515,"Missing"
D15-1111,P14-1023,0,0.0992359,"kett, 2007) with related units aligned by human annotators. Evident from these alignments is the fact that aligned units are typically semantically similar or related. Existing aligners utilize a variety of resources and techniques for computing similarity between units: WordNet (MacCartney et al., 2008; Thadani and McKeown, 2011), PPDB (Yao et al., 2013b; Sultan et al., 2014a), distributional similarity measures (MacCartney et al., 2008; Yao et al., 2013b) and string similarity measures (MacCartney et al., 2008; Yao et al., 2013a). Recent work on neural word embeddings (Mikolov et al., 2013; Baroni et al., 2014) have advanced the state of distributional similarity, but remain largely unexplored in the context of alignment. Lexical or phrasal similarity does not entail alignment, however. Consider function words: the alignment (5, 4) in Figure 1 exists not just because both units are the word a, but also because they modify semantically equivalent units: jail and police station. The influence of context on content word alignment becomes salient particularly in the presence of competing words. In Figure 1, (soldiers(10), troops(2)) are not aligned despite the two words’ semantic equivalence in isolatio"
D15-1111,P06-1009,0,0.0229418,"Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experiments. While our work focuses primarily on word alignment, given a mechanism to compute phrasal similarity, the notion of cooperating words can be exploited to extend our model for phrasal alignment. Another important future direction is the construction of a ro"
D15-1111,P14-1139,0,0.0209365,"wever, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experiments. While our work focuses primarily on word alignment, given a mechanism to compute phrasal similarity, the notion of cooperating words can be exploited to extend our model for phrasal alignment. Another important future direction is the construction of a robust representation o"
D15-1111,P09-1053,0,0.0398622,"Missing"
D15-1111,N15-1086,0,0.0221853,"Missing"
D15-1111,de-marneffe-etal-2006-generating,0,0.0224136,"Missing"
D15-1111,D08-1084,0,0.292638,"Missing"
D15-1111,N12-1019,0,0.0361639,"Missing"
D15-1111,C04-1051,0,0.142163,"Missing"
D15-1111,N13-1092,0,0.0537795,"Missing"
D15-1111,P11-1076,0,0.0667393,"Missing"
D15-1111,J03-1002,0,0.0075941,"king principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experiments. While our work focuses primarily on word alignment, given a mechanism to compute phrasal similarity, the notion of cooperating words can be exploited to extend our model for phrasal alignment. Another important future direction is"
D15-1111,S13-1005,0,0.0518446,"Missing"
D15-1111,N10-1145,0,0.0278636,"he two above issues. 6 of a robust aligner. Acknowledgments This material is based in part upon work supported by the National Science Foundation under Grant Numbers EHR/0835393 and EHR/0835381. Related Work We mentioned major standalone monolingual aligners and briefly discussed their working principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results"
D15-1111,W07-1428,0,0.0414413,"Missing"
D15-1111,P09-1034,0,0.0576288,"Missing"
D15-1111,D12-1016,0,0.0209314,"owledgments This material is based in part upon work supported by the National Science Foundation under Grant Numbers EHR/0835393 and EHR/0835381. Related Work We mentioned major standalone monolingual aligners and briefly discussed their working principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experi"
D15-1111,Q14-1018,1,0.283744,"ltan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be represented as pairs of words – in the form of manyto-many mappings among the two phrases’ component words in the"
D15-1111,S14-2039,1,0.337388,"ltan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be represented as pairs of words – in the form of manyto-many mappings among the two phrases’ component words in the"
D15-1111,S15-2027,1,0.871751,"Missing"
D15-1111,C08-1110,0,0.0124497,"work supported by the National Science Foundation under Grant Numbers EHR/0835393 and EHR/0835381. Related Work We mentioned major standalone monolingual aligners and briefly discussed their working principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monolingual alignment that demonstrates top results in intrinsic and extrinsic evaluation experiments. While our work focuses primarily on word align"
D15-1111,P11-2044,0,0.229794,"2009; Madnani et al., 2012), textual similarity identification (Agirre et al., 2015; Sultan et al., 2015) and recognition of textual entailment (Dagan and Glickman, 2004; Pad´o et al., 2015). And they underpin applications such as short answer grading (Mohler et al., 2011), question answering (Hixon et al., 2015), machine translation evaluation (Pad´o et al., 2009), and machine reading (de Marneffe et al., 2007). A central problem underlying all text comparison tasks is that of alignment: pairing related semantic units (i.e. words and phrases) across the two snippets (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012; Yao et al., 2013a; Yao et al., 2013b; Sultan et al., 2014a). Studies have shown that such tasks can benefit from an explicit alignment component (Hickl and Bensley, 2007; Sultan et al., 2014b; Sultan et al., 2015). However, alignment is still an open research problem. We present a supervised monolingual aligner that produces top results in several intrinsic and extrinsic evaluation experiments. We pinpoint a set of key challenges for alignment and design a model with components targeted at each. Lexical and phrasal alignments can both be represented as pairs of words –"
D15-1111,C12-2120,0,0.23875,"Missing"
D15-1111,C10-1131,0,0.0259646,"r performs the best, but still suffers from the two above issues. 6 of a robust aligner. Acknowledgments This material is based in part upon work supported by the National Science Foundation under Grant Numbers EHR/0835393 and EHR/0835381. Related Work We mentioned major standalone monolingual aligners and briefly discussed their working principles in Section 2. There are, however, at least two additional groups of related work which can inform future research on monolingual alignment. First, alignment is often performed in the context of extrinsic tasks, e.g., textual entailment recognition (Wang and Manning, 2010), question answering (Heilman and Smith, 2010), discourse generation (Roth and Frank, 2012) and redundancy detection (Thadani and McKeown, 2008). Such systems may contain useful design elements yet to be utilized by standalone aligners. Second, a large body of work exists in the bilingual alignment literature (Och and Ney, 2003; Blunsom and Cohn, 2006; Chang et al., 2014), elements of which (such as the machine learning models) can be useful for monolingual aligners (see (Yao et al., 2013a) for an example). 7 Conclusions and Future Work We present a two-stage classification framework for monol"
D15-1111,P13-2123,0,0.166433,"Missing"
D15-1111,D13-1056,0,0.303865,"Missing"
D15-1111,S15-2045,0,\N,Missing
D15-1271,N06-1046,0,0.0146148,"ee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact . ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without retraining the original model. 4 Centering theory const"
D15-1271,D08-1031,0,0.0360294,"atives adopted as soft constraints. When testing on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 h"
D15-1271,W09-1206,0,0.10973,"Missing"
D15-1271,W10-2608,0,0.0583813,"Missing"
D15-1271,N07-1030,0,0.0191877,"tion and its antecedent” (Lee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact . ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without retraining the original mod"
D15-1271,D13-1203,0,0.511163,", 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 https://catalog.ldc.upenn.edu/LDC2011T03 2 tackle the various aspects of coreference by using the simplest possible set of features. Its advantage is that the system can both implicitly model important linguistic effects and capture other patterns in the data that are not easily teased out by hand. With a simple set of features including head/first/last words, preceding/following words, length, exact string match, head match, sentence/menti"
D15-1271,P08-2012,0,0.0147353,"string match between a mention and its antecedent” (Lee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact . ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without"
D15-1271,finlayson-etal-2014-n2,0,0.0598125,"Missing"
D15-1271,P14-5010,1,0.0237744,"Missing"
D15-1271,P10-2029,0,0.0269714,"Missing"
D15-1271,J95-2003,0,0.766789,"uge effect on information flow across sentences. Since they are almost void of meaning (only signal gender and number of the antecedent), the discourse referent to be picked up must be particularly salient, so that it can be readily identified by the reader (Stede, 2011). The discourse center hypothesis (HudsonD’Zmura, 1988) states that at any point in discourse understanding, there is one single entity that is the most salient discourse referent at that point. This referent is called the center. Centering theory is a key element of the discourse center hypothesis used in anaphora resolution (Grosz et al., 1995). Beaver (2004) reformulates the centering theory in terms of Optimality Theory (Prince and Smolensky, 2004). Six ranked constraints – Agree, Disjoint, ProTop, FamDef, Cohere and Align – are used to make anaphora decisions. We adopt four of these constraints in our ILP model as follows: Disjoint “Co-arguments of a predicate4 are disjoint.” For each j &lt; i such that mi and mj are subject and object arguments of a non-reflexive predicate, we introduce a constraint indicator variable cdisjoint,i,j , and add the ILP constraint vij − cdisjoint,i,j = 0. ProTop “The topic of a sentence which is the en"
D15-1271,D09-1120,0,0.0229707,"stateof-the-art Berkeley coreference resolution system trained on OntoNotes3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 https://catalog.ldc.upenn.edu/LDC2011T03 2 tackle the various aspects of coreference by usi"
D15-1271,N10-1061,0,0.0133744,"on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 https://catalog.ldc.upenn.edu/LDC2011T03 2 tackle t"
D15-1271,W11-1902,0,0.539948,"oreference resolution system trained on OntoNotes3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 https://catalog.ldc.upenn.edu/LDC2011T03 2 tackle the various aspects of coreference by using the simplest pos"
D19-1260,D18-1237,0,0.0165894,"l., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2578–2589, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this work we propose an unsupervised algorithm for the selection of multi-hop justifications from unstructured knowledge bases (KB). Unlike other supervised selection methods (Dehghani et al., 2019; Bao et al., 2016; Lin et al., 2018; Wang e"
D19-1260,C16-1236,0,0.164837,"6; De Cao et al., 2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2578–2589, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this work we propose an unsupervised algorithm for the selection of multi-hop justifications from unstructured knowledge bases (KB). Unlike other supervised selection methods (Dehghani et al., 2019; Bao et al., 2016; Lin et al., 2018; Wang et al., 2018b,a; Tran and Niedere´ee, 2018; Trivedi et al., 2019), our approach does not require any training data for justification selection. Unlike approaches that rely on structured KBs, which are expensive to create, (Khashabi et al., 2016; Khot et al., 2017; Zhang et al., 2018; Khashabi et al., 2018b; Cui et al., 2017; Bao et al., 2016), our method operates over KBs of only unstructured texts. We demonstrate that our approach has a bigger impact on downstream QA approaches that use these justification sentences as additional signal than a strong baseline that rel"
D19-1260,D18-1454,0,0.0734574,"Missing"
D19-1260,N19-1240,0,0.0377168,"Missing"
D19-1260,D15-1075,0,0.0336108,"aining data to learn how to select justification sentences (i.e., questions and answers coupled with correct justifications); (b) methods that treat justifications as latent variables and learn jointly how to answer questions and how to select justifications from questions and answers alone; (c) approaches that rely on information retrieval to select justification sentences; and, lastly, (d) methods that do not use justification sentences at all. In the first category, previous works (e.g., (Trivedi et al., 2019)) have used entailment resources including labeled trained datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) to train components for selecting justification sentences for QA. Other works have explicitly focused on training sentence selection components for QA models (Min et al., 2018; Lin et al., 2018; Wang et al., 2019). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019"
D19-1260,C18-1014,0,0.0405596,"tion sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two directions, our approach requires no training"
D19-1260,P17-1171,0,0.0410073,"nces and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because these methods cannot easily explain their inference. Note that some of the works discussed here transfer knowledge from external datasets into the QA task they address (Chung et al., 2017; Sun et al., 2018; Pan et al., 2019; Min et al., 2017; Qiu et al., 2018; Chen et al., 2017). In this work, we focus solely on the resources provided in the task itself because such compatible external resources may not be available in real-world applications of QA. 3 Approach ROCC, coupled with a QA system, operates in the following steps (illustrated in Figure 2): KBs (e.g., ARC), we retrieve the top n sentences1 from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We implemented this using the BM25 IR model with the default parameters in Lucene2 . For reading comprehension datasets where the qu"
D19-1260,N19-1405,0,0.0958436,"Missing"
D19-1260,P17-1020,0,0.0344093,"19). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two dire"
D19-1260,N18-1143,0,0.05971,"Missing"
D19-1260,P17-1021,0,0.0307899,"rks (e.g., (Trivedi et al., 2019)) have used entailment resources including labeled trained datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) to train components for selecting justification sentences for QA. Other works have explicitly focused on training sentence selection components for QA models (Min et al., 2018; Lin et al., 2018; Wang et al., 2019). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attentio"
D19-1260,P18-1031,0,0.0137097,"n. In particular, we employed BERT as a binary classifier operating over two texts. The first text consists of the concatenated question and answer, and the second text consists of the justification text. The classifier operates over the hidden states of the two texts, i.e., the state corresponding to the [CLS] token (Devlin et al., 2018).5 We observed empirically that pre-training the BERT classifier on all n sentences retrieved by BM25, and then fine tuning on the ROCC justifications improves performance on all datasets we experimented with. This resembles the transfer learning discussed by Howard and Ruder (2018), where the source domain would be the BM25 sentences, and the target domain the ROCC justifications. However, one important distinction is that, in our case, all this knowledge comes solely from the resources provided within each dataset, and is retrieved using unsupervised method (BM25). We conjecture that this helped mainly because the pretraining step exposed BERT to more data which, even if imperfect, is topically related to the corresponding question and answer. scored by annotator with a precision of 12 because the first justification sentence is not relevant, and a coverage of 12 becau"
D19-1260,N18-1023,0,0.315856,"de adoption of ML solutions in many fields such as healthcare, finance, and law (Samek et al., 2017; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017; Gilpin et al., 2018; Biran and Cotton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et"
D19-1260,P17-2049,0,0.156226,"Missing"
D19-1260,Q19-1014,0,0.0420159,"ton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natura"
D19-1260,C18-1181,0,0.0203987,"ere gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two directions, our approa"
D19-1260,P18-1161,0,0.124236,"2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2578–2589, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this work we propose an unsupervised algorithm for the selection of multi-hop justifications from unstructured knowledge bases (KB). Unlike other supervised selection methods (Dehghani et al., 2019; Bao et al., 2016; Lin et al., 2018; Wang et al., 2018b,a; Tran and Niedere´ee, 2018; Trivedi et al., 2019), our approach does not require any training data for justification selection. Unlike approaches that rely on structured KBs, which are expensive to create, (Khashabi et al., 2016; Khot et al., 2017; Zhang et al., 2018; Khashabi et al., 2018b; Cui et al., 2017; Bao et al., 2016), our method operates over KBs of only unstructured texts. We demonstrate that our approach has a bigger impact on downstream QA approaches that use these justification sentences as additional signal than a strong baseline that relies on information"
D19-1260,D18-1260,0,0.473433,"finance, and law (Samek et al., 2017; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017; Gilpin et al., 2018; Biran and Cotton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue that more effort must"
D19-1260,P17-2081,0,0.0314461,"e quality of the justification sentences and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because these methods cannot easily explain their inference. Note that some of the works discussed here transfer knowledge from external datasets into the QA task they address (Chung et al., 2017; Sun et al., 2018; Pan et al., 2019; Min et al., 2017; Qiu et al., 2018; Chen et al., 2017). In this work, we focus solely on the resources provided in the task itself because such compatible external resources may not be available in real-world applications of QA. 3 Approach ROCC, coupled with a QA system, operates in the following steps (illustrated in Figure 2): KBs (e.g., ARC), we retrieve the top n sentences1 from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We implemented this using the BM25 IR model with the default parameters in Lucene2 . For readi"
D19-1260,P18-1160,0,0.0435189,"uestions and how to select justifications from questions and answers alone; (c) approaches that rely on information retrieval to select justification sentences; and, lastly, (d) methods that do not use justification sentences at all. In the first category, previous works (e.g., (Trivedi et al., 2019)) have used entailment resources including labeled trained datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) to train components for selecting justification sentences for QA. Other works have explicitly focused on training sentence selection components for QA models (Min et al., 2018; Lin et al., 2018; Wang et al., 2019). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend"
D19-1260,D19-5804,0,0.0334415,"ortant for both the quality of the justification sentences and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because these methods cannot easily explain their inference. Note that some of the works discussed here transfer knowledge from external datasets into the QA task they address (Chung et al., 2017; Sun et al., 2018; Pan et al., 2019; Min et al., 2017; Qiu et al., 2018; Chen et al., 2017). In this work, we focus solely on the resources provided in the task itself because such compatible external resources may not be available in real-world applications of QA. 3 Approach ROCC, coupled with a QA system, operates in the following steps (illustrated in Figure 2): KBs (e.g., ARC), we retrieve the top n sentences1 from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We implemented this using the BM25 IR model with the default parameters in L"
D19-1260,P18-2034,0,0.0225965,"ustification sentences and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because these methods cannot easily explain their inference. Note that some of the works discussed here transfer knowledge from external datasets into the QA task they address (Chung et al., 2017; Sun et al., 2018; Pan et al., 2019; Min et al., 2017; Qiu et al., 2018; Chen et al., 2017). In this work, we focus solely on the resources provided in the task itself because such compatible external resources may not be available in real-world applications of QA. 3 Approach ROCC, coupled with a QA system, operates in the following steps (illustrated in Figure 2): KBs (e.g., ARC), we retrieve the top n sentences1 from this KB using an IR query that concatenates the question and the candidate answer, similar to Clark et al. (2018); Yadav et al. (2019). We implemented this using the BM25 IR model with the default parameters in Lucene2 . For reading comprehension d"
D19-1260,P18-2124,0,0.0844555,"Missing"
D19-1260,P08-1082,1,0.737225,"earchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Choi et al., 2017; Lai et al., 2018; Geva and Berant, 2018) or PageRank (Surdeanu et al., 2008) to learn how to select justification sentences without explicit training data. Other works have used end-to-end (mostly RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two directions, our approach requires no training data at all for the justification se"
D19-1260,N19-1302,0,0.213355,"Missing"
D19-1260,D17-1093,0,0.0166111,"iple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue that more effort must be dedicated to explaining their inference process. 2578 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2578–2589, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics In this work we propose an unsupervised algorithm for the selection of multi-hop justifications from unstructured knowledge bases (KB). Unlike other supervised sel"
D19-1260,K19-1065,0,0.405005,"tions from questions and answers alone; (c) approaches that rely on information retrieval to select justification sentences; and, lastly, (d) methods that do not use justification sentences at all. In the first category, previous works (e.g., (Trivedi et al., 2019)) have used entailment resources including labeled trained datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) to train components for selecting justification sentences for QA. Other works have explicitly focused on training sentence selection components for QA models (Min et al., 2018; Lin et al., 2018; Wang et al., 2019). In datasets where gold justification sentences are not provided, researchers have trained such components by retrieving justifications from structured KBs (Cui et al., 2017; Bao et al., 2016; Zhang et al., 2016; Hao et al., 2017) such as ConceptNet (Speer et al., 2017), or from IR systems coupled 2579 with denoising components (Wang et al., 2019). While these works offer exciting directions, they all rely on training data for justifications, which is expensive to generate and may not be available in real-world use cases. The second group of methods tend to rely on reinforcement learning (Cho"
D19-1260,P18-1178,0,0.0511533,"Missing"
D19-1260,Q18-1021,0,0.0574361,"such as healthcare, finance, and law (Samek et al., 2017; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017; Gilpin et al., 2018; Biran and Cotton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et al., 2018), we argue"
D19-1260,N19-1274,1,0.753069,"RNNs with attention mechanisms) QA architectures for learning to pay more attention on better justification sentences (Min et al., 2018; Seo et al., 2016; Yu et al., 2014; Gravina et al., 2018). While these approaches do not require annotated justifications, they need large amounts of question/answer pairs during training so they can discover the latent justifications. In contrast to these two directions, our approach requires no training data at all for the justification selection process. The third category of methods utilize IR techniques to retrieve justifications from both unstructured (Yadav et al., 2019) and structured (Khashabi et al., 2016) KBs. Our approach is closer in spirit to this direction, but it is adjusted to account for more intentional knowledge aggregation. As we show in Section 4, this is important for both the quality of the justification sentences and the performance of the downstream QA system. The last group of QA approaches learn how to classify answers without any justification sentences (Mihaylov et al., 2018; Sun et al., 2018; Devlin et al., 2018). While this has been shown to obtain good performance for answer classification, we do not focus on it in this work because"
D19-1260,D18-1259,0,0.0369764,"ons in many fields such as healthcare, finance, and law (Samek et al., 2017; Alvarez-Melis and Jaakkola, 2017; Arras et al., 2017; Gilpin et al., 2018; Biran and Cotton, 2017) For complex natural language processing (NLP) such as question answering (QA), human readable explanations of the inference process have been proposed as a way to interpret QA models (Zhou et al., 2018). Recently, multiple datasets have been proposed for multi-hop QA, in which questions can only be answered when considering information from multiple sentences and/or documents (Clark et al., 2018; Khashabi et al., 2018a; Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Bauer et al., 2018; Dunn et al., 2017; Dhingra et al., 2017; Lai et al., 2017; Rajpurkar et al., 2018; Sun et al., 2019). The task of selecting justification sentences is complex for multi-hop QA, because of the additional knowledge aggregation requirement (examples of such questions and answers are shown in Figures 1 and 2). Although various neural QA methods have achieved high performance on some of these datasets (Sun et al., 2018; Trivedi et al., 2019; Tymoshenko et al., 2017; Seo et al., 2016; Wang and Jiang, 2016; De Cao et al., 2018; Back et"
D19-1260,C18-1171,0,0.0410304,"Missing"
E12-1034,P08-1090,0,0.133535,"other events that are likely to belong to the script. Our work aims to answer key questions about how best to (1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 1 within that script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) or that generates a story using the selected events (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010). In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script. In particular, we consider the following questions: • How should representative chains of events be selected from the source text? • Given an event chain, how should statistics be gathered from it? • Given event n-gram statistics, which ranking function best predicts the events for a script? In the"
E12-1034,P09-1068,0,0.501443,"to belong to the script. Our work aims to answer key questions about how best to (1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 1 within that script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) or that generates a story using the selected events (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010). In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script. In particular, we consider the following questions: • How should representative chains of events be selected from the source text? • Given an event chain, how should statistics be gathered from it? • Given event n-gram statistics, which ranking function best predicts the events for a script? In the process of answering these qu"
E12-1034,P11-1098,0,0.0157306,"ecting narrative event statistics, and show that this approach performs better than classic n-gram statistics. Introduction There has been recent interest in automatically acquiring world knowledge in the form of scripts (Schank and Abelson, 1977), that is, frequently recurring situations that have a stereotypical sequence of events, such as a visit to a restaurant. All of the techniques so far proposed for this task share a common sub-task: given an event or partial chain of events, predict other events that belong to the same script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2011; Manshadi et al., 2008; McIntyre and Lapata, 2009; McIntyre and Lapata, 2010; Regneri et al., 2010). Such a model can then serve as input to a system that identifies the order of the events • We propose a new method for ranking events given a partial script, and show that it performs substantially better than ranking methods from prior work. • We propose a new evaluation procedure (using Recall@N) for the cloze test, and advocate its usage instead of average rank used previously in the literature. • We provide a systematic analysis of the interactions between the choices made when constructin"
E12-1034,guthrie-etal-2006-closer,0,0.0137734,"ointwise mutual informations between the event e and each of the events in the script: • 1-skip bigrams. We collect pairs of events that occur with 0 or 1 events intervening between them. For example, given the chain (saw, SUBJ), (kissed, OBJ), (blushed, SUBJ), we would extract three bigrams: the two regular bigrams ((saw, SUBJ), (kissed, OBJ)) and ((kissed, OBJ), (blushed, SUBJ)), plus the 1skip-bigram, ((saw, SUBJ), (blushed, SUBJ)). This approach to collecting n-gram statistics is sometimes called skip-gram modeling, and it can reduce data sparsity by extracting more event pairs per chain (Guthrie et al., 2006). It has not previously been applied in the task of predicting script events, but it may be quite appropriate to this task because in most scripts it is possible to skip some events in the sequence. Chambers and Jurafsky’s description of this score suggests that it is unordered, such that P (a, b) = P (b, a). Thus the probabilities must be defined as: f (e, c) = log i P (e1 , e2 ) = P (ci , e) P (ci )P (e) C(e1 , e2 ) + C(e2 , e1 ) PP C(ei , ej ) ei ej C(e) 0 e0 C(e ) P (e) = P where C(e1 , e2 ) is the number of times that the ordered event pair (e1 , e2 ) was counted in the training data, and"
E12-1034,P03-1054,0,0.0243036,", (3) applying a coreference resolution system and (4) identifying event chains via entities and dependencies. First, articles that had no narrative content were removed from the corpora. In the Reuters Corpus, we removed all files solely listing stock exchange values, interest rates, etc., as well as all articles that were simply summaries of headlines from different countries or cities. After removing these files, the Reuters corpus was reduced to 788, 245 files. Removing files from the Fairy Tale corpus was not necessary – all 437 stories were retained. We then applied the Stanford Parser (Klein and Manning, 2003) to identify the dependency structure of each sentence in each article in the corpus. This parser produces a constitutent-based syntactic parse tree for each sentence, and then converts this tree to a collapsed dependency structure via a set of tree patterns. Next we applied the OpenNLP coreference engine5 to identify the entities in each article, and the noun phrases that were mentions of each entity. Finally, to identify the event chains, we took each of the entities proposed by the coreference system, walked through each of the noun phrases associated with that entity, retrieved any subject"
E12-1034,P09-1025,0,0.247915,"(1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 1 within that script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) or that generates a story using the selected events (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010). In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script. In particular, we consider the following questions: • How should representative chains of events be selected from the source text? • Given an event chain, how should statistics be gathered from it? • Given event n-gram statistics, which ranking function best predicts the events for a script? In the process of answering these questions, this article makes several contributions to the field of script and na"
E12-1034,P10-1158,0,0.474702,"event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 1 within that script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) or that generates a story using the selected events (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010). In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script. In particular, we consider the following questions: • How should representative chains of events be selected from the source text? • Given an event chain, how should statistics be gathered from it? • Given event n-gram statistics, which ranking function best predicts the events for a script? In the process of answering these questions, this article makes several contributions to the field of script and narrative event chain understa"
E12-1034,P10-1100,0,0.522091,"ics. Introduction There has been recent interest in automatically acquiring world knowledge in the form of scripts (Schank and Abelson, 1977), that is, frequently recurring situations that have a stereotypical sequence of events, such as a visit to a restaurant. All of the techniques so far proposed for this task share a common sub-task: given an event or partial chain of events, predict other events that belong to the same script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2011; Manshadi et al., 2008; McIntyre and Lapata, 2009; McIntyre and Lapata, 2010; Regneri et al., 2010). Such a model can then serve as input to a system that identifies the order of the events • We propose a new method for ranking events given a partial script, and show that it performs substantially better than ranking methods from prior work. • We propose a new evaluation procedure (using Recall@N) for the cloze test, and advocate its usage instead of average rank used previously in the literature. • We provide a systematic analysis of the interactions between the choices made when constructing an event prediction model. 336 Proceedings of the 13th Conference of the European Chapter of the A"
E17-2118,S15-2136,1,0.513914,"ontains relation which is the most frequent temporal relation type in clinical data (Styler IV et al., 2014). Consider the sentence: Patient was diagnosed with a rectal cancer in May of 2010. It can be said that the temporal expression May of 2010 in this sentence contains the cancer event. The same relation can exist between two events: During the surgery the patient experienced severe tachycardia. Here, the surgery event contains the tachycardia event. The vast majority of systems in temporal information extraction challenges, such as the i2b2 (Sun et al., 2013) and Clinical TempEval tasks (Bethard et al., 2015; Bethard et al., 2016), used classifiers with a large number of manually engineered features. This is not ideal, as most NLP components used for feature extraction experience a significant accuracy drop when applied to out-of-domain data (Wu et al., 2014; McClosky et al., 2010; Daum´e III, 2009; Blitzer et al., 2006), propagating the error to the downstream components and ultimately leading to significant performance degradation. In this work, we propose a novel temporal relation extraction framework that requires minimal linguistic pre-processing and can operate on raw tokens. We experiment"
E17-2118,W06-1615,0,0.0383876,"t between two events: During the surgery the patient experienced severe tachycardia. Here, the surgery event contains the tachycardia event. The vast majority of systems in temporal information extraction challenges, such as the i2b2 (Sun et al., 2013) and Clinical TempEval tasks (Bethard et al., 2015; Bethard et al., 2016), used classifiers with a large number of manually engineered features. This is not ideal, as most NLP components used for feature extraction experience a significant accuracy drop when applied to out-of-domain data (Wu et al., 2014; McClosky et al., 2010; Daum´e III, 2009; Blitzer et al., 2006), propagating the error to the downstream components and ultimately leading to significant performance degradation. In this work, we propose a novel temporal relation extraction framework that requires minimal linguistic pre-processing and can operate on raw tokens. We experiment with two neural architectures for temporal relation extraction: a convolutional neural network (CNN) (LeCun et al., 1998) and a long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997). Little work exists on using these methods for relation extraction; to the best of our knowledge no work exists"
E17-2118,D14-1181,0,0.00672077,"Missing"
E17-2118,W16-2914,1,0.784286,", 2015; Bethard et al., 2016). The gold standard annotations include time expressions, events (both medical and general), and temporal relations. We used the standard split established by Clinical TempEval 2016, using the development set for evaluating models and tuning model parameters, and evaluating our best event-event and event-time models on the test set. Following Clinical TempEval, we focus only on the contains relation, which was the most common relation and had the highest inter-annotator agreement. 3.2 Experiments We compare the performance of our neural models to the THYME system (Lin et al., 2016a), 747 Model Argument representation THYME full system THYME tokens only CNN tokens CNN tokens CNN pos tags LSTM tokens LSTM pos tags CNN token + pos tags LSTM token + pos tags n/a n/a position embeddings XML tags XML tags XML tags XML tags XML tags XML tags Event-time relations P R F1 0.583 0.810 0.678 0.564 0.786 0.657 0.647 0.627 0.637 0.660 0.775 0.713 0.707 0.708 0.707 0.691 0.626 0.657 0.754 0.657 0.702 0.727 0.681 0.703 0.698 0.660 0.679 Event-event relations P R F1 0.569 0.574 0.572 0.562 0.539 0.550 0.580 0.324 0.416 0.566 0.522 0.543 0.630 0.204 0.309 0.610 0.418 0.496 0.603 0.212 0"
E17-2118,W15-1506,0,0.369979,"his work, we propose a novel temporal relation extraction framework that requires minimal linguistic pre-processing and can operate on raw tokens. We experiment with two neural architectures for temporal relation extraction: a convolutional neural network (CNN) (LeCun et al., 1998) and a long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997). Little work exists on using these methods for relation extraction; to the best of our knowledge no work exists on using LSTM models for relation extraction or CNN models for temporal information extraction. Zeng et al. (2014) and Nguyen and Grishman (2015) employ CNNs for non-temporal relation extraction and show that CNNs can be effective for relation classification and perform as well as token-based baselines for relation extraction. Our experiments, on the other hand, show that neural relation extraction models can compete with a complex featurebased state-of-the-art relation extraction system. Another important difference that sets our work apart is our representation of the argument positions: previous work used token position features (embedded in a 50-dimensional space) to encode the relative distance of the words in the sentence to the"
E17-2118,D14-1162,0,0.0986284,"Missing"
E17-2118,C14-1220,0,0.255061,"mance degradation. In this work, we propose a novel temporal relation extraction framework that requires minimal linguistic pre-processing and can operate on raw tokens. We experiment with two neural architectures for temporal relation extraction: a convolutional neural network (CNN) (LeCun et al., 1998) and a long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997). Little work exists on using these methods for relation extraction; to the best of our knowledge no work exists on using LSTM models for relation extraction or CNN models for temporal information extraction. Zeng et al. (2014) and Nguyen and Grishman (2015) employ CNNs for non-temporal relation extraction and show that CNNs can be effective for relation classification and perform as well as token-based baselines for relation extraction. Our experiments, on the other hand, show that neural relation extraction models can compete with a complex featurebased state-of-the-art relation extraction system. Another important difference that sets our work apart is our representation of the argument positions: previous work used token position features (embedded in a 50-dimensional space) to encode the relative distance of th"
E17-2118,Q14-1012,1,\N,Missing
E17-2118,S16-1165,1,\N,Missing
E17-2118,N10-1004,0,\N,Missing
I17-1010,J81-4005,0,0.750902,"Missing"
I17-1010,N09-1017,0,0.0215323,"proach improves state-of-the-art performance on implicit semantic role labeling with less reliance than prior work on manually constructed language resources. 1 Introduction Semantic role labeling (SRL) has traditionally focused on semantic frames consisting of verbal or nominal predicates and explicit arguments that occur within the clause or sentence that contains the predicate. However, many predicates, especially nominal ones, may bear arguments that are left implicit because they regard common sense knowledge or because they are mentioned earlier in a discourse (Ruppenhofer et al., 2010; Gerber et al., 2009). These arguments, called implicit arguments, are resolved by another semantic task, implicit semantic role labeling (iSRL). Consider a NomBank (Meyers et al., 2004) annotation example: [A0 The network] had been expected to have [NP losses] [A1 of $20 million] . . . Those [NP losses] may widen because of the short Series. 90 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 90–99, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP their implicit semantic roles. Our PRNSFM-based iSRL model improves state-of-the-art performance, outperformin"
I17-1010,P10-1160,0,0.420457,"he second sentence has no associated arguments. However, for a good reader, a reasonable interpretation of the second loss should be that it receives the same A0 and A1 as the first instance. These arguments are implicit to the second loss. As an emerging task, implicit semantic role labeling faces a lack of resources. First, hand-crafted implicit role annotations for use as training data are seriously limited: SemEval 2010 Task 10 (Baker et al., 1998) provided FrameNet-style (Baker et al., 1998) annotations for a fairly large number of predicates but with few annotations per predicate, while Gerber and Chai (2010) provided PropBank-style (Palmer et al., 2005) data with many more annotations per predicate but covering just 10 predicates. Second, most existing iSRL systems depend on other systems (explicit semantic role labelers, named entity taggers, lexical resources, etc.), and as a result not only need iSRL annotations to train the iSRL system, but annotations or manually built resources for all of their sub-systems as well. We propose an iSRL approach that addresses these challenges, requiring no manually annotated iSRL data and only a single sub-system, an explicit semantic role labeler. We introdu"
I17-1010,J12-4003,0,0.0627672,"Discussion Experimental Setup In the baseline mode, instead of using the PRNSFM, we only use the deterministic prediction by the explicit SRL system. We refer to this mode as Baseline in Table 1. In the main mode, the joint embedding LSTM model (Model 1) and the separate embedding LSTM model (Model 2) were trained on the same dataset which is a combination of the automatic SRL annotations and the gold standard CoNLL small values reported in the article achieved similar results with faster processing times. 2 Following Schenk and Chiarcos (2016), we do not perform the alternative evaluation of Gerber and Chai (2012) that evaluates systems on the iSRL training set, since the iSRL training set overlaps with the CoNLL 2009 explicit semantic role training set on which MATE is trained. 3 For iSRL, one implicit role may receive more than one annotated filler across a coreference chain in the discourse. 96 NER system WordNet SRL system PRNSFM training data iSRL data Method Gerber and Chai (2010) Laparra and Rigau (2013) Schenk and Chiarcos (2016) Baseline Skip-gram Model 1: Joint Embedding Model 2: Separate Embedding Model 1: Joint Embedding Model 2: Separate Embedding P R F1 X X X 44.5 40.4 42.3 X X X 47.9 43."
I17-1010,P13-1116,0,0.310098,"presented in section 5. Finally, we conclude our work and suggest some future work in section 6. 2 processing tasks, not semantic frame processing. Semantic Role Labeling In unsupervised SRL, Woodsend and Lapata (2015) and Titov and Khoddam (2015) induce embeddings to represent a predicate and its arguments from unannotated texts, but in their approaches, the arguments are words only, not the semantic role labels, while in our models, both are considered. Low-resource Implicit Semantic Role Labeling Several approaches have attempted to address the lack of resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constructed resources: an explicit SRL system, WordNet super-senses, a named entity tagger, and a manual categorization of SuperSenseTagger semantic classes. Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art of Laparra and Rigau (2013). Schenk and Chiarcos (2016) proposed an approach to induce proto"
I17-1010,W04-2705,0,0.300191,"Missing"
I17-1010,C10-3009,0,0.0855675,"Missing"
I17-1010,J05-1004,0,0.0495999,"However, for a good reader, a reasonable interpretation of the second loss should be that it receives the same A0 and A1 as the first instance. These arguments are implicit to the second loss. As an emerging task, implicit semantic role labeling faces a lack of resources. First, hand-crafted implicit role annotations for use as training data are seriously limited: SemEval 2010 Task 10 (Baker et al., 1998) provided FrameNet-style (Baker et al., 1998) annotations for a fairly large number of predicates but with few annotations per predicate, while Gerber and Chai (2010) provided PropBank-style (Palmer et al., 2005) data with many more annotations per predicate but covering just 10 predicates. Second, most existing iSRL systems depend on other systems (explicit semantic role labelers, named entity taggers, lexical resources, etc.), and as a result not only need iSRL annotations to train the iSRL system, but annotations or manually built resources for all of their sub-systems as well. We propose an iSRL approach that addresses these challenges, requiring no manually annotated iSRL data and only a single sub-system, an explicit semantic role labeler. We introduce a predictive recurrent neural semantic fram"
I17-1010,P16-1028,0,0.0256194,"model (PRNSFM) from these explicit frames and roles. Our PRNSFM views semantic frames as a sequence: a predicate, followed by the arguments in their textual order, and terminated by a special EOS symbol. We draw predicates from PropBank Language Modeling Language models, from ngram models to continuous space language models (Mikolov et al., 2013; Pennington et al., 2014), provide probability distributions over sequences of words and have shown their usefulness in many natural language processing tasks. However, to our knowledge, they have not yet been used to model semantic frames. Recently, Peng and Roth (2016) developed two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities, but these models focus on discourse 91 verbal semantic frames, and represent arguments with their nominal/pronominal heads. For example, Michael Phelps swam at the Olympics is represented as [swam:PRED, Phelps:A0, Olympics:AMLOC, EOS], where the predicate is labeled PRED and the arguments Phelps and Olympics are labeled A0 and AM-LOC, respectively. Our PRNSFM’s task is thus to take a predicate and zero or more arguments, and predic"
I17-1010,D14-1162,0,0.0796109,"Missing"
I17-1010,J15-4003,0,0.0365329,"ts are words only, not the semantic role labels, while in our models, both are considered. Low-resource Implicit Semantic Role Labeling Several approaches have attempted to address the lack of resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constructed resources: an explicit SRL system, WordNet super-senses, a named entity tagger, and a manual categorization of SuperSenseTagger semantic classes. Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art of Laparra and Rigau (2013). Schenk and Chiarcos (2016) proposed an approach to induce prototypical roles using distributed word representations, which required only an explicit SRL system and a large unannotated corpus, but their model performance was almost 10 points lower than the state-of-the-art of Laparra and Rigau (2013). Similar to Schenk and Chiarcos (2016), our model requires only an explicit SRL system and a large unannotated corpus, but we tak"
I17-1010,W09-2417,0,0.313807,"Missing"
I17-1010,N16-1173,0,0.510939,"f resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constructed resources: an explicit SRL system, WordNet super-senses, a named entity tagger, and a manual categorization of SuperSenseTagger semantic classes. Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art of Laparra and Rigau (2013). Schenk and Chiarcos (2016) proposed an approach to induce prototypical roles using distributed word representations, which required only an explicit SRL system and a large unannotated corpus, but their model performance was almost 10 points lower than the state-of-the-art of Laparra and Rigau (2013). Similar to Schenk and Chiarcos (2016), our model requires only an explicit SRL system and a large unannotated corpus, but we take a very different approach to leveraging these, and as a result improve state-of-the-art performance. 3 Related work Predictive Recurrent Neural Semantic Frame Model Our goal is to use unlabeled"
I17-1010,N15-1001,0,0.0248588,"es the related work. Second, section 3 proposes the predictive recurrent neural semantic frame model including the formal definition, architecture, and an algorithm to extract selectional preferences from the trained model. Third, in section 4, we introduce the application of our PRNSFM in implicit semantic role labeling. Fourth, the experimental results and discussions are presented in section 5. Finally, we conclude our work and suggest some future work in section 6. 2 processing tasks, not semantic frame processing. Semantic Role Labeling In unsupervised SRL, Woodsend and Lapata (2015) and Titov and Khoddam (2015) induce embeddings to represent a predicate and its arguments from unannotated texts, but in their approaches, the arguments are words only, not the semantic role labels, while in our models, both are considered. Low-resource Implicit Semantic Role Labeling Several approaches have attempted to address the lack of resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constructed resources: an explicit S"
I17-1010,D15-1295,0,0.0211042,"llows: First, section 2 describes the related work. Second, section 3 proposes the predictive recurrent neural semantic frame model including the formal definition, architecture, and an algorithm to extract selectional preferences from the trained model. Third, in section 4, we introduce the application of our PRNSFM in implicit semantic role labeling. Fourth, the experimental results and discussions are presented in section 5. Finally, we conclude our work and suggest some future work in section 6. 2 processing tasks, not semantic frame processing. Semantic Role Labeling In unsupervised SRL, Woodsend and Lapata (2015) and Titov and Khoddam (2015) induce embeddings to represent a predicate and its arguments from unannotated texts, but in their approaches, the arguments are words only, not the semantic role labels, while in our models, both are considered. Low-resource Implicit Semantic Role Labeling Several approaches have attempted to address the lack of resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constru"
L16-1541,W10-1908,0,0.032136,"First, we present our author profiling dataset for health support forums. Our corpus is the first of its kind for author profiling of medical forum data. Second, we motivate author profile analysis in this type of data. Third, we propose a system that can predict age and gender of health forum users and present benchmarking results for future research in this area. We also discuss interesting findings about the salient topics in the different population groups. 2. Related Work Medical forum data has been used in a variety of different research works since it is a comprehensive source of data. Jha and Elhadad (2010) have tried to predict the cancer stage of a patient by using the text in their posts and their online behavior. They formulated the problem as a multi-class classification problem with four cancer stages. They used unigrams and bigrams as their text based features and also 3394 explored the use of network features with the hypothesis that patients in similar stages of cancer will interact more with each other. A combination of these three features gave them the best results. Rolia et al. (2013) tried to make predictions about the condition a person is suffering from based on similarities of t"
L16-1599,llorens-etal-2012-timen,0,0.0584891,"(Fischer and Str¨otgen, 2015) to mining patient records for potential causes of disease (Lin et al., 2014). The most popular scheme for annotating such normalized forms is ISO-TimeML (Pustejovsky et al., 2010), an extension of the TIDES annotation guidelines (Ferro et al., 2005). Figure 1 shows a sample of such annotations. Time expressions are annotated as phrases and the VALUE attribute indicates the normalized form. This type of annotation has formed the basis for a wide variety of annotated corpora, including the TimeBank (Pustejovsky et al., 2003), WikiWars (Mazur and Dale, 2010), TimeN (Llorens et al., 2012), and the various TempEval shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). TIMEX3 TIMEX3 TYPE =DATE TYPE =DATE VALUE =1996 VALUE =1995-05-22 ANCHORT IME On May 22, 1995 . . . and the following year Figure 1: ISO-TimeML annotation of times in a sentence from the TimeBank article APW19980418.0210 However, there are a few drawbacks of the ISO-TimeML approach. First, it struggles to represent times that do not align to a single calendar unit (day, week, month, etc.), such as the past three summers, since this cannot be described with some prefix of a YYYY-MM-DDTH"
L16-1599,D10-1089,0,0.457991,"m the study of literary texts (Fischer and Str¨otgen, 2015) to mining patient records for potential causes of disease (Lin et al., 2014). The most popular scheme for annotating such normalized forms is ISO-TimeML (Pustejovsky et al., 2010), an extension of the TIDES annotation guidelines (Ferro et al., 2005). Figure 1 shows a sample of such annotations. Time expressions are annotated as phrases and the VALUE attribute indicates the normalized form. This type of annotation has formed the basis for a wide variety of annotated corpora, including the TimeBank (Pustejovsky et al., 2003), WikiWars (Mazur and Dale, 2010), TimeN (Llorens et al., 2012), and the various TempEval shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). TIMEX3 TIMEX3 TYPE =DATE TYPE =DATE VALUE =1996 VALUE =1995-05-22 ANCHORT IME On May 22, 1995 . . . and the following year Figure 1: ISO-TimeML annotation of times in a sentence from the TimeBank article APW19980418.0210 However, there are a few drawbacks of the ISO-TimeML approach. First, it struggles to represent times that do not align to a single calendar unit (day, week, month, etc.), such as the past three summers, since this cannot be described with"
L16-1599,pustejovsky-etal-2010-iso,0,0.137447,"ment of 0.821. Keywords: time expressions, normalization, compositionality 1. Introduction Time normalization is the task of translating natural language expressions of time, such as three days ago, to computer-readable forms, such as 2015-10-12. Accurate time normalization is critical for enabling temporallyconstrained search over free text language resources. Applications range from the study of literary texts (Fischer and Str¨otgen, 2015) to mining patient records for potential causes of disease (Lin et al., 2014). The most popular scheme for annotating such normalized forms is ISO-TimeML (Pustejovsky et al., 2010), an extension of the TIDES annotation guidelines (Ferro et al., 2005). Figure 1 shows a sample of such annotations. Time expressions are annotated as phrases and the VALUE attribute indicates the normalized form. This type of annotation has formed the basis for a wide variety of annotated corpora, including the TimeBank (Pustejovsky et al., 2003), WikiWars (Mazur and Dale, 2010), TimeN (Llorens et al., 2012), and the various TempEval shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). TIMEX3 TIMEX3 TYPE =DATE TYPE =DATE VALUE =1996 VALUE =1995-05-22 ANCHORT IME"
L16-1599,Q14-1012,1,0.896499,"Missing"
L16-1599,S13-2001,0,0.150354,"The most popular scheme for annotating such normalized forms is ISO-TimeML (Pustejovsky et al., 2010), an extension of the TIDES annotation guidelines (Ferro et al., 2005). Figure 1 shows a sample of such annotations. Time expressions are annotated as phrases and the VALUE attribute indicates the normalized form. This type of annotation has formed the basis for a wide variety of annotated corpora, including the TimeBank (Pustejovsky et al., 2003), WikiWars (Mazur and Dale, 2010), TimeN (Llorens et al., 2012), and the various TempEval shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). TIMEX3 TIMEX3 TYPE =DATE TYPE =DATE VALUE =1996 VALUE =1995-05-22 ANCHORT IME On May 22, 1995 . . . and the following year Figure 1: ISO-TimeML annotation of times in a sentence from the TimeBank article APW19980418.0210 However, there are a few drawbacks of the ISO-TimeML approach. First, it struggles to represent times that do not align to a single calendar unit (day, week, month, etc.), such as the past three summers, since this cannot be described with some prefix of a YYYY-MM-DDTHH:MM:SS date-time. Second, it does not permit times to be defined relative to events (only relative to times"
L16-1599,S07-1014,0,0.0385896,"tential causes of disease (Lin et al., 2014). The most popular scheme for annotating such normalized forms is ISO-TimeML (Pustejovsky et al., 2010), an extension of the TIDES annotation guidelines (Ferro et al., 2005). Figure 1 shows a sample of such annotations. Time expressions are annotated as phrases and the VALUE attribute indicates the normalized form. This type of annotation has formed the basis for a wide variety of annotated corpora, including the TimeBank (Pustejovsky et al., 2003), WikiWars (Mazur and Dale, 2010), TimeN (Llorens et al., 2012), and the various TempEval shared tasks (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). TIMEX3 TIMEX3 TYPE =DATE TYPE =DATE VALUE =1996 VALUE =1995-05-22 ANCHORT IME On May 22, 1995 . . . and the following year Figure 1: ISO-TimeML annotation of times in a sentence from the TimeBank article APW19980418.0210 However, there are a few drawbacks of the ISO-TimeML approach. First, it struggles to represent times that do not align to a single calendar unit (day, week, month, etc.), such as the past three summers, since this cannot be described with some prefix of a YYYY-MM-DDTHH:MM:SS date-time. Second, it does not permit times to be defi"
L16-1599,S10-1010,0,\N,Missing
L16-1599,N13-3004,0,\N,Missing
N15-1010,P11-1030,1,0.796217,"le-domain corpus, where there is only one topic that all authors are writing about, and a multi-domain corpus, where there are multiple different topics. The latter allows us to test the generalization of AA models, by testing them on a different topic from that used for training. The first collection is the CCAT topic class, a subset of the Reuters Corpus Volume 1 (Lewis et al., 2004). Although this collection was not gathered for the goal of doing authorship attribution studies, previous work has reported results for AA with 10 and 50 authors (Stamatatos, 2008; Plakias and Stamatatos, 2008; Escalante et al., 2011). We refer to these as CCAT 10 and CCAT 50, respectively. Both CCAT 10 and CCAT 50 belong to CCAT category (about corporate/industrial news) and are balanced across authors, with 100 documents sampled for each author. Manual inspection of the dataset revealed that some of the authors in this collection consistently used signatures at the end of documents. Also, we noticed some writers use quotations a lot. ConCCAT 10 CCAT 50 Guardian1 Guardian2 10 50 13 13 SC Category prefix suffix space-prefix space-suffix whole-word mid-word multi-word beg-punct mid-punct end-punct #docs #sentences #words /a"
N15-1010,W14-0908,0,0.265256,"Missing"
N15-1010,C08-1065,0,0.338084,"Missing"
N15-1010,J00-4001,0,0.172465,"dra,bethard}@cis.uab.edu Manuel Montes-y-G´omez Instituto Nacional de Astrof´ısica Optica y Electr´onica Puebla, Mexico mmontesg@ccc.inaoep.mx Thamar Solorio University of Houston 4800 Calhoun Rd Houston, TX, 77004, USA solorio@cs.uh.edu Abstract tential candidate authors have an important effect on the accuracy of AA approaches (Moore, 2001; Luyckx and Daelemans, 2008; Luyckx and Daelemans, 2010). We can also point out the most common features that have been used successfully in AA work, including: bag-of-words (Madigan et al., 2005; Stamatatos, 2006), stylistic features (Zheng et al., 2006; Stamatatos et al., 2000), and word and character level n-grams (Kjell et al., 1994; Keselj et al., 2003; Peng et al., 2003; Juola, 2006). Character n-grams have been identified as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood. We identify subgroups of character n-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style. We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and"
N19-1274,C18-1139,0,0.160093,"porting paragraphs from a knowledge base (KB) given a question and candidate answer. Then AHE aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency (IDF) of the corresponding question/answer term. AHE’s overall alignment score is the sum of the IDF weighted scores of each of the question/answer term. Importantly, AHE’s alignment function operates over contextualized embeddings that model the underlying text at different levels of abstraction: character (FLAIR) (Akbik et al., 2018), word (BERT) (Devlin et al., 2018), and sentence (InferSent) (Conneau et al., 2017), where the latter is the only supervised component in the proposed approach. The different representations are combined through an ensemble approach that by default is unsupervised (using a variant of the NoisyOr formula), but can be replaced with a supervised meta-classifier. The contributions of our work are the following: 1. To our knowledge, this is the first unsupervised alignment approach for QA that: (a) operates over contextualized embeddings, and (b) captures text at multiple levels of abstraction, in"
N19-1274,P06-4018,0,0.0432538,". We use as query the question concatenated with the corresponding answer candidate, and BM25 (Robertson et al., 2009) as the ranking function3 . For each query, we keep the top C Lucene documents, where each document consists of a sentence retrieved from the ARC corpus. Similar to our previous work (Yadav et al., 2018), we boost candidate answer terms by a factor of 3 while keeping question terms as it is in the BM25 ranking function. All texts were preprocessed by discarding the case of the tokens, removing the stop words from Lucene’s list, and lemmatizing the remaining tokens using NLTK (Bird, 2006). For all experiments reported on the ARC dataset we used C = 20. Here we also calculate the IDF of each query term qi (required later during alignment): N − docfreq(qi ) + 0.5 idf (qi ) = log (1) docfreq(qi ) + 0.5 where N is the number of documents (e.g., 14.3M for the ARC KB) and docf req(qi ) is the number of documents that contain qi . 3.2 Alignment Algorithm For representations that produce word embeddings (e.g., FLAIR, BERT, GloVe), we use the alignment algorithm in Figure 3. Our method computes the alignment score of each query token with every token in the given KB paragraph, using th"
N19-1274,J93-2003,0,0.0953459,"017), etc. Other works have utilized word alignments as features in supervised models (Surdeanu et al., 2011; Wang and Ittycheriah, 2015). For example, Wang and Ittycheriah (2015) utilized the alignment of words between two questions as a feature in a feedforward neural network that matches similar FAQ questions. Recently, Yadav et al. (2018) showed that alignment methods remain competitive for non-factoid QA. However, the majority of alignment models that rely on representation learning utilize uncontextualized word embeddings such as GloVe, coupled with other BoW models such as IBM Model 1 (Brown et al., 1993) for alignment (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018). To our knowledge, we are the first to adapt these ideas to contextualized embeddings, which mitigates the BoW limitations of previous efforts (as shown in Figure 1). While contextualized representations have been shown to be extremely useful for multiple NLP tasks (Devlin et al., 2018; Peters et al., 2018; Howard and Ruder, 2018), our work is the first to apply them to an unsupervised alignment approach. Further, we show that different contextualized representations of text (character, word, sentence) capture com"
N19-1274,P16-1223,0,0.0346033,"nces containing the correct answers (Yang et al., 2015). Alignment models have also been proposed for other types of QA, such as reading comprehension (RC) QA (Chakravarti et al., 2017). We believe AHE can be similarly extended to RC, but, in this work, we have limited our experiments to answer selection and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Kumar et al., 2016; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely 2682 on annotated training data, and, some, on structured"
N19-1274,D16-1053,0,0.0603275,"Missing"
N19-1274,D17-1070,0,0.458125,"r. Then AHE aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph, and weighs each alignment score with the inverse document frequency (IDF) of the corresponding question/answer term. AHE’s overall alignment score is the sum of the IDF weighted scores of each of the question/answer term. Importantly, AHE’s alignment function operates over contextualized embeddings that model the underlying text at different levels of abstraction: character (FLAIR) (Akbik et al., 2018), word (BERT) (Devlin et al., 2018), and sentence (InferSent) (Conneau et al., 2017), where the latter is the only supervised component in the proposed approach. The different representations are combined through an ensemble approach that by default is unsupervised (using a variant of the NoisyOr formula), but can be replaced with a supervised meta-classifier. The contributions of our work are the following: 1. To our knowledge, this is the first unsupervised alignment approach for QA that: (a) operates over contextualized embeddings, and (b) captures text at multiple levels of abstraction, including character, word, and sentence. 2. We obtain (near) state-of-the-art results"
N19-1274,N16-1108,0,0.396075,"(Clark et al., 2018); or answer sentence selection, where candidate answer sentences are provided and the task is to select the sentences containing the correct answers (Yang et al., 2015). Alignment models have also been proposed for other types of QA, such as reading comprehension (RC) QA (Chakravarti et al., 2017). We believe AHE can be similarly extended to RC, but, in this work, we have limited our experiments to answer selection and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Kumar et al., 2016; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves"
N19-1274,P18-1031,0,0.0159015,"QA. However, the majority of alignment models that rely on representation learning utilize uncontextualized word embeddings such as GloVe, coupled with other BoW models such as IBM Model 1 (Brown et al., 1993) for alignment (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018). To our knowledge, we are the first to adapt these ideas to contextualized embeddings, which mitigates the BoW limitations of previous efforts (as shown in Figure 1). While contextualized representations have been shown to be extremely useful for multiple NLP tasks (Devlin et al., 2018; Peters et al., 2018; Howard and Ruder, 2018), our work is the first to apply them to an unsupervised alignment approach. Further, we show that different contextualized representations of text (character, word, sentence) capture complementary information, and combining them improves performance further. 3 Approach The core component of our approach computes the score of a candidate answer by aligning two texts. For multiple-choice questions, the first text consists of the question concatenated with the candidate answer, and the second is a supporting paragraph such as the one shown in Figure 1, which consists of one or more sentences ret"
N19-1274,P16-1045,0,0.0213277,"2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely 2682 on annotated training data, and, some, on structured KBs, which are expensive to create (Jauhar et al., 2016). Further, as we demonstrate in Section 5, these methods tend to be tailored to a specific dataset and do not port well to other domains or even within different splits of the same dataset. In contrast, our method is mostly unsupervised and does not require training. Even then, our approach performs well on three distinct QA datasets, with top three performance in all. Our work is inspired by previous efforts on using alignment methods for NLP (Echihabi and Marcu, 2003). Unsupervised alignment models have been proposed for several NLP tasks such as short text similarity (Kenter and De Rijke, 2"
N19-1274,P03-1003,0,0.461272,"b.com/vikas95/AHE electrical → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representations (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018), they generally underperfom supervised neural methods due to their underlying bag-of-word (BoW) assumptions and reliance on uncontextualized word representations such as GloVe (Pennington et al., 2014). In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations, which mitigate the above BoW limitations. For example, Figure"
N19-1274,D18-1260,0,0.212973,"ed text text, structured – text, structured text, structured text, structured 18 19 20 21 No No Minimal Minimal text text text text 22 23 Yes Yes text text 24 – text Model Baselines Random baseline AI2 IR Solver (Clark et al., 2018) AI2 IR Solver (our implementation) Sanity Check (Yadav et al., 2018) AHE over GloVe AHE over FLAIR AHE over BERT AHE over InferSent (trained on NLI) AHE over InferSent (trained on ARC) Previous work Tuple-Inf (Clark et al., 2018) Decomp-att (Clark et al., 2018) DGEM (Clark et al., 2018) BiDAF (for ARC) (Clark et al., 2018) KG2 (Zhang et al., 2018) Bi-LSTM max-out (Mihaylov et al., 2018) NCRF++/match-LSTM (Musa et al., 2018) TriAN+f(dir)(cs)+f(ind)(cs) (Zhong et al., 2018b) ET-RR (Ni et al., 2018) Unsupervised AHE AHE (FLAIR+BERT) AHE (FLAIR+BERT+GloVe) AHE (FLAIR+BERT+InferSent) AHE (FLAIR+BERT+GloVe+InferSent) Supervised AHE AHE (FLAIR+BERT+GloVe+InferSent) AHE (FLAIR+BERT+GloVe+InferSent) with grade Oracle Oracle ensemble (FLAIR+BERT+GloVe+InferSent) Easy P@1 Challenge P@1 25.02 59.99 60.31 58.36 60.71 62.29 62.73 32.13 54.01 25.02 23.98 23.74 26.56 28.75 31.05 32.76 25.36 31.66 60.71 52.95 58.97 51.05 34.26 52.22 - 23.83 24.40 27.11 26.54 31.70 33.87 33.20 33.39 36.56 63."
N19-1274,D16-1166,0,0.0313402,"ng et al., 2015). Alignment models have also been proposed for other types of QA, such as reading comprehension (RC) QA (Chakravarti et al., 2017). We believe AHE can be similarly extended to RC, but, in this work, we have limited our experiments to answer selection and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Kumar et al., 2016; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely 2682 on annotated training data, and, some, on structured KBs, which are expensive to create (Jauha"
N19-1274,D16-1147,0,0.175913,"7 No Yes No No No No Yes 8 9 10 11 12 13 14 15 16 17 18 19 20 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 21 22 23 24 No No Minimal Minimal 25 Yes 26 – Model Baselines Wgt Word Cnt (Yang et al., 2015) LCLR (Yang et al., 2015) Sanity Check (Yadav et al., 2018) AHE over GloVe AHE over FLAIR AHE over BERT AHE over InferSent Previous work CNN+Cnt (Yang et al., 2015) RNN-1way (Jurczyk et al., 2016) RNN-Attention pool (Jurczyk et al., 2016) CNN (avg + emb) (Jurczyk et al., 2016) AP-CNN (dos Santos et al., 2016) LSTM-att (Miao et al., 2016) ABCNN (Yin et al., 2015) Key-value memory network (Miller et al., 2016) CubeCNN (He and Lin, 2016) BiMPM (Wang et al., 2017b) (Tymoshenko et al., 2017) Compare-Aggregate (Wang and Jiang, 2016) (Li et al., 2018) Unsupervised AHE AHE (FLAIR+BERT) AHE (FLAIR+BERT+Glove) AHE (FLAIR+BERT+InferSent) AHE (FLAIR+BERT+Glove+InferSent) Supervised AHE AHE (FLAIR+BERT+Glove+InferSent) Oracle Oracle ensemble (FLAIR+BERT+Glove+InferSent) MAP MRR 50.99 59.93 64.02 63.40 64.91 65.13 66.93 51.32 60.86 65.39 66.51 66.40 68.70 65.20 66.64 67.47 68.78 68.86 68.86 69.21 70.69 70.90 71.80 72.19 74.33 75.41 66.52 68.70 68.92 70.82 69.57 70.69 71.08 72.65 72.34 73.10 74.08 75.45 76.59 6"
N19-1274,D17-1061,0,0.0296577,"election and multiple-choice QA tasks. Most QA approaches today use neural, supervised methods. Most use stacked architectures usually coupled with attention mechanisms (He and Lin, 2016; Yin et al., 2015; Seo et al., 2016; Xiong et al., 2016b; Kumar et al., 2016; Tan et al., 2015; Wang et al., 2017a; Chen et al., 2016; Cheng et al., 2016; Golub and He, 2016). Some of these works also rely on structured knowledge bases (Zhong et al., 2018a; Ni et al., 2018) such as ConceptNet (Speer et al., 2017). Some approaches use query expansion methods in addition to the above methods (Musa et al., 2018; Nogueira and Cho, 2017; Ni et al., 2018). For example, Musa et al. (2018) used a sequence to sequence model (Sutskever et al., 2014) to generate an enhanced query for ARC which retrieves better supporting passages. However, in general, all these approaches rely 2682 on annotated training data, and, some, on structured KBs, which are expensive to create (Jauhar et al., 2016). Further, as we demonstrate in Section 5, these methods tend to be tailored to a specific dataset and do not port well to other domains or even within different splits of the same dataset. In contrast, our method is mostly unsupervised and does"
N19-1274,D14-1162,0,0.0982023,"s that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representations (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018), they generally underperfom supervised neural methods due to their underlying bag-of-word (BoW) assumptions and reliance on uncontextualized word representations such as GloVe (Pennington et al., 2014). In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations, which mitigate the above BoW limitations. For example, Figure 1 shows an example of a question from AI2’s Reasoning Challenge (ARC) dataset (Clark et al., 2018), 2681 Proceedings of NAACL-HLT 2019, pages 2681–2691 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics which is not answered correctly by a state-of-theart BoW alignment method (Yadav et al., 2018), but is correctly answered by our alignment approach when oper"
N19-1274,N18-1202,0,0.0242007,"itive for non-factoid QA. However, the majority of alignment models that rely on representation learning utilize uncontextualized word embeddings such as GloVe, coupled with other BoW models such as IBM Model 1 (Brown et al., 1993) for alignment (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018). To our knowledge, we are the first to adapt these ideas to contextualized embeddings, which mitigates the BoW limitations of previous efforts (as shown in Figure 1). While contextualized representations have been shown to be extremely useful for multiple NLP tasks (Devlin et al., 2018; Peters et al., 2018; Howard and Ruder, 2018), our work is the first to apply them to an unsupervised alignment approach. Further, we show that different contextualized representations of text (character, word, sentence) capture complementary information, and combining them improves performance further. 3 Approach The core component of our approach computes the score of a candidate answer by aligning two texts. For multiple-choice questions, the first text consists of the question concatenated with the candidate answer, and the second is a supporting paragraph such as the one shown in Figure 1, which consists of"
N19-1274,J11-2003,1,0.876306,"al → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representations (Kenter and De Rijke, 2015; Kim et al., 2017; Yadav et al., 2018), they generally underperfom supervised neural methods due to their underlying bag-of-word (BoW) assumptions and reliance on uncontextualized word representations such as GloVe (Pennington et al., 2014). In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations, which mitigate the above BoW limitations. For example, Figure 1 shows an example of"
N19-1274,D17-1093,0,0.299413,"rect sequence, and cannot be answered correctly when relying on uncontextualized embeddings. Introduction The “deep learning tsunami”(Manning, 2015) has had a major impact on important natural language processing (NLP) applications such as question answering (QA). Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks (Seo et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; 1 Code: https://github.com/vikas95/AHE electrical → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representatio"
N19-1274,P18-1158,0,0.0204218,"y when relying on uncontextualized embeddings. Introduction The “deep learning tsunami”(Manning, 2015) has had a major impact on important natural language processing (NLP) applications such as question answering (QA). Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks (Seo et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; 1 Code: https://github.com/vikas95/AHE electrical → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite their initial successes (Echihabi and Marcu, 2003; Surdeanu et al., 2011, inter alia). While a few recent efforts have adapted these alignment methods to operate over word representations (Kenter and De Rijke, 2015; Kim et al"
N19-1274,P17-1018,0,0.3508,"e ARC dataset with the correct answer in bold font. This question is answered correctly by our alignment method that relies on contextualized word embeddings that capture the correct sequence, and cannot be answered correctly when relying on uncontextualized embeddings. Introduction The “deep learning tsunami”(Manning, 2015) has had a major impact on important natural language processing (NLP) applications such as question answering (QA). Many neural approaches for QA have been proposed in the past few years, with impressive results on several QA tasks (Seo et al., 2016; Wang and Jiang, 2016; Wang et al., 2017b; 1 Code: https://github.com/vikas95/AHE electrical → light → chemical electrical →chemical → light chemical → light → electrical chemical → electrical → light Tymoshenko et al., 2017; Xiong et al., 2016a; Wang et al., 2018; Radford et al., 2018; Li et al., 2018, inter alia). However, an undesired effect of this focus on neural approaches was that other methods have fallen out of focus, including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches. For instance, alignment approaches have received considerably less interest recently, despite t"
N19-1274,D15-1237,0,0.0930922,"Missing"
N19-4008,P98-1013,0,0.252612,"Missing"
N19-4008,L18-1529,1,0.864912,"Missing"
N19-4008,E12-2021,0,0.0648887,"Missing"
N19-4008,L16-1599,1,0.888366,"Missing"
N19-4008,L16-1050,1,0.530295,"Missing"
N19-4008,C18-1182,1,0.857504,"Missing"
N19-4008,P17-4018,1,0.857295,"Missing"
N19-4008,S19-2232,1,0.846579,"Missing"
N19-4008,Q18-1025,1,0.846998,"Missing"
P08-2045,W06-1618,1,0.880455,"Missing"
P08-2045,bethard-etal-2008-building,1,0.809728,"Missing"
P08-2045,S07-1003,0,0.0539268,"Missing"
P08-2045,W03-1210,0,0.380817,"Missing"
P08-2045,kingsbury-palmer-2002-treebank,0,0.125292,"Missing"
P08-2045,S07-1014,0,0.0554537,"Missing"
P11-2047,N07-1053,0,0.0340212,"Missing"
P11-2047,W99-0613,0,0.0271612,"(LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work • Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Surdeanu et al., 2006). The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi-supervised approach to time expression recognition. They begin by selecting 100 time expressions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding tokens, parts-of-speech, syntactic chunks etc.) and t"
P11-2047,W09-2207,0,0.0228935,"Missing"
P11-2047,D09-1003,1,0.935715,"zer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are available explicitly in the newswire training data. We therefore introduce a semisupervised approach for expanding the training data, where we take words from temporal expressions in the data, substitute these words with likely synonyms, and add the generated examples to the training set. We select synonyms both via WordNet, and via predictions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work • Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and doc"
P11-2047,S10-1071,0,0.0136579,"be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strötgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are ava"
P11-2047,C96-1079,0,0.0125111,"Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia. 1 Introduction The recognition of time expressions such as April 2011, mid-September and early next week is a crucial first step for applications like question answering that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strötgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the s"
P11-2047,W06-2207,0,0.036447,"Missing"
P11-2047,S10-1072,1,0.85663,"training corpus for learning a supervised model rather than for selecting high precision seeds, we generate additional training examples using synonyms rather than bootstrapping based on patterns, and we evaluate on Reuters and Wikipedia data that differ from the domain on which our model was trained. 3 Method The proposed method implements a supervised machine learning approach that classifies each chunk-phrase candidate top-down starting at the parse tree root provided by the OpenNLP parser. Time expressions are identified as phrasal chunks with spans derived from the parse as described in (Kolomiyets and Moens, 2010). 3.1 Basic TempEval Model We implemented a logistic regression model with the following features for each phrase-candidate: • The head word of the phrase • The part-of-speech tag of the head word • All tokens and part-of-speech tags in the phrase as a bag of words 272 • • • 3.2 The word-shape representation of the head word and the entire phrase, e.g. Xxxxx 99 for the expression April 30 The condensed word-shape representation for the head word and the entire phrase, e.g. X(x) (9) for the expression April 30 The concatenated string of the syntactic types of the children of the phrase in the p"
P11-2047,S10-1063,0,0.0182554,"ally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strötgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are available explicitly in th"
P11-2047,P95-1026,0,0.194589,"tions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work • Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Surdeanu et al., 2006). The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi-supervised approach to time expression recognition. They begin by selecting 100 time expressions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding"
P11-2047,P00-1010,0,\N,Missing
P11-2047,S10-1010,0,\N,Missing
P12-1010,S07-1025,1,0.760854,"Missing"
P12-1010,P12-1010,1,0.106103,"Missing"
P12-1010,W06-1623,0,0.541437,"Missing"
P12-1010,D08-1073,0,0.555514,"Missing"
P12-1010,S07-1052,0,0.459892,"Missing"
P12-1010,P09-2093,0,0.0167344,"Missing"
P12-1010,W11-0116,1,0.804338,"Missing"
P12-1010,S10-1063,0,0.187638,"Missing"
P12-1010,P09-1025,0,0.0231566,"Missing"
P12-1010,P06-1050,0,0.222722,"Missing"
P12-1010,W11-0419,0,0.0439569,"Missing"
P12-1010,D11-1036,0,0.0573,"Missing"
P12-1010,S10-1062,0,0.388708,"Missing"
P12-1010,W03-3023,0,\N,Missing
P12-1010,S07-1014,0,\N,Missing
P12-1010,J08-4003,0,\N,Missing
P12-1010,H05-1066,0,\N,Missing
P12-1010,S10-1010,0,\N,Missing
P12-1010,P09-1046,0,\N,Missing
P12-1010,bethard-etal-2012-annotating,1,\N,Missing
P14-2014,S07-1025,1,0.791506,"cat] recording the index of a neighbor node. VP 2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and ba"
P14-2014,S13-2002,1,0.913047,"by taking advantage of relatively strict English word ordering. Like SST and PTK, the DPK requires the root category of two subtrees to be the same for the similarity to be greater than zero. Unlike SST and PTK, once the root category comparison is successfully completed, DPK looks at all paths that go through it and accumulates their similarity scores independent of ordering – in other words, it will ignore the ordering of the children in its pro4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the C ONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: KC (s1 , s2 ) = τ P X p=1 KT (tp1 , tp2 )+KF (f1 , f2 ) (2) where si is an instance object composed of flat features fi and a syntactic tree ti . A syntactic tree ti 84 can have multiple representations, as in Bag Tree"
P14-2014,strassel-etal-2010-darpa,0,0.0132148,"sentations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal relation discovery performance on THYME data2 (Styler et al., 2014). In this paper, we also use syntactic structure-enriched temporal relation discovery as a vehicle to test our proposed kernel. here Figure 1: Three example tree pairs. matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates"
P14-2014,S13-2012,0,0.0248395,"2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between"
P14-2014,P07-1026,0,0.0305882,"discovery as a vehicle to test our proposed kernel. here Figure 1: Three example tree pairs. matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates a very large but sparse feature space. To Figure 1(b), the PTK generates fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a] [NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among others, for the second tree. This allows for partial matching – substructure (ii) – while also generating some fragments that violate grammatical intuitions. Zhang et al. (2007) address the restrictiveness of SST by allowing soft matching of production rules. They allow partial matching of optional nodes based on the Treebank. For example, the rule N P → DT JJ N N indicates a noun phrase consisting of a determiner, adjective, and common noun. Zhang et al.’s method designates the JJ as optional, since the Treebank contains instances of a reduced version of the rule without the JJ node (N P → DT N N ). They also allow node matching among similar preterminals such as JJ, JJR, and JJS, mapping them to one equivalence class. Other relevant approaches are the spectrum tree"
P14-2014,E12-1019,0,0.0156425,"m events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal relation discovery performance on THYME data2 (Styler et al., 2014). In this paper, we also use syntactic structure-enric"
P14-2014,S10-1063,0,0.0247263,"f a neighbor node. VP 2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech ta"
P14-2014,W13-1903,1,0.87219,"thinking of it as cheaply calculating rule production similarity by taking advantage of relatively strict English word ordering. Like SST and PTK, the DPK requires the root category of two subtrees to be the same for the similarity to be greater than zero. Unlike SST and PTK, once the root category comparison is successfully completed, DPK looks at all paths that go through it and accumulates their similarity scores independent of ordering – in other words, it will ignore the ordering of the children in its pro4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the C ONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: KC (s1 , s2 ) = τ P X p=1 KT (tp1 , tp2 )+KF (f1 , f2 ) (2) where si is an instance object composed of flat features fi and a syntactic tree ti . A syntactic tre"
P14-2014,Y09-1038,0,0.0260023,"comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al."
P14-2082,bethard-etal-2008-building,1,0.363636,"Missing"
P14-2082,S13-2002,1,0.796379,"did not appear that anyone else was injured. The other customers fled, and the police said it did not appear that anyone else was injured. Figure 1: A TimeBank annotated document is on the left, and this paper’s TimeBank-Dense annotation is on the right. Solid arrows indicate BEFORE relations and dotted arrows indicate INCLUDED IN relations. Events Times Rels R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval-07 6832 1249 5790 0.7 TempEval-10 5688 2117 4907 0.6 TempEval-13 11145 2078 11098 0.8 Kolomiyets-12 1233 – 1139 0.9 Do 20122 324 232 3132 5.6 This work 1729 289 12715 6.3 (Bethard, 2013). We describe the first annotation framework that forces annotators to annotate all pairs1 . With this new process, we created a dense ordering of document events that can properly evaluate both relation identification and relation annotation. Figure 1 illustrates one document before and after our new annotations. 2 Previous Annotation Work The majority of corpora and competitions for event ordering contain sparse annotations. Annotators for the original TimeBank (Pustejovsky et al., 2003) only annotated relations judged to be salient by the annotator. Subsequent TempEval competitions (Verhage"
P14-2082,P09-1046,0,0.229847,"ion is to assume that they are vague or ambiguous. However, all 6 edges have clear well-defined ordering relations: belonged BEFORE confirmed belonged BEFORE found found BEFORE confirmed belonged BEFORE Friday confirmed IS INCLUDED IN Friday found IS INCLUDED IN Friday3 Learning algorithms handle these unlabeled edges by making incorrect assumptions, or by ignoring large parts of the temporal graph. Several models with rich temporal reasoners have been published, but since they require more connected graphs, improvement over pairwise classifiers have been minimal (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This paper thus proposes an annotation process that builds denser graphs with formal properties that learners can rely on, such as locally complete subgraphs. 3.1 edges is prohibitive. We approximate completeness by creating locally complete graphs over neighboring sentences. The resulting event graph for a document is strongly connected, but not complete. Specifically, the following edge types are included: 1. Event-Event, Event-Time, and Time-Time pairs in the same sentence 2. Event-Event, Event-Time, and Time-Time pairs between the current and next sentence 3. Event-DCT pairs for every ev"
P14-2082,W06-1623,0,0.656964,"notator did not look at the pair of events, so a relation may or may not exist. 3. The annotator failed to look at the pair of events, so a single relation may exist. Training and evaluation of temporal reasoners is hampered by this ambiguity. To combat this, our 1 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be stro"
P14-2082,D08-1073,1,0.849299,"he 3 unlabeled edges. One option is to assume that they are vague or ambiguous. However, all 6 edges have clear well-defined ordering relations: belonged BEFORE confirmed belonged BEFORE found found BEFORE confirmed belonged BEFORE Friday confirmed IS INCLUDED IN Friday found IS INCLUDED IN Friday3 Learning algorithms handle these unlabeled edges by making incorrect assumptions, or by ignoring large parts of the temporal graph. Several models with rich temporal reasoners have been published, but since they require more connected graphs, improvement over pairwise classifiers have been minimal (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This paper thus proposes an annotation process that builds denser graphs with formal properties that learners can rely on, such as locally complete subgraphs. 3.1 edges is prohibitive. We approximate completeness by creating locally complete graphs over neighboring sentences. The resulting event graph for a document is strongly connected, but not complete. Specifically, the following edge types are included: 1. Event-Event, Event-Time, and Time-Time pairs in the same sentence 2. Event-Event, Event-Time, and Time-Time pairs between the current and next sentence 3. Eve"
P14-2082,D12-1062,0,0.602665,"his ambiguity. To combat this, our 1 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be strongly connected. Table 1 compares the size and density of our corpus to others. Ours is the densest and it contains the largest number of temporal relations. 2 Do et al. (2012) reports 6264 relations, but this includes both t"
P14-2082,P12-1010,1,0.580125,"failed to look at the pair of events, so a single relation may exist. Training and evaluation of temporal reasoners is hampered by this ambiguity. To combat this, our 1 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be strongly connected. Table 1 compares the size and density of our corpus to others. Ours is the densest a"
P14-2082,S13-2001,0,0.0782729,"syntactic positions: events and times in the same noun phrase, main events in consecutive sentences, etc. We now aim for a shift in the community wherein all pairs are considered candidates for temporal ordering, allowing researchers to ask questions such as: how must algorithms adapt to label the complete graph of pairs, and if the more difficult and ambiguous event pairs are included, how must feature-based learners change? We are not the first to propose these questions, but this paper is the first to directly propose the means by which they can be addressed. The stated goal of TempEval-3 (UzZaman et al., 2013) was to focus on relation identification instead of classification, but the training and evaluation data followed the TimeBank approach where only a subset of event pairs were labeled. As a result, many systems focused on classification, with the top system classifying pairs in only three syntactic constructions 501 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 501–506, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Current Systems & Evaluations This Proposal There were four or five peo"
P14-2082,S07-1014,0,0.080368,"s on the TimeBank setup. This paper addresses one of its shortcomings: sparse annotation. We describe a new annotation framework (and a TimeBank-Dense corpus) that we believe is needed to fulfill the data needs of deeper reasoners. The TimeBank includes a small subset of all possible relations in its documents. The annotators were instructed to label relations critical to the document’s understanding. The result is a sparse labeling that leaves much of the document unlabeled. The TempEval contests have largely followed suit and focused on specific types of event pairs. For instance, TempEval (Verhagen et al., 2007) only labeled relations between events that syntactically dominated each other. This paper is the first attempt to annotate a document’s entire temporal graph. A consequence of focusing on all relations is a shift from the traditional classification task, where the system is given a pair of events and asked only to label the type of relation, to an identification task, where the system must determine for itself which events in the document to pair up. For example, in TempEval-1 and 2 (Verhagen et al., 2007; Verhagen et al., 2010), systems were given event pairs in specific syntactic positions:"
P14-2082,S10-1010,0,\N,Missing
P14-5010,P05-1045,1,0.148222,"ides a PTBstyle tokenizer, extended to reasonably handle noisy and web text. The corresponding components for Chinese and Arabic provide word and clitic segmentation. The tokenizer saves the character offsets of each token in the input text. cleanxml Removes most or all XML tags from the document. ssplit Splits a sequence of tokens into sentences. truecase Determines the likely true case of tokens in text (that is, their likely case in well-edited text), where this information was lost, e.g., for all upper case text. This is implemented with a discriminative model using a CRF sequence tagger (Finkel et al., 2005). pos Labels tokens with their part-of-speech (POS) tag, using a maximum entropy POS tagger (Toutanova et al., 2003). lemma Generates the lemmas (base forms) for all tokens in the annotation. gender Adds likely gender information to names. ner Recognizes named (PERSON, LOCATION, ORGANIZATION, MISC) and numerical (MONEY, NUMBER, DATE, TIME, DURATION, SET) entities. With the default Figure 4: A simple, complete example program. annotators in a pipeline is controlled by standard Java properties in a Properties object. The most basic property to specify is what annotators to run, in what order, as"
P14-5010,J13-4004,1,0.147631,"(IDEOLOGY), nationalities (NATIONALITY), religions (RELIGION), and titles (TITLE). parse Provides full syntactic analysis, including both constituent and dependency representation, based on a probabilistic parser (Klein and Manning, 2003; de Marneffe et al., 2006). sentiment Sentiment analysis with a compositional model over trees using deep learning (Socher et al., 2013). Nodes of a binarized tree of each sentence, including, in particular, the root node of each sentence, are given a sentiment score. dcoref Implements mention detection and both pronominal and nominal coreference resolution (Lee et al., 2013). The entire coreference graph of a text (with head words of mentions as nodes) is provided in the Annotation. Figure 5: An example of a simple custom annotator. The annotator marks the words of possibly multi-word locations that are in a gazetteer. props.setProperty(""tokenize.whitespace"", ""true""); props.setProperty(""ssplit.eolonly"", ""true""); or via command-line flags: -tokenize.whitespace -ssplit.eolonly We do not attempt to describe all the properties understood by each annotator here; they are available in the documentation for Stanford CoreNLP. However, we note that they follow the pattern"
P14-5010,D13-1170,1,0.163373,"Annotator is to provide a simple framework to allow a user to incorporate NE labels that are not annotated in traditional NL corpora. For example, a default list of regular expressions that we distribute in the models file recognizes ideologies (IDEOLOGY), nationalities (NATIONALITY), religions (RELIGION), and titles (TITLE). parse Provides full syntactic analysis, including both constituent and dependency representation, based on a probabilistic parser (Klein and Manning, 2003; de Marneffe et al., 2006). sentiment Sentiment analysis with a compositional model over trees using deep learning (Socher et al., 2013). Nodes of a binarized tree of each sentence, including, in particular, the root node of each sentence, are given a sentiment score. dcoref Implements mention detection and both pronominal and nominal coreference resolution (Lee et al., 2013). The entire coreference graph of a text (with head words of mentions as nodes) is provided in the Annotation. Figure 5: An example of a simple custom annotator. The annotator marks the words of possibly multi-word locations that are in a gazetteer. props.setProperty(""tokenize.whitespace"", ""true""); props.setProperty(""ssplit.eolonly"", ""true""); or via comman"
P14-5010,bethard-etal-2014-cleartk,1,0.157881,"Missing"
P14-5010,N03-1033,1,0.145833,"nese and Arabic provide word and clitic segmentation. The tokenizer saves the character offsets of each token in the input text. cleanxml Removes most or all XML tags from the document. ssplit Splits a sequence of tokens into sentences. truecase Determines the likely true case of tokens in text (that is, their likely case in well-edited text), where this information was lost, e.g., for all upper case text. This is implemented with a discriminative model using a CRF sequence tagger (Finkel et al., 2005). pos Labels tokens with their part-of-speech (POS) tag, using a maximum entropy POS tagger (Toutanova et al., 2003). lemma Generates the lemmas (base forms) for all tokens in the annotation. gender Adds likely gender information to names. ner Recognizes named (PERSON, LOCATION, ORGANIZATION, MISC) and numerical (MONEY, NUMBER, DATE, TIME, DURATION, SET) entities. With the default Figure 4: A simple, complete example program. annotators in a pipeline is controlled by standard Java properties in a Properties object. The most basic property to specify is what annotators to run, in what order, as shown here. But as discussed below, most annotators have their own properties to allow further customization of the"
P14-5010,de-marneffe-etal-2006-generating,1,\N,Missing
P14-5010,chang-manning-2012-sutime,1,\N,Missing
P14-5010,clarke-etal-2012-nlp,0,\N,Missing
P14-5010,P02-1022,0,\N,Missing
P16-1210,W06-1615,0,0.571498,"io University of Houston solorio@cs.uh.edu Manuel Montes-y-G´omez Instituto Nacional de Astrof´ısica, Optica y Electr´onica mmontesg@inaoep.mx Steven Bethard University of Alabama at Birmingham bethard@uab.edu Abstract One of the scenarios that has received limited attention is cross-domain authorship attribution, when we need to identify the author of a text but all the text with known authors is from a different topic, genre, or modality. Here we propose to solve the problem of cross-domain authorship attribution by adapting the Structural Correspondence Learning (SCL) algorithm proposed by Blitzer et al. (2006). We make the following contributions: We present the first domain adaptation model for authorship attribution to leverage unlabeled data. The model includes extensions to structural correspondence learning needed to make it appropriate for the task. For example, we propose a median-based classification instead of the standard binary classification used in previous work. Our results show that punctuation-based character n-grams form excellent pivot features. We also show how singular value decomposition plays a critical role in achieving domain adaptation, and that replacing (instead of concat"
P16-1210,P07-1033,0,0.194974,"Missing"
P16-1210,P11-1030,1,0.872381,"he standard SCL formulation. The extensions to SCL that we propose in this work are likely to yield performance improvements in other tasks where SCL has been successfully applied, such as part-of-speech tagging and sentiment analysis. We plan to investigate this further in the future. 2 Related Work Cross-Domain Authorship Attribution Almost all previous authorship attribution studies have tackled traditional (single-domain) authorship problems where the distribution of the test data is the same as that of the training data (Madigan et al., 2005; Stamatatos, 2006; Luyckx and Daelemans, 2008; Escalante et al., 2011). However, there are a handful of authorship attribution studies that explore cross-domain authorship attribution scenarios (Mikros and Argiri, 2007; Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013; Sapkota et al., 2014). Here, following prior work, cross-domain is a cover term for cross-topic, cross-genre, cross-modality, etc., though most work focuses on the cross-topic scenario. Mikros and Argiri (2007) illustrated that many stylometric variables are actually discriminating topic rather than author. Therefore, the authors suggest their use in authorship attribution sho"
P16-1210,E09-1039,0,0.210101,"pplied, such as part-of-speech tagging and sentiment analysis. We plan to investigate this further in the future. 2 Related Work Cross-Domain Authorship Attribution Almost all previous authorship attribution studies have tackled traditional (single-domain) authorship problems where the distribution of the test data is the same as that of the training data (Madigan et al., 2005; Stamatatos, 2006; Luyckx and Daelemans, 2008; Escalante et al., 2011). However, there are a handful of authorship attribution studies that explore cross-domain authorship attribution scenarios (Mikros and Argiri, 2007; Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013; Sapkota et al., 2014). Here, following prior work, cross-domain is a cover term for cross-topic, cross-genre, cross-modality, etc., though most work focuses on the cross-topic scenario. Mikros and Argiri (2007) illustrated that many stylometric variables are actually discriminating topic rather than author. Therefore, the authors suggest their use in authorship attribution should be done with care. However, the study did not attempt to construct authorship attribution models where the source and target domains differ. Goldstein-Stewart et al. (2009) per"
P16-1210,C08-1065,0,0.612836,"Missing"
P16-1210,C14-1116,1,0.326485,"to investigate this further in the future. 2 Related Work Cross-Domain Authorship Attribution Almost all previous authorship attribution studies have tackled traditional (single-domain) authorship problems where the distribution of the test data is the same as that of the training data (Madigan et al., 2005; Stamatatos, 2006; Luyckx and Daelemans, 2008; Escalante et al., 2011). However, there are a handful of authorship attribution studies that explore cross-domain authorship attribution scenarios (Mikros and Argiri, 2007; Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013; Sapkota et al., 2014). Here, following prior work, cross-domain is a cover term for cross-topic, cross-genre, cross-modality, etc., though most work focuses on the cross-topic scenario. Mikros and Argiri (2007) illustrated that many stylometric variables are actually discriminating topic rather than author. Therefore, the authors suggest their use in authorship attribution should be done with care. However, the study did not attempt to construct authorship attribution models where the source and target domains differ. Goldstein-Stewart et al. (2009) performed a study on cross-topic authorship attribution by concat"
P16-1210,N15-1010,1,0.371067,"-grams used in authorship attribution that might make good pivot features. 3.2.1 Untyped Character N -grams Classical character n-grams are simply the sequences of characters in the text. For example, given the text: We propose to use as pivot features the p most frequent character n-grams. For non-pivot features, we use the remaining features from prior work (Sapkota et al., 2014). These include both the remaining (lower frequency) character n-grams, as well as stop-words and bag-of-words lexical features. We call this the untyped formulation of pivot features. 3.2.2 Typed Character N -grams Sapkota et al. (2015) showed that classical character n-grams lose some information in merging together instances of n-grams like the which could be a prefix (thesis), a suffix (breathe), or a standalone word (the). Therefore, untyped character n-grams were separated into ten distinct categories. Four of the ten categories are related to affixes: prefix, suffix, space-prefix, and space-suffix. Three are wordrelated: whole-word, mid-word, and multi-word. The final three are related to the use of punctuation: beg-punct, mid-punct, and end-punct. For example, the character n-grams from the last section would instead"
P16-1210,D07-1129,0,0.0627668,"Missing"
P16-1210,N09-2046,0,0.0227359,", the assumption is that there will still be some general features that share similar characteristics in both domains. SCL has been applied to tasks such as sentiment analysis, dependency parsing, and partof-speech tagging, but has not yet been explored for the problem of authorship attribution. The common feature representation in SCL is created by learning a projection to “pivot” features from all other features. These pivot features are a critical component of the successful use of SCL, and their selection is something that has to be done carefully and specifically to the task at the hand. Tan and Cheng (2009) studied sentiment analysis, using frequently occurring sentiment words as pivot features. Similarly, Zhang et al. (2010) proposed a simple and efficient method for selecting pivot features in domain adaptive sentiment analysis: choose the frequently occurring words or wordbigrams among domains computed after applying some selection criterion. In dependency parsing, Shimizu and Nakagawa (2007) chose the presence of a preposition, a determiner, or a helping verb between two tokens as the pivot features. For partof-speech tagging, Blitzer et al. (2006) used words that occur more than 50 times in"
Q14-1012,J86-2003,0,0.0356834,"l subdomain. 3 Interpreting ‘Event’ and Temporal Expressions in the Clinical Domain Much prior work has been done on standardizing the annotation of events and temporal expressions in text. The most widely used approach is the ISOTimeML specification (Pustejovsky et al., 2010), an ISO standard that provides a common framework for annotating and analyzing time, events, and event relations. As defined by ISO-TimeML, an E VENT refers to anything that can be said “to obtain or hold true, to happen or to occur”. This is a broad notion of event, consistent with Bach’s use of the term “eventuality” (Bach, 1986) as well as the notion of fluents in AI (McCarthy, 2002). Because the goals of the THYME project involve automatically identifying the clinical timeline for a patient from clincal records, the scope of what should be admitted into the domain of events is interpreted more broadly than in ISO-TimeML3 . Within the THYME-TimeML guideline, an E VENT is anything relevant to the clinical timeline, i.e., anything that would show up on a detailed timeline of the patient’s care or life. The best single-word syntactic head for the E VENT is then used as its span. For example, a diagnosis would certainly"
Q14-1012,S13-2002,1,0.783065,"rk A BEFORE C, then an equivalent inferred TLINK would be used to match it. E VENT and T IMEX 3 IAA was generated based on exact and overlapping spans, respectively. These results are reported in Table 3. The THYME corpus also differs from ISOTimeML in terms of E VENT properties, with the addition of DocTimeRel, ContextualModality and ContextualAspect. IAA for these properties is in Table 4. 7.3 Baseline Systems To get an idea of how much work will be necessary to adapt existing temporal information extraction systems to the clinical domain, we took the freely available ClearTK-TimeML system (Bethard, 2013), 151 which was among the top performing systems in TempEval 2013 (UzZaman et al., 2013), and evaluated its performance on the THYME corpus. ClearTK-TimeML uses support vector machine classifiers trained on the TempEval 2013 training data, employing a small set of features including character patterns, tokens, stems, part-of-speech tags, nearby nodes in the constituency tree, and a small time word gazetteer. For E VENTs and T IMEX 3s, the ClearTK-TimeML system could be applied directly to the THYME corpus. For DocTimeRels, the relation for an E VENT was taken from the TLINK between that E VENT"
Q14-1012,W13-1903,1,0.813475,"ation for these events, (3) the interaction of general and domain-specific events and their importance in the final timeline, and, more generally, (4) the importance of rough temporality and narrative containers as a step towards finer-grained timelines. We have several avenues of ongoing and future work. First, we are working to demonstrate the utility of the THYME corpus for training machine learning models. We have designed support vector machine models with constituency tree kernels that were able to reach an F1-score of 0.737 on an E VENT-T IMEX 3 narrative container identification task (Miller et al., 2013), and we are working on training models to identify events, times and the remaining types of temporal relations. Second, as per our motivating use cases, we are working to integrate this annotation data with timeline visualization tools and to use these annotations in quality-of-care research. For example, we are using temporal reasoning built on this work to investigate the liver toxicity of methotrexate across a large corpus of EHRs (Lin et al., under review)]. Finally, we plan to explore the application of our notion of an event (anything that should be visible on a domain-appropriate timel"
Q14-1012,miltsakaki-etal-2004-penn,0,0.0158666,"h the number of events and times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force"
Q14-1012,W04-0210,0,0.0401525,"d times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force annotations to"
Q14-1012,W11-0419,1,0.93622,"s, as well as to develop state-of-the-art algointerventions and diagnostics which have been thus rithms to train and test on this dataset. far attempted. In other sections, the doctor may outDeriving timelines from news text requires the conline her current plan for the patient’s treatment, then crete realization of context-dependent assumptions later describe the patient’s specific medical history, about temporal intervals, orderings and organization, allergies, care directives, and so forth. underlying the explicit signals marked in the text Most critically for temporal reasoning, each clin(Pustejovsky and Stubbs, 2011). Deriving patient ical note reflects a single time in the patient’s treathistory timelines from clinical notes also involves ment history at which all of the doctor’s statements these types of assumptions, but there are special deare accurate (the D OCTIME), and each section tends mands imposed by the characteristics of the clinical to describe events of a particular timeframe. For narrative. Due to both medical shorthand practices example, ‘History of Present illness’ predominantly and general domain knowledge, many event-event describes events occuring before D OCTIME, whereas relations are"
Q14-1012,pustejovsky-etal-2010-iso,1,0.785939,"e can 2 The Nature of Clinical Documents be both domain-specific and complex, and are often we have been examining left implicit, requiring significant domain knowledge In the THYME corpus, 1 notes from a large healthcare 1,254 de-identified to accurately detect and interpret. In this paper, we discuss the demands on accurately practice (the Mayo Clinic), representing two distinct annotating such temporal information in clinical fields within oncology: brain cancer, and colon cannotes. We describe an implementation and extension cer. To date, we have principally examined two difof ISO-TimeML (Pustejovsky et al., 2010), devel- ferent general types of clinical narrative in our EHRs: oped specifically for the clinical domain, which we clinical notes and pathology reports. Clinical notes are records of physician interactions refer to as the “THYME Guidelines to ISO-TimeML” (“THYME-TimeML”), where THYME stands for with a patient, and often include multiple, clearly “Temporal Histories of Your Medical Events”. A sim- delineated sections detailing different aspects of the plified version of these guidelines formed the basis patient’s care and present illness. These notes are for the 2012 i2b2 medical-domain tempo"
Q14-1012,S13-2001,1,0.830816,"Missing"
Q14-1012,W08-0606,0,0.0123269,"for situations where doctors proffer a diagnosis, but do so cautiously, to avoid legal liability for an incorrect diagnosis or for overlooking a correct one. For example: (3) a. The signal in the MRI is not inconsistent with a tumor in the spleen. b. The rash appears to be measles, awaiting antibody test to confirm. These H EDGED E VENTs are more real than a hypothetical diagnosis, and likely merit inclusion on a timeline as part of the diagnostic history, but must not be conflated with confirmed fact. These (and other forms of uncertainty in the medical domain) are discussed extensively in (Vincze et al., 2008). In contrast, G ENERIC E VENTs do not refer to the patient’s illness or treatment, but instead discuss illness or treatment in general (often in the patient’s specific demographic). For example: (4) In other patients without significant comorbidity that can tolerate adjuvant chemotherapy, there is a benefit to systemic adjuvant chemotherapy. These sections would be true if pasted into any patient’s note, and are often identical chunks of text repeatedly used to justify a course of action or treatment as well as to defend against liability. Contextual Aspect (to distinguish from grammatical as"
Q14-1018,S13-1004,0,0.0539099,"ly, without stop words (i.e. while aligning content words only) recall drops just a little for each corpus, which is expected as content words form the vast majority of non-identical word alignments. 4.2 Extrinsic Evaluation We extrinsically evaluate our system on textual similarity identification and paraphrase detection. Here we discuss each task and the results of evaluation. 4.2.1 Semantic Textual Similarity Given two short input text fragments (commonly sentences), the goal of this task is to identify the degree to which the two fragments are semantically similar. The *SEM 2013 STS task (Agirre et al., 2013) assessed a number of STS systems on four test datasets by comparing their output scores to human annotations. Pearson correlation coefficient with human annotations was computed individually for each test set and a weighted sum of the correlations was used as the final evaluation metric (the weight for each dataset was proportional to its size). We apply our aligner to the task by aligning each sentence pair and taking the proportion of content words aligned in the two sentences (by normalizing with the harmonic mean of their number of content words) as a proxy of their semantic similarity. O"
Q14-1018,P05-1074,0,0.118546,"e or more lexical resources like WordNet) Align identical word sequences Align named entities Align content words Align stop words Figure 1: System overview and distributional similarity (computed from word co-occurrence statistics in large corpora). An important trade-off between precision, coverage and speed exists here and aligners commonly rely on only a subset of these measures (Thadani and McKeown, 2011; Yao et al., 2013a). We use the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), which is a large resource of lexical and phrasal paraphrases constructed using bilingual pivoting (Bannard and Callison-Burch, 2005) over large parallel corpora. 3 System Our system operates as a pipeline of alignment modules that differ in the types of word pairs they align. Figure 1 is a simplistic representation of the pipeline. Each module makes use of contextual evidence to make alignment decisions. In addition, the last two modules are informed by a word semantic similarity algorithm. Because of their phrasal nature, we treat named entities separately from other content words. The rationale behind the order in which the modules are arranged is discussed later in this section (3.3.5). Before discussing each alignment"
Q14-1018,S12-1059,0,0.255446,"Missing"
Q14-1018,W07-1427,0,0.137436,"Missing"
Q14-1018,N10-1066,0,0.00773047,"eatures, it is difficult to compare the role of context in the two models because of the large paradigmatic disparity. An extension of JacanaAlign was proposed for phrasal alignments in (Yao et al., 2013b), but the contextual features remained largely unchanged. Noticeable in all the above systems is the use of contextual evidence as a feature for alignment, but in our opinion, not to an extent sufficient to harness its full potential. Even though deeper dependencybased modeling of contextual commonalities can be found in some other studies (Kouylekov and Magnini, 2005; Chambers et al., 2007; Chang et al., 2010; Yao et al., 2013c), we believe there is further scope for systematic exploitation of contextual evidence for alignment, which we aim to do in this work. On the contrary, word semantic similarity has been a central component of most aligners; various measures of word similarity have been utilized, including string similarity, resource-based similarity (derived from one or more lexical resources like WordNet) Align identical word sequences Align named entities Align content words Align stop words Figure 1: System overview and distributional similarity (computed from word co-occurrence statisti"
Q14-1018,P09-1053,0,0.078978,"Missing"
Q14-1018,de-marneffe-etal-2006-generating,0,0.0213261,"Missing"
Q14-1018,C04-1051,0,0.121789,"ply three different values derived from its output as proxies of semantic similarity: a) aligned content word proportion, b) the Viterbi decoding score, and c) the normalized decoding score. Of the three, (b) gives the best results, which we show in row 2 of Table 4. Our aligner outperforms JacanaAlign by a large margin. 4.2.2 Paraphrase Identification The goal of paraphrase identification is to decide if two sentences have the same meaning. The output is a yes/no decision instead of a real-valued similarity score as in STS. We use the MSR paraphrase corpus6 (4076 dev pairs, 1725 test pairs) (Dolan et al., 2004) to evaluate our aligner and compare with other aligners. Following earlier work (MacCartney et al., 2008; Yao et al., 2013b), we use a normalized alignment score of the two sentences to make a decision based on a threshold which we set using the dev set. Alignments with a higher-than-threshold score are taken to be paraphrases and the rest non-paraphrases. Again, this is an oversimplified application of the aligner, even more so than in STS, since a small change in linguistic properties of two sentences (e.g. polarity or modality) can turn them into non5 https://code.google.com/p/jacana/ http"
Q14-1018,P05-1045,0,0.029737,"Missing"
Q14-1018,N13-1092,0,0.0823384,"Missing"
Q14-1018,S13-1005,0,0.0981728,"mance also on the dataset reported in (Thadani et al., 2012). Additionally, we present results of two extrinsic evaluations, namely textual similarity identification and paraphrase detection. Our aligner not only outperforms existing aligners in each task, but also approaches top systems for the extrinsic tasks. 2 Background Monolingual alignment has been applied to various NLP tasks including textual entailment recognition (Hickl et al., 2006; Hickl and Bensley, 2007), paraphrase identification (Das and Smith, 2009; Madnani et al., 2012), and textual similarity assessment (B¨ar et al., 2012; Han et al., 2013) – in some cases explicitly, i.e., as a separate module. But many such systems resort to simplistic and/or ad-hoc strategies for alignment and in most such work, the alignment modules were not separately evaluated on alignment benchmarks, making their direct assessment difficult. With the introduction of the MSR alignment corpus (Brockett, 2007) developed from the second Recognizing Textual Entailment challenge data (BarHaim et al., 2006), direct evaluation and comparison of aligners became possible. The first aligner trained and evaluated on the corpus was a phrasal aligner called MANLI (MacC"
Q14-1018,W07-1428,0,0.100191,"Missing"
Q14-1018,D08-1084,0,0.22411,"c units in a pair of sentences expressed in a natural language. Such alignments provide valuable information regarding how and to what extent the two sentences are related. Consequently, alignment is a central component of a number of important tasks involving text comparison: textual entailment recognition, textual similarity identification, paraphrase detection, question answering and text summarization, to name a few. The high utility of monolingual alignment has spawned significant research on the topic in the recent past. Major efforts that have treated alignment as a standalone problem (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a) are primarily supervised, thanks to the manually aligned corpus with training and test sets from Microsoft Research (Brockett, 2007). Primary concerns of such work include both quality and speed, due to the fact that alignment is frequently a component of larger NLP tasks. Driven by similar motivations, we seek to devise a lightweight, easy-to-construct aligner that produces high-quality output and is applicable to various end tasks. Amid a variety of problem formulations and ingenious approaches to alignment, we take a step back and examine clos"
Q14-1018,N12-1019,0,0.0612105,"Missing"
Q14-1018,P11-2044,0,0.520149,"ences expressed in a natural language. Such alignments provide valuable information regarding how and to what extent the two sentences are related. Consequently, alignment is a central component of a number of important tasks involving text comparison: textual entailment recognition, textual similarity identification, paraphrase detection, question answering and text summarization, to name a few. The high utility of monolingual alignment has spawned significant research on the topic in the recent past. Major efforts that have treated alignment as a standalone problem (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a) are primarily supervised, thanks to the manually aligned corpus with training and test sets from Microsoft Research (Brockett, 2007). Primary concerns of such work include both quality and speed, due to the fact that alignment is frequently a component of larger NLP tasks. Driven by similar motivations, we seek to devise a lightweight, easy-to-construct aligner that produces high-quality output and is applicable to various end tasks. Amid a variety of problem formulations and ingenious approaches to alignment, we take a step back and examine closely the effectiveness of tw"
Q14-1018,C12-2120,0,0.301609,"Missing"
Q14-1018,N03-1033,0,0.0100448,"nal arrows), then each orientation represents a set of possible ways in which the S and T dependencies (unidirectional arrows) can provide evidence of similarity between the contexts of s in S and t in T . Each such set comprises equivalent dependency type pairs for that orientation. In the example of Figure 2, (dobj, rcmod) is such a pair for orientation (c), given s = t = “wrote” and rs = rt = “book”. We apply the notion of dependency type equivalence to intra-category alignment of content words in four major lexical categories: verbs, nouns, adjectives and adverbs (the Stanford POS tagger (Toutanova et al., 2003) is used to identify the categories). Table 1 shows dependency type equivalences for each lexical category of s and t. The ‘←’ sign on column 5 of some rows represents a duplication of the column 4 content of the 222 (b) rt (c) (d) Figure 3: Parent-child orientations in dependencies same row. For each row, columns 4 and 5 show two sets of dependency types; each member of the first is equivalent to each member of the second for the current orientation (column 1) and lexical categories of the associated words (columns 2 and 3). For example, row 2 represents the fact that an agent relation (betwe"
Q14-1018,P13-2123,0,0.233856,"Missing"
Q14-1018,D13-1056,0,0.341906,"Missing"
Q14-1018,N13-1106,0,0.0163057,"Missing"
Q14-1018,W07-1400,0,\N,Missing
Q14-1022,S07-1025,1,0.264648,"part because of the sparsity of temporal relations in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006;"
Q14-1022,S13-2002,1,0.839357,"evaluate both identification and classification. We are not the first to approach relation identification with classification, but this paper is the first to directly and comprehensively address it. Most recently, TempEval 3 (UzZaman et al., 2013a) proposed a labeling of raw text without prior relation identification, but the challenge ultimately relied on the TimeBank. Systems were only evaluated on its subset of labeled event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away from the idea of monolithic learners, and propose smaller specialized classifiers. Inspiration comes from the recent success in named entity coreference with sieve-based learning (Lee et al., 2013). CAEVO contains a host of classi273 Transactions of the Association for Computational Linguistics, 2 (2014) 273–284. Action Editor: Ellen Riloff. c Submitted 11/2013; Rev"
Q14-1022,C82-1006,0,0.602682,"e been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliability” approaches to parsing and speech (Borghesi and Favareto, 1982; Corazza et al., 1991). D’Souza and Ng (2013) recently combined a rule-based model with a machine learned model, but lacked the fine-grained formality of a cascade of sieves. This paper is inspired by the above and is the first to apply it to temporal ordering as an extensible, formal architecture. 275 TimeBank-Dense There were four or five people inside, and they just started firing There were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. Ms. Sanders was hit several times and was pronounced dead at the scene."
Q14-1022,J93-2003,0,0.0437312,"; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliability” approaches to parsing and speech (Borghesi and Favareto, 1982; Corazza et al., 1991). D’Souza and Ng (2013) recently combined a rule-based model with a machine learned model, but lacked the fine-grained formality of a cascade of sieves. This paper is inspired by the above and is the first to apply it to temporal ordering as an extensible, formal architecture. 275 TimeBank-Dense There were four or five people inside, and they just started firing There were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pron"
Q14-1022,P14-2082,1,0.609047,"and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. Ms. Sanders was hit several times and was pronounced dead at the scene. The other customers fled, and the police said it did not appear that anyone else was injured. The other customers fled, and the police said it did not appear that anyone else was injured. Figure 1: TimeBank document on the left with TimeBankDense on the right. Solid arrows indicate BEFORE and dotted INCLUDED IN. Relations with the DCT not shown. 3 TimeBank-Dense: A Dense Ordering We use a new corpus, called TimeBank-Dense (Cassidy et al., 2014), to motivate and evaluate our architecture. This section highlights its main features. The TimeBank-Dense corpus was created to address the sparsity in current corpora. It is unique in that the annotators were forced to label all local edges, even in ambiguous cases. The corpus is not a complete graph over events and time expressions, but it approximates completeness by labeling locally complete graphs over neighboring sentences. All pairs of events and time expressions in the same sentence and all pairs of events and time expressions in the immediately following sentence were labeled. It als"
Q14-1022,D08-1073,1,0.754947,"; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models ("
Q14-1022,S13-2012,1,0.504215,"events share the same lemma, or are each a member of the same synset are thus labeled VAGUE. Time-time edges with the same lemma/synset are labeled SIMULTANEOUS. 4.1.8 All Vague The majority class baseline for this task is to label all edges as VAGUE. This sieve is added to the end of the gauntlet, labeling any remaining unlabeled edges. 4.2 Machine Learned Sieves Current state-of-the-art models for temporal ordering are machine learned classifiers. The top systems in the latest TempEval-3 contest used supervised classifiers for the different types of edges in the event graph (Bethard, 2013; Chambers, 2013). In the spirit of the sieve architecture, rather than training one large classifier for all types of edges, we create targeted classifiers for each type of edge. The resulting models are again ranked by precision and mutually 279 Event-Time Features Token, lemma, POS tag of event Tense, aspect, class of event Bigram of event and time expression words Token path from event to time Syntactic parse tree path between the event and time Typed dependency edge path between the events Boolean: syntactically dominates or is-dominated Boolean: time expression concludes sentence? Boolean: time expressio"
Q14-1022,S07-1052,0,0.0270224,"ity of temporal relations in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsk"
Q14-1022,1991.iwpt-1.24,0,0.115917,"Missing"
Q14-1022,W13-0107,0,0.0408812,"ed to by the verb occurs. • Point of reference (R) A single time with respect to which S is ordered by one dimension of tense, and to which E is ordered by another. Reichenbach’s account maps the set of possible orderings of S, E, and R, where each pair of elements can be ordered with &lt; (before), = (simultaneous), or &gt; (after), onto a set of tense names given by: {anterior, simple, posterior} × {past, present, f uture}. The relative ordering of E and R is indicated by the first dimension, while that of S and R is given by the second. Figure 2 depicts examples of the ordering R &lt; S. Similar to Derczynski and Gaizauskas (2013) we map a subset of Reichenbach’s tenses onto pairs of TimeML tense and aspect attribute values, which are given by {simple, perf ect, progressive} × {past, present, f uture}, where the first dimension is grammatical aspect and the second is tense. We refer to an event’s tense/aspect combination as its tense-aspect profile. Consider two events e1 and e2 associated with S1 /E1 /R1 and S2 /E2 /R2 . Intuitively, given R1 = R2 and the time point orderings for each event (derived from its tense-aspect profile) we can enumerate the possible interval relations that might hold between E1 and E2 , whic"
Q14-1022,D12-1062,0,0.83008,"o 2012 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relations to events + times (R) in various corpora. To avoid such sparse annotations, researchers have explored schemes that encourage annotators to connect all events. Bramsen et al. (2006) annotated timelines as directed acyclic graphs, though they annotated multi-sentence segments of text rather than individual events. Kolomiyets et al. (2012) annotated “temporal dependency structures” (i.e. dependency trees of temporal relations), though they only focused on pairs of events. In Do et al. (2012), “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, and then more relations were automatically inferred after the annotation was complete. In contrast, in our work we required annotators to label every possible pair of events/times in a given window, and our event graphs are guaranteed to be strongly connected, with every edge verified by the annotators. Table 1 compares the density of relation annotation across various corpora. A major dilemma from this prior work is that unlabeled event/time pairs are inherently ambiguous. The unlabeled pair h"
Q14-1022,N13-1112,0,0.357589,"Missing"
Q14-1022,W13-1203,0,0.0183503,"Missing"
Q14-1022,P12-1010,1,0.467407,"Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4907 0.6 TempEval 2013 11145 2078 11098 0.8 Kolomiyets 2012 1233 – 1139 0.9 1 Do 2012 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relations to events + times (R) in various corpora. To avoid such sparse annotations, researchers have explored schemes that encourage annotators to connect all events. Bramsen et al. (2006) annotated timelines as directed acyclic graphs, though they annotated multi-sentence segments of text rather than individual events. Kolomiyets et al. (2012) annotated “temporal dependency structures” (i.e. dependency trees of temporal relations), though they only focused on pairs of events. In Do et al. (2012), “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, and then more relations were automatically inferred after the annotation was complete. In contrast, in our work we required annotators to label every possible pair of events/times in a given window, and our event graphs are guaranteed to be strongly connected, with every edge verified by the annotators. Table 1 compares the density of relatio"
Q14-1022,J13-4004,1,0.881597,"event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away from the idea of monolithic learners, and propose smaller specialized classifiers. Inspiration comes from the recent success in named entity coreference with sieve-based learning (Lee et al., 2013). CAEVO contains a host of classi273 Transactions of the Association for Computational Linguistics, 2 (2014) 273–284. Action Editor: Ellen Riloff. c Submitted 11/2013; Revised 5/2014; Published 10/2014. 2014 Association for Computational Linguistics. fiers that each specialize on different types of edges. The classifiers are ordered by their individual precision, and run in order starting with the most precise. Each sieve informs those below it by passing on its decisions as graph constraints. These more precise constraints are then used by later sieves to assist their less precise decisions."
Q14-1022,S10-1063,0,0.0144695,"aining corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et"
Q14-1022,C08-1108,0,0.0340181,"and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and i"
Q14-1022,S10-1062,0,0.0171222,"tions in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikant"
Q14-1022,S13-2001,0,0.16096,"d of a complete graph labeling algorithm is that of relation identification. Research on the TimeBank and in the TempEval contests has largely focused on relation classification. The event pairs are given, and the task is to classify them. We are now forced to first determine which events should be paired up before classification. Our experiments here fully integrate and evaluate both identification and classification. We are not the first to approach relation identification with classification, but this paper is the first to directly and comprehensively address it. Most recently, TempEval 3 (UzZaman et al., 2013a) proposed a labeling of raw text without prior relation identification, but the challenge ultimately relied on the TimeBank. Systems were only evaluated on its subset of labeled event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away f"
Q14-1022,S07-1014,0,0.0464592,"Missing"
Q14-1022,P09-1046,0,0.148581,"et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliab"
Q14-1022,S10-1010,0,\N,Missing
Q14-1022,W06-1623,0,\N,Missing
Q18-1025,D13-1078,1,0.946491,"Missing"
Q18-1025,Q16-1026,0,0.0499504,"Missing"
Q18-1025,N16-1155,0,0.0644049,"Missing"
Q18-1025,C16-1087,0,0.157211,"Missing"
Q18-1025,N16-1030,0,0.128014,"Missing"
Q18-1025,P14-1135,0,0.493465,"Missing"
Q18-1025,llorens-etal-2012-timen,0,0.0412384,"Missing"
Q18-1025,P16-1101,0,0.0373429,"Missing"
Q18-1025,D10-1089,0,0.0754338,"Missing"
Q18-1025,P16-2067,0,0.0357178,"Missing"
Q18-1025,pustejovsky-etal-2010-iso,0,0.0565985,"Missing"
Q18-1025,D15-1063,0,0.366466,"Missing"
Q18-1025,S13-2003,0,0.0247148,"Missing"
Q18-1025,N03-1033,0,0.0104958,"Missing"
Q18-1025,S13-2001,0,0.237508,"Missing"
Q18-1025,S07-1014,0,0.110703,"Missing"
S07-1025,N01-1025,0,0.0223916,"the Iran-Iraq [EVENT war]. If Task B said (war BEFORE 08−15−90) then since 08−15−90=1990−08−15=today, the relation (war BEFORE today) must hold. 3 Models Using the features described in the previous section, each temporal relation — an event paired with a time or another event — was translated into a set of feature values. Pairing those feature values with the TempEval labels (BEFORE, AFTER, etc.) we trained a statistical classifier for each task. We chose support vector machines3 (SVMs) for our classifiers as they have shown good performance on a variety of natural language processing tasks (Kudo and Matsumoto, 2001; Pradhan et al., 2005). Using cross-validations on the training data, we performed a simple feature selection where any feature whose removal improved the cross-validation F-score was discarded. The resulting features for each task are listed in Table 1. After feature selection, we set the SVM free parameters, e.g. the kernel degree and cost of misclassification, by performing additional cross-validations on the training data, and selecting the model parameters which yielded the highest F-score for each task4 . 3 We used the TinySVM implementation from http://chasen.org/%7Etaku/software/TinyS"
S07-1025,P06-1095,0,0.125556,"Missing"
S07-1025,S07-1014,0,0.164462,"sider a sentence like: (1) The top commander of a Cambodian resistance force said Thursday he has sent a team to recover the remains of a British mine removal expert kidnapped and presumed killed by Khmer Rouge guerrillas almost two years ago. English speakers immediately recognize that kidnapping came first, then sending, and finally saying, even though before and after never appeared in the text. How can machines learn to do the same? The 2007 TempEval competition tries to address this question by establishing a common corpus on which research systems can compete to find temporal relations (Verhagen et al., 2007). TempEval considers the following types of event-time temporal relations: In each of these tasks, systems attempt to annotate pairs with one of the following relations: BEFORE, BEFORE - OR - OVERLAP , OVERLAP, OVERLAP - OF AFTER , AFTER or VAGUE. Competing systems are instructed to find all temporal relations of these types in a corpus of newswire documents. We approach these tasks as pair-wise classification problems, where each event/time pair is assigned one of the TempEval relation classes (BEFORE, AFTER, etc.). Event/time pairs are encoded using syntactically and semantically motivated f"
S12-1048,J96-2004,0,0.00843001,"les are assigned both to phrases and their headwords, but only the headwords are evaluated for this task. The spatial relations indicate a triplet of these roles. The general-type is assigned to each triplet of spatial indicator, trajector and landmark. At the starting point two annotators including one task-organizer and another non-expert annotator, annotated 325 sentences for the spatial roles and relations. The purpose was to realize the disagreement points and prepare a set of instructions in a way to achieve highest-possible agreement. From the first effort an inter-annotator agreement (Carletta, 1996) of 0.89 for Cohen’s kappa was obtained. We continued with the a third annotator for the remaining 888 sentences. The annotator had an explanatory session and received a set of instructions and annotated examples to decrease the ambiguity in the annotations. To avoid complexity only the relations that are directly expressed in the sentence are annotated and spatial reasoning was avoided during the annotations. Sometimes the trajectors and landmarks or both are implicit, meaning that there is no word in the sentence to represent them. For example in the sentence Come over here, the trajector yo"
S12-1048,kordjamshidi-etal-2010-spatial,1,0.615597,"Missing"
S12-1048,mani-etal-2008-spatialml,0,0.167952,"such as ACE, GUM, GML, KML, TRML which are briefly described and compared to SpatialML scheme in (MITRE Corporation, 2010). But to our knowledge, the main obstacles for employing machine learning in this context and the very limited usage of this effective approach have been (a) the lack of an agreement on a unique semantic model for spatial information; (b) the diversity of formal spatial relations; and consequently (c) the lack of annotated data on which machine learning can be employed to learn and extract the spatial relations. The most systematic work in this area includes the SpatialML (Mani et al., 2008) scheme which focuses on geographical information, and the work of (Pustejovsky and Moszkowicz, 2009) in which the pivot of the spatial information is the spatial verb. The most recent and active work is the ISO-Space scheme (Pustejovsky et al., 2011) which is based on the above two schemes. The ideas behind ISOSpace are closely related to our annotation scheme in (Kordjamshidi et al., 2010b), however it considers more detailed and fine-grained spatial and linguistic elements which makes the preparation of the data for machine learning more difficult. Spatial information is directly related to"
S12-1048,S12-1056,0,0.111317,"s a true prediction requires all the three elements are correctly predicted at the same time. The last evaluation is on how well the systems are able to retrieve the relations and their general type i.e {region, direction, distance} at the same time. To evaluate the GENERAL - TYPE similarly the RELA TION tag is checked. For a true prediction, an exact match between the ground-truth and all the elements of the predicted RELATION tag including TR, LM,SP and GENERAL - TYPE is required. 7 Systems and results One system with two runs was submitted from the University of Texas Dallas. The two runs (Roberts and Harabagiu, 2012), UTDS P RL- SUPERVISED 1 and UTDS P RL- SUPERVISED 2 are based on the joint classification of the spatial triplets in a binary classification setting. To produce the candidate (indicator, trajector, landmark) triples, in the first stage heuristic rules targeting a high recall are used. Then a binary support vector machine classifier is employed to predict whether a triple is a spatial relation or not. Both runs start with a large number of manually engineered features, and use floating forward feature selection to select the most important ones. The difference between the two runs of UTDS P R"
S12-1048,C08-2024,0,\N,Missing
S13-1025,S12-1051,0,0.0400405,"imilar texts, asking for an approach different from those that were designed for larger texts. Since its inception, the problem has seen a large number of solutions in a relatively small amount of time. The central idea behind most solutions is the identification and alignment of semantically similar or related words across the two sentences, and the aggregation of these similarities to generate an overall similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2008; Šarić et al., 2012). The Semantic Textual Similarity task (STS) organized as part of the Semantic Evaluation Exercises (see (Agirre et al., 2012) for a description of STS 2012) provides a common platform for evaluation of such systems via comparison with humanannotated similarity scores over a large dataset. In this paper, we present a system which was submitted in STS 2013. Our system is based on very simple measures of lexical and character-level overlap, semantic overlap between the two sentences based on word relatedness measures, and surface features like the sentences’ lengths. These measures are used as features for a support vector regression model that we train with annotated data from SemEval STS 2012. Finally, the trained mo"
S13-1025,S12-1059,0,0.0249311,"13. Our system is based on very simple measures of lexical and character-level overlap, semantic overlap between the two sentences based on word relatedness measures, and surface features like the sentences’ lengths. These measures are used as features for a support vector regression model that we train with annotated data from SemEval STS 2012. Finally, the trained model is applied on the STS 2013 test pairs. Our approach is inspired by the success of similar systems in STS 2012: systems that combine multiple measures of similarity using a machine learning model to generate an overall score (Bär et al., 2012; Šarić et al., 2012). We wanted to investigate how a minimal system of this kind, making use of very few external resources, performs on a large dataset. Our experiments reveal that the performance of such a system depends highly on the training data. While training on one dataset yielded a best 176 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 176–180, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics correlation (among our three runs, described later in this do"
S13-1025,S12-1060,0,0.0230994,"e high specificity and low topicality of the expressed information, and potentially small lexical overlap even between very similar texts, asking for an approach different from those that were designed for larger texts. Since its inception, the problem has seen a large number of solutions in a relatively small amount of time. The central idea behind most solutions is the identification and alignment of semantically similar or related words across the two sentences, and the aggregation of these similarities to generate an overall similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2008; Šarić et al., 2012). The Semantic Textual Similarity task (STS) organized as part of the Semantic Evaluation Exercises (see (Agirre et al., 2012) for a description of STS 2012) provides a common platform for evaluation of such systems via comparison with humanannotated similarity scores over a large dataset. In this paper, we present a system which was submitted in STS 2013. Our system is based on very simple measures of lexical and character-level overlap, semantic overlap between the two sentences based on word relatedness measures, and surface features like the sentences’ lengths. These measures are used as f"
S13-2002,W06-1618,1,0.621267,"Missing"
S13-2002,S07-1025,1,0.514732,"Missing"
S13-2002,S10-1063,0,0.0538893,"Missing"
S13-2002,llorens-etal-2012-timen,0,0.0870497,"Missing"
S13-2002,S10-1062,0,0.0354406,"Missing"
S13-2002,S13-2001,0,0.522008,"Missing"
S13-2002,S07-1014,0,0.116629,"Missing"
S13-2002,S10-1010,0,\N,Missing
S13-2044,S13-2096,0,0.37199,"nces is required for a positive match under condition that the roles 259 tp tp + f n (1) (2) where tp is the number of true positives (the number of instances that are correctly found), f p is the number of false positives (number of instances that are predicted by the system but not a true instance), and f n is the number of false negatives (missing results). • Task E: Semantic classification of spatial relations identified in Task D. 5 tp tp + f p P recision · Recall P recision + Recall (3) System Description and Evaluation Results UNITOR. The UNITOR-HMM-TK system addressed Tasks A,B and C (Bastianelli et al., 2013). In Tasks A and C, roles are labeled by a sequencebased classifier: each word in a sentence is classified with respect to the possible spatial roles. An approach based on the SVM-HMM learning algorithm, formulated in (Tsochantaridis et al., 2006), was used. It is in line with other methods based on sequence-based classifier for Spatial Role Labeling, such as Conditional Random Fields (Kordjamshidi et al., 2011), and the same SVM-HMM learning algorithm (Kordjamshidi et al., 2012b). UNITOR’s labeling approach has been inspired by the work in (Croce et al., 2012), where an SVMHMM learning algori"
S13-2044,D11-1096,0,0.0486304,"Missing"
S13-2044,kordjamshidi-etal-2010-spatial,1,0.885302,"Missing"
S13-2044,S12-1048,1,0.784942,"Building upon the previous work, we used the notions of trajectors, landmarks and spatial indicators as introduced by Kordjamshidi et al. (2010). In addition, we further expanded the set of spatial roles labels with motion indicators, paths, directions and distances to capture fine-grained spatial semantics of static spatial relations (as the ones which do not involve motions), and to accommodate dynamic spatial relations (the ones which do involve motions). Introduction Spatial Role Labeling at SemEval-2013 is the second iteration of the task, which was initially introduced at SemEval-2012 (Kordjamshidi et al., 2012a). The second iteration extends the previous work with an additional training corpus, which contains besides “static” spatial relations, annotated motions. Motion detection is a novel task for annotating trajectors (objects, which are moving), landmarks (spatial context in which the motion is performed), motion indicators (lexical triggers which signals trajector’s motion), paths (a path along which the motion is performed), directions (absolute or relative directions of trajector’s motion) and distances (a distance as a product of motion). For annotating motions the existing annotation schem"
S13-2044,S12-1056,0,0.287434,"mine the proper conjunction of all roles, a Smoothed Partial Tree Kernel (SPTK) within the classifier that enhances both syntactic and lexical information of the examples was applied (Croce et al., 260 2011). This is a convolution kernel that measures the similarity between syntactic structures, which are partially similar and whose nodes can be different, but are, nevertheless, semantically related. Each example is represented as a tree-structure which is directly derived from the sentence dependency parse, and thus allows for avoiding manual feature engineering as in contrast to the work of Roberts and Harabagiu (2012). In the end, the similarity score between lexical nodes is measured by the Word Space model. UNITOR submitted two runs for the IAPR TC12 Image benchmark corpus (we refer to them as to UNITOR.Run1.1 and UNITOR.Run1.2) and one run for the Confluence Project corpus (UNITOR.Run2.1), based on the models individually trained on the different corpora. The difference between UNITOR.Run1.1 and UNITOR.Run1.2 is that for UNITOR.Run1.1 the results are obtained for all spatial roles (also the ones that have no spatial relation), and UNITOR.Run1.2 only provided the roles for which also spatial relations we"
S13-2044,W11-0416,0,0.0444525,"Format One important change to the data was made in SpRL-2013. In contrast to SpRL-2012, where spatial roles were annotated over “head words” whose indexes were part of unique identifiers, in SpRL2013 we switched to span-based annotations. Moreover, in order to provide a single data format for the task, we transformed SpRL-2012 data into spanbased annotations, in course of which, we identified a number of annotation errors and made further improvements for about 50 annotations. For annotating the Confluence Project corpus we used a freely available annotation tool MAE created by Amber Stubbs (Stubbs, 2011). The resulting data format uses the same annotation tags as in SpRL2012, but each role annotation refers to a character offset in the original text2 . Spatial relations are composed of references to annotations by their unique identifiers. Similarly to SpRL-2012, we allowed annotators to provide non-consuming annotations, where entity mentions, for which spatial roles can be identified, are omitted in text but necessary for a spatial relation triggered by either a spatial indicator or a motion indicator. Two spatial roles are eligible for non-consuming annotations: trajectors and landmarks. 4"
S13-2101,W12-2002,1,0.836387,"correct, partially correct but incomplete, contradictory, irrelevant and not in the domain (Dzikovska et al., 2013). We chose to work on the 2-way response task only 603 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 603–607, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics because for our application, we need to simply know if a student answer is correct or incorrect. Our application is an interactive essay-based personalized learning environment (Bethard et al., 2012). The overarching goal of our application is to create a scalable online service that recommends resources to users based on the their conceptual understanding expressed in an essay or short answer form. Our application automatically constructs a domain knowledge base from digital library resources and identifies the core concepts in the domain knowledge base.It detects flaws and gaps in users’ science knowledge and recommends digital library resources to address users’ misconceptions and knowledge gaps. The gaps are detected by identifying the core concepts which the user has not discussed. T"
S13-2101,S13-2045,0,0.413873,"d with up to 19 categories as in (Ahmad, 2009). In general, the finergrained assessments are more difficult to assess. 2 The Student Response Analysis Task The student response analysis task was posed as follows: Given a question, a known correct/reference answer and a 1 or 2 sentence student answer, classify the student answer into two, three or five categories. The two categories were correct and incorrect; the three categories were correct, contradictory and incorrect; while the five categories were correct, partially correct but incomplete, contradictory, irrelevant and not in the domain (Dzikovska et al., 2013). We chose to work on the 2-way response task only 603 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 603–607, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics because for our application, we need to simply know if a student answer is correct or incorrect. Our application is an interactive essay-based personalized learning environment (Bethard et al., 2012). The overarching goal of our application is to create a scalable online service that recommend"
S13-2101,J00-4006,0,0.0388604,"hat in our system because in our application we do not have access to that information. We were trying to build a system that would work in our current essay-based application. Some of the student answers in the dataset have a particular reference answer which they match. However, we do not make use of this information in our system either. We assume that for a particular ques604 tion, all the corresponding reference answers can be used to determine the correctness of any of the student answers. 3.1 Features The features we use are: 1. CosineSimilarity : This is the average cosine similarity (Jurafsky and James, 2000) between a student answer vector and all the corresponding reference answer vectors. The vectors are based on word counts. The words were lowercased and included stopwords and punctuations. 2. CosineSimilarityNormalized : This is the average cosine similarity between a student answer vector and all the corresponding reference answer vectors, with the word counts within the vectors divided by the word counts in Gigaword, a background corpus. We divided the raw counts by the counts in Gigaword to ensure that punctuations, stopwords and other non-discriminatory words do not artificially increase"
S14-2039,S12-1061,0,0.168899,"re et al., 2013). Here we confine our discussion to systems that participated in these tasks. With designated training data for several test sets, supervised systems were the most successful ˇ c et al., 2012; in STS 2012 (B¨ar et al., 2012; Sari´ This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of STS features (e.g., string similarity, syntactic similarity and word or phrase-level semantic similarity) to generate a final similarity score. This approach continued to do well in 2013 (Han et al., 2013; Wu et al., 2013; Shareghi and Bergler, 2013) even without domain-specific training data, but the best results were demonstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically im"
S14-2039,S12-1051,0,0.610074,"an STS algorithm that is only slightly different from the algorithm in (Sultan et al., 2014). The approach remains equally successful on STS 2014 data. Introduction Semantic textual similarity (STS), in the context of short text fragments, has drawn considerable attention in recent times. Its application spans a multitude of areas, including natural language processing, information retrieval and digital learning. Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and so on. The annual series of SemEval STS tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014) is an important platform where STS systems are evaluated on common data and evaluation criteria. In this article, we describe an STS system which participated and outperformed all other systems at SemEval 2014. The algorithm is a straightforward application of the monolingual word aligner presented in (Sul2 Background We focus on two relevant topics in this section: the state of the art of STS research, and the word aligner presented in (Sultan et al., 2014). 2.1 Semantic Textual Similarity Since the inception of textual similarity research for short"
S14-2039,S13-1004,0,0.48548,"t is only slightly different from the algorithm in (Sultan et al., 2014). The approach remains equally successful on STS 2014 data. Introduction Semantic textual similarity (STS), in the context of short text fragments, has drawn considerable attention in recent times. Its application spans a multitude of areas, including natural language processing, information retrieval and digital learning. Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and so on. The annual series of SemEval STS tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014) is an important platform where STS systems are evaluated on common data and evaluation criteria. In this article, we describe an STS system which participated and outperformed all other systems at SemEval 2014. The algorithm is a straightforward application of the monolingual word aligner presented in (Sul2 Background We focus on two relevant topics in this section: the state of the art of STS research, and the word aligner presented in (Sultan et al., 2014). 2.1 Semantic Textual Similarity Since the inception of textual similarity research for short text, perhaps with t"
S14-2039,S12-1060,0,0.0619307,"Missing"
S14-2039,S14-2010,0,0.0366777,"fferent from the algorithm in (Sultan et al., 2014). The approach remains equally successful on STS 2014 data. Introduction Semantic textual similarity (STS), in the context of short text fragments, has drawn considerable attention in recent times. Its application spans a multitude of areas, including natural language processing, information retrieval and digital learning. Examples of tasks that benefit from STS include text summarization, machine translation, question answering, short answer scoring, and so on. The annual series of SemEval STS tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014) is an important platform where STS systems are evaluated on common data and evaluation criteria. In this article, we describe an STS system which participated and outperformed all other systems at SemEval 2014. The algorithm is a straightforward application of the monolingual word aligner presented in (Sul2 Background We focus on two relevant topics in this section: the state of the art of STS research, and the word aligner presented in (Sultan et al., 2014). 2.1 Semantic Textual Similarity Since the inception of textual similarity research for short text, perhaps with the studies reported by"
S14-2039,S13-1029,0,0.0411115,"onal Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of STS features (e.g., string similarity, syntactic similarity and word or phrase-level semantic similarity) to generate a final similarity score. This approach continued to do well in 2013 (Han et al., 2013; Wu et al., 2013; Shareghi and Bergler, 2013) even without domain-specific training data, but the best results were demonstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically important in the context of a larger system rather than on their own, so high latency is an obvious drawback for such systems. We present an STS system that has simplicity, high accuracy and speed as its design goals, can be deployed without any supervision, operates in a linguistically principled manner with purely semantic"
S14-2039,Q14-1018,1,0.592924,"he SemEval 2014 STS task (task 10), our system demonstrated the best performance (measured by correlation with human annotations) among 38 system runs. 1 1. They are semantically similar. 2. They occur in similar semantic contexts in the respective sentences. The output of the word aligner for a sentence pair can be used to predict the pair’s semantic similarity by taking the proportion of their aligned content words. Intuitively, the more semantic components in the sentences we can meaningfully align, the higher their semantic similarity should be. In experiments on STS 2013 data reported by Sultan et al. (2014), this approach was found highly effective. We also adopt this hypothesis of semantic compositionality for STS 2014. We implement an STS algorithm that is only slightly different from the algorithm in (Sultan et al., 2014). The approach remains equally successful on STS 2014 data. Introduction Semantic textual similarity (STS), in the context of short text fragments, has drawn considerable attention in recent times. Its application spans a multitude of areas, including natural language processing, information retrieval and digital learning. Examples of tasks that benefit from STS include text"
S14-2039,S12-1059,0,0.07912,"Missing"
S14-2039,S13-1021,0,0.14252,"ion 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of STS features (e.g., string similarity, syntactic similarity and word or phrase-level semantic similarity) to generate a final similarity score. This approach continued to do well in 2013 (Han et al., 2013; Wu et al., 2013; Shareghi and Bergler, 2013) even without domain-specific training data, but the best results were demonstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically important in the context of a larger system rather than on their own, so high latency is an obvious drawback for such systems. We present an STS system that has simplicity, high accuracy and speed as its design goals, can be deployed without any supervision, operates in a linguistically principled"
S14-2039,N13-1092,0,0.0276054,"Missing"
S14-2039,S13-1005,0,0.43427,"e Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 241 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland, August 23-24, 2014. Jimenez et al., 2012). Such systems typically apply a regression algorithm on a large number of STS features (e.g., string similarity, syntactic similarity and word or phrase-level semantic similarity) to generate a final similarity score. This approach continued to do well in 2013 (Han et al., 2013; Wu et al., 2013; Shareghi and Bergler, 2013) even without domain-specific training data, but the best results were demonstrated by an unsupervised system (Han et al., 2013). This has important implications for STS since extraction of each feature adds to the latency of a supervised system. STS systems are typically important in the context of a larger system rather than on their own, so high latency is an obvious drawback for such systems. We present an STS system that has simplicity, high accuracy and speed as its design goals, can be deployed without any supervision, operates in a linguist"
S14-2039,W07-1401,0,\N,Missing
S15-2027,S12-1051,0,0.741225,"er which most modern algorithms operate: computing sentence similarity as a mean of word similarities across the two input sentences. With no human annotated STS data set available, these algorithms were unsupervised and were evaluated extrinsically on tasks like paraphrase detection and textual entailment recognition. The SemEval STS task series has made an important contribution through the large annotated data set, enabling intrinsic evaluation of STS systems and making supervised STS systems a reality. At SemEval 2012, domain-specific training data was provided for most of the test pairs (Agirre et al., 2012) and consequently, supervised systems were ˇ c et al., the most successful (B¨ar et al., 2012; Sari´ 2012). These systems combined different similarity measures, e.g., lexico-semantic, syntactic and string similarity, using regression models. However, at the 2013 and 2014 STS events, no such training data was provided; instead, the systems were allowed to use all past data to train their systems. Interestingly, the best systems at these two events were unsupervised (Han et al., 2013; Sultan et al., 2014b); some super148 Proceedings of the 9th International Workshop on Semantic Evaluation (SemE"
S15-2027,S13-1004,0,0.402536,"larity, 5: identicality) and the average 2 http://clic.cimec.unitn.it/composes/ semantic-vectors.html 3 https://code.google.com/p/word2vec/ 150 Data Set answers-forums answers-students belief headlines images Source of Text forum answers student short answers belief annotations news headlines image descriptions # of Pairs 375 750 375 750 750 Table 1: Test sets at SemEval STS 2015. of the annotations was taken as their final similarity score. We describe each data set briefly in Table 1. We trained our supervised systems using data from the past three years of SemEval STS (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). For answers-forums, answers-students and belief, we used all past annotations. For headlines, we used all headlines (2013), headlines (2014), deft-news (2014) and smtnews (2012) pairs. For images, we used all msrpar (2012; train and test), msrvid (2012; train and test) and images (2014) pairs. The specific training corpus selections for the two latter data sets were based on our experiments with past headlines and images data, where these subsets yielded better results than an all-inclusive training set (seemingly due to the fact that they were drawn from similar domain"
S15-2027,S14-2010,0,0.282575,"Missing"
S15-2027,S15-2045,0,0.390866,"Missing"
S15-2027,S12-1059,0,0.102086,"Missing"
S15-2027,P14-1023,0,0.304861,"cally related terms across the two sentences are aligned at first and then their semantic similarity is computed as a monotonically increasing function of the degree of alignment. At SemEval 2015, we submitted an unsupervised system based on word alignments which is almost identical to our winning system at SemEval 2014 (Sultan et al., 2014b). We also submitted a supervised ridge regression model that uses (1) the output of our unsupervised system, and (2) the cosine similarity between the vector representations of the two sentences (derived from neural word embeddings of their content words (Baroni et al., 2014)) as its features. Our unsupervised system ranked 5th and the two supervised runs ranked 1st and 3rd. Evaluation also shows that our best run outperforms the winning systems at all past SemEval STS events. all words in the two sentences. In other words, given sentences S (1) and S (2) , sts(S (1) , S (2) ) = nac (S (1) ) + nac (S (2) ) nc (S (1) ) + nc (S (2) ) We describe our three system runs in this section in order of their complexity – new capabilities and/or features are added with each run. where nc (S (i) ) and nac (S (i) ) are the number of content words and the number of aligned cont"
S15-2027,W05-1203,0,0.0521505,"ilarity (STS) task series (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) has become a central platform for the task: a publicly available corpus of more than 14,000 sentence pairs have been developed over the past four years with human annotations of similarity for each pair; and a total of 290 system runs have been evaluated. In this article, we describe a set of systems that were submitted at the SemEval 2015 English STS task (Agirre et al., 2015). Given two English sentences, the objective is to compute their semantic Early work on sentence similarity (Corley and Mihalcea, 2005; Mihalcea et al., 2006; Li et al., 2006; Islam and Inkpen, 2008) established the basic procedural framework under which most modern algorithms operate: computing sentence similarity as a mean of word similarities across the two input sentences. With no human annotated STS data set available, these algorithms were unsupervised and were evaluated extrinsically on tasks like paraphrase detection and textual entailment recognition. The SemEval STS task series has made an important contribution through the large annotated data set, enabling intrinsic evaluation of STS systems and making supervised"
S15-2027,N13-1092,0,0.0614921,"Missing"
S15-2027,S13-1005,0,0.362279,"Missing"
S15-2027,S14-2078,0,0.13002,"Missing"
S15-2027,S12-1060,0,0.0685412,"Missing"
S15-2027,Q14-1018,1,0.824685,"yielded better experimental results on data from past STS events. The aligner aligns words based on their semantic similarity and the similarity between their local semantic contexts in the two sentences. It uses the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to identify semantically similar words, and relies on dependencies and surface-form neighbors of the two words to determine their contextual similarity. Word pairs are aligned in decreasing order of a weighted sum of their semantic and contextual similarity. Figure 1 shows an example set of alignments. For more details, see (Sultan et al., 2014a). We also consider a levenshtein distance1 of 1 between a misspelled word and a correctly spelled word (of length > 2) to be a match. In all runs, we truncate at the extremes to keep the score in [0, 5]. 2.1 2.2 2 System Overview Run 1: U This is an unsupervised system that first aligns related words across the two input sentences and then outputs the proportion of aligned content words as their semantic similarity. It is similar to our last year’s system (Sultan et al., 2014b) based on the word aligner described in (Sultan et al., 2014a). However, where last year’s system computed a separat"
S15-2027,S14-2039,1,0.754952,"yielded better experimental results on data from past STS events. The aligner aligns words based on their semantic similarity and the similarity between their local semantic contexts in the two sentences. It uses the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to identify semantically similar words, and relies on dependencies and surface-form neighbors of the two words to determine their contextual similarity. Word pairs are aligned in decreasing order of a weighted sum of their semantic and contextual similarity. Figure 1 shows an example set of alignments. For more details, see (Sultan et al., 2014a). We also consider a levenshtein distance1 of 1 between a misspelled word and a correctly spelled word (of length > 2) to be a match. In all runs, we truncate at the extremes to keep the score in [0, 5]. 2.1 2.2 2 System Overview Run 1: U This is an unsupervised system that first aligns related words across the two input sentences and then outputs the proportion of aligned content words as their semantic similarity. It is similar to our last year’s system (Sultan et al., 2014b) based on the word aligner described in (Sultan et al., 2014a). However, where last year’s system computed a separat"
S15-2027,S13-1021,0,0.0858521,"Missing"
S15-2072,bethard-etal-2014-cleartk,1,0.821433,"order spans into single discontinuous disorders, and association of the final (dis)continuous disorder spans with CUIs. 2.1.1 Disorder Span Annotation Span identification in Task 1 was accomplished with the same begin-inside-outside (BIO) token classification methodology as in previous work (Gung, 2014) but using the updated training data. Spans of putative disorders were labeled using a linear chain CRF with features identical to those used in previous work. Examples of these features are shown in Table 1. The disorder span tagger was implemented using the ClearTK machine learning framework (Bethard et al., 2014) which presents a UIMA interface for machine learning models and wraps classifiers such as CRFSuite (Okazaki, 2007). 418 Named entity Example Feature First token of each of the two annotations Part-of-speech tags (e.g, NN) of each of the two annotations Phrase chunks (e.g., NP, VP) between the two annotations Max distance to common ancestor of the two annotations Concatenation of head word and governing word for each of the two annotations Number of named entity mentions between the two annotations Table 1: Feature types and examples for features used to associated disjoint spans into a discon"
S15-2072,S14-2007,0,0.0287509,"Missing"
S15-2136,N13-3004,0,0.0310777,"RE P OST E XP or S ET 4. Adjudicators revised and finalized the temporal relations More details on the corpus annotation process are documented in a separate article (Styler et al., 2014a). Because the data contained incompletely deidentified clinical data (the time expressions were retained), participants were required to sign a data use agreement with the Mayo Clinic to obtain the raw text of the clinical notes and pathology reports.1 The event, time and temporal relation annotations were distributed separately from the text, in an open source repository2 using the Anafora standoff format (Chen and Styler, 2013). • Identifying event expressions (EVENT annotations in the THYME corpus) consisting of the following components: – The spans (character offsets) of the expression in the text – Contextual Modality: ACTUAL, H YPO THETICAL, H EDGED or G ENERIC – Degree: M OST, L ITTLE or N/A – Polarity: P OS or N EG – Type: A SPECTUAL, E VIDENTIAL or N/A 1 The details of this process are described at http://thyme. healthnlp.org/ 2 https://github.com/stylerw/thymedata 807 Train Dev 293 147 38890 20974 3833 2078 11176 6173 3 Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not"
S15-2136,W11-0419,1,0.496993,"AL, E VIDENTIAL or N/A 1 The details of this process are described at http://thyme. healthnlp.org/ 2 https://github.com/stylerw/thymedata 807 Train Dev 293 147 38890 20974 3833 2078 11176 6173 3 Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not completed in time. • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (B EFORE, OVER LAP , B EFORE -OVERLAP or A FTER ), represented by D OC T IME R EL annotations in the THYME corpus – Narrative container relations (Pustejovsky and Stubbs, 2011) between events and/or times, represented by T LINK annotations with T YPE =C ONTAINS in the THYME corpus The evaluation was run in two phases: 1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F1 : P = |S ∩ H| |S| R= |S ∩ H| |H| F1 = 2·P ·R P +R where S is th"
S15-2136,Q14-1012,1,0.497138,"Missing"
S15-2136,P11-2061,0,0.0272505,"ated by dividing the F1 on the attribute by the F1 on identifying the spans: A= attribute F1 span F1 For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A C ON - TAINS B and B C ONTAINS C, so A C ONTAINS C): Pclosure = Rclosure |S ∩ closure(H)| |S| 6 |closure(S) ∩ H| = |H| Fclosure = Participating Systems Three research teams submitted a total of 13 runs: 2 · Pclosure · Rclosure Pclosure + Rclosure These measures take the approach of prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 was predicted to be a narrative container, containing only the closest event expression to it in the text. Ba"
S15-2136,S13-2001,1,0.954536,"l of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations. 1 April 23, 2014: The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea. And output annotations over the text that capture the following kinds of information: Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic. This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understan"
S15-2136,S07-1014,1,0.916093,"Missing"
S15-2136,S10-1010,1,\N,Missing
S16-1099,S13-1004,0,0.043029,"composition scheme to construct a vector representation of 2 The minimum number of single-character edits needed to change one word into the other, where an edit is an insertion, a deletion or a substitution. 3 http://clic.cimec.unitn.it/composes/ semantic-vectors.html 4 https://code.google.com/p/word2vec/ Dataset answer-answer headlines plagiarism postediting question-question Source of Text Q&A forums news headlines plagiarised answers post-edited MT pairs Q&A forums briefly in Table 1. We train our supervised systems using data from the past four years of SemEval STS (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). The selections vary by test set, which we discuss in the next section. # of Pairs 254 249 230 244 209 Table 1: Test sets at SemEval STS 2016. each input sentence and then take the cosine similarity between the two sentence vectors as our second feature for this run. The vector representing a sentence is simply the sum of its content lemma vectors. Word n-gram Overlap. This feature computes the proportion of word n-grams (lemmatized) that are in both S (1) and S (2) . We employ separate instances of this feature for n = 1, 2, 3. The goal is to identi"
S16-1099,S12-1059,0,0.0752515,"Missing"
S16-1099,P14-1023,0,0.0391739,"re nc (S (i) ) and nac (S (i) ) are the number of content words and the number of aligned content words in S (i) , respectively. Sentence Embedding. A fundamental limitation of the above feature is that it only relies on PPDB to identify semantically similar words; consequently, similar word pairs are limited to only lexical paraphrases. Hence it fails to utilize semantic similarity or relatedness between non-paraphrase word pairs (e.g., sister and related). In the current feature, we leverage neural word embeddings to overcome this limitation. We use the 400-dimensional vectors3 developed by Baroni et al. (2014). They used the word2vec toolkit4 to extract these vectors from a corpus of about 2.8 billion tokens. These vectors perform well across different word similarity datasets in their experiments. Details on their approach and findings can be found in (Baroni et al., 2014). Instead of comparing word vectors across the two input sentences, we adopt a simple vector composition scheme to construct a vector representation of 2 The minimum number of single-character edits needed to change one word into the other, where an edit is an insertion, a deletion or a substitution. 3 http://clic.cimec.unitn.it/"
S16-1099,P08-1007,0,0.091031,"Missing"
S16-1099,P13-1100,0,0.0498192,"Missing"
S16-1099,N13-1092,0,0.0974861,"Missing"
S16-1099,S13-1005,0,0.0485077,"Missing"
S16-1099,S12-1061,0,0.0340626,"that are in both S (1) and S (2) . We employ separate instances of this feature for n = 1, 2, 3. The goal is to identify high local similarities in the two snippets and learn the influence that such local similarities might have on human judgment of sentence similarity. Character n-gram Overlap. This feature computes the proportion of character n-grams that are in both S (1) and S (2) in their surface form. We employ separate instances of this feature for n = 3, 4. The goal is to identify and correct for spelling errors as well as incorrect lemmatizations. Soft Cardinality. Soft Cardinality (Jimenez et al., 2012) is a measure of set cardinality where similar items in a set contribute less to its cardinality than dissimilar items. Jimenez et al. (2012) propose a parameterized measure of semantic similarity based on soft cardinality that computes sentence similarity from word similarity and the latter from character n-gram similarity. This measure was highly successful at SemEval-2012 (Agirre et al., 2012). We employ this measure with untuned parameter values as a feature for our model: p = 1, bias = 0, α = 0.5, biassim = 0, αsim = 0.5, q1 = 2, and q2 = 4. (Please see the original article for a detailed"
S16-1099,D11-1035,0,0.0374244,"Missing"
S16-1099,S14-2078,0,0.0337238,"Missing"
S16-1099,P11-1076,0,0.0473835,"Missing"
S16-1099,W15-0612,0,0.0227622,"Missing"
S16-1099,D13-1044,0,0.0643107,"Missing"
S16-1099,Q14-1018,1,0.849384,"ith L2 error and L2 regularization) to combine a set of similarity measures. The model is trained on SemEval 2012–2015 data. Our three runs differ in the subset of features drawn from the feature pool. We describe the feature set in this section; the individual runs will be discussed in Section 4. 2.1 Features Word Alignment Proportion. This feature operationalizes the hypothesis that highly semantically similar sentences should also have a high degree of conceptual alignment between their semantic units, i.e., words and phrases. To that end, we apply the monolingual word aligner developed by Sultan et al. (2014a) to input sentence pairs.1 This aligner aligns words based on their semantic similarity and the similarity between their local semantic contexts in the two sentences. It uses the paraphrase database PPDB (Ganitkevitch et al., 2013) to identify semantically similar words, and relies on dependencies and surface-form neighbors of the two words to determine their contextual similarity. Word pairs are aligned in decreasing order of a weighted sum of their semantic and contextual similarity. Figure 1 shows an example set of alignments. 1 https://github.com/ma-sultan/ monolingual-word-aligner 651 n"
S16-1099,S14-2039,1,0.722263,"ith L2 error and L2 regularization) to combine a set of similarity measures. The model is trained on SemEval 2012–2015 data. Our three runs differ in the subset of features drawn from the feature pool. We describe the feature set in this section; the individual runs will be discussed in Section 4. 2.1 Features Word Alignment Proportion. This feature operationalizes the hypothesis that highly semantically similar sentences should also have a high degree of conceptual alignment between their semantic units, i.e., words and phrases. To that end, we apply the monolingual word aligner developed by Sultan et al. (2014a) to input sentence pairs.1 This aligner aligns words based on their semantic similarity and the similarity between their local semantic contexts in the two sentences. It uses the paraphrase database PPDB (Ganitkevitch et al., 2013) to identify semantically similar words, and relies on dependencies and surface-form neighbors of the two words to determine their contextual similarity. Word pairs are aligned in decreasing order of a weighted sum of their semantic and contextual similarity. Figure 1 shows an example set of alignments. 1 https://github.com/ma-sultan/ monolingual-word-aligner 651 n"
S16-1099,S15-2027,1,0.621246,"26 .6599 .7356 Runs 2 .5599 .8033 .8123 .8442 .6423 .7330 3 .5453 .8033 .8195 .8442 .6666 .7355 Best Score .6924 .8275 .8414 .8669 .7471 .7781 Table 2: Performance on STS 2016 data. Each number in rows 1–5 is the correlation (Pearson’s r) between system output and Data Set answer-answer headlines plagiarism postediting question-question Weighted Mean improves performance on most test sets. Runs 1, 2 1, 3 2, 3 the value of the final evaluation metric for each run as well as the top-performing system. From the overall performances in Table 2, it is clear that the three new features added to the Sultan et al. (2015) model do not improve performance. Therefore, we run a feature ablation study only on the run 1 model. Table 3 shows the results. Similar to the findings reported in (Sultan et al., 2015), the alignment-based feature performs better across test sets. However, the addition of the embedding feature improves performance on almost all test sets. 5.2 Relation between the Runs We compute pairwise correlations between the predictions of our three runs to see how different they are. As Table 4 shows, the predictions are highly correlated, which is expected given the results in Table 2. 6 Conclusions a"
S16-1099,S12-1060,0,0.101584,"Missing"
S16-1099,P13-1136,0,0.0683108,"Missing"
S16-1099,S13-1021,0,0.0345561,"Missing"
S16-1099,N13-1106,0,0.0724158,"Missing"
S16-1099,S12-1051,0,\N,Missing
S16-1165,S16-1195,0,0.0501484,"Missing"
S16-1165,S16-1196,0,0.0406396,"Missing"
S16-1165,S15-2136,1,0.542156,"Missing"
S16-1165,S16-1193,0,0.0677741,"Missing"
S16-1165,N13-3004,1,0.0547857,"Missing"
S16-1165,S16-1192,0,0.064286,"Missing"
S16-1165,S16-1198,0,0.0757468,"Missing"
S16-1165,S16-1190,0,0.0552623,"Missing"
S16-1165,S16-1200,0,0.0379618,"Missing"
S16-1165,S16-1201,0,0.0940692,"Missing"
S16-1165,S16-1199,0,0.0614168,"Missing"
S16-1165,P14-5010,1,0.0104397,"Missing"
S16-1165,W11-0419,1,0.613193,"event expressions (EVENT annotations in the THYME corpus) consisting of the following components: – The span (character offsets) of the expression in the text – Contextual Modality: ACTUAL, H YPO THETICAL, H EDGED or G ENERIC – Degree: M OST, L ITTLE or N/A – Polarity: P OS or N EG – Type: A SPECTUAL, E VIDENTIAL or N/A • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (B EFORE, OVER LAP , B EFORE -OVERLAP or A FTER ), represented by D OC T IME R EL annotations. – Narrative container relations (Pustejovsky and Stubbs, 2011), which indicate that an event or time is temporally contained in (i.e., occurred during) another event or time, represented by T LINK annotations with T YPE =C ONTAINS. The evaluation was run in two phases: 1. Systems were provided access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were provided access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R)"
S16-1165,Q14-1012,1,0.456145,"Missing"
S16-1165,P11-2061,0,0.0570732,"w a comparison across systems for assigning attribute values, even when different systems produce different numbers of events and times. This metric is calculated by dividing the F1 on the attribute by the F1 on identifying the spans: A= attribute F1 span F1 For narrative container relations, the P and R definitions were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A C ONTAINS B and B C ONTAINS C, so A C ONTAINS C): P = |S ∩ closure(H)| |S| R= |closure(S) ∩ H| |H| Similar measures were used in prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Baseline Systems Two rule-based systems were used as baselines to compare the participating systems against."
S16-1165,S13-2001,1,0.208832,"on a corpus of clinical and pathology notes from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. 14 teams submitted a total of 40 system runs, with the best systems achieving near-human performance on identifying events and times. On identifying temporal relations, there was a gap between the best systems and human performance, but the gap was less than half the gap of Clinical TempEval 2015. 1 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. In recent years, the community has moved toward testing such information extraction systems on clinical data (Sun et al., 2013; Bethard et al., 2015) to broaden our understanding of the language of time beyond newswire expressions and structure. Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, rel"
S16-1165,S07-1014,1,0.223174,"Missing"
S17-2093,S15-2136,1,0.731438,"Missing"
S17-2093,N13-3004,0,0.0211527,"Missing"
S17-2093,S17-2177,0,0.026199,"tor machines and structured perceptrons with features including words and part-of-speech tags. For domain adaptation, KULeuven-LIIR tried assigning higher weight to the brain cancer training data, and representing unknown words in the input vocabulary. LIMSI-COT (Tourille et al., 2017) combined recurrent neural networks with character and word embeddings, and support vector machines with features including words and part-of-speech tags. For domain adaptation, LIMSI-COT tried disallowing modification of pre-trained word embeddings, and representing unknown words in the input vocabulary. NTU-1 (Huang et al., 2017) combined support vector machines and conditional random fields with features including word n-grams, part-of-speech tags, word shapes, named entities, dependency trees, and UMLS concept types. ULISBOA (Lamurias et al., 2017) combined conditional random fields and rules with features including character n-grams, words, part-ofspeech tags, and UMLS concept types. XJNLP (Long et al., 2017) combined rules, support vector machines, and recurrent and convolutional neural networks, with features including words, word embeddings, and verb tense. Several other teams (WuHanNLP, UNICA, UTD, and IIIT) al"
S17-2093,S17-2179,0,0.0278089,"words in the input vocabulary. LIMSI-COT (Tourille et al., 2017) combined recurrent neural networks with character and word embeddings, and support vector machines with features including words and part-of-speech tags. For domain adaptation, LIMSI-COT tried disallowing modification of pre-trained word embeddings, and representing unknown words in the input vocabulary. NTU-1 (Huang et al., 2017) combined support vector machines and conditional random fields with features including word n-grams, part-of-speech tags, word shapes, named entities, dependency trees, and UMLS concept types. ULISBOA (Lamurias et al., 2017) combined conditional random fields and rules with features including character n-grams, words, part-ofspeech tags, and UMLS concept types. XJNLP (Long et al., 2017) combined rules, support vector machines, and recurrent and convolutional neural networks, with features including words, word embeddings, and verb tense. Several other teams (WuHanNLP, UNICA, UTD, and IIIT) also competed, but did not submit a system description. 8 Evaluation Results Tables 2 to 4 show the results of the evaluation. In all tables, the best system score from each column is in bold. Systems marked with † were submitt"
S17-2093,S17-2181,0,0.0428032,"Missing"
S17-2093,S17-2178,0,0.0383126,"Missing"
S17-2093,S17-2180,0,0.0292967,"on to it in the text. 7 Participating Systems 11 teams submitted a total of 28 runs, 10 for the unsupervised domain adaptation phase, and 18 for the supervised domain adaptation phase. All participating systems trained some form of supervised classifiers, with common features including character n-grams, words, part-of-speech tags, and Unified Medical Language System (UMLS) concept types. Below is a brief description of each participating team, and a note if they performed any more elaborate domain adaptation than simply adding the extra 30 brain cancer notes to their training data. 568 GUIR (MacAvaney et al., 2017) combined conditional random fields, rules, and decision tree ensembles, with features including character n-grams, words, word shapes, word clusters, word embeddings, part-of-speech tags, syntactic and dependency tree paths, semantic roles, and UMLS concept types. Team GUIR KULeuven-LIIR LIMSI-COT ULISBOA Hitachi baseline WuHanNLP Hitachi (P R et al., 2017) combined conditional random fields, rules, neural networks, and decision tree ensembles, with features including character n-grams, word n-grams, word shapes, word embeddings, verb tense, section headers, and sentence embeddings. GUIR LIMS"
S17-2093,S17-2176,0,0.0310189,"Missing"
S17-2093,W11-0419,1,0.216445,"data. All colon cancer data was released as part of Clinical TempEval 2015 and 2016. The Train-10 column is the data from the first 10 patients of the brain cancer Train data, which was the only additional training data released in Clinical TempEval 2017. tested on the annotations of the brain cancer Test set. Systems were again free to use all the raw brain cancer text if they had a way to do so. Note that across all phases, the only brain cancer data released was the Train-10 set. The remainder of the brain cancer data was reserved for future evaluations. 3 – Narrative container relations (Pustejovsky and Stubbs, 2011), which indicate that an event or time is temporally contained in (i.e., occurred during) another event or time, represented by T LINK annotations with T YPE =C ONTAINS. 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P ), recall (R) and F1 : Tasks Nine tasks were included (the same as those of Clinical TempEval 2015 and 2016), grouped into three categories: P = • Identifying time expressions (TIMEX3 annotations in the THYME corpus) consisting of the following components: |S ∩ H| |S| R= |S ∩ H| |H| F1 = 2·P ·R P +R where S is the set of items predi"
S17-2093,S17-2098,0,0.0529925,"Missing"
S17-2093,P11-2061,0,0.101468,"ns were modified to take into account temporal closure, where additional relations are deterministically inferred from other relations (e.g., A C ON TAINS B and B C ONTAINS C, so A C ONTAINS C): P = |S ∩ closure(H)| |S| R= |closure(S) ∩ H| |H| To predict with the model, the raw text of the test data was searched for all exact character matches of any of the memorized phrases, preferring longer phrases when multiple matches overlapped. Wherever a phrase match was found, an event or time with the memorized (most frequent) attribute values was predicted. Similar measures were used in prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of systempredicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of human-annotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Human Agreement We also provide two types of human agreement on the tasks, measured with the same evaluation"
S17-2093,S13-2001,1,0.94542,"s were trained on colon cancer patients, but tested on brain cancer patients. The diseases, symptoms, procedures, etc. vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients. As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016. 2 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007, 2010; UzZaman et al., 2013). In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures. In the Clinical TempEval shared tasks (Bethard et al., 2015, 2016), participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations. For example, Figure 1 shows the annotations that a system is expected to produce when given the text: Data The C"
S17-2093,S07-1014,1,0.86365,"cer patients, in 2017, systems were trained on colon cancer patients, but tested on brain cancer patients. The diseases, symptoms, procedures, etc. vary widely across these two patient populations, and the doctors treating these different kinds of cancer make a variety of different linguistic choices when discussing such patients. As a result, systems that participated in Clinical TempEval 2017 were faced with a much more challenging task than systems from 2015 or 2016. 2 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007, 2010; UzZaman et al., 2013). In recent years the community has moved toward testing such information extraction systems on clinical data, to address a common need of doctors and clinical researchers to search over timelines of clinical events like symptoms, diseases, and procedures. In the Clinical TempEval shared tasks (Bethard et al., 2015, 2016), participant systems have competed to identify critical components of the timeline of a clinical text: time expressions, event expressions, and temporal relations. For example, Figure 1 shows the annotations that a system is expected to produce wh"
S18-1011,S17-2093,1,0.912197,"Missing"
S18-1011,N13-3004,0,0.134366,"VAL of the time entity other, and so on. Finally, all the time entities must be completed with some additional properties needed for their interpretation. For example, the time entity other should have a VALUE of 2, the E ND -I NTERVAL of since is the Document Creation Time, etc. Once again, the properties required by each time entity type are defined by the SCATE schema.1 Every resulting graph, composed of a set of linked time entities, represents a time expression that can be semantically interpreted. For this purpose, we provide a Scala library2 that reads the graphs in Anafora XML format (Chen and Styler, 2013) and converts them into intervals on the timeline. An example of interpreting the time entities corresponding to the expression every Saturday since March 6 relative to an anchor time of April 21, 2017 is given in Figure 2. In this example, the values today and result store the entities that represent the time expressions April 21, 2017 and every Saturday since March 6 respectively. The Scala command on the right side interprets the latter and produces the corresponding time intervals. The task includes two evaluation methods, one for the parsing step, i.e. time entity identification Tasks The"
S18-1011,Q18-1025,1,0.912451,"nts SCATE entities SCATE time exp. SCATE bounded Newswire Clinical Train Dev Test Train Dev Test 64 14 20 232 35 141 1,628 402 398 14,936 2,896 9,530 636 146 186 4,469 879 2,815 391 80 93 2,303 430 1,471 Table 1: Number of documents and SCATE annotations for both sections of the corpus following the SCATE schema. 5 divided by the total length of the intervals in the second set. Formally:  IS IH = {i ∩ j : i ∈ IS ∧ j ∈ IH } P |i| T Pint (IS , IH ) = i∈COMPACT(IS P i∈IS Rint (IS , IH ) = P P Two systems were used as baselines to compare the participating systems against. Character-based model (Laparra et al., 2018) is a novel supervised approach for time normalization that follows the SCATE schema. This model decomposes the normalization of time expressions into two modules: IH ) |i| i∈COMPACT(IS i∈∪IH T IH ) Baseline systems |i| time entity identification detects the spans of characters that belong to each time expression and labels them with their corresponding time entity type. This step is performed by character-based recurrent neural network with two stacked bidirectional Gated Recurrent Units. |i| where IS and IH are sets of intervals, i ∩ j is the possibly empty interval in common between the int"
S18-1011,D13-1078,1,0.843655,"ng Time Normalizations Egoitz Laparra University of Arizona Tucson, AZ 85721, USA Dongfang Xu University of Arizona Tucson, AZ 85721, USA Steven Bethard University of Arizona Tucson, AZ 85721, USA laparra@email.arizona.edu dongfangxu9@email.arizona.edu bethard@email.arizona.edu Ahmed S. Elsayed University of Colorado Boulder Boulder, CO 80309 Martha Palmer University of Colorado Boulder Boulder, CO 80309 ahmed.s.elsayed@colorado.edu martha.palmer@colorado.edu Abstract normalization; the most accurate systems for normalizing times are still based on sets of complex, manually-constructed rules (Bethard, 2013; Lee et al., 2014; Str¨otgen and Gertz, 2015). The Parsing Time Normalizations shared task is a new approach to time normalization based on the Semantically Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016), in which times are annotated as compositional time entities. Such entities are more expressive, being able to represent many more time expressions, and are more machine-learnable, as they can naturally be viewed as a semantic parsing task. The top of Figure 1 shows an example. Each annotation in the example corresponds to a formally defined time entity"
S18-1011,P14-1135,0,0.0223854,"zations Egoitz Laparra University of Arizona Tucson, AZ 85721, USA Dongfang Xu University of Arizona Tucson, AZ 85721, USA Steven Bethard University of Arizona Tucson, AZ 85721, USA laparra@email.arizona.edu dongfangxu9@email.arizona.edu bethard@email.arizona.edu Ahmed S. Elsayed University of Colorado Boulder Boulder, CO 80309 Martha Palmer University of Colorado Boulder Boulder, CO 80309 ahmed.s.elsayed@colorado.edu martha.palmer@colorado.edu Abstract normalization; the most accurate systems for normalizing times are still based on sets of complex, manually-constructed rules (Bethard, 2013; Lee et al., 2014; Str¨otgen and Gertz, 2015). The Parsing Time Normalizations shared task is a new approach to time normalization based on the Semantically Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016), in which times are annotated as compositional time entities. Such entities are more expressive, being able to represent many more time expressions, and are more machine-learnable, as they can naturally be viewed as a semantic parsing task. The top of Figure 1 shows an example. Each annotation in the example corresponds to a formally defined time entity. For instance, th"
S18-1011,S15-2136,1,0.901333,"Missing"
S18-1011,S18-1012,0,0.0182708,"d to be executed in a specific order. However, some of them were swapped and not analyzed in the expected order. Second, the system was supposed to assume there is only one year, one month, and one day mentioned per temporal phrase. This worked for the month and day, however, it was failing with 4-digit years. Finally, for most parsing methods the system loops through each token in the temporal phrase but it skipped the loop when identifying full numeric expressions, like ”1953” or ”08091998”. Thus, phrases like ”Last 1953” were not being counted as having any numeric values in them. C HRONO (Olex et al., 2018) is a primarily rulebased system that performs time normalization by running the following three steps: 1) Temporal tokens are identified and flagged using regex expressions to identify formatted dates/times, and by parsing out specific temporal words and numeric tokens. 2) Temporal phrases are identified by searching for consecutive numeric/temporal tokens according to certain constraints. 3) Temporal phrases are parsed and normalized into the SCATE schema via detailed rulebased parsing, including the utilization of partof-speech tags, to identify each component of an expression and link sub-"
S18-1011,L16-1599,1,0.936786,"u9@email.arizona.edu bethard@email.arizona.edu Ahmed S. Elsayed University of Colorado Boulder Boulder, CO 80309 Martha Palmer University of Colorado Boulder Boulder, CO 80309 ahmed.s.elsayed@colorado.edu martha.palmer@colorado.edu Abstract normalization; the most accurate systems for normalizing times are still based on sets of complex, manually-constructed rules (Bethard, 2013; Lee et al., 2014; Str¨otgen and Gertz, 2015). The Parsing Time Normalizations shared task is a new approach to time normalization based on the Semantically Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016), in which times are annotated as compositional time entities. Such entities are more expressive, being able to represent many more time expressions, and are more machine-learnable, as they can naturally be viewed as a semantic parsing task. The top of Figure 1 shows an example. Each annotation in the example corresponds to a formally defined time entity. For instance, the annotation on top of since corresponds to a B ETWEEN entity that identifies an interval starting at the most recent March 6 and ending at the document creation time. The bottom of Figure 1 shows how those time entities can b"
S18-1011,D15-1063,0,0.0267168,"Missing"
S18-1011,S16-1165,1,0.926046,"Missing"
S18-1011,S13-2001,0,0.346177,"n a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals). 1 Introduction The task of extracting and normalizing time expressions (e.g., finding phrases like two days ago and converting them to a standardized form like 2017-07-17) is a fundamental component of any time-aware language processing system. TempEval 2010 and 2013 (Verhagen et al., 2010; UzZaman et al., 2013) included a restricted version of a time normalization task as part of their shared tasks. However, the annotation scheme used in these tasks (TimeML; (ISO, 2012)) has some significant limitations: it assumes times can be described as a prefix of YYYY-MM-DDTHH:MM:SS (so it can’t represent, e.g., the past three summers), it is unable to represent times that are are relative to events (e.g., three weeks postoperative), and it fails to reflect the compositional nature of time expressions (e.g., that following represents a similar temporal operation in the following day and the following year). Th"
S18-2021,Q17-1010,0,0.0302625,"submitted to CoNLL 2002 (Sang, 2002; Cucerzan and Yarowsky, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003) as well as by systems for NER in biomedical texts (Saha et al., 2009). We have used prefix and suffix features by filtering our trigrams based on frequency, which better approximate the true affixes of the language. We show in Section 5 that our filtered set of trigram affixes performs better than simply adding all beginning and ending trigrams. Bian et al. (2014) incorporated both affix and syllable information into their learned word representations. The Fasttext word embeddings (Bojanowski et al., 2017) represent each word as a bag of n-grams and thus incorporate sub-word information. Here, we provide explicit representation for only the high-frequency n-grams and learn a task-specific semantic representation of them. We show in Section 5 that including all n-grams reduces performance. Other sub-word units, such as phonemes (from Epitran2 - a tool for transliterating orthographic text as International Phonetic Alphabet), have also been found to be useful for NER (Bharadwaj et al., 2016). Tkachenko and Simanovsky (2012) explored contributions of various features, including affixes, on the CoN"
S18-2021,Q16-1026,0,0.0321177,"r state-of-the-art performance in English NER and DrugBank drug NER, despite using no external dictionaries. 2 Related Work Recent neural network (RNN) state of the art techniques for NER have proposed a basic two-layered RNN architecture, first over characters of a word and second over the words of a sentence (Ma and Hovy, 2016; Lample et al., 2016). Many variants of such approaches have been introduced, e.g., to model multilingual NER (Gillick et al., 2016) or to incorporate transfer-learning (Yang et al., 2016). Such approaches have typically relied on just the words and characters, though Chiu and Nichols (2016) showed that incorporating dictionary and orthography-based features in such neural networks improves English NER. In other domains such as DrugNER, dictionary features are extensively used for NER (Segura Bedmar et al., 2013; Liu et al., 2015), but relying on these resources limits the languages and domains in which an approach can operate, hence we propose a model that does not use external dictionary resources. Morphological features were highly effective in named entity recognizers before neural networks became the new state-of-the-art. For example, prefix and suffix features were used by"
S18-2021,W02-2007,0,0.627291,"in such neural networks improves English NER. In other domains such as DrugNER, dictionary features are extensively used for NER (Segura Bedmar et al., 2013; Liu et al., 2015), but relying on these resources limits the languages and domains in which an approach can operate, hence we propose a model that does not use external dictionary resources. Morphological features were highly effective in named entity recognizers before neural networks became the new state-of-the-art. For example, prefix and suffix features were used by several of the original systems submitted to CoNLL 2002 (Sang, 2002; Cucerzan and Yarowsky, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003) as well as by systems for NER in biomedical texts (Saha et al., 2009). We have used prefix and suffix features by filtering our trigrams based on frequency, which better approximate the true affixes of the language. We show in Section 5 that our filtered set of trigram affixes performs better than simply adding all beginning and ending trigrams. Bian et al. (2014) incorporated both affix and syllable information into their learned word representations. The Fasttext word embeddings (Bojanowski et al., 2017) represent each word as a bag of n-grams"
S18-2021,J05-4005,0,0.0584929,"ntation of them. We show in Section 5 that including all n-grams reduces performance. Other sub-word units, such as phonemes (from Epitran2 - a tool for transliterating orthographic text as International Phonetic Alphabet), have also been found to be useful for NER (Bharadwaj et al., 2016). Tkachenko and Simanovsky (2012) explored contributions of various features, including affixes, on the CoNLL 2003 dataset. Additionally, morpheme dictionaries have been effective in developing features for NER tasks in languages like Japanese (Sasano and Kurohashi, 2008), Turkish (Yeniterzi, 2011), Chinese (Gao et al., 2005), and Arabic (Maloney and Niv, 1998). However, such morphological features have not yet been integrated into the new neural network models for NER. 3 Approach We consider affixes at the beginnings and ends of words as sub-word features for NER. Our base model is similar to Lample et al. (2016) where we apply an long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer over the characters of a word and then concatenate the output with a word embedding to create a word representation that combines both character-level and word-level information. Then, another layer of LSTM is applie"
S18-2021,N16-1030,0,0.420425,"nce on Lexical and Computational Semantics (*SEM), pages 167–172 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics 3. We establish a new state-of-the-art for Spanish, Dutch, and German NER, and MedLine drug NER. Additionally, we achieve near state-of-the-art performance in English NER and DrugBank drug NER, despite using no external dictionaries. 2 Related Work Recent neural network (RNN) state of the art techniques for NER have proposed a basic two-layered RNN architecture, first over characters of a word and second over the words of a sentence (Ma and Hovy, 2016; Lample et al., 2016). Many variants of such approaches have been introduced, e.g., to model multilingual NER (Gillick et al., 2016) or to incorporate transfer-learning (Yang et al., 2016). Such approaches have typically relied on just the words and characters, though Chiu and Nichols (2016) showed that incorporating dictionary and orthography-based features in such neural networks improves English NER. In other domains such as DrugNER, dictionary features are extensively used for NER (Segura Bedmar et al., 2013; Liu et al., 2015), but relying on these resources limits the languages and domains in which an approac"
S18-2021,D15-1104,0,0.110143,"Missing"
S18-2021,W13-3512,0,0.12845,"Missing"
S18-2021,P16-1101,0,0.0255089,"e 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 167–172 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics 3. We establish a new state-of-the-art for Spanish, Dutch, and German NER, and MedLine drug NER. Additionally, we achieve near state-of-the-art performance in English NER and DrugBank drug NER, despite using no external dictionaries. 2 Related Work Recent neural network (RNN) state of the art techniques for NER have proposed a basic two-layered RNN architecture, first over characters of a word and second over the words of a sentence (Ma and Hovy, 2016; Lample et al., 2016). Many variants of such approaches have been introduced, e.g., to model multilingual NER (Gillick et al., 2016) or to incorporate transfer-learning (Yang et al., 2016). Such approaches have typically relied on just the words and characters, though Chiu and Nichols (2016) showed that incorporating dictionary and orthography-based features in such neural networks improves English NER. In other domains such as DrugNER, dictionary features are extensively used for NER (Segura Bedmar et al., 2013; Liu et al., 2015), but relying on these resources limits the languages and domai"
S18-2021,W98-1002,0,0.0644009,"tion 5 that including all n-grams reduces performance. Other sub-word units, such as phonemes (from Epitran2 - a tool for transliterating orthographic text as International Phonetic Alphabet), have also been found to be useful for NER (Bharadwaj et al., 2016). Tkachenko and Simanovsky (2012) explored contributions of various features, including affixes, on the CoNLL 2003 dataset. Additionally, morpheme dictionaries have been effective in developing features for NER tasks in languages like Japanese (Sasano and Kurohashi, 2008), Turkish (Yeniterzi, 2011), Chinese (Gao et al., 2005), and Arabic (Maloney and Niv, 1998). However, such morphological features have not yet been integrated into the new neural network models for NER. 3 Approach We consider affixes at the beginnings and ends of words as sub-word features for NER. Our base model is similar to Lample et al. (2016) where we apply an long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer over the characters of a word and then concatenate the output with a word embedding to create a word representation that combines both character-level and word-level information. Then, another layer of LSTM is applied over these word representations to"
S18-2021,S13-2058,0,0.0978551,"Missing"
S18-2021,I08-2080,0,0.0421771,"high-frequency n-grams and learn a task-specific semantic representation of them. We show in Section 5 that including all n-grams reduces performance. Other sub-word units, such as phonemes (from Epitran2 - a tool for transliterating orthographic text as International Phonetic Alphabet), have also been found to be useful for NER (Bharadwaj et al., 2016). Tkachenko and Simanovsky (2012) explored contributions of various features, including affixes, on the CoNLL 2003 dataset. Additionally, morpheme dictionaries have been effective in developing features for NER tasks in languages like Japanese (Sasano and Kurohashi, 2008), Turkish (Yeniterzi, 2011), Chinese (Gao et al., 2005), and Arabic (Maloney and Niv, 1998). However, such morphological features have not yet been integrated into the new neural network models for NER. 3 Approach We consider affixes at the beginnings and ends of words as sub-word features for NER. Our base model is similar to Lample et al. (2016) where we apply an long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer over the characters of a word and then concatenate the output with a word embedding to create a word representation that combines both character-level and word-l"
S18-2021,S13-2056,0,0.144305,"Missing"
S18-2021,W03-0419,0,0.39936,"Missing"
S18-2021,P11-3019,0,0.0316058,"k-specific semantic representation of them. We show in Section 5 that including all n-grams reduces performance. Other sub-word units, such as phonemes (from Epitran2 - a tool for transliterating orthographic text as International Phonetic Alphabet), have also been found to be useful for NER (Bharadwaj et al., 2016). Tkachenko and Simanovsky (2012) explored contributions of various features, including affixes, on the CoNLL 2003 dataset. Additionally, morpheme dictionaries have been effective in developing features for NER tasks in languages like Japanese (Sasano and Kurohashi, 2008), Turkish (Yeniterzi, 2011), Chinese (Gao et al., 2005), and Arabic (Maloney and Niv, 1998). However, such morphological features have not yet been integrated into the new neural network models for NER. 3 Approach We consider affixes at the beginnings and ends of words as sub-word features for NER. Our base model is similar to Lample et al. (2016) where we apply an long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) layer over the characters of a word and then concatenate the output with a word embedding to create a word representation that combines both character-level and word-level information. Then, ano"
S19-1008,C18-1139,0,0.0278821,"the word’s first character. Flair achieves state-of-the-art or competitive results on part-of-speech tagging and named entity tagging (Akbik et al., 2018). Though they do not pre-train a LM, Bohnet et al. (2018) similarly apply a bidirectional long short term memory network (LSTM) layer on all characters of a sentence and generate contextual word embeddings by concatenating the forward and backward LSTM hidden states of the first and last character in each word. Together with other techniques, they achieve state-of-the-art performance on part-of-speech and morphological tagging. However, both Akbik et al. (2018) and Bohnet et al. (2018) discard all other contextual character embeddings, and no analyses of the models are performed at the character-level. Introduction Pre-trained language models (LMs) such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), OpenAI GPT (Radford et al., 2018), Flair (Akbik et al., 2018) and Bert (Devlin et al., 2018) have shown great improvements in NLP tasks ranging from sentiment analysis to named entity recognition to question answering. These models are trained on huge collections of unlabeled data and produce contextualized word embeddings, i.e., each wo"
S19-1008,D14-1162,0,0.084612,"s (LMs) such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), OpenAI GPT (Radford et al., 2018), Flair (Akbik et al., 2018) and Bert (Devlin et al., 2018) have shown great improvements in NLP tasks ranging from sentiment analysis to named entity recognition to question answering. These models are trained on huge collections of unlabeled data and produce contextualized word embeddings, i.e., each word receives a different vector representation in each context, rather than a single common vector representation regardless of context as in word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Research is ongoing to study these models and determine where their benefits are coming from In the current paper, we derive pre-trained contextual character embeddings from Flair’s forwardbackward LM trained on a 1-billion word corpus of 68 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 68–74 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics English (Chelba et al., 2014), and observe if these embeddings yield the same large improvements for character-level tasks as yielded by pre-trained contextual word embeddings"
S19-1008,L16-1599,1,0.821277,"iment with the following embedding layers: Rand(128): the original setting of Laparra et al. (2018a), where 128-dimensional character embeddings are randomly initialized. Rand(4096): 4096-dimensional character embeddings are randomly initialized, matching the dimensionality of the Flair forward-backward LM hidden states, i.e., matching the dimensionality of Cont(4096). Cont(4096): 4096-dimensional pre-trained contextual character embeddings are derived by runFramework The parsing time normalizations task is based on the Semantically Compositional Annotation of Time Expressions (SCATE) schema (Bethard and Parker, 2016), in which times are annotated as compositional time entities. Laparra et al. (2018a) decomposes the Parsing Time Normalizations task into two subtasks: a) time entity identification using a character-level sequence tagger which detects 69 Model Rand(128)-ori Rand(128) Rand(4096) Cont(4096) Rand(128)-ori Rand(128) Rand(4096) Cont(4096) Domain Ident. Parsing Interv. News 61.5 51.2 76.4 News 59.4 50.5 64.6 News 64.8 54.1 68.2 News 80.3 66.8 81.5 Clinical 84.7 57.9 72.1 Clinical 92.8 65.3 82.1 Clinical 93.2 65.3 83.8 Clinical 95.2 67.3 85.8 Rand(128) Rand(4096) Cont(4096) Rand(128) Rand(4096) Con"
S19-1008,N18-1202,0,0.0370162,"ly a bidirectional long short term memory network (LSTM) layer on all characters of a sentence and generate contextual word embeddings by concatenating the forward and backward LSTM hidden states of the first and last character in each word. Together with other techniques, they achieve state-of-the-art performance on part-of-speech and morphological tagging. However, both Akbik et al. (2018) and Bohnet et al. (2018) discard all other contextual character embeddings, and no analyses of the models are performed at the character-level. Introduction Pre-trained language models (LMs) such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), OpenAI GPT (Radford et al., 2018), Flair (Akbik et al., 2018) and Bert (Devlin et al., 2018) have shown great improvements in NLP tasks ranging from sentiment analysis to named entity recognition to question answering. These models are trained on huge collections of unlabeled data and produce contextualized word embeddings, i.e., each word receives a different vector representation in each context, rather than a single common vector representation regardless of context as in word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Research is ongo"
S19-1008,P18-1246,0,0.0504961,"Missing"
S19-1008,N18-2084,0,0.0625242,"Missing"
S19-1008,P18-1031,0,0.0161501,"term memory network (LSTM) layer on all characters of a sentence and generate contextual word embeddings by concatenating the forward and backward LSTM hidden states of the first and last character in each word. Together with other techniques, they achieve state-of-the-art performance on part-of-speech and morphological tagging. However, both Akbik et al. (2018) and Bohnet et al. (2018) discard all other contextual character embeddings, and no analyses of the models are performed at the character-level. Introduction Pre-trained language models (LMs) such as ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), OpenAI GPT (Radford et al., 2018), Flair (Akbik et al., 2018) and Bert (Devlin et al., 2018) have shown great improvements in NLP tasks ranging from sentiment analysis to named entity recognition to question answering. These models are trained on huge collections of unlabeled data and produce contextualized word embeddings, i.e., each word receives a different vector representation in each context, rather than a single common vector representation regardless of context as in word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Research is ongoing to study these models and det"
S19-1008,W18-5448,0,0.0280312,"Missing"
S19-1008,P18-1027,0,0.0278011,"rformance improvements in F1 of Cont(4096) 1 We upgraded Keras from 1.2 to 2.1 and fixed a code bug that allowed predictions to be made on padding tokens. 2 70 We used a paired bootstrap resampling significance test. Rand(128) Rand(4096) Cont(4096) Rand(128) Rand(4096) Cont(4096) Train Clinical Clinical Clinical News News News Target News News News Clinical Clinical Clinical Dev 63.4 62.6 68.3 45.3 43.8 57.1 improvements are significant (p < 0.001). We conclude that pre-trained contextual character embeddings generalize better across domains. Test 65.5 66.9 78.5 46.3 44.3 59.5 4.4 Inspired by Khandelwal et al. (2018)’s analysis of the effective context size of a word-based language model, we present an ablation study to measure performance when contextual information is removed. Specifically, when evaluating models, we retain only the characters in a small window around each time entity in the dev and test sets, and replace all other characters with padding characters. Figures 2a and 2b evaluate the Cont(4096), Rand(4096) and Rand(128) models across different context window sizes on the news dev and test set, respectively. Rand(128) performs similarly across all context sizes, suggesting that it makes lit"
S19-1008,C16-1087,0,0.0537728,"Missing"
S19-1008,Q18-1025,1,0.879966,"ngs Lead to Major Improvements in Time Normalization: a Detailed Analysis Dongfang Xu Egoitz Laparra Steven Bethard School of Information University of Arizona Tucson, AZ {dongfangxu9,laparra,bethard}@email.arizona.edu Abstract (Peters et al., 2018; Radford et al., 2018; Khandelwal et al., 2018; Qi et al., 2018; Zhang and Bowman, 2018). The analyses have focused on wordlevel models, yet character-level models have been shown to outperform word-level models in some NLP tasks, such as text classification (Zhang et al., 2015), named entity recognition (Kuru et al., 2016), and time normalization (Laparra et al., 2018a). Thus, there is a need to study pre-trained contextualized character embeddings, to see if they also yield improvements, and if so, to analyze where those benefits are coming from. Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time n"
S19-1031,C18-1248,0,0.0576836,"Missing"
S19-1031,W14-4012,0,0.119427,"Missing"
S19-1031,N18-1036,0,0.0629772,"Missing"
S19-1031,D18-1471,0,0.0216898,"Missing"
S19-1031,W18-5117,0,0.0490952,"ic excuses scorned Table 2: Examples of the GRU-based model predictions on the Russian troll Twitter data. Since the GRU model with no auxiliary features or pre-training performed best on the development set, we evaluated the performance of this model on the test set. It achieved 48.07 F-measure for namecalling and 52.77 for vulgarity, scores roughly similar to what we had seen on the development data. 7 Incivility Prediction in Twitter Though we built our models to detect incivilities in newspaper comments, we were interested in how well they would perform in other domains of social ˇ media. Karan and Snajder (2018) has showed that cross-domain adaptation for detecting abusive language is possible- hence we would like to observe how well our model performs on a set of tweets. In June 2018, The United States House Intelligence Committee released a list of 3841 Twitter account names that were human-operated troll accounts associated with Russia’s Internet Research Agency. Darren Linvill and Patrick Warren from Clemson University collected all the tweets published since June 2015 from these accounts, cleaned them, and published a set of almost 3 million tweets (Linvill and Warren, 2018). These tweets are pu"
S19-1031,D14-1162,0,0.0807277,"Missing"
S19-1031,W15-2602,1,0.868273,"Missing"
S19-1031,N16-2013,0,0.0376553,"none of these papers present any machine learning model that can be used for vulgarity detection, Holgate et al. (2018) claim their work to be the first in vulgarity prediction. They classified functionality of vulgarity in five different cohorts: aggression, emotion expression, emphasis, auxiliary and signalling group identity; and used binary logistic regression classifiers to identify vulgar texts. They also showed the correlation among demographic variables and vulgarity and found that age, faith, and political ideology have significant correlation with vulgarity usage. 2.3 Racism/sexism Waseem and Hovy (2016) has presented machine learning models that can be used to detect racism and sexism in social media. They have collected and annotated a set of almost 17000 tweets, and used them to build character based n-gram models for offensive tweet detection. They have provided an extensive list of criteria that identify a tweet as racially and sexually offensive, and showed that demographic information does not add much performance to a character-level model. 2.4 Personal attacks Wulczyn et al. (2017) introduced a methodology to generate annotations for personal attacks. They have used crowdsourcing to"
S19-1031,N18-1095,0,0.122764,"Missing"
S19-2232,C18-1139,0,0.0470975,"Missing"
S19-2232,P18-1119,0,0.335716,"red Task 12, in which we focus mainly on toponym detection. For this sub-task, we propose a recurrent neural network that combines word, character and affix information. By making use of the baseline provided by the organizers for toponym disambiguation, we also obtain results for the end-to-end sub-task. 2 Introduction Geoparsing is the task of detecting geolocation phrases in unstructured text and normalizing them to a unique identifier, e.g. GeoNames1 IDs. Although many automatic resolvers have been released in the past years, their performance fluctuates when applied to different domains (Gritta et al., 2018b). Most have also not been applied to and evaluated on scientific publications. The SemEval 2019 Shared Task 12: Toponym Resolution in Scientific Papers (Weissenbacher et al., 2019) aims to boost the research on geoparsing for the scientific domain by focusing on epidemiology journal articles. The task includes three sub-tasks: toponym detection, toponym disambiguation, and end-to-end toponym resolution. The first one requires participants to detect the text boundaries of all toponym mentions in articles. In toponym disambiguation, the toponym mentions are known, and the resolver has to align"
S19-2232,E12-2021,0,0.0868602,"Missing"
S19-2232,W03-0419,0,0.348497,"Missing"
S19-2232,S19-2155,0,0.128207,"Missing"
S19-2232,C18-1182,1,0.916829,"t (Stenetorp et al., 2012). The organizers also released a strong baseline that combines the model by Magge et al. (2018) for toponomy detection and the P opulation heuristic described in (Weissenbacher et al., 2015) for disambiguation.2 4 Approach 4.1 Preprocessing We used the tokenizer included in the baseline provided by the organizers as we observed it provided the best final results among other options (see Section 5.3). Again using baseline system preprocessing codes, we converted the data into CoNLL 2003 format (Tjong Kim Sang and De Meulder, 2003) for task 1. Following our prior work (Yadav and Bethard, 2018), we have used a BIO encoding instead of the IO encoding provided by the baseline system. We used the model proposed by Yadav et al. (2018) for Named Entity Recognition (NER), shown in figure 1, which uses character, word and affix information. In this architecture, a word is represented by concatenating its word embedding, an LSTM representation over the characters of the word, and learned embeddings for prefixes and suffixes of the word3 . Then another LSTM is used at the sentence level to give a contextual representation of each word. These representations of words in the sentence are given"
S19-2232,S18-2021,1,0.919374,"nce than both the individual heuristics. Gritta et al. (2018a) used a feedforward neural network approach for the disambiguation of geolocations. 1319 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1319–1323 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics B-LOC Label I-LOC O Word CRF Word LSTM-B Word LSTM-F Word Representation Yor ork York Word Features Cit ∅ ity City ∅ ’s Char LSTM-B Char LSTM-F Char Embedding Characters Y o r k C i t y ’ s Figure 1: Word+character+affix neural network architecture from Yadav et al. (2018). 3 Data and Baseline 4.2 The corpus of the task is composed of 150 journal articles downloaded from PubMed Central. After removing the author names, acknowledgments and references, titles and body text were fully annotated. The annotators identified and labelled toponyms with their corresponding coordinates according to GeoNames. For cases not found in GeoNames, they used Google Maps and Wikipedia. If the coordinates of a toponym were not available in any of these resources the special value N/A was used. The data is provided in Brat format (Stenetorp et al., 2012). The organizers also releas"
W06-1618,W95-0107,0,0.0109457,"events and the temporal relations between these events. (1) Identifying which words and phrases are EVENTs, and (2) Identifying their semantic classes. The next section describes how we turn these tasks into machine learning problems. 4 1 Event Identification as Classification These examples are derived from (Pustejovsky, et. al. 2003b) 147 Word Event Label Event Semantic Class Label The company ’s sales force applauded the shake up . O O O O O B O B I O O O O O O B_I_ACTION O B_OCCURRENCE I_OCCURRENCE O Table 1: Event chunks for sentence (1) is (B)eginning, (I)nside or (O)utside of a chunk (Ramshaw & Marcus, 1995). So, for example, under this scheme, sentence (1) would have its words labeled as in Table 1. (1) The company’s sales force [EVENT(I_ACTION) applauded] the [EVENT(OCCURRENCE) shake up] The two columns of labels in Table 1 show how the class labels differ depending on our task. If we’re interested only in the simple event identification task, it’s sufficient to know that applauded and shake both begin events (and so have the label B), up is inside an event (and so has the label I), and all other words are outside events (and so have the label O). These labels are shown in the column labeled Ev"
W06-1618,H05-1088,0,0.171473,"Missing"
W06-1618,W02-2004,0,0.024117,"ast holds true for a time span much longer than the typical newswire document and would therefore not be labeled as an EVENT. In addition to identifying which words in the TimeBank are EVENTs, the TimeBank also provides a semantic class label for each EVENT. The possible labels include OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE, I_STATE, I_ACTION, and MODAL, and are described in more detail in (Pustejovsky, et. al. 2003a). We consider two tasks on this data: 3 We view event identification as a classification task using a word-chunking paradigm similar to that used by Carreras et. al. (2002). For each word in a document, we assign a label indicating whether the word is inside or outside of an event. We use the standard B-I-O formulation of the word-chunking task that augments each class label with an indicator of whether the given word Events in the TimeBank TimeBank (Pustejovsky, et. al. 2003b) consists of just under 200 documents containing 70,000 words; it is drawn from news texts from a variety of different domains, including newswire and transcribed broadcast news. These documents are annotated using the TimeML annotation scheme (Pustejovsky, et. al. 2003a), which aims to id"
W06-1618,N01-1025,0,0.01564,"ince there are no WordNet senses labeled in our data, we accept a word as falling into one of the above hierarchies if any of its senses fall into that hierarchy. 6 Classifier Parameters The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classification decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM6 support vector machine (SVM) implementation in conjunction with YamCha7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking. YamCha has a number of parameters that define how it learns. The first of these is the window width of the “sliding window” that it uses. 5 We also considered the reverse classifiers, which classified all words in the hierarchy as non-events and all words outside the hierarchy as events. 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/ Word The company ’s sales force applauded The shake up . POS DT NN POS NNS NN VBD DT NN RP . Stem the company ’s sale force applaud the shake up . Label O O O O O B O B Table 2: A wind"
W06-1618,N04-1030,1,0.870127,"Missing"
W06-1618,W96-0213,0,\N,Missing
W06-1618,J00-4004,0,\N,Missing
W06-1618,M98-1002,0,\N,Missing
W09-2002,E06-1042,0,0.157995,"Missing"
W09-2002,W06-3506,0,0.188576,"Missing"
W09-2002,W07-0103,0,0.111579,"Missing"
W10-0719,D09-1030,0,0.0337069,"Missing"
W10-0719,W09-1904,0,0.0736523,"that are utilizing crowdsourcing technologies, all of them somewhat novel to the NLP community but with potential for future research in computational linguistics. For each, we also discuss methods for evaluating quality, finding the crowdsourced results to often be indistinguishable from controlled laboratory experiments. Introduction Crowdsourcing’s greatest contribution to language studies might be the ability to generate new kinds of data, especially within experimental paradigms. The speed and cost benefits for annotation are certainly impressive (Snow et al., 2008; CallisonBurch, 2009; Hsueh et al., 2009) but we hope to show that some of the greatest gains are in the very nature of the phenomena that we can now study. For psycholinguistic experiments in particular, we are not so much utilizing ‘artificial artificial’ intelligence as the plain intelligence and linguistic intuitions of each crowdsourced worker – the ‘voices in the crowd’, so to speak. In many experiments we are studying gradient phenomena where there are no right answers. Even when there is binary response we are often interested in the distribution of responses over many speakers rather than specific data points. This different"
W10-0719,meyers-etal-2004-annotating,0,0.0136638,"26 Figure 4: Odds ratio of a Nominal Agent being embedded within a Sentential Agent or non-Agent, relative to random chance. (ρ &lt; 0.001 for all) that competence grammar includes access to probability distributions. Meanwhile, the strong correlations across populations offer encouraging evidence in support of using the latter in psycholinguistic judgment research. 6 Confirming corpus trends Crowdsourcing can also be used to establish the validity of corpus trends found in otherwise skewed data. The experiments in this section were motivated by the NomBank corpus of nominal predicate/arguments (Meyers et al., 2004) where we found that an Agent semantic role was much more likely to be embedded within a sentential Agent. For example, (1) is more likely than (2) to receive the Agent interpretation for the ‘the police’, but both have same potential range of meanings: (1) “The investigation of the police took 3 weeks to complete” (2) “It took 3 weeks to complete the investigation of the police” While the trend is significant (ρ &lt; 0.001), the corpus is not representative speech. First, there are no minimal pairs of sentences in NomBank like (1) and (2) that have the same potential range of meanings. Second, t"
W10-0719,D08-1027,0,0.0650986,"Missing"
W11-0116,W09-3208,0,0.0257341,"of events. Since an event’s duration is highly dependent on context, our algorithm models this aspectual property as a distribution over durations rather than a single mean duration. For example, a “war” typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon wit"
W11-0116,W04-3205,0,0.104792,"pired by the standard use of web patterns for the acquisition of relational lexical knowledge. Hearst (1998) first observed that a phrase like “. . . algae, such as Gelidium. . . ” indicates that “Gelidium” is a type of “algae”, and so hypernym-hyponym relations can be identified by querying a text collection with patterns like “such <noun> as <noun>” and “<noun> , including <noun>”. A wide variety of pattern-based work followed, including the application of the idea in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like “to <verb> and then <verb>” (Chklovski and Pantel, 2004). More recent work has learned nominal gender and animacy by matching patterns like “<noun> * himself” and “<noun> and her” to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like “John Joseph”, which were observed often with masculine pronouns and never with feminine or neuter pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can predict person names as well as a fully supervised named entity recognition system. Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to the task o"
W11-0116,D09-1120,0,0.00678247,"ration using a corpus collected through Amazon’s Mechanical Turk. We make available a new database of events and their duration distributions for use in research involving the temporal and aspectual properties of events. 1 Introduction Bridging the gap between lexical knowledge and world knowledge is crucial for achieving natural language understanding. For example, knowing whether a nominal is a person or organization and whether a person is male or female substantially improves coreference resolution, even when such knowledge is gathered through noisy unsupervised approaches (Bergsma, 2005; Haghighi and Klein, 2009). However, existing algorithms and resources for such semantic knowledge have focused primarily on static properties of nominals (e.g. gender or entity type), not dynamic properties of verbs and events. This paper shows how to learn one such property: the typical duration of events. Since an event’s duration is highly dependent on context, our algorithm models this aspectual property as a distribution over durations rather than a single mean duration. For example, a “war” typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rar"
W11-0116,Y09-1024,0,0.0116617,"t “Gelidium” is a type of “algae”, and so hypernym-hyponym relations can be identified by querying a text collection with patterns like “such <noun> as <noun>” and “<noun> , including <noun>”. A wide variety of pattern-based work followed, including the application of the idea in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like “to <verb> and then <verb>” (Chklovski and Pantel, 2004). More recent work has learned nominal gender and animacy by matching patterns like “<noun> * himself” and “<noun> and her” to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like “John Joseph”, which were observed often with masculine pronouns and never with feminine or neuter pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can predict person names as well as a fully supervised named entity recognition system. Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to the task of estimating event durations. One difference from previous work is the distributional nature of the extracted knowledge. In the time domain, unlike in most previous relation-extraction"
W11-0116,P03-1054,0,0.00293524,"ion, maximum entropy and support vector machine classifiers, but as discussed in Section 8, the maximum entropy model performed best in cross-validations on the training data. 5 Unsupervised Approach While supervised learning is effective for many NLP tasks, it is sensitive to the amount of available training data. Unfortunately, the training data for event durations is very small, consisting of only 58 news articles (Pan et al., 2006), and labeling further data is quite expensive. This motivates our desire to find an 1 We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). 147 approach that does not rely on labeled data, but instead utilizes the large amounts of text available on the Web to search for duration-specific patterns. This section describes our web-based approach to learning event durations. 5.1 Web Query Patterns Temporal properties of events are often described explicitly in language-specific constructions which can help us infer an event’s duration. Consider the following two sentences from our corpus: • Many spend hours surfing the Internet. • The answer is coming up in a few minutes. These sentences explicitly describe the duration of the event"
W11-0116,W10-0719,1,0.809058,"Missing"
W11-0116,P06-1050,0,0.150641,"g event distributions based on web counts. We then evaluate both of these models on an existing annotated corpus of event durations and make comparisons to durations we collected using Amazon’s Mechanical Turk. Finally, we present a generated database of event durations. 145 2 Previous Work Early work on extracting event properties focused on linguistic aspect, for example, automatically distinguishing culminated events that have an end point from non-culminated events that do not (Siegel and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed by Pan et al. (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al., 2003) with duration lower and upper bounds. They then trained support vector machines on their annotated corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes, hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and is also a good baseline. We replicate their work and also add new features as described below. Our approac"
W11-0116,W09-2418,0,0.0138796,"sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon with duration distributions for common English events. We first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations. Next, we present our approach to learning"
W11-0116,J00-4004,0,0.0166472,"entation of the latest supervised system for predicting event durations. Next, we present our approach to learning event distributions based on web counts. We then evaluate both of these models on an existing annotated corpus of event durations and make comparisons to durations we collected using Amazon’s Mechanical Turk. Finally, we present a generated database of event durations. 145 2 Previous Work Early work on extracting event properties focused on linguistic aspect, for example, automatically distinguishing culminated events that have an end point from non-culminated events that do not (Siegel and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed by Pan et al. (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al., 2003) with duration lower and upper bounds. They then trained support vector machines on their annotated corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes, hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and i"
W11-0116,D08-1027,0,0.0272319,"Missing"
W11-0116,S07-1014,0,0.011816,"typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon with duration distributions for common English events. We first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations. Next, we"
W11-0116,P87-1001,0,\N,Missing
W12-2002,C08-1023,1,0.848158,"Missing"
W12-2002,P11-2098,0,0.0402386,"Missing"
W12-2002,J10-2002,0,0.0449821,"Missing"
W12-2002,W07-1401,0,\N,Missing
W13-1903,W12-2404,0,0.0820558,"tion for this task. 1 In this work we focus on extracting a particular temporal relation, C ONTAINS, that holds between a time expression and an event expression. This level of representation is based on the computational discourse model of narrative containers (Pustejovsky and Stubbs, 2011), which are time expressions or events which are central to a section of a text, usually manifested by being relative hubs of temporal relation links. We argue that containment relations are useful as an intermediate level of granularity between full temporal relation extraction and “coarse” temporal bins (Raghavan et al., 2012) like before admission, on admission, and after admission. Correctly extracting C ON TAINS relations will, for example, allow for more accurate placement of events on a timeline, to the resolution possible by the number of time expressions in the document. We suspect that this finer grained information will also be more useful for downstream applications like coreference, for which coarse information was found to be useful. The approach we develop is a supervised machine Introduction Clinical narratives are a rich source of unstructured information that hold great potential for impacting clini"
W13-1903,strassel-etal-2010-darpa,0,0.107061,"Missing"
W13-1903,tannier-muller-2008-evaluation,0,0.0173461,"he same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able to give credit for non-redundant, nonminimum links (Cherry et al., 2013)"
W13-1903,P11-2061,0,0.0184812,"t is possible to define the same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able to give credit for non-redundant, nonminimum li"
W13-1903,S07-1014,0,0.206982,"Missing"
W13-1903,N06-1037,0,0.0278824,"lt for using constituency structure in tree kernels for temporal relation extraction. The Path Tree representation uses a sub-tree of the whole constituent tree, but removes all nodes that are not along the path between the two arguments. Path information has been used in standard feature kernels (Pradhan et al., 2008), with each individual path being a possible boolean feature. The Path-Enclosed Tree representation is based on the smallest sub-tree that encloses the two proposed arguments. This is a representation that has shown value in other work using tree kernels for relation extraction (Zhang et al., 2006; Mirroshandel et al., 2009). The information contained in the PET representation is a superset of that contained in the Path Tree representation, since it includes the full path between arguments as well as the structure between arguments and the argument text. This means that it can take into account path information while also considering constituent structure between arguments that may play a role in determining whether the two arguments are related. For example, temporal cue words like after or during may occur between arguments and will not be captured by Path Trees. Like the PT represen"
W13-1903,S07-1025,1,0.869954,"ata, annotated the I NCLUDES relation, but merged it with other relations for the evaluation due to low inter-annotator agreement. Since no narrative container-annotated corpora exist, there are also no existing models for extracting narrative container relations. However, we can draw on the various methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, includ"
W13-1903,E12-1019,0,0.205712,"Missing"
W13-1903,S10-1063,0,0.0415784,"ES relation, but merged it with other relations for the evaluation due to low inter-annotator agreement. Since no narrative container-annotated corpora exist, there are also no existing models for extracting narrative container relations. However, we can draw on the various methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, including only tokenbased inf"
W13-1903,Y09-1038,0,0.428527,"methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, including only tokenbased information (words, part of speech tags) between the event and time, and found that adding such tree kernels on top of a baseline set of features improved event-time linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). While Mirroshandel et"
W13-1903,C04-1008,0,0.0380021,"ial properties of temporal relations, because it is possible to define the same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able t"
W13-1903,J08-2006,1,0.41968,"of unrelated structure that adds noise to the classifier. Here we include it in an attempt to get to the root of an apparent discrepancy in the tree kernel literature, as explained in Section 2, in which Hovy et al. (2012) report a negative result and Mirroshandel et al. (2009) report a positive result for using constituency structure in tree kernels for temporal relation extraction. The Path Tree representation uses a sub-tree of the whole constituent tree, but removes all nodes that are not along the path between the two arguments. Path information has been used in standard feature kernels (Pradhan et al., 2008), with each individual path being a possible boolean feature. The Path-Enclosed Tree representation is based on the smallest sub-tree that encloses the two proposed arguments. This is a representation that has shown value in other work using tree kernels for relation extraction (Zhang et al., 2006; Mirroshandel et al., 2009). The information contained in the PET representation is a superset of that contained in the Path Tree representation, since it includes the full path between arguments as well as the structure between arguments and the argument text. This means that it can take into accoun"
W13-1903,W11-0419,0,0.0857915,"are related by narrative containers. We use support vector machines with composite kernels, which allows for integrating standard feature kernels with tree kernels for representing structured features such as constituency trees. Our experiments show that using tree kernels in addition to standard feature kernels improves F1 classification for this task. 1 In this work we focus on extracting a particular temporal relation, C ONTAINS, that holds between a time expression and an event expression. This level of representation is based on the computational discourse model of narrative containers (Pustejovsky and Stubbs, 2011), which are time expressions or events which are central to a section of a text, usually manifested by being relative hubs of temporal relation links. We argue that containment relations are useful as an intermediate level of granularity between full temporal relation extraction and “coarse” temporal bins (Raghavan et al., 2012) like before admission, on admission, and after admission. Correctly extracting C ON TAINS relations will, for example, allow for more accurate placement of events on a timeline, to the resolution possible by the number of time expressions in the document. We suspect th"
W13-1903,N07-1070,1,\N,Missing
W13-1903,S10-1010,0,\N,Missing
W13-1903,S13-2001,0,\N,Missing
W14-3907,li-etal-2012-mandarin,1,0.84418,"Missing"
W14-3907,W14-3917,0,0.103569,"Missing"
W14-3907,W14-3909,0,0.0388692,"Missing"
W14-3907,W14-3915,0,0.0710818,"Missing"
W14-3907,D13-1084,0,0.207232,"Missing"
W14-3907,W14-3911,1,0.921109,"t exploiting. For instance, the NE lexicons might account for the best results in the NE class in both the Twitter data and the Surprise genre (see Table 4 last row for SPA-EN and second to last for SPAEN Surprise). Most systems showed considerable 67 F-measure 1 0.9 Baseline 0.894 0.892 0.888 0.838 0.8 0.7 0.6 (Jain and Bhat, 2014) (Lin et al., 2014) (Chittaranjan et al., 2014) (King et al., 2014) F-measure (a) MAN-EN Baseline Test1 0.4 Baseline Test2 0.3 0.196 0.2 0.152 0.118 0.1 (Chittaranjan et al., 2014) 0.417 0.360 0.338 0.260 (King et al., 2014) 0.095 0.048 0.044 (Jain and Bhat, 2014) (Elfardy et al., 2014) (Lin et al., 2014) F-measure (b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2 Baseline 1 0.952 0.962 (King et al., 2014) (Lin et al., 2014) 0.975 0.974 0.972 0.977 0.9 0.8 (Jain and Bhat, 2014) (Shrestha, 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (c) NEP-EN F-measure 1 0.9 Baseline 0.8 0.7 0.6 0.703 0.754 0.753 0.783 0.793 0.822 0.634 (Shrestha, 2014) (King et al., 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (Jain and Bhat, 2014) (Lin et al., 2014) (Bar and Dershowitz, 2014) (d) SPA-EN Figure 1: Prediction results on"
W14-3907,W14-3916,0,0.102327,"Missing"
W14-3907,W14-3910,0,0.0496265,"Missing"
W14-3907,C82-1023,0,0.500666,"olumbia.edu pascale@ece.ust.hk ayc2135@columbia.edu Abstract this direction. We define CS broadly as a communication act, whether spoken or written, where two or more languages are being used interchangeably. In its spoken form, CS has probably been around ever since different languages first came in contact. Linguists have studied this phenomenon since the mid 1900s. In contrast, the Natural Language Processing (NLP) community has only recently started to pay attention to CS, with the earliest work in this area dating back to Joshi’s theoretical work proposing an approach to parsing CS data (Joshi, 1982) based on the Matrix and Embedded language framework. With the wide-spread use of social media, CS is now being used more and more in written language and thus we are seeing an increase in published papers dealing with CS. We are specifically interested in intrasentential code switched phenomena. As a result of this task, we have successfully created the first set of annotated data for several language pairs with a coherent set of labels across the languages. As the shared task results show, CS poses new research questions that warrant new NLP approaches, and thus we expect to see a significan"
W14-3907,P11-2007,0,0.0996179,"Missing"
W14-3907,N13-1131,0,0.144834,"Missing"
W14-3907,W14-3912,0,0.116613,"Missing"
W14-3907,W15-3116,0,\N,Missing
W14-3907,E14-1001,0,\N,Missing
W14-3907,W15-2902,0,\N,Missing
W14-3907,N15-1109,0,\N,Missing
W14-3907,W15-5936,0,\N,Missing
W15-1608,J08-4004,0,0.0578495,"ality control of this task contained 1,000 tweets that were annotated by two in-lab annotators. 3.3 Review and Agreement To judge the validity of the CrowdFlower annotations, one-way in-lab review was performed on small segments of the crowdsourced results. 1,000 tweets were reviewed from jobs using the PDF instruction scheme and another 500 were reviewed from the job using the inline instruction scheme. Inter-annotator agreement measures were calculated between the original and reviewed annotations for each scheme. The measures used were observed agreement, Fleiss multi-π, and Cohen multi-κ (Artstein and Poesio, 2008) calculated for the full data set, as well as observed agreement per annotation category. The CrowdFlower annotation results’ agreement with the in-lab review was above expectations. All three overall agreement measures were at or above 0.9. At the category level, agreement was high for the simpler categories, such as Lang1, Lang2, and Other, but dipped considerably for the more complicated ones such as named entities. This is consistent with the error analysis done by King and Abney (2013), where the most frequent source of error was named entities. Ambiguous and Mixed made up only approximat"
W15-1608,W12-2108,0,0.0347668,"code-switching, as well as research in statistical methods for the automated processing of code-switching. Therefore, the annotations are theory agnostic, and follow a pragmatic definition of code-switching. Finally, to show that the processing of codeswitching text requires further advancement of our NLP technology, we present a case study in language identification with our corpora. Language identification of monolingual text has been considered a solved problem for some time now (McNamee, 2005) and even in Twitter the problem has been shown to be tractable when annotated data is available (Bergsma et al., 2012). However, as we demonstrate in this paper, when code-switching is present, the performance of state-of-the-art systems is not on par with that of monolingual sources. We predict that the difficulty increases for deeper and higher-level NLP tasks. In fact, Solorio and Liu (2008b) have shown 73 already that part-of-speech tagging performance in code-switching data is also lagging behind that observed in monolingual sources. 2 Related Work Although code-switching has not been investigated as deeply as monolingual text in the natural language processing field, there has been some work on the topi"
W15-1608,W10-0701,0,0.0678343,"Missing"
W15-1608,C82-1023,0,0.836204,"Missing"
W15-1608,N13-1131,0,0.445052,"In each of these projects, however, code-switching data was scarce, coming primarily from conversations. Because of complications with traditional evaluation measures, the code-switching point detection project used a new evaluation method, in which artificial code-switched content was generated and compared with genuine content (Solorio and Liu, 2008a). In the past, most language identification research has been done at the document level. Some researchers, however, have developed methods to identify languages within multilingual documents (Singh and Gorla, 2007; Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013). Their test data comes from a variety of sources, including web pages, bilingual forum posts, and jumbled data from monolingual sources, but none of them are trained on code-switched data, opting instead for a monolingual training set per language. This could prove to be a problem when working on code-switched data, particularly in shorter samples such as social media data, as the codeswitching context is not present in training material. One system tackled both the problems of codeswitching and social media in language and codeswitched status identification (Lignos and Marcus, 2013). Lignos"
W15-1608,D14-1108,0,0.0598944,"Missing"
W15-1608,li-etal-2012-mandarin,0,0.0976505,"Missing"
W15-1608,P08-1099,0,0.0721365,"Missing"
W15-1608,D13-1084,0,0.338601,"Missing"
W15-1608,roberts-etal-2012-empatweet,0,0.0309149,"of a population that is almost entirely multilingual. In both cases the two languages are written using the same Latin script. This is true for Nepali, even though Devanagari is its official script, because the education system in Nepal teaches typing only for English, so for digital content like social media it is common for Nepalese speakers to type using English characters. We chose Twitter as the source of our data as the informal nature of tweets makes them a more natural source for code-switching phenomena. Many researchers have turned to Twitter as a source of data for research (i.e. (Roberts et al., 2012; Reyes et al., 2013; Tomlinson et al., 2014; Kong et al., 2014; Temnikova et al., 2014; Williams and Katz, 2012)). Typically, collecting Twitter data is a straightforward 1 2 http://www.lenafoundation.org/ https://www.dataminr.com/ 72 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 72–84, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics process involving the Twitter API, specifying the desired language, and a set of keywords or hash tags. For example, in the research on user intentions some of the hash tags used include: #mygoal, #iwon, #m"
W15-1608,D08-1102,1,0.905506,"rther advancement of our NLP technology, we present a case study in language identification with our corpora. Language identification of monolingual text has been considered a solved problem for some time now (McNamee, 2005) and even in Twitter the problem has been shown to be tractable when annotated data is available (Bergsma et al., 2012). However, as we demonstrate in this paper, when code-switching is present, the performance of state-of-the-art systems is not on par with that of monolingual sources. We predict that the difficulty increases for deeper and higher-level NLP tasks. In fact, Solorio and Liu (2008b) have shown 73 already that part-of-speech tagging performance in code-switching data is also lagging behind that observed in monolingual sources. 2 Related Work Although code-switching has not been investigated as deeply as monolingual text in the natural language processing field, there has been some work on the topic. An earlier example is the work by Joshi (Joshi, 1982), where he proposes a system that can help to parse and generate code-switching sentences. His approach is based on the matrix language-embedded language formalism and although the paper has a good justification it lacks a"
W15-1608,D08-1110,1,0.861076,"rther advancement of our NLP technology, we present a case study in language identification with our corpora. Language identification of monolingual text has been considered a solved problem for some time now (McNamee, 2005) and even in Twitter the problem has been shown to be tractable when annotated data is available (Bergsma et al., 2012). However, as we demonstrate in this paper, when code-switching is present, the performance of state-of-the-art systems is not on par with that of monolingual sources. We predict that the difficulty increases for deeper and higher-level NLP tasks. In fact, Solorio and Liu (2008b) have shown 73 already that part-of-speech tagging performance in code-switching data is also lagging behind that observed in monolingual sources. 2 Related Work Although code-switching has not been investigated as deeply as monolingual text in the natural language processing field, there has been some work on the topic. An earlier example is the work by Joshi (Joshi, 1982), where he proposes a system that can help to parse and generate code-switching sentences. His approach is based on the matrix language-embedded language formalism and although the paper has a good justification it lacks a"
W15-1608,temnikova-etal-2014-building,0,0.0212016,"s are written using the same Latin script. This is true for Nepali, even though Devanagari is its official script, because the education system in Nepal teaches typing only for English, so for digital content like social media it is common for Nepalese speakers to type using English characters. We chose Twitter as the source of our data as the informal nature of tweets makes them a more natural source for code-switching phenomena. Many researchers have turned to Twitter as a source of data for research (i.e. (Roberts et al., 2012; Reyes et al., 2013; Tomlinson et al., 2014; Kong et al., 2014; Temnikova et al., 2014; Williams and Katz, 2012)). Typically, collecting Twitter data is a straightforward 1 2 http://www.lenafoundation.org/ https://www.dataminr.com/ 72 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 72–84, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics process involving the Twitter API, specifying the desired language, and a set of keywords or hash tags. For example, in the research on user intentions some of the hash tags used include: #mygoal, #iwon, #madskills, #imapro, #dowhatisay, #kissmyfeet, #proud. A similar process was followed by"
W15-1608,tomlinson-etal-2014-mygoal,0,0.0233503,"ultilingual. In both cases the two languages are written using the same Latin script. This is true for Nepali, even though Devanagari is its official script, because the education system in Nepal teaches typing only for English, so for digital content like social media it is common for Nepalese speakers to type using English characters. We chose Twitter as the source of our data as the informal nature of tweets makes them a more natural source for code-switching phenomena. Many researchers have turned to Twitter as a source of data for research (i.e. (Roberts et al., 2012; Reyes et al., 2013; Tomlinson et al., 2014; Kong et al., 2014; Temnikova et al., 2014; Williams and Katz, 2012)). Typically, collecting Twitter data is a straightforward 1 2 http://www.lenafoundation.org/ https://www.dataminr.com/ 72 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 72–84, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics process involving the Twitter API, specifying the desired language, and a set of keywords or hash tags. For example, in the research on user intentions some of the hash tags used include: #mygoal, #iwon, #madskills, #imapro, #dowhatisay, #kissmyfeet,"
W15-1608,williams-katz-2012-new,0,0.0272467,"same Latin script. This is true for Nepali, even though Devanagari is its official script, because the education system in Nepal teaches typing only for English, so for digital content like social media it is common for Nepalese speakers to type using English characters. We chose Twitter as the source of our data as the informal nature of tweets makes them a more natural source for code-switching phenomena. Many researchers have turned to Twitter as a source of data for research (i.e. (Roberts et al., 2012; Reyes et al., 2013; Tomlinson et al., 2014; Kong et al., 2014; Temnikova et al., 2014; Williams and Katz, 2012)). Typically, collecting Twitter data is a straightforward 1 2 http://www.lenafoundation.org/ https://www.dataminr.com/ 72 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 72–84, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics process involving the Twitter API, specifying the desired language, and a set of keywords or hash tags. For example, in the research on user intentions some of the hash tags used include: #mygoal, #iwon, #madskills, #imapro, #dowhatisay, #kissmyfeet, #proud. A similar process was followed by all of the previous work l"
W15-2602,P14-5010,1,0.0161718,"Missing"
W15-3809,bethard-etal-2014-cleartk,1,0.831713,"terministically extracted from those assignments. The CRF tagger processes one sentence at a time, assigning labels to all tokens within that sentence simultaneously. Systems We implemented a variety of systems in an attempt to empirically evaluate the best way to model the time span classification task. For all systems, the temporal expression extractor is implemented within Apache cTAKES3 (clinical Text Analysis and Knowledge Extraction System) (Savova et al., 2011), making use of its components for feature generation as well as its interface to the source general-domain NLP system ClearTK (Bethard et al., 2014) which in turn interfaces with different machine learning libraries, including LibSVM (Chang and Lin, 2011) and CRFSuite (Okazaki, 2007). 2.2.1 Sequence Models We developed three sequence-based models for this task, each with different perceived strengths. The first system is perhaps the simplest, a standard BIO (Begin-Inside-Outside) tagger using an off the shelf support vector machine (SVM) classifier (Cortes and Vapnik, 1995). BIO taggers work by labeling every token in a sentence as the beginning (B), inside (I), or outside (O) of some subsequence in the data (in this case a temporal expre"
W15-3809,S13-2002,1,0.885603,"Missing"
W15-3809,S10-1071,0,0.0716282,"Missing"
W15-3809,S13-2003,0,0.0273532,"Missing"
W15-3809,Q14-1012,1,0.917189,"Missing"
W15-3809,W13-1903,1,0.851141,"tejovsky et al., 2003) spurred much of the early research by providing a manually annotated corpus of events, times and temporal relations. Shared tasks such as TERN1 , which focused on time expressions, and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), which included events and temporal relations as well, helped build a comIntroduction Temporal information is ubiquitous in clinical narratives, and accurately extracting temporal information has recently been the focus of a great deal of work in clinical natural language processing (NLP) (Raghavan et al., 2012; Miller et al., 2013; Sun et al., 2013). Relevant temporal information includes events, time expressions, and temporal relations between pairs of events and/or times. The accurate extraction of temporal information would be enabling technology for sophisticated downstream processing that requires temporal awareness of patient status. One promising application is question answering, where a physician can directly ask questions about a patient’s medical record. Many question types of interest are explicitly temporal (When was the patient’s last colonoscopy? ), but almost all are implicitly temporal in the sense tha"
W15-3809,S13-2001,0,0.024644,"stexp type, which is specific to the clinical domain. Exemplified by terms like postoperatively, this type represents time spans relative to some event, often an operation. Temporal information extraction has been a topic of a great deal of work both in the clinical and general NLP domains. In the general NLP domain, the TimeBank (Pustejovsky et al., 2003) spurred much of the early research by providing a manually annotated corpus of events, times and temporal relations. Shared tasks such as TERN1 , which focused on time expressions, and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), which included events and temporal relations as well, helped build a comIntroduction Temporal information is ubiquitous in clinical narratives, and accurately extracting temporal information has recently been the focus of a great deal of work in clinical natural language processing (NLP) (Raghavan et al., 2012; Miller et al., 2013; Sun et al., 2013). Relevant temporal information includes events, time expressions, and temporal relations between pairs of events and/or times. The accurate extraction of temporal information would be enabling technology for sophisticated downstream processing th"
W15-3809,S07-1014,0,0.0265878,"ation from general domain methods is the Prepostexp type, which is specific to the clinical domain. Exemplified by terms like postoperatively, this type represents time spans relative to some event, often an operation. Temporal information extraction has been a topic of a great deal of work both in the clinical and general NLP domains. In the general NLP domain, the TimeBank (Pustejovsky et al., 2003) spurred much of the early research by providing a manually annotated corpus of events, times and temporal relations. Shared tasks such as TERN1 , which focused on time expressions, and TempEval (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), which included events and temporal relations as well, helped build a comIntroduction Temporal information is ubiquitous in clinical narratives, and accurately extracting temporal information has recently been the focus of a great deal of work in clinical natural language processing (NLP) (Raghavan et al., 2012; Miller et al., 2013; Sun et al., 2013). Relevant temporal information includes events, time expressions, and temporal relations between pairs of events and/or times. The accurate extraction of temporal information would be enabling technol"
W15-3809,W12-2404,0,\N,Missing
W15-3809,S10-1010,0,\N,Missing
W16-0322,W15-1202,0,0.0269119,"ost of the errors of our system were due to new vocabulary not present in the training set. We tried to account for this with the use of a smoothing parameter in the classifier but more work is needed in this respect. One way could be to train a word embedding using the unlabeled data in such a way that semantic similarities of words not present in the training samples can be modeled in the test set. 6 Related work In the previous versions of the workshop some systems have been proposed to solve similar challenging problems using some or similar features to the ones we used in our system. In (Mitchell et al., 2015) a system was developed for quantifying the language of schizophrenia in social media based on the LIWC lexicon. This study also showed that character ngrams over specific tweets in the user’s history can be used to separate schizophrenia sufferers from a control group. In (Pedersen, 2015) a system based on decision lists was developed to identify Twitter users who suffer from Depression or Post Traumatic Stress Disorder (PTSD). The features in this system are based on n-grams of up to 6 words. In this system, the usage of larger n-grams performed better 174 than bigrams. In our experiments, w"
W16-0322,W15-1206,1,0.82427,"ay that semantic similarities of words not present in the training samples can be modeled in the test set. 6 Related work In the previous versions of the workshop some systems have been proposed to solve similar challenging problems using some or similar features to the ones we used in our system. In (Mitchell et al., 2015) a system was developed for quantifying the language of schizophrenia in social media based on the LIWC lexicon. This study also showed that character ngrams over specific tweets in the user’s history can be used to separate schizophrenia sufferers from a control group. In (Pedersen, 2015) a system based on decision lists was developed to identify Twitter users who suffer from Depression or Post Traumatic Stress Disorder (PTSD). The features in this system are based on n-grams of up to 6 words. In this system, the usage of larger n-grams performed better 174 than bigrams. In our experiments, we only tried with n-grams up to length 3 and found that the best performing system in the cross-validation of the training data was obtained using bigrams. 7 Conclusion In this paper, we have briefly described our submission to the CLPsych 2016 shared task. We found that the best result wa"
W16-2914,S15-2136,1,0.767766,"Missing"
W16-2914,S16-1165,1,0.926678,"n a pair of arguments. Algorithm 1 Expansion for event-time relations 1: Given a gold-standard annotated event-time relation r(e,t), where e is an event, t is a temporal expression, r ∈ {C ONTAINS, B EFORE, . . . , N ONE} 2: for UMLS entity u ∈ E XPAND(e) do 3: Create relation r0 (u, t), r0 ← r 4: Add r0 to training data 5: end for 3 3.1 Experiments Dataset We tested our event expansion technique on a publicly available clinical corpus: the colon cancer set of the THYME corpus (Styler et al., 2014) used in SemEval 2015 Task 6 (Bethard et al., 2015) and SemEval 2016 Task 12: Clinical TempEval (Bethard et al., 2016). It contains 600 documents (400 oncology notes and 200 pathology notes) of 200 colon cancer patients. The gold standard annotations contain events (including both medical and general events, all annotated by head words), temporal expressions (e.g. tomorrow, postoperative, and March-11-2009), and temporal relations. We used the same training/development/test split as Clinical TempEval. The development set was used for testing research questions and building final models. Once the models were deemed finalized, they were rebuilt on the combined training and development sets and tested on the tes"
W16-2914,P11-2047,1,0.850314,"nguage System (UMLS) (Lindberg et al., 1993). It bridges the gap between different syntactic annotations of events in clinical corpora. We show that this method is superior to representing the same information as additional features, that it differs from plain upsampling, and that the primary mechanism of improvement is in the better representation of between-argument features. Our method can be viewed as a new form of data augmentation, akin to the generation of image variants for vision recognition (Krizhevsky et al., 2012) or the generation of word substitutions for information extraction (Kolomiyets et al., 2011). 108 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 108–113, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics Contains E VENT Algorithm 2 Expansion for event-event relations T IME A CT- scan of abdomen and pelvis performed on Mar 11 ⇓ UMLS UMLS C ONTAINS UMLS C ONTAINS 1: Given a gold-standard annotated event-event relation r(ea ,eb ), where ea , eb are events, r ∈ {C ONTAINS, N ONE} 2: for UMLS entity ua ∈ E XPAND(ea ) do 3: if not overlaps(span(ua ), span(eb )) then 4: Create relation r0 (ua , eb ), r0 ← r 5: Add r0 to tra"
W16-2914,Q14-1012,1,0.911843,"Missing"
W16-2914,P11-2061,0,0.177114,"AINS(scan, March 11) is duplicated in Figure 1. Thus we also compare our UMLS-informed expansion of instances to simple duplication of instances1 . 4. Which types of features benefit most from the expansion? There are three groups of token We test all research questions by training on the training set and testing on the development set with token-based features for the event-time relations. Note that expansion is applied only to the training set, not to the development or test set. 3.4 Evaluation For results on the development set, we calculate closure-enhanced precision, recall and F1-score (UzZaman and Allen, 2011) on just the withinsentence relations (since that’s what our models are able to predict). Precision is the percentage of system-generated relations that can be verified in the transitive closure of the gold standard relations. Recall is the percentage of gold standard relations that can be found in the transitive closure of the system-generated relations. The final F1-score is the harmonic mean of the transitiveclosure-processed precision and recall. For results on the test set, we used the official Clinical TempEval evaluation scripts so that our results are directly comparable with the outco"
W16-2914,S13-2001,0,0.0395915,"linical temporal relation extraction, works beyond featurizing or duplicating the same information, can generalize between-argument signals in a more effective and robust fashion. We also report a new state-of-the-art result, which is a two point improvement over the best Clinical TempEval 2016 system. 1 Introduction Temporal relation extraction is important for understanding ordering of events from a narrative text. Recent years have seen annotated corpora created for temporal information extraction, from newspaper text (Pustejovsky et al., 2003; Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), to clinical narratives (Savova et al., 2009; Sun et al., 2013; Styler et al., 2014), all with the aim of developing systems for building event timelines from textual descriptions of events. Such narrative timelines are important for information extraction tasks such as question answering (Kahn et al., 1990), clinical outcomes prediction (Schmidt et al., 2005; Lin et al., 2014), and the identification of temporal patterns (Zhou and Hripcsak, 2007) among many. In a typical supervised approach to the temporal relation extraction task, argument pairs consist of pairs of events or temporal expres"
W16-2914,S07-1014,0,0.0247806,"System (UMLS). This method notably improves clinical temporal relation extraction, works beyond featurizing or duplicating the same information, can generalize between-argument signals in a more effective and robust fashion. We also report a new state-of-the-art result, which is a two point improvement over the best Clinical TempEval 2016 system. 1 Introduction Temporal relation extraction is important for understanding ordering of events from a narrative text. Recent years have seen annotated corpora created for temporal information extraction, from newspaper text (Pustejovsky et al., 2003; Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), to clinical narratives (Savova et al., 2009; Sun et al., 2013; Styler et al., 2014), all with the aim of developing systems for building event timelines from textual descriptions of events. Such narrative timelines are important for information extraction tasks such as question answering (Kahn et al., 1990), clinical outcomes prediction (Schmidt et al., 2005; Lin et al., 2014), and the identification of temporal patterns (Zhou and Hripcsak, 2007) among many. In a typical supervised approach to the temporal relation extraction task, argument pairs"
W16-2914,S10-1010,0,\N,Missing
W16-6009,W09-1206,0,0.0606362,"Missing"
W16-6009,D15-1271,1,0.837037,"virtual-world translation problem based on a probabilistic graphical model that maps text and its semantic annotations (generated by more traditional NLP modules, like semantic role labelers or coreference resolvers) to the knowledge representation of the graphical engine, which is defined in predicate logic. In the process, we discovered several failings of traditional NLP systems when faced with this task: To address this, we introduced a technique based on recurrent neural networks for automatically generating additional training data that was similar to the target domain (Do et al., 2014; Do et al., 2015b). For each selected word (predicate, argument head word) from the source domain, a list of replacement words from the target domain which we believe can occur at the same position as the selected word, are generated by using a recurrent neural network (RNN) language model (Mikolov et al., 2010). In addition, linguistic resources such as part of speech tags, WordNet (Miller, 1995), and VerbNet (Schuler, 2005), are used as filters to select the best replacement words. We primarily targeted improving the results of the four circumstance roles AM-LOC, AMTMP, AM-MNR and AM-DIR, which are importan"
W16-6009,D13-1203,0,0.060961,"Missing"
W16-6203,W16-0307,0,0.0772871,"how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental ailments. Users in these forums can either initiate a thread, or reply to a thread initiated by others. We focused on the forums Depression, Relationship Health, and Brain/Nervous System Disorders. Wh"
W16-6203,P14-5010,1,0.020172,"Missing"
W16-6203,W15-2602,1,0.830778,"While these contributions obviously do not represent a solution to depression, we believe they form a significant first step towards understanding how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental ailments. Users in these forums can either initiate a"
W16-6203,W14-4108,0,0.0313908,"tely predict which users will withdraw from a forum. While these contributions obviously do not represent a solution to depression, we believe they form a significant first step towards understanding how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental"
W16-6203,D13-1170,0,0.00917784,"Missing"
W17-2320,W06-1615,0,0.545828,"is the task of using labeled data from one domain (the source domain) to train a classifier that will be applied to a new domain (the target domain). When there is some labeled data available in the target domain, this is referred to as supervised domain adaptation, and when there is no labeled data in the target domain, the task is called unsupervised domain adaptation (UDA). As the unsupervised version of the problem more closely aligns to real-world clinical use cases, we focus on that setting. One common UDA method in natural language processing is structural correspondence learning (SCL; Blitzer et al. (2006)). SCL hypothesizes that some features act consistently across domains (socalled pivot features) while others are still informative but are domain-dependent. The SCL method 165 Proceedings of the BioNLP 2017 workshop, pages 165–170, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics combines source and target extracted feature sets, and trains classifiers to predict the value of pivot features, uses singular value decomposition to reduce the dimensionality of the pivot feature space, and uses this reduced dimensionality space as an additional set of features. T"
W17-2320,W07-1011,0,0.0866677,"Missing"
W17-2320,P07-1034,0,0.0748422,"ied to negation detection (or any other biomedical NLP tasks). One difficulty of SCL is in selecting the pivot features, for which most existing approaches use heuristics about what features are likely to be domain independent. Another approach to UDA, known as bootstrapping or self-training, uses a classifier trained in the source domain to label target instances, and adds confidently predicted target instances to the training data with the predicted label. This method has been successfully applied to POS tagging, spam email classification, named entity classification, and syntactic parsing (Jiang and Zhai, 2007; McClosky et al., 2006). Clinical negation detection has a long history because of its importance to clinical information extraction. Rule-based systems such as Negex (Chapman et al., 2001) and its successor, ConText (Harkema et al., 2009) contain manually curated lists of negation cue words and apply rules about their scopes based on word distance and intervening cues. While these methods do not learn, the word distance parameter can be tuned by experts to apply to their own datasets. The DepNeg system (Sohn et al., 2012) used manually curated dependency path features in a rule-based system"
W17-2320,N06-1020,0,0.0189948,"ion (or any other biomedical NLP tasks). One difficulty of SCL is in selecting the pivot features, for which most existing approaches use heuristics about what features are likely to be domain independent. Another approach to UDA, known as bootstrapping or self-training, uses a classifier trained in the source domain to label target instances, and adds confidently predicted target instances to the training data with the predicted label. This method has been successfully applied to POS tagging, spam email classification, named entity classification, and syntactic parsing (Jiang and Zhai, 2007; McClosky et al., 2006). Clinical negation detection has a long history because of its importance to clinical information extraction. Rule-based systems such as Negex (Chapman et al., 2001) and its successor, ConText (Harkema et al., 2009) contain manually curated lists of negation cue words and apply rules about their scopes based on word distance and intervening cues. While these methods do not learn, the word distance parameter can be tuned by experts to apply to their own datasets. The DepNeg system (Sohn et al., 2012) used manually curated dependency path features in a rule-based system to abstract away from su"
W17-2320,P16-1210,1,0.8902,"Missing"
W17-2320,N15-1010,1,0.836611,"ent. The SCL method 165 Proceedings of the BioNLP 2017 workshop, pages 165–170, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics combines source and target extracted feature sets, and trains classifiers to predict the value of pivot features, uses singular value decomposition to reduce the dimensionality of the pivot feature space, and uses this reduced dimensionality space as an additional set of features. This method has been successful for part of speech tagging (Blitzer et al., 2006), sentiment analysis (Blitzer et al., 2007), and authorship attribution (Sapkota et al., 2015), among others, but to our knowledge has not been applied to negation detection (or any other biomedical NLP tasks). One difficulty of SCL is in selecting the pivot features, for which most existing approaches use heuristics about what features are likely to be domain independent. Another approach to UDA, known as bootstrapping or self-training, uses a classifier trained in the source domain to label target instances, and adds confidently predicted target instances to the training data with the predicted label. This method has been successfully applied to POS tagging, spam email classification"
W17-2320,P15-2028,0,0.0156238,"rization with 10x penalty, SCL A+N is structural correspondence learning with all features in addition to projected (new) features, SCL P+N is SCL with pivot features and projected features, BS-All=Bootstrapping with instances of all classes added to source, BS-Minority=Bootstrapping with only instances of minority class added to source, ISF=Instance similarity features. label. We therefore experiment with only adding minority class instances, enriching the training data to have a more even class distribution. The final UDA algorithm we experiment with uses instance similarity features (ISF) (Yu and Jiang, 2015). This method extends the feature space in the source domain with a set of similarity features computed by comparison to features extracted from target domain instances. Formally, the method selects a random subset of K exemplar instances from Dt and normalizes them as ~ˆe = ||~~ee ||. Similarity feature k for instance i in the source data set is computed as the dot product Xt [i] · ~ˆe[k]. Following Yu and Jiang, we set K = 50 and concatenate the similarity features to the full set of extracted features for each source instance at training. These exemplar instances must be kept around past tr"
W17-2320,P07-1056,0,\N,Missing
W17-2341,D13-1078,1,0.809433,"an as many as 10 tokens. CNNs, which represent meaning through fragments of word sequences, might struggle to compose these fragments to represent the meaning of time expressions. For example, can a CNN properly generalize that May 7 as a date is closer to April 30 than May 20? Can it embed years like 2012 and 2040 to recognize that the former was in the past, while the latter is in the future? Time normalization systems can handle such phenomena, but they are complex and language-specific, and often require significant manual effort to re-engineer for a new domain (Str¨otgen and Gertz, 2013; Bethard, 2013). Fortunately, not all tasks require full time normalization, so if the CNN can at least embed a meaningful subset of the time expression semantics, it may still be helpful in such tasks. An open question then, is how to best feed time expressions to the CNN so that it can usefully generalize over them as part of its solution to a larger task. We propose representing time expressions as single pseudo-tokens, with single vector representations (as in Figure 1), that encode easily extractable information about the time expression that is valuable for the task of temporal relation extraction. The"
W17-2341,E17-2118,1,0.828981,"traction. 2 Methods We trained two CNN-based classifiers for recognizing two types of within-sentence temporal relations, event-event and event-time relations, as they usually call for different temporal cues (Lin et al., 2016a). The input to our classifiers was manually annotated (gold) events and time expressions during both training and testing stages. That way we isolated the task of time expression representation for temporal relation extraction from the tasks of event and time expression recognition. We adopted the same xml-tag marked-up token sequence representation and model setup as (Dligach et al., 2017). Figure 2(1) illustrates the marked-up token sequence for an event-time instance, in which the event is marked by hei and h/ei and the time expression is marked by hti and h/ti. Event-event instances are handled similarly, e.g. a he1i surgery h/e1i is he2i scheduled h/e2i on march 11. We tried different ways of representing a time expression as a one-token tag. The most coarse option would be to represent all time expressions with one universal tag, htimexi, as in Figure 2(2). For more granular options, we experimented with these additional representations: 1) The time class1 of a time expres"
W17-2341,P14-1062,0,0.00458677,"not be an ideal representation for time expressions, which are long, highly varied, and semantically complex. We describe a method for representing time expressions with single pseudotokens for CNNs. With this method, we establish a new state-of-the-art result for a clinical temporal relation extraction task. 1 Introduction Convolutional Neural Networks (CNNs) utilize convolving filters and pooling layers for exploring and subsampling a feature space, and show excellent results in tasks such as semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalchbrenner et al., 2014), and many other natural language processing (NLP) tasks (Collobert et al., 2011). Token sequences are often used as the input for a CNN model in NLP. Each token is represented as a vector. Such vectors could be either word embeddings trained on the fly (Kalchbrenner et al., 2014), pre-trained on a corpus (Pennington et al., 2014; Mikolov et al., 2013), or one-hot vectors that index the token into a vocabulary (Johnson and Zhang, 2014). CNN filters then act as n-grams over continuous representations. Subsequent network layers learn to combine these n-gram filters to detect patterns in the inpu"
W17-2341,W16-2914,1,0.909188,"ring (Das and Musen, 1995; Kahn et al., 1990), clinical outcomes prediction (Schmidt et al., 2005), and the recognition of temporal patterns and timelines (Zhou and Hripcsak, 2007; Lin et al., 2014). Through experiments, we not only demonstrate the usefulness of one-tag representations for time expressions, but also establish a new state-of-theart result for clinical temporal relation extraction. 2 Methods We trained two CNN-based classifiers for recognizing two types of within-sentence temporal relations, event-event and event-time relations, as they usually call for different temporal cues (Lin et al., 2016a). The input to our classifiers was manually annotated (gold) events and time expressions during both training and testing stages. That way we isolated the task of time expression representation for temporal relation extraction from the tasks of event and time expression recognition. We adopted the same xml-tag marked-up token sequence representation and model setup as (Dligach et al., 2017). Figure 2(1) illustrates the marked-up token sequence for an event-time instance, in which the event is marked by hei and h/ei and the time expression is marked by hti and h/ti. Event-event instances are"
W17-2341,W15-3809,1,0.831139,"scheduled on hti mar 11 h/ti . 2: a hei surgery h/ei is scheduled on hti htimexi h/ti . 3: a hei surgery h/ei is scheduled on hti hdatei h/ti . 4: a hei surgery h/ei is scheduled on hti hnn cdi h/ti . 5: a hei surgery h/ei is scheduled on hti hdate nn cdi h/ti . 6: a hei surgery h/ei is scheduled on hti hindex 721i h/ti . 7: a hei surgery h/ei is scheduled on hti mar 11 hdatei h/ti . 8: hoi hoi hoi hoi hoi hbi hii hoi hoi 9: hoi hoi hoi hoi hoi hb datei hi datei hoi hoi 10: a he1i surgery h/e1i is he2i scheduled h/e2i on . Figure 2: Representations of an input sequence head and high accuracy (Miller et al., 2015). 2) CNN filters are more effective because they operate over the time expression as one unit. The filter process can thus focus on the informative surrounding context to catch generalizable patterns instead of being trapped within lengthy time expressions. We explored a variety of one-tag representations for time expressions, from very specific to very general. We also experimented with other ways to inject temporal information into the CNN models and compared them with our one-tag representations. We picked a challenging learning task where time expressions are critical cues for evaluating o"
W17-2341,D14-1162,0,0.124571,"ional Neural Networks (CNNs) utilize convolving filters and pooling layers for exploring and subsampling a feature space, and show excellent results in tasks such as semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalchbrenner et al., 2014), and many other natural language processing (NLP) tasks (Collobert et al., 2011). Token sequences are often used as the input for a CNN model in NLP. Each token is represented as a vector. Such vectors could be either word embeddings trained on the fly (Kalchbrenner et al., 2014), pre-trained on a corpus (Pennington et al., 2014; Mikolov et al., 2013), or one-hot vectors that index the token into a vocabulary (Johnson and Zhang, 2014). CNN filters then act as n-grams over continuous representations. Subsequent network layers learn to combine these n-gram filters to detect patterns in the input sequence. This token vector sequence representation has worked for many NLP tasks, but has not been wellstudied for temporal relation extraction. Time expressions are complex linguistic expressions that are challenging to represent because of their length and variety. For example, for the time expressions in the THYME (Styler I"
W17-2341,Q14-1012,1,0.302362,"Missing"
W17-2341,P14-2105,0,0.0108944,"tional Neural Networks (CNNs) in natural language processing. However, they might not be an ideal representation for time expressions, which are long, highly varied, and semantically complex. We describe a method for representing time expressions with single pseudotokens for CNNs. With this method, we establish a new state-of-the-art result for a clinical temporal relation extraction task. 1 Introduction Convolutional Neural Networks (CNNs) utilize convolving filters and pooling layers for exploring and subsampling a feature space, and show excellent results in tasks such as semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalchbrenner et al., 2014), and many other natural language processing (NLP) tasks (Collobert et al., 2011). Token sequences are often used as the input for a CNN model in NLP. Each token is represented as a vector. Such vectors could be either word embeddings trained on the fly (Kalchbrenner et al., 2014), pre-trained on a corpus (Pennington et al., 2014; Mikolov et al., 2013), or one-hot vectors that index the token into a vocabulary (Johnson and Zhang, 2014). CNN filters then act as n-grams over continuous representations. Sub"
W17-2341,C16-1223,0,0.0297418,"nt relations have lower inter-annotator agreement and usually leverage more of the syntactic information and event properties (Xu et al., 2013), which are not perfectly captured by token sequences. The class imbalance issues are more severe for event-event relations than for event-time relations as well (Dligach et al., 2017). These likely lead to a lower performance for event-event CNNs. In the future, we will investigate methods to improve the event-event model including incorporating syntactic information and event properties into a deep neural framework, and positive instance augmentation Yu and Jiang (2016). Word embeddings trained by conventional methods such as word2vec and GloVe did not prove to be useful in our preliminary experiments. This is likely due to (1) lack of sufficiently large publicly available domain-specific corpora, and (2) inability of the conventional methods to capture the semantic properties of events that are key for the relation extraction task (such as event durations). Currently, when we combined our encoded CNN-based event-time model with the THYME event-event model, we achieved the state-of-theart performance (0.621F) on the colon cancer data. The best 2016 Clinical"
W17-2341,S16-1165,1,\N,Missing
W18-5619,E17-2118,1,0.624941,"multiple external data sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to i"
W18-5619,W06-0204,0,0.0465686,"14; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006; Rosenfeld and Feldman, 2007; Xu, 2008; Xu et al., 2007, 2010). To our best knowledge, we are the first to use self-training in a deep neural network setting for a clinical relation extraction task. Our motivation lies in two folds: 1) Self-training is computationally efficient as there is no other parallel learning goals such 166 as minimizing the reconstruction errors in Generative Adversarial Networks-based semi-supervised learning. With primitive features, DNN-based selftraining can effectively and efficiently evaluate a large amount of instances; 2) We hypothesize that not all unlabeled"
W18-5619,guthrie-etal-2006-closer,0,0.0342654,"nal unlabeled out-of-domain data (i.e. brain cancer clinical notes). 3.2.1 Clinical Word Embeddings To train word embeddings with good vocabulary coverage and high representational power, we took advantage of the clinical notes from MIMIC-III (Medical Information Mart for Intensive Care) dataset (Johnson et al., 2016). The publicly available MIMIC III contains 879 million words from Beth Israel Deaconess Medical Center’s Intensive Care Unit. We merged MIMIC-III data with the unlabeled colon cancer set above and trained 300dimension embeddings with fastText (Joulin et al., 2016) and skip-gram (Guthrie et al., 2006) models. 3.2.2 Social Media Word Embeddings While unlabeled clinical data provides a domainmatched source for training embeddings, additional data can be freely obtained from social media posts about colon cancer. To explore the benefits of extra coverage of such datasets versus the domain specificity of clinical embeddings, we obtain another set of embeddings using user-generated content about colon cancer from two social media platforms, namely Twitter and Reddit. For this purpose, we first generate a keyword list from two sources: a) the most frequent medical terms in the unlabeled colon ca"
W18-5619,D13-1137,0,0.0278187,"pes used by these networks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods. For pre-training, we investigate the use of multiple external data sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framewo"
W18-5619,S15-2136,1,0.871968,"ral networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to information extraction can outperform featureengineering approaches, we find that self-training works better in the neural network setting than with existing state-of-the-art feature-engineering approaches. Finally, we show that these methods generalize to new clinical domains better than the feature-engineering approaches we compare them"
W18-5619,S17-2093,1,0.910128,"Missing"
W18-5619,W16-2914,1,0.827441,"ing that such performance parity is not obtainable without extensive feature engineering. Unlike other settings that have seen performance gains, information extraction tasks related to text typically have much smaller supervised training sets, and the neural network algorithms presumably do not see enough instances to optimally tune the large parameter space. In this paper, we examine the important information extraction task of temporal relation extraction from clinical text. The state-of-the-art for this task is a machine learner with a heavily-engineered set of features (Sun et al., 2013; Lin et al., 2016a). The identification of temporal relations from the clinical text in the electronic medical records has been drawing growing attention because of its potential to provide accurate fine-grained analyses of many medical phenomena (e.g., disease progression, longitudinal effects of medications), with many clinical applications such as question answering (Das and Musen, 1995; Kahn et al., 1990), clinical outcomes prediction (Schmidt et al., 2005), and recognition of temporal patterns and timelines (Zhou and Hripcsak, 2007; Lin et al., 2014). Obtaining large supervised datasets for clinical tasks"
W18-5619,W17-2341,1,0.609658,"sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to information extracti"
W18-5619,E17-1108,0,0.372144,"re engineering methods. For pre-training, we investigate the use of multiple external data sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger d"
W18-5619,S17-2180,0,0.112986,"sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to information extracti"
W18-5619,P04-3028,0,0.053647,"available gold standard datasets for temporal information extraction. The results of Clinical TempEval 2017 (Bethard et al., 2017) strongly support this latter point, as the performance of submitted systems drops severely when trained on gold instances in one domain and tested on a new domain. We are thus inspired to make use of unlabeled data in addition to gold standard data with a simple semisupervised learning method–self-training and combine it with varieties of pre-trained word embeddings to overcome gaps in training data coverage. In self-training (Yarowsky, 1995; Riloff et al., 2003; Maeireizo et al., 2004), a classifier is first trained on existing labeled data, and then applied to unlabeled data (typically a much larger amount). The predicted instances above a confidence threshold are added to the training set and the classifier is re-trained. Self-training is especially attractive in a neural network setting because the primitive feature types used by these networks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods. For pre-training, we investigate the use of multiple external data sources to train word em"
W18-5619,P06-1095,0,0.0208379,"esentations for all three relational candidates, in which the event in an event-time relation pair is marked by hei and h/ei and the time expression is marked by hti and h/ti. The time expression is further encoded by its time class, hti hdatei h/ti, which is a gold standard attribute of a time expression annotation (Styler IV et al., 2014). Event-event instances are marked with additional indexes 1 and 2, e.g. a he1i surgery h/e1i is he2i scheduled h/e2i on march 11. We also follow previous best practice in applying transitive closure to existing gold CONTAINS relations on the training data (Mani et al., 2006; Lin et al., 2016a). Depending on the order of the relational arguments, there are three types of gold standard relational labels, CONTAINS, CONTAINED3 http://ctakes.apache.org E VENT 2 T IME A surgery was scheduled on March 11, 2014 ⇓ Candidate 1: a hei surgery h/ei was scheduled on hti hdatei h/ti; Candidate 2: a surgery was hei scheduled h/ei on hti hdatei h/ti; Candidate 3: a he1i surgery h/e1i was he2i scheduled h/e2i on march Figure 2: Representations of event-event and eventtime relational candidates in a sentence BY, and NONE. 4.3 Bidirectional RNN Classifier We use a bi-directional r"
W18-5619,W15-3809,1,0.841909,"E VENT 1 tion, since we use the official Clinical TempEval 2017 scoring tool, our models are penalized for the missed cross-sentence relations. 4.1 Preprocessing We process the labeled and unlabeled clinical data through the sentence detection and tokenization modules of Apache cTAKES3 . For the labeled clinical data, we use gold standard event and time expression annotations and their time classes (Styler IV et al., 2014) for both model development and final validation. For the unlabeled clinical text data, we use the cTAKES event annotator (Lin et al., 2016a) and time expression annotator (Miller et al., 2015) to automatically annotate event and time expressions along with their time classes (e.g., TIME, DATE, SET). Both labeled and unlabeled corpora are transformed to lower case as shown in Figure 2. 4.2 Instance Representation We first create a dataset of within-sentence CONTAINS-relation candidates from the colon cancer text of the labeled clinical data. Given all gold standard events and time expressions within a sentence, we link every pair of events, and every event to a time expression (if present) to form CONTAINS candidates. To mark the position of the relational arguments in a candidate p"
W18-5619,P14-1076,0,0.0203719,"it was trained on. The source domain of Clinical TempEval 2017 is colon cancer clinical text while the target domain is brain cancer clinical text. Few domain adaptation techniques are applied by the participants: 1) modeling unknown words to accommodate unseen vocabulary in the new domain; 2) using pretrained domain-independent word embeddings; 3) for supervised domain adaptation, assigning higher weights to samples from the new domain during model training. The performance on the domain adaptation task plummetted. Other domain adaptation methods used in general relation extraction include (Nguyen et al., 2014; Nguyen and Grishman, 2014; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel"
W18-5619,P14-2012,0,0.0200696,"he source domain of Clinical TempEval 2017 is colon cancer clinical text while the target domain is brain cancer clinical text. Few domain adaptation techniques are applied by the participants: 1) modeling unknown words to accommodate unseen vocabulary in the new domain; 2) using pretrained domain-independent word embeddings; 3) for supervised domain adaptation, assigning higher weights to samples from the new domain during model training. The performance on the domain adaptation task plummetted. Other domain adaptation methods used in general relation extraction include (Nguyen et al., 2014; Nguyen and Grishman, 2014; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Gr"
W18-5619,P06-1015,0,0.0372333,"., 2014; Nguyen and Grishman, 2014; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006; Rosenfeld and Feldman, 2007; Xu, 2008; Xu et al., 2007, 2010). To our best knowledge, we are the first to use self-training in a deep neural network setting for a clinical relation extraction task. Our motivation lies in two folds: 1) Self-training is computationally efficient as there is no other parallel learning goals such 166 as minimizing the reconstruction errors in Generative Adversarial Networks-based semi-supervised learning. With primitive features, DNN-based selftraining can effectively and efficiently evaluate a large amount of instances; 2) We hypo"
W18-5619,P13-1147,0,0.0212914,"l TempEval 2017 is colon cancer clinical text while the target domain is brain cancer clinical text. Few domain adaptation techniques are applied by the participants: 1) modeling unknown words to accommodate unseen vocabulary in the new domain; 2) using pretrained domain-independent word embeddings; 3) for supervised domain adaptation, assigning higher weights to samples from the new domain during model training. The performance on the domain adaptation task plummetted. Other domain adaptation methods used in general relation extraction include (Nguyen et al., 2014; Nguyen and Grishman, 2014; Plank and Moschitti, 2013). Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006;"
W18-5619,W11-0419,0,0.325967,"o, obtaining state-of-the-art performance in an unsupervised domain adaptation setting. 2 Related Work In recent years, several shared tasks on temporal relation extraction from clinical text have been organized. Among them, the i2b2 temporal challenge evaluates the i2b2 corpus (Sun et al., 2013), and Clinical TempEval series (Bethard et al., 2015, 2016, 2017) evaluate systems using the THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) per an extension of the TimeML specifications (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011). Challenge participants develop methods to extract EVENT and TIMEX3 entities, CONTAINS relations and document creation time relations. Herein, we focus on CONTAINS relation, which signals an EVENT occurs entirely within the temporal bounds of an narrative container. The narrative container is either another EVENT or TIMEX3. Conventional learning methods, such as support vector machines (SVM) and conditional random fields (CRF) (Sun et al., 2013), have been develClinical TempEval 2017 introduces the task of domain adaptation, as the most frequent use case would be the application of a model on"
W18-5619,W03-0404,0,0.14125,"Missing"
W18-5619,P07-1076,0,0.0197391,"Semi-supervised learning has been a popular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006; Rosenfeld and Feldman, 2007; Xu, 2008; Xu et al., 2007, 2010). To our best knowledge, we are the first to use self-training in a deep neural network setting for a clinical relation extraction task. Our motivation lies in two folds: 1) Self-training is computationally efficient as there is no other parallel learning goals such 166 as minimizing the reconstruction errors in Generative Adversarial Networks-based semi-supervised learning. With primitive features, DNN-based selftraining can effectively and efficiently evaluate a large amount of instances; 2) We hypothesize that not all unlabeled data are useful. Our goal is"
W18-5619,D12-1110,0,0.0274721,"ks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods. For pre-training, we investigate the use of multiple external data sources to train word embeddings that form the input layer of the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clin"
W18-5619,P17-2035,0,0.624158,"the model. Since our 165 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 165–176 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics oped for this task. Neural networks used in general relation extraction (Hashimoto et al., 2013; Socher et al., 2012), have also been adopted in clinical temporal relation extraction, such as structured perceptron (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017) and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017). Classifiers are usually trained and tested in the same domain for the same medical condition, e.g. models are trained and tested on the colon cancer set of the THYME corpus for Clinical TempEval 2015 and 2016 (Bethard et al., 2015, 2016). Figure 1: A RNN-based Self-training Framework task is in the clinical setting, we use available clinical data sources, but also experiment with general domain sources trained on much larger datasets. Besides showing that neural network approaches to information extraction can outperform featureengineering approaches, we find that self"
W18-5619,P11-2061,0,0.0343755,"YME corpus and silver instances predicted from the unlabeled colon cancer data. Models were tested on the gold colon cancer and gold brain cancer development sets of the THYME corpus, comparing in-domain and cross-domain performance to select the best models for testing. The best models were tested on the gold colon cancer and brain cancer test sets (Clinical TempEval 2017 test sets). All models were evaluated with the metrics precision (P), recall (R) and F1-score (F), using the standard Clinical TempEval evaluation script, where the P and R definitions are enhanced through temporal closure (UzZaman and Allen, 2011; UzZaman et al., 2012): when calculating precision, we run temporal closure on the gold relations but not on the system-generated ones; when calculating recall, we run temporal closure on the systemgenerated relations but not on the gold ones. 169 6 Results Table 3 shows performance of the THYME system and various bi-directional RNN methods on the colon cancer and brain cancer development sets. For RNNs, we evaluated both LSTM and GRU models. For embedding combinations, we tested using the clinical embedding alone (C), using both clinical and cancer-related social media embeddings (CS), using"
W18-5619,C10-2155,0,0.0593427,"Missing"
W18-5619,P07-1074,0,0.0303758,"pular approach for improving coverage and model generalizability for various information extraction tasks by exploring unlabeled data. Besides semi-supervised methods developed for feature-based learners (Le and Kim, 2015; Li and Zhou, 2010), there are such algorithms for deep neural network structures (DNN) (Laine and Aila, 2016; Kingma et al., 2014). Self-training or bootstrapping is a standard and straightforward semi-supervised learning method and widely used (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006; Greenwood and Stevenson, 2006; Rosenfeld and Feldman, 2007; Xu, 2008; Xu et al., 2007, 2010). To our best knowledge, we are the first to use self-training in a deep neural network setting for a clinical relation extraction task. Our motivation lies in two folds: 1) Self-training is computationally efficient as there is no other parallel learning goals such 166 as minimizing the reconstruction errors in Generative Adversarial Networks-based semi-supervised learning. With primitive features, DNN-based selftraining can effectively and efficiently evaluate a large amount of instances; 2) We hypothesize that not all unlabeled data are useful. Our goal is to use a straightforward me"
W18-5619,P95-1026,0,0.678514,"much broader than what is covered by available gold standard datasets for temporal information extraction. The results of Clinical TempEval 2017 (Bethard et al., 2017) strongly support this latter point, as the performance of submitted systems drops severely when trained on gold instances in one domain and tested on a new domain. We are thus inspired to make use of unlabeled data in addition to gold standard data with a simple semisupervised learning method–self-training and combine it with varieties of pre-trained word embeddings to overcome gaps in training data coverage. In self-training (Yarowsky, 1995; Riloff et al., 2003; Maeireizo et al., 2004), a classifier is first trained on existing labeled data, and then applied to unlabeled data (typically a much larger amount). The predicted instances above a confidence threshold are added to the training set and the classifier is re-trained. Self-training is especially attractive in a neural network setting because the primitive feature types used by these networks (i.e., tokens) are computationally more efficient to obtain than the sophisticated features typically used by feature engineering methods. For pre-training, we investigate the use of m"
W19-1908,E17-1108,0,0.0592049,"d for specific domains (Lee et al., 2019) or serve as a backbone model to be fine-tuned with one output layer for a wide range of tasks. Figure 1: Representations of three candidate relations produced from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical Temp"
W19-1908,C18-1139,0,0.0215405,"es of BERT motivate us to apply it to a traditionally sentence-level task – temporal relation extraction from clinical text. The identification of temporal relations in the clinical narrative can lead to accurate fine-grained analyses of many medical phenomena (e.g., disease progression, longitudinal effects of medications), with a variety of clinical applications such as question answering (Das and Musen, 1995; Kahn et al., 2 Background Recently, several pre-trained general-purposed language encoders have been proposed, including CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), GPT (Radford et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2018). These models are trained on vast amounts of unlabeled text to achieve 65 Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 65–71 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics E VENT 1 generalizable contextualized word embeddings, and some can be fine-tuned to fit a supervised task. E VENT 2 T IME . A surgery was scheduled on March 11, 2014. ⇓ #1: . a es surgery ee was scheduled on ts date te . #2: . a surgery was es scheduled ee on ts dat"
W19-1908,S15-2136,1,0.956288,"thard@email.arizona.edu Abstract 1990), clinical outcomes prediction (Schmidt et al., 2005), and recognition of temporal patterns and timelines (Zhou and Hripcsak, 2007; Lin et al., 2014). However, the labeled instances for this clinical information extraction task are limited, so neural models trained from scratch may not be able to learn complex linguistic phenomena. Pre-trained models like BERT could potentially provide rich representations as they are trained on massive data. Classic models for clinical temporal relation extraction have framed the task within a sentence (Sun et al., 2013; Bethard et al., 2015, 2016, 2017), making them susceptible to sentence detection errors. Using BERT, on the other hand, eliminates this sensitivity to sentence boundary errors. The key contributions of this paper are: (1) introducing BERT to the challenging task of clinical temporal relation extraction and evaluating its performance on a widely used testbed (THYME corpus; Styler IV et al., 2014), (2) developing a universal processing mechanism based on a fixed, sentenceboundary agnostic window of contiguous tokens, (3) pre-training BERT on MIMIC-III (Medical Information Mart for Intensive Care) dataset (Johnson e"
W19-1908,W18-5619,1,0.837014,"ed from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Methods Task definition We process the THYME corpus using the segmentation and tokenization modules of Apache cTAKES (http://ctakes.a"
W19-1908,S17-2093,1,0.901456,"Missing"
W19-1908,W17-2341,1,0.929808,"th one output layer for a wide range of tasks. Figure 1: Representations of three candidate relations produced from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Methods Task definition W"
W19-1908,N18-1202,0,0.0331039,"ple sentences. These advantages of BERT motivate us to apply it to a traditionally sentence-level task – temporal relation extraction from clinical text. The identification of temporal relations in the clinical narrative can lead to accurate fine-grained analyses of many medical phenomena (e.g., disease progression, longitudinal effects of medications), with a variety of clinical applications such as question answering (Das and Musen, 1995; Kahn et al., 2 Background Recently, several pre-trained general-purposed language encoders have been proposed, including CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), GPT (Radford et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2018). These models are trained on vast amounts of unlabeled text to achieve 65 Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 65–71 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics E VENT 1 generalizable contextualized word embeddings, and some can be fine-tuned to fit a supervised task. E VENT 2 T IME . A surgery was scheduled on March 11, 2014. ⇓ #1: . a es surgery ee was scheduled on ts date te . #2: . a surgery w"
W19-1908,E17-2118,1,0.936835,"el to be fine-tuned with one output layer for a wide range of tasks. Figure 1: Representations of three candidate relations produced from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Method"
W19-1908,W18-5607,0,0.317533,"tandard entities, of which two are events, “surgery” and “scheduled”, and one is a time expression, “March 11, 2014”, whose time class is “date”. One can form three candidate relations for these three entities. CONTAINS relations are by far the most frequent type of relation in the THYME corpus. They signal that an EVENT occurs entirely within the temporal bounds of a narrative container (Pustejovsky and Stubbs, 2011). The THYME corpus is limited in size so models developed on it may suffer from low generalizability. Recent efforts to improve performance have attempted tree-structured models (Galvan et al., 2018) or assistance from unlabeled data (Lin et al., 2018). Years of shared work on this problem and plateauing scores may have suggested that performance on this task is at its peak. However, given the successful application of BERT on many different tasks in the general domain, as well as more recent work in relation extraction tasks (Wang et al., 2019; Lee et al., 2019), we wanted to explore applying this new model to the clinical temporal relation extraction task. 3.2 Window-based processing We aim to build a BERT-based model for both within- and cross-sentence relations. Figure 2 presents the"
W19-1908,W11-0419,0,0.659617,"ears have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Methods Task definition We process the THYME corpus using the segmentation and tokenization modules of Apache cTAKES (http://ctakes.apache.org). We consume gold standard event annotations, gold time expressions and their classes (Styler IV et al., 2014) for generating instances of containment relation candidates. Each instance consists of a pair of event entities, or an event entity and a time expression entity. We preserve the natural order of the two entities in their o"
W19-1908,P17-2035,0,0.150458,"entations of three candidate relations produced from an example token sequence. sentence-boundary detection errors. The input sequences of arbitrary lengths that BERT operates on cover both within-sentence and cross-sentence situations, enabling us to design a universal model that is sentence boundary agnostic. For the task of clinical temporal relation extraction, recent years have seen the rise of neural approaches – structured perceptrons (Leeuwenberg and Moens, 2017), convolutional neural networks (CNNs) (Dligach et al., 2017; Lin et al., 2017), and Long Short-Term memory (LSTM) networks (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2018) – where minimally-engineered inputs have been adopted over heavily featureengineered techniques (Sun et al., 2013). The THYME corpus (Styler IV et al., 2014), which is annotated with time expressions (TIMEX3), events (EVENT), and temporal relations (TLINK) using an extension of TimeML (Pustejovsky et al., 2003; Pustejovsky and Stubbs, 2011), is a popular choice for evaluation and was used in the Clinical TempEval series (Bethard et al., 2015, 2016, 2017). 3 3.1 Methods Task definition We process the THYME corpus using the segmentation and tokenization"
W19-1908,P19-1132,0,0.0558117,"Missing"
W19-2506,E12-1036,0,0.0201807,"e first to look at extracting metadata fields specific to such documents, such as the lead federal agency. Though there is some relation between extracting lead agencies and extracting other organizational information like affiliations (Jonnalagadda and Topham, 2010) or science funding bodies (Kayal et al., 2017), the different role that lead agencies play in drafting environmental policy documents yields a different information extraction problem. There is some prior work on automatically analyzing edits between document versions. Some have focused on classifying edits in Wikipedia articles (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013), and Goyal et al. (2017) measured the importance of different kinds of changes between versions of news articles. The EIS documents we analyze have a very different semantics to their versioning. The NEPA process specifies that a public comment period must come between the draft and final EIS, and it is expected that the changes between versions will address issues raised during this period. Thus, our data yields a unique possibility of investigating how external comments influence document versions. 7 interface (API) to provide access to researchers and other"
W19-2506,D18-2029,0,0.027505,"Missing"
W19-2506,D13-1055,0,0.0155713,"cting metadata fields specific to such documents, such as the lead federal agency. Though there is some relation between extracting lead agencies and extracting other organizational information like affiliations (Jonnalagadda and Topham, 2010) or science funding bodies (Kayal et al., 2017), the different role that lead agencies play in drafting environmental policy documents yields a different information extraction problem. There is some prior work on automatically analyzing edits between document versions. Some have focused on classifying edits in Wikipedia articles (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013), and Goyal et al. (2017) measured the importance of different kinds of changes between versions of news articles. The EIS documents we analyze have a very different semantics to their versioning. The NEPA process specifies that a public comment period must come between the draft and final EIS, and it is expected that the changes between versions will address issues raised during this period. Thus, our data yields a unique possibility of investigating how external comments influence document versions. 7 interface (API) to provide access to researchers and other interested parties. The server a"
W19-2506,D17-1295,0,0.0158951,"h documents, such as the lead federal agency. Though there is some relation between extracting lead agencies and extracting other organizational information like affiliations (Jonnalagadda and Topham, 2010) or science funding bodies (Kayal et al., 2017), the different role that lead agencies play in drafting environmental policy documents yields a different information extraction problem. There is some prior work on automatically analyzing edits between document versions. Some have focused on classifying edits in Wikipedia articles (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013), and Goyal et al. (2017) measured the importance of different kinds of changes between versions of news articles. The EIS documents we analyze have a very different semantics to their versioning. The NEPA process specifies that a public comment period must come between the draft and final EIS, and it is expected that the changes between versions will address issues raised during this period. Thus, our data yields a unique possibility of investigating how external comments influence document versions. 7 interface (API) to provide access to researchers and other interested parties. The server and API will be hosted at"
W19-2506,W17-2327,0,0.0220456,"science documents, such as newswire sources (S¨onmez et al., 2016) or historical archives (Zervanou et al., 2011). However, to the best of our knowledge, ours is the first 49 project to consider the large number of environmental policy documents produced within the NEPA framework. Our project is also the first to look at extracting metadata fields specific to such documents, such as the lead federal agency. Though there is some relation between extracting lead agencies and extracting other organizational information like affiliations (Jonnalagadda and Topham, 2010) or science funding bodies (Kayal et al., 2017), the different role that lead agencies play in drafting environmental policy documents yields a different information extraction problem. There is some prior work on automatically analyzing edits between document versions. Some have focused on classifying edits in Wikipedia articles (Bronner and Monz, 2012; Daxenberger and Gurevych, 2013), and Goyal et al. (2017) measured the importance of different kinds of changes between versions of news articles. The EIS documents we analyze have a very different semantics to their versioning. The NEPA process specifies that a public comment period must c"
W19-2506,W11-1507,0,0.0832153,"Missing"
