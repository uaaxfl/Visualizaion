2001.jeptalnrecital-long.13,P99-1044,1,0.897487,"ment Question/Phrase Séquence ordonnée de 250 et 50 caractères Figure 1. Architecture du système QALC L'analyse des questions est réalisée par un analyseur partiel dédié qui attribue aux questions des catégories correspondant aux types d'entités nommées pouvant répondre à la question. La sélection d’un sous-ensemble de documents pertinents repose sur la reconnaissance des termes de la question ou de leurs variantes dans les documents sélectionnés par un moteur de Entités nommées et variantes dans un système de question-réponse recherche classique. Cette reconnaissance est effectuée par FASTR (Jacquemin, 1999) sur la base des termes extraits de la question. Cette sélection revêt toute son importance lorsque le système applique les processus ultérieurs, à savoir la reconnaissance des entités nommées telles que les personnes, organisations, lieux et valeurs numériques, et la comparaison entre phrase et question, processus fortement consommateurs de temps de traitement. Le dernier module, qui propose un ensemble limité de réponses à chaque question, met en œuvre un calcul de similarité entre une question, représentée par un vecteur contenant ses mots pleins lemmatisés, ses termes et le type attendu de"
2001.jeptalnrecital-long.14,C98-1062,1,0.929344,"produit scalaire de ces vecteurs permet ensuite de regrouper ou de séparer les zones qu’ils décrivent s’ils sont proches ou non. La deuxième méthode d’analyse, quant à elle, utilise des sources de connaissances externes non dédiées. En généralisant la notion de répétition de termes, la cohésion thématique d’un texte se traduit par l’utilisation de termes faisant référence à une même entité par l’emploi de synonymes, d’hyponymes, de mots liés sémantiquement ou appartenant au même domaine. Afin d’introduire ces caractéristiques dans une analyse vectorielle d’un texte, les travaux décrits dans (Ferret et al., 1998) utilisent un réseau de collocations construit automatiquement sur un corpus d’articles de journaux afin d’enrichir les vecteurs décrivant un paragraphe. Le principe utilisé consiste à augmenter la valeur de descripteurs lorsqu’ils sont liés de manière significative dans le réseau à un descripteur du paragraphe. Cette modification des vecteurs tend ainsi à rapprocher des paragraphes comportant des mots liés. Notre projet vise à exploiter la complémentarité des approches statistiques et de l’approche linguistique. Les deux méthodes statistiques offrent le moyen de segmenter thématiquement des t"
2001.jeptalnrecital-long.14,J97-1003,0,0.38874,"sur des critères statistiques ou numériques, part du constat que le développement d’un thème entraîne la reprise de termes spécifiques, notamment lorsqu’il s’agit de textes techniques ou scientifiques. La reconnaissance de parties de texte liées à un même sujet est alors fondée sur la distribution des mots et leur récurrence. Si un mot apparaît souvent dans l’ensemble du texte, il est peu significatif, alors que sa répétition dans une zone limitée est très significative pour caractériser le thème de cette partie de texte. Le principe général appliqué par les différents systèmes (Masson, 1995, Hearst, 1997) consiste à associer un vecteur de descripteurs à une zone de Olivier Ferret, Brigitte Grau, Jean-Luc Minel et Sylvie Porhiel texte, où les descripteurs sont typiquement les mots lemmatisés du texte et leurs valeurs, le nombre d’occurrences de ces termes dans la zone. Le produit scalaire de ces vecteurs permet ensuite de regrouper ou de séparer les zones qu’ils décrivent s’ils sont proches ou non. La deuxième méthode d’analyse, quant à elle, utilise des sources de connaissances externes non dédiées. En généralisant la notion de répétition de termes, la cohésion thématique d’un texte se traduit"
2001.jeptalnrecital-long.14,W97-0707,0,0.0280881,"ois, si la notion de cohérence a été abondamment commentée (Halliday et al., 1976, Charolles, 1995, 1997), elle a été peu prise en compte dans les systèmes de résumé automatique ou d’analyse thématique. En fait, lorsqu’il s’agit de repérer automatiquement les thématiques d’un texte et de prendre en charge la cohérence thématique, les systèmes de Olivier Ferret, Brigitte Grau, Jean-Luc Minel et Sylvie Porhiel résumé automatique se heurtent à toutes sortes de difficultés. Premièrement, les modèles qui intègrent et exploitent des connaissances ou des ressources linguistiques (Berri et al., 1996, Mitra et al., 1997) ne s’appuient pas sur une vision globale du texte et des thèmes abordés : ils se fondent sur la notion de saillance d’une unité textuelle, d’une phrase ou d’un paragraphe, et cette saillance est calculée indépendamment de la structure thématique du texte. Deuxièmement, ces systèmes ne répondent que partiellement aux besoins des utilisateurs : ce qui est pertinent pour les uns ne l’est pas pour les autres (Sparck Jones, 1993, Minel et al. 1997), notamment parce qu’un utilisateur peut être intéressé par une thématique qui n’est pas prise en charge directement par l’auteur. Afin d’améliorer la p"
2002.jeptalnrecital-long.28,P99-1044,0,0.0119111,"re longueur qui le forment. L&apos;analyse de la question se fonde sur l&apos;utilisation d&apos;un analyseur robuste dans le but d&apos;extraire plusieurs informations de la question, informations utiles pour la sélection de phrases ou pour l’extraction de la réponse. Cette partie sera développée dans la section suivante. Le module de traitement des documents utilise les sorties fournies par le NIST1, résultat de l&apos;application d’un moteur de recherche de type vectoriel sur le corpus de documents pour l’ensemble des questions de l’évaluation TREC. Les 200 meilleurs documents sont ré-indexés par le système FASTR (Jacquemin, 1999), analyseur transformationnel de surface qui reconnaît les occurrences et les variantes des termes produits par le module d&apos;extraction de 1 Le NIST est l’organisateur des conférences TREC. 309 Ferret O., Grau B., Hurault-Plantet M., Illouz G., Monceaux L., Robba I., Vilnat A. termes. Chaque occurrence ou variante constitue un index qui est ensuite utilisé dans le processus de classement des documents. En effet, ces index permettent à QALC de réordonner les documents et de sélectionner les plus pertinents (Ferret et al. 2001). Le module de reconnaissance des entités nommées est ensuite appliqué"
2003.jeptalnrecital-long.9,A97-1012,0,0.0518277,"Missing"
2003.jeptalnrecital-long.9,P02-1054,0,0.031363,"pté pour la recherche des réponses dans la collection de référence doublée d&apos; une autre dans une autre source d’informations afin de confronter les résultats des deux recherches. Le principe est de favoriser des réponses trouvées dans les deux sources, par rapport aux réponses, même fortement pondérées, mais trouvées dans une seule collection. Un tel raisonnement s’applique d’autant mieux que les sources de connaissances sont de nature différente, ainsi notre deuxième recherche s’effectue sur le Web, qui, de surcroît, par sa diversité et sa redondance conduit à trouver de nombreuses réponses (Magnini et al., 2002a et 2002b ; Clarke et al., 2001 ; Brill et al., 2001). Après la présentation générale de notre système, QALC, section 2, nous décrivons section 3 la reformulation des questions pour interroger le Web. La section 4 présente ensuite l’extraction des réponses pour une seule source de connaissances, et la section 5 les stratégies pour réaliser le choix final. Les résultats de QALC sont décrits en section 6 avant de rapprocher notre travail de ce qui existe dans le domaine. 2 Le système QALC Le système QALC (figure 1) participe aux évaluations TREC depuis 4 ans et a été conçu pour rechercher des r"
2005.jeptalnrecital-long.22,P94-1006,0,0.0650485,"Missing"
2005.jeptalnrecital-long.22,J97-1003,0,0.057678,"e : afin de manipuler des unités de texte de différentes granularités (i.e. différents degrés informationnels), de fournir un contexte à une information ciblée, de permettre une navigation intra-documentaire, etc. (Moens & Busser, 2001; Choi, 2002; Couto et al., 2004). En particulier nous nous focalisons sur la micro-structure d’un texte (niveau phrastique voire propositionnel). Nous affichons ainsi une complémentarité aux approches globales tout en offrant la possibilité de raffiner leur modèle. En effet qu’elles supposent une organisation plate et linéaire du flot d’informations communiqué (Hearst, 1997; Choi, 2002), ou bien une organisation plus riche en arbres (Moens & Busser, 2001; Couto et al., 2004), les approches globales sont généralement fondées sur des mesures de cohésion lexicale (notamment à travers le suivi de chaînes lexicales) qui souffrent d’un manque de précision quant à la délimitation des unités de texte (appelées segment). De plus elles prennent rarement en compte dans leur analyse les phénomènes discursifs locaux (e.g. annonces thématiques – e.g. “Les points que nous allons traiter sont :”, structures énumératives, transitions, etc.). Notre approche se situe parmi les tra"
2005.jeptalnrecital-long.22,P99-1047,0,0.162034,"r des mesures de cohésion lexicale (notamment à travers le suivi de chaînes lexicales) qui souffrent d’un manque de précision quant à la délimitation des unités de texte (appelées segment). De plus elles prennent rarement en compte dans leur analyse les phénomènes discursifs locaux (e.g. annonces thématiques – e.g. “Les points que nous allons traiter sont :”, structures énumératives, transitions, etc.). Notre approche se situe parmi les travaux qui proposent de rechercher le point d’attache optimal d’un énoncé entrant dans la structure en cours de construction. Parmi les approches existantes, Marcu (1999) propose un système pour la détection automatique de la structure rhétorique d’un texte, Choi (2002) s’intéresse à une structuration thématique fine, Kruijff-Korbayová & Kruijff (1996) analysent le discours en terme de progression thématique. Ces systèmes constituent de sérieuses avancées mais requièrent encore la prise en compte de plus d’indices discursifs et de modèles plus souples pour appréhender les différents mécanismes de structuration du discours. Dans ce papier, nous présentons un système de Détection de Structures fines de Texte (appelé DST). DST utilise un modèle prédictif obtenu p"
2006.jeptalnrecital-long.20,W02-1033,0,0.0939832,"Missing"
2007.jeptalnrecital-poster.17,C00-1039,0,0.0313156,"Missing"
2011.jeptalnrecital-court.7,P06-1114,0,0.0337413,"Missing"
2011.jeptalnrecital-court.7,quintard-etal-2010-question,1,0.881271,"Missing"
2011.jeptalnrecital-court.8,P08-1004,0,0.0388798,"Missing"
2011.jeptalnrecital-court.8,P04-1053,0,0.105174,"Missing"
2011.jeptalnrecital-court.8,P06-2094,0,0.0457351,"Missing"
2011.jeptalnrecital-court.8,N06-1039,0,0.059062,"Missing"
2011.jeptalnrecital-long.27,P04-1054,0,0.13033,"Missing"
2011.jeptalnrecital-long.27,N10-1004,0,0.0433158,"Missing"
2011.jeptalnrecital-long.27,E06-1015,0,0.0603226,"Missing"
2011.jeptalnrecital-long.27,W08-0602,0,0.0392329,"Missing"
2011.jeptalnrecital-long.27,N06-1037,0,0.0421661,"Missing"
2011.jeptalnrecital-long.27,P05-1053,0,0.0863018,"Missing"
2015.jeptalnrecital-long.11,P05-1045,0,0.0406432,"Missing"
2015.jeptalnrecital-long.11,O97-1002,0,0.509187,"Missing"
2015.jeptalnrecital-long.11,W06-1416,0,0.0810715,"Missing"
2015.jeptalnrecital-long.11,P03-1054,0,0.0234065,"Missing"
2015.jeptalnrecital-long.11,levy-andrew-2006-tregex,0,0.061128,"Missing"
2015.jeptalnrecital-long.11,P97-1009,0,0.173308,"Missing"
2015.jeptalnrecital-long.11,W03-0203,0,0.139574,"Missing"
2015.jeptalnrecital-long.11,W09-0207,0,0.0375078,"Missing"
2015.jeptalnrecital-long.11,pho-etal-2014-multiple,1,0.834521,"Missing"
2015.jeptalnrecital-long.15,S12-1051,0,0.0482533,"Missing"
2015.jeptalnrecital-long.15,P12-1047,0,0.0252732,"Missing"
2015.jeptalnrecital-long.15,C04-1051,0,0.0605597,"Missing"
2015.jeptalnrecital-long.15,I05-5002,0,0.124557,"Missing"
2015.jeptalnrecital-long.15,N10-1145,0,0.0513326,"Missing"
2015.jeptalnrecital-long.15,S13-2047,0,0.0315149,"Missing"
2015.jeptalnrecital-long.15,J10-3003,0,0.0730465,"Missing"
2015.jeptalnrecital-long.15,N12-1019,0,0.0360965,"Missing"
2015.jeptalnrecital-long.15,U06-1019,0,0.0611377,"Missing"
2015.jeptalnrecital-long.15,W07-1412,0,0.0747443,"Missing"
2017.jeptalnrecital-long.13,P14-1023,0,0.0994656,"Missing"
2017.jeptalnrecital-long.13,D07-1074,0,0.239996,"Missing"
2017.jeptalnrecital-long.13,K16-1026,0,0.0357843,"Missing"
2017.jeptalnrecital-long.13,Q15-1016,0,0.091516,"Missing"
2017.jeptalnrecital-long.13,Q15-1023,0,0.0405613,"Missing"
2017.jeptalnrecital-long.13,P09-1113,0,0.0726146,"Missing"
2017.jeptalnrecital-long.13,Q14-1019,0,0.0635932,"Missing"
2017.jeptalnrecital-long.13,D14-1167,0,0.0489415,"Missing"
2017.jeptalnrecital-long.13,K16-1025,0,0.0395777,"Missing"
2020.lrec-1.528,D07-1074,0,0.0739626,"nges to NLP researchers since, despite a large amount of available data, social media posts are often short and noisy, making information extraction tasks more difficult. Among these tasks, one key processing step is to map a named mention (also known as surface form) within a text to an actual entity defined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but general"
2020.lrec-1.528,D18-1227,0,0.0125974,"es, RSS500 (R¨oder et al., 2014) contains articles from international newspapers, OKE2015 (Nuzzolese et al., 2015) uses sentences from Wikipedia articles, the different Entity Discovery and Linking (EDL) tasks from the TAC campaigns use newswires along with broadcast conversation or discussion forums (Ji et al., 2010). Concerning social media, (Rizzo et al., 2015) proposed the NEEL2015 Microposts dataset composed of event-annotated tweets. One can note that all the aforementioned datasets link entities to general-domain knowledge bases, typically DBpedia or Freebase. More similar to our work, Dai et al. (2018) proposed to link mentions to entities defined in a specific social media KB. They constructed the Yelp-EL dataset from the Yelp platform where mentions in Yelp reviews are linked to Yelp business entities, but this corpus does not include visual information. Entity linking on tweets. While the EL task was initially defined for documents such as newspaper articles, it was also applied to new textual forms such as tweets. In this context, collective approaches such as (Huang et al., 2014) use mention co-referencing to collectively resolve them while Liu et al. (2013) measure mention-mention and"
2020.lrec-1.528,C10-1032,0,0.0460433,"large amount of available data, social media posts are often short and noisy, making information extraction tasks more difficult. Among these tasks, one key processing step is to map a named mention (also known as surface form) within a text to an actual entity defined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but generally lacks visual information that could be used"
2020.lrec-1.528,Q14-1021,0,0.016783,"ents such as newspaper articles, it was also applied to new textual forms such as tweets. In this context, collective approaches such as (Huang et al., 2014) use mention co-referencing to collectively resolve them while Liu et al. (2013) measure mention-mention and mention-entity similarities from groups of related tweets. Shen et al. (2013) propose a graph-based approach to model the interaction between the topic of interest of Twitter users and all KB entities. Other approaches combine user’s social features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target mention. Multimodal corpora. Many corpora provide images with associated textual content, in particular for the tasks of automatic image annotation (Young et al., 2014; Ginsca et al., 2015), cross-media retrieval (Karpathy and Fei-Fei, 2015; Tran et al., 2016a), image-sentence matching (Hodosh et al., 2013; Ordonez et al., 2011), text illustration (Feng and Lapata, 2010; Chami et al., 2017) and cross-media classification (Tran et al., 2016b; Tamaazousti et al.,"
2020.lrec-1.528,N10-1125,0,0.0375168,"features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target mention. Multimodal corpora. Many corpora provide images with associated textual content, in particular for the tasks of automatic image annotation (Young et al., 2014; Ginsca et al., 2015), cross-media retrieval (Karpathy and Fei-Fei, 2015; Tran et al., 2016a), image-sentence matching (Hodosh et al., 2013; Ordonez et al., 2011), text illustration (Feng and Lapata, 2010; Chami et al., 2017) and cross-media classification (Tran et al., 2016b; Tamaazousti et al., 2017). Most corpora used in this context consist in images with captions from Flickr (Ordonez et al., 2011; Hodosh et al., 2013; Young et al., 2014) or using Amazon’s Mechanical Turk (Rashtchian et al., 2010; Lin et al., 2014). Other authors nevertheless preferred to build datasets of images with captions in a real context, that is to say extracted from real news articles (Feng and Lapata, 2010; Tirilly et al., 2010; Hollink et al., 2016). In terms of size, these datasets contain from few thousands (F"
2020.lrec-1.528,P16-1059,0,0.0178354,"fined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but generally lacks visual information that could be used in the EL task. On the other hand, social media posts like tweets provide poor and noisy textual contexts which make the Entity linking task harder, but is often associated with complementary visual information. In this paper, we propose to automatically build large-"
2020.lrec-1.528,D11-1072,0,0.182492,"Missing"
2020.lrec-1.528,L16-1219,0,0.0423041,"Missing"
2020.lrec-1.528,P14-1036,0,0.0218804,"ets link entities to general-domain knowledge bases, typically DBpedia or Freebase. More similar to our work, Dai et al. (2018) proposed to link mentions to entities defined in a specific social media KB. They constructed the Yelp-EL dataset from the Yelp platform where mentions in Yelp reviews are linked to Yelp business entities, but this corpus does not include visual information. Entity linking on tweets. While the EL task was initially defined for documents such as newspaper articles, it was also applied to new textual forms such as tweets. In this context, collective approaches such as (Huang et al., 2014) use mention co-referencing to collectively resolve them while Liu et al. (2013) measure mention-mention and mention-entity similarities from groups of related tweets. Shen et al. (2013) propose a graph-based approach to model the interaction between the topic of interest of Twitter users and all KB entities. Other approaches combine user’s social features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target me"
2020.lrec-1.528,Q15-1023,0,0.0218725,"l be released at https://github.com/OA256864/MEL_Tweets 2. Related Work The MEL task is intrinsically related to several research domains: first, EL from text, which is characterized by a set of evaluation frameworks. But EL was also applied to specific forms of text such as social media posts. Finally, MEL is closely related to work about multimedia as it combines text and image contents. Since we propose a new corpus, we particularly focus on the corpora developed in these three areas. Entity linking corpora. Existing corpora for evaluating entity linking systems are thoroughly reviewed in (Ling et al., 2015; Usbeck et al., 2015; Van Erp et al., 2016; Rosales-M´endez, 2019). They were mostly constructed from news articles: AIDA-YAGO2 (Hoffart et al., 2011) uses the Reuters newswire articles also used in the CoNLL 2003 shared task on named entity recognition, MEANTIME (Minard et al., 2016) uses Wikinews articles, RSS500 (R¨oder et al., 2014) contains articles from international newspapers, OKE2015 (Nuzzolese et al., 2015) uses sentences from Wikipedia articles, the different Entity Discovery and Linking (EDL) tasks from the TAC campaigns use newswires along with broadcast conversation or discussio"
2020.lrec-1.528,P13-1128,0,0.0108287,". More similar to our work, Dai et al. (2018) proposed to link mentions to entities defined in a specific social media KB. They constructed the Yelp-EL dataset from the Yelp platform where mentions in Yelp reviews are linked to Yelp business entities, but this corpus does not include visual information. Entity linking on tweets. While the EL task was initially defined for documents such as newspaper articles, it was also applied to new textual forms such as tweets. In this context, collective approaches such as (Huang et al., 2014) use mention co-referencing to collectively resolve them while Liu et al. (2013) measure mention-mention and mention-entity similarities from groups of related tweets. Shen et al. (2013) propose a graph-based approach to model the interaction between the topic of interest of Twitter users and all KB entities. Other approaches combine user’s social features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target mention. Multimodal corpora. Many corpora provide images with associated textual c"
2020.lrec-1.528,L16-1699,0,0.0517092,"Missing"
2020.lrec-1.528,P18-1186,0,0.0232207,"cted from real news articles (Feng and Lapata, 2010; Tirilly et al., 2010; Hollink et al., 2016). In terms of size, these datasets contain from few thousands (Feng and Lapata, 2010; Rashtchian et al., 2010; Hodosh et al., 2013) to several hundred thousand (Hollink et al., 2016) and even one million of captioned images (Ordonez et al., 2011). Other corpora have also been created for more specific usages such as visual question answering (Antol et al., 2015), visual dialogs (Das et al., 2017) or understanding the interactions and relationships between objects in an image (Krishna et al., 2017). Moon et al. (2018) addressed the task of multimedia entity linking and evaluate their approach on a corpus of 12K user-generated image and textual caption pairs from Snapshat, where mentions are linked to the general-domain Freebase KB. However, the evaluation corpus was not released and the authors provide little information on its char4286 acteristics and the method used to build it. To the best of our knowledge, our approach is thus the first to allow to build a dataset for MEL evaluation, with full access to image and corresponding text, and usable for reproducible researches. 3. MEL Task Definition As in s"
2020.lrec-1.528,N18-1049,0,0.0247563,"s representation of their textual contexts using an unsupervised language model. Moreover, the visual contexts of mi and ei are determined using a pre-trained convolution neural network employed as an image feature extractor. We additionally provide other traditional features based on popularity and BM25 similarity. We use the ExtraTrees classifier to combine these features, perform classification over the mention-entity pairs and select the best entity among candidate entities. Textual Context Features For mention and entity textual context representations, we used the unsupervised Sent2Vec (Pagliardini et al., 2018) 4288 Figure 3: KB entities examples. Each entity in the knowledge base represents a twitter user characterized by its timeline (set of pair of text-image). We assume that combining visual and textual contexts of each entity helps discriminating entities for better EL performance. sentence embedding model, and more precisely, a pretrained version on a large Twitter corpus6 . We adopted this model because we observed, in preliminary experiments, that representations built from the same type of data as ours (social media posts in our case) give better results. Therefore, the textual context of a"
2020.lrec-1.528,N15-1026,0,0.0185726,"to an actual entity defined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but generally lacks visual information that could be used in the EL task. On the other hand, social media posts like tweets provide poor and noisy textual contexts which make the Entity linking task harder, but is often associated with complementary visual information. In this paper, we propose to a"
2020.lrec-1.528,P11-1138,0,0.0407317,"on (also known as surface form) within a text to an actual entity defined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but generally lacks visual information that could be used in the EL task. On the other hand, social media posts like tweets provide poor and noisy textual contexts which make the Entity linking task harder, but is often associated with complementary visu"
2020.lrec-1.528,roder-etal-2014-n3,0,0.0540912,"Missing"
2020.lrec-1.528,tirilly-etal-2010-news,0,0.0393798,"age-sentence matching (Hodosh et al., 2013; Ordonez et al., 2011), text illustration (Feng and Lapata, 2010; Chami et al., 2017) and cross-media classification (Tran et al., 2016b; Tamaazousti et al., 2017). Most corpora used in this context consist in images with captions from Flickr (Ordonez et al., 2011; Hodosh et al., 2013; Young et al., 2014) or using Amazon’s Mechanical Turk (Rashtchian et al., 2010; Lin et al., 2014). Other authors nevertheless preferred to build datasets of images with captions in a real context, that is to say extracted from real news articles (Feng and Lapata, 2010; Tirilly et al., 2010; Hollink et al., 2016). In terms of size, these datasets contain from few thousands (Feng and Lapata, 2010; Rashtchian et al., 2010; Hodosh et al., 2013) to several hundred thousand (Hollink et al., 2016) and even one million of captioned images (Ordonez et al., 2011). Other corpora have also been created for more specific usages such as visual question answering (Antol et al., 2015), visual dialogs (Das et al., 2017) or understanding the interactions and relationships between objects in an image (Krishna et al., 2017). Moon et al. (2018) addressed the task of multimedia entity linking and ev"
2020.lrec-1.528,L16-1693,0,0.0349981,"Missing"
2020.lrec-1.528,Q14-1006,0,0.0475257,"s of related tweets. Shen et al. (2013) propose a graph-based approach to model the interaction between the topic of interest of Twitter users and all KB entities. Other approaches combine user’s social features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target mention. Multimodal corpora. Many corpora provide images with associated textual content, in particular for the tasks of automatic image annotation (Young et al., 2014; Ginsca et al., 2015), cross-media retrieval (Karpathy and Fei-Fei, 2015; Tran et al., 2016a), image-sentence matching (Hodosh et al., 2013; Ordonez et al., 2011), text illustration (Feng and Lapata, 2010; Chami et al., 2017) and cross-media classification (Tran et al., 2016b; Tamaazousti et al., 2017). Most corpora used in this context consist in images with captions from Flickr (Ordonez et al., 2011; Hodosh et al., 2013; Young et al., 2014) or using Amazon’s Mechanical Turk (Rashtchian et al., 2010; Lin et al., 2014). Other authors nevertheless preferred to build datasets of images with cap"
2021.jeptalnrecital-taln.17,P16-2011,0,0.0386643,"Missing"
2021.jeptalnrecital-taln.17,P08-1030,0,0.176483,"Missing"
2021.jeptalnrecital-taln.17,P13-1008,0,0.0590742,"Missing"
2021.jeptalnrecital-taln.17,P10-1081,0,0.119234,"Missing"
2021.jeptalnrecital-taln.17,P17-1164,0,0.0332215,"Missing"
2021.jeptalnrecital-taln.17,P16-1100,0,0.0777097,"Missing"
2021.jeptalnrecital-taln.17,2020.coling-main.4,0,0.0878685,"Missing"
2021.jeptalnrecital-taln.17,N16-1034,0,0.0464364,"Missing"
2021.jeptalnrecital-taln.17,W16-1618,0,0.0383935,"Missing"
2021.jeptalnrecital-taln.17,P15-2060,0,0.0600112,"Missing"
2021.jeptalnrecital-taln.17,N18-1202,0,0.140932,"Missing"
2021.jeptalnrecital-taln.17,D19-1585,0,0.044561,"Missing"
2021.jeptalnrecital-taln.17,N19-1105,0,0.0225046,"Missing"
2021.jeptalnrecital-taln.17,P19-1522,0,0.0234393,"Missing"
2021.jeptalnrecital-taln.17,P18-2066,0,0.0283046,"Missing"
2021.semdeep-1.4,P19-1279,0,0.327607,"lation validation setup is modified and presented as an entailment problem, where systems learn whether the text entails the relation based on linguistic features. In this paper, we propose not only to learn the representation of the relation type, but also to learn the representation of the validation knowledge by using a neural architecture for modelling relation 1 https://catalog.ldc.upenn.edu/LDC2018T22 validation, inspired by neural entailment models. We aim to decide whether the text supports the relation by encoding the text and the triplet2 in a transformer architecture as in (Baldini Soares et al., 2019; Zhao et al., 2019). Once a model for relation validation is learned, we use it to validate the output of a relation classification model. Our experiments show that our proposal outperforms robust neural models for relation classification but fails to improve most recent works. The remainder of this paper is structured as follows: Section 2 presents some relevant models for relation classification and validation. Section 3 details our strategy to classify relations based on relation validation. Then, the experimental setup and results are presented in Sections 4. Finally, conclusions are draw"
2021.semdeep-1.4,H05-1091,0,0.319331,"c features for recognising if a relation is expressed in a text by exploiting rich linguistic knowledge from multiple lexical, syntactic, and semantic levels. In Wang and Neumann (2008), the relation to validate is transformed by simple patterns in a sentence and an alignment between the two texts is performed by a kernel-based approach. Traditional methods for relation extraction are based on feature engineering and rely on lexical and syntactic information. Dependency trees provide clues for deciding the presence of a relation in unsupervised relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Fundel et al., 2007). Gamallo et al. (2012) defined patterns of relation by parsing the dependencies in open information extraction. Words around the entity mentions in sentences give clues to characterise the semantics of a relation (Niu et al., 2012; Hoffmann et al., 2011; Yao et al., 2011; Riedel et al., 2010; Mintz et al., 2009). In addition to linguistic information, collective information about the entities and their relations were exploited for RV (Rahman et al., 2018) by adding features based on a graph of entities and for RE by Augenstein 2 We are aware that our model mainly based i"
2021.semdeep-1.4,P04-1054,0,0.272589,"quires considering linguistic features for recognising if a relation is expressed in a text by exploiting rich linguistic knowledge from multiple lexical, syntactic, and semantic levels. In Wang and Neumann (2008), the relation to validate is transformed by simple patterns in a sentence and an alignment between the two texts is performed by a kernel-based approach. Traditional methods for relation extraction are based on feature engineering and rely on lexical and syntactic information. Dependency trees provide clues for deciding the presence of a relation in unsupervised relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Fundel et al., 2007). Gamallo et al. (2012) defined patterns of relation by parsing the dependencies in open information extraction. Words around the entity mentions in sentences give clues to characterise the semantics of a relation (Niu et al., 2012; Hoffmann et al., 2011; Yao et al., 2011; Riedel et al., 2010; Mintz et al., 2009). In addition to linguistic information, collective information about the entities and their relations were exploited for RV (Rahman et al., 2018) by adding features based on a graph of entities and for RE by Augenstein 2 We are aware tha"
2021.semdeep-1.4,E17-2118,0,0.0180533,"the importance of adding information about the entities in the triplet. The above approaches rely on Natural Language Processing (NLP) tools for syntactic analysis and on lexical knowledge for identifying triggers. Thus, it remains difficult to overcome the lexical gap between texts and relation names when learning relation patterns for different types of relations in an open domain. Recently, end-to-end neural network (NN) based approaches have been emerged and getting lots of attention for the relation classification task (dos Santos et al., 2015; Nguyen and Grishman, 2015; Vu et al., 2016; Dligach et al., 2017; Zheng et al., 2016; Zhang et al., 2018). However, they do not leverage any triplet representation of a relation for better understanding the relatedness between the text and the triplet. A lot of NN models for evaluating the similarity of two sentences have been proposed. They encode each entry by a CNN or an RNN (e.g., LSTM or BiLSTM), and compute a similarity between the sentence representations (Severyn and Moschitti, 2015) or compute interactions between the texts by an attention layer (Yin et al., 2016). Most recent models encode one or two sentences by using the pre-trained neural mode"
2021.semdeep-1.4,P15-1061,0,0.0266987,"formation about the object of a relation. The latter model shows the importance of adding information about the entities in the triplet. The above approaches rely on Natural Language Processing (NLP) tools for syntactic analysis and on lexical knowledge for identifying triggers. Thus, it remains difficult to overcome the lexical gap between texts and relation names when learning relation patterns for different types of relations in an open domain. Recently, end-to-end neural network (NN) based approaches have been emerged and getting lots of attention for the relation classification task (dos Santos et al., 2015; Nguyen and Grishman, 2015; Vu et al., 2016; Dligach et al., 2017; Zheng et al., 2016; Zhang et al., 2018). However, they do not leverage any triplet representation of a relation for better understanding the relatedness between the text and the triplet. A lot of NN models for evaluating the similarity of two sentences have been proposed. They encode each entry by a CNN or an RNN (e.g., LSTM or BiLSTM), and compute a similarity between the sentence representations (Severyn and Moschitti, 2015) or compute interactions between the texts by an attention layer (Yin et al., 2016). Most recent model"
2021.semdeep-1.4,W12-0702,0,0.0548313,"Missing"
2021.semdeep-1.4,P19-1024,0,0.0245168,"Missing"
2021.semdeep-1.4,P11-1055,0,0.0732228,"tween the two texts is performed by a kernel-based approach. Traditional methods for relation extraction are based on feature engineering and rely on lexical and syntactic information. Dependency trees provide clues for deciding the presence of a relation in unsupervised relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Fundel et al., 2007). Gamallo et al. (2012) defined patterns of relation by parsing the dependencies in open information extraction. Words around the entity mentions in sentences give clues to characterise the semantics of a relation (Niu et al., 2012; Hoffmann et al., 2011; Yao et al., 2011; Riedel et al., 2010; Mintz et al., 2009). In addition to linguistic information, collective information about the entities and their relations were exploited for RV (Rahman et al., 2018) by adding features based on a graph of entities and for RE by Augenstein 2 We are aware that our model mainly based its improvements on input modification. However, we strongly believe that this is unfairly underestimated in the field. (2016) that integrated global information about the object of a relation. The latter model shows the importance of adding information about the entities in t"
2021.semdeep-1.4,P09-1113,0,0.565876,"ies have not associated information for relations such as place of birth or nationality (Dong et al., 2014). Most approaches model the relation classification (RC) (dos Santos et al., 2015; Nguyen and Grishman, 2015) task as a learning problem where it is required to predict if a passage contains a type of relation (multi-class classification). This setup requires annotated examples of each class, i.e. each Brigitte Grau LIMSI, UPR 3251 CNRS F - 91405 Orsay , France bg@limsi.fr type of relation, which can be difficult to obtain. To overcome this problem, distant supervision has been proposed (Mintz et al., 2009) for automatically annotating texts given relation triplets existing in a KB by projecting triplets into texts to increase the input data. Its main counterpart is that distant supervision models must deal with wrongly annotated examples. The difficulty of the task is shown by the results of the TAC KBP slot filling task1 . For instance, in 2014, the maximum F1-score of the task was 0.3672 (Surdeanu and Ji, 2014). Another trend is trying to collect information directly from the web in an unsupervised setting, i.e. the open IE paradigm (Banko et al., 2007). In these two last settings, one crucia"
2021.semdeep-1.4,W15-1506,0,0.0242248,"bject of a relation. The latter model shows the importance of adding information about the entities in the triplet. The above approaches rely on Natural Language Processing (NLP) tools for syntactic analysis and on lexical knowledge for identifying triggers. Thus, it remains difficult to overcome the lexical gap between texts and relation names when learning relation patterns for different types of relations in an open domain. Recently, end-to-end neural network (NN) based approaches have been emerged and getting lots of attention for the relation classification task (dos Santos et al., 2015; Nguyen and Grishman, 2015; Vu et al., 2016; Dligach et al., 2017; Zheng et al., 2016; Zhang et al., 2018). However, they do not leverage any triplet representation of a relation for better understanding the relatedness between the text and the triplet. A lot of NN models for evaluating the similarity of two sentences have been proposed. They encode each entry by a CNN or an RNN (e.g., LSTM or BiLSTM), and compute a similarity between the sentence representations (Severyn and Moschitti, 2015) or compute interactions between the texts by an attention layer (Yin et al., 2016). Most recent models encode one or two sentenc"
2021.semdeep-1.4,D19-1005,0,0.0322064,"Missing"
2021.semdeep-1.4,P15-1018,0,0.0191563,"is learned, we use it to validate the output of a relation classification model. Our experiments show that our proposal outperforms robust neural models for relation classification but fails to improve most recent works. The remainder of this paper is structured as follows: Section 2 presents some relevant models for relation classification and validation. Section 3 details our strategy to classify relations based on relation validation. Then, the experimental setup and results are presented in Sections 4. Finally, conclusions are drawn in Section 5. 2 Related Work Different ensemble models (Viswanathan et al., 2015) have been defined for the relation validation KBP task based on the prediction made by the RE systems. However, Yu et al. (2014) show that relation validation requires considering linguistic features for recognising if a relation is expressed in a text by exploiting rich linguistic knowledge from multiple lexical, syntactic, and semantic levels. In Wang and Neumann (2008), the relation to validate is transformed by simple patterns in a sentence and an alignment between the two texts is performed by a kernel-based approach. Traditional methods for relation extraction are based on feature engin"
2021.semdeep-1.4,N16-1065,0,0.0389899,"Missing"
2021.semdeep-1.4,P19-1132,0,0.0499059,"Missing"
2021.semdeep-1.4,P16-1123,0,0.0407322,"Missing"
2021.semdeep-1.4,D11-1135,0,0.0242449,"performed by a kernel-based approach. Traditional methods for relation extraction are based on feature engineering and rely on lexical and syntactic information. Dependency trees provide clues for deciding the presence of a relation in unsupervised relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Fundel et al., 2007). Gamallo et al. (2012) defined patterns of relation by parsing the dependencies in open information extraction. Words around the entity mentions in sentences give clues to characterise the semantics of a relation (Niu et al., 2012; Hoffmann et al., 2011; Yao et al., 2011; Riedel et al., 2010; Mintz et al., 2009). In addition to linguistic information, collective information about the entities and their relations were exploited for RV (Rahman et al., 2018) by adding features based on a graph of entities and for RE by Augenstein 2 We are aware that our model mainly based its improvements on input modification. However, we strongly believe that this is unfairly underestimated in the field. (2016) that integrated global information about the object of a relation. The latter model shows the importance of adding information about the entities in the triplet. The ab"
2021.semdeep-1.4,Q16-1019,0,0.0962729,"Missing"
2021.semdeep-1.4,C14-1149,0,0.0272592,"t neural models for relation classification but fails to improve most recent works. The remainder of this paper is structured as follows: Section 2 presents some relevant models for relation classification and validation. Section 3 details our strategy to classify relations based on relation validation. Then, the experimental setup and results are presented in Sections 4. Finally, conclusions are drawn in Section 5. 2 Related Work Different ensemble models (Viswanathan et al., 2015) have been defined for the relation validation KBP task based on the prediction made by the RE systems. However, Yu et al. (2014) show that relation validation requires considering linguistic features for recognising if a relation is expressed in a text by exploiting rich linguistic knowledge from multiple lexical, syntactic, and semantic levels. In Wang and Neumann (2008), the relation to validate is transformed by simple patterns in a sentence and an alignment between the two texts is performed by a kernel-based approach. Traditional methods for relation extraction are based on feature engineering and rely on lexical and syntactic information. Dependency trees provide clues for deciding the presence of a relation in u"
2021.semdeep-1.4,D18-1244,0,0.0846858,"the entities in the triplet. The above approaches rely on Natural Language Processing (NLP) tools for syntactic analysis and on lexical knowledge for identifying triggers. Thus, it remains difficult to overcome the lexical gap between texts and relation names when learning relation patterns for different types of relations in an open domain. Recently, end-to-end neural network (NN) based approaches have been emerged and getting lots of attention for the relation classification task (dos Santos et al., 2015; Nguyen and Grishman, 2015; Vu et al., 2016; Dligach et al., 2017; Zheng et al., 2016; Zhang et al., 2018). However, they do not leverage any triplet representation of a relation for better understanding the relatedness between the text and the triplet. A lot of NN models for evaluating the similarity of two sentences have been proposed. They encode each entry by a CNN or an RNN (e.g., LSTM or BiLSTM), and compute a similarity between the sentence representations (Severyn and Moschitti, 2015) or compute interactions between the texts by an attention layer (Yin et al., 2016). Most recent models encode one or two sentences by using the pre-trained neural models. Their use in RC has been successfully"
ayari-etal-2010-fine,costa-sarmento-2006-component,0,\N,Missing
ayari-etal-2010-fine,gillard-etal-2006-question,0,\N,Missing
ayari-etal-2010-fine,grau-etal-2006-frasques,1,\N,Missing
ayari-etal-2010-fine,quintard-etal-2010-question,1,\N,Missing
ayari-etal-2010-fine,W01-0905,1,\N,Missing
ayari-etal-2010-fine,W04-3101,0,\N,Missing
C98-1062,J97-1003,0,0.419541,"a useful p r o c e s s in many applications, such as text summarization or information extraction task. Approaches that address this problem can be classified in knowledge-based approaches or word-based approaches. K n o w l e d g e - b a s e d systems as G r o s z and Sidner's (1986) require an e x t e n s i v e manual k n o w l e d g e engineering effort to create the knowledge base (semantic network and/or frames) and this is only possible in very limited and well-known domains. To overcome this limitation, and to process a large amount of texts, word-based approaches have been developed. Hearst (1997) and M a s s o n (1995) make use of the word distribution in a text to find a thematic segmentation. These works are well adapted to technical or scientific texts characterized by a specific vocabulary. To process narrative or expository texts such as newspaper articles, Kozima's (1993) and Morris and Hirst's (1991) a p p r o a c h e s are based on lexical cohesion computed from a lexical network. These methods depend on the presence of the text vocabulary inside their network. So, to avoid any restriction about domains in such 392 collocations have been collected is 20 words wide and takes in"
C98-1062,P93-1041,0,0.380973,"Missing"
C98-1062,J91-1002,0,0.373813,"Missing"
C98-1062,C94-2187,0,0.704391,"Missing"
C98-1062,C94-1095,0,0.0228951,"exts, they have to be represented by their significant features from that point of view. So, we only hold for each text the lemmatized form of its nouns, verbs and adjectives. This has been done by combining existing tools. MtSeg from the Multext project presented in V6ronis and Khouri (1995) is used for segmenting the raw texts. As compound nouns are less polysemous than single ones, we have added to MtSeg the ability to identify 2300 compound nouns. We have retained the most frequent compound nouns in 11 years of the French Le Monde newspaper. They have been collected with the INTEX tool of Silberztein (1994). The part of speech tagger TreeTagger of Schmid (1994) is applied to disambiguate the lexical category of the words and to provide their lemmatized form. The selection of the meaningful words, which do not include proper nouns and abbreviations, ends the pre-processing. This one is applied to the texts both for building the collocation network and for their thematic segmentation. 4. /max ---- log2 N 2 ( S w - 1) with N: corpus size and Sw: window size 5. Thematic segmentation lexical network without The first method, based on a numerical analysis of the vocabulary distribution in the text, is"
C98-1062,J90-1003,0,\N,Missing
C98-1062,J86-3001,0,\N,Missing
D14-1199,P04-1056,0,0.0406074,"is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An im"
D14-1199,P11-1114,0,0.568475,"asks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is gen"
D14-1199,E12-1029,0,0.32774,"les) whose values are text excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on"
D14-1199,lambert-etal-2012-automatic,0,0.014731,"earning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word representations are scalable and"
D14-1199,P14-2050,0,0.034226,"Missing"
D14-1199,N13-1090,0,0.0175318,"ifferent parameters, compared to the learning curve of TIER (Huang and Riloff, 2012a). The grey points represent the performances of other IE systems. Figure 1 presents the average F1-score results, computed over the slots PerpInd, PerpOrg, Target, Victim and Weapon. We observe that models relying on word embeddings globally outperform the state-of-the-art results, which demonstrates that the word embeddings capture enough semantic information to perform the task of event newswire corpus 4 W2V-50 are the embeddings induced from the MUC4 data set using the negative sampling training algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), available at https://code.google.com/ p/word2vec/ role labeling on “String Slots” without using any additional hand-engineered features. Moreover, our representations (DRVR-50) clearly surpass the models based on generic embeddings (C&W-50 and HLBL-50) and obtain better results than W2V50, based the competitive model of (Mikolov et al., 2013a), even if the difference is small. We can also note that the performance of our model is good even with a small amount of training data, which makes it a good candidate to easily develop an event extractio"
D14-1199,D07-1075,0,0.64854,"rmance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An important step forwards is TIERlight (Huang and Riloff, 2012a) that targeted the minimization of human supervision with a bootstrapping technique for event roles detection. Also, PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) distinguishes between relevant and irrelevant regions and learns domain-relevant extraction patterns using a semantic affinity measure. Another possible approach for dealing with this problem is to combine the use a restricted set of manually annotated data with a much larger set of data extracted in an unsupervised way from a corpus. This approach was experimented for relations in the context of Open Information Extraction (Soderland et al., 2010) but not for extracting events and their participants to our knowledge. In this paper, we propose to approach the task of labeli"
D14-1199,P03-1029,0,0.0356832,"ction, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to"
D14-1199,W06-2207,0,0.0176454,"lefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a dif"
D14-1199,P10-1040,0,0.287413,"Missing"
D14-1199,C00-2136,0,0.0908768,"t excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of"
D14-1199,P14-2089,0,0.040448,"Missing"
D14-1199,D09-1016,0,0.558717,"traction constitutes a challenging task. An event is described by a set of participants (i.e. attributes or roles) whose values are text excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering depl"
D14-1199,I08-2089,0,0.0173342,"roles by automatically learning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word represent"
D14-1199,P03-1028,0,\N,Missing
D14-1199,M92-1021,0,\N,Missing
F12-2001,C10-1089,0,0.0390148,"Missing"
F12-2001,W08-0602,0,0.0719198,"Missing"
F12-2001,W11-0201,0,0.0332349,"Missing"
F12-2001,P08-1040,0,0.0574046,"Missing"
F12-2001,D11-1038,0,0.0623467,"Missing"
F12-2001,N06-1037,0,0.0848249,"Missing"
F13-1026,I05-2045,0,0.0197361,"Missing"
F13-1026,C04-1051,0,0.176404,"Missing"
F13-1026,eichler-etal-2008-unsupervised,0,0.0406469,"Missing"
F13-1026,D11-1142,0,0.0816668,"Missing"
F13-1026,ferret-2010-testing,1,0.895539,"Missing"
F13-1026,W12-0702,0,0.0548574,"Missing"
F13-1026,P04-1053,0,0.0805501,"Missing"
F13-1026,D12-1094,0,0.0426977,"Missing"
F13-1026,P09-1113,0,0.124317,"Missing"
F13-1026,N10-1047,0,0.0686786,"Missing"
F13-1026,D11-1048,0,0.0534093,"Missing"
F13-1026,P06-2094,0,0.0679576,"Missing"
F13-1026,N06-1039,0,0.0533402,"Missing"
F13-1026,wang-etal-2012-evaluation,1,0.888778,"Missing"
F13-1026,D11-1135,0,0.0503165,"Missing"
F14-1003,P04-1056,0,0.107549,"Missing"
F14-1003,P03-1028,0,0.0808192,"Missing"
F14-1003,W09-1407,0,0.0413785,"Missing"
F14-1003,C96-1079,0,0.0659003,"Missing"
F14-1003,P10-1029,0,0.0367011,"Missing"
F14-1003,P11-1114,0,0.0314191,"Missing"
F14-1003,M92-1008,0,0.174587,"Missing"
F14-1003,D07-1075,0,0.0400329,"Missing"
F14-1003,D09-1016,0,0.0412256,"Missing"
F14-1003,strassel-etal-2008-linguistic,0,0.0214512,"Missing"
F14-1003,P03-1029,0,0.122251,"Missing"
F14-1003,W06-2207,0,0.0344801,"Missing"
F14-1003,P10-1040,0,0.019682,"Missing"
F14-1003,C00-2136,0,0.143887,"Missing"
gleize-grau-2014-hierarchical,C10-2113,0,\N,Missing
gleize-grau-2014-hierarchical,W07-1431,0,\N,Missing
gleize-grau-2014-hierarchical,N03-1033,0,\N,Missing
gleize-grau-2014-hierarchical,P13-2079,0,\N,Missing
gleize-grau-2014-hierarchical,S13-2045,0,\N,Missing
gleize-grau-2014-hierarchical,N10-4002,0,\N,Missing
grappy-etal-2010-corpus,cramer-etal-2006-building,0,\N,Missing
grappy-etal-2010-corpus,rosset-petel-2006-ritel,0,\N,Missing
grappy-etal-2010-corpus,varasai-etal-2008-building,0,\N,Missing
I17-1101,P14-2111,0,0.159126,"cons (de Does and Depuydt, 2013) and characterbased errors statistics obtained from a corrected training set (Kumar and Lehal, 2016). However, they have some drawbacks that limit their usefulness for specific, low-resource domains. Such resources are expensive to create and for highly specialized texts (e.g., medical domain) not always possible to obtain. The recent advances in neural network models, based on textual context and needing no external resources, provide new opportunities for OCR post-correction. Character-level sequence modeling architectures are especially suited for this task (Chrupała, 2014; Schmaltz et al., 2016), as they reduce the complexity at output time. Moreover, current systems are often limited to processing texts with a limited degree of OCR corruption, i.e., so-called single-error word corrections (Kissos and Dershowitz, 2016) and correction of OCRed corpora that have been generated by older OCR engines can prove too challenging. The correct recognition of historical texts remains an open challenge (Kluzner et al., 2009). A general-purpose OCR post-correction tool should be adaptable to the ratio of error that is present in the OCR output in order to deal with both ty"
I17-1101,W16-6108,1,0.886811,"Missing"
I17-1101,H05-1109,0,0.0554927,"into a Bayesian model (Tong and Evans, 1996) or a HMM model (Borovikov et al., 2004) to select the optimal word candidate. These systems are explicitly or implicitly limited to cases in which an erroneous word appears in an otherwise clean context. For serious degrees of corruption (e.g., historical texts), the common approach aims to optimally combine an ensemble of multiple OCR engines (Nakano et al., 2004; Lund and Ringger, 2009). ‘Noisy channel paradigm’ aims to learn error models describing the OCR output generation from the reference text, and as such combine error and language models. Kolak and Resnik (2005) used finite state machines on a small set of training material while Llobet et al. (2010) combined all OCR process hypotheses for each recognized character. Such models need a large amount of training material which is costly and not always easily available. In response, the TextInduced Corpus Clean-up (TICCL) system (Reynaert, 2011) was developed to run with no annotated training data. It takes noisy texts and extracts the high-frequency word variants through statistical analysis and clusters typographical word variants within a user-defined Levenshtein distance. Recently, Neural Network Lan"
I17-1101,W11-4114,0,0.0497845,"Missing"
I17-1101,W16-0528,0,0.0315064,"d Depuydt, 2013) and characterbased errors statistics obtained from a corrected training set (Kumar and Lehal, 2016). However, they have some drawbacks that limit their usefulness for specific, low-resource domains. Such resources are expensive to create and for highly specialized texts (e.g., medical domain) not always possible to obtain. The recent advances in neural network models, based on textual context and needing no external resources, provide new opportunities for OCR post-correction. Character-level sequence modeling architectures are especially suited for this task (Chrupała, 2014; Schmaltz et al., 2016), as they reduce the complexity at output time. Moreover, current systems are often limited to processing texts with a limited degree of OCR corruption, i.e., so-called single-error word corrections (Kissos and Dershowitz, 2016) and correction of OCRed corpora that have been generated by older OCR engines can prove too challenging. The correct recognition of historical texts remains an open challenge (Kluzner et al., 2009). A general-purpose OCR post-correction tool should be adaptable to the ratio of error that is present in the OCR output in order to deal with both types of errors. In this p"
I17-1101,W96-0108,0,0.388647,"lexicon. 2 Background The problem of OCR post-correction has been studied since the seventies (Kukich, 1992). While traditional OCR error detection systems focused on constructing ‘confusion matrices’ of likely character (pairs) to detect corruptions of existing words into non-words, recent systems improve accuracy using information on the language context in which the error appears (Evershed and Fitch, 2014), using bigrams (Kissos and Dershowitz, 2016), large-scale word n-grams and character ngrams from the web (Bassil and Alwani, 2012) or associating confusion scores into a Bayesian model (Tong and Evans, 1996) or a HMM model (Borovikov et al., 2004) to select the optimal word candidate. These systems are explicitly or implicitly limited to cases in which an erroneous word appears in an otherwise clean context. For serious degrees of corruption (e.g., historical texts), the common approach aims to optimally combine an ensemble of multiple OCR engines (Nakano et al., 2004; Lund and Ringger, 2009). ‘Noisy channel paradigm’ aims to learn error models describing the OCR output generation from the reference text, and as such combine error and language models. Kolak and Resnik (2005) used finite state mac"
P15-1091,S12-1051,0,0.015429,"framework to capture syntactic structure and alignments between the two sentences. We experiment on three different natural sentence rewriting tasks and obtain state-of-the-art results for all of them. 1 Introduction Detecting implications of sense between statements stands as one of the most sought-after goals in computational linguistics. Several high level tasks look for either one-way rewriting between single sentences, like recognizing textual entailment (RTE) (Dagan et al., 2006), or two-way rewritings like paraphrase identification (Dolan et al., 2004) and semantic textual similarity (Agirre et al., 2012). In a similar fashion, selecting sentences containing the answer to a question can be seen as finding the best rewritings of the question among answer candidates. These problems are naturally framed as classification tasks, and as such most current solutions make use of supervised machine learning. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn 2 Type-Enriched String Rewriting Kernel Kernel functions measure the similarity between two elements. Used in machine learning met"
P15-1091,P12-1047,0,0.164898,"its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for syntactic structure, term alignments and lexicosemantic typed variations in a unified approach. We detail how to efficiently compute our kernel and lastly experiment on three different high-level NLP tasks, demonstrating the vast applicability of our method. Our system based on type-enriched st"
P15-1091,J10-3003,0,0.0131517,"wo instances of same label, and a low value for two instances of different label. 2.1 γp ≈ ⊆ Σ × Σ. To a type γv ∈ Γv ,we associate the typing relation γv ; ⊆ D × D. Together with the typing relations, we call the association of Γp and Γv the typing scheme of the kernel. Let Σp be defined as [ γ Σp = {[a|b] |∃a, b ∈ Σ, a ≈ b} (2) String rewriting kernel String rewriting kernels (Bu et al., 2012) count the number of common rewritings between two pairs of sentences seen as sequences of words. The rewriting rule (A) in Figure 1 can be viewed as a kind of phrasal paraphrase with linked variables (Madnani and Dorr, 2010). Rule (A) rewrites (B)’s first sentence into its second but it does not however rewrite the sentences in (C), which is what we try to fix in this paper. Following the terminology of string kernels, we use the term string and character instead of sentence and word. We denote (s, t) ∈ (Σ∗ × Σ∗ ) an instance of string rewriting, with a source string s and a target string t, both finite sequences of elements in Σ the finite set of characters. Suppose that we are given training data of such instances labeled in {+1, −1}, for paraphrase/nonparaphrase or entailment/non-entailment in applications. We"
P15-1091,N12-1019,0,0.0127066,"ving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for syntactic structure, term alignments and lexicosemantic typed variations in a unified approach. We detail how to efficiently compute our kernel and lastly experiment on three different high-level NLP tasks, demonstra"
P15-1091,N10-1066,0,0.0232344,"(Bu et al., 2012) introduced a string rewriting kernel which can capture at once lexical equivalents and common syntactic dependencies on pair of sentences. All these kernel methods require an exact match or assume prior partial matches between words, thus limiting the kind of learned rewriting rules. Our contribution addresses this issue with a typeenriched string rewriting kernel which can account for lexico-semantic variations of words. Limitations of our rewriting rules include the impossibility to skip a pattern word and to replace wildcards by multiple words. Some recent contributions (Chang et al., 2010; Wang and Manning, 2010) also provide a uniform way to learn both intermediary representations and a decision function using potentially rich feature sets. They use heuristics in the joint learning process to reduce the computational cost, while our kernel approach with a simple sequential representation of sentences has the benefit of efficiently computing an exact number of common rewriting rules between rewriting pairs. This in turn allows to precisely fine-tune the shape of desired rewriting rules through the design of the typing scheme. Table 4: Evaluation results on QA IDF-weighted comm"
P15-1091,I05-5002,0,0.0146938,"used LIBSVM (Chang and Lin, 2011) to train a binary SVM classifier on the training data with our two kernels. The default SVM algorithm in LIBSVM uses a parameter C, roughly akin to a regularization parameter. We 10-fold cross-validated this parameter on the training data, optimizing with a grid search for f-score, or MRR for question-answering. All kernels were normal˜ √ ized using K(x, y) = √ K(x,y) . We deK(x,x) 4.2 Paraphrase identification Paraphrase identification asks whether two sentences have the same meaning. The dataset we used to evaluate our systems is the MSR Paraphrase Corpus (Dolan and Brockett, 2005), containing 4,076 training pairs of sentences and 1,725 testing pairs. For example, the sentences ”An injured woman co-worker also was hospitalized and was listed in good condition.” and ”A woman was listed in good condition at Memorial’s HealthPark campus, he said.” are paraphrases in this corpus. On the other hand, ”’There are a number of locations in our community, which are essentially vulnerable,’ Mr Ruddock said.” and ”’There are a range of risks which are being seriously examined by competent authorities,’ Mr Ruddock said.” are not paraphrases. We report in Table 2 our best results, th"
P15-1091,C04-1051,0,0.025263,"tactic parsing but is still able to provide a unified framework to capture syntactic structure and alignments between the two sentences. We experiment on three different natural sentence rewriting tasks and obtain state-of-the-art results for all of them. 1 Introduction Detecting implications of sense between statements stands as one of the most sought-after goals in computational linguistics. Several high level tasks look for either one-way rewriting between single sentences, like recognizing textual entailment (RTE) (Dagan et al., 2006), or two-way rewritings like paraphrase identification (Dolan et al., 2004) and semantic textual similarity (Agirre et al., 2012). In a similar fashion, selecting sentences containing the answer to a question can be seen as finding the best rewritings of the question among answer candidates. These problems are naturally framed as classification tasks, and as such most current solutions make use of supervised machine learning. They have to tackle several challenges: picking an adequate language representation, aligning semantically equivalent elements and extracting relevant features to learn 2 Type-Enriched String Rewriting Kernel Kernel functions measure the similar"
P15-1091,N10-1145,0,0.0189755,"the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our fra"
P15-1091,U06-1019,0,0.0285661,"r Ruddock said.” and ”’There are a range of risks which are being seriously examined by competent authorities,’ Mr Ruddock said.” are not paraphrases. We report in Table 2 our best results, the system TESRK + PR, defined by the sum of PR and typed-enriched kb-SRKs with k from 1 to 4, with types Γp = Γv = {stem, synonym}. We observe K(y,y) note by ”+” a sum of kernels, with normalizations applied both before and after summing. Following Bu et al. (Bu et al., 2012) experimental setup, we introduced an auxiliary vector kernel denoted PR of features named unigram precision and recall, defined in (Wan et al., 2006). In our experiments a linear kernel seemed to yield the best re944 Type id idMinusTag lemma stem synonym, antonym hypernym, hyponym entailment, holonym ne lvhsn Typing relation on words (a, b) words have same surface form and tag words have same surface form words have same lemma words have same stem words are [type] b is a [type] of a Tool/resources OpenNLP tagger OpenNLP tokenizer WordNetStemmer Porter stemmer WordNet WordNet a and b are both tagged with the same Named Entity words are at edit distance of 1 BBN Identifinder Levenshtein distance Table 1: Types Accuracy 66.5 75.6 76.3 76.8 77"
P15-1091,S13-2047,0,0.0343151,"Missing"
P15-1091,C10-1131,0,0.0602928,"Missing"
P15-1091,D07-1003,0,0.0999086,"Missing"
P15-1091,N13-1106,0,0.0350238,"Missing"
P15-1091,P13-1171,0,0.0141522,"Paris-Sud, Orsay, France gleize@limsi.fr Brigitte Grau LIMSI-CNRS, Orsay, France ENSIIE, Evry, France bg@limsi.fr Abstract the final decision. Bag-of-words and by extension bag-of-ngrams are traditionally the most direct approach and features rely mostly on lexical matching (Wan et al., 2006; Lintean and Rus, 2011; Jimenez et al., 2013). Moreover, a good solving method has to account for typically scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the op"
P15-1091,W07-1412,0,0.0163148,"ly scarce labeled training data, by enriching its model with lexical semantic resources like WordNet (Miller, 1995) to bridge gaps between surface forms (Mihalcea et al., 2006; Islam and Inkpen, 2009; Yih et al., 2013). Models based on syntactic trees remain the typical choice to account for the structure of the sentences (Heilman and Smith, 2010; Wang and Manning, 2010; Socher et al., 2011; Calvo et al., 2014). Usually the best systems manage to combine effectively different methods, like Madnani et al.’s meta-classifier with machine translation metrics (Madnani et al., 2012). A few methods (Zanzotto et al., 2007; Zanzotto et al., 2010; Bu et al., 2012) use kernel functions to learn what makes two sentence pairs similar. Building on this work, we present a typeenriched string rewriting kernel giving the opportunity to specify in a fine-grained way how words match each other. Unlike previous work, rewriting rules learned using our framework account for syntactic structure, term alignments and lexicosemantic typed variations in a unified approach. We detail how to efficiently compute our kernel and lastly experiment on three different high-level NLP tasks, demonstrating the vast applicability of our met"
P98-1065,J97-1003,0,0.141878,"tion according to a topical criterion is a useful process in many applications, such as text summarization or information extraction task. Approaches that address this problem can be classified in knowledge-based approaches or word-based approaches. Knowledge-based systems as Grosz and Sidner's (1986) require an extensive manual knowledge engineering effort to create the knowledge base (semantic network and/or frames) and this is only possible in very limited and well-known domains. To overcome this limitation, and to process a large amount of texts, word-based approaches have been developed. Hearst (1997) and Masson (1995) make use of the word distribution in a text to find a thematic segmentation. These works are well adapted to technical or scientific texts characterized by a specific vocabulary. To process narrative or expository texts such as newspaper articles, Kozima's (1993) and Morris and Hirst's (1991) approaches are based on lexical cohesion computed from a lexical network. These methods depend on the presence of the text vocabulary inside their network. So, to avoid any restriction about domains in such 392 collocations have been collected is 20 words wide and takes into account the"
P98-1065,P93-1041,0,0.0448345,"Missing"
P98-1065,C94-2187,0,0.0295054,"Missing"
P98-1065,C94-1095,0,0.0259555,"y have to be represented by their significant features from that point of view. So, we only hold for each text the lemmatized form of its nouns, verbs and adjectives. This has been done by combining existing tools. MtSeg from the Multext project presented in V6ronis and Khouri (1995) is used for s e g m e n t i n g the raw texts. As compound nouns are less polysemous than single ones, we have added to MtSeg the ability to identify 2300 compound nouns. We have retained the most frequent compound nouns in 11 years of the French Le Monde newspaper. They have been collected with the INTEX tool of Silberztein (1994). The part of speech tagger TreeTagger of Schmid (1994) is applied to disambiguate the lexical category of the words and to provide their lemmatized form. The selection of the meaningful words, which do not include proper nouns and abbreviations, ends the pre-processing. This one is applied to the texts both for building the collocation network and for their thematic segmentation. 4. /max = log2 N2(Sw - 1) with N: corpus size and Sw: window size 5. Thematic segmentation lexical network without The first method, based on a numerical analysis of the vocabulary distribution in the text, is derive"
P98-1065,J90-1003,0,\N,Missing
P98-1065,J91-1002,0,\N,Missing
P98-1065,J86-3001,0,\N,Missing
pho-etal-2014-multiple,W12-2704,0,\N,Missing
pho-etal-2014-multiple,P03-1054,0,\N,Missing
pho-etal-2014-multiple,P13-2076,1,\N,Missing
pho-etal-2014-multiple,E12-2021,0,\N,Missing
pho-etal-2014-multiple,W06-1416,0,\N,Missing
pho-etal-2014-multiple,P05-1045,0,\N,Missing
quintard-etal-2010-question,ayache-etal-2006-equer,1,\N,Missing
quintard-etal-2010-question,bernard-etal-2010-question,1,\N,Missing
R11-1086,W08-0602,0,0.305199,"ve for precision without broad generalization capacity. So, other approaches are based on supervised machine learning. (Uzuner et al., 2010) use SVM (Support Vector Machines) to class relations between medical problems, tests and treatments in clinical reports. They defined surface features (ordering of the concepts, distance, etc.), lexical features (lexical trigrams, tokens-in-concepts, etc.), and shallow syntactic features (verbs, syntactic bigrams, syntactic link path, etc.). Results show an F-measure from 0.60 to 0.85, but for under-represented relations the classification did not work. (Roberts et al., 2008) also use a SVM to extract relations in the corpus of the Clinical E-Science Framework (CLEF) project that hold between entities (e.g. condition, drug, result) and modifiers (e.g. negation) in clinical records of cancer patients. There are seven classes of relations and each entity pair can be linked by one relation only (except between an investigation and a condition). So the classification task is considered as a binary classification (i.e. the detection of relation) between a type of relation and the nonrelation class. The classification is also based on lexical, morpho-syntactic and seman"
R11-1086,P05-1053,0,0.460645,"n the corpus of the Clinical E-Science Framework (CLEF) project that hold between entities (e.g. condition, drug, result) and modifiers (e.g. negation) in clinical records of cancer patients. There are seven classes of relations and each entity pair can be linked by one relation only (except between an investigation and a condition). So the classification task is considered as a binary classification (i.e. the detection of relation) between a type of relation and the nonrelation class. The classification is also based on lexical, morpho-syntactic and semantic features. In the general domain, (Zhou et al., 2005) use SVM to identify relations between people, organizations and places, etc. on the ACE corpus. Our system also uses SVM to classify finegrained relations. We make use of classical features as well as features specific to the domain, as the semantic types of the UMLS3 and medical abbreviation lists, and features specific to the writing style of texts, for handling concept coordination. 3 TrNAP TrWP TrCP TrAP TeCP TeRP PIP Table 1: The eight relations to identify and test (749 instances of relations). For the final evaluation, i2b2 organizers gave participants a corpus of 477 documents (9070 i"
S13-2100,N12-1021,0,0.136717,"ically, for each reference answer A, we compute the n closest variants of the student answer to A’s variant set. In our experiments, n = 10. We finally rank the reference answers according to the average distance 600 3.2 Classifying student answers SemEval 2013 task 7 offers 3 problems: a 5-way task, with 5 different answer judgements, and 3-way and 2-way tasks, conflating more judgement categories each time. Two different corpora, Beetle and SciEntsBank, were labeled with the 5 following labels: Correct, Partially correct incomplete, Contradictory, Irrelevant and Non Domain, as described in (Dzikovska et al., 2012). We see the n-way task as a n-way classification problem. The instances of this problem are the pairs (student answer, reference answer). We compute for each instance the following features: For each of the n closest variants of the student answer to some variant of the reference answer computed in the pre-ranking phase: • Jaccard similarity stopwords. coefficient on non• A boolean representing if the two statements have the same polarity or not, where polarity is defined as the number of neg dependencies in the Stanford Parser dependency graph. • Number of “paraphrasing steps” necessary to o"
S13-2100,J10-3003,0,0.0145762,"ference answer” and a 1- or 2-sentence student answer, the goal is to determine the student’s answer accuracy (Dzikovska et al., 2013). This can be seen as a paraphrase identification problem between student answers and reference answers. Paraphrase identification searches whether two sentences have essentially the same meaning (Culicover, 1968). Automatically generating or extracting semantic equivalences for the various units of language – words, phrases, and sentences – is an important problem in NLP and is being increasingly employed to improve the performance of several NLP applications (Madnani and Dorr, 2010), like question-answering and machine translation. Paraphrase identification would benefit from a precise and broad-coverage semantic language model. This is unfortunately difficult to obtain to its full extent for any natural language, due to the size of a typical lexicon and the complexity of grammatical constructions. Our hypothesis is that the simpler the language lexicon is, the easier it will be to access and compare meaning of sentences. This assumption is justified by the multiple attempts at controlled natural languages (Schwitter, 2010) and especially simplified forms of English. One"
S13-2100,C10-2113,0,0.0205446,"The system we present in this paper is the first step towards an open-domain machine reading system capable of understanding and reasoning. Direct modeling of the semantics of a full natural language appears too difficult. We therefore decide to first project the English language onto a simpler English, so that it is easier to model and draw inferences from. One complementary approach to a minimalistic language model, is to accept that texts are replete with gaps: missing information that cannot be inferred by reasoning on the text alone, but require a certain amount of background knowledge. Penas and Hovy (2010) show that these gaps can be filled by maintaining a background knowledge base built from a large corpus. Although Simple Wiktionary is not a large corpus by any means, it can serve our purpose of acquiring basic knowledge for assessing exercise answers, and has the advantage to be in constant evolution and expansion, as well as interfacing very easily with the richer Wiktionary and Wikipedia. Our future work will be focused on enriching and improving the robustness of our knowledge acquisition step from Simple Wiktionary, as well as introducing a true normalization of English to Basic English"
S13-2100,C10-2128,0,0.0233702,"performance of several NLP applications (Madnani and Dorr, 2010), like question-answering and machine translation. Paraphrase identification would benefit from a precise and broad-coverage semantic language model. This is unfortunately difficult to obtain to its full extent for any natural language, due to the size of a typical lexicon and the complexity of grammatical constructions. Our hypothesis is that the simpler the language lexicon is, the easier it will be to access and compare meaning of sentences. This assumption is justified by the multiple attempts at controlled natural languages (Schwitter, 2010) and especially simplified forms of English. One of them, Basic English (Ogden, 1930), has been adopted by the Wikipedia Project as the preferred language of the Simple English Wikipedia1 and its sister project the Simple English Wiktionary2 . Our method starts with acquiring paraphrases from the Simple English Wiktionary’s definitions. Using those, we generate variants of both sentences whose meanings are to be compared. Finally, we compute traditional lexical and semantic similarity measures on those two sets of variants to produce features to train a classifier on the SemEval 2013 datasets"
S13-2100,S13-2045,0,\N,Missing
W01-0905,C00-1039,0,0.0231477,"Missing"
W01-0905,C00-1043,0,0.0606601,"Missing"
W01-0905,P99-1044,0,0.14937,"ce), number (a time expression or a number expression). For example the pattern how far yields to the answer type length: Question: How far away is the moon? Answer type: LENGTH Answer within the document : With a <b_numex_TYPE=""NUMBER""&gt; 28 <e_numex&gt; -power telescope you can see it on the moon <b_numex_TYPE=""LENGTH""&gt; 250,000 miles <e_numex&gt; away. 2.2 Selection of relevant documents The second module is a classic search engine, giving, for each question, a ranked list of documents, each of which could contain the answer. This set of documents is then processed by a third module, made of FASTR (Jacquemin, 1999), a shallow transformational natural language analyser and of a ranker. This module can select, among documents found by the search engine, a subset that satisfies more refined criteria. FASTR improves things because it indexes documents with a set of terms, including not only the (simple or compound) words of the initial question, but also their morphological, syntactic and semantic variants. Each index is given a weight all the higher as it is close to the original word in the question, or as it is significant. For instance, original terms are considered more reliable than semantic variants,"
W01-0905,L00-1000,0,\N,Missing
W01-0909,P93-1041,0,0.0201086,"ion of their mutual information to capture semantic and pragmatic relations between them, computed from their co-occurrence count. In order to build class of words linked to a same topic, we first realize a topic segmentation of the texts in thematic units (TU) whose words refer to the same topic, and learning is applied on these thematic units. Text segmentation is based on the use of the collocation network. A topic is detected by computing a cohesion value for each word resulting from the relations found in the network between these words and their neighbors in a text. As in Kozima’s work (Kozima, 1993), this computation operates on words belonging to a focus window that is moved all over the text.The cohesion values lead to build a graph and by successive transformations applied to it, texts are automatically divided in discourse segments. Such a method leads to delimit small segments, whose size is equivalent to a paragraph, i. e. capable of retrieving topic variations in short texts, as newswires for example. Table 1 shows an extract of the words belonging to a cohesive segment about a dedication of a book. 2.2 Semantic Domain learning in SEGAPSITH Learning a semantic domain consists of a"
W01-1207,C00-1039,1,0.815335,"es for multi-word term and named entity extraction with a specific concern for term conflation through variant recognition. Since named entity recognition has already been described extensively in other publications (Baluja 1999), we present the contribution of terminological variants to adding knowledge to our system. The two main activities involving terminology in NLP are term acquisition and term recognition. Basically, terms can be viewed as a particular type of lexical data. Term variation may involve structural, morphological, and semantic transformations of single or multiwords terms (Fabre and Jacquemin, 2000). In this paper, we describe how QALC uses high level indexes, made of terms and variants, to select among documents the most relevant ones with regard to a question, and then to match candidate answers with this question. In the selection process, the documents first retrieved by a search engine, are then postfiltered and ranked through a weighting scheme based on high level indexes, in order to retain the top ranked ones. Similarly, all systems that participated in TREC9 have a search engine component that firstly selects a subset of the provided database of about one million documents. Sinc"
W01-1207,C00-1043,0,0.0368035,"Missing"
W01-1207,P99-1044,1,0.904337,"uestion relies on a shallow parser which spots discriminating patterns and assigns categories to the question. The categories correspond to the types of entities that are likely to constitute the answer to the question. In order to select the best documents from the results given by the search engine and to locate the answers inside them, we work with terms and their variants, i.e. morphologic, syntactic and semantic equivalent expressions. A term extractor has been developed, based on syntactic patterns which describe complex nominal phrases and their subparts. These terms are used by FASTR (Jacquemin 1999), a shallow transformational natural language analyzer that recognizes their occurrences and their variants. Each occurrence or variant constitutes an index that is subsequently used in the processes of document ranking and question/document matching. Documents are ordered according to a weight computed thanks to the number and the quality of the terms and variants they contain. For example, original terms with proper names are considered more reliable than semantic variants. An analysis of the weight graph enables the system to select a relevant subpart of the documents, whose size varies alo"
W06-1904,1999.tc-1.8,0,0.0248958,"ies, the only way for us to get their translation is to combine all the different term translations. The main drawback of this approach is the generated noise, for none of the terms constituting the biterm is disambiguated. For example, three different translations are found for the biterm Conseil de d´efense : defense council, defense advice and defense counsel ; but only the first of those should be finally retained by our system. To reduce this noise, an interesting possibility is to validate the obtained biterms by searching them or their variants in the complete collection of documents. (Grefenstette, 1999) reports a quite similar experiment in the context of a machine translation task : he uses the Web in order to order the possible translations of noun phrases, and in particular noun biterms. Fastr (Jacquemin, 1996) is a parser which takes as input a corpus and a list of terms (multi or monoterms) and outputs the indexed corpus in which terms and their variants are recognized. Hence, Fastr is quite adequate for biterms validation : it tags all the biterms present in the collection, whether in their original form or in a variant that can be semantic or syntactic. In order to validate the biterm"
W06-1904,2006.jeptalnrecital-long.20,1,0.820536,"Missing"
W09-4504,M98-1001,0,0.0400933,"in the texts, and link them to QKDB values, mostly automatically, for example with WordNet (for some inflections and derivations). The different variations of a term will thus be normalized to a standard form. Besides recognizing the terms of the domain, we will have to work on the selection of relevant results, so that an annotator who will use our assistant tool will not have too erroneous propositions to discard. We will have to define with experts where to draw the line between recall over precision. 5 Relevant work Template based IE systems were developped during the MUC conferences (see [1] for MUC-7 definition task and [7] for MUC-7 results). In MUC7, the Template Relation Task was dedicated to extract relational information on employee of, manufacture of, and location of relations as the Scenario Template Task consisted in extracting prespecified event information and relating the event information to particular organizations, persons, or artifact entities involved in the event. Some of these systems as LaSIE [6] have been adapted to extract biological information, designing 26 PASTA [3]. PASTA aim at extracting information about the roles of residues in protein molecules. The"
W09-4504,W02-0311,0,0.163446,"be decomposed into two tasks: QKDB website: http://physiome.ibisc.fr/qkdb/ • highlighting the including passages and the values of the descriptors for a selected result. The information we look for can be modelled by a template that represents the description of an experimentation in kidney studies. Even if many systems apply IE techniques to scientific papers, they are generally dedicated to the domain of molecular biology and they often look for specific entities and some relations between these entities and not for a complex template. We can find such a problem in systems (see for example [3]) issued from MUC evaluations [6], in which most entities were named entities such as person, organization or location names, that can be recognized using gazetteers and rules relying on linguistic features. In our case, if the result value corresponds to a named entity, other descriptors are domain-specific terms, whose recognition would require to refer to an ontology dedicated to this domain that does not exist currently. Furthermore, it also requires the modelling of the relations between an experimentation and each of its descriptors. Most systems only use abstracts for the extraction tas"
W09-4504,M98-1007,0,0.384286,"DB website: http://physiome.ibisc.fr/qkdb/ • highlighting the including passages and the values of the descriptors for a selected result. The information we look for can be modelled by a template that represents the description of an experimentation in kidney studies. Even if many systems apply IE techniques to scientific papers, they are generally dedicated to the domain of molecular biology and they often look for specific entities and some relations between these entities and not for a complex template. We can find such a problem in systems (see for example [3]) issued from MUC evaluations [6], in which most entities were named entities such as person, organization or location names, that can be recognized using gazetteers and rules relying on linguistic features. In our case, if the result value corresponds to a named entity, other descriptors are domain-specific terms, whose recognition would require to refer to an ontology dedicated to this domain that does not exist currently. Furthermore, it also requires the modelling of the relations between an experimentation and each of its descriptors. Most systems only use abstracts for the extraction task; only few of them analyse full-"
W09-4504,M98-1002,0,0.051171,"B values, mostly automatically, for example with WordNet (for some inflections and derivations). The different variations of a term will thus be normalized to a standard form. Besides recognizing the terms of the domain, we will have to work on the selection of relevant results, so that an annotator who will use our assistant tool will not have too erroneous propositions to discard. We will have to define with experts where to draw the line between recall over precision. 5 Relevant work Template based IE systems were developped during the MUC conferences (see [1] for MUC-7 definition task and [7] for MUC-7 results). In MUC7, the Template Relation Task was dedicated to extract relational information on employee of, manufacture of, and location of relations as the Scenario Template Task consisted in extracting prespecified event information and relating the event information to particular organizations, persons, or artifact entities involved in the event. Some of these systems as LaSIE [6] have been adapted to extract biological information, designing 26 PASTA [3]. PASTA aim at extracting information about the roles of residues in protein molecules. The extraction task consists of filli"
W09-4504,M95-1017,0,\N,Missing
W12-0512,O08-3002,0,0.0241167,"Missing"
W12-0512,P07-1098,0,0.0237748,"nd r the rank. The equality bonus, found empirically, is given for each systems pair. The value is 3 if the two answers are equal, 2 if an answer is included in the other and 1 otherwise. When an answer is found by two or more systems, the higher confidence score is kept. The result of this method is that the answers extracted by more than one system are favored. An answer found by only one system, even with a very high confidence score, may be downgraded. 6 Machine-learning-based method for answer re-ranking To solve a re-ranking problem, machine learning approaches can be used (for example (Moschitti et al., 2007)). But in most of the cases, the objective is to re-rank answers provided by one system, that means to re-rank multiple hypotheses from one system. In our case, we want to re-rank multiple answers from different systems. We decided to use an SVM-based approach, namely SVMrank (Joachims, 2006), which is well adapted to our problem. An important aspect is then to choose the pertinent features for such a task. Our objective is to consider robust enough features to deal with different systems’ answers without introducing biases. Two classes of characteristic should be able to give a useful represe"
W12-0512,quintard-etal-2010-question,1,0.888898,"Missing"
W12-0512,toney-etal-2008-evaluation,1,0.833409,"verage distance between the answer and each of the question words in the passage. Other criteria are the passage rank given by using results of the passage analysis, the question category, i.e. definition, characterization of an entity, verb modifier or verb complement, etc. 3.2 The RITEL systems 3.3 General overview The RITEL system (see Figure 1) which we used in these experiments is fully described in (Bernard et al., 2009). This system has been developed within the framework of the Ritel project which aimed at building a human-machine dialogue system for question-answering in open domain (Toney et al., 2008). The same multilevel analysis is carried out on both queries and documents. The objective of this analysis is to find the bits of information that may be of use for search and extraction, called pertinent information chunks. These can be of different categories: named entities, linguistic entities (e.g., verbs, prepositions), or specific entities (e.g., scores). All words that do not fall into such chunks are automatically grouped into chunks via a longest-match strategy. The analysis is hierarchical, resulting in a set of trees. Both answers and important elements of the questions are suppos"
W16-6108,W11-4114,0,0.160936,"Missing"
W18-5309,P17-1171,0,0.0209279,"d ones (500 answers). We found that some answers contained the whole snippet as an answer and that 3503 snippets are repeated in the 6B train set. After filtering those repeated snippets we found 3286 different snippets containing exact matching answers extracted automatically from gold standard data and 4965 unique snippets manually annotated with correct answers. 5 Experiments The goal of our experiments is to study the impact of the data augmentation on training and evaluating a system. Henceforth, we follow the process of (Wiese et al., 2017b) and use a machine reading model developed by (Chen et al., 2017) that is pre-trained on SQUAD dataset (Rajpurkar et al., 2016) for open domain questions and fine tuned to biomedical questions. To study the impact on the training process and the evaluations, we train the models using separately the automatically annotated data and the fully manually annotated data. We also evaluate them using both kinds of data separately. 5.1 QA system overview Figure 2: Transfer learning from open domain to biomedical domain We present here the adaptation of an existing model named DRQA reader by (Chen et al., 2017) to the biomedical domain as presented in (Kamath et al.,"
W18-5309,P16-1145,0,0.0326616,"ance. We show the impact of the enriched data by experimenting on 5B and 6B training datasets. Our method outperforms the best1 2 Related Work 1. Question processing for question type detection and lexical answer type detection. 2. Document retrieval (Task B Phase A) 3. Answer extraction by answer re-ranking on the candidate answers generated in the previous phases, done in a supervised learning manner. In the open domain, deep learning models are extensively used in machine reading task. Datasets such as MS Marco by (Nguyen et al., 2016), SQUAD by (Rajpurkar et al., 2016) and Wikireading by (Hewlett et al., 2016) have made it easier for deep learning models to perform better on machine reading task. One of the first attempts to use deep learning algorithms for the Bioasq task was reported in BIOASQ 5 by (Wiese et al., 2017b) where the dataset was adapted to be used as a machine reading dataset whose goal is to extract answers from snippets. The authors use a model trained on open domain questions, and perform domain adaptation to biomedical domain using BIOASQ data. Their system got one of the best results whose methods are reported in the section 5. http://bioasq.org/ https://rajpurkar.github.io/SQuA"
W18-5309,D16-1264,0,0.175684,"d standard answers on the evaluation performance. We show the impact of the enriched data by experimenting on 5B and 6B training datasets. Our method outperforms the best1 2 Related Work 1. Question processing for question type detection and lexical answer type detection. 2. Document retrieval (Task B Phase A) 3. Answer extraction by answer re-ranking on the candidate answers generated in the previous phases, done in a supervised learning manner. In the open domain, deep learning models are extensively used in machine reading task. Datasets such as MS Marco by (Nguyen et al., 2016), SQUAD by (Rajpurkar et al., 2016) and Wikireading by (Hewlett et al., 2016) have made it easier for deep learning models to perform better on machine reading task. One of the first attempts to use deep learning algorithms for the Bioasq task was reported in BIOASQ 5 by (Wiese et al., 2017b) where the dataset was adapted to be used as a machine reading dataset whose goal is to extract answers from snippets. The authors use a model trained on open domain questions, and perform domain adaptation to biomedical domain using BIOASQ data. Their system got one of the best results whose methods are reported in the section 5. http://bi"
W18-5309,W16-3105,0,0.061634,"Missing"
W18-5309,E12-2021,0,0.0410262,"Missing"
W18-5309,K17-1029,0,0.0244576,"Missing"
W18-5309,W17-2309,0,0.125596,"detection. 2. Document retrieval (Task B Phase A) 3. Answer extraction by answer re-ranking on the candidate answers generated in the previous phases, done in a supervised learning manner. In the open domain, deep learning models are extensively used in machine reading task. Datasets such as MS Marco by (Nguyen et al., 2016), SQUAD by (Rajpurkar et al., 2016) and Wikireading by (Hewlett et al., 2016) have made it easier for deep learning models to perform better on machine reading task. One of the first attempts to use deep learning algorithms for the Bioasq task was reported in BIOASQ 5 by (Wiese et al., 2017b) where the dataset was adapted to be used as a machine reading dataset whose goal is to extract answers from snippets. The authors use a model trained on open domain questions, and perform domain adaptation to biomedical domain using BIOASQ data. Their system got one of the best results whose methods are reported in the section 5. http://bioasq.org/ https://rajpurkar.github.io/SQuAD-explorer/ 72 Proceedings of the 2018 EMNLP Workshop BioASQ: Large-scale Biomedical Semantic Indexing and Question Answering, pages 72–78 c Brussels, Belgium, November 1st, 2018. 2018 Association for Computational"
W18-5309,W16-3104,0,0.0280202,"dataset provided by the organizers contains answers, but not all their variants. Henceforth a manual annotation was performed to extract all forms of correct answers. This article shows the impact of using all occurrences of correct answers for training on the evaluation scores which are improved significantly. 1 2 Several works in the past BIOASQ tasks have used classical question answering pipeline architecture adapted to the biomedical domain. Some use the domain-specific information from UMLS tools such as Metamap (Schulze et al., 2016), along with other NLP tools like Corenlp, LingPipe (Yang et al., 2016). A typical question answering pipeline consists of: Introduction BIOASQ1 challenge is a large-scale biomedical semantic indexing and question answering task (Tsatsaronis et al., 2015) which has been successful for 5 years. The challenge proposes several tasks using Biomedical data. One of the tasks focuses on Biomedical question answering (Task B Phase B - we further refer it as B) where the goal is to extract answers for a given question from relevant snippets. Several teams have participated actively, and a noticeable aspect is that the results of the task B are much lower compared to open"
wang-etal-2012-evaluation,P04-1053,0,\N,Missing
wang-etal-2012-evaluation,P08-1004,0,\N,Missing
wang-etal-2012-evaluation,N06-1039,0,\N,Missing
