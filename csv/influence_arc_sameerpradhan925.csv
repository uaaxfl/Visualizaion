2021.acl-short.59,D18-1016,0,0.302656,"Missing"
2021.acl-short.59,P15-1136,0,0.125976,"Missing"
2021.acl-short.59,D16-1245,0,0.0993721,"Missing"
2021.acl-short.59,P16-1061,0,0.14783,"hich groups the markables into clusters. As a key component in Natural Language Understanding (NLU), the task can benefit a series of downstream applications such as Entity Linking, Dialogue Systems, Machine Translation, Summarization, and more (Poesio et al., 2016). In recent years, deep learning models have achieved high scores in coreference resolution. The end-to-end approach (Lee et al., 2017, 2018) jointly scoring mention detection and resolution currently not only beats earlier rule-based and statistical methods but also outperforms other deep learning approaches (Wiseman et al., 2016; Clark and Manning, 2016a,b). Additionally, language models trained on billions of words significantly improve performance by providing rich word and contextlevel information for classifiers (Lee et al., 2018; Joshi et al., 2019a,b). However, scores on the identity coreference layer of benchmark OntoNotes dataset (Pradhan et al., 2013) do not reflect the generalizability of these systems. Moosavi and Strube (2017) pointed out that lexicalized coreference resolution models, including neural models using word embeddings, face a covert overfitting problem because of a large overlap between the vocabulary of coreferring"
2021.acl-short.59,N19-1423,0,0.0299582,"339 2,057 19,378 Proper 283 934 40 259 501 796 117 171 108 22 564 53 3,848 Pron. 262 796 728 1,700 1,223 340 1,336 245 165 600 300 1,001 8,696 Other 687 582 259 781 898 667 844 185 193 260 475 1,003 68,34 Clusters 421 487 176 469 608 477 578 134 133 149 348 491 4,471 Table 1: Genre-breakdown Statistics of OntoGUM. tion has been small and homogeneous: WikiCoref only contains 30 documents with ∼60K tokens, much smaller than the OntoNotes test set, and the single genre Wiki domain in both WikiCoref and GAP is arguably not very far from some OntoNotes materials. Second, pretrained LMs, e.g. BERT (Devlin et al., 2019), popularized after the WikiCoref paper, can learn better representations of markables and surrounding sentences. Other than GAP, which targets a highly specific subtask, no study has investigated if contextualized embeddings encounter the same overfitting problem identified by Moosavi and Strube. Third, previous work may underestimate the performance degradation on WikiCoref in particular due to bias: In Moosavi and Strube (2018), embeddings were also trained on Wikipedia themselves, potentially making the model easier to learn coreference relations in Wikipedia text, despite limitations in o"
2021.acl-short.59,D13-1203,0,0.155764,"Missing"
2021.acl-short.59,Q14-1037,0,0.158737,"Missing"
2021.acl-short.59,P19-1064,0,0.331068,"Missing"
2021.acl-short.59,L16-1021,0,0.190587,"Missing"
2021.acl-short.59,2020.tacl-1.5,0,0.147222,"Missing"
2021.acl-short.59,D19-1588,0,0.542553,"Missing"
2021.acl-short.59,J13-4004,0,0.449818,"Missing"
2021.acl-short.59,P17-2003,0,0.265394,"Missing"
2021.acl-short.59,D18-1018,0,0.233213,"OntoNotes test set, and the single genre Wiki domain in both WikiCoref and GAP is arguably not very far from some OntoNotes materials. Second, pretrained LMs, e.g. BERT (Devlin et al., 2019), popularized after the WikiCoref paper, can learn better representations of markables and surrounding sentences. Other than GAP, which targets a highly specific subtask, no study has investigated if contextualized embeddings encounter the same overfitting problem identified by Moosavi and Strube. Third, previous work may underestimate the performance degradation on WikiCoref in particular due to bias: In Moosavi and Strube (2018), embeddings were also trained on Wikipedia themselves, potentially making the model easier to learn coreference relations in Wikipedia text, despite limitations in other genres. In this paper, we explore the generalizability of existing coreference models on a new benchmark dataset, which we make freely available. Compared with work using WikiCoref and GAP, our contributions can be summarized as follows: • We propose OntoGUM, the largest open, gold dataset consistent with OntoNotes, with 168 documents (∼150K tokens, 19,378 mentions, 4,471 coref chains) in 12 genres,1 including conversational"
2021.acl-short.59,P14-2006,1,0.939478,"Missing"
2021.acl-short.59,W12-4501,1,0.882189,"Missing"
2021.acl-short.59,N13-1071,0,0.261138,"Missing"
2021.acl-short.59,Q18-1042,0,0.336558,"Missing"
2021.acl-short.59,N18-2108,0,0.355434,"Missing"
2021.acl-short.59,P14-5010,0,0.00774702,"Missing"
2021.acl-short.59,P15-1137,0,0.45641,"Missing"
2021.acl-short.59,D17-1018,0,0.092808,"ases and the pronouns that refer to them. The task entails detecting correct mention or ‘markable’ boundaries and creating a link with previous mentions, or antecedents. A coreference chain is a series of decisions which groups the markables into clusters. As a key component in Natural Language Understanding (NLU), the task can benefit a series of downstream applications such as Entity Linking, Dialogue Systems, Machine Translation, Summarization, and more (Poesio et al., 2016). In recent years, deep learning models have achieved high scores in coreference resolution. The end-to-end approach (Lee et al., 2017, 2018) jointly scoring mention detection and resolution currently not only beats earlier rule-based and statistical methods but also outperforms other deep learning approaches (Wiseman et al., 2016; Clark and Manning, 2016a,b). Additionally, language models trained on billions of words significantly improve performance by providing rich word and contextlevel information for classifiers (Lee et al., 2018; Joshi et al., 2019a,b). However, scores on the identity coreference layer of benchmark OntoNotes dataset (Pradhan et al., 2013) do not reflect the generalizability of these systems. Moosavi a"
2021.acl-short.59,N16-1114,0,0.105694,"Missing"
2021.crac-1.15,D18-1016,0,0.0305378,"Missing"
2021.crac-1.15,P15-1136,0,0.0573536,"Missing"
2021.crac-1.15,N19-1423,0,0.0550597,"Missing"
2021.crac-1.15,D13-1203,0,0.0290261,"airs, used for out-of-domain evaluation but limited in coreferent types and genre. Several other comprehensive coreference datasets exist as well, such as ARRAU (Poesio et al., 2018) and PreCo (Chen et al., 2018), but these corpora cannot be used for out-of-domain evaluation because they do not follow the OntoNotes scheme. Their conversion has not been attempted to date. Coreference resolution systems Prior to the introduction of deep learning systems, the coreference task was approached using deterministic linguistic rules (Lee et al., 2013; Recasens et al., 2013) and statistical approaches (Durrett and Klein, 2013, 2014). More recently, three neural models achieved SOTA performance on this task: 1) ranking the candidate mention pairs (Wiseman et al., 2015; Clark and Manning, 2016a), 2) modeling global features of entity clusters (Clark and Manning, 2015, 2016b; Wiseman et al., 2016), and 3) end-to-end (e2e) approaches with joint loss for mention detection and coreferent pair scoring (Lee et al., 2017, 2018; Fei et al., 2019). The e2e method has become the dominant one, gaining the best scores on OntoNotes. To investigate differences between deterministic and deep learning models on unseen data, we eval"
2021.crac-1.15,Q14-1037,0,0.0622641,"Missing"
2021.crac-1.15,P19-1064,0,0.0287493,"Missing"
2021.crac-1.15,L16-1021,0,0.0564613,"Missing"
2021.crac-1.15,2020.tacl-1.5,0,0.0213842,"Missing"
2021.crac-1.15,D19-1588,0,0.0614652,"Missing"
2021.crac-1.15,J13-4004,0,0.0417716,"xtbook; Spoken: Interview/Political/Vlog/Conversation. pronoun-name pairs, used for out-of-domain evaluation but limited in coreferent types and genre. Several other comprehensive coreference datasets exist as well, such as ARRAU (Poesio et al., 2018) and PreCo (Chen et al., 2018), but these corpora cannot be used for out-of-domain evaluation because they do not follow the OntoNotes scheme. Their conversion has not been attempted to date. Coreference resolution systems Prior to the introduction of deep learning systems, the coreference task was approached using deterministic linguistic rules (Lee et al., 2013; Recasens et al., 2013) and statistical approaches (Durrett and Klein, 2013, 2014). More recently, three neural models achieved SOTA performance on this task: 1) ranking the candidate mention pairs (Wiseman et al., 2015; Clark and Manning, 2016a), 2) modeling global features of entity clusters (Clark and Manning, 2015, 2016b; Wiseman et al., 2016), and 3) end-to-end (e2e) approaches with joint loss for mention detection and coreferent pair scoring (Lee et al., 2017, 2018; Fei et al., 2019). The e2e method has become the dominant one, gaining the best scores on OntoNotes. To investigate differ"
2021.crac-1.15,D17-1018,0,0.0662272,"2018) using contextualized embeddings; however GAP only contains name-pronoun corefguage Understanding (NLU), the task can benefit erence, a very specific subset of coreference, and a series of downstream applications such as Entity Linking, Dialogue Systems, Machine Translation, is limited in domain to the same single source – Wikipedia. Summarization, and more (Poesio et al., 2016). In recent years, deep learning models have Though previous work has already identified the achieved high scores in coreference resolution. The overfitting problem, it has three main shortcomend-to-end approach (Lee et al., 2017, 2018) jointly ings. First, the scale of out-of-domain evaluation scoring mention detection and coreference resolu- has been small and homogeneous: WikiCoref only tion currently not only beats earlier rule-based and contains 30 documents with ∼60K tokens, much statistical methods but also outperforms other deep smaller than the OntoNotes test set, and the single learning approaches (Wiseman et al., 2016; Clark genre Wiki domain in both WikiCoref and GAP is and Manning, 2016a,b). Additionally, language arguably not very far from some OntoNotes matemodels trained on billions of words significan"
2021.crac-1.15,N18-2108,0,0.0293726,"Missing"
2021.crac-1.15,D16-1245,0,0.0377212,"Missing"
2021.crac-1.15,P14-5010,0,0.00778444,"Missing"
2021.crac-1.15,P16-1061,0,0.0142879,"et al., 2018) and PreCo (Chen et al., 2018), but these corpora cannot be used for out-of-domain evaluation because they do not follow the OntoNotes scheme. Their conversion has not been attempted to date. Coreference resolution systems Prior to the introduction of deep learning systems, the coreference task was approached using deterministic linguistic rules (Lee et al., 2013; Recasens et al., 2013) and statistical approaches (Durrett and Klein, 2013, 2014). More recently, three neural models achieved SOTA performance on this task: 1) ranking the candidate mention pairs (Wiseman et al., 2015; Clark and Manning, 2016a), 2) modeling global features of entity clusters (Clark and Manning, 2015, 2016b; Wiseman et al., 2016), and 3) end-to-end (e2e) approaches with joint loss for mention detection and coreferent pair scoring (Lee et al., 2017, 2018; Fei et al., 2019). The e2e method has become the dominant one, gaining the best scores on OntoNotes. To investigate differences between deterministic and deep learning models on unseen data, we evaluate the two approaches on OntoGUM. 3 Dataset Conversion Order of conversion steps &gt; Remove bridging & cataphora &gt; Contract verbal spans &gt; Merge appositions &gt; Remove NN"
2021.crac-1.15,P17-2003,0,0.0308209,"Missing"
2021.crac-1.15,D18-1018,0,0.0268855,"Other 687 582 259 781 898 667 844 185 193 260 475 1,003 68,34 Clusters 421 487 176 469 608 477 578 134 133 149 348 491 4,471 Table 1: Genre-breakdown Statistics of OntoGUM. et al., 2019), popularized after the WikiCoref paper, can learn better representations of markables and surrounding sentences. Aside from GAP, which targets a highly specific subtask, no study has investigated whether contextualized embeddings encounter the same overfitting problem identified by Moosavi and Strube. Third, previous work may underestimate the performance degradation on WikiCoref in particular due to bias: In Moosavi and Strube (2018), embeddings were also trained on Wikipedia themselves, potentially making it easier for the model to learn coreference relations in Wikipedia text, despite limitations in other genres. In this paper, we explore the generalizability of existing coreference models on a new benchmark dataset, which we make freely available. Compared with work using WikiCoref and GAP, our contributions can be summarized as follows: • We propose OntoGUM, the largest open, gold standard dataset consistent with OntoNotes, with 168 documents (∼150K tokens, 19,378 mentions, 4,471 coref chains) in 12 genres,1 including"
2021.crac-1.15,2021.acl-short.59,1,0.0948548,"Missing"
2021.crac-1.15,P14-2006,1,0.836791,"Missing"
2021.crac-1.15,W12-4501,1,0.677914,"3 62.2 66.4 68.0 64.8 68.2 60.6 61.7 67.0 55.5 73.3 62.1 57.4 64.6 79.6 91.0 87.7 93.0 91.1 85.9 71.9 85.3 91.9 60.0 89.4 78.9 93.2 85.4 89.1 55.2 74.5 77.9 67.7 70.4 70.5 68.1 69.4 72.2 85.4 75.5 52.4 69.2 86.5 68.7 80.5 84.8 77.7 77.3 71.2 75.8 79.0 65.5 87.4 77.2 67.1 76.5 87.8 Table 3: Results on the OntoGUM’s test dataset with the deterministic coref model (top) and the SOTA coreference model (bottom). The underlined text is the lowest score across 12 genres and bold text is the highest. 4 Experiments We evaluate two systems on the 12 OntoGUM genres, using the official CoNLL-2012 scorer (Pradhan et al., 2012, 2014). The primary score is the average F1 of three metrics – MUC, B3 , and CEAFφ4 . SOTA neural model Combining the e2e approach with a contextualized LM and span masking is the current SOTA on OntoNotes. The system utilizes the pretrained SpanBERT-large model, finetuned on the OntoNotes training set. Hyperparameters are identical to the evaluation of OntoNotes test to ensure comparable results between the benchmarks. We note that while we choose the SOTA system as a ‘best case scenario’, most off-the-shelf neural NLP toolkits (e.g. spaCy) actually use somewhat simpler e2e models than SpanB"
2021.crac-1.15,N13-1071,0,0.0809976,"Missing"
2021.crac-1.15,Q18-1042,0,0.0313702,"Missing"
2021.crac-1.15,P15-1137,0,0.0137871,"such as ARRAU (Poesio et al., 2018) and PreCo (Chen et al., 2018), but these corpora cannot be used for out-of-domain evaluation because they do not follow the OntoNotes scheme. Their conversion has not been attempted to date. Coreference resolution systems Prior to the introduction of deep learning systems, the coreference task was approached using deterministic linguistic rules (Lee et al., 2013; Recasens et al., 2013) and statistical approaches (Durrett and Klein, 2013, 2014). More recently, three neural models achieved SOTA performance on this task: 1) ranking the candidate mention pairs (Wiseman et al., 2015; Clark and Manning, 2016a), 2) modeling global features of entity clusters (Clark and Manning, 2015, 2016b; Wiseman et al., 2016), and 3) end-to-end (e2e) approaches with joint loss for mention detection and coreferent pair scoring (Lee et al., 2017, 2018; Fei et al., 2019). The e2e method has become the dominant one, gaining the best scores on OntoNotes. To investigate differences between deterministic and deep learning models on unseen data, we evaluate the two approaches on OntoGUM. 3 Dataset Conversion Order of conversion steps &gt; Remove bridging & cataphora &gt; Contract verbal spans &gt; Merge"
2021.crac-1.15,N16-1114,0,0.0284237,"Missing"
J08-2006,W04-2412,0,0.104375,"Missing"
J08-2006,W05-0620,0,0.488033,"Missing"
J08-2006,P05-1022,0,0.154686,"Missing"
J08-2006,J02-3001,0,0.954218,"labeled with a preﬁx “R-” appended. We follow the standard convention of using Section 02 to Section 21 as the training set, Section 00 as the development set, and Section 23 as the test set. The training set comprises about 90,000 predicates instantiating about 250,000 arguments and the test set comprises about 5,000 predicates instantiating about 12,000 arguments. 3. Task Description In ASSERT, the task of semantic role labeling is implemented by assigning role labels to constituents of a syntactic parse. Parts of the overall process can be analyzed as three different tasks as introduced by Gildea and Jurafsky (2002): 1. Argument Identiﬁcation—This is the process of identifying parsed constituents in the sentence that represent semantic arguments of Figure 1 Syntax tree for a sentence illustrating the PropBank tags. 291 Computational Linguistics Volume 34, Number 2 Figure 2 Syntax tree for a sentence illustrating the PropBank arguments. a given predicate. Each node in a parse tree can be classiﬁed (with respect to a given predicate) as either one that represents a semantic argument (i.e., a NON -N ULL node) or one that does not represent any semantic argument (i.e., a N ULL node). 2. Argument Classiﬁcatio"
J08-2006,W04-2416,1,0.599093,"feature. 4.2.8 Head Word. Syntactic head of the constituent. 4.2.9 Head Word POS. Part of speech of the head word. 4.2.10 Named Entities in Constituents. Binary features for seven named entities (P ERSON , O RGANIZATION , L OCATION , P ERCENT, M ONEY, T IME , D ATE) tagged by IdentiFinder (Bikel, Schwartz, and Weischedel 1999). 4.2.11 Path Generalizations. 1. Partial Path—Path from the constituent to the lowest common ancestor of the predicate and the constituent. 2. Clause-based path variations—Position of the clause node (S, SBAR) seems to be an important feature in argument identiﬁcation (Hacioglu et al. 2004). Therefore we experimented with four clause-based path feature variations. (a) 294 Replacing all the nodes in a path other than clause nodes with an asterisk. For example, the path NP↑S↑VP↑SBAR↑NP↑VP↓VBD becomes NP↑S↑*S↑*↑*↓VBD. Pradhan, Ward, and Martin (b) (c) (d) Towards Robust Semantic Role Labeling Retaining only the clause nodes in the path, which for the given example would produce NP↑S↑S↓VBD. Adding a binary feature that indicates whether the constituent is in the same clause as the predicate. Collapsing the nodes between S nodes, which gives NP↑S↑NP↑VP↓VBD. 3. Path n-grams—This featu"
J08-2006,W05-0625,0,0.0245547,"rg. ∗∗ The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309. E-mail: whw@colorado.edu. † The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309. E-mail: martin@colorado.edu. Submission received: 15 July 2006; revised submission received: 3 May 2007; accepted for publication: 19 June 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 2 been possible to achieve accuracies within the range of human inter-annotator agreement. More recent approaches have involved using improved features such as n-best parses (Koomen et al. 2005; Toutanova, Haghighi, and Manning 2005); exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypotheses from different labeling systems using inference (M`arquez et al. 2005); as well as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information. Some have also tried to jointly decode the syntactic and semantic structures (Yi and Palmer 2005; Musill"
J08-2006,W00-0730,0,0.040706,"assignment. The basic inputs are a sentence and a syntactic parse of the sentence. For each constituent in the parse tree, the system extracts a set of features and uses a classiﬁer to assign a label to the constituent. The set of labels used are the PropBank argument labels plus NULL, which means no argument is assigned to that constituent for the predicate under consideration. Support vector machines (SVMs) (Burges 1998; Vapnik 1998) have been shown to perform well on text classiﬁcation tasks, where data is represented in a high dimensional space using sparse feature vectors (Joachims 1998; Kudo and Matsumoto 2000; Lodhi et al. 2002). We formulate the problem as a multi-class classiﬁcation problem using an SVM classiﬁer. We employ a O NE vs A LL (OVA) approach to train n classiﬁers for a multi-class problem. The classiﬁers are trained to discriminate between examples 1 www.cemantix.org/assert. 292 Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling of each class, and those belonging to all other classes combined. During testing, the classiﬁer scores on an example are combined to predict its class label. ASSERT was developed using TinySVM2 along with YamCha3 (Kudo and Matsumoto 2000, 2001) a"
J08-2006,N01-1025,0,0.160766,"Missing"
J08-2006,P98-2127,0,0.213323,"s used to identify passives. 4.2.6 SubCategorization. This is the phrase structure rule expanding the predicate’s parent node in the parse tree. For example, in Figure 3, the subcategorization for the predicate “went” is VP→VBD-PP-NP. 4.2.7 Predicate Cluster. The distance function used for clustering is based on the intuition that verbs with similar semantics will tend to have similar direct objects. For example, verbs such as eat, devour, and savor will tend to all occur with direct objects describing food. The clustering algorithm uses a database of verb–direct-object relations extracted by Lin (1998). The verbs were clustered into 64 classes using the probabilistic cooccurrence model of Hofmann and Puzicha (1998). We then use the verb class of the current predicate as a feature. 4.2.8 Head Word. Syntactic head of the constituent. 4.2.9 Head Word POS. Part of speech of the head word. 4.2.10 Named Entities in Constituents. Binary features for seven named entities (P ERSON , O RGANIZATION , L OCATION , P ERCENT, M ONEY, T IME , D ATE) tagged by IdentiFinder (Bikel, Schwartz, and Weischedel 1999). 4.2.11 Path Generalizations. 1. Partial Path—Path from the constituent to the lowest common ance"
J08-2006,J93-2004,0,0.031636,"Missing"
J08-2006,N06-1020,0,0.0841384,"Missing"
J08-2006,P06-1043,0,0.0441905,"Missing"
J08-2006,N06-2025,0,0.0152797,"ible to achieve accuracies within the range of human inter-annotator agreement. More recent approaches have involved using improved features such as n-best parses (Koomen et al. 2005; Toutanova, Haghighi, and Manning 2005); exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypotheses from different labeling systems using inference (M`arquez et al. 2005); as well as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information. Some have also tried to jointly decode the syntactic and semantic structures (Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject of two CoNLL shared tasks (Carreras and M`arquez 2004; Carreras and M`arquez 2005). Although all of these systems perform quite well on the standard test data, they show signiﬁcant performance degradation when applied to test data drawn from a genre different from the data on which the system was trained. The focus of this article is to present results from an examination"
J08-2006,N06-2026,0,0.0261916,". 2005; Toutanova, Haghighi, and Manning 2005); exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypotheses from different labeling systems using inference (M`arquez et al. 2005); as well as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information. Some have also tried to jointly decode the syntactic and semantic structures (Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject of two CoNLL shared tasks (Carreras and M`arquez 2004; Carreras and M`arquez 2005). Although all of these systems perform quite well on the standard test data, they show signiﬁcant performance degradation when applied to test data drawn from a genre different from the data on which the system was trained. The focus of this article is to present results from an examination into the primary causes of the lack of portability across genres of data. To set the stage for these experiments we ﬁrst describe the operation of ASSERT, our state-of-the art SRL syst"
J08-2006,C04-1100,0,0.0309742,"entiﬁcation transfer relatively well to a new corpus, argument classiﬁcation does not. An analysis of the reasons for this is presented and these generally point to the nature of the more lexical/semantic features dominating the classiﬁcation task where more general structural features are dominant in the argument identiﬁcation task. 1. Introduction Automatic, accurate, and wide-coverage techniques that can annotate naturally occurring text with semantic structure can play a key role in NLP applications such as information extraction (Harabagiu, Bejan, and Morarescu 2005), question answering (Narayanan and Harabagiu 2004), and summarization. Semantic role labeling (SRL) is one method for producing such semantic structure. When presented with a sentence, a semantic role labeler should, for each predicate in the sentence, ﬁrst identify and then label its semantic arguments. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning speciﬁc labels to them. In the bulk of recent work, this problem has been cast as a problem in supervised machine learning. Using these techniques with hand-corrected syntactic parses, it has ∗ Department of Speech and Language"
J08-2006,J05-1004,0,0.791424,"Missing"
J08-2006,N04-1030,1,0.707395,"ypotheses for each node in the syntax tree. A Viterbi search through the lattice uses the probabilities assigned by the sigmoid as the observation probabilities, along with the argument sequence language model probabilities, to ﬁnd the maximum likelihood path such that each node is either assigned a value belonging to the PropBank arguments, or N ULL. The search is also constrained so that no two nodes that overlap are both assigned NON -N ULL labels. 4.2 Features The feature set used in ASSERT is a combination of features described in Gildea and Jurafsky (2002) as well as those introduced in Pradhan et al. (2004), Surdeanu et al. (2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is the list of features used. 4.2.1 Predicate. This is the predicate whose arguments are being identiﬁed. The surface form as well as the lemma are added as features. 4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the predicate being classiﬁed. For example, in Figure 3, the path from A RG 0 (The lawyers) to the predicate went is represented with the string NP↑S↓VP↓VBD. ↑ and ↓ represent upward and downward movement in the tree, respectively. 4.2.3 Phrase T"
J08-2006,P05-1072,1,0.910047,"Missing"
J08-2006,P03-1002,0,0.0610249,"in the syntax tree. A Viterbi search through the lattice uses the probabilities assigned by the sigmoid as the observation probabilities, along with the argument sequence language model probabilities, to ﬁnd the maximum likelihood path such that each node is either assigned a value belonging to the PropBank arguments, or N ULL. The search is also constrained so that no two nodes that overlap are both assigned NON -N ULL labels. 4.2 Features The feature set used in ASSERT is a combination of features described in Gildea and Jurafsky (2002) as well as those introduced in Pradhan et al. (2004), Surdeanu et al. (2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is the list of features used. 4.2.1 Predicate. This is the predicate whose arguments are being identiﬁed. The surface form as well as the lemma are added as features. 4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the predicate being classiﬁed. For example, in Figure 3, the path from A RG 0 (The lawyers) to the predicate went is represented with the string NP↑S↓VP↓VBD. ↑ and ↓ represent upward and downward movement in the tree, respectively. 4.2.3 Phrase Type. Syntactic category"
J08-2006,P05-1073,0,0.063777,"Missing"
J08-2006,W04-3212,0,0.402624,"obabilities assigned by the sigmoid as the observation probabilities, along with the argument sequence language model probabilities, to ﬁnd the maximum likelihood path such that each node is either assigned a value belonging to the PropBank arguments, or N ULL. The search is also constrained so that no two nodes that overlap are both assigned NON -N ULL labels. 4.2 Features The feature set used in ASSERT is a combination of features described in Gildea and Jurafsky (2002) as well as those introduced in Pradhan et al. (2004), Surdeanu et al. (2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is the list of features used. 4.2.1 Predicate. This is the predicate whose arguments are being identiﬁed. The surface form as well as the lemma are added as features. 4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the predicate being classiﬁed. For example, in Figure 3, the path from A RG 0 (The lawyers) to the predicate went is represented with the string NP↑S↓VP↓VBD. ↑ and ↓ represent upward and downward movement in the tree, respectively. 4.2.3 Phrase Type. Syntactic category (NP, PP, etc.) of the constituent. 4.2.4 Position. Whether the cons"
J08-2006,W05-0639,0,0.0234221,"parses (Koomen et al. 2005; Toutanova, Haghighi, and Manning 2005); exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypotheses from different labeling systems using inference (M`arquez et al. 2005); as well as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information. Some have also tried to jointly decode the syntactic and semantic structures (Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject of two CoNLL shared tasks (Carreras and M`arquez 2004; Carreras and M`arquez 2005). Although all of these systems perform quite well on the standard test data, they show signiﬁcant performance degradation when applied to test data drawn from a genre different from the data on which the system was trained. The focus of this article is to present results from an examination into the primary causes of the lack of portability across genres of data. To set the stage for these experiments we ﬁrst describe the operation of ASSERT, our"
K15-2001,P09-1075,0,0.0149588,"course of the sixteen CoNLL shared 1 http://www.seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative a"
K15-2001,P12-1007,0,0.0215894,"omputational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the com"
K15-2001,P14-1048,0,0.0131111,"tational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “d"
K15-2001,W12-1622,0,0.047129,"anguage Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain cruci"
K15-2001,P14-1002,0,0.0686762,"re generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learn"
K15-2001,K15-2004,0,0.0849821,"Missing"
K15-2001,P13-2013,0,0.189614,"uly 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard”"
K15-2001,P13-1047,0,0.0359978,"ociation for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based lear"
K15-2001,W14-4320,0,0.0531135,"of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The re"
K15-2001,K15-2006,0,0.17442,"Missing"
K15-2001,P14-1003,0,0.00827391,"or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The rest of this overvi"
K15-2001,K15-2007,0,0.107459,"Missing"
K15-2001,D09-1036,1,0.729252,"Missing"
K15-2001,P14-5010,0,0.00276217,", 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the trainBrown clusters VerbNet Sentiment lexicon Word embeddings (word2vec) • Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014)) As it turned out, all of the teams this year chose to participate in the closed track. 4.2 Evaluation Platform: TIRA We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. This year, however, we shifted this evaluation paradigm, asking participants to deploy their systems on a remote virtual machine, and to use the TIRA web platform (tira.io) to run their syste"
K15-2001,W11-1901,1,0.0252812,"Nianwen Xue∗ Hwee Tou Ng† Sameer Pradhan‡ Rashmi Prasad3 Christopher Bryant† Attapol T. Rutherford∗ ∗ Brandeis University xuen,tet@brandeis.edu † National University of Singapore nght,bryant@comp.nus.edu.sg ‡ Boulder Language Technologies pradhan@bltek.com 3 University of Wisconsin-Milwaukee prasadr@uwm.edu Abstract tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks ("
K15-2001,J93-2004,0,0.058942,"if present; 2. Identify the spans of text that serve as the two arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments; 4. Predict the sense of the discourse relation (e.g., “Cause”, “Condition”, “Contrast”). 3 Data 3.1 Training and Development The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse TreeBank 2.0. (PDTB-2.0.) (Prasad et al., 2008; Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005). The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text. Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two. A limited but well-defined set of implicit relations are also annotated within sentences. Arguments of relations are annotated in each case, f"
K15-2001,W12-4501,1,0.544745,"Ng† Sameer Pradhan‡ Rashmi Prasad3 Christopher Bryant† Attapol T. Rutherford∗ ∗ Brandeis University xuen,tet@brandeis.edu † National University of Singapore nght,bryant@comp.nus.edu.sg ‡ Boulder Language Technologies pradhan@bltek.com 3 University of Wisconsin-Milwaukee prasadr@uwm.edu Abstract tasks organized over the past two decades, progressing gradually to tackle phenomena at the word and phrase level phenomena and then the sentence and extra-sentential level, it was only very recently that discourse level processing has been addressed, with coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). The 2015 shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et"
K15-2001,W04-2703,1,0.817359,"Missing"
K15-2001,W15-0210,1,0.806958,"shared task takes the community a step further in that direction, with the potential to impact scores of richer language applications (Webber et al., 2012). Given an English newswire text as input, the goal of the shared task is to detect and categorize discourse relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annot"
K15-2001,K15-2010,0,0.14262,"Missing"
K15-2001,prasad-etal-2008-penn,1,0.768574,"course relations between discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.1 The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text. A discourse relation can be expressed explicitly or imp"
K15-2001,J05-1004,0,0.00719659,"wo arguments for each relation; 3. Label the arguments as (Arg1 or Arg2) to indicate the order of the arguments; 4. Predict the sense of the discourse relation (e.g., “Cause”, “Condition”, “Contrast”). 3 Data 3.1 Training and Development The training data for the CoNLL-2015 Shared Task was adapted from the Penn Discourse TreeBank 2.0. (PDTB-2.0.) (Prasad et al., 2008; Prasad et al., 2014), annotated over the one million word Wall Street Journal (WSJ) corpus that has also been annotated with syntactic structures (the Penn TreeBank) (Marcus et al., 1993) and propositions (the Proposition Bank) (Palmer et al., 2005). The PDTB annotates discourse relations that hold between eventualities and propositions mentioned in text. Following a lexically grounded approach to annotation, the PDTB annotates relations realized explicitly by discourse connectives drawn from syntactically well-defined classes, as well as implicit relations between adjacent sentences when no explicit connective exists to relate the two. A limited but well-defined set of implicit relations are also annotated within sentences. Arguments of relations are annotated in each case, following the minimality principle for selecting all and only t"
K15-2001,J14-4007,1,0.868703,"een discourse segments in the text. Just as there are different grammatical formalisms and representation frameworks in syntactic parsing, there are also different conceptions of the discourse structure of a text, and data sets annotated following these different theoretical frameworks (Stede, 2012; Webber et al., 2012; Prasad and Bunt, 2015). For example, the RST-DT Corpus (Carlson et al., 2003) is based on the Rhetorical Structure Theory of Mann and Thompson (1988) and produces a complete treestructured RST analysis of a text, whereas the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014) provides a shallow representation of discourse structure, in that each discourse relation is annotated independently of other discourse relations, leaving room for a high-level analysis that may attempt to connect them. For the CoNLL-2015 shared task, we chose to use the PDTB, as it is currently the largest data set annotated with discourse relations.1 The CoNLL-2015 Shared Task is on Shallow Discourse Parsing, a task focusing on identifying individual discourse relations that are present in a natural language text. A discourse relation can be expressed explicitly or implicitly, and takes two"
K15-2001,W12-1614,0,0.349783,"ared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the rela"
K15-2001,E14-1068,1,0.369296,"s. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and weaknesses of “standard” feature-based learning techniques and “deep” representation learning techniques. The rest of this overview paper is structured as follows. In Section"
K15-2001,N07-1051,0,0.00516358,"like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants to use the following linguistic resources in the closed track, other than the trainBrown clusters VerbNet Sentiment lexicon Word embeddings (word2vec) • Phrase structure parses (predicted using the Berkeley parser (Petrov and Klein, 2007)) • Dependency parses (converted from phrase structure parses using the Stanford converter (Manning et al., 2014)) As it turned out, all of the teams this year chose to participate in the closed track. 4.2 Evaluation Platform: TIRA We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams were asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. This year, however, we shifted this evaluation paradigm, asking participants t"
K15-2001,K15-2012,0,0.0750505,"Missing"
K15-2001,C08-2022,0,0.230737,"arsing (SDP). In the course of the sixteen CoNLL shared 1 http://www.seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques o"
K15-2001,P09-1077,0,0.665554,"seas.upenn.edu/˜pdtb 1 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared tas"
K15-2001,N09-1064,0,0.0519034,"Missing"
K15-2001,K15-2002,0,0.195871,"Standard “shallow” architectures typically make use of discrete features while neural networks generally use continuous real-valued features such as word and paragraph embeddings. For discourse connective and argument extraction, token level features extracted from a fixed window centered on the target word token are generally used, and so are features extracted from syntactic parses. Distributional representations such as Brown clusters have generally been used to determine the senses (Chiarcos and Schenk, 2015; Devi et al., 2015; Kong et al., 2015; Song et al., 2015; Stepanov et al., 2015; Wang and Lan, 2015; Wang et al., 2015; Yoshida et al., 2015), although one team also used them in the sequence labeling task for argument extraction (Nguyen et al., 2015). Additional resources used by some systems for sense determination include word embeddings (Chiarcos and Schenk, 2015; Wang et al., 2015), VerbNet classes (Devi et al., 2015; Kong et al., 2015), and the MPQA polarity lexicon (Devi et al., 2015; Kong et al., 2015; Wang and Lan, 2015). Table 4 provides a summary of the different approaches. 6 Results Table 5 shows the performance of all participating systems across the three test evaluation sets"
K15-2001,C12-1168,0,0.04565,"c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an ideal platform for the community to gain crucial insights on the relative strengths and"
K15-2001,K15-2014,0,0.132851,"Missing"
K15-2001,K15-2015,0,0.0995344,"Missing"
K15-2001,C10-2172,0,0.24464,"nth Conference on Computational Natural Language Learning: Shared Task, pages 1–16, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics are generally in the form of sentences, clauses, or in some rare cases, noun phrases. To detect a discourse relation, a participating system needs to: The necessary conditions are also in place for such a task. The release of the RST-DT and PDTB has attracted a significant amount of research on discourse parsing (Pitler et al., 2008; Duverle and Prendinger, 2009; Lin et al., 2009; Pitler et al., 2009; Subba and Di Eugenio, 2009; Zhou et al., 2010; Feng and Hirst, 2012; Ghosh et al., 2012; Park and Cardie, 2012; Wang et al., 2012; Biran and McKeown, 2013; Lan et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014; Li and Nenkova, 2014; Li et al., 2014; Lin et al., 2014; Rutherford and Xue, 2014), and the momentum is building. Almost all of these recent attempts at discourse parsing use machine learning techniques, which is consistent with the theme of the CoNLL conference. The resurgence of deep learning techniques opens the door for innovative approaches to this problem. A shared task on shallow discourse parsing provides an idea"
K15-2001,W01-1605,0,\N,Missing
K15-2001,K15-2003,0,\N,Missing
K16-2001,K16-2003,0,0.135731,"nvolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes, MaxEnt word embeddings no no cip2016 (Kang et al., 2016) Institute of Automation, CAS syntactic parses, MPQA subjectivity, VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitted a system description paper are marked with ∗. subtask are represented in this competition. One is to collect all candidate discourse connective by looking up a list of possible connectives compiled from the training data and train a classifier to disambiguate them. There are two variants in this approach: one strategy is to train a"
K16-2001,K16-2015,0,0.0587712,"6) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes, MaxEnt word embeddings no no cip2016 (Kang et al., 2016) Institute of Automation, CAS syntactic parses, MPQA subjectivity, VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitt"
K16-2001,K16-2018,0,0.0493477,"Missing"
K16-2001,P13-2013,0,0.0614398,"unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse relation senses, participants have generally adopted “conventional” machine learning techniques such as SVM and MaxEnt models that rely on manually designed features. Explicit discourse relation senses can be predicted with high accuracy. The main challenge is predicting implicit discourse relation senses, which has received a considerable amount of attention in recent years (Pitler et al., 2009; Biran and McKeown, 2013; Rutherford and Xue, 2014). Determining implicit discourse relation senses relies on information from the two arguments of the relation. For this subtask, there is a good balance between “conventional” machine learning techniques such as Support Vector Machines and Maximum Entropy models that rely heavily on handcrafted features, and neural network based approaches. A wide variety of features have been used for this subtask, and they include features extracted from syntactic parses (Kang et al., 2016; Kong et al., 2016; Stepanov and Riccardi, 2016; Jain and Majumder, 2016; Wang and Lan, 2016;"
K16-2001,K16-2016,0,0.0652388,"olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labeling and sense) SVM and Maxent (Scikit-learn) syntactic parses, Brown clusters none Word embeddings, parse trees,"
K16-2001,K15-2004,0,0.140524,"Missing"
K16-2001,K16-2009,0,0.166348,"IST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labe"
K16-2001,K16-2021,0,0.296275,"lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labeling and sense) SVM and Maxent (Scikit-learn)"
K16-2001,K16-2013,0,0.191078,"tional overhead. On TIRA, the blind test set can only be accessed in the Connective identification The identification of discourse connectives is not a simple dictionary lookup as some discourse connective expressions are ambiguous and may function as discourse connectives in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia T"
K16-2001,K16-2011,0,0.0335294,"Missing"
K16-2001,K16-2017,0,0.107933,"ay function as discourse connectives in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for i"
K16-2001,K16-2008,0,0.0961919,"Missing"
K16-2001,K16-2022,0,0.0226855,"Missing"
K16-2001,P09-1077,0,0.372145,"ourse connectives are unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse relation senses, participants have generally adopted “conventional” machine learning techniques such as SVM and MaxEnt models that rely on manually designed features. Explicit discourse relation senses can be predicted with high accuracy. The main challenge is predicting implicit discourse relation senses, which has received a considerable amount of attention in recent years (Pitler et al., 2009; Biran and McKeown, 2013; Rutherford and Xue, 2014). Determining implicit discourse relation senses relies on information from the two arguments of the relation. For this subtask, there is a good balance between “conventional” machine learning techniques such as Support Vector Machines and Maximum Entropy models that rely heavily on handcrafted features, and neural network based approaches. A wide variety of features have been used for this subtask, and they include features extracted from syntactic parses (Kang et al., 2016; Kong et al., 2016; Stepanov and Riccardi, 2016; Jain and Majumder,"
K16-2001,I05-3025,1,0.0947611,"as follows: • News articles were extracted from the Wikinews XML dump2 using the publicly available WikiExtractor.py script.3 • Additional processing was done to remove any remaining XML annotations and produce a raw text version of each article (including its title and date). 4 Evaluation The scorer that computes all of the available evaluation metrics is open-source with some contribution from the participants during the task period6 . • Articles written purely in simplified Chinese were identified using the Dragon Mapper4 Python library, and segmented using the NUS Chinese word segmenter (Low et al., 2005). 4.1 Main evaluation metric: End-to-end discourse parsing 1 https://zh.wikinews.org/ https://dumps.wikimedia.org/zhwikinews/20151020/ zhwikinews-20151020-pages-meta-current.xml.bz2 3 http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 4 http://dragonmapper.readthedocs.io/en/latest/index. html A shallow discourse parser (SDP) is evaluated based on the end-to-end F1 score on a per2 5 6 4 https://www.seas.upenn.edu/~pdtb/tools.shtml#annotator http://www.github.com/attapol/conll16st. Sense Definition Alternative Causation Condition Conjunction Contrast Relation between two alternatives Relation"
K16-2001,P14-5010,0,0.00268188,"nefits from the precise evaluation of the progress and improvement since the system is based off the exact same implementation. • Brown clusters (implementation from (Liang, 2005)) • Word embeddings (word2vec) To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation produced using state-of-the-art NLP tools: For English, • Phrase structure parses predicted using the Berkeley parser (Petrov and Klein, 2007); • Dependency parses converted from phrase structure parses using the Stanford converter (Manning et al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and L"
K16-2001,prasad-etal-2008-penn,1,0.839291,"ilingual Shallow Discourse Parsing (SDP). While the 2015 task focused on newswire text data in English, this year we added a new language, Chinese. Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text. The conceptual framework of the Shallow Discourse Parsing 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics task is that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014), where a discourse relation is viewed as a predicate that takes two abstract objects as arguments. The two arguments may be realized as clauses or sentences, or occasionally phrases. It is “shallow” in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized. As such, it differs from analyses according to either Rhetorical Structure (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). The rest of this overview paper"
K16-2001,K16-2014,0,0.241056,"Missing"
K16-2001,J14-4007,1,0.889259,"ourse Parsing (SDP). While the 2015 task focused on newswire text data in English, this year we added a new language, Chinese. Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text. The conceptual framework of the Shallow Discourse Parsing 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics task is that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014), where a discourse relation is viewed as a predicate that takes two abstract objects as arguments. The two arguments may be realized as clauses or sentences, or occasionally phrases. It is “shallow” in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized. As such, it differs from analyses according to either Rhetorical Structure (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). The rest of this overview paper is structured as follo"
K16-2001,K16-2020,0,0.0356651,"Missing"
K16-2001,K16-2010,0,0.282836,"ources used Extra resources nguyenlab (Nguyen, 2016) JAIST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters n"
K16-2001,K16-2002,0,0.416621,", VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitted a system description paper are marked with ∗. subtask are represented in this competition. One is to collect all candidate discourse connective by looking up a list of possible connectives compiled from the training data and train a classifier to disambiguate them. There are two variants in this approach: one strategy is to train a classifier for each individual discourse connective expression (Oepen et al., 2016), and the other is to train one classifier for all discourse connective expressions (Wang and Lan, 2016; Kong et al., 2015; Laali et al., 2016). Alternatively, connective identification is treated as a token-level sequence labeling task, solved with sequence labeling models like CRF (Stepanov and Riccardi, 2016). Argument extraction Different strategies were used for extracting the arguments for explicit and for implicit discourse relations. Determining the arguments of implicit discourse relations is relatively straightforward. Most systems adopted a heuristics–based extraction strategy that"
K16-2001,E14-1068,1,0.449193,"ent algorithms and models can be more meaningfully compared. In the open track, the focus of the evaluation is on the overall performance and the use of all possible means to improve the performance of a task. This distinction was easier to maintain for early CoNLL tasks such as noun phrase chunking and named entity recognition, where competitive performance could be achieved without having to use resources other than the provided training set. However, this is no longer true for a high-level task like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants in the closed track to use the following linguistic resources, in addition to the training set: For English, For purposes of evaluation, an explicit discourse connective predicted by a parser is considered correct if and only if the predicted raw connective includes the gold raw connective head, while allowing for the tokens of the pre"
K16-2001,K16-2019,0,0.096966,"Missing"
K16-2001,K16-2007,1,0.844548,"relations. A variety of neural network architectures are represented. (Schenk et al., 2016) used a feedforward neural network, with dependency structures used to re-weight the word embeddings used as input to the network. (Wang and Lan, 2016; Qin et al., 2016) achieved competitive performance using a Convolutional Neural Network architecture for this subtask. Finally, (Weiss and Bajec, 2016) produced competitive results with a focused RNN. Word embeddings were typically used as input to the neural network models and different pooling methods have been used to derive the vectors for arguments. Rutherford and Xue (2016) used simple summation pooling in a feedforward network and achieved competitive performance in classifying implicit discourse relation senses. Relation sense classification All systems have separate classifiers for explicit and implicit discourse connectives. For explicit relations, the discourse connective itself is the best predictor of the discourse relation. Many discourse connectives are unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse rela"
K16-2001,K16-2012,0,0.156471,"Missing"
K16-2001,K15-2002,0,0.156384,"al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and Lan (2015), which has components for identifying discourse connectives and extracting their arguments, for determining the presence or absence of discourse relations in a particular context, and for predicting the senses of the discourse relations. Here we briefly summarize the approaches used in each subtask. We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams have been asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. Sta"
K16-2001,K16-2004,0,0.188138,"ves in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes,"
K16-2001,P14-1069,1,0.15687,"ation from (Liang, 2005)) • Word embeddings (word2vec) To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation produced using state-of-the-art NLP tools: For English, • Phrase structure parses predicted using the Berkeley parser (Petrov and Klein, 2007); • Dependency parses converted from phrase structure parses using the Stanford converter (Manning et al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and Lan (2015), which has components for identifying discourse connectives and extracting their arguments, for determining the presence or absence of discours"
K16-2001,K16-2006,0,0.0402375,"ments for explicit and for implicit discourse relations. Determining the arguments of implicit discourse relations is relatively straightforward. Most systems adopted a heuristics–based extraction strategy that parallels the PDTB annotation strategy for implicit discourse relations: for each pair of adjacent sentences that do not straddle a paragraph boundary, if an explicit discourse relation does not already exist, posit 8 ID Institution Learning methods Resources used Extra resources nguyenlab (Nguyen, 2016) JAIST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16"
K16-2001,K15-2001,1,0.641779,"prises a long pipeline, and it is hard for teams that do not have a pre-existing system to put together a competitive full system. This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments. 3 Data 3.1 Training and Development The training and development sets for English remain exactly the same as those used in the CoNLL-2015 shared task. Details regarding how the data was adapted from the Penn Discourse TreeBank 2.0 (PDTB 2.0) are provided in the overview paper of the CoNLL 2015 shared task (Xue et al., 2015). The Chinese training and development sets are taken from the Chinese Discourse TreeBank (CDTB) 0.5 (Zhou and Xue, 2012; Zhou and Xue, 2015), available from the LDC (http://ldc.upenn.edu), supplemented with additional annotated data from the Chinese TreeBank (Xue et al., 2005). The CDTB adopts the general annotation strategy of the PDTB, associating discourse relations with explicit or implicit discourse connectives and the two spans that serve as their arguments. In the case of explicit discourse relations (Example 1), there is an overt discourse connective, which may be realized syntactical"
K16-2001,P12-1008,1,0.873541,"ull system. This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments. 3 Data 3.1 Training and Development The training and development sets for English remain exactly the same as those used in the CoNLL-2015 shared task. Details regarding how the data was adapted from the Penn Discourse TreeBank 2.0 (PDTB 2.0) are provided in the overview paper of the CoNLL 2015 shared task (Xue et al., 2015). The Chinese training and development sets are taken from the Chinese Discourse TreeBank (CDTB) 0.5 (Zhou and Xue, 2012; Zhou and Xue, 2015), available from the LDC (http://ldc.upenn.edu), supplemented with additional annotated data from the Chinese TreeBank (Xue et al., 2005). The CDTB adopts the general annotation strategy of the PDTB, associating discourse relations with explicit or implicit discourse connectives and the two spans that serve as their arguments. In the case of explicit discourse relations (Example 1), there is an overt discourse connective, which may be realized syntactically as a subordinating or coordinating conjunction, or a discourse adverbial. Implicit discourse relations are cases wher"
L18-1231,W13-2322,1,0.869887,"esent analysis of some appealing characteristics of this final dataset, and present preliminary results of training and evaluating SRL systems on this combined set, to spur usage of this challenging new dataset. Keywords: Propbank,SRL, Semantic Roles, Corpora 1. Introduction We introduce the conversion of all existing Propbank data — constituting more than half a million predicate instances in English — into a format in which etymologically related senses from different parts are speech are merged, making that data compatible with the predicate senses used for Abstract Meaning Representation (Banarescu et al., 2013). This constitutes a large set of data made consistent to use the same frames and conventions, both increasing the amount of training data available for Propbank SRL and also providing a large corpus of semantic role labeling whose rolesets and numbered arguments match those of the Abstract Meaning Representation data (while containing over twice as many predicate instances). We describe the combination of automatic and manual methods used in converting these corpora, provide some analysis to characterize the resulting corpora, and present preliminary SRL results against the test sets presente"
L18-1231,bonial-etal-2014-propbank,1,0.854515,"esets used in AMR; while there is a large drop in verbal senses (notably due to the omission of semantically light predicates), it still remains a very large corpus. Work is ongoing adding more of the Propbank nominal and adjectival rolesets to AMR. 2.5. Expansion of multi-word predicate coverage More recent work has expanded coverage of the Propbank lexicon to encompass multi-word predicates as well, such as take with a grain of salt, cut slack, or jump on bandwagon. Propbank has long annotated certain classes of 1458 multi-word predicates such as verb-particle constructions and light verbs (Bonial et al., 2014), and is now expanded to arbitrarily structured semi-fixed expressions. We added coverage to many of the most high-frequency multi-word predicates in the corpus, and created lexical entries for each multi-word predicate. The important contribution is not simply the detection of these MWP elements (which can also be found in larger resources such as PARSEME (Savary et al., 2017)), but the annotation of semantic roles for each MWP. For example, something like “jump on the bandwagon” would be as follows: • jump-on-bandwagon.09: join an activity or group because of its popularity – Arg0: person ju"
L18-1231,W05-0620,0,0.0208076,"Missing"
L18-1231,W15-3904,0,0.033934,"Missing"
L18-1231,D15-1112,0,0.0253945,"Missing"
L18-1231,W10-1810,1,0.807463,"d Alu´ısio, 2011). Such Propbank semantic role labels are generally annotated by labeling individual phrases within a constituency parse (which can then be converted into surface forms (Carreras and M`arquez, 2005; Pradhan et al., 2011) or dependency parses (Surdeanu et al., 2008)), but AMR annotation is done by directly building a semantic graph for a sentence – utilizing Propbank senses and numbered arguments – without explicit linking of that graph to phrases within a sentence. While Propbank 1.0 (Kingsbury and Palmer, 2002) annotated only verbal predicates, it was later expanded to nouns (Hwang et al., 2010) and predicative adjectives (Bonial et al., 2017), creating new Propbank senses (called “rolesets”) for those nouns and adjectives. Parallel work in the Abstract Meaning Representation project also handled nouns and adjectives, but did so by representing them with etymologically related verbal rolesets – so that a noun such as “insertion” would not have its own rolesets, but would instead be labeled with a verbal sense of “insert”. This approach has a range of useful properties in reducing the number of senses with small amounts of training data, and also better conforms to the approach of Fra"
L18-1231,N06-2001,0,0.02169,"Missing"
L18-1231,N16-1030,0,0.0134305,"t to the first, concatenating outputs for each element (Graves et al., 2013). In our approach, and that of Zhou and Xu (2015), the result of the forward pass is used as input to the backward pass, enabling repeated stacking of these layers to form a deep topology. Instead of scoring each label locally, the addition of a CRF loss function allows for globally normalized scoring of all possible sequences of labels, maximizing the sequence-level loglikelihood (Collobert et al., 2011). This approach has been shown to improve performance on a variety of tasks (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). Following standard practices for applying neural architectures to NLP tasks, we initialize our network with word embeddings trained on orders of magnitude more data than is available for our task (SRL). Specifically, we use publicly available GloVe 100-dimensional vectors trained on 6 billion words from Wikipedia and Gigaword (Pennington et al., 2014). These embeddings are updated during training as network parameters along with a single outof-vocabulary (OOV) vector, which is randomly initial1460 ized. We simplify the features used in the original model of (Zhou and Xu, 2015), using only th"
L18-1231,P16-1101,0,0.0941005,"rom the last element to the first, concatenating outputs for each element (Graves et al., 2013). In our approach, and that of Zhou and Xu (2015), the result of the forward pass is used as input to the backward pass, enabling repeated stacking of these layers to form a deep topology. Instead of scoring each label locally, the addition of a CRF loss function allows for globally normalized scoring of all possible sequences of labels, maximizing the sequence-level loglikelihood (Collobert et al., 2011). This approach has been shown to improve performance on a variety of tasks (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016). Following standard practices for applying neural architectures to NLP tasks, we initialize our network with word embeddings trained on orders of magnitude more data than is available for our task (SRL). Specifically, we use publicly available GloVe 100-dimensional vectors trained on 6 billion words from Wikipedia and Gigaword (Pennington et al., 2014). These embeddings are updated during training as network parameters along with a single outof-vocabulary (OOV) vector, which is randomly initial1460 ized. We simplify the features used in the original model of (Zhou and Xu"
L18-1231,D14-1162,0,0.082664,"Missing"
L18-1231,W11-1901,1,0.223768,"predicatespecific arguments (following the “individual thematic roles” of Dowty (1991)), Propbank approaches can achieve a high inter-annotator agreement rate, and the methodology has been adapted to Chinese (Palmer et al., 2005), Korean (Palmer et al., 2006), Hindi/Urdu (Bhatt et al., 2009), Finnish (Haverinen et al., 2013), Turkish (Sahin, 2016) and Brazilian Portuguese (Duran and Alu´ısio, 2011). Such Propbank semantic role labels are generally annotated by labeling individual phrases within a constituency parse (which can then be converted into surface forms (Carreras and M`arquez, 2005; Pradhan et al., 2011) or dependency parses (Surdeanu et al., 2008)), but AMR annotation is done by directly building a semantic graph for a sentence – utilizing Propbank senses and numbered arguments – without explicit linking of that graph to phrases within a sentence. While Propbank 1.0 (Kingsbury and Palmer, 2002) annotated only verbal predicates, it was later expanded to nouns (Hwang et al., 2010) and predicative adjectives (Bonial et al., 2017), creating new Propbank senses (called “rolesets”) for those nouns and adjectives. Parallel work in the Abstract Meaning Representation project also handled nouns and a"
L18-1231,W17-1704,0,0.015137,"word predicates as well, such as take with a grain of salt, cut slack, or jump on bandwagon. Propbank has long annotated certain classes of 1458 multi-word predicates such as verb-particle constructions and light verbs (Bonial et al., 2014), and is now expanded to arbitrarily structured semi-fixed expressions. We added coverage to many of the most high-frequency multi-word predicates in the corpus, and created lexical entries for each multi-word predicate. The important contribution is not simply the detection of these MWP elements (which can also be found in larger resources such as PARSEME (Savary et al., 2017)), but the annotation of semantic roles for each MWP. For example, something like “jump on the bandwagon” would be as follows: • jump-on-bandwagon.09: join an activity or group because of its popularity – Arg0: person jumping on the bandwagon – Arg1: popular thing joined – Arg2: action done which gets one on the bandwagon This is a step forward in not simply detecting these MWPs, but being able to represent them in structured semantic representations such as AMR. 3. OntoNotes (ON) EWT BOLT ON+EWT+BOLT in AMR Subset Conll-2012 nouns (light v.) 40,163 (2,215) 9,453 (732) 18,839 (1973) 68,455 (49"
L18-1231,W08-2121,0,0.0687151,"Missing"
L18-1231,Q15-1003,0,0.022375,"Missing"
L18-1231,P15-2141,1,0.860405,"for specific semantic functions and the reification of semantic roles, such as INCLUDE -91 for set operations or HAVE - ORG - ROLE -91 for organizational membership. Other than those AMR rolesets ending in 91, the rolesets used within AMR are a subset of the rolesets used in Propbank, adopting nearly every verbal form in Propbank and most nominal and adjectival senses. The most common reason why a verbal roleset is not in AMR is because AMR deletes “semantically light” predicates, such as copular “be” or auxiliary “have”. As the use of existing SRL systems has been shown to help AMR parsing (Wang et al., 2015), we expect that the introduction of a larger SRL dataset with closer alignment to the AMR lexicon should increase that utility. The AMR SUBSET line in Table 1 illustrates the size of these corpora when limited to only the rolesets used in AMR; while there is a large drop in verbal senses (notably due to the omission of semantically light predicates), it still remains a very large corpus. Work is ongoing adding more of the Propbank nominal and adjectival rolesets to AMR. 2.5. Expansion of multi-word predicate coverage More recent work has expanded coverage of the Propbank lexicon to encompass"
L18-1231,P15-1109,0,0.0613342,"ow the coverage differs between each resource, by illustrating the kinds of nominal predicates unique to each annotation project. Propbank rolesets which overlap with FrameNet and NomBank (the left column) illustrate prototypical nominalizations. In contrast, nouns not represented in Propbank but captured in FrameNet or NomBank show their coverage over more traditional entities, objects and relational nouns. Preliminary Results Semantic Role Labeler Our SRL system (Gung and Pradhan, 2018) uses a deep neural network model which does not include explicit syntactic information. We closely follow Zhou and Xu (2015), treating SRL as an IOB tagging problem and using deep bidirectional LSTMs with a linear chain conditional random field (Lafferty et al., 2001) loss function. Long-short term memory networks (LSTMs) are a form of recurrent neural network (RNN) that has been successfully applied to many NLP tasks. Sequential inputs are often processed using pairs of RNNs, with one RNN processing from the first to last element and the other RNN processing from the last element to the first, concatenating outputs for each element (Graves et al., 2013). In our approach, and that of Zhou and Xu (2015), the result"
L18-1231,W09-3036,1,0.853669,"adigm for the development of semantic role labeling corpora, designed for large-scale annotation. It focuses upon annotation of coarse-grained senses (“rolesets”) which provide predicate-specific definitions of numbered arguments (ARG 0, ARG 1, etc.) to represent semantic roles. By using coarse-grained sense labels and these predicatespecific arguments (following the “individual thematic roles” of Dowty (1991)), Propbank approaches can achieve a high inter-annotator agreement rate, and the methodology has been adapted to Chinese (Palmer et al., 2005), Korean (Palmer et al., 2006), Hindi/Urdu (Bhatt et al., 2009), Finnish (Haverinen et al., 2013), Turkish (Sahin, 2016) and Brazilian Portuguese (Duran and Alu´ısio, 2011). Such Propbank semantic role labels are generally annotated by labeling individual phrases within a constituency parse (which can then be converted into surface forms (Carreras and M`arquez, 2005; Pradhan et al., 2011) or dependency parses (Surdeanu et al., 2008)), but AMR annotation is done by directly building a semantic graph for a sentence – utilizing Propbank senses and numbered arguments – without explicit linking of that graph to phrases within a sentence. While Propbank 1.0 (Ki"
L18-1231,W11-4519,0,0.562612,"Missing"
L18-1231,W13-5609,0,0.0231877,"Missing"
L18-1231,N06-2015,1,0.556117,"nglish. Light verbs are annotated using nominal frames (Hwang et al. 2010) and therefore included in those counts core data (Strassel and Tracey, 2016), image captions of the Flickr 8k corpus (Hodosh et al., 2013), MASC data (Ide et al., 2008), and Earth Science data affiliated with the ClearEarth project (Duerr et al., 2016). These additional corpora – as noted in Table 2 below – illustrate the range of domain-specific annotations of Propbank data which might be utilized for semantic role labeling in specific domains. Larger Landscape of Propbank Resources L ORELEI MASC The OntoNotes corpus (Hovy et al., 2006), most recently released as part of the Conll-2012 (Pradhan et al., 2011), was developed during the DARPA-GALE annotation project covering a wide range of domains, and has been the largest resource for training and evaluating semantic role labeling systems. However, the range of other corpora that have been annotated with Propbank roles since OntoNotes – most notably, the English Web Treebank and the BOLT corpora – collectively constitute an amount of additional predicate annotations the same size as OntoNotes itself. The English Web Treebank encompasses a range of genres of the web, such as r"
L18-1231,ide-etal-2008-masc,0,0.0285513,"tecting these MWPs, but being able to represent them in structured semantic representations such as AMR. 3. OntoNotes (ON) EWT BOLT ON+EWT+BOLT in AMR Subset Conll-2012 nouns (light v.) 40,163 (2,215) 9,453 (732) 18,839 (1973) 68,455 (4920) 63,585 (4714) 20,305 adjectives 750 3,305 10,957 15,012 10,121 0 Table 1: Core Corpora Annotated with Propbank rolesets for general English. Light verbs are annotated using nominal frames (Hwang et al. 2010) and therefore included in those counts core data (Strassel and Tracey, 2016), image captions of the Flickr 8k corpus (Hodosh et al., 2013), MASC data (Ide et al., 2008), and Earth Science data affiliated with the ClearEarth project (Duerr et al., 2016). These additional corpora – as noted in Table 2 below – illustrate the range of domain-specific annotations of Propbank data which might be utilized for semantic role labeling in specific domains. Larger Landscape of Propbank Resources L ORELEI MASC The OntoNotes corpus (Hovy et al., 2006), most recently released as part of the Conll-2012 (Pradhan et al., 2011), was developed during the DARPA-GALE annotation project covering a wide range of domains, and has been the largest resource for training and evaluating"
L18-1231,kingsbury-palmer-2002-treebank,1,0.783934,"d manual methods used in converting these corpora, provide some analysis to characterize the resulting corpora, and present preliminary SRL results against the test sets presented here, as a baseline for future evaluation of SRL. We suggest that evaluating against the combined test sets (OntoNotes, English Web Treebank and BOLT) can provide a challenging test set that could encourage the community to build SRL systems with a greater coverage over nominal, adjectival and light verb data, and with robustness to a range of difficult genres. 1.1. Motivations for Unifying Parts of Speech Propbank (Kingsbury and Palmer, 2002) is a paradigm for the development of semantic role labeling corpora, designed for large-scale annotation. It focuses upon annotation of coarse-grained senses (“rolesets”) which provide predicate-specific definitions of numbered arguments (ARG 0, ARG 1, etc.) to represent semantic roles. By using coarse-grained sense labels and these predicatespecific arguments (following the “individual thematic roles” of Dowty (1991)), Propbank approaches can achieve a high inter-annotator agreement rate, and the methodology has been adapted to Chinese (Palmer et al., 2005), Korean (Palmer et al., 2006), Hin"
L18-1231,W04-2705,0,0.383619,"Missing"
L18-1231,W05-0309,1,0.442672,"g Parts of Speech Propbank (Kingsbury and Palmer, 2002) is a paradigm for the development of semantic role labeling corpora, designed for large-scale annotation. It focuses upon annotation of coarse-grained senses (“rolesets”) which provide predicate-specific definitions of numbered arguments (ARG 0, ARG 1, etc.) to represent semantic roles. By using coarse-grained sense labels and these predicatespecific arguments (following the “individual thematic roles” of Dowty (1991)), Propbank approaches can achieve a high inter-annotator agreement rate, and the methodology has been adapted to Chinese (Palmer et al., 2005), Korean (Palmer et al., 2006), Hindi/Urdu (Bhatt et al., 2009), Finnish (Haverinen et al., 2013), Turkish (Sahin, 2016) and Brazilian Portuguese (Duran and Alu´ısio, 2011). Such Propbank semantic role labels are generally annotated by labeling individual phrases within a constituency parse (which can then be converted into surface forms (Carreras and M`arquez, 2005; Pradhan et al., 2011) or dependency parses (Surdeanu et al., 2008)), but AMR annotation is done by directly building a semantic graph for a sentence – utilizing Propbank senses and numbered arguments – without explicit linking of"
L18-1231,song-etal-2014-collecting,0,0.0170062,"ce for training and evaluating semantic role labeling systems. However, the range of other corpora that have been annotated with Propbank roles since OntoNotes – most notably, the English Web Treebank and the BOLT corpora – collectively constitute an amount of additional predicate annotations the same size as OntoNotes itself. The English Web Treebank encompasses a range of genres of the web, such as reviews and emails (Bies et al., 2012), and the BOLT datasets encompass informal corpora of English discussion forum data, SMS text, and translations of conversational data (Garland et al., 2012; Song et al., 2014). All of these resources are either currently released or in the process of being released, with stand-off SRL annotations available at propbank.github.io . We suggest that the combination of the test sets of the three major corpora provides a more interesting and challenging dataset against which to evaluate a semantic role labeling system. This is due to both the challenging informal domains (such as SMS messages and discussion forum posts) as well as to the increase in the coverage of nouns and adjectives in the data. Table 1 illustrates the size of these corpora, broken down by the parts o"
L18-1231,L16-1521,0,0.0139295,"ng joined – Arg2: action done which gets one on the bandwagon This is a step forward in not simply detecting these MWPs, but being able to represent them in structured semantic representations such as AMR. 3. OntoNotes (ON) EWT BOLT ON+EWT+BOLT in AMR Subset Conll-2012 nouns (light v.) 40,163 (2,215) 9,453 (732) 18,839 (1973) 68,455 (4920) 63,585 (4714) 20,305 adjectives 750 3,305 10,957 15,012 10,121 0 Table 1: Core Corpora Annotated with Propbank rolesets for general English. Light verbs are annotated using nominal frames (Hwang et al. 2010) and therefore included in those counts core data (Strassel and Tracey, 2016), image captions of the Flickr 8k corpus (Hodosh et al., 2013), MASC data (Ide et al., 2008), and Earth Science data affiliated with the ClearEarth project (Duerr et al., 2016). These additional corpora – as noted in Table 2 below – illustrate the range of domain-specific annotations of Propbank data which might be utilized for semantic role labeling in specific domains. Larger Landscape of Propbank Resources L ORELEI MASC The OntoNotes corpus (Hovy et al., 2006), most recently released as part of the Conll-2012 (Pradhan et al., 2011), was developed during the DARPA-GALE annotation project cov"
N04-1030,A00-2031,0,0.0077464,"Missing"
N04-1030,P01-1017,0,0.09119,"Missing"
N04-1030,W03-1006,0,0.234797,"Missing"
N04-1030,C92-3145,0,0.0180452,"Missing"
N04-1030,N03-2008,0,0.0468064,"Missing"
N04-1030,W03-1008,0,0.271799,"Missing"
N04-1030,P00-1065,0,0.0435523,"Missing"
N04-1030,J02-3001,0,0.853591,"Missing"
N04-1030,P02-1031,0,0.133044,"Missing"
N04-1030,N03-2009,1,0.474549,"Missing"
N04-1030,W00-0730,0,0.418325,"Missing"
N04-1030,N01-1025,0,0.381966,"Missing"
N04-1030,P98-2127,0,0.297893,"Missing"
N04-1030,H94-1020,0,0.017141,"Missing"
N04-1030,P03-1002,0,0.75586,"Missing"
N04-1030,J03-4003,0,\N,Missing
N04-1030,C98-2122,0,\N,Missing
N04-4036,P01-1017,0,0.047049,"Missing"
N04-4036,P00-1065,1,0.910949,"Missing"
N04-4036,J02-3001,1,0.584303,"Missing"
N04-4036,P02-1031,0,0.086694,"Missing"
N04-4036,N03-2009,1,0.830112,"Missing"
N04-4036,kingsbury-palmer-2002-treebank,0,0.0553927,"Missing"
N04-4036,P98-1013,0,0.180142,"Missing"
N04-4036,W00-0730,0,0.0872495,"Missing"
N04-4036,N01-1025,0,0.0931378,"Missing"
N04-4036,C98-1013,0,\N,Missing
N04-4036,J02-3004,0,\N,Missing
N04-4036,P03-1002,0,\N,Missing
N07-1070,boas-2002-bilingual,0,0.0319352,"Missing"
N07-1070,W05-0620,0,0.647894,"Missing"
N07-1070,P05-1022,0,0.0107265,"Missing"
N07-1070,N04-4008,0,0.0164569,"Missing"
N07-1070,J02-3001,0,0.908723,"language studies. The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus – sections F, G, K, L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 2 We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 Semantic Annotation and Corpora PropBank1 In the corpus (Palmer et al., 2005"
N07-1070,W01-0521,0,0.158443,"Missing"
N07-1070,W00-0730,0,0.0457092,"Corpus – sections F, G, K, L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 2 We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 Semantic Annotation and Corpora PropBank1 In the corpus (Palmer et al., 2005), predicate argument relations are marked for the verbs in the text. PropBank was constructed by assigning semantic ar"
N07-1070,N01-1025,0,0.0294945,"L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 2 We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 Semantic Annotation and Corpora PropBank1 In the corpus (Palmer et al., 2005), predicate argument relations are marked for the verbs in the text. PropBank was constructed by assigning semantic arguments to constituents of"
N07-1070,H94-1020,0,0.0142693,"constructed by assigning semantic arguments to constituents of the handcorrected Treebank parses. The arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT (usually the subject of a transitive verb) A RG 1 is the P ROTO -PATIENT (usually its direct object), etc. In addition to these C ORE A RGU MENTS , 16 additional A DJUNCTIVE A RGUMENTS , referred to as A RG Ms are also marked. More recently the PropBanking effort has been extended to encompass multiple corpora. In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1994) and part of the Brown portion of the Penn Treebank. The WSJ PropBank data comprise 24 sections of the WSJ, each section representing about 100 documents. PropBank release 1.0 contains about 114,000 predicates instantiating about 250,000 arguments and covering about 3,200 verb lemmas. Section 23, which is a standard test set and a test set in some of our experiments, comprises 5,400 predicates instantiating about 12,000 arguments. The Brown corpus is a Standard Corpus of American English that consists of about one million words of English text printed in the calendar year 1961 1 3 SRL System D"
N07-1070,N06-1020,0,0.0211819,"rather than gold standard ones. For this experiment, we used the same amount of training data from WSJ as available in the Brown training set – that is about 14,000 predicates. The examples from WSJ were selected randomly. The Brown test set is the same as used in the previous experiment, and the WSJ test set is the entire section 23. Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al., 2006a; McClosky et al., 2006b). The performance of these parsers as reported in the respective literature are shown in Table 6 shows the performance (as reported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Brown, When trained and tested on Brown, and when trained on WSJ and adapted with NANC. Train WSJ WSJ Brown WSJ+NANC Test WSJ Brown Brown Brown 2. F 91.0 85.2 88.4 87.9 Table 6: Charniak parser performance. We describe the results of Semantic Role Labeling under the following five conditions: 1. The SRL system is trained on feature"
N07-1070,P06-1043,0,0.0431706,"rather than gold standard ones. For this experiment, we used the same amount of training data from WSJ as available in the Brown training set – that is about 14,000 predicates. The examples from WSJ were selected randomly. The Brown test set is the same as used in the previous experiment, and the WSJ test set is the entire section 23. Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al., 2006a; McClosky et al., 2006b). The performance of these parsers as reported in the respective literature are shown in Table 6 shows the performance (as reported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Brown, When trained and tested on Brown, and when trained on WSJ and adapted with NANC. Train WSJ WSJ Brown WSJ+NANC Test WSJ Brown Brown Brown 2. F 91.0 85.2 88.4 87.9 Table 6: Charniak parser performance. We describe the results of Semantic Role Labeling under the following five conditions: 1. The SRL system is trained on feature"
N07-1070,C04-1100,0,0.0309553,"Missing"
N07-1070,J05-1004,0,0.830264,"7 Association for Computational Linguistics Surprisingly, it does not matter much whether the data is from another newswire, or a completely different type of text – as in the Brown corpus. These results indicate that the systems are being over-fit to the specific genre of text. Many performance improvements on the WSJ PropBank corpus may reflect tuning to the corpus. For the technology to be widely accepted and useful, it must be robust to change in genre of the data. Until recently, data tagged with similar semantic argument structure was not available for multiple genres of text. Recently, Palmer et al., (2005), have PropBanked a significant portion of the Treebanked Brown corpus which enables us to perform experiments to analyze the reasons behind the performance degradation, and suggest potential solutions. (Kuˇcera and Francis, 1967). The corpus contains about 500 samples of 2000+ words each. The idea behind creating this corpus was to create a heterogeneous sample of English text so that it would be useful for comparative language studies. The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus – sections F, G, K, L, M, N, P and R. Palmer et al"
N07-1070,N04-1030,1,0.620811,"he argument identification task. So far, most of the work on SRL systems has been focused on improving the labeling performance on a test set belonging to the same genre of text as the training set. Both the Treebank on which the syntactic parser is trained and the PropBank on which the SRL systems are trained represent articles from the year 1989 of the WSJ. While all these systems perform quite well on the WSJ test data, they show significant performance degradation (approximately 10 point drop in F-score) when applied to label test data that is different than the genre that WSJ represents (Pradhan et al., 2004; Carreras and M`arquez, 2005). 556 Proceedings of NAACL HLT 2007, pages 556–563, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Surprisingly, it does not matter much whether the data is from another newswire, or a completely different type of text – as in the Brown corpus. These results indicate that the systems are being over-fit to the specific genre of text. Many performance improvements on the WSJ PropBank corpus may reflect tuning to the corpus. For the technology to be widely accepted and useful, it must be robust to change in genre of the data. Until recent"
N07-1070,P05-1072,1,0.579955,"05. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 2 We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 Semantic Annotation and Corpora PropBank1 In the corpus (Palmer et al., 2005), predicate argument relations are marked for the verbs in the text. PropBank was constructed by assigning semantic arguments to constituents of the handcorrected Treebank parses. The arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT (usually the subject of a transitive verb) A RG 1 is the P ROTO -PATIENT (usually its direct object), etc. In addition to these C ORE A RGU MENTS"
N07-1070,P03-1002,0,0.0943289,"Missing"
N07-1070,A00-2018,0,\N,Missing
N07-1070,W04-2416,1,\N,Missing
N07-1070,J93-2004,0,\N,Missing
N07-1070,N06-2025,0,\N,Missing
N07-1070,H05-1081,0,\N,Missing
N07-1070,W03-1008,0,\N,Missing
N07-1070,W04-3212,0,\N,Missing
N07-1070,W05-0639,0,\N,Missing
N07-1070,W05-0623,0,\N,Missing
N07-1070,hockenmaier-steedman-2002-acquiring,0,\N,Missing
N07-1070,W05-0625,0,\N,Missing
N07-1070,P02-1043,0,\N,Missing
N07-1070,P98-2184,0,\N,Missing
N07-1070,C98-2179,0,\N,Missing
N07-1070,N06-2026,0,\N,Missing
N07-1070,P05-1073,0,\N,Missing
N07-1070,P98-2127,0,\N,Missing
N07-1070,C98-2122,0,\N,Missing
N07-1070,W04-2412,0,\N,Missing
N07-1070,W06-2303,0,\N,Missing
N15-1040,W13-2322,0,0.695346,"ntuitive and capture the regularities in the mapping between the dependency structure and the AMR of a sentence. Third, our parser runs in nearly linear time in practice in spite of a worst-case complexity of O(n2 ). 1 Sameer Pradhan Harvard Medical School Sameer.Pradhan@ childrens.harvard.edu want want-01 xcomp nsubj arrest police det to dobj ARG0 police prep aux pobj Karras Abstract Meaning Representation (AMR) is a rooted, directed, edge-labeled and leaf-labeled graph that is used to represent the meaning of a sentence. The AMR formalism has been used to annotate the AMR Annotation Corpus (Banarescu et al., 2013), a corpus of over 10 thousand sentences that is still undergoing expansion. The building blocks for an AMR representation are concepts and relations between them. Understanding these concepts and their relations is crucial to understanding the meaning of a sentence and could potentially benefit a number of natural language applications such (a) Dependency tree location Singapore name Singapore name nn Michael arrest-01 ARG1 person in The Introduction ARG1 ARG0 op1 “Michael” op2 “Karras” (b) AMR graph Figure 1: Dependency tree and AMR graph for the sentence, “The police want to arrest Micheal"
N15-1040,D12-1133,0,0.00852229,"and parsing state s. score(t, s) = ω ~ · φ(t, s) (1) where ω ~ is the weight vector and φ is a function that extracts the feature vector representation for one possible state-action pair ht, si. First, the algorithm initializes the state s with the sentence w and its dependency tree Dw . At each iteration, it gets all the possible actions for current state s (line 3). Then, it chooses the action with the highest score given by function score() and applies it to s (line 4-5). When the current state reaches a terminal state, the parser stops and returns the parsed graph. 371 As pointed out in (Bohnet and Nivre, 2012), constraints can be added to limit the number of possible actions to be evaluated at line 3. There could be formal constraints on states such as the constraint that the SWAP action should not be applied twice to the same pair of nodes. We could also apply soft constraints to filter out unlikely concept labels, relation labels and candidate nodes k for REATTACH and REENTRANCE. In our parser, we enforce the constraint that NEXT-NODE-lc can only choose from concept labels that co-occur with the current node’s lemma in the training data. We also empirically set the constraint that REATTACHk could"
N15-1040,P13-2131,0,0.556982,"Missing"
N15-1040,P13-1091,0,0.0291139,"oncept fragments obtained from the first stage. In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008), where a series of actions are performed to transform a sentence to a dependency tree. As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing. There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing. Existing work along this line is still theoretical in nature and no empirical results have been reported yet. 7 Conclusion and Future Work We presented a novel transition-based parsing algorithm that takes the dependency tree of a sentence as input and transforms it into an Abstract Meaning Representation graph through a sequence of actions. We show that our approach is linguistically intuitive and our experimental results also show that our parser outperformed the previous best reported results by a significant margin. In future work we p"
N15-1040,P04-1015,0,0.103759,"Missing"
N15-1040,W02-1001,0,0.237841,"at NEXT-NODE-lc can only choose from concept labels that co-occur with the current node’s lemma in the training data. We also empirically set the constraint that REATTACHk could only choose k among σ0 ’s grandparents and great grandparents. Additionally, REENTRANCEk could only choose k among its siblings. These constraints greatly reduce the search space, thus speeding up the parser. 4 Learning 4.1 Learning Algorithm As stated in section 3.2, the parameter of our model is weight vector ω ~ in the score function. To train the weight vector, we employ the averaged perceptron learning algorithm (Collins, 2002). Algorithm 2 Learning algorithm Input: sentence w = w0 . . . wn , Dw , Gw Output: ω ~ 1: s ← s0 (Dw , w) 2: while s ∈ / St do 3: T ← all possible actions according to s 4: bestT ← arg maxt∈T score(t, s) 5: goldT ← oracle(s, Gw ) 6: if bestT 6= goldT then 7: ω ~ ←ω ~ − φ(bestT, s) + φ(goldT, s) 8: end if 9: s ← apply goldT to s 10: end while For each sentence w and its corresponding AMR annotation GAM R in the training corpus, we could get the dependency tree Dw of w with a dependency parser. Then we represent GAM R as span graph Gw , which serves as our learning target. The learning algorithm"
N15-1040,P14-1134,0,0.815762,"in the sentence and the nodes in the dependency tree, AMR is an abstract representation where the word order of the corresponding sentence is not maintained. In addition, some words become abstract concepts or relations while other words are simply deleted because they do not contribute to meaning. The alignment between the word tokens and the concepts is non-trivial, but in order to learn the transition from a dependency tree to an AMR graph, we have to first establish the alignment between the word tokens in the sentence and the concepts in the AMR. We use the aligner that comes with JAMR (Flanigan et al., 2014) to produce this alignment. The JAMR aligner attempts to greedily align every concept or graph fragment in the AMR graph with a contiguous word token sequence in the sentence. s0,1 :ROOT want-01 ARG1 ARG0 police s3,4 :want-01 arrest-01 ARG0 ARG1 ARG1 ARG0 person s5,6 :arrest-01 ARG0 s2,3 :police name ARG1 name op1 “Micheal” (a) AMR graph op2 “Karras” s6,8 : person+name (b) Span graph Figure 2: AMR graph and its span graph for the sentence, “The police want to arrest Micheal Karras.” We use a data structure called span graph to represent an AMR graph that is aligned with the word tokens in a se"
N15-1040,P03-1054,0,0.012039,"Missing"
N15-1040,P14-5010,0,0.00390732,"Missing"
N15-1040,W03-3017,0,0.0532707,"tion. They treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For rela374 tion identification, they adopt the graph-based techniques for non-projective dependency parsing. Instead of finding maximum-scoring trees over words, they propose an algorithm to find the maximum spanning connected subgraph (MSCG) over concept fragments obtained from the first stage. In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008), where a series of actions are performed to transform a sentence to a dependency tree. As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing. There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing. Existing work along this line is still theoretical in nature and no empirical results have been reported yet. 7 Conclusion"
N15-1040,J08-4003,0,0.0520299,"set that our model predicts consists of both those from the concepts in the original AMR graph and those as a result of collapsing the AMR subgraphs. person name s6,8 :person+name name op1 op2 “Micheal” “Karras” Figure 3: Collapsed nodes Representing AMR graph this way allows us to formulate the AMR parsing problem as a joint learning problem where we can design a set of actions to simultaneously predict the concepts (nodes) and relations (arcs) in the AMR graph as well as the labels on them. 3 Transition-based AMR Parsing 3.1 Transition System Similar to transition-based dependency parsing (Nivre, 2008), we define a transition system for AMR parsing as a quadruple S = (S, T, s0 , St ), where • S is a set of parsing states (configurations). • T is a set of parsing actions (transitions), each of which is a function t : S → S. • s0 is an initialization function, mapping each input sentence w and its dependency tree D to an initial state. 368 • St ⊆ S is a set of terminal states. Each state (configuration) of our transition-based parser is a triple (σ, β, G). σ is a buffer that stores indices of the nodes which have not been processed and we write σ = σ0 |σ 0 to indicate that σ0 is the topmost e"
N15-1040,P09-1040,0,0.0284183,"Missing"
N15-1040,C08-1095,0,0.186364,"eat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For rela374 tion identification, they adopt the graph-based techniques for non-projective dependency parsing. Instead of finding maximum-scoring trees over words, they propose an algorithm to find the maximum spanning connected subgraph (MSCG) over concept fragments obtained from the first stage. In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008), where a series of actions are performed to transform a sentence to a dependency tree. As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing. There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing. Existing work along this line is still theoretical in nature and no empirical results have been reported yet. 7 Conclusion and Future Work We prese"
N15-1040,P13-1014,0,0.0521965,"r set to null if σ0 is a leaf node. G is initialized with all the nodes and edges of D. Initially, all the nodes of G have a span length of one and all the labels for nodes and edges are set to null. As the parsing procedure goes on, the parser will process all the nodes and their outgoing edges in dependency tree D in a bottom-up left-right manner, and at each state certain action will be applied to the current node or edge. The parsing process will terminate when both σ and β are empty. The most important part of the transition-based parser is the set of actions (transitions). As stated in (Sartorio et al., 2013), the design space of possible actions is actually infinite since the set of parsing states is infinite. However, if the problem is amenable to transition-based parsing, we can design a finite set of actions by categorizing all the possible situations we run into in the parsing process. In §5.2 we show this is the case here and our action set can account for almost all the transformations from dependency trees to AMR graphs. We define 8 types of actions for the actions set T , which is summarized in Table 1. The action set could be divided into two categories based on conditions of buffer β. W"
N15-1040,W03-3023,0,0.212539,"tion and relation identification. They treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For rela374 tion identification, they adopt the graph-based techniques for non-projective dependency parsing. Instead of finding maximum-scoring trees over words, they propose an algorithm to find the maximum spanning connected subgraph (MSCG) over concept fragments obtained from the first stage. In contrast, we adopt a transition-based approach that finds its root in transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2003; Sagae and Tsujii, 2008), where a series of actions are performed to transform a sentence to a dependency tree. As should be clear from our description, however, the actions in our parser are very different in nature from the actions used in transition-based dependency parsing. There is also another line of research that attempts to design graph grammars such as hyperedge replacement grammar (HRG) (Chiang et al., 2013) and efficient graph-based algorithms for AMR parsing. Existing work along this line is still theoretical in nature and no empirical results have been reported yet."
N15-1040,N07-1050,0,\N,Missing
P05-1072,P00-1065,1,0.378562,"Missing"
P05-1072,J02-3001,1,0.885159,"/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common a"
P05-1072,P02-1031,0,0.536652,"Missing"
P05-1072,C04-1186,1,0.578802,"man, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to"
P05-1072,N03-2009,1,0.791276,"Missing"
P05-1072,W04-2416,1,0.736909,"Missing"
P05-1072,hockenmaier-steedman-2002-acquiring,0,0.0421125,"organized as follows. We first describe a baseline system based on the best published techniques. We then report on two sets of experiments using techniques that improve performance on the problem of finding arguments when they are present in the syntactic analysis. In the first set of experiments we explore new 581 Proceedings of the 43rd Annual Meeting of the ACL, pages 581–588, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics S features, including features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu,"
P05-1072,kingsbury-palmer-2002-treebank,0,0.0227165,"lities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR"
P05-1072,W00-0730,0,0.0185823,"tion. Table 9: Head-word based performance using Charniak and Minipar parses. 5.2 Chunk-based Semantic Labeler Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al., 2004). This system uses SVM classifiers to first chunk input text into flat chunks or base phrases, each labeled with a syntactic tag. A second SVM is trained to assign semantic labels to the chunks. The system is trained 586 SVMs were trained for begin (B) and inside (I) classes of all arguments and outside (O) class for a total of 78 one-vs-all classifiers. Again, TinySVM5 along with YamCha6 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used as the SVM training and test software. Table 11 presents the system performances on the PropBank test set for the chunk-based system. 5 6 http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 6 Combining Semantic Labelers We combined the semantic parses as follows: i) scores for arguments were converted to calibrated probabilities, and arguments with scores below a threshold value were deleted. Separate thresholds were used for each parser. ii) For the remaining arguments, the more probable ones among overlapping ones were selec"
P05-1072,N01-1025,0,0.456011,"lti-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1 2 3 http://www.cis.upenn.edu/˜ace/ http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or afte"
P05-1072,H94-1020,0,0.0271304,"ntic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for de"
P05-1072,J05-1004,0,0.248645,"periments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Fig"
P05-1072,N04-1030,1,0.905723,"O -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for development and Section 23 for testing. We formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1 2 3 http://www.cis.upenn.edu/˜ace/ http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers"
P05-1072,P03-1002,0,0.108903,"tion ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the predicate and the constituent. V ERB S ENSE I NFORMATI"
P05-1072,W04-3212,0,0.685533,"M−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the predicate and the constituent. V ERB S ENSE I NFORMATION: Oracle verb sense information from PropBank H EAD W ORD OF PP:"
P05-1072,A00-2018,0,0.16996,"luding features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected Tr"
P05-1072,W03-1006,0,0.252892,"Missing"
P05-1072,W03-1008,0,\N,Missing
P05-1072,P02-1043,0,\N,Missing
P14-2005,N06-2015,1,0.757794,"45.93 36.45 61.72 55.12 56.01 62.33 36.18 39.79 62.94 40.76 37.18 BLANC 58.75 55.04 55.42 53.86 52.87 52.65 54.42 52.11 46.47 50.44 46.04 45.10 34.80 36.54 31.85 40 ● ● ● 10 20 ●●●● ● ●● ● ● ● ●● ● ●● ● ● ● ● ● MUC ● 30 40 50 60 70 CEAF−m 0 10 ● 20 30 40 50 CEAF−e Figure 1: Correlation plot between the proposed BLANC and the other measures based on the CoNLL 2011/2012 results. All values are F1 scores. other measures along R, P and F1 (Table 3), showing that BLANC is able to capture most entity-based similarities measured by B-cubed and CEAF. However, the CoNLL data sets come from OntoNotes (Hovy et al., 2006), where singleton entities are not annotated, and BLANC has a wider dynamic range on data sets with singletons (Recasens and Hovy, 2011). So the correlations will likely be lower on data sets with singleton entities. 6 Conclusion The original BLANC-gold (Recasens and Hovy, 2011) requires that system mentions be identical to gold mentions, which limits the metric’s utility since detected system mentions often have missing key mentions or spurious mentions. The proposed BLANC is free from this assumption, and we have shown that it subsumes the original BLANCgold. Since BLANC works on imperfect s"
P14-2005,H05-1004,1,0.892092,"m mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data. 1 Introduction Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity. It entails partitioning (often imperfect) mentions into equivalence classes. A critically important problem is how to measure the quality of a coreference resolution system. Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANCgold (Recasens and Hovy, 2011). B-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based. In particular, MUC measures the degree of agreement between key coreference links (i.e., links among mentions within entities) and response coreference links, while non-coreference links (i.e., links formed by mentions from different entities) are not explicitly taken into account. This leads to a phenomenon where coreference systems ou"
P14-2005,W11-1901,1,0.88997,"cd), (ce)}. Therefore, Ck ∩ Cr = {(bc)}, Nk ∩ Nr = {(bd), (cd)}, and Rc = 13 , Pc = 12 , Fc = 25 ; Rn = 2 2 4 17 3 , Pn = 4 , Fn = 7 . Finally, BLANC = 35 . Example 2. Key entity is {a}; response entity is {b}. This is boundary case (1): BLANC = 0. Example 3. Key entities are {a}{b}{c}; response entities are {a}{b}{d}. This is boundary case (2): there are no coreference links. Since Nk = {(ab), (bc), (ca)}, Results 5.1 CoNLL-2011/12 We have updated the publicly available CoNLL coreference scorer1 with the proposed BLANC, and used it to compute the proposed BLANC scores for all the CoNLL 2011 (Pradhan et al., 2011) and 2012 (Pradhan et al., 2012) participants in the official track, where participants had to automatically predict the mentions. Tables 1 and 2 report the updated results.2 5.2 Correlation with Other Measures Figure 1 shows how the proposed BLANC measure works when compared with existing metrics such as MUC, B-cubed and CEAF, using the BLANC and F1 scores. The proposed BLANC is highly positively correlated with the 1 http://code.google.com/p/reference-coreference-scorers The order is kept the same as in Pradhan et al. (2011) and Pradhan et al. (2012) for easy comparison. 2 27 30 R P F1 0.975"
P14-2005,W12-4501,1,0.869863,"{(bc)}, Nk ∩ Nr = {(bd), (cd)}, and Rc = 13 , Pc = 12 , Fc = 25 ; Rn = 2 2 4 17 3 , Pn = 4 , Fn = 7 . Finally, BLANC = 35 . Example 2. Key entity is {a}; response entity is {b}. This is boundary case (1): BLANC = 0. Example 3. Key entities are {a}{b}{c}; response entities are {a}{b}{d}. This is boundary case (2): there are no coreference links. Since Nk = {(ab), (bc), (ca)}, Results 5.1 CoNLL-2011/12 We have updated the publicly available CoNLL coreference scorer1 with the proposed BLANC, and used it to compute the proposed BLANC scores for all the CoNLL 2011 (Pradhan et al., 2011) and 2012 (Pradhan et al., 2012) participants in the official track, where participants had to automatically predict the mentions. Tables 1 and 2 report the updated results.2 5.2 Correlation with Other Measures Figure 1 shows how the proposed BLANC measure works when compared with existing metrics such as MUC, B-cubed and CEAF, using the BLANC and F1 scores. The proposed BLANC is highly positively correlated with the 1 http://code.google.com/p/reference-coreference-scorers The order is kept the same as in Pradhan et al. (2011) and Pradhan et al. (2012) for easy comparison. 2 27 30 R P F1 0.975 0.981 0.941 0.797 0.844 0.942 0"
P14-2005,P09-1074,0,0.0603219,"ll, precision and F-measure separately on coreference and noncoreference links in the usual way, and defines the overall recall, precision and F-measure as the mean of the respective measures for coreference and non-coreference links. The BLANC-gold metric was developed with the assumption that response mentions and key mentions are identical. In reality, however, mentions need to be detected from natural language text and the result is, more often than not, imperfect: some key mentions may be missing in the response, and some response mentions may be spurious—so-called “twinless” mentions by Stoyanov et al. (2009). Therefore, the identicalmention-set assumption limits BLANC-gold’s applicability when gold mentions are not available, or when one wants to have a single score measuring both the quality of mention detection and coreference resolution. The goal of this paper is to extend the BLANC-gold metric to imperfect response mentions. We first briefly review the original definition of BLANC, and rewrite its definition using set notation. We then argue that the gold-mention assumption in Recasens and Hovy (2011) can be lifted without changing the original definition. In fact, the proposed BLANC metric s"
P14-2005,M95-1005,0,0.704928,"oposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data. 1 Introduction Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity. It entails partitioning (often imperfect) mentions into equivalence classes. A critically important problem is how to measure the quality of a coreference resolution system. Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANCgold (Recasens and Hovy, 2011). B-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based. In particular, MUC measures the degree of agreement between key coreference links (i.e., links among mentions within entities) and response coreference links, while non-coreference links (i.e., links formed by mentions from different entities) are not explicitly taken into account. Th"
P14-2006,W12-4501,1,0.648168,"ity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations"
P14-2006,W10-4305,1,0.648818,"ainst them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted, as opposed to key (or gold) mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially h"
P14-2006,D09-1101,1,0.646014,"e mapping between the key and predicted mentions, assuming that the original measures cannot be applied to predicted mentions. Below we first provide an overview of these variations and then discuss the unnecessity of this assumption. Coining the term twinless mentions for those mentions that are either spurious or missing from the predicted mention set, Stoyanov et al. (2009) proposed two variations to B3 — B3all and B30 —to handle them. In the first variation, all predicted twinless mentions are retained, whereas the latter discards them and penalizes recall for twinless predicted mentions. Rahman and Ng (2009) proposed another variation by removing “all and only those twinless system mentions that are singletons before applying B3 and CEAF.” Following upon this line of research, Cai and Strube (2010) proposed a unified solution for both B3 and CEAF m , leaving the question of handling CEAF e as future work because “it produces unintuitive results.” The essence of their solution involves manipulating twinless key and predicted mentions by adding them either from the predicted partition to the key partition or vice versa, depending on whether one is computing precision or recall. The Cai and Strube ("
P14-2006,I13-1193,1,0.853507,"very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recas"
P14-2006,P11-1082,1,0.341706,"he OntoNotes corpus, and by the i2b2 2011 shared task on coreference resolution using an assortment of clinical notes corpora (Uzuner et al., 2012).1 It was later identified by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task. However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B3 metric.2 In a latter paper, Rahman and Ng (2011) correctly state that “CEAF can compare partitions with twinless mentions without any modification.” We will look at this further in Section 4.3. We argue that manipulations of key and response mentions/entities, as is done in the existing B3 variations, not only confound the evaluation process, but are also subject to abuse and can seriously jeopardize the fidelity of the evaluVariations of Scoring Measures Two commonly used coreference scoring metrics —B3 and CEAF—are underspecified in their application for scoring predicted, as opposed to key mentions. The examples in the papers describing"
P14-2006,W09-2411,1,0.899331,"Missing"
P14-2006,doddington-etal-2004-automatic,0,0.0166341,"ith this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the BLANC metric for partitions of predicted mentions. Different interpretations as to how to compute B3 and CEAF scores for coreference systems when predicted mentions do not perfectly align with key mentions—which is usually the case— led to variations of these metrics that manipulate the gold st"
P14-2006,N13-1071,1,0.782346,"Missing"
P14-2006,P09-1074,0,0.498213,"arnegie Mellon University, Pittsburgh, PA 5 HLTRI, University of Texas at Dallas, Richardson, TX, 6 HITS, Heidelberg, Germany 3 sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com, hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org Abstract purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et"
P14-2006,N13-2001,0,0.0493029,"pora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 200"
P14-2006,Q14-1012,1,0.042996,"Missing"
P14-2006,P14-2005,1,0.929886,"ne-to-one mention mapping (Stoyanov et al., 2009; Cai and Strube, 2010). Some of these variations arguably produce rather unintuitive results, while others are not faithful to the original measures. In this paper, we address the issues in scoring coreference partitions of predicted mentions. Specifically, we justify our decision to go back to the original scoring algorithms by arguing that manipulation of key or predicted mentions is unnecessary and could in fact produce unintuitive results. We demonstrate the use of our recent extension of BLANC that can seamlessly handle predicted mentions (Luo et al., 2014). We make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures that do not involve mention manipulation and is faithful to the original intentions of the proposers of these metrics. We republish the CoNLL-2011/2012 results based on this scorer, so that future systems can use it for evaluation and have the CoNLL results available for comparison. The rest of the paper is organized as follows. Section 2 provides an overview of the variations of the existing measures. We present our newly updated coreference scoring package in Section 3 t"
P14-2006,M95-1005,0,0.887578,"proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified"
P14-2006,H05-1004,1,0.706057,"re informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted, as opposed to key (or"
P14-2006,W11-1901,1,0.482275,"al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setbac"
P14-2006,J00-4006,0,\N,Missing
P14-2006,D08-1067,1,\N,Missing
P14-2006,S10-1001,1,\N,Missing
P14-2014,S07-1025,1,0.791506,"cat] recording the index of a neighbor node. VP 2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and ba"
P14-2014,S13-2002,1,0.913047,"by taking advantage of relatively strict English word ordering. Like SST and PTK, the DPK requires the root category of two subtrees to be the same for the similarity to be greater than zero. Unlike SST and PTK, once the root category comparison is successfully completed, DPK looks at all paths that go through it and accumulates their similarity scores independent of ordering – in other words, it will ignore the ordering of the children in its pro4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the C ONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: KC (s1 , s2 ) = τ P X p=1 KT (tp1 , tp2 )+KF (f1 , f2 ) (2) where si is an instance object composed of flat features fi and a syntactic tree ti . A syntactic tree ti 84 can have multiple representations, as in Bag Tree"
P14-2014,strassel-etal-2010-darpa,0,0.0132148,"sentations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal relation discovery performance on THYME data2 (Styler et al., 2014). In this paper, we also use syntactic structure-enriched temporal relation discovery as a vehicle to test our proposed kernel. here Figure 1: Three example tree pairs. matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates"
P14-2014,S13-2012,0,0.0248395,"2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between"
P14-2014,P07-1026,0,0.0305882,"discovery as a vehicle to test our proposed kernel. here Figure 1: Three example tree pairs. matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates a very large but sparse feature space. To Figure 1(b), the PTK generates fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a] [NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among others, for the second tree. This allows for partial matching – substructure (ii) – while also generating some fragments that violate grammatical intuitions. Zhang et al. (2007) address the restrictiveness of SST by allowing soft matching of production rules. They allow partial matching of optional nodes based on the Treebank. For example, the rule N P → DT JJ N N indicates a noun phrase consisting of a determiner, adjective, and common noun. Zhang et al.’s method designates the JJ as optional, since the Treebank contains instances of a reduced version of the rule without the JJ node (N P → DT N N ). They also allow node matching among similar preterminals such as JJ, JJR, and JJS, mapping them to one equivalence class. Other relevant approaches are the spectrum tree"
P14-2014,E12-1019,0,0.0156425,"m events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal relation discovery performance on THYME data2 (Styler et al., 2014). In this paper, we also use syntactic structure-enric"
P14-2014,S10-1063,0,0.0247263,"f a neighbor node. VP 2.2 VP RB l=1: [NP-DT],[NP-NN], Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). S c) DT NP DT ADVP l=0: [NP],[DT],[NN] NP PRP VBZ ADVP she comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech ta"
P14-2014,W13-1903,1,0.87219,"thinking of it as cheaply calculating rule production similarity by taking advantage of relatively strict English word ordering. Like SST and PTK, the DPK requires the root category of two subtrees to be the same for the similarity to be greater than zero. Unlike SST and PTK, once the root category comparison is successfully completed, DPK looks at all paths that go through it and accumulates their similarity scores independent of ordering – in other words, it will ignore the ordering of the children in its pro4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the C ONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: KC (s1 , s2 ) = τ P X p=1 KT (tp1 , tp2 )+KF (f1 , f2 ) (2) where si is an instance object composed of flat features fi and a syntactic tree ti . A syntactic tre"
P14-2014,Y09-1038,0,0.0260023,"comes RB Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1 . The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al."
P15-2141,W13-2322,0,0.506583,"Missing"
P15-2141,P13-2131,0,0.239952,"arger and more diverse dataset (C HARNIAK (ON)) yields the best overall AMR parsing performance. Subsequent experiments are all based on this version of the Charniak parser. We first tune and evaluate our system on the newswire section of LDC2013E117 dataset. Then we show our parser’s performance on the recent LDC2014T12 dataset. 4.1 Experiments on LDC2013E117 We first conduct our experiments on the newswire section of AMR annotation corpus (LDC2013E117). The train/dev/test split of dataset is 4.0K/2.1K/2.1K, which is identical to the settings of JAMR. We evaluate our parser with Smatch v2.0 (Cai and Knight, 2013) on all the experiments. System Charniak (ON) Charniak Stanford Malt Turbo P .67 .66 .64 .65 .65 R .64 .62 .62 .61 .61 4.1.2 In Table 2 we present results from extending the transition-based AMR parser. All experiments are conducted on the development set. From Table 2, we can see that the I NFER action yields a 4 point improvement in F1 score over the C HAR NIAK (ON) system. Adding Brown clusters improves the recall by 1 point, but the F1 score remains unchanged. Adding semantic role features on top of the Brown clusters leads to an improvement of another 2 points in F1 score, and gives us th"
P15-2141,P05-1022,0,0.037047,"aluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for C HARNIAK (ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few 1 Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xi"
P15-2141,W02-1001,0,0.150911,"coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding the new action significantly improves the parser performance, and (iii) semantic role information is beneficial to AMR parsing when used as features, while the Brown clusters do not make a difference and coreference information slightly hurts the AMR parsing performance. The rest of the paper is"
P15-2141,P14-1134,0,0.446603,"atures actually slightly hurts the performance. F1 .65 .64 .63 .63 .63 Table 1: AMR parsing performance on development set using different syntactic parsers. System Charniak (ON) +I NFER +I NFER+BROWN +I NFER+BROWN+SRL +I NFER+BROWN+SRL+COREF P .67 .71 .71 .72 .72 R .64 .67 .68 .69 .69 Impact of parser extensions F1 .65 .69 .69 .71 .70 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trai"
P15-2141,N04-1030,1,0.857032,"er et al., 2005), and it would seem that information produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding"
P15-2141,N06-2015,0,0.0238792,"er (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for C HARNIAK (ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few 1 Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xinhua newswirte) from the training data while retraining the Charniak parser, ASSERT semantic role labeler, and IMS frameset disambiguation tool). The full list of overlapping documents is available at http://cemantix.org/ontonotes/ontonotesamr-document-overlap.txt 860 System Our system JAMR (GitHub)2 JAMR (Flanigan et al., 2014) Stanford SHRG-based Wang"
P15-2141,W13-3516,1,0.458104,"Missing"
P15-2141,P08-1068,0,0.0291106,"coreference feature and semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For action that predicts the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by the semantic role labeler. For example, for partial graph in Figure 4, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is equal to node wants’s frameset want-01 as predicted by the semantic role labeler, the value of feature EQ FRAMESET is set to true. 2) IS ARGUMENT ."
P15-2141,P10-1040,0,0.182545,"ure 4: An example of coreference feature and semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For action that predicts the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by the semantic role labeler. For example, for partial graph in Figure 4, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is equal to node wants’s frameset want-01 as predicted by the semantic role labeler, the value of feature EQ FRAMESET is set to true"
P15-2141,J13-4004,0,0.0491347,"mation produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding the new action significantly improves the"
P15-2141,N15-1040,1,0.806,"igan et al. (2014) treat concept identification as a sequence labeling task and utilize a semiMarkov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. A competitive alternative to the MSCG approach is transition-based AMR parsing. Our previous work (Wang et al., 2015) describes a transition-based system that also involves two stages. In the first step, an input sentence is country ARG0-of name have-org-role-91 name ARG1 1 ARG1 person ARG2 op1 “South” op2 “Korea” minister name mod op1 foreign “Israel” Figure 1: An example showing abstract concept have-org-role-91 for the sentence “Israel foreign minister visits South Korea.” Unlike a dependency parse where each leaf node corresponds to a word in a sentence and there is an inherent alignment between the words in a sentence and the leaf nodes in the parse tree, the alignment between the word tokens in a sente"
P15-2141,P14-5010,0,0.0057335,".71 .70 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for C HARNIAK (ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few 1 Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We exc"
P15-2141,P15-1095,0,0.18983,"1 .65 .64 .63 .63 .63 Table 1: AMR parsing performance on development set using different syntactic parsers. System Charniak (ON) +I NFER +I NFER+BROWN +I NFER+BROWN+SRL +I NFER+BROWN+SRL+COREF P .67 .71 .71 .72 .72 R .64 .67 .68 .69 .69 Impact of parser extensions F1 .65 .69 .69 .71 .70 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank,"
P15-2141,P10-4014,0,0.0220356,"igure 3: I NFER -have-org-role-91 action 3.2 For action REENTRANCEbelieve -ARG1 SHARE DEPENDENT : true DEPENDENT LABEL : dobj Figure 4: An example of coreference feature and semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For action that predicts the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by the semantic role labeler. For example, for partial graph in Figure 4, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is"
P15-2141,P13-2109,0,0.010285,"f JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for C HARNIAK (ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few 1 Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xinhua newswirte) from the training data while retraining the Charniak parser, ASSERT semantic role labeler, and IMS frameset disambiguation tool). The full list of overlapping documents is"
P15-2141,H05-1066,0,0.0984606,"Missing"
P15-2141,P81-1022,0,0.772079,"Missing"
P15-2141,J05-1004,0,0.15216,"concepts, and existing AMR parsers do not have a systematic way of inferring such abstract concepts. 857 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 857–862, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Current AMR parsers are in their early stages of development, and their features are not yet fully developed. For example, the AMR makes heavy use of the framesets and semantic role labels used in the Proposition Bank (Palmer et al., 2005), and it would seem that information produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al.,"
P15-2141,nivre-etal-2006-maltparser,0,\N,Missing
P15-2141,K15-1004,0,\N,Missing
P16-4021,J05-1004,0,0.0315562,"hly 1.4 million words of text. We are in the process of cleaning up about the same amount of collected data for future distribution. 7 Future Work In the near future we plan to evaluate applying a statistical labeler trained on existing corpora to the task of Role assignment. This approach should provide increased robustness to novel input and substantially reduce the human annotation effort required to attain a given level of coverage. The 1 125 http://corpora.boulderlearning.com/myst Proposition Bank (PropBank) provides a corpus of sentences annotated with domain-independent semantic roles (Palmer et al., 2005). PropBank has been widely used for the development of machine learning based Semantic Role Labeling (SRL) systems. Pradhan et al. (2005) used a rich set of syntactic and semantic features to obtain a performance with F-score in the low-80s. It has been an integral component of most question answering systems for the past decade. Since its first application to the newswire text, PropBank has been extended to cover many more predicates and diverse genres in the DARPA OntoNotes project (Weischedel et al., 2011; Pradhan et al., 2013) and the DARPA BOLT program. We plan to map PropBank SRL output"
P16-4021,W13-3516,1,0.896896,"Missing"
Q14-1012,J86-2003,0,0.0356834,"l subdomain. 3 Interpreting ‘Event’ and Temporal Expressions in the Clinical Domain Much prior work has been done on standardizing the annotation of events and temporal expressions in text. The most widely used approach is the ISOTimeML specification (Pustejovsky et al., 2010), an ISO standard that provides a common framework for annotating and analyzing time, events, and event relations. As defined by ISO-TimeML, an E VENT refers to anything that can be said “to obtain or hold true, to happen or to occur”. This is a broad notion of event, consistent with Bach’s use of the term “eventuality” (Bach, 1986) as well as the notion of fluents in AI (McCarthy, 2002). Because the goals of the THYME project involve automatically identifying the clinical timeline for a patient from clincal records, the scope of what should be admitted into the domain of events is interpreted more broadly than in ISO-TimeML3 . Within the THYME-TimeML guideline, an E VENT is anything relevant to the clinical timeline, i.e., anything that would show up on a detailed timeline of the patient’s care or life. The best single-word syntactic head for the E VENT is then used as its span. For example, a diagnosis would certainly"
Q14-1012,S13-2002,1,0.783065,"rk A BEFORE C, then an equivalent inferred TLINK would be used to match it. E VENT and T IMEX 3 IAA was generated based on exact and overlapping spans, respectively. These results are reported in Table 3. The THYME corpus also differs from ISOTimeML in terms of E VENT properties, with the addition of DocTimeRel, ContextualModality and ContextualAspect. IAA for these properties is in Table 4. 7.3 Baseline Systems To get an idea of how much work will be necessary to adapt existing temporal information extraction systems to the clinical domain, we took the freely available ClearTK-TimeML system (Bethard, 2013), 151 which was among the top performing systems in TempEval 2013 (UzZaman et al., 2013), and evaluated its performance on the THYME corpus. ClearTK-TimeML uses support vector machine classifiers trained on the TempEval 2013 training data, employing a small set of features including character patterns, tokens, stems, part-of-speech tags, nearby nodes in the constituency tree, and a small time word gazetteer. For E VENTs and T IMEX 3s, the ClearTK-TimeML system could be applied directly to the THYME corpus. For DocTimeRels, the relation for an E VENT was taken from the TLINK between that E VENT"
Q14-1012,W13-1903,1,0.813475,"ation for these events, (3) the interaction of general and domain-specific events and their importance in the final timeline, and, more generally, (4) the importance of rough temporality and narrative containers as a step towards finer-grained timelines. We have several avenues of ongoing and future work. First, we are working to demonstrate the utility of the THYME corpus for training machine learning models. We have designed support vector machine models with constituency tree kernels that were able to reach an F1-score of 0.737 on an E VENT-T IMEX 3 narrative container identification task (Miller et al., 2013), and we are working on training models to identify events, times and the remaining types of temporal relations. Second, as per our motivating use cases, we are working to integrate this annotation data with timeline visualization tools and to use these annotations in quality-of-care research. For example, we are using temporal reasoning built on this work to investigate the liver toxicity of methotrexate across a large corpus of EHRs (Lin et al., under review)]. Finally, we plan to explore the application of our notion of an event (anything that should be visible on a domain-appropriate timel"
Q14-1012,miltsakaki-etal-2004-penn,0,0.0158666,"h the number of events and times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force"
Q14-1012,W04-0210,0,0.0401525,"d times, and the task quickly becomes unmanageable. There are, however, strategies that we can adopt to make this labeling task more tractable. Temporal ordering relations in text are of three kinds: 1. Relations between two events 2. Relations between two times 148 3. Relations between a time and an event. ISO-TimeML, as a formal specification of the temporal information conveyed in language, makes no distinction between these ordering types. Humans, however, do make distinctions, based on local temporal markers and the discourse relations established in a narrative (Miltsakaki et al., 2004; Poesio, 2004). Because of the difficulty of humans capturing every relationship present in the note (and the disagreement which arises when annotators attempt to do so), it is vital that the annotation guidelines describe an approach that reduces the number of relations that must be considered, but still results in maximally informative temporal links. We have found that many of the weaknesses in prior annotation approaches stem from interaction between two competing goals: • The guideline should specify certain types of annotations that should be performed; • The guideline should not force annotations to"
Q14-1012,W11-0419,1,0.93622,"s, as well as to develop state-of-the-art algointerventions and diagnostics which have been thus rithms to train and test on this dataset. far attempted. In other sections, the doctor may outDeriving timelines from news text requires the conline her current plan for the patient’s treatment, then crete realization of context-dependent assumptions later describe the patient’s specific medical history, about temporal intervals, orderings and organization, allergies, care directives, and so forth. underlying the explicit signals marked in the text Most critically for temporal reasoning, each clin(Pustejovsky and Stubbs, 2011). Deriving patient ical note reflects a single time in the patient’s treathistory timelines from clinical notes also involves ment history at which all of the doctor’s statements these types of assumptions, but there are special deare accurate (the D OCTIME), and each section tends mands imposed by the characteristics of the clinical to describe events of a particular timeframe. For narrative. Due to both medical shorthand practices example, ‘History of Present illness’ predominantly and general domain knowledge, many event-event describes events occuring before D OCTIME, whereas relations are"
Q14-1012,pustejovsky-etal-2010-iso,1,0.785939,"e can 2 The Nature of Clinical Documents be both domain-specific and complex, and are often we have been examining left implicit, requiring significant domain knowledge In the THYME corpus, 1 notes from a large healthcare 1,254 de-identified to accurately detect and interpret. In this paper, we discuss the demands on accurately practice (the Mayo Clinic), representing two distinct annotating such temporal information in clinical fields within oncology: brain cancer, and colon cannotes. We describe an implementation and extension cer. To date, we have principally examined two difof ISO-TimeML (Pustejovsky et al., 2010), devel- ferent general types of clinical narrative in our EHRs: oped specifically for the clinical domain, which we clinical notes and pathology reports. Clinical notes are records of physician interactions refer to as the “THYME Guidelines to ISO-TimeML” (“THYME-TimeML”), where THYME stands for with a patient, and often include multiple, clearly “Temporal Histories of Your Medical Events”. A sim- delineated sections detailing different aspects of the plified version of these guidelines formed the basis patient’s care and present illness. These notes are for the 2012 i2b2 medical-domain tempo"
Q14-1012,S13-2001,1,0.830816,"Missing"
Q14-1012,W08-0606,0,0.0123269,"for situations where doctors proffer a diagnosis, but do so cautiously, to avoid legal liability for an incorrect diagnosis or for overlooking a correct one. For example: (3) a. The signal in the MRI is not inconsistent with a tumor in the spleen. b. The rash appears to be measles, awaiting antibody test to confirm. These H EDGED E VENTs are more real than a hypothetical diagnosis, and likely merit inclusion on a timeline as part of the diagnostic history, but must not be conflated with confirmed fact. These (and other forms of uncertainty in the medical domain) are discussed extensively in (Vincze et al., 2008). In contrast, G ENERIC E VENTs do not refer to the patient’s illness or treatment, but instead discuss illness or treatment in general (often in the patient’s specific demographic). For example: (4) In other patients without significant comorbidity that can tolerate adjuvant chemotherapy, there is a benefit to systemic adjuvant chemotherapy. These sections would be true if pasted into any patient’s note, and are often identical chunks of text repeatedly used to justify a course of action or treatment as well as to defend against liability. Contextual Aspect (to distinguish from grammatical as"
S07-1016,W04-2412,0,0.0234898,"Missing"
S07-1016,W05-0620,0,0.029889,"Missing"
S07-1016,N06-1016,1,0.634629,"Missing"
S07-1016,W02-0817,0,0.234086,"Missing"
S07-1016,W04-0827,0,0.0139734,"Missing"
S07-1016,N06-2015,1,0.63929,"between the verb and noun performance seems to indicate that in general the verbs were more difficult than the nouns. However, this might just be owing to this particular test sample having more verbs with higher perplexities, and maybe even ones that are indeed difficult to disambiguate – in spite of high human agreement. The hope is that better knowledge sources can overcome the gap still existing between the system performance and human agreement. Overall, however, this data indicates that the approach suggested by (Palmer, 2000) and that is being adopted in the ongoing OntoNotes project (Hovy et al., 2006) does result in higher system performance. Whether or not the more coarse-grained senses are effective in improving natural language processing applications remains to be seen. Lemma turn.v go.v come.v set.v hold.v raise.v work.v keep.v start.v lead.v see.v ask.v find.v fix.v buy.v begin.v kill.v join.v end.v do.v examine.v report.v regard.v recall.v prove.v claim.v build.v feel.v care.v contribute.v maintain.v complain.v propose.v promise.v produce.v prepare.v explain.v believe.v occur.v grant.v enjoy.v need.v disclose.v point.n position.n defense.n carrier.n order.n exchange.n system.n sourc"
S07-1016,kipper-etal-2006-extending,1,0.60897,"ciated with each tag, we can not make generalizations across verb classes. In contrast, the use of a shared set of role labels, such 91 System UBC-UPC UBC-UPC RTV Without “say” UBC-UPC UBC-UPC RTV Type Open Closed Closed Precision 84.51 85.04 81.82 Recall 82.24 82.07 70.37 F 83.36±0.5 83.52±0.5 75.66±0.6 Open Closed Closed 78.57 78.67 74.15 74.70 73.94 57.85 76.60±0.8 76.23±0.8 65.00±0.9 Table 5: System performance on PropBank arguments. as VerbNet roles, would facilitate both inferencing and generalization. VerbNet has more traditional labels such as Agent, Patient, Theme, Beneficiary, etc. (Kipper et al., 2006). Therefore, we chose to annotate the corpus using two different role label sets: the PropBank role set and the VerbNet role set. VerbNet roles were generated using the SemLink mapping (Loper et al., 2007), which provides a mapping between PropBank and VerbNet role labels. In a small number of cases, no VerbNet role was available (e.g., because VerbNet did not contain the appropriate sense of the verb). In those cases, the PropBank role label was used instead. We proposed two levels of participation in this task: i) Closed – the systems could use only the annotated data provided and nothing el"
S07-1016,W04-0803,0,0.0234443,"guments. (1) a. What do lobsters like to eat? b. Recent studies have shown that lobsters primarily feed on live fish, dig for clams, sea urchins, and feed on algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. Traditionally, SRL systems have been trained on either the PropBank corpus (Palmer et al., 2005) – for two years, the CoNLL workshop (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005) has made this their shared task, or the FrameNet corpus – Senseval-3 used this for their shared task (Litkowski, 2004). However, there is still little consensus in the linguistics and NLP communities about what set of role labels are most appropriate. The PropBank corpus avoids this issue by using theoryagnostic labels (A RG 0, A RG 1, . . . , A RG 5), and by defining those labels to have only verb-specific meanings. Under this scheme, PropBank can avoid making any claims about how any one verb’s arguments relate to other verbs’ arguments, or about general distinctions between verb arguments and adjuncts. However, there are several limitations to this approach. The first is that it can be difficult to make in"
S07-1016,J05-1004,1,0.118792,"semantic systems. For example, in order to determine that question (1a) is answered by sentence (1b), but not by sentence (1c), we must determine the relationships between the relevant verbs (eat and feed) and their arguments. (1) a. What do lobsters like to eat? b. Recent studies have shown that lobsters primarily feed on live fish, dig for clams, sea urchins, and feed on algae and eel-grass. c. In the early 20th century, Mainers would only eat lobsters because the fish they caught was too valuable to eat themselves. Traditionally, SRL systems have been trained on either the PropBank corpus (Palmer et al., 2005) – for two years, the CoNLL workshop (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005) has made this their shared task, or the FrameNet corpus – Senseval-3 used this for their shared task (Litkowski, 2004). However, there is still little consensus in the linguistics and NLP communities about what set of role labels are most appropriate. The PropBank corpus avoids this issue by using theoryagnostic labels (A RG 0, A RG 1, . . . , A RG 5), and by defining those labels to have only verb-specific meanings. Under this scheme, PropBank can avoid making any claims about how any one verb’s ar"
S10-1011,S07-1002,0,0.178345,"ystem. The evaluation framework of SemEval-2010 WSI task considered two types of evaluation. In the first one, unsupervised evaluation, systems’ answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009). Neither of these measures were used in the SemEval2007 WSI task. Manandhar & Klapaftis (2009) provide more details on the choice of this evaluation setting and its differences with the previous evaluation. The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Query to the target word sense for which the query was created. The relations considered were WordNet’s hypernyms, hyponyms, synonyms, meronyms and holonyms. Each query was manually checked by one of the organisers to remove ambiguous words. The following example shows the query created for the first1 and second2 WordNet sense of the target noun failure. The created queries were issued to Yahoo! search API3 and for each query a maximum of 10"
S10-1011,D09-1056,0,0.357532,"Missing"
S10-1011,N06-2015,0,0.0362233,"hed the POS of the target word in our dataset. Training dataset 2.2 The target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs. The training dataset for each target noun or verb was created by following a web-based semi-automatic method, similar to the method for the construction of Topic Signatures (Agirre et al., 2001). Specifically, for each WordNet (Fellbaum, 1998) sense of a target word, we created a query of the following form: Testing dataset The testing dataset consisted of instances of the same target words from the training dataset. This dataset is part of OntoNotes (Hovy et al., 2006). We used the sense-tagged dataset in which sentences containing target word instances are tagged with OntoNotes (Hovy et al., 2006) senses. The texts come from various news sources including CNN, ABC and others. <Target Word> AND <Relative Set> The <Target Word> consisted of the target word stem. The <Relative Set> consisted of a disjunctive set of word lemmas that were related 1 An act that fails An event that does not accomplish its intended purpose 3 http://developer.yahoo.com/search/ [Access:10/04/2010] 2 64 C1 C2 C3 C4 G1 10 20 1 5 G2 10 50 10 0 G3 15 0 60 0 When H(S|K) is 0, the solutio"
S10-1011,W09-2419,1,0.616074,"Missing"
S10-1011,D07-1043,0,0.711201,"ils failure AND (loss OR nonconformity OR test OR surrender OR ”force play” OR ...) failure AND (ruination OR flop OR bust OR stall OR ruin OR walloping OR ...) Table 2: Training set creation: example queries for target word failure for sense induction. Treating the testing data as new unseen instances ensures a realistic evaluation that allows to evaluate the clustering models of each participating system. The evaluation framework of SemEval-2010 WSI task considered two types of evaluation. In the first one, unsupervised evaluation, systems’ answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009). Neither of these measures were used in the SemEval2007 WSI task. Manandhar & Klapaftis (2009) provide more details on the choice of this evaluation setting and its differences with the previous evaluation. The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). In this evaluation, induced senses are mapped to gold standard senses using a mapping corpus, and systems are then evaluated in a standard WSD task. 2.1 Query to the target word sense for which the quer"
S10-1011,H05-1059,0,0.00864447,"onsidered were WordNet’s hypernyms, hyponyms, synonyms, meronyms and holonyms. Each query was manually checked by one of the organisers to remove ambiguous words. The following example shows the query created for the first1 and second2 WordNet sense of the target noun failure. The created queries were issued to Yahoo! search API3 and for each query a maximum of 1000 pages were downloaded. For each page we extracted fragments of text that occurred in <p> </p> html tags and contained the target word stem. In the final stage, each extracted fragment of text was POS-tagged using the Genia tagger (Tsuruoka and Tsujii, 2005) and was only retained, if the POS of the target word in the extracted text matched the POS of the target word in our dataset. Training dataset 2.2 The target word dataset consisted of 100 words, i.e. 50 nouns and 50 verbs. The training dataset for each target noun or verb was created by following a web-based semi-automatic method, similar to the method for the construction of Topic Signatures (Agirre et al., 2001). Specifically, for each WordNet (Fellbaum, 1998) sense of a target word, we created a query of the following form: Testing dataset The testing dataset consisted of instances of the"
S14-2007,S14-2108,0,0.0812075,"Missing"
S14-2007,S14-2134,0,0.0735188,"ion of development data helps, there are still systems that perform in the lower percentile who have used both training and development data for training, indicating that both the features and the machine learning classifier contribute to the models. A novel aspect of the SemEval-2014 shared task that differentiates it from the ShARE/CLEF task—other than the fact that it used more data and a new test set—is the fact that SemEval-2014 allowed the use of a much larger set of unlabeled MIMIC notes to inform the models. Surprisingly, only two of the systems (ULisboa (Leal et al., 2014) and UniPi (Attardi et al., 2014)) used the unlabeled MIMIC corpus to generalize the lexical features. Another team—UTH CCB(Zhang et al., 2014)—used off-the-shelf Brown clusters10 as opposed to training them on the unlabeled MIMIC II data. For Task B, the accuracy of a system using the strict metric was positively correlated with its recall on the disorder mentions that were input to it (i.e., recall for Task A), and did not get penalized for lower precision. Therefore one could essentially gain higher accuracy in Task B by tuning a system to provide the highest mention recall in Task A potentially at the cost of precision an"
S14-2007,S14-2147,0,0.0395526,"0.4 40.2 36.4 35.8 33.3 33.2 31.9 25.3 24.8 24.4 16.5 12.5 87.3 88.0 88.3 90.9 91.2 90.8 86.2 78.3 87.8 87.4 79.7 85.5 87.0 82.5 84.9 74.8 68.3 86.1 86.3 55.8 77.1 69.9 69.3 79.7 61.5 61.2 60.6 59.5 83.4 69.6 69.1 69.6 47.9 47.7 47.3 92.8 53.4 T+D T+D T+D T+D T+D T T+D T+D T T T T+D T+D T T+D T+D T+D T+D T T+D T T+D T+D T T T T T+D T+D T+D T+D T+D T+D T+D T+D P P the best accuracy for Task B and vice-versa for run 0 with run 1 in between the two. In order to fairly compare the performance between two systems one would have to provide perfect mentions as input to Task B. One of the systems—UWM Ghiasvand and Kate (2014)—did run some ablation experiments using gold standard mentions as input to Task B and obtained a best performance of 89.5F1 -score (Table 5 of Ghiasvand and Kate (2014)) as opposed to 62.3 F1 -score (Table 7) in the more realistic setting which is a huge difference. In the upcoming SemEval-2014 where this same evaluation is going to carried out under Task 14, we plan to perform supplementary evaluation where gold disorder mentions would be input to the system while attempting Task B. An interesting outcome of planning a follow-on evaluation to the ShARe/CLEF eHealth 2013 task was that we coul"
S14-2007,S14-2143,0,0.0959678,"Missing"
S14-2007,S14-2127,0,0.109755,"Missing"
S14-2007,S14-2019,0,0.0811758,"and 7. We have inserted the best performing system score from the ShARe/CLEF eHealth 2013 task in these tables. For Task A, referring to Tables 4 and 5, there is a boost of 3.7 absolute percent points for the F1 -score over the same task (Task 1a) in the ShARe/CLEF eHealth 2013. For Task B, referring to Tables 6 and 7, there is a boost of 13.7 percent points for the F1 -score over the same task (Task 1b) in the ShARe/CLEF eHealth 2013 evaluation. The participants used various approaches for tackling the tasks, ranging from purely rule-based/unsupervised (RelAgent (Ramanan and Nathan, 2014), (Matos et al., 2014), KUL11 ) to a hybrid of rules and machine learning classifiers. The top performing systems typically used the latter. Various versions of the IOB formulation were used for tagging the disorder mentions. None of the standard variations on the IOB formulation were explicitly designed or used to handle discontiguous mentions. Some systems used novel variations on this approach. Probably the simplest variation was applied by the UWM team (Ghiasvand and Kate, 2014). In this formulation the following labeled sequence “the/O left/B atrium/I is/O moderately/O Table 6: Performance on test data for par"
S14-2007,D09-1020,0,0.0182625,"entity recognition and word sense disambiguation. Neither of these problems are new to NLP. Research in general-domain NLP goes back to about two decades. For an overview of the development in the field through roughly 2009, we refer the refer to Nadeau and Sekine (2007). NLP has also penetrated the field of bimedical informatics and has been particularly focused on biomedical literature for over the past decade. Advances in that sub-field has also been documented in surveys such as one by Leaman and Gonzalez (2008). Word sense disambiguation also has a long history in the general NLP domain (Navigli, 2009). In spite of word sense annotations in the biomedical literature, recent work by Savova et al. (2008) highlights the importance of annotating them in clinical notes. This is true for many other clinical and linguistic phenomena as the various characteristics of the clinical narrative present a unique challenge to NLP. Recently various initiatives have led to annotated corpora for clinical NLP research. Probably the first comprehensive annotation performed on a clinical corpora was by Roberts et al. (2009), but unfortunately that corpus is not publicly available owing to privacy regulations. T"
S14-2007,S14-2007,1,0.107262,"ct = Number of correctly normalized disorder mentions; and Tg = Total number of disorder mentions in the gold standard. For Task B, the systems were only evaluated on annotations they identified in Task A. Relaxed accuracy only measured the ability to normalize correct spans. Therefore, it was possible to obtain very high values for this measure by simply dropping any mention with a low confidence span. 5 Participants A total of 21 participants from across the world participated in Task A and out of them 18 also participated in Task B. Unfortunately, although interested, the ThinkMiners team (Parikh et al., 2014) could not participate in Task B owing to some UMLS licensing issues. The participating organizations along with the contact user’s User ID and their chosen Team ID are mentioned in Table 3. Eight teams submitted three runs, six submitted two runs and seven submitted just one run. Out of these, only 13 submitted system description papers. We based our analysis on those system descriptions. 6 System Results Tables 4 and 6 show the performance of the systems on Tasks A and B. None of the systems used any additional annotated data so we did not have to compare them separately. Both tables mention"
S14-2007,S14-2083,0,0.0707782,"ts are presented in Tables 5 and 7. We have inserted the best performing system score from the ShARe/CLEF eHealth 2013 task in these tables. For Task A, referring to Tables 4 and 5, there is a boost of 3.7 absolute percent points for the F1 -score over the same task (Task 1a) in the ShARe/CLEF eHealth 2013. For Task B, referring to Tables 6 and 7, there is a boost of 13.7 percent points for the F1 -score over the same task (Task 1b) in the ShARe/CLEF eHealth 2013 evaluation. The participants used various approaches for tackling the tasks, ranging from purely rule-based/unsupervised (RelAgent (Ramanan and Nathan, 2014), (Matos et al., 2014), KUL11 ) to a hybrid of rules and machine learning classifiers. The top performing systems typically used the latter. Various versions of the IOB formulation were used for tagging the disorder mentions. None of the standard variations on the IOB formulation were explicitly designed or used to handle discontiguous mentions. Some systems used novel variations on this approach. Probably the simplest variation was applied by the UWM team (Ghiasvand and Kate, 2014). In this formulation the following labeled sequence “the/O left/B atrium/I is/O moderately/O Table 6: Performanc"
S14-2007,S15-2052,0,\N,Missing
S14-2007,S14-2142,0,\N,Missing
S15-2051,S15-2071,0,0.0854138,"predicted spans that overlap with a gold-standard span, then only one of them is chosen to be true positive (the longest ones), and the other predicted spans are considered false positives. 5 (6) where for each true-positive span there is a goldstandard value gsi,k and a predicted value psi,k for slot sk . 307 5.1 Results Task 1 16 teams participated in Task 1. Strict and relaxed precision, recall, and F metrics are reported in Figure 1. We relied on the strict F to rank different submissions. The best system from team ezDI reported 75.7 strict F, also reporting the highest relaxed F (78.7) (Pathak et al., 2015). For disorder span recognition, most teams used a CRF-based approach. Features explored included traditional NER features: lexical (bag of words and bigrams, orthographic features), syntactic features derived from either part-of-speech and phrase chunking information or dependency parsing, and domain features (note type and section headers of clinical note). Lookup to dictionary (either UMLS or customized lexicon of disorders) was an essential feature for performance. To leverage further these lexicons, for instance, Xu and colleagues (Xu et al., 2015) implemented a vector-space model similar"
S15-2051,S14-2007,1,0.756531,"Missing"
S16-1181,D15-1198,0,0.186545,"CG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of exi"
S16-1181,W13-2322,0,0.42263,"Missing"
S16-1181,P13-2131,0,0.176692,"ng Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator trained using a word sense disambiguation system— IMS (Zhong and Ng, 2010). All these components viz., the Charniak parser, Stanford named entity tagger, ASSERT, and IMS word sense disambiguator were retrained on the OntoNotes v5.0 training 1176 data2 (Pradhan et al., 2013)3 . We use the version of CAMR described in (Wang et al., 2015a) (without the feature extensions) as the baseline. We evaluate our parser with Smatch v2.0.2 (Cai and Knight, 2013) on all the experiments. It should be noted that all the rows in Table 2 except for the last one get implicitly penalized by the scorer for lack of wikification information. 5.1 SemEval Development Set As discussed in (Wang et al., 2015a), the performance of the syntactic parser in the first stage has a high impact on the AMR parsing accuracy. We first do a sanity check to choose the best first stage parser. Here we only consider two scenarios: the Charniak parser trained on WSJ and OntoNotes, as shown in Table 1. As using the Charniak parser trained on OntoNotes yields slightly better AMR par"
S16-1181,P05-1022,0,0.0786156,"(CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics labels produced by an automatic SRL system. The rest of the paper is organized as follows. In Section 2 we briefly describe CAMR, and in Section 3 we describ"
S16-1181,P14-1134,0,0.635756,"d step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Association for Computationa"
S16-1181,P14-5010,0,0.00352027,"ly on the Semeval release (c.). In Section 5.3 we also use the full test set of release (b.) to evaluate the performance improvement made to CAMR as part of the SemEval evaluations against previously reported performance. Syntactic Parser Charniak (ON) Charniak (WSJ) P 70.76 69.88 R 60.57 60.24 F1 65.27 64.70 Table 1: AMR parsing performance on the SemEval development set (LDC2015E86) across two Charniak parser models 5 Experiments We use the official release dataset and standard train/dev/test split of SemEval Task 8 for experiments. All the sentences are preprocessed using Stanford CoreNLP (Manning et al., 2014) to get tokenization, lemma, named entity tag, POS tag. And we use the aligner that comes with JAMR (Flanigan et al., 2014) to align the sentence with its AMR graph. We then parse the tokenized sentences using Charniak parser (Charniak and Johnson, 2005)(Its phrase structure output is converted to dependency structure using a slightly modified version of the Stanford CoreNLP converter). Rich named entity tags are generated using Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator tr"
S16-1181,H05-1066,0,0.057511,"Missing"
S16-1181,N15-1119,1,0.758601,"urrent head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it against the dependent. If the argument predicted by ASSERT matches the dependent, the value of the IS ARGUMENT is set to true. Word Clusters For the semi-supervised word cluster feature, we use Brown clusters, more specifically, the 1000-class word clusters trained by Turian et al. (2010). We use prefixes of lengths 4, 6, 10 and 20 of the word’s bit-string as features. 1175 3.2 Wikification We apply an AMR based wikification system (Pan et al., 2015) which utilizes AMR to represent semantic information about entity mentions expressed in their textual context. Given an entity mention m, this system first constructs a Knowledge Graph g(m) with m at the hub and leaf nodes obtained from entity mentions reachable by AMR graph traversal from m. A subset of the leaf nodes are selected as collaborators of m. Mentions connected by AMR conjunction relations are grouped into sets of coherent mentions. For each entity mention m, an initial ranked list of entity candidates E = (e1 , . . . , en ) is generated based on a salience measure (Medelyan and W"
S16-1181,K15-1004,0,0.347794,"Missing"
S16-1181,N04-1030,1,0.146339,"e input sentence is in the verbalization list. semantic role labeling: wants, want-01, ARG0: the boy, ARG1: the girl to believe him wants boy ARG1 girl believe him For action NEXT- NODE-want-01 EQ FRAMESET: true Figure 2: An example of semantic role labeling feature in partial parsing graph of sentence,“The boy wants the girl to believe him.” Semantic role labeling features We use the following semantic role labeling features: 1) EQ FRAMESET. For actions that predict the concept label (N EXT- NODE -lc ), we check whether the candidate concept label lc matches the frameset predicted by ASSERT (Pradhan et al., 2004). For example, in the partial graph in Figure 2, when we examine node wants, one of the candidate actions would be N EXT- NODE-want-01. Since the candidate concept label want-01 is equal to node wants’s frameset want-01 as predicted by ASSERT, the value of feature EQ FRAMESET is set to true. 2) IS ARGUMENT. For actions that predict the edge label, we check whether ASSERT predicts that the current dependent is an argument of the current head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it agai"
S16-1181,W13-3516,1,0.868503,"Missing"
S16-1181,D15-1136,0,0.326487,"s used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial Grammar (CCG) to construct the lambdacalculus representations of the sentence, then further resolve non-compositional dependencies using a factor graph. Peng et al.(2015) and Pust et al.(2015) formalize parsing AMR as a machine translation problem by learning string-graph/string-tree rules from the annotated data. Although the field of AMR parsing is growing and several systems (Wang et al., 2015a; Artzi et al., 2015; Pust et al., 2015; Flanigan et al., 2014) have substantially advanced the state of the art, the overall performance of existing AMR parsers is far less accurate than syntactic parsers (Charniak and Johnson, 2005). This makes it difficult to use in downstream NLP tasks. In this paper, we aim to boost the AMR parsing performance by introducing additional features. We mainly experiment with three sets of features derived from: 1) rich named entities, 2) a verbalization list provided by ISI, and 3) semantic role 1173 Proceedings of SemEval-2016, pages 1173–1178, c San Diego, California, June 16-17, 2016. 2016 Asso"
S16-1181,P10-1040,0,0.0357655,"EQ FRAMESET is set to true. 2) IS ARGUMENT. For actions that predict the edge label, we check whether ASSERT predicts that the current dependent is an argument of the current head. Note that arguments output by the semantic role labeler are typically constituents in a syntactic tree. We find the head of the argument and match it against the dependent. If the argument predicted by ASSERT matches the dependent, the value of the IS ARGUMENT is set to true. Word Clusters For the semi-supervised word cluster feature, we use Brown clusters, more specifically, the 1000-class word clusters trained by Turian et al. (2010). We use prefixes of lengths 4, 6, 10 and 20 of the word’s bit-string as features. 1175 3.2 Wikification We apply an AMR based wikification system (Pan et al., 2015) which utilizes AMR to represent semantic information about entity mentions expressed in their textual context. Given an entity mention m, this system first constructs a Knowledge Graph g(m) with m at the hub and leaf nodes obtained from entity mentions reachable by AMR graph traversal from m. A subset of the leaf nodes are selected as collaborators of m. Mentions connected by AMR conjunction relations are grouped into sets of cohe"
S16-1181,P15-2141,1,0.766369,"014), performs AMR parsing in two stages: concept identification and relation identification. Flanigan et al. (2014) treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial"
S16-1181,N15-1040,1,0.829287,"014), performs AMR parsing in two stages: concept identification and relation identification. Flanigan et al. (2014) treat concept identification as a sequence labeling task and utilize a semi-Markov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. Wang et al. (2015b) describes a transition-based parser that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, it transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. There are also approaches which utilize grammar induction to parse the AMR. Artzi et al. (2015) presents a model that first use Combinatory Categorial"
S16-1181,P10-4014,0,0.0295827,"g. And we use the aligner that comes with JAMR (Flanigan et al., 2014) to align the sentence with its AMR graph. We then parse the tokenized sentences using Charniak parser (Charniak and Johnson, 2005)(Its phrase structure output is converted to dependency structure using a slightly modified version of the Stanford CoreNLP converter). Rich named entity tags are generated using Stanford named entity tagger. The semantic role labels are generated using ASSERT—a semantic role labeler (Pradhan et al., 2005), including a frameset disambiguator trained using a word sense disambiguation system— IMS (Zhong and Ng, 2010). All these components viz., the Charniak parser, Stanford named entity tagger, ASSERT, and IMS word sense disambiguator were retrained on the OntoNotes v5.0 training 1176 data2 (Pradhan et al., 2013)3 . We use the version of CAMR described in (Wang et al., 2015a) (without the feature extensions) as the baseline. We evaluate our parser with Smatch v2.0.2 (Cai and Knight, 2013) on all the experiments. It should be noted that all the rows in Table 2 except for the last one get implicitly penalized by the scorer for lack of wikification information. 5.1 SemEval Development Set As discussed in (Wa"
W04-2416,W04-2412,0,0.211539,"Missing"
W04-2416,W95-0107,0,0.0497133,"(i.e. base phrases) instead of words or the constituents derived from syntactic trees. This system is referred to as the phraseby-phrase (P-by-P) semantic role classifier. We participate in the “closed challenge” of the CoNLL-2004 shared task and report results on both development and test sets. A detailed description of the task, data and related work can be found in (Carreras and M`arquez, 2004). 2 System Description 2.1 Data Representation In this paper, we change the representation of the original data as follows: • Bracketed representation of roles is converted into IOB2 representation (Ramhsaw and Marcus, 1995; Sang and Veenstra, 1995) • Word tokens are collapsed into base phrase (BP) tokens. Since the semantic annotation in the PropBank corpus does not have any embedded structure there is no loss of information in the first change. However, this results in a simpler representation with a reduced set of tagging labels. In the second change, it is possible to miss some information in cases where the semantic chunks do not align with the sequence of BPs. However, in Section 3.2 we show that the loss in performance due to the misalignment is much less than the gain in performance that can be achieved"
W04-2416,E99-1023,0,0.098933,"Missing"
W04-2416,N04-1030,1,\N,Missing
W04-3211,J02-3001,0,0.0324111,"STITUENT P HRASE T YPE: the syntactic type assigned to the constituent/argument being classified H EAD W ORD (HW): the head word of the target constituent PARSE T REE PATH (PATH ): the sequence of parse tree constituent labels from the argument to its predicate P OSITION: a binary value indicating whether the target argument precedes or follows its predicate VOICE: a binary value indicating whether the predicate was used in an active or passive phrase S UB -C ATEGORIZATION: the parse tree expansion of the predicate’s grandparent constituent Figure 2: Baseline feature set of experiment 1, see (Gildea and Jurafsky, 2002) for details 4 The Experiments Four experiments are reported: the first uses the baseline features of Gildea and Jurafsky (2002); the second is composed of features proposed by Pradhan et al. (2003) and Surdeanu et al. (2003); the third experiment evaluates a new feature set; and the final experiment addresses a method of reducing the feature space. The experiments all focus strictly on the classification task – given a syntactic constituent known to be an argument of a given predicate, decide which argument role is the appropriate one to assign to the constituent. 4.1 Experiment 1: Baseline F"
W04-3211,P02-1031,0,0.0421002,"composed of features proposed by Pradhan et al. (2003) and Surdeanu et al. (2003); the third experiment evaluates a new feature set; and the final experiment addresses a method of reducing the feature space. The experiments all focus strictly on the classification task – given a syntactic constituent known to be an argument of a given predicate, decide which argument role is the appropriate one to assign to the constituent. 4.1 Experiment 1: Baseline Feature Set The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with backoff (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al., 2003), and a Support Vector Machine (SVM) (Pradhan et al., 2003). The baseline feature set utilized in this experiment is described in Figure 2 (see (Gildea and Jurafsky, 2002) for details). Surdeanu et al. omit the S U B -C A T E G O R I Z A T I O N feature, but add a binary-valued feature that indicates the governing category of noun-phrase argument constituents. This feature takes on the value S or VP depending on which constituent type (sentence or verb phase respectively) eventually dominates the argument in the parse tree. This generally ind"
W04-3211,H94-1020,0,0.0280969,"me observations and features are used as described by (Pradhan et al., 2003). They acquired the original data from the July 15, 2002 release of PropBank, which the University of Pennsylvania created by manually labeling the constituents S NP She Arg0 VP bought Predicate NP PP the vase Arg1 in Egypt ArgM-Loc Section training development test # sent 28 1.2 1.5 # words 651 28 33 # preds 50 2.2 2.7 # args 129 5.7 7.0 Table 1: Number of sentences, words, marked predicates, and labeled arguments in thousands Figure 1: Syntactic parse of the sentence in (2) of the Penn TreeBank gold-standard parses (Marcus et al., 1994). Predicate usages (at present, strictly verbs) are hand annotated with 22 possible semantic roles plus the null role to indicate grammatical constituents that are not arguments of the predicate. The argument labels can have different meanings depending on their target predicate, but the annotation method attempted to assign consistent meanings to labels, especially when associated with similar verbs. There are seven core roles or arguments, labeled A R G 0-5 and A R G 9. A R G 0 usually corresponds to the semantic agent and A R G 1 to the entity most affected by the action. In addition to the"
W04-3211,P03-1002,0,0.013195,"om the argument to its predicate P OSITION: a binary value indicating whether the target argument precedes or follows its predicate VOICE: a binary value indicating whether the predicate was used in an active or passive phrase S UB -C ATEGORIZATION: the parse tree expansion of the predicate’s grandparent constituent Figure 2: Baseline feature set of experiment 1, see (Gildea and Jurafsky, 2002) for details 4 The Experiments Four experiments are reported: the first uses the baseline features of Gildea and Jurafsky (2002); the second is composed of features proposed by Pradhan et al. (2003) and Surdeanu et al. (2003); the third experiment evaluates a new feature set; and the final experiment addresses a method of reducing the feature space. The experiments all focus strictly on the classification task – given a syntactic constituent known to be an argument of a given predicate, decide which argument role is the appropriate one to assign to the constituent. 4.1 Experiment 1: Baseline Feature Set The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with backoff (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al., 2003), and"
W04-3211,N04-1030,1,\N,Missing
W05-0634,W05-0620,0,0.305505,"Missing"
W05-0634,J02-3001,1,0.761681,"Missing"
W05-0634,N03-2009,1,0.749207,"tracted from the input. This approach is neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produc"
W05-0634,W04-2416,1,0.861755,"neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produce any constituent that corresponded to t"
W05-0634,kingsbury-palmer-2002-treebank,0,0.0855298,"Missing"
W05-0634,W00-0730,0,0.0185759,"erent syntactic views. Our goal is to preserve the robustness and flexibility of the segmentation of the phrase-based chunker, but to take advantage of features from full syntactic parses. We also want to combine features from different syntactic parses to gain additional robustness. To this end, we use features generated from a Charniak parser and a Collins parser, as supplied for the CoNLL-2005 closed task. 2 System Description We again formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifiers. TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of"
W05-0634,N01-1025,0,0.0421455,"goal is to preserve the robustness and flexibility of the segmentation of the phrase-based chunker, but to take advantage of features from full syntactic parses. We also want to combine features from different syntactic parses to gain additional robustness. To this end, we use features generated from a Charniak parser and a Collins parser, as supplied for the CoNLL-2005 closed task. 2 System Description We again formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifiers. TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of semantic roles. Chunk base"
W05-0634,N04-1030,1,0.950041,"e in that sentence. Our approach is to use supervised machine learning classifiers to produce the role labels based on features extracted from the input. This approach is neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors fo"
W05-0634,P05-1072,1,0.75393,"y the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produce any constituent that corresponded to the correct segmentation for the semantic argument. In Pradhan et al. (2005), we reported on a first attempt to overcome this problem by combining semantic role labels produced from different syntactic parses. The hope is that the syntactic parsers will make different errors, and that combining their outputs will improve on 217 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 217–220, Ann Arbor, June 2005. 2005 Association for Computational Linguistics either system alone. This initial attempt used features from a Charniak parser, a Minipar parser and a chunk based parser. It did show some improvement from the combination,"
W05-0634,W95-0107,0,0.0248543,"The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of semantic roles. Chunk based systems classify each base phrase as being the B(eginning) of a semantic role, I(nside) a semantic role, or O(utside) any semantic role (ie. N ULL). This is referred to as an IOB representation (Ramshaw and Marcus, 1995). The constituent level roles are mapped to the IOB representation used by the chunker. The IOB tags are then used as features for a separate base-phase semantic role labeler (chunker), in addition to the standard set of features used by the chunker. An n-fold cross-validation paradigm is used to train the constituent based role classifiers 1 2 http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 218 and the chunk based classifier. For the system reported here, two full syntactic parsers were used, a Charniak parser and a Collins parser. Features were extracted by"
W05-0634,P03-1002,0,0.0642219,"Missing"
W05-0634,W04-3212,0,0.189369,"Missing"
W10-1836,W03-1006,0,0.060505,". 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument,"
W10-1836,choi-etal-2010-propbank-instance,1,0.897928,"integrated manner, how to identify one construction from the other, figuring out a language specific reliable diagnostic test, and whether we deal with these constructions as a whole unit or as separate parts; and how? (Hwang, et al., 2010) 4.2 Tools Frameset files are created in an XML format. During the Pilot Propbank project these files were created manually by editing the XML file related to a particular predicate. This proved to be time consuming and prone to many formatting errors. The Frame File creation for the revised APB is now performed with the recently developed Cornerstone tool (Choi et al., 2010a), which is a PropBank frameset editor that allows the creation and editing of Propbank framesets without requiring any prior knowledge of XML. Moreover, the annotation is now performed by Jubilee, a new annotation tool, which has improved the annotation process by displaying several types of relevant syntactic and semantic information at the same time. Having everything displayed helps the annotator quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation (Choi et al., 20010b). Both tools are available as Op"
W10-1836,P08-1091,1,0.8821,"e Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experiencer and ‘movies’ w"
W10-1836,J02-3001,0,0.0350371,"the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the"
W10-1836,P02-1031,1,0.76542,"d multi-word expressions. New tools facilitate the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the"
W10-1836,N07-2014,0,0.0683748,"Missing"
W10-1836,N06-2015,1,0.851769,"Missing"
W10-1836,W10-1810,1,0.834857,"ing of cohesion in the group. The APB has decided to thoroughly tackle light verb constructions and multi-word expressions as part of an effort to facilitate mapping between the different languages that are being PropBanked. In the process of setting this up a number of challenges have surfaced which include: how can we cross-linguistically approach these phenomena in a (semi) integrated manner, how to identify one construction from the other, figuring out a language specific reliable diagnostic test, and whether we deal with these constructions as a whole unit or as separate parts; and how? (Hwang, et al., 2010) 4.2 Tools Frameset files are created in an XML format. During the Pilot Propbank project these files were created manually by editing the XML file related to a particular predicate. This proved to be time consuming and prone to many formatting errors. The Frame File creation for the revised APB is now performed with the recently developed Cornerstone tool (Choi et al., 2010a), which is a PropBank frameset editor that allows the creation and editing of Propbank framesets without requiring any prior knowledge of XML. Moreover, the annotation is now performed by Jubilee, a new annotation tool, w"
W10-1836,P04-1043,0,0.0648245,"e in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be lab"
W10-1836,W05-0630,0,0.0204675,"tomated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experi"
W10-1836,J05-1004,1,0.599345,"aper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experiencer and ‘movies’ would be the theme/content. According to PropBank, ‘John’ is labeled Arg0 (or enjoyer) and ‘movies’ is labeled Arg1 (or thing enjoyed). Crucially, that independent of the l"
W10-1836,N04-1030,1,0.800833,"New tools facilitate the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’"
W10-1836,W04-3212,1,0.799084,"implify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, a"
W10-1836,P80-1024,0,0.623039,"Missing"
W10-1836,P09-5003,0,\N,Missing
W10-1836,W05-0620,0,\N,Missing
W10-1836,palmer-etal-2008-pilot,1,\N,Missing
W11-1901,W06-0609,1,0.725549,"he Switchboard Treebank. Given the frequency of disfluencies and the performance with which one can identify them automatically,8 a probable processing pipeline would filter them out before parsing. Since we did not have a readily available tagger for tagging disfluencies, we decided to remove them using oracle information available in the Treebank. Propositions The propositions in OntoNotes constitute PropBank semantic roles. Most of the verb predicates in the corpus have been annotated with their arguments. Recent enhancements to the PropBank to make it synchronize better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK - PCR) and selectional preferences (LINK SLC ). More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7 There is another phrase type – EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. 8 A study by Charniak and Johnson (2001) shows that one can identify"
W11-1901,P06-1005,0,0.929236,"ers including the parses, semantic roles, word senses, and named entities. As is customary for CoNLL tasks, there were two tracks, closed and open. For the closed track, systems were limited to using the distributed resources, in order to allow a fair comparison of algorithm performance, while the open track allowed for almost unrestricted use of external resources in addition to the provided data. 4.2.1 Closed Track In the closed track, systems were limited to the provided data, plus the use of two pre-specified external resources: i) WordNet and ii) a pre-computed number and gender table by Bergsma and Lin (2006). For the training and test data, in addition to the underlying text, predicted versions of all the supplementary layers of annotation were provided, where those predictions were derived using off-the-shelf tools (parsers, semantic role labelers, named entity taggers, etc.) as described in Section 4.4.2. For the training data, however, in addition to predicted values for the other layers, we also provided manual gold-standard annotations for all the layers. Participants were allowed to use either the gold-standard or predicted annotation for training their systems. They were also free to use t"
W11-1901,W10-4305,0,0.156408,"Missing"
W11-1901,N01-1016,0,0.0275817,"ze better with the Treebank (Babko-Malaya et al., 2006) have enhanced the information in the proposition by the addition of two types of LINKs that represent pragmatic coreference (LINK - PCR) and selectional preferences (LINK SLC ). More details can be found in the addendum to the PropBank guidelines9 in the OntoNotes 4.0 re7 There is another phrase type – EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases, so we decided not to remove that from the data. 8 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. 9 doc/propbank/english-propbank.pdf lease. Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the Release 1.0 of the Proposition Bank. This functionality is part of the OntoNotes DB Tool.10 Word Sense Gold word sense annotation was supplied using sense numbers as specified in the O"
W11-1901,P05-1022,0,0.0274644,"coreference but that have been annotated for other layers. For training 10 11 http://cemantix.org/ontonotes.html It should be noted that word sense annotation in OntoNotes is note complete, so only some of the verbs and nouns have word sense tags specified. 10 Senses Lemmas 1 2 &gt;2 1,506 1,046 1,016 Table 6: Word sense polysemy over verb and noun lemmas in OntoNotes models for each of the layers, where feasible, we used all the data that we could for that layer from the training portion of the entire OntoNotes release. Parse Trees Predicted parse trees were produced using the Charniak parser (Charniak and Johnson, 2005).12 Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were appropriately extended. The parser was then re-trained on the training portion of the release 4.0 data using 10-fold crossvalidation. Table 5 shows the performance of the re-trained Charniak parser on the CoNLL-2011 test set. We did not get a chance to re-train the re-ranker, and since the stock re-ranker crashes when run on nbest parses containing NMLs, because it has no"
W11-1901,N07-1011,0,0.525918,"fined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In"
W11-1901,N07-1030,0,0.0811053,"to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress,"
W11-1901,N10-1061,0,0.393386,"y work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surfa"
W11-1901,N01-1008,0,0.21035,"Missing"
W11-1901,N06-2015,1,0.528696,"2009) devoted to joint learning of syntactic and semantic dependencies. A principle ingredient for joint learning is the presence of multiple layers of semantic information. One fundamental question still remains, and that is – what would it take to improve the state of the art in coreference resolution that has not been attempted so far? Many different algorithms have been tried in the past 15 years, but one thing that is still lacking is a corpus comprehensively tagged on a large scale with consistent, multiple layers of semantic information. One of the many goals of the OntoNotes project2 (Hovy et al., 2006; Weischedel et al., 2011) is to explore whether it can fill this void and help push the progress further – not only in coreference, but with the various layers of semantics that it tries to capture. As one of its layers, it has created a corpus for general anaphoric coreference that cov1 2 http://projects.ldc.upenn.edu/ace/data/ http://www.bbn.com/nlp/ontonotes 2 ers entities and events not limited to noun phrases or a limited set of entity types. A small portion of this corpus from the newswire and broadcast news genres (∼120k) was recently used for a S EM E VAL task (Recasens et al., 2010)."
W11-1901,H05-1004,0,0.943059,"Missing"
W11-1901,J93-2004,1,0.0615149,"Missing"
W11-1901,P00-1023,0,0.0803844,"partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collabor"
W11-1901,P10-1142,0,0.220993,"and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC). These corpora were tagged with coreferring entities identified by noun phrases in the text. The de facto standard datasets for current coreference studies are the MUC (Hirschman and Chin1 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1–27, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics chor, 1997; Chinchor, 2001; Chinchor and Sun"
W11-1901,J05-1004,1,0.333011,"Missing"
W11-1901,passonneau-2004-computing,0,0.00517084,"ing and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998). This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation – both of which seek to take information access technology to the next level – we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge – possibly linked to knowledge sources – and development of learning algorithms that would effectively incorporate them is a necessary next step towards improving"
W11-1901,W05-0311,0,0.0114861,", but represent small training and test sets. The ACE corpora, on the other hand, have much more annotation, but are restricted to a small subset of entities. They are also less consistent, in terms of inter-annotator agreement (ITA) (Hirschman et al., 1998). This lessens the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order for these to be most useful for language understanding applications such as question answering or distillation – both of which seek to take information access technology to the next level – we need more consistent annotation of larger amounts of broad coverage data for training better automatic techniques for entity and event identification. Identification and encoding of richer knowledge – possibly linked to knowledge sources – and development of learning algorithms that would effectively incorporate them is a necessary next step"
W11-1901,P09-5006,0,0.0282076,"Missing"
W11-1901,N06-1025,0,0.730558,"d-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gende"
W11-1901,D09-1101,0,0.522965,"annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques st"
W11-1901,W09-2411,0,0.155304,"Missing"
W11-1901,J01-4004,0,0.993465,"stering them into equivalence classes, has been well recognized in the natural language processing community. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it can require world knowledge which is not well-defined and partly owing to the lack of substantial annotated data. Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with using decision trees and hand-written rules. A systematic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of"
W11-1901,P09-1074,0,0.237943,"ry of evaluations on coreference tasks, variation in the evaluation criteria and in the training data used have made it difficult for researchers to be clear about the state of the art or to determine which particular areas require further attention. There are many different parameters involved in defining a coreference task. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is somewhat easier (Culotta et al., 2007). Given the space constraints, we refer the reader to Stoyanov et al. (2009) for a detailed treatment of the issue. Limitations in the size and scope of the available datasets have also constrained research progress. The MUC and ACE corpora are the two that have been used most for reporting comparative results, but they differ in the types of entities and coreference annotated. The ACE corpus is also one that evolved over a period of almost five years, with different incarnations of the task definition and different corpus cross-sections on which performance numbers have been reported, making it hard to untangle and interpret the results. The availability of the OntoN"
W11-1901,D07-1052,0,0.0160767,"ic study was then conducted using decision trees by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have been developed to push the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Various different knowledge sources from shallow semantics to encyclopedic knowledge are being exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Researchers continued finding novel ways of exploiting ontologies such as WordNet. Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entiti"
W11-1901,M95-1005,0,0.967872,"2011 coreference task are likely to be lower than for coref evaluations based on MUC, where the mention spans are specified in the input,17 or those based on ACE data, where an approximate match is often allowed based on the specified head of the NP mention. 4.5.1 Metrics As noted above, the choice of an evaluation metric for coreference has been a tricky issue and there does not appear to be any silver bullet approach that addresses all the concerns. Three metrics have been proposed for evaluating coreference performance over an unrestricted set of entity types: i) The link based MUC metric (Vilain et al., 1995), ii) The mention based B - CUBED metric (Bagga and Baldwin, 1998) and iii) The entity based CEAF (Constrained Entity Aligned F-measure) metric (Luo, 2005). Very recently BLANC (BiLateral Assessment of NounPhrase Coreference) measure (Recasens and Hovy, 17 2011) has been proposed as well. Each of the metric tries to address the shortcomings or biases of the earlier metrics. Given a set of key entities K, and a set of response entities R, with each entity comprising one or more mentions, each metric generates its variation of a precision and recall measure. The MUC measure if the oldest and mos"
W11-1901,W08-2121,0,\N,Missing
W11-1901,E06-2015,0,\N,Missing
W11-1901,D08-1067,0,\N,Missing
W11-1901,S10-1001,0,\N,Missing
W11-1901,doddington-etal-2004-automatic,1,\N,Missing
W11-1901,W04-2327,0,\N,Missing
W12-4501,W06-0609,0,0.0197938,"ion of the corpus is all newswire, this had no impact on it. However, for both Chinese and Arabic, since we remove trace tokens corresponding to dropped pronouns, all the other layers of annotation had to be remapped to the remaining sequence of tree tokens. Propositions The propositions in OntoNotes are PropBank-style semantic roles for English, Chinese and Arabic. Most of the verb predicates in the corpus have been annotated with their arguments. As part of the OntoNotes effort, some enhancements were made to the English PropBank and Treebank to make them synchronize better with each other (Babko-Malaya et al., 2006). One of the outcomes of this effort was that two types of LINKs that represent pragmatic coreference (LINK - PCR) and selec14 There is another phrase type — EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases by different speakers, so we decided not to remove that from the data. 15 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recal"
W12-4501,2011.mtsummit-papers.22,1,0.579383,"s that of number and gender. There are many different ways of predicting these values, with differing accuracies, so in order to ensure that participants in the closed track were working from the same data, thus allowing clearer algorithmic comparisons, we specified a particular table of number and gender predictions generated by Bergsma and Lin (2006), for use during both training and testing. Unfortunately neither Arabic, nor Chinese have comparable resources available that we could allow participants to use. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. 4.1.2 Open Track In addition to resources available in the closed track, in the open track, systems were allowed to use 7 There are a few instances of novel senses introduced in OntoNotes which were not present in WordNet, and so lack a mapping back to the WordNet senses Algorithm 1 Procedure used to create OntoNotes training, development and test partitions. Procedure: Generate Partitions(OntoNotes) returns Train, Dev, Test 1: Train ← ∅ 2: Dev ← ∅ 3: Test ← ∅ 4: for all Source ∈ OntoNotes do 5: if Source = Wall Street Journal then 6: Train ← Train ∪ S"
W12-4501,P06-1005,0,0.473873,"f WordNet senses, systems could also map from the predicted or gold-standard word senses to the sets of underlying WordNet senses. Another significant piece of knowledge that is particularly useful for coreference but that is not available in the layers of OntoNotes is that of number and gender. There are many different ways of predicting these values, with differing accuracies, so in order to ensure that participants in the closed track were working from the same data, thus allowing clearer algorithmic comparisons, we specified a particular table of number and gender predictions generated by Bergsma and Lin (2006), for use during both training and testing. Unfortunately neither Arabic, nor Chinese have comparable resources available that we could allow participants to use. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. 4.1.2 Open Track In addition to resources available in the closed track, in the open track, systems were allowed to use 7 There are a few instances of novel senses introduced in OntoNotes which were not present in WordNet, and so lack a mapping back to the WordNet senses Algorithm 1 Procedur"
W12-4501,W10-4305,0,0.0238143,"distribution of the participants by country and the participation by language and task type. 4.5.3 Scoring Metrics Implementation We used the same core scorer implementation25 that was used for the S EM E VAL-2010 task, and which implemented all the different metrics. There were a couple of modifications done to this scorer since then. 6 1. Only exact matches were considered correct. Previously, for S EM E VAL-2010 nonexact matches were judged partially correct with a 0.5 score if the heads were the same and the mention extent did not exceed the gold mention. 2. The modifications suggested by Cai and Strube (2010) have been incorporated in the scorer. Since there are differences in the version used for CoNLL and the one available on the download site, and it is possible that the latter would be revised in the future, we have archived the version of the scorer on the CoNLL-2012 task webpage.26 5 Participants A total of 41 different groups demonstrated interest in the shared task by registering on the task 25 26 http://www.lsi.upc.edu/∼esapena/downloads/index.php?id=3 http://conll.bbn.com/download/scorer.v4.tar.gz 20 Country Participants Brazil China Germany Italy Switzerland USA 1 8 3 1 1 2 Table 13: Pa"
W12-4501,W11-1907,0,0.144398,"ghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution"
W12-4501,P11-2037,0,0.0414211,"ghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution"
W12-4501,N01-1016,0,0.027844,"ents. As part of the OntoNotes effort, some enhancements were made to the English PropBank and Treebank to make them synchronize better with each other (Babko-Malaya et al., 2006). One of the outcomes of this effort was that two types of LINKs that represent pragmatic coreference (LINK - PCR) and selec14 There is another phrase type — EMBED in the telephone conversation genre which is similar to the EDITED phrase type, and sometimes identifies insertions, but sometimes contains logical continuation of phrases by different speakers, so we decided not to remove that from the data. 15 A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. tional preferences (LINK - SLC) were added to PropBank. More details can be found in the addendum to the PropBank guidelines16 in the OntoNotes v5.0 release. Since the community is not used to this representation which relies heavily on the trace structure in the Treebank which we are excluding, we decided to unfold the LINKs back to their original representation as in the PropBank 1.0 release. This functionality is part of the OntoNotes DB"
W12-4501,P05-1022,0,0.00733886,"have been annotated for other layers. For training models for each of the layers, where feasible, we used all the data that we could 16 17 doc/propbank/english-propbank.pdf http://cemantix.org/ontonotes.html 14 Layer English Chinese Arabic Verb Noun All Verb Noun Sense Inventories 2702 Frames 5672 2194 1335 763 150 20134 2743 111 532 Table 7: Number of senses defined for English, Chinese and Arabic in the OntoNotes v5.0 corpus. for that layer from the training portion of the entire OntoNotes v5.0 release. Parse Trees Predicted parse trees for English were produced using the Charniak parser18 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags, to Penn-style part of speech tags. We used the mapping that is included with the Arabic treebank. The predicted parses for the training portion of the data wer"
W12-4501,N07-1011,0,0.0270615,"d multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high interannotator agreement possibly by restricting the coreference annotating to phenomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation scenario with an official evaluation setup, and possibly"
W12-4501,N07-1030,0,0.0061787,"/mt/ three languages. As we will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used"
W12-4501,doddington-etal-2004-automatic,0,0.253776,"Uryupina University of Trento, 38123 Povo (TN) Italy Yuchen Zhang Brandeis University, Waltham, MA 02453 USA uryupina@gmail.com yuchenz@brandeis.edu Early work on corpus-based coreference resolution dates back to the mid-90s by McCarthy and Lenhert (1995) where they experimented with decision trees and hand-written rules. Corpora to support supervised learning of this task date back to the Message Understanding Conferences (MUC) (Hirschman and Chinchor, 1997; Chinchor, 2001; Chinchor and Sundheim, 2003). The de facto standard datasets for current coreference studies are the MUC and the ACE 1 (Doddington et al., 2004) corpora. These corpora were tagged with coreferring entities in the form of noun phrases in the text. The MUC corpora cover all noun phrases in text but are relatively small in size. The ACE corpora, on the other hand, cover much more data, but the annotation is restricted to a small subset of entities. Automatic identification of coreferring entities and events in text has been an uphill battle for several decades, partly because it is a problem that requires world knowledge to solve and word knowledge is hard to define, and partly owing to the lack of substantial annotated data. Aside from"
W12-4501,N10-1061,0,0.0387987,"liarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rath"
W12-4501,N01-1008,0,0.104449,"Missing"
W12-4501,N06-2015,0,0.586083,"enomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation scenario with an official evaluation setup, and possibly several ablation settings to capture the range of performance. This can then be used as a standard benchmark by the research community. (iii) Continue to improve learning algorithms that better incorporate world knowledge and jointly incorporate information from other layers of syntactic and semantic annotation to improve the state of the art. One of the many goals of the OntoNotes project2 (Hovy et al., 2006; Weischedel et al., 2011) 2 was to explore whether it could fill this void and help push the progress further — not only in coreference, but with the various layers of semantics that it tries to capture. As one of its layers, it has created a corpus for general anaphoric coreference that covers entities and events not limited to noun phrases or a subset of entity types. The coreference layer in OntoNotes constitutes just one part of a multilayered, integrated annotation of shallow semantic structures in text with high inter-annotator agreement. This addresses the first issue. In the language"
W12-4501,W11-1902,0,0.174907,"Missing"
W12-4501,H05-1004,0,0.90641,"Missing"
W12-4501,W04-1602,0,0.00799095,"Missing"
W12-4501,J93-2004,0,0.0513136,"Missing"
W12-4501,P00-1023,0,0.125028,"ist.gov/iad/mig/publications/ASRhistory/index.html http://www.itl.nist.gov/iad/mig/tests/mt/ three languages. As we will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaborative"
W12-4501,P10-1142,0,0.204917,"semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techniques still rely primarily on surface level features such as string match, proximity, and edit distance; syntactic features such as apposition; and shallow semantic features such as number, gender, named entities, semantic class, Hobbs’ distance, etc. Further research to reduce the knowledge gap is essential to take coreference resolution techniques to the next level. The OntoNotes project has created a large-scale corpus of accurate and integrated annotation of multiple levels of"
W12-4501,J05-1004,0,0.387003,"Missing"
W12-4501,palmer-etal-2008-pilot,0,0.00787827,"Missing"
W12-4501,passonneau-2004-computing,0,0.00467497,"high agreement which likely lessened the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and 1 http://projects.ldc.upenn.edu/ace/data/ 1 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order to take language understanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has ma"
W12-4501,N07-1051,0,0.00750508,"Number of senses defined for English, Chinese and Arabic in the OntoNotes v5.0 corpus. for that layer from the training portion of the entire OntoNotes v5.0 release. Parse Trees Predicted parse trees for English were produced using the Charniak parser18 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the NML tag that has recently been added to capture internal NP structure, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags, to Penn-style part of speech tags. We used the mapping that is included with the Arabic treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-fold for Arabic) cross-validation. The development and test parses were generated using a model trained on the entire training portion. We used OntoNotes v5.0 training data for training the Chinese and Arabic parser models, but the OntoNotes v4.0 subset of OntoNotes v5.0 data was used for training the English m"
W12-4501,W05-0311,0,0.0332271,"t equally annotatable with high agreement which likely lessened the reliability of statistical evidence in the form of lexical coverage and semantic relatedness that could be derived from the data and 1 http://projects.ldc.upenn.edu/ace/data/ 1 Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 1–40, c Jeju Island, Korea, July 13, 2012. 2012 Association for Computational Linguistics used by a classifier to generate better predictive models. The importance of a well-defined tagging scheme and consistent ITA has been well recognized and studied in the past (Poesio, 2004; Poesio and Artstein, 2005; Passonneau, 2004). There is a growing consensus that in order to take language understanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation sce"
W12-4501,P09-5006,0,0.0290366,"Missing"
W12-4501,N06-1025,0,0.0301668,"tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress,"
W12-4501,D10-1048,0,0.0903952,"tem uses feature templates defined on mention pairs. bj¨orkelund mentions that disallowing transitive closures gave performance improvement of 0.6 and 0.4 respectively for English and Chinese/Arabic. bj¨orkelund also mentions seeing a considerable increase in performance after adding features that correspond to the Shortest Edit Script (Myers, 1986) between surface forms and unvocalised Buckwalter forms, respectively. These could be better at capturing the differences in gender and number signaled by certain morphemes than hand-crafted rules. chen built upon the sieve architecture proposed in Raghunathan et al. (2010) and added one more sieve — head match — for Chinese and modified two sieves. Some participants tried to incorporate peculiarities of the corpus in their systems. For example, martschat excluded adjectival nation names. Unlike English, and especially in absence of an external resource, it is hard to make a gender distinction in Arabic and Chinese. martschat used the information 23 that 先 生(sir) and 女士(lady) often suggest gender information. bo and martschat used plurality markers 们 to identify plurals. For example, 同学 (student) is singular and 同学们 (students) is plural. bo also uses a heuristic"
W12-4501,D09-1101,0,0.142437,"will see later, peculiarities of each of these languages had to be considered in creating the evaluation framework. 2 The OntoNotes Corpus The first systematic learning-based study in coreference resolution was conducted on the MUC corpora, using a decision tree learner, by Soon et al. (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorith"
W12-4501,W09-2411,0,0.101275,"Missing"
W12-4501,J01-4004,0,0.987173,"plex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high interannotator agreement possibly by restricting the coreference annotating to phenomena that can be annotated with high consistency, and covering an unrestricted set of entities and events; and (ii) Create a standard evaluation sc"
W12-4501,P09-1074,0,0.0393009,"rstanding applications such as question answering or distillation to the next level, we need more consistent annotation for larger amounts of broad coverage data to train better automatic models for entity and event detection. (iii) Complex evaluation with multiple evaluation metrics and multiple evaluation scenarios, complicated with varying training and test partitions, led to situations where many researchers report results with only one or a few of the available metrics and under a subset of evaluation scenarios. This has made it hard to gauge the improvement in algorithms over the years (Stoyanov et al., 2009), or to determine which particular areas require further attention. Looking at various numbers reported in literature can greatly affect the perceived difficulty of the task. It can seem to be a very hard problem (Soon et al., 2001) or one that is relatively easy (Culotta et al., 2007). (iv) the knowledge bottleneck which has been a well-accepted ceiling that has kept the progress in this task at bay. These issues suggest that the following steps might take the community in the right direction towards improving the state of the art in coreference resolution: (i) Create a large corpus with high"
W12-4501,P08-4003,1,0.780554,"P and based pruning; Genre specific selected NE in Arabic. Learning to prune non-referential mentions models NP, PRP and PRP $ in all languages; PN in Logistic Regression Chinese; all NE in English. Exclude pleonastic (LIBLINEAR) it in English. Prune smaller mentions with same head. Eight different mention types for English, and Directed multigraph adjectival use for nations and a few NEs are representation where the filtered as well as embedded mentions and weights are learned over the pleonastic pronouns. Four mention types in training data (on top of BART Chinese. Copulas are also handled (Versley et al., 2008)) appropriately. Latent Structure Learning All noun phrases, pronouns and name entities modification of BART using multi-objective optimization. Standard rules for English and Classifier to Domain specific classifiers for identify markable NPs in Chinese and Arabic. nw and bc genre. Memory based learning NP, PRP and PRP $ in English, and all NP in (T I MBL) Chinese and Arabic. Singleton classifier. All phrase types that are mentions in training MaxEnt are considered as mentions and a classifier is trained to identify potential mentions. C4.5 and deterministic rules All noun phrases, pronouns a"
W12-4501,D07-1052,0,0.0121409,". (2001). Significant improvements have been made in the field of language processing in general, and improved learning techniques have pushed the state of the art in coreference resolution forward (Morton, 2000; Harabagiu et al., 2001; McCallum and Wellner, 2004; Culotta et al., 2007; Denis and Baldridge, 2007; Rahman and Ng, 2009; Haghighi and Klein, 2010). Researchers have continued to find novel ways of exploiting ontologies such as WordNet. Various knowledge sources from shallow semantics to encyclopedic knowledge have been exploited (Ponzetto and Strube, 2005; Ponzetto and Strube, 2006; Versley, 2007; Ng, 2007). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). More recently researchers have used graph based algorithms (Cai et al., 2011a) rather than pair-wise classifications. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. In spite of all the progress, current techni"
W12-4501,M95-1005,0,0.953302,"Missing"
W12-4501,J08-2004,1,0.444195,"agging where The NULL arguments are first filtered out, and the remaining NON - NULL arguments are classified into one of the argument types. The argument identification module used an ensemble of ten classifiers — each trained on a tenth of the training data and combined using unweighted voting. This should still give a close to state-of-theart performance given that the argument identification performance tends to start to be asymptotic around 10K training instances (Pradhan et al., 2005). The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on all the training portion of the OntoNotes v5.0 data. No propositional structures were provided for Arabic due to resource constraints. Table 9 shows the detailed performance numbers. The CoNLL-2005 scorer was used to compute the scores. At first glance, the performance on the English newswire genre is much lower than what has been reported for WSJ Section 23. This could be attributed to several factors: i) the fact that we had to compromise on the training method, ii) the newswire in OntoNotes not only contains WSJ data, but also Xinhua news, iii) The WSJ training and test porti"
W12-4501,C10-2158,1,0.696855,"Missing"
W12-4501,W10-1836,1,0.72614,"Missing"
W12-4501,P10-4014,0,0.0186234,"44 74.99 76.19 Table 6: Parser performance on the CoNLL-2012 test set. R English Broadcast Conversation [BC] Broadcast News [BN] Magazine [MZ] Newswire [NW] Weblogs and Newsgroups [WB] Overall 81.3 81.5 78.8 85.7 77.6 81.2 82.0 79.1 85.7 77.5 81.2 81.7 79.0 85.7 77.5 82.5 82.5 82.5 Chinese Broadcast Conversation [BC] Broadcast News [BN] Magazine [MZ] Newswire [NW] Overall Arabic Accuracy P F Newswire [NW]19 - - 80.5 85.4 82.4 89.1 - - 84.3 75.2 75.9 75.6 Table 8: Word sense performance over both verbs and nouns in the CoNLL-2012 test set. Word Sense This year we used the IMS (It Makes Sense) (Zhong and Ng, 2010) word sense tagger.20 Word sense information, unlike syntactic parse information is not central to approaches taken by current coreference systems and so we decided to use a better word sense tagger to get a good state of the art accuracy estimate, at the cost of a completely fair (but, still close enough) comparison with English CoNLL-2011 results. This will also allow potential future uses to benefit from it. IMS was trained on all the word sense data that is present in the training portion of the OntoNotes corpus using crossvalidated predictions on the input layers similar to the propositio"
W12-4501,W08-2121,0,\N,Missing
W12-4501,E06-2015,0,\N,Missing
W12-4501,D08-1067,0,\N,Missing
W12-4501,W09-1201,1,\N,Missing
W12-4501,S10-1001,0,\N,Missing
W13-1903,W12-2404,0,0.0820558,"tion for this task. 1 In this work we focus on extracting a particular temporal relation, C ONTAINS, that holds between a time expression and an event expression. This level of representation is based on the computational discourse model of narrative containers (Pustejovsky and Stubbs, 2011), which are time expressions or events which are central to a section of a text, usually manifested by being relative hubs of temporal relation links. We argue that containment relations are useful as an intermediate level of granularity between full temporal relation extraction and “coarse” temporal bins (Raghavan et al., 2012) like before admission, on admission, and after admission. Correctly extracting C ON TAINS relations will, for example, allow for more accurate placement of events on a timeline, to the resolution possible by the number of time expressions in the document. We suspect that this finer grained information will also be more useful for downstream applications like coreference, for which coarse information was found to be useful. The approach we develop is a supervised machine Introduction Clinical narratives are a rich source of unstructured information that hold great potential for impacting clini"
W13-1903,strassel-etal-2010-darpa,0,0.107061,"Missing"
W13-1903,tannier-muller-2008-evaluation,0,0.0173461,"he same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able to give credit for non-redundant, nonminimum links (Cherry et al., 2013)"
W13-1903,P11-2061,0,0.0184812,"t is possible to define the same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able to give credit for non-redundant, nonminimum li"
W13-1903,S07-1014,0,0.206982,"Missing"
W13-1903,N06-1037,0,0.0278824,"lt for using constituency structure in tree kernels for temporal relation extraction. The Path Tree representation uses a sub-tree of the whole constituent tree, but removes all nodes that are not along the path between the two arguments. Path information has been used in standard feature kernels (Pradhan et al., 2008), with each individual path being a possible boolean feature. The Path-Enclosed Tree representation is based on the smallest sub-tree that encloses the two proposed arguments. This is a representation that has shown value in other work using tree kernels for relation extraction (Zhang et al., 2006; Mirroshandel et al., 2009). The information contained in the PET representation is a superset of that contained in the Path Tree representation, since it includes the full path between arguments as well as the structure between arguments and the argument text. This means that it can take into account path information while also considering constituent structure between arguments that may play a role in determining whether the two arguments are related. For example, temporal cue words like after or during may occur between arguments and will not be captured by Path Trees. Like the PT represen"
W13-1903,S07-1025,1,0.869954,"ata, annotated the I NCLUDES relation, but merged it with other relations for the evaluation due to low inter-annotator agreement. Since no narrative container-annotated corpora exist, there are also no existing models for extracting narrative container relations. However, we can draw on the various methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, includ"
W13-1903,E12-1019,0,0.205712,"Missing"
W13-1903,S10-1063,0,0.0415784,"ES relation, but merged it with other relations for the evaluation due to low inter-annotator agreement. Since no narrative container-annotated corpora exist, there are also no existing models for extracting narrative container relations. However, we can draw on the various methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, including only tokenbased inf"
W13-1903,Y09-1038,0,0.428527,"methods applied to related temporal relation tasks. Most relevant is the work on linking events to timestamps. This was one of the subtasks in TempEval 2007 and 2010, and systems used a variety of features including words, part-of-speech tags, and the syntactic path between the event and the time (Bethard and Martin, 2007; Llorens et al., 2010). Syntactic path features were also used in the 2012 i2b2 Challenge, where they provided gains especially for intra-sentential temporal links (Xu et al., 2013). Recent research has also looked to syntactic tree kernels for temporal relation extraction. Mirroshandel et al. (2009) used a path-enclosed tree (i.e., selecting only the sub-tree containing the event and time), and used various weighting scheme variants of this approach on the TimeBank (Pustejovsky et al., 2003a) and Opinion4 corpora. Hovy et al. (2012) used a flat tree structure for each event-time pair, including only tokenbased information (words, part of speech tags) between the event and time, and found that adding such tree kernels on top of a baseline set of features improved event-time linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). While Mirroshandel et"
W13-1903,C04-1008,0,0.0380021,"ial properties of temporal relations, because it is possible to define the same set of relations using different set of axioms. To take a very simple example, given a gold set of relations A<B and B<C, and given the system output A<B, A<C and B<C, if one were to compute a plain precision/recall metric, then the axiom A<C would be counted against the system, when one can easily infer from the gold set of relations that it is indeed correct. With more relations the inference process becomes more complex. Recently there has been some work trying to address the shortcomings of the plain F1 score (Muller and Tannier, 2004; Setzer et al., 2006; UzZaman and Allen, 2011; Tannier and Muller, 2008; Tannier and Muller, 2011). However, the community has not yet come to a consensus on the best evaluation approach. Two recent evaluations, TempEval-3 (UzZaman et al., 2013) and the 2012 i2b2 Challenge (Sun et al., 2013), used an implementation of the proposal by (UzZaman and Allen, 2011). However, as described in Cherry et al. (2013), this algorithm, which uses a greedy graph minimization approach, is sensitive to the order in which the temporal relations are presented to the scorer. In addition, the scorer is not able t"
W13-1903,J08-2006,1,0.41968,"of unrelated structure that adds noise to the classifier. Here we include it in an attempt to get to the root of an apparent discrepancy in the tree kernel literature, as explained in Section 2, in which Hovy et al. (2012) report a negative result and Mirroshandel et al. (2009) report a positive result for using constituency structure in tree kernels for temporal relation extraction. The Path Tree representation uses a sub-tree of the whole constituent tree, but removes all nodes that are not along the path between the two arguments. Path information has been used in standard feature kernels (Pradhan et al., 2008), with each individual path being a possible boolean feature. The Path-Enclosed Tree representation is based on the smallest sub-tree that encloses the two proposed arguments. This is a representation that has shown value in other work using tree kernels for relation extraction (Zhang et al., 2006; Mirroshandel et al., 2009). The information contained in the PET representation is a superset of that contained in the Path Tree representation, since it includes the full path between arguments as well as the structure between arguments and the argument text. This means that it can take into accoun"
W13-1903,W11-0419,0,0.0857915,"are related by narrative containers. We use support vector machines with composite kernels, which allows for integrating standard feature kernels with tree kernels for representing structured features such as constituency trees. Our experiments show that using tree kernels in addition to standard feature kernels improves F1 classification for this task. 1 In this work we focus on extracting a particular temporal relation, C ONTAINS, that holds between a time expression and an event expression. This level of representation is based on the computational discourse model of narrative containers (Pustejovsky and Stubbs, 2011), which are time expressions or events which are central to a section of a text, usually manifested by being relative hubs of temporal relation links. We argue that containment relations are useful as an intermediate level of granularity between full temporal relation extraction and “coarse” temporal bins (Raghavan et al., 2012) like before admission, on admission, and after admission. Correctly extracting C ON TAINS relations will, for example, allow for more accurate placement of events on a timeline, to the resolution possible by the number of time expressions in the document. We suspect th"
W13-1903,N07-1070,1,\N,Missing
W13-1903,S10-1010,0,\N,Missing
W13-1903,S13-2001,0,\N,Missing
W13-3516,P05-1022,0,0.0426867,"umption. For the spoken genres – BC, BN and TC – we use the manual transcriptions rather than the output of a speech recognizer, as would be the case in real world. The performance on various layers for these genres would therefore be artificially inflated, and should be taken into account while analyzing results. Not many studies have previously reported on syntactic and semantic analysis for spoken genre. Favre et al. (2010) report the performance on the English subset of an earlier version of OntoNotes. 4.1 Syntax Predicted parse trees for English were produced using the Charniak parser11 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the nominal (NML) tag, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags to Penn-style part of speech tags. We used the mapping that is included with the Arabic Treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-folds for Arabic) cross-"
W13-3516,P08-1091,1,0.914176,"in the Treebank and (ii) we decided to exclude, we unfold the LINKs back to their original representation as in the PropBank 1.0 release. We used ASSERT15 (Pradhan et al., 2005) to predict the propositional structure for English. We made a small modification to ASSERT, and replaced the TinySVM classifier with a CRF16 to speed up training the model on all the data. The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on the OntoNotes v5.0 data. The Arabic propositional structure was predicted using the system described in Diab et al. (2008). (Diab et al., 2008) Table 5 shows the detailed per14 The Frame ID column indicates the F-score for English and Arabic, and accuracy for Chinese for the same reasons as word sense. 15 16 http://cemantix.org/assert.html http://leon.bottou.org/projects/sgd Frame Total ID Sent. English BC BN MZ NW TC WB PT Overall Chinese BC BN MZ NW TC WB Arabic Overall NW Total Prop. 93.2 1994 5806 92.7 1218 4166 90.8 740 2655 92.8 2122 6930 91.8 837 1718 90.7 1139 2751 96.6 1208 2849 92.8 9,261 26,882 87.7 885 2,323 93.3 929 4,419 92.3 451 2,620 96.6 481 2,210 82.2 968 1,622 87.8 758 1,761 90.9 4,472 14,955 8"
W13-3516,W00-1322,0,0.0291123,"Missing"
W13-3516,W06-0609,0,0.0675644,"and some noun instances, partial verb and noun word senses, coreference, and named entities. Table 1 gives an overview of the number of documents that have been annotated in the entire OntoNotes corpus. 2.1 Layers of Annotation This section provides a very concise overview of the various layers of annotations in OntoNotes. For a more detailed description, the reader is referred to (Weischedel et al., 2011) and the documentation accompanying the v5.04 release. 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank (Marcus et al., 1993; Babko-Malaya et al., 2006), the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004). There were two updates made to the parse trees as part of the OntoNotes project: i) the introduction of NML phrases, in the English portion, to mark nominal sub-constituents of flat NPs that do not follow the default right-branching structure, and ii) re-tokenization of hyphenated tokens into multiple tokens in English and Chinese. The Arabic Treebank on the other hand was also significantly revised in an effort to increase consistency. 2.1.2 Word Sense Coarse-grained word senses are tagged for the mos"
W13-3516,2011.mtsummit-papers.22,1,0.659518,"reference decisions are made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities that were produced in the earlier sections. Each document part from the documents that were split into multiple parts during coreference annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic informatio"
W13-3516,P06-1005,0,0.0274234,"able. 4.5 Coreference The task is to automatically identify mentions of entities and events in text and to link the coreferring mentions together to form entity/event chains. The coreference decisions are made using automatically predicted information on other structural and semantic layers including the parses, semantic roles, word senses, and named entities that were produced in the earlier sections. Each document part from the documents that were split into multiple parts during coreference annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features i"
W13-3516,W12-4503,1,0.323243,"Missing"
W13-3516,W11-1905,1,0.8623,"Missing"
W13-3516,W10-4305,0,0.00957321,"thm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic information. Recently, it was discovered that there is possibly a bug in the official scorer used for the CoNLL 2011/2012 and the SemEval 2010 coreference tasks. This relates to the mis-implementation of the method proposed by (Cai and Strube, 2010) for scoring predicted mentions. This issue has also been recently reported in Recasens et al., (2013). As of this writing, the BCUBED metric has been fixed, and the correctness of the CEAFm , CEAFe and BLANC metrics is being verified. We will be updating the CoNLL shared task webpages22 with more detailed information and also release the patched scripts as soon as they are available. We will also re-generate the scores for previous shared tasks, and the coreference layer in this paper and make them available along with the models and system outputs for other layers. Table 7 shows the performa"
W13-3516,P11-2037,0,0.0435593,"tructure provided by the PropBank layer. Whereas in English, most traces represent syntactic phenomena such as movement and raising, in Chinese and Arabic, they can also represent dropped subjects/objects. These subset of traces directly affect the coreference layer, since, unlike English, traces in Chinese and Arabic (*pro* and * respectively) are legitimate targets of mentions and are considered for coreference annotation in OntoNotes. Recovering traces in text is a hard problem, and the most recently reported numbers in literature for Chinese are around a F-score of 50 (Yang and Xue, 2010; Cai et al., 2011). For Arabic there have not been much studies on recovering these. A study by Gabbard (2010) shows that these can be recovered with an F-score of 55 with automatic parses and roughly 65 using gold parses. Considering the low level of prediction accuracy of these tokens, and their relative low frequency, we decided to consider predicting traces in trees out of the scope of this study. In other words, we removed the manually identified traces and function tags from the Treebanks across all three languages, in all the three – training, development and test partitions. This meant removing any and"
W13-3516,W05-0620,0,0.0226073,"Missing"
W13-3516,P05-1045,0,0.0391503,"of the CEAFm , CEAFe and BLANC metrics is being verified. We will be updating the CoNLL shared task webpages22 with more detailed information and also release the patched scripts as soon as they are available. We will also re-generate the scores for previous shared tasks, and the coreference layer in this paper and make them available along with the models and system outputs for other layers. Table 7 shows the performance of the system on the Table 6: Performance of the named entity recognizer on the CoNLL-2012 test set. 4.4 Named Entities We retrained the Stanford named entity recognizer20 (Finkel et al., 2005) on the OntoNotes data. Table 6 shows the performance details for all the languages across all 18 name types broken down by genre. In English, BN has the highest performance followed by the NW genre. There is a significant drop from those and the TC and WB genre. Somewhat similar trend is observed in the Chinese data, with Arabic having the lowest scores. Since the Pivot Text portion (PT) of OntoNotes was not tagged with names, we could not compute the accuracy for that cross-section of the data. Previously Finkel and Manning (2009) performed 17 The number of sentences in this table are a subs"
W13-3516,W01-0521,0,0.0213903,"ver the sense inventories (and frame files) are defined per lemma – independent of the part of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, develo"
W13-3516,hockenmaier-steedman-2002-acquiring,0,0.0152391,"g5 1 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA 2 University of Trento, University of Trento, 38123 Povo (TN), Italy 3 QCRI, Qatar Foundation, 5825 Doha, Qatar 4 Brandeis University, Brandeis University, Waltham, MA 02453, USA 5 National University of Singapore, Singapore, 117417 6 University of Stuttgart, 70174 Stuttgart, Germany Abstract the Penn Discourse Treebank (Prasad et al., 2008), and many other annotation projects, all annotate the same underlying body of text. It was also converted to dependency structures and other syntactic formalisms such as CCG (Hockenmaier and Steedman, 2002) and LTAG (Shen et al., 2008), thereby creating an even bigger impact through these additional syntactic resources. The most recent one of these efforts is the OntoNotes corpus (Weischedel et al., 2011). However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages — Chinese and Arabic: portions of the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004) have been used to sample the genre of text that they represent. One of the cu"
W13-3516,W04-1602,0,0.0421028,"d to dependency structures and other syntactic formalisms such as CCG (Hockenmaier and Steedman, 2002) and LTAG (Shen et al., 2008), thereby creating an even bigger impact through these additional syntactic resources. The most recent one of these efforts is the OntoNotes corpus (Weischedel et al., 2011). However, unlike the previous extensions of the Treebank, in addition to using roughly a third of the same WSJ subcorpus, OntoNotes also added several other genres, and covers two other languages — Chinese and Arabic: portions of the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004) have been used to sample the genre of text that they represent. One of the current hurdles in language processing is the problem of domain, or genre adaptation. Although genre or domain are popular terms, their definitions are still vague. In OntoNotes, “genre” means a type of source – newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), telephone conversation (TC), web data (WB) or pivot text (PT). Changes in the entity and event profiles across source types, and even in the same source over a time duration, as explicitly expressed by surface lexical forms, usually"
W13-3516,J93-2004,0,0.0664902,"sitions for most verb and some noun instances, partial verb and noun word senses, coreference, and named entities. Table 1 gives an overview of the number of documents that have been annotated in the entire OntoNotes corpus. 2.1 Layers of Annotation This section provides a very concise overview of the various layers of annotations in OntoNotes. For a more detailed description, the reader is referred to (Weischedel et al., 2011) and the documentation accompanying the v5.04 release. 2.1.1 Syntax This represents the layer of syntactic annotation based on revised guidelines for the Penn Treebank (Marcus et al., 1993; Babko-Malaya et al., 2006), the Chinese Treebank (Xue et al., 2005) and the Arabic Treebank (Maamouri and Bies, 2004). There were two updates made to the parse trees as part of the OntoNotes project: i) the introduction of NML phrases, in the English portion, to mark nominal sub-constituents of flat NPs that do not follow the default right-branching structure, and ii) re-tokenization of hyphenated tokens into multiple tokens in English and Chinese. The Arabic Treebank on the other hand was also significantly revised in an effort to increase consistency. 2.1.2 Word Sense Coarse-grained word s"
W13-3516,N06-1020,0,0.0154191,"inventories (and frame files) are defined per lemma – independent of the part of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so a"
W13-3516,P02-1014,0,0.0344201,"ce annotation were treated as separate document. We used the number and gender predictions generated by Bergsma and Lin (2006). Unfortunately neither Arabic, nor Chinese have comparable data available. Chinese, in particular, does not have number or gender inflections for nouns, but (Baran and Xue, 2011) look at a way to infer such information. We trained the Bj¨orkelund and Farkas (2012) coreference system21 which uses a combination of two pair-wise resolvers, the first is an incremental chain-based resolution algorithm (Bj¨orkelund and Farkas, 2012), and the second is a best-first resolver (Ng and Cardie, 2002). The two resolvers are combined by stacking, i.e., the output of the first resolver is used as features in the second one. The system uses a large feature set tailored for each language which, in addition to classic coreference features, includes both lexical and syntactic information. Recently, it was discovered that there is possibly a bug in the official scorer used for the CoNLL 2011/2012 and the SemEval 2010 coreference tasks. This relates to the mis-implementation of the method proposed by (Cai and Strube, 2010) for scoring predicted mentions. This issue has also been recently reported"
W13-3516,J05-1004,0,0.0531944,"rformance. 1 Introduction Roughly a million words of text from the Wall Street Journal newswire (WSJ), circa 1989, has had a significant impact on research in the language processing community — especially those in the area of syntax and (shallow) semantics, the reason for this being the seminal impact of the Penn Treebank project which first selected this text for annotation. Taking advantage of a solid syntactic foundation, later researchers who wanted to annotate semantic phenomena on a relatively large scale, also used it as the basis of their annotation. For example the Proposition Bank (Palmer et al., 2005), BBN Name Entity and Pronoun coreference corpus (Weischedel and Brunstein, 2005), 1 A portion of the English data in the OntoNotes corpus is a selected set of sentences that were annotated for parse and word sense information. These sentences are present in a document of their own, and so the documents for parse layers for English are inflated by about 3655 documents and for the word sense are inflated by about 8797 documents. 143 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143–152, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Comput"
W13-3516,palmer-etal-2008-pilot,0,0.0620251,"Missing"
W13-3516,N07-1051,0,0.0519693,"ile analyzing results. Not many studies have previously reported on syntactic and semantic analysis for spoken genre. Favre et al. (2010) report the performance on the English subset of an earlier version of OntoNotes. 4.1 Syntax Predicted parse trees for English were produced using the Charniak parser11 (Charniak and Johnson, 2005). Some additional tag types used in the OntoNotes trees were added to the parser’s tagset, including the nominal (NML) tag, and the rules used to determine head words were extended correspondingly. Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). In the case of Arabic, the parsing community uses a mapping from rich Arabic part of speech tags to Penn-style part of speech tags. We used the mapping that is included with the Arabic Treebank. The predicted parses for the training portion of the data were generated using 10-fold (5-folds for Arabic) cross-validation. For testing, we used a model trained on the entire training portion. Table 3 shows the precision, recall and F1 -scores of the re-trained parsers on the CoNLL-2012 test along with the part of speech accuracies (POS) using the standard evalb scorer. The performance on the PT ge"
W13-3516,J08-2006,1,0.738119,"of speech realized in the text. and telephone conversation genre — are very long which prohibited efficient annotation in their entirety. These are split into smaller parts, and each part is considered a separate document for the sake of coreference evaluation. 3 Given the scope of the corpus and the multitude of settings one can run evaluations, we had to restrict this study to a relatively focused subset. There has already been evidence of models trained on WSJ doing poorly on non-WSJ data on parses (Gildea, 2001; McClosky et al., 2006), semantic role labeling (Carreras and M`arquez, 2005; Pradhan et al., 2008), word sense (Escudero et al., 2000; ?), and named entities. The phenomenon of coreference is somewhat of an outlier. The winning system in the CoNLL-2011 shared task was one that was completely rule-based and not directly trained on the OntoNotes corpus. Given this overwhelming evidence, we decided not to focus on potentially complex cross-genre evaluations. Instead, we decided on evaluating the performance on each layer of annotation using an appropriately selected, stratified training, development and test set, so as to facilitate future studies. 2.1.3 Proposition The propositions in OntoNo"
W13-3516,W11-1901,1,0.408033,"Missing"
W13-3516,W12-4501,1,0.81236,"he exact same phenomena have been annotated on a broad cross-section of the same language before OntoNotes. The OntoNotes corpus thus provides an opportunity for studying the genre effect on different syntactic, semantic and discourse analyzers. Parts of the OntoNotes Corpus have been used for various shared tasks organized by the language processing community. The word sense layer was the subject of prediction in two SemEval-2007 tasks, and the coreference layer was the subject of prediction in the SemEval-20102 (Recasens et al., 2010), CoNLL-2011 and 2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012). The CoNLL-2012 shared task provided predicted information to the participants, however, that did not include a few layers such as the named entities for Chinese and Arabic, propositions for Arabic, and for better comparison of the English data with the CoNLL-2011 task, a smaller OntoNotes v4.0 portion of the English parse and propositions was used for training. This paper is a first attempt at presenting a coherent high-level picture of the performance of various publicly available state-of-the-art tools on all the layers of OntoNotes in all three languages, so as to pave the way for further"
W13-3516,prasad-etal-2008-penn,0,0.0477485,"Missing"
W13-3516,S10-1001,0,0.0530331,"Missing"
W13-3516,N13-1071,0,0.0213813,"Missing"
W13-3516,J08-2004,1,0.227032,"nd LINK - PCR. Since the community is not used to the new PropBank representation which (i) relies heavily on the trace structure in the Treebank and (ii) we decided to exclude, we unfold the LINKs back to their original representation as in the PropBank 1.0 release. We used ASSERT15 (Pradhan et al., 2005) to predict the propositional structure for English. We made a small modification to ASSERT, and replaced the TinySVM classifier with a CRF16 to speed up training the model on all the data. The Chinese propositional structure was predicted with the Chinese semantic role labeler described in (Xue, 2008), retrained on the OntoNotes v5.0 data. The Arabic propositional structure was predicted using the system described in Diab et al. (2008). (Diab et al., 2008) Table 5 shows the detailed per14 The Frame ID column indicates the F-score for English and Arabic, and accuracy for Chinese for the same reasons as word sense. 15 16 http://cemantix.org/assert.html http://leon.bottou.org/projects/sgd Frame Total ID Sent. English BC BN MZ NW TC WB PT Overall Chinese BC BN MZ NW TC WB Arabic Overall NW Total Prop. 93.2 1994 5806 92.7 1218 4166 90.8 740 2655 92.8 2122 6930 91.8 837 1718 90.7 1139 2751 96.6"
W13-3516,C10-2158,1,0.243983,"nd the proposition structure provided by the PropBank layer. Whereas in English, most traces represent syntactic phenomena such as movement and raising, in Chinese and Arabic, they can also represent dropped subjects/objects. These subset of traces directly affect the coreference layer, since, unlike English, traces in Chinese and Arabic (*pro* and * respectively) are legitimate targets of mentions and are considered for coreference annotation in OntoNotes. Recovering traces in text is a hard problem, and the most recently reported numbers in literature for Chinese are around a F-score of 50 (Yang and Xue, 2010; Cai et al., 2011). For Arabic there have not been much studies on recovering these. A study by Gabbard (2010) shows that these can be recovered with an F-score of 55 with automatic parses and roughly 65 using gold parses. Considering the low level of prediction accuracy of these tokens, and their relative low frequency, we decided to consider predicting traces in trees out of the scope of this study. In other words, we removed the manually identified traces and function tags from the Treebanks across all three languages, in all the three – training, development and test partitions. This mean"
W13-3516,W10-1836,1,0.361229,"Missing"
W13-3516,P10-4014,1,0.579474,"Missing"
W13-3516,D08-1105,1,0.881098,"Missing"
W13-3516,N01-1016,0,\N,Missing
W13-3516,N07-1070,1,\N,Missing
W13-3516,N09-1037,0,\N,Missing
W15-2707,W05-0305,1,0.799224,"Missing"
W15-2707,D12-1083,0,0.012738,"(3) Now, we regard this as a largely phony issue, but the “long term” is nonetheless a big salon topic all around the Beltway. (5) PropBank: Verb = suspend Arg0 = The federal government Arg1 = sales of U.S. savings bonds ARGM-CAU = because Congress hasn’t lifted the ceiling on government debt (4) The U.S. wants the removal of . . .barriers to investment; Japan denies there are real barriers. Researchers working on discourse parsing have commented that intra-sentential (intra-S) discourse relations are, in general, easier to recognize than ones whose arguments are found in separate sentences (Joty et al., 2012; Lin et al., (6) PDTB: Connective = because Arg1 = The federal government suspended sales of U.S. savings bonds Arg2 = Congress hasn’t lifted the ceiling on government debt Sense = Contingency.Cause.Reason 64 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 64–69, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. ARGM - ADV (2235) ARGM - CAU (657) ARGM - TMP (2503) ARGM - PNC (66) ARGM - MNR (13) TOTAL (5475) TEMPORAL CONTINGENCY COMPARISON EXPANSION TOTAL 222 14 2258 0 0 2494 1067 650 523"
W15-2707,J93-2004,0,0.0504287,"Missing"
W15-2707,W04-2705,0,0.14572,"Missing"
W15-2707,J05-1004,0,0.340826,"mproving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clause annotated as ARGM and assigned the role CAU (causal). The PDTB annotation for the same example, shown in (6), marks because as the connective, ‘Contingency.Cause.Reason’ as the sense, the adjunct clause as Arg2 (defined as the argument attached to the connective), and the matrix clause as Arg1 (defined as the non-Arg2 argument)."
W15-2707,N04-1030,1,0.764484,"Missing"
W15-2707,W13-3516,1,0.897991,"Missing"
W15-2707,prasad-etal-2008-penn,1,0.826615,"bber@ed.ac.uk 3 Institute for Research in Cognitive Science, University of Pennsylvania {aleewk,joshi}@seas.upenn.edu 4 Boulder Language Technologies pradhan@bltek.com Abstract 2012; Feng, 2014). They are also quite useful in Language Technology applications that exploit sentence-level relations. Thus, there is particular value in improving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clau"
W15-2707,C10-2118,1,0.939227,"Missing"
W15-2707,J14-4007,1,0.857151,"leewk,joshi}@seas.upenn.edu 4 Boulder Language Technologies pradhan@bltek.com Abstract 2012; Feng, 2014). They are also quite useful in Language Technology applications that exploit sentence-level relations. Thus, there is particular value in improving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clause annotated as ARGM and assigned the role CAU (causal). The PDTB annotation for the same"
W15-2707,N07-1069,0,0.060601,"Missing"
