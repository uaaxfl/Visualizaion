2021.wnut-1.5,Fine-grained Temporal Relation Extraction with Ordered-Neuron {LSTM} and Graph Convolutional Networks,2021,-1,-1,2,0,116,minh phu,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Fine-grained temporal relation extraction (FineTempRel) aims to recognize the durations and timeline of event mentions in text. A missing part in the current deep learning models for FineTempRel is their failure to exploit the syntactic structures of the input sentences to enrich the representation vectors. In this work, we propose to fill this gap by introducing novel methods to integrate the syntactic structures into the deep learning models for FineTempRel. The proposed model focuses on two types of syntactic information from the dependency trees, i.e., the syntax-based importance scores for representation learning of the words and the syntactic connections to identify important context words for the event mentions. We also present two novel techniques to facilitate the knowledge transfer between the subtasks of FineTempRel, leading to a novel model with the state-of-the-art performance for this task."
2021.wanlp-1.27,Improving Cross-Lingual Transfer for Event Argument Extraction with Language-Universal Sentence Structures,2021,-1,-1,1,1,117,minh nguyen,Proceedings of the Sixth Arabic Natural Language Processing Workshop,0,"We study the problem of Cross-lingual Event Argument Extraction (CEAE). The task aims to predict argument roles of entity mentions for events in text, whose language is different from the language that a predictive model has been trained on. Previous work on CEAE has shown the cross-lingual benefits of universal dependency trees in capturing shared syntactic structures of sentences across languages. In particular, this work exploits the existence of the syntactic connections between the words in the dependency trees as the anchor knowledge to transfer the representation learning across languages for CEAE models (i.e., via graph convolutional neural networks {--} GCNs). In this paper, we introduce two novel sources of language-independent information for CEAE models based on the semantic similarity and the universal dependency relations of the word pairs in different languages. We propose to use the two sources of information to produce shared sentence structures to bridge the gap between languages and improve the cross-lingual performance of the CEAE models. Extensive experiments are conducted with Arabic, Chinese, and English to demonstrate the effectiveness of the proposed method for CEAE."
2021.sigdial-1.6,Improving Named Entity Recognition in Spoken Dialog Systems by Context and Speech Pattern Modeling,2021,-1,-1,1,1,117,minh nguyen,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"While named entity recognition (NER) from speech has been around as long as NER from written text has, the accuracy of NER from speech has generally been much lower than that of NER from text. The rise in popularity of spoken dialog systems such as Siri or Alexa highlights the need for more accurate NER from speech because NER is a core component for understanding what users said in dialogs. Deployed spoken dialog systems receive user input in the form of automatic speech recognition (ASR) transcripts, and simply applying NER model trained on written text to ASR transcripts often leads to low accuracy because compared to written text, ASR transcripts lack important cues such as punctuation and capitalization. Besides, errors in ASR transcripts also make NER from speech challenging. We propose two models that exploit dialog context and speech pattern clues to extract named entities more accurately from open-domain dialogs in spoken dialog systems. Our results show the benefit of modeling dialog context and speech patterns in two settings: a standard setting with random partition of data and a more realistic but also more difficult setting where many named entities encountered during deployment are unseen during training."
2021.naacl-main.3,Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks,2021,-1,-1,1,1,117,minh nguyen,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four tasks that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a dependency graph for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new regularization mechanism is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve representation learning. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages."
2021.mrl-1.6,Learning Cross-lingual Representations for Event Coreference Resolution with Multi-view Alignment and Optimal Transport,2021,-1,-1,3,0,711,duy phung,Proceedings of the 1st Workshop on Multilingual Representation Learning,0,"We study a new problem of cross-lingual transfer learning for event coreference resolution (ECR) where models trained on data from a source language are adapted for evaluations in different target languages. We introduce the first baseline model for this task based on XLM-RoBERTa, a state-of-the-art multilingual pre-trained language model. We also explore language adversarial neural networks (LANN) that present language discriminators to distinguish texts from the source and target languages to improve the language generalization for ECR. In addition, we introduce two novel mechanisms to further enhance the general representation learning of LANN, featuring: (i) multi-view alignment to penalize cross coreference-label alignment of examples in the source and target languages, and (ii) optimal transport to select close examples in the source and target languages to provide better training signals for the language discriminators. Finally, we perform extensive experiments for cross-lingual ECR from English to Spanish and Chinese to demonstrate the effectiveness of the proposed methods."
2021.findings-acl.211,Event Extraction from Historical Texts: A New Dataset for Black Rebellions,2021,-1,-1,2,0,3224,viet lai,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.439,Modeling Document-Level Context for Event Detection via Important Context Selection,2021,-1,-1,2,0,1761,amir veyseh,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"The task of Event Detection (ED) in Information Extraction aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g., BERT) as a critical component in state-of-the-art models for ED. However, the length limit for input texts is a barrier for such ED models as they cannot encode long-range document-level context that has been shown to be beneficial for ED. To address this issue, we propose a novel method to model document-level context for ED that dynamically selects relevant sentences in the document for the event prediction of the target sentence. The target sentence will be then augmented with the selected sentences and consumed entirely by transformer-based language models for improved representation learning for ED. To this end, the REINFORCE algorithm is employed to train the relevant sentence selection for ED. Several information types are then introduced to form the reward function for the training process, including ED performance, sentence similarity, and discourse relations. Our extensive experiments on multiple benchmark datasets reveal the effectiveness of the proposed model, leading to new state-of-the-art performance."
2021.emnlp-main.440,Crosslingual Transfer Learning for Relation and Event Extraction via Word Category and Class Alignments,2021,-1,-1,1,1,117,minh nguyen,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Previous work on crosslingual Relation and Event Extraction (REE) suffers from the monolingual bias issue due to the training of models on only the source language data. An approach to overcome this issue is to use unlabeled data in the target language to aid the alignment of crosslingual representations, i.e., via fooling a language discriminator. However, as this approach does not condition on class information, a target language example of a class could be incorrectly aligned to a source language example of a different class. To address this issue, we propose a novel crosslingual alignment method that leverages class information of REE tasks for representation learning. In particular, we propose to learn two versions of representation vectors for each class in an REE task based on either source or target language examples. Representation vectors for corresponding classes will then be aligned to achieve class-aware alignment for crosslingual representations. In addition, we propose to further align representation vectors for language-universal word categories (i.e., parts of speech and dependency relations). As such, a novel filtering mechanism is presented to facilitate the learning of word category representations from contextualized representations on input texts based on adversarial learning. We conduct extensive crosslingual experiments with English, Chinese, and Arabic over REE tasks. The results demonstrate the benefits of the proposed method that significantly advances the state-of-the-art performance in these settings."
2021.eacl-demos.4,{C}ov{R}elex: A {COVID}-19 Retrieval System with Relation Extraction,2021,-1,-1,7,0,11006,vu tran,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"This paper presents CovRelex, a scientific paper retrieval system targeting entities and relations via relation extraction on COVID-19 scientific papers. This work aims at building a system supporting users efficiently in acquiring knowledge across a huge number of COVID-19 scientific papers published rapidly. Our system can be accessed via https://www.jaist.ac.jp/is/labs/nguyen-lab/systems/covrelex/."
2021.eacl-demos.10,Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing,2021,-1,-1,1,1,117,minh nguyen,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"We introduce Trankit, a light-weight Transformer-based Toolkit for multilingual Natural Language Processing (NLP). It provides a trainable pipeline for fundamental NLP tasks over 100 languages, and 90 pretrained pipelines for 56 languages. Built on a state-of-the-art pretrained language model, Trankit significantly outperforms prior multilingual NLP pipelines over sentence segmentation, part-of-speech tagging, morphological feature tagging, and dependency parsing while maintaining competitive performance for tokenization, multi-word token expansion, and lemmatization over 90 Universal Dependencies treebanks. Despite the use of a large pretrained transformer, our toolkit is still efficient in memory usage and speed. This is achieved by our novel plug-and-play mechanism with Adapters where a multilingual pretrained transformer is shared across pipelines for different languages. Our toolkit along with pretrained models and code are publicly available at: https://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also available at: http://nlp.uoregon.edu/trankit. Finally, we create a demo video for Trankit at: https://youtu.be/q0KGP3zGjGc."
2020.paclic-1.24,Iterative Multilingual Neural Machine Translation for Less-Common and Zero-Resource Language Pairs,2020,-1,-1,1,1,117,minh nguyen,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.findings-emnlp.409,The Dots Have Their Values: Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction,2020,-1,-1,2,0.833333,2798,hieu tran,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences in the documents are used as the nodes of the document graphs for representation learning. However, this model does not capture the representations for the nodes in the graphs, thus preventing it from effectively encoding the specific and relevant information of the nodes for DRE. To address this issue, we propose to explicitly compute the representations for the nodes in the graph-based edge-oriented model for DRE. These node representations allow us to introduce two novel representation regularization mechanisms to improve the representation vectors for DRE. The experiments show that our model achieves state-of-the-art performance on two benchmark datasets."
2020.findings-emnlp.411,Structural and Functional Decomposition for Personality Image Captioning in a Communication Game,2020,-1,-1,1,1,117,minh nguyen,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Personality image captioning (PIC) aims to describe an image with a natural language caption given a personality trait. In this work, we introduce a novel formulation for PIC based on a communication game between a speaker and a listener. The speaker attempts to generate natural language captions while the listener encourages the generated captions to contain discriminative information about the input images and personality traits. In this way, we expect that the generated captions can be improved to naturally represent the images and express the traits. In addition, we propose to adapt the language model GPT2 to perform caption generation for PIC. This enables the speaker and listener to benefit from the language encoding capacity of GPT2. Our experiments show that the proposed model achieves the state-of-the-art performance for PIC."
2020.coling-main.86,Answering Legal Questions by Learning Neural Attentive Text Representation,2020,-1,-1,5,0,21148,phi kien,Proceedings of the 28th International Conference on Computational Linguistics,0,"Text representation plays a vital role in retrieval-based question answering, especially in the legal domain where documents are usually long and complicated. The better the question and the legal documents are represented, the more accurate they are matched. In this paper, we focus on the task of answering legal questions at the article level. Given a legal question, the goal is to retrieve all the correct and valid legal articles, that can be used as the basic to answer the question. We present a retrieval-based model for the task by learning neural attentive text representation. Our text representation method first leverages convolutional neural networks to extract important information in a question and legal articles. Attention mechanisms are then used to represent the question and articles and select appropriate information to align them in a matching process. Experimental results on an annotated corpus consisting of 5,922 Vietnamese legal questions show that our model outperforms state-of-the-art retrieval-based methods for question answering by large margins in terms of both recall and NDCG."
2020.aacl-srw.15,Document-Level Neural Machine Translation Using {BERT} as Context Encoder,2020,-1,-1,2,0,23204,zhiyu guo,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Large-scale pre-trained representations such as BERT have been widely used in many natural language understanding tasks. The methods of incorporating BERT into document-level machine translation are still being explored. BERT is able to understand sentence relationships since BERT is pre-trained using the next sentence prediction task. In our work, we leverage this property to improve document-level machine translation. In our proposed model, BERT performs as a context encoder to achieve document-level contextual information, which is then integrated into both the encoder and decoder. Experiment results show that our proposed method can significantly outperform strong document-level machine translation baselines on BLEU score. Moreover, the ablation study shows our method can capture document-level context information to boost translation performance."
W19-3631,Isolating the Effects of Modeling Recursive Structures: A Case Study in Pronunciation Prediction of {C}hinese Characters,2019,-1,-1,1,1,117,minh nguyen,Proceedings of the 2019 Workshop on Widening NLP,0,"Finding that explicitly modeling structures leads to better generalization, we consider the task of predicting Cantonese pronunciations of logographs (Chinese characters) using logographs{'} recursive structures. This task is a suitable case study for two reasons. First, logographs{'} pronunciations depend on structures (i.e. the hierarchies of sub-units in logographs) Second, the quality of logographic structures is consistent since the structures are constructed automatically using a set of rules. Thus, this task is less affected by confounds such as varying quality between annotators. Empirical results show that modeling structures explicitly using treeLSTM outperforms LSTM baseline, reducing prediction error by 6.0{\%} relative."
W18-2410,Statistical Machine Transliteration Baselines for {NEWS} 2018,2018,0,1,2,0,28442,snigdha singhania,Proceedings of the Seventh Named Entities Workshop,0,"This paper reports the results of our trans-literation experiments conducted on NEWS 2018 Shared Task dataset. We focus on creating the baseline systems trained using two open-source, statistical transliteration tools, namely Sequitur and Moses. We discuss the pre-processing steps performed on this dataset for both the systems. We also provide a re-ranking system which uses top hypotheses from Sequitur and Moses to create a consolidated list of transliterations. The results obtained from each of these models can be used to present a good starting point for the participating teams."
D18-1320,Multimodal neural pronunciation modeling for spoken languages with logographic origin,2018,0,2,1,1,117,minh nguyen,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Graphemes of most languages encode pronunciation, though some are more explicit than others. Languages like Spanish have a straightforward mapping between its graphemes and phonemes, while this mapping is more convoluted for languages like English. Spoken languages such as Cantonese present even more challenges in pronunciation modeling: (1) they do not have a standard written form, (2) the closest graphemic origins are logographic Han characters, of which only a subset of these logographic characters implicitly encodes pronunciation. In this work, we propose a multimodal approach to predict the pronunciation of Cantonese logographic characters, using neural networks with a geometric representation of logographs and pronunciation of cognates in historically related languages. The proposed framework improves performance by 18.1{\%} and 25.0{\%} respective to unimodal and multimodal baselines."
C18-1060,An Empirical Study on Fine-Grained Named Entity Recognition,2018,0,3,3,0,30768,khai mai,Proceedings of the 27th International Conference on Computational Linguistics,0,"Named entity recognition (NER) has attracted a substantial amount of research. Recently, several neural network-based models have been proposed and achieved high performance. However, there is little research on fine-grained NER (FG-NER), in which hundreds of named entity categories must be recognized, especially for non-English languages. It is still an open question whether there is a model that is robust across various settings or the proper model varies depending on the language, the number of named entity categories, and the size of training datasets. This paper first presents an empirical comparison of FG-NER models for English and Japanese and demonstrates that LSTM+CNN+CRF (Ma and Hovy, 2016), one of the state-of-the-art methods for English NER, also works well for English FG-NER but does not work well for Japanese, a language that has a large number of character types. To tackle this problem, we propose a method to improve the neural network-based Japanese FG-NER performance by removing the CNN layer and utilizing dictionary and category embeddings. Experiment results show that the proposed method improves Japanese FG-NER F-score from 66.76{\%} to 75.18{\%}."
C18-1193,Who is Killed by Police: Introducing Supervised Attention for Hierarchical {LSTM}s,2018,0,7,1,1,117,minh nguyen,Proceedings of the 27th International Conference on Computational Linguistics,0,"Finding names of people killed by police has become increasingly important as police shootings get more and more public attention (police killing detection). Unfortunately, there has been not much work in the literature addressing this problem. The early work in this field (Keith etal., 2017) proposed a distant supervision framework based on Expectation Maximization (EM) to deal with the multiple appearances of the names in documents. However, such EM-based framework cannot take full advantages of deep learning models, necessitating the use of handdesigned features to improve the detection performance. In this work, we present a novel deep learning method to solve the problem of police killing recognition. The proposed method relies on hierarchical LSTMs to model the multiple sentences that contain the person names of interests, and introduce supervised attention mechanisms based on semantical word lists and dependency trees to upweight the important contextual words. Our experiments demonstrate the benefits of the proposed model and yield the state-of-the-art performance for police killing detection."
P17-4007,Extended Named Entity Recognition {API} and Its Applications in Language Education,2017,5,4,4,0,712,tuan nguyen,"Proceedings of {ACL} 2017, System Demonstrations",0,None
I17-1054,Sentence Modeling with Deep Neural Architecture using Lexicon and Character Attention Mechanism for Sentiment Classification,2017,0,0,2,0,21768,huy nguyen,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Tweet-level sentiment classification in Twitter social networking has many challenges: exploiting syntax, semantic, sentiment, and context in tweets. To address these problems, we propose a novel approach to sentiment analysis that uses lexicon features for building lexicon embeddings (LexW2Vs) and generates character attention vectors (CharAVs) by using a Deep Convolutional Neural Network (DeepCNN). Our approach integrates LexW2Vs and CharAVs with continuous word embeddings (ContinuousW2Vs) and dependency-based word embeddings (DependencyW2Vs) simultaneously in order to increase information for each word into a Bidirectional Contextual Gated Recurrent Neural Network (Bi-CGRNN). We evaluate our model on two Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking."
I17-1065,An Ensemble Method with Sentiment Features and Clustering Support,2017,0,5,2,0,21768,huy nguyen,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Deep learning models have recently been applied successfully in natural language processing, especially sentiment analysis. Each deep learning model has a particular advantage, but it is difficult to combine these advantages into one model, especially in the area of sentiment analysis. In our approach, Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) were utilized to learn sentiment-specific features in a freezing scheme. This scenario provides a novel and efficient way for integrating advantages of deep learning models. In addition, we also grouped documents into clusters by their similarity and applied the prediction score of Naive Bayes SVM (NBSVM) method to boost the classification accuracy of each group. The experiments show that our method achieves the state-of-the-art performance on two well-known datasets: IMDB large movie reviews for document level and Pang {\&} Lee movie reviews for sentence level."
E17-1085,Building Lexical Vector Representations from Concept Definitions,2017,0,1,2,0,33041,danilo carvalho,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"The use of distributional language representations have opened new paths in solving a variety of NLP problems. However, alternative approaches can take advantage of information unavailable through pure statistical means. This paper presents a method for building vector representations from meaning unit blocks called concept definitions, which are obtained by extracting information from a curated linguistic resource (Wiktionary). The representations obtained in this way can be compared through conventional cosine similarity and are also interpretable by humans. Evaluation was conducted in semantic similarity and relatedness test sets, with results indicating a performance comparable to other methods based on single linguistic resource extraction. The results also indicate noticeable performance gains when combining distributional similarity scores with the ones obtained using this approach. Additionally, a discussion on the proposed method{'}s shortcomings is provided in the analysis of error cases."
K16-2020,{SDP}-{JAIST}: A Shallow Discourse Parsing system @ {C}o{NLL} 2016 Shared Task,2016,11,1,1,1,117,minh nguyen,Proceedings of the {C}o{NLL}-16 shared task,0,None
S15-2038,{JAIST}: Combining multiple features for Answer Selection in Community Question Answering,2015,13,39,4,0,4061,quan tran,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In this paper, we describe our system for SemEval-2015 Task 3: Answer Selection in Community Question Answering. In this task, the systems are required to identify the good or potentially good answers from the answer thread in Community Question Answering collections. Our system combines 16 features belong to 5 groups to predict answer quality. Our final model achieves the best result in subtask A for English, both in accuracy and F1score."
P14-1076,Robust Domain Adaptation for Relation Extraction via Clustering Consistency,2014,37,7,1,1,117,minh nguyen,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains. We address two challenges: negative transfer when knowledge in source domains is used without considering the differences in relation distributions; and lack of adequate labeled samples for rarer relations in the new domain, due to a small labeled data set and imbalance relation distributions. Our framework leverages on both labeled and unlabeled data in the target domain. First, we determine the relevance of each source domain to the target domain for each relation type, using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain. To overcome the lack of labeled samples for rarer relations, these clusterings operate on both the labeled and unlabeled data in the target domain. Second, we trade-off between using relevance-weighted sourcedomain predictors and the labeled target data. Again, to overcome the imbalance distribution, the source-domain predictors operate on the unlabeled target data. Our method outperforms numerous baselines and a weakly-supervised relation extraction method on ACE 2004 and YAGO."
I13-1088,Learning Based Approaches for {V}ietnamese Question Classification Using Keywords Extraction from the Web,2013,16,6,4,0,41683,dang tran,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper presents our research on automatic question classification for Vietnamese using machine learning approaches. We have experimented with several machine learning algorithms utilizing two kinds of feature groups: bag-of-words and keywords. Our research focuses on two most important tasks which are corpus building and features extraction by crawling data from the Web to build a keyword corpus. The performance of our approach is promising where our systemxe2x80x99s precision outperforms the state-of-the-art Tree Kernel approach (Collins and Duffy, 2001) on a Vietnamese question corpus."
I13-1138,Using Shallow Semantic Parsing and Relation Extraction for Finding Contradiction in Text,2013,15,0,2,0,5485,minh pham,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Finding contradiction text is a fundamental problem in natural language understanding. Previous work on finding contradiction in text incorporate information derived from predicate-argument structures as features in supervised machine learning frameworks. In contrast to previous work, we combine shallow semantic representations derived from semantic role labeling with binary relations extracted from sentences in a rule-based framework. Evaluation experiments conducted on standard data sets indicated that our system achieves better recall and F1 score for contradiction detection than most of baseline methods, and the same recall as a state of the art supervised method for the task."
Y11-1042,A Listwise Approach to Coreference Resolution in Multiple Languages,2011,15,0,3,0,17636,oanh tran,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"This paper presents a listwise approach as an alternative to commonly used pairwise approaches to the task of coreference resolution in multiple languages. In this listwise approach, all antecedent candidates are examined simultaneously and assigned corresponding scores expressing the probability that each candidate is coreferent with a given mention. The experimental results on the corpora of SemEval-2010 shared task 1 showed that our proposed system gave the good results in English and Spanish, and comparative results in Catalan when compared to previous participating systems. These results prove that this approach is appropriate and quite efficient for Coreference Resolution in Multiple Languages."
2009.mtsummit-papers.10,Improving a Lexicalized Hierarchical Reordering Model Using Maximum Entropy,2009,-1,-1,3,0,41974,vinh nguyen,Proceedings of Machine Translation Summit XII: Papers,0,None
W08-2119,A Tree-to-String Phrase-based Model for Statistical Machine Translation,2008,17,13,4,0,5764,thai nguyen,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"Though phrase-based SMT has achieved high translation quality, it still lacks of generalization ability to capture word order differences between languages. In this paper we describe a general method for tree-to-string phrase-based SMT. We study how syntactic transformation is incorporated into phrase-based SMT and its effectiveness. We design syntactic transformation models using unlexicalized form of synchronous context-free grammars. These models can be learned from source-parsed bitext. Our system can naturally make use of both constituent and non-constituent phrasal translations in the decoding phase. We considered various levels of syntactic analysis ranging from chunking to full parsing. Our experimental results of English-Japanese and English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems."
Y05-1015,A Structured {SVM} Semantic Parser Augmented by Semantic Tagging with Conditional Random Field,2005,15,1,1,1,117,minh nguyen,"Proceedings of the 19th Pacific Asia Conference on Language, Information and Computation",0,"This paper presents a novel method of semantic parsing that maps a natural language (NL) sentence to a logical form. We propose a semantic parsing method by conducting separately two steps as follows; 1) The first step is to predict semantic tags for a given input sentence. 2) The second step is to build a semantic representation structure for the sentence using the sequence of semantic tags. We formulate the problem of semantic tagging as a sequence learning using a conditional random field models (CRFs). We then represent a tree structure of a given sentence in which syntactic and semantic information are integrated in that tree. The learning problem is to map a given input sentence to a tree structure using a structure support vector model. Experimental results on the CLANG corpus show that the semantic tagging performance achieved a sufficiently high result. In addition, the precision and recall of mapping NL sentences to logical forms i.e. the meaning representation in CLANG show an improvement in comparison with the previous work."
C04-1107,Probabilistic Sentence Reduction Using Support Vector Machines,2004,11,23,1,1,117,minh nguyen,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,This paper investigates a novel application of support vector machines (SVMs) for sentence reduction. We also propose a new probabilistic sentence reduction method based on support vector machine learning. Experimental results show that the proposed methods outperform earlier methods in term of sentence reduction performance.
Y03-1030,Translation Template Learning Based on Hidden {M}arkov Modeling,2003,9,0,1,1,117,minh nguyen,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,This paper addresses a novel translation method based on Hidden Markov Model using template rules after learning them from the bilingual corpus. The method can enhance the translation accuracy and ensure a low complexity in comparing with the pervious template learning translation method and draws a new perspective for applying statistical machine learning on example based translations. domain.
Y03-1033,A New Sentence Reduction based on Decision Tree Model,2003,11,5,1,1,117,minh nguyen,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,This paper addresses a novel sentence reduction algorithm base on decision tree model where semantic information is used to enhance the accuracy of sentence reduction. The proposed algorithm is able to deal with the changeable order problem in sentence reduction. Experimental show a better result when comparing with the original methods.
W03-1119,A Sentence Reduction using Syntax Control,2003,10,6,1,1,117,minh nguyen,Proceedings of the Sixth International Workshop on Information Retrieval with {A}sian Languages,0,"This paper present a method based on the behavior of nonnative speaker for reduction sentence in foreign language. We demonstrate an algorithm using semantic information in order to produce two reduced sentences in two difference languages and ensure both grammatical and sentence meaning of the original sentence in reduced sentences. In addition, the orders of reduced sentences are able to be different from original sentences."
