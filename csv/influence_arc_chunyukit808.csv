2011.mtsummit-papers.61,W05-0909,0,0.163946,"Missing"
2011.mtsummit-papers.61,W95-0110,0,0.118428,"ed out by a stoplist. Another measure, inverse document frequency (idf), first defined in Spärck Jones (1972), concerns the specificity of a term according to its distribution over documents. Its underlying assumption is that the specificity of a term is inversely related to its probability of occurring in a particular document, meaning that the fewer documents containing a term, the more informative it is, and vice versa. It is formulated as ሺሻ ܦ where ሺሻ is the number of documents containing word  and  ܦthe total number of documents in the text collection in question. As noted in Church and Gale (1995a), tf and idf of a word are highly correlated to each other in general but also fundamentally different in that the former puts aside the density of distribution over documents. An observation is that we have ݐሺሻ  بሺሻ for many words whose multiple occurrences “burst” within a small number of documents. This is typically true for content-bearing ሺሻ ൌ  words, e.g., “boycott”. On the other hand, we have ݐሺሻ ൎ ሺሻ for many words that are evenly distributed, e.g., “somewhat”. They tend to occur in almost every document and hence are less informative. Such a correlation betwe"
2011.mtsummit-papers.61,N09-1060,0,0.0159174,"tion can be captured by the measure burstiness (bur) (Church and Gale, 1995b) or term clustering, defined as ܾݎݑሺሻ ൌ ݐሺሻ ሺሻ In our experiment, we use its reverse version ͳȀܾݎݑሺሻ as a goodness measure. Church and Gale (1995a) elaborate the notion of uneven distribution of words in documents with another measure variance (var). It compares the actual number of occurrences of a word with its expected frequency in a document  that is estimated by Poisson distribution, assuming that the occurrences of a word in different documents follow a statistical distribution pattern. As used in Kireyev (2009), the mean expected word frequency rate is straightforwardly estimated as ሺሻ ൌ ݐሺሻ ܦ and correspondingly,  ͳ ሺݐሺǡ ሻ െ ሺሻሻଶ ݎܽݒሺሻ ൌ ܦെͳ ୀଵ It reflects that a larger variance indicates a greater deviation from the expected frequency of occurrence in a document, meaning that the word in question is more salient in terms of its information load. Church and Gale (1995a) also introduce another measure, namely residual idf (ridf), to quantify the notion of deviation by comparing the actual idf of a word with its predicted idf. It is defined as ݎሺሻ ൌ ሺሻ െ ሺͳ െ ሺͲ"
2011.mtsummit-papers.61,N01-1004,0,0.0332106,"n with term frequency in the following way, forming the tf-idf and tf-ridf measures: ݐǦሺሻ ൌ ݐሺሻ ή ሺሻ and ݐǦݎሺሻ ൌ ݐሺሻ ή ݎሺሻ In this way the virtue of term frequency to locate keywords in a document is combined with the discriminative power of idf and ridf in order to filter out high frequency non-content words. In contrast to the tf-idf that has become a popular informativeness measure in multiple domains, the tf-ridf has rather limited use, although it is been shown to be a better choice in specific applications such as automatic summarization (Orăsan, 2009). Papineni (2001) presents an information theoretic measure, namely gain, which is defined as ܽሺሻ ൌ ሺሻ ሺሻ ሺሻ ቆ െ ͳ െ  ቇ ܦ ܦ ܦ This measure is formulated as a response to one of the main criticisms that idf overwhelmingly favors words of extremely low frequencies. Accordingly, it tends to assign low values to both very high- and low- frequency words, and treats the mid-frequency words as having the strongest “resolving power”. The information theoretic measures illustrated in Mladenić and Grobelnik (1998) for document categorization are also applicable to term-weighting, according to Orăs"
2011.mtsummit-papers.61,2001.mtsummit-papers.68,0,0.0384117,"994). In general, it measures the amount of semantic information conveyed in an MT output that users can identify, so as to indirectly assess its quality and understandability as a translation. The informativeness is found to be highly correlated with the adequacy of MT outputs (White, 2003). Its measurement gives a valid indication of translation quality and is hence adopted as an important criterion for translation assessment. In the studies of automatic evaluation metrics, however, there are only a few attempts that adopt term informativeness as an evaluation feature. As a variant of BLEU (Papineni et al., 2001), the NIST metric (Doddington, 2002) is proposed to include several variations of n-gram scoring, one of which introduces information weights to different n-gram counts in the way that n-grams of fewer occurrences in the reference translation set are weighted more heavily. Babych and Hartley (2004) extend BLEU with frequency weightings that use the standard tf-idf measure and their own S-score initially designed for information extraction. These works show that information-weighted metrics bring in observable improvement on correlation with human judgments, particularly in terms of adequacy ra"
2011.mtsummit-papers.61,1994.amta-1.25,0,0.146117,"Missing"
2015.mtsummit-papers.3,J07-2003,0,0.24639,"Missing"
2015.mtsummit-papers.3,P11-2031,0,0.0198494,"and applying refinement rule grow-diag-final-and Koehn et al. (2003). A 4-gram language model was trained on the Xinhua section of Gigaword by SRILM toolkit Stolcke et al. (2002). We also extracted SCFG rules from the word-aligned training data. The translation performance was measured by case-insensitive BLEU Papineni et al. (2002). We used minimum error rate training (MERT) (Och.2003a) to tune the log-linear feature weights. As MERT is normally instable, we ran the tuning process three times for all our experiments and presented the average BLEU scores on the three MERT runs as suggested by Clark et al (2011). The open source toolkit DISSECT1 was applied to obtain PMI-based vector space word representations with a context window of 5 words, and Word2Vec2 to acquire neural word representations, with each word represented as a 50-dimensional vector. When adopted Word2Vec, we just set the context window of size 5 and using continuous bag-of-words model. DISSECT was also adpoted to train weights in semantic composition of weighted vector addition. Unsupervised greedy RAE was trained in the way following Socher et al. (2011). In the bilingual projection neural network, 50 hidden units were used in the"
2015.mtsummit-papers.3,N03-1017,0,0.119057,"d sense discrimination (Clark and Pulman,2007) and thesaurus compilation (Yang and Powers,2008). In this paper, we explore how to learn semantic representations of bilingual phrases, rather than monolingual words, in the context of statistical machine translation (SMT) to facilitate the computation of semantic similarity between translation equivalents at the phrase level. We also study whether semantic similarity scores calculated in terms of bilingual distributed phrase representations are complementary to phrase translation probabilities estimated by the conventional counting method in SMT Koehn et al. (2003). Very recently we have witnessed some studies on learning bilingual distributed representations for SMT. Mikolov et al. (2013b) train two neural network (NN) based models to learn word embeddings in the source and the target language, respectively, and then map the embeddings from the source to the target language space using a transformation matrix that is learned ∗Corresponding author Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 32 by minimizing the mapping cost on all word pairs. Zou et al. (2013) introduce bilingual word embeddings into phrase-"
2015.mtsummit-papers.3,P08-1028,0,0.134647,"Missing"
2015.mtsummit-papers.3,P03-1021,0,0.0367911,"Missing"
2015.mtsummit-papers.3,J03-1002,0,0.00795929,"Missing"
2015.mtsummit-papers.3,J07-2002,0,0.0236259,"Missing"
2015.mtsummit-papers.3,P02-1040,0,0.0919351,"Missing"
2015.mtsummit-papers.3,D11-1014,0,0.146961,"f representations at the word level Mikolov et al. (2013b); Zou et al. (2013), so as to keep consistency with the SMT that uses phrases rather than words as basic translation units. • We learn phrase representations from distributed word representations via semantic composition, instead of from raw phrases Gao et al. (2013) in order to avoid the data sparseness issue of directly learning phrase representations from data. Particularly, we empirically compare two different composition methods in our framework, namely, weighted vector addition (Mitchell and Lapata,2008) and recursive autoencoder Socher et al. (2011). • Rather than jointly learning phrase representations with feature weights of the log-linear model of SMT Gao et al. (2013), we take a loose coupling strategy to simplify the learning process. We adopt a neural network to project phrase representations from the source onto the target language semantic space, in separation from the process of feature weight tuning in SMT. • Rather than capturing only the linear transformation between the source and target language semantic space Mikolov et al. (2013b), our neural network for the bilingual projection can model both linear and nonlinear transfo"
2015.mtsummit-papers.3,D12-1110,0,0.106474,"Missing"
2015.mtsummit-papers.3,P10-1040,0,0.0103434,"Missing"
2015.mtsummit-papers.3,P12-1079,1,0.885606,"Missing"
2015.mtsummit-papers.3,D13-1141,0,0.0745415,"ted by the conventional counting method in SMT Koehn et al. (2003). Very recently we have witnessed some studies on learning bilingual distributed representations for SMT. Mikolov et al. (2013b) train two neural network (NN) based models to learn word embeddings in the source and the target language, respectively, and then map the embeddings from the source to the target language space using a transformation matrix that is learned ∗Corresponding author Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 32 by minimizing the mapping cost on all word pairs. Zou et al. (2013) introduce bilingual word embeddings into phrase-based machine translation: Word representations are first learned from language A via an NN-based model, word embeddings in the parallel language B are then initialized according to A’s embeddings and word alignments between A and B, and the final word representations of B are obtained by a further training process that optimizes a combined objective on bilingual data. Gao et al. (2013) extend distributional representations from the word level to the phrase level, adopting a fully connected neural network to transfer bag-of-words vector represen"
2015.mtsummit-papers.3,P14-1011,0,0.0370829,"Missing"
2020.coling-main.567,P17-1055,0,0.0150853,"st models for this task. To validate our multichoice relational reasoning module, we replace it with the pointer-sum attention mechanism suggested by (Kadlec et al., 2016). As shown in Table 2, the new model, McR2 (−mr), results in a substantial performance drop on the CBT datasets. This proves the validity of performing reasoning on candidates. To 6452 Model Humans (Hill et al., 2016) CBT-NE dev test CBT-CN dev test WDW-Strict dev test WDW-Relaxed dev test – 81.6 – 81.6 – 84.0 – – AS Reader (Kadlec et al., 2016) IAA Reader (Sordoni et al., 2016) EpiReader (Trischler et al., 2016) AOA Reader (Cui et al., 2017) GA Reader (Dhingra et al., 2017) Fine-grained Gate (Yang et al., 2016) DGR (Ghaeini et al., 2018) 73.8 75.2 75.3 77.8 78.5 79.1 77.9 68.6 68.6 69.7 72.0 74.9 74.9 75.4 68.8 72.1 71.5 72.2 74.4 75.3 73.8 63.4 69.2 67.4 69.4 70.7 72.0 70.7 – – – – 71.61 – 71.78 57.0 – – – 71.2 71.7 72.0 – – – – 72.16 – 72.26 59.0 – – – 72.6 72.6 72.9 AS Reader* (Kadlec et al., 2016) EpiReader* (Trischler et al., 2016) IAA Reader* (Sordoni et al., 2016) AOA Reader* (Cui et al., 2017) 76.2 76.6 76.9 78.9 71.0 71.8 72.0 74.5 71.1 73.6 74.1 74.7 68.9 70.6 71.0 70.8 – – – – – – – – – – – – – – – – NSE (T=1) (Munkhda"
2020.coling-main.567,P17-1168,0,0.136902,"ts general form is to ask a computer to answer questions in natural language according to its understanding of a given article or a context. As a specific form of MRC, cloze-style reading comprehension has recently gained increasing attention. Cloze-style MRC is a task to fill in a blank in the query with an appropriate word or phrase according to given context.Several large-scale datasets (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016) for this task have been released, facilitating the development of various machine learning models (Kadlec et al., 2016; Trischler et al., 2016; Dhingra et al., 2017; Ghaeini et al., 2018). Most of these learning systems are built upon multi-hop architectures and attention mechanisms (Dhingra et al., 2017), which have been shown to excel at distilling useful information and learning the “importance” distribution over inputs. There are also some works that mimic the cognitive process of human reasoning with complicated hypothesis testing frameworks (Trischler et al., 2016; Munkhdalai and Yu, 2016). However, none of these models provide explicit reasoning among candidate answers with respect to a given context and query.As a practical skill for cloze test,"
2020.coling-main.567,C18-1282,0,0.172088,"ask a computer to answer questions in natural language according to its understanding of a given article or a context. As a specific form of MRC, cloze-style reading comprehension has recently gained increasing attention. Cloze-style MRC is a task to fill in a blank in the query with an appropriate word or phrase according to given context.Several large-scale datasets (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016) for this task have been released, facilitating the development of various machine learning models (Kadlec et al., 2016; Trischler et al., 2016; Dhingra et al., 2017; Ghaeini et al., 2018). Most of these learning systems are built upon multi-hop architectures and attention mechanisms (Dhingra et al., 2017), which have been shown to excel at distilling useful information and learning the “importance” distribution over inputs. There are also some works that mimic the cognitive process of human reasoning with complicated hypothesis testing frameworks (Trischler et al., 2016; Munkhdalai and Yu, 2016). However, none of these models provide explicit reasoning among candidate answers with respect to a given context and query.As a practical skill for cloze test, humans often need to ta"
2020.coling-main.567,D18-1130,0,0.0316992,"Missing"
2020.coling-main.567,P16-1086,0,0.324908,"arious clues from texts (Seo et al., 2016). Its general form is to ask a computer to answer questions in natural language according to its understanding of a given article or a context. As a specific form of MRC, cloze-style reading comprehension has recently gained increasing attention. Cloze-style MRC is a task to fill in a blank in the query with an appropriate word or phrase according to given context.Several large-scale datasets (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016) for this task have been released, facilitating the development of various machine learning models (Kadlec et al., 2016; Trischler et al., 2016; Dhingra et al., 2017; Ghaeini et al., 2018). Most of these learning systems are built upon multi-hop architectures and attention mechanisms (Dhingra et al., 2017), which have been shown to excel at distilling useful information and learning the “importance” distribution over inputs. There are also some works that mimic the cognitive process of human reasoning with complicated hypothesis testing frameworks (Trischler et al., 2016; Munkhdalai and Yu, 2016). However, none of these models provide explicit reasoning among candidate answers with respect to a given context a"
2020.coling-main.567,D16-1241,0,0.103234,"achine reading comprehension (MRC) is a challenging task that requires much semantic understanding and reasoning using various clues from texts (Seo et al., 2016). Its general form is to ask a computer to answer questions in natural language according to its understanding of a given article or a context. As a specific form of MRC, cloze-style reading comprehension has recently gained increasing attention. Cloze-style MRC is a task to fill in a blank in the query with an appropriate word or phrase according to given context.Several large-scale datasets (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016) for this task have been released, facilitating the development of various machine learning models (Kadlec et al., 2016; Trischler et al., 2016; Dhingra et al., 2017; Ghaeini et al., 2018). Most of these learning systems are built upon multi-hop architectures and attention mechanisms (Dhingra et al., 2017), which have been shown to excel at distilling useful information and learning the “importance” distribution over inputs. There are also some works that mimic the cognitive process of human reasoning with complicated hypothesis testing frameworks (Trischler et al., 2016; Munkhdalai and Yu, 20"
2020.coling-main.567,D14-1162,0,0.0837461,"on the same events as query. Deleted tokens in this corpus are always person named entities. In addition, samples that can be easily solved by simple baselines have been filtered, making the task more challenging. There are two versions of training set, WDW-Strict and WDW-Relaxed, in company with the same development and test sets. WDW-Strict is a small but tidy training set while WDW-Relaxed is larger and noisier. Our model is trained on both for respective results on the same validation and test sets. 6.2 Experimental Setups We initialize all word embeddings with pre-trained GloVe vectors (Pennington et al., 2014). All hidden states of Bi-GRUs have 128 dimensions except those of 75 dimensions for character Bi-GRU. The internal weights of GRUs are initialized with random orthogonal matrices and the gradient clipping threshold is set to 5 in order to deal with gradient exploding issues with GRU units. We adopt the ADAM optimizer (Kingma and Ba, 2014) for weight updating with an initial learning rate of 0.001 and apply dropout (Srivastava et al., 2014) to word embeddings to avoid overfitting. We present two variants of our model, one using 100-dimensional GloVe vectors to initialize the word embeddings th"
2020.coling-main.567,D16-1013,0,0.036672,"Missing"
2020.coling-main.567,P17-1018,0,0.267251,"a given context and query.As a practical skill for cloze test, humans often need to tactically compare two candidate answers while making decision, especially when more than one candidate appears to be competent. In this paper, we propose a multi-choice relational reasoning (McR2 ) model to imitate the above process. It first learns representations of document and query via a hierarchical multi-stage encoding architecture to explore the relations between document and query. The encoding architecture integrates the mechanisms of bidirectional attention flow (Seo et al., 2016), self-attention (Wang et al., 2017) and document-gated query reading (Dhingra et al., 2017) to learn dependencies between context and query and map them to representations rich in semantic information. The model then utilizes a multi-choice relational reasoning module to realize comparing and inferring over candidates. ∗ Xiaojun Quan is the corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. License details: 6448 Proceedings of the 28th International Conference on Computational Linguistics, pages 6448–6458 Barcelona, Spain (Onli"
C12-2116,D11-1033,0,0.0530956,"0; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training data selection for machine translation, in three different ways: the first directly measured cross entropy for the source side of the text; the second is similar to (Moore and Lewis, 2010) and ranked the data using cross entropy difference; and the third, took into account the bilingual data on both the source and target side of translations. Both studies showed that the selected subset of training data worked better than the entire training corpus for machine translation. Plank and van Noord (2011) investigated several different trai"
C12-2116,P07-1033,0,0.133576,"Missing"
C12-2116,W99-0701,1,0.863971,"entropy gain (EG), is defined as in Eq 5, where q is a probability distribution estimated from C and q1 is one estimated from C + s, a new corpus formed by adding s to C . Intuitively, if s is similar to C , q1 will be very similar to q and EG(s, c) will be small. EG(s, C) = |H(C + s, q1) − H(C, q) | (5) The measures in Eq 3-5 can all be normalized by sentence length. For instance, Eq 6 shows the normalized entropy gain. We call it Average Entropy Gain (AEG). AEG(s, C) = 3.4 EG(s, C) leng th(s) (6) Descriptive Length Gain (DLG) Description length gain (DLG) is a goodness measure proposed by (Kit and Wilks, 1999) as an unsupervised learning approach to lexical acquisition (Kit, 2005; Kit and Zhao, 2007). Intuitively, the DLG of a string str w.r.t. a corpus C, DLG(str, C), indicates the reduction of description length of C when the characters in str are treated as a unit and all the occurrences of str in C are replaced by the index of the unit. Therefore, the more frequent str is in C and the longer str is, the higher DLG(str,C) is. The DLG calculation resorts to a re-implementation of the suffix array approach to counting n-grams (Kit and Wilks, 1998). Based on this property, we define a similarity me"
C12-2116,J93-2004,0,0.0460848,"ntropy of two probability distributions estimated from the training data and the test data. If that is the case, it implies that entropy-based measures could be effective for training data selection. We then propose several new entropy-based measures and test their effects on two NLP tasks: CWS and POS tagging. For evaluation, we use the Chinese Penn Treebank as described below. 2.1 Data The Chinese Penn Treebank (CTB) was developed in the late 1990s (Xia et al., 2000) and each sentence is word segmented, part-of-speech tagged, and bracketed with a scheme similar to the English Penn Treebank (Marcus et al., 1993). Its latest release is version 7.0 1 , which contains more than one million words from five genres: Broadcast Conversation (BC), Broadcast News (BN), Magazine (MZ), Newswire (NW), and Weblog (WB). Some statistics of CTB7 are given in Table 1. We have used CTB 7.0 for multiple experiments, some of them not directly related to this study. To prepare the data for all of our experiments, we divide the data in each genre into 1 Linguistic Data Consortium No. LDC2010T07 1192 Genre # of chars 275,289 Broadcast Conversation (BC) Broadcast News 482,667 (BN) # of words 184,161 # of files 86 287,442 1,1"
C12-2116,P06-1043,0,0.029327,"Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training data selection for machine translation, in three different ways: the first directly measured"
C12-2116,N10-1004,0,0.0183548,"statistically significant improvement over random selection for both tasks. KEYWORDS: Domain Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. Axelrod et al. (2011) adopted the idea of cross entropy measurement for training"
C12-2116,P10-2041,0,0.0575601,"lts on the Chinese Penn Treebank indicate that some of the measures provide a statistically significant improvement over random selection for both tasks. KEYWORDS: Domain Adaptation, Training Data Selection, Entropy-based measures. Proceedings of COLING 2012: Posters, pages 1191–1200, COLING 2012, Mumbai, December 2012. 1191 1 Introduction The performance of Natural Language Processing (NLP) systems often degrades significantly when training and testing data come from different domains. There has been extensive research on methods for domain adaptation including training data selection (e.g.,(Moore and Lewis, 2010; Plank and van Noord, 2011)), model combination (e.g., (McClosky et al., 2010)), feature copying (Daume III, 2007), semi-supervised learning (e.g., (McClosky et al., 2006)), and many more. The goal of training data selection is to choose a subset of training data that was similar to a given test data set. The challenge is to find a good measure for calculating the similarity between training sentences and the test data set. Moore and Lewis (2010) calculated the difference of the cross entropy values for a given sentence, based on language models from the source domain and the target domain. A"
C12-2116,P11-1157,0,0.0869203,"Missing"
C12-2116,song-xia-2012-using,1,0.730643,"Missing"
C12-2116,N03-1033,0,0.0438304,"study, we chose to use the same data split for training and test to facilitate comparison with our other experiments. Training Development Test BC 211,795 30,678 32,816 BN 211,826 30,760 48,317 MZ 211,834 30,708 37,531 NW 211,853 30,726 44,543 WB 211,796 30,746 33,623 Table 2: CTB 7.0 Genre character counts for data splitting. 2.2 System performance and cross entropy In order to determine whether entropy-based measures are helpful in training data selection, we first check whether cross entropy correlates with system performance. For this, we first trained and tested the Stanford POS Tagger2 (Toutanova et al., 2003) on the CTB 7.0. The results are in Table 3, in which the training and testing genres are indicated by row labels and column labels, respectively. In the top part of the table, each cell (i, j) has two numbers, where i is the row and j is the column. The first number is the tagging accuracy, when the tagger is trained on the training data of the genre i , and tested on the test data of the genre j . The second number is cross entropy of the test data, estimated by a trigram language model built from the training data using the CMU-Cambridge LM Toolkit3 . The final row in the table lists the 2"
C12-2116,xia-etal-2000-developing,1,0.750312,"n (CWS) and POS tagging. 2 Methodology In this study, we first test whether there is a strong correlation between system performance and cross entropy of two probability distributions estimated from the training data and the test data. If that is the case, it implies that entropy-based measures could be effective for training data selection. We then propose several new entropy-based measures and test their effects on two NLP tasks: CWS and POS tagging. For evaluation, we use the Chinese Penn Treebank as described below. 2.1 Data The Chinese Penn Treebank (CTB) was developed in the late 1990s (Xia et al., 2000) and each sentence is word segmented, part-of-speech tagged, and bracketed with a scheme similar to the English Penn Treebank (Marcus et al., 1993). Its latest release is version 7.0 1 , which contains more than one million words from five genres: Broadcast Conversation (BC), Broadcast News (BN), Magazine (MZ), Newswire (NW), and Weblog (WB). Some statistics of CTB7 are given in Table 1. We have used CTB 7.0 for multiple experiments, some of them not directly related to this study. To prepare the data for all of our experiments, we divide the data in each genre into 1 Linguistic Data Consortiu"
C92-4173,C90-2043,0,0.0256143,"Missing"
C92-4173,C90-2057,0,0.0206259,"Missing"
C92-4173,J89-1001,0,0.0577152,"Missing"
D09-1004,W09-1201,0,0.0466519,"Missing"
D09-1004,W05-0620,0,0.68883,"nal Institute of Information and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 haizhao@cityu.edu.hk, chenwl@nict.go.jp Abstract into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws"
D09-1004,W08-2134,0,0.125114,"ish-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these two tasks is that the identification of semantic predicates is required in the task of CoNLL-2008 but not in CoNLL-2009. 31 et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2 , lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3 . Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We explain the last item, support verb(noun). From a given word to the synt"
D09-1004,W08-2122,0,0.130649,"comes better. Though not so surprised, the results do show that the argument traverse scheme synP th always outperforms the other linP th. The result of this comparison partially shows that an integrated semantic role labeler is sensitive to the order of how argument candidates are traversed to some extent. The performance given by synP th is compared to some other systems that participated in the CoNLL-2008 shared task. They were chosen among the 20 participating systems either because they held better results (the first four participants) or because they used some joint learning techniques (Henderson et al., 2008). The results of (Titov et al., 2009) that use the similar joint learning technique as (Henderson et al., 2008) are also included9 . Results of these evaluations on the test set are in Table 6. Top three systems of CoNLL2008, (Johansson and Nugues, 2008; Ciaramita et al., 2008; Che et al., 2008), used SRL pipelines. In this work, we partially use the similar techniques (synP th) for our participation in the shared tasks of CoNLL-2008 and 2009 (Zhao and Kit, 2008; Zhao et al., 2009b; Zhao et al., 2009a). Here we report that all SRL sub-tasks are tackled in one integrated model, while the predic"
D09-1004,I08-1012,1,0.827391,"ependency Parsers We consider three types of syntactic information to feed the SRL task. One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-ofthe-art syntactic parser described in (Johansson and Nugues, 2008)7 (it is referred to Johansson) and an integrated parser described as the following (referred to MSTM E ). The parser is basically based on the MSTParser8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. 1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2"
D09-1004,W06-1617,0,0.283236,"u et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance."
D09-1004,W08-2138,0,0.0466915,"Missing"
D09-1004,W08-2123,0,0.069069,"Missing"
D09-1004,P05-1006,0,0.0217851,"metimes takes itself as its argument. The above pruning algorithm has been shown effective. However, it is still inefficient for a SRL 1 CoNLL-2008 is an English-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these two tasks is that the identification of semantic predicates is required in the task of CoNLL-2008 but not in CoNLL-2009. 31 et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2 , lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3 . Syntactic Connection. This includes syntactic head (h), left(right) far"
D09-1004,P08-1068,0,0.0284694,"scribed as the following (referred to MSTM E ). The parser is basically based on the MSTParser8 using all the features presented by (McDonald et al., 2006) with projective parsing. Moreover, we exploit three types of additional features to improve the parser. 1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transitionSyn-Parser LAS MSTM E Johansson Gold 88.39 89.28 100."
D09-1004,D08-1034,0,0.0312437,"mplate Selection Based on the above mentioned elements, 781 feature templates (hereafter the set of these templates is referred to F T )6 are initially considered. Feature templates in this initial set are constructed in a generalized way. For example, if we find that a feature template a.lm.lemma was once used in some existing work, then such three templates, a.rm.lemma, a.rn.lemma, a.ln.lemma will be also added into the set. As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in (Jiang and Ng, 2006; Ding and Chang, 2008) is applied. The detailed algorithm is described in Algorithm 1. Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008) requires O(n2 ) times of training/test routines, it cannot handle a set that consists of hundreds of templates. As the time complexity of Algorithm 1 is only O(n), it permits a large scale feature selection accomplished by paying a reasonable time cost. Though the time complexity of the algorithm given by (Jiang and Ng, 2006) is also linear, it should assume all feature templates in the initial selected set ‘good’ enough and"
D09-1004,W05-0625,0,0.022033,"and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 haizhao@cityu.edu.hk, chenwl@nict.go.jp Abstract into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little at"
D09-1004,J02-3001,0,0.194096,"oLeft A1 f g A0 noRight h . Training samples will generated from c to g according to the above sequence. We use a Maximum Entropy classifier with a tunable Gaussian prior as usual. Our implementation of the model adopts L-BFGS algorithm for parameter optimization. 3 Feature Templates 3.1 Elements for Feature Generation Motivated by previous works, we carefully consider those factors from a wide range of features that can help semantic role labeling for both predicate disambiguation, argument’s identification and classification as the predicate is either verbal or nominal. These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen 2 In CoNLL-2008, Treebank tokens are split at the position that a hyphen (-) or a forward slash (/) occurs. This leads to two types of feature columns, non-split and split. 3 Lemma and pos for either training or test are from automatically pre-analyzed columns in the input files. 4 Note that the meaning of support verb is slightly different between (Toutanova et al., 2005) and (Xue, 2006; Jiang and Ng, 2006) 32 first includes all syntactic children (children), the second also includes all but excludes the left most and the right most children (noFarChildren"
D09-1004,P07-1027,0,0.0120512,"e. However, it is still inefficient for a SRL 1 CoNLL-2008 is an English-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these two tasks is that the identification of semantic predicates is required in the task of CoNLL-2008 but not in CoNLL-2009. 31 et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2 , lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3 . Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child (lm, ln, rm, and rn), and high(low) support verb or noun. We ex"
D09-1004,P05-1073,0,0.109413,"Missing"
D09-1004,W04-3212,0,0.642466,"unyu Kit† (揭 揭春 雨 ) † Department of Chinese, Translation and Linguistics City University of Hong Kong Tat Chee Avenue, Kowloon, Hong Kong, China ∗ Language Infrastructure Group, MASTAR Project National Institute of Information and Communications Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 haizhao@cityu.edu.hk, chenwl@nict.go.jp Abstract into multiple stages is two-fold, one is each subtask asks for its favorable features, the other is at the consideration of computational efficiency. Generally speaking, a joint system is slower than a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBan"
D09-1004,W06-2932,0,0.0610293,"Missing"
D09-1004,N06-1055,0,0.371434,"al predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two We present an integrated dependencybased semantic role labeling system for English from both NomBank and PropBank. By introducing assistant argument labels and considering much more feature templates, two optimal feature template sets are obtained through an effective feature selection proce"
D09-1004,W04-2705,0,0.0326022,"unt out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming a"
D09-1004,W08-2127,1,0.845355,"performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two optimal feature template sets have been proven available, for the first time we report that an integrated SRL system may provide a result close to the state-of-the-art achieved by those SRL pipelines or individual systems for some specific predicates. 2 Initialization: Set the given predicate as the current node; Adaptive Argument Pruning A word-pair classification is used to formulate semantic dependency parsing as in (Zhao and Kit, 2008). As for predicate identification or disambiguation, the first word is set as a virtual root (which is virtually set before the beginning of the sentence.) and the second as a predicate candidate. As for argument identification/classification, the first word in a word pair is specified as a predi(1) The current node and all of its syntactic children are selected as argument candidates (children are traversed from left to right.). (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached. Note that this pruning algorithm is slightly different from that of (X"
D09-1004,P08-1108,0,0.0290597,"1) Chen et al. (2008) used features derived from short dependency pairs based on large-scale auto-parsed data to enhance dependency parsing. Here, the same features are used, though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data. 2) Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, the same features are also used as word clusters are generated only from the training data. 3) Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer. For the sake of efficiency, we use a fast transitionSyn-Parser LAS MSTM E Johansson Gold 88.39 89.28 100.00 synPth+F Tsyn Sem Sem-F1 F1 /LAS 80.53 91.10 80.94 90.66 84.57 84.57 linPth+F Tlin Sem Sem-F1 F1 /LAS 79.83 90.31 79.84 89.43 83.34 83.34 Table 5: Semantic Labeled F1 based parser based on maximum entropy as in Zhao and Kit (2008). We still use the similar feature"
D09-1004,W09-1209,1,0.855764,"em still holds some sort of merits, being easier to implement, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all required semantic role information and so on. The shared tasks at the CoNLL 2008 and 2009 are devoted to the joint learning of syntactic and semantic dependencies, which show that SRL can be well performed using only dependency syntax input. Using data and evaluation settings of the CoNLL-2008 shared task, this work will only focus on semantic dependency parsing and compares the best-performing SRL system in the CoNLL-2009 shared Task (Zhao et al., 2009b) with those in the CoNLL-2008 shared task (Surdeanu et al., 2008; Hajiˇc et al., 2009)1 . Aiming at main drawbacks of an integrated approach, two key techniques will be applied. 1) Assistant argument labels are introduced for the further improvement of argument pruning. This helps the development of a fast and lightweight SRL system. 2) Using a greedy feature selection algorithm, a large-scale feature engineering is performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two op"
D09-1004,J05-1004,0,0.0315932,"han a pipeline system in training. (Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated"
D09-1004,W09-1208,1,0.769841,"em still holds some sort of merits, being easier to implement, a single-stage feature selection benefiting the whole system, an all-in-one model outputting all required semantic role information and so on. The shared tasks at the CoNLL 2008 and 2009 are devoted to the joint learning of syntactic and semantic dependencies, which show that SRL can be well performed using only dependency syntax input. Using data and evaluation settings of the CoNLL-2008 shared task, this work will only focus on semantic dependency parsing and compares the best-performing SRL system in the CoNLL-2009 shared Task (Zhao et al., 2009b) with those in the CoNLL-2008 shared task (Surdeanu et al., 2008; Hajiˇc et al., 2009)1 . Aiming at main drawbacks of an integrated approach, two key techniques will be applied. 1) Assistant argument labels are introduced for the further improvement of argument pruning. This helps the development of a fast and lightweight SRL system. 2) Using a greedy feature selection algorithm, a large-scale feature engineering is performed on a much larger feature template set than that in previous work. This helps us find features that may be of benefit to all SRL sub-tasks as long as possible. As two op"
D09-1004,N04-1030,0,0.0522951,"idates are in Table 4. The statistics is conducted on three different syntactic inputs. The coverage rate in the table means the ratio of how many true arguments are covered by the selected pruning scheme. Note that the adaptive pruning of argument candidates using assistant labels does not change this rate. This ratio only depends on which path, either synP th or linP th, is chosen, and how good the syntactic input is (if synP th is the case). From the results, we see that more than a half of argument candidates can be effectively pruned for synP th and even 2/3 for linP th. As mentioned by (Pradhan et al., 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). The results also indicate that such an assumption holds that arguments trend to close with their predicate, at either type of distance, syntactic or linear. Based on different syntactic inputs, we obtain different results on semantic dependency parsing 7 It is a 2-order maximum spanning tree parser with pseudo-projective techniques. A syntact"
D09-1004,P05-1072,0,0.478575,"its argument. The above pruning algorithm has been shown effective. However, it is still inefficient for a SRL 1 CoNLL-2008 is an English-only task, while CoNLL2009 is a multilingual one. Though the English corpus in CoNLL-2009 is almost identical to the corpus in the CoNLL2008 shared task evaluation, the latter holds more sophisticated input structure as in (Surdeanu et al., 2008). The most difference for these two tasks is that the identification of semantic predicates is required in the task of CoNLL-2008 but not in CoNLL-2009. 31 et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). Most feature templates that we will adopt for this work will come from various combinations or integrations of the following basic elements. Word Property. This type of elements include word form (form and its split form, spForm)2 , lemma (lemma,spLemma), and part-of-speech tag (pos, spPos), syntactic dependency label (dprel), and semantic dependency label (semdprel)3 . Syntactic Connection. This includes syntactic head (h), left(right) farthest(nearest) child ("
D09-1004,W05-0302,0,0.0146086,"son and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). As a complement to PropBank, NomBank (Meyers et al., 2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006). (Pustejovsky et al., 2005) discussed the issue of merging various treebanks, including PropBank, NomBank, and others. The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al., 2008). However, few empirical studies support the necessity of an integrated learning strategy from NomBank and PropBank. Though aiming at Chinese SRL, (Xue, 2006) reported that their experiments show that simply adding the verb data to the training set of NomBank and extracting the same features from the verb and noun instances will hurt the overall performance. From the results of CoNLL-20"
D09-1004,W08-2121,0,0.0732528,"Missing"
D09-1004,H05-1081,0,\N,Missing
D12-1097,W09-2404,0,0.0518842,"T. A phase of its work is to have grammatical devices, 1 As cited in van Slype (1979). 1061 such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic labelers for them that can be integrated into an MT model. For lexical cohesion, it has been only partially and indirectly addressed in terms of translation consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through huma"
D12-1097,W10-1750,0,0.16116,"Missing"
D12-1097,D11-1084,0,0.280283,"Missing"
D12-1097,2003.mtsummit-papers.30,0,0.0601831,"ation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006), that adopt a sentence-by-sentence fashion to score MT outputs. The evaluation result for a document by any of them is usually a simple average of its sentence scores. A This paper studies the inter-sentential linguistic features of cohesion and coherence and presents plausible ways to incorporate them into the sentence-based metrics to support MT evaluation at the document level. In the Framework for MT Evaluation in the International Standards of Language Engineering (FEMTI) (King et al., 2003), coherence is defined as “the degree to which the reader can describe the role of each individual sentence (or group of sentences) with respect to the text as a whole”. The measurement of coherence has to rely on cohesion, referring to the “relations of meaning that exist within the text” (Halliday and Hasan, 1976). Cohesion is realized via the interlinkage of grammatical and lexical elements across sentences. Grammatical 1060 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1060–1068, Jeju Island"
D12-1097,2001.mtsummit-papers.42,0,0.0442317,"abelers for them that can be integrated into an MT model. For lexical cohesion, it has been only partially and indirectly addressed in terms of translation consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through human annotation. Its automatic versions TER and TERp (Snover et al., 2009), however, remain sentencebased metrics. Comelles et al. (2010) present a family of automatic MT evaluation measures, based on the Discourse Representation Theory (Kamp and Reyle, 1993), that generate semantic"
D12-1097,J91-1002,0,0.808907,"luctuate in accord with different evaluation data. Lexical cohesion has far been neglected in both MT and MT evaluation, even though it is the single most important form of cohesion devices, accounting for nearly half of the cohesion devices in English (Halliday and Hasan, 1976). It is also a significant feature contributing to translation equivalence of texts by preserving their texture (Lotfipour-Saedi, 1997). The lexical cohesion devices in a text can be represented as lexical chains conjoining related entities. There are many methods of computing lexical chains for various purposes, e.g., Morris and Hirst (1991), Barzilay and Elhadad (1997), Chan (2004), Li et al. (2007), among many others. Contrary to grammatical cohesion highly depending on syntactic well-formedness of a text, lexical cohesion is less affected by grammatical errors. Its computation has to rely on a thesaurus, which is usually available for almost every language. In this research, a number of formulations of lexical cohesion, with or without reliance on external language resource, will be explored for the purpose of MT evaluation. 3 Lexical Cohesion in Machine and Human Translation This section presents a comparative study of MT and"
D12-1097,1993.tmi-1.18,0,0.427728,"appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translation. A recent COMTIS project (Cartoni et al., 2011) begins to exploit inter-sentential information for statistical MT. A phase of its work is to have grammatical devices, 1 As cited in van Slype (1979). 1061 such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic"
D12-1097,A92-1028,0,0.135343,"er level quality criteria beyond many others such as syntactic well-formedness. Post-editors tend to correct syntactic errors first before any amendment for improving the cohesion and coherence of an MT output. Also, as Wilks (1978)1 noted, it is rather unlikely for a sufficiently large sample of translations to be coherent and totally wrong at the same time. Cohesion and coherence are appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translat"
D12-1097,C96-2137,0,0.0232062,"ntactic well-formedness. Post-editors tend to correct syntactic errors first before any amendment for improving the cohesion and coherence of an MT output. Also, as Wilks (1978)1 noted, it is rather unlikely for a sufficiently large sample of translations to be coherent and totally wrong at the same time. Cohesion and coherence are appropriate to serve as criteria for the overall quality of MT output. Previous researches in MT predominantly focus on specific types of cohesion devices. For grammatical cohesion, a series of works, including Nakaiwa and Ikehara (1992), Nakaiwa et al. (1995), and Nakaiwa and Shirai (1996), present approaches to resolving Japanese zero pronouns and to integrating them into a Japanese-English transferred-based MT system. Peral et al. (1999) propose an interlingual mechanism for pronominal anaphora generation by exploiting a rich set of lexical, syntactic, morphologic and semantic information. Murata and Nagao (1993) and Murata et al. (2001) develop a rule base to identify the referential properties of Japanese noun phrases, so as to facilitate anaphora resolution for Japanese and article generation for English during translation. A recent COMTIS project (Cartoni et al., 2011) be"
D12-1097,P02-1040,0,0.0868162,"The connectivity of sentences is surely a significant factor contributing to the understandability of a text as a whole. Introduction Machine translation (MT) has benefited a lot from the advancement of automatic evaluation in the past decade. To a certain degree, its progress is also confined to the limitations of evaluation metrics in use. Most efforts devoted to evaluate the quality of MT output so far have still focused on the sentence level without sufficient attention to how a larger text is structured. This is notably reflected in the representative MT evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006), that adopt a sentence-by-sentence fashion to score MT outputs. The evaluation result for a document by any of them is usually a simple average of its sentence scores. A This paper studies the inter-sentential linguistic features of cohesion and coherence and presents plausible ways to incorporate them into the sentence-based metrics to support MT evaluation at the document level. In the Framework for MT Evaluation in the International Standards of Language Engineering (FEMTI) (King et al., 2003), coherence is defined as “the de"
D12-1097,W99-0210,0,0.0861282,"Missing"
D12-1097,2006.amta-papers.25,0,0.54035,"buting to the understandability of a text as a whole. Introduction Machine translation (MT) has benefited a lot from the advancement of automatic evaluation in the past decade. To a certain degree, its progress is also confined to the limitations of evaluation metrics in use. Most efforts devoted to evaluate the quality of MT output so far have still focused on the sentence level without sufficient attention to how a larger text is structured. This is notably reflected in the representative MT evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006), that adopt a sentence-by-sentence fashion to score MT outputs. The evaluation result for a document by any of them is usually a simple average of its sentence scores. A This paper studies the inter-sentential linguistic features of cohesion and coherence and presents plausible ways to incorporate them into the sentence-based metrics to support MT evaluation at the document level. In the Framework for MT Evaluation in the International Standards of Language Engineering (FEMTI) (King et al., 2003), coherence is defined as “the degree to which the reader can describe the role of each individual"
D12-1097,W09-0441,0,0.0184853,"ion that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing effort through human annotation. Its automatic versions TER and TERp (Snover et al., 2009), however, remain sentencebased metrics. Comelles et al. (2010) present a family of automatic MT evaluation measures, based on the Discourse Representation Theory (Kamp and Reyle, 1993), that generate semantic trees to put together different text entities for the same referent according to their contexts and grammatical connections. Apart from MT evaluation, automated essay scoring programs such as E-rater (Burstein, 2003) also employ a rich set of discourse features for assessment. However, the parsing process needed for these linguistic-heavy approaches may suffer seriously from grammatical"
D12-1097,C96-2189,0,0.128407,"ir critical distinctions that human translators tend to use more cohesion devices than machine. Various ways to apply this feature to evaluate machinetranslated documents are presented, including one without reliance on reference translation. Experimental results show that incorporating this feature into sentence-level evaluation metrics can enhance their correlation with human judgements. 1 The accuracy of MT output at the document level is particularly important to MT users, for they care about the overall meaning of a text in question more than the grammatical correctness of each sentence (Visser and Fuji, 1996). Post-editors particularly need to ensure the quality of a whole document of MT output when revising its sentences. The connectivity of sentences is surely a significant factor contributing to the understandability of a text as a whole. Introduction Machine translation (MT) has benefited a lot from the advancement of automatic evaluation in the past decade. To a certain degree, its progress is also confined to the limitations of evaluation metrics in use. Most efforts devoted to evaluate the quality of MT output so far have still focused on the sentence level without sufficient attention to h"
D12-1097,wong-2010-semantic,1,0.882679,"Missing"
D12-1097,P94-1019,0,0.0190956,"r stemmer (1980). To classify the semantic relationships of words, WordNet (Fellbaum, 1998) is used as a lexical resource, which clusters words of the same sense (i.e., synonyms) into a semantic group, namely a synset. Synsets are interlinked in WordNet according to their semantic relationships. Superordinate and collocation are formed by words in a proximate semantic relationship, such as bicycle and vehicle (hypernym), bicycle and wheel (meronym), bicycle and car (coordinate term), and so on. They are defined as synset pairs with a distance of 1 in WordNet. The measure of semantic distance (Wu and Palmer, 1994) is also applied to identify near-synonyms, i.e., words that are synonyms in a broad sense but not grouped in the same synset. It quantifies the semantic similarity of word pairs as a real number in between 0 and 1 (the higher the more similar) as Identification of Lexical Cohesion Devices Lexical cohesion is achieved through word choices of two major types: reiteration and collocation. Reiteration can be realized in a continuum or a cline of specificity, with repetition of the same lexical item at one end and the use of a general noun to point to the 1062 MetricsMATR MTC4 8 6 25 100 249 919 4"
D12-1097,2011.mtsummit-papers.13,0,0.225485,"on for statistical MT. A phase of its work is to have grammatical devices, 1 As cited in van Slype (1979). 1061 such as verbal tense/aspect/mode, discourse connectives and pronouns, manually annotated in multilingual corpora, in hopes of laying a foundation for the development of automatic labelers for them that can be integrated into an MT model. For lexical cohesion, it has been only partially and indirectly addressed in terms of translation consistency in MT output. Different approaches to maintaining consistency in target word choices are proposed (Itagaki et al., 2007; Gong et al., 2011; Xiao et al., 2011). Carpuat (2009) also observes a general tendency in human translation that a given sense is usually lexicalized in a consistent manner throughout the whole translation. Nevertheless there are only a few evaluation methods explicitly targeting on the quality of a document. Miller and Vanni (2001) devise a human evaluation approach to measure the comprehensibility of a text as a whole, based on the Rhetorical Structure Theory (Mann and Thompson, 1988), a theory of text organization specifying coherence relations in an authentic text. Snover et al. (2006) proposes HTER to assess post-editing eff"
D12-1097,2007.mtsummit-papers.36,0,\N,Missing
D12-1097,W97-0703,0,\N,Missing
D12-1097,W05-0909,0,\N,Missing
I05-1020,M95-1012,0,0.0478851,"Missing"
I05-1020,J96-1002,0,0.00525352,"Missing"
I05-1020,W02-2018,0,0.0132511,"en trained with various context length and feature sets. Maxent model is intended to achieve the most unbiased probabilistic distribution on the data set for training. It is also a nice framework for integrating heterogeneous information into a model for classiﬁcation purpose. It has been popular in NLP community for various language processing tasks since Berger et al. [2] and Della Pietra et al. [3] presenting its theoretical basis and basic training techniques. Ratnaparkhi [9] applied it to tackle several NL ambiguity problems, including sentence boundary detection. Wallach [14] and Malouf [4] compared the eﬀectiveness of several training algorithms for Maxent model. There are a number of full-ﬂedged implementations of Maxent models available from the Web. Using the OpenNLP MAXENT package from http:// maxent.sourceforge.net/, acknowledged here with gratitude, we are released from the technical details of its implementation and can concentrate on examining the eﬀectiveness of context length and feature space on period disamPeriod Disambiguation with Maxent Model 225 biguation. Basically, our exploration is carried out along the following working procedure: (1) prepare a set of train"
I05-1020,J93-2004,0,0.0326497,"Missing"
I05-1020,A00-2035,0,0.111481,"Missing"
I05-1020,J97-2002,0,0.0474636,"Missing"
I05-1020,A97-1004,0,0.0673836,"Missing"
I05-1020,H89-2048,0,0.242578,"Missing"
I05-1020,H94-1013,0,\N,Missing
I05-3021,P04-1059,0,0.0338359,"Missing"
I05-3021,W99-0701,1,0.801975,"a sequence. However, a serious problem with this examplebased approach is the sparse data problem. Long exemplar fragments are more reliable but small 147 in number, whereas short ones are large in number but less reliable. In the case of no exemplar fragment available for an input sentence, this approach draws back to the maximal match segmentation with a dictionary. How to incorporate statistical inference into example-based segmentation to infer more reliable optimal segmentation beyond string matching remains a critical issue for us to tackle. 3 DLG-based segmentation DLG is formulated in Kit and Wilks (1999) and Kit (2000) as an empirical measure for the compression effect of extracting a substring from a given corpus as a lexical item. DLG optimization is applied to detect OOV words for our participation in CWSB-2. It works as follows in two steps. 1. Calculate the DLG for all known words and all new word candidate (i.e., substrings with frequency ≥ 2, preferably, in the test corpus), based on frequency information in the training and the test corpora; 2. Find the optimal sequence of such items over an input sentence with the greatest sum of DLG. Step 2 above in our system re-implements only the"
I05-3021,W02-1808,1,0.833575,"pproach to recognize IV words and follows description length gain (DLG) to infer OOV words in terms of their text compression effect. Sections 2 and 3 below introduce the examplebased and DLG-based segmentation respectively. Section 4 presents a strategy to combine their strength and Section 5 reports our system’s performance in CWSB-2. Following error analysis in Section 6, Section 7 concludes the paper. 2 Example-based segmentation How to utilize as much information as possible from the training corpus to adapt a segmentation system towards a segmentation standard has been a critical issue. Kit et al. (2002) and Kit et al. (2003) attempt to integrate case-based learning with statistical models (e.g., n-gram) by extracting transformation rules from the training corpus for disambiguation via error correction; Gao et al. (2004) adopt a similar strategy for adaptive segmentation, with transformation templates (instead of case-based rules) to modify word boundaries (instead of individual words). The basic idea of example-based segmentation is very simple: existing pre-segmented strings in training corpus provide reliable examples for segmenting similar strings in input texts. In contrast to dictionary"
I05-3021,W03-1724,1,0.778098,"V words and follows description length gain (DLG) to infer OOV words in terms of their text compression effect. Sections 2 and 3 below introduce the examplebased and DLG-based segmentation respectively. Section 4 presents a strategy to combine their strength and Section 5 reports our system’s performance in CWSB-2. Following error analysis in Section 6, Section 7 concludes the paper. 2 Example-based segmentation How to utilize as much information as possible from the training corpus to adapt a segmentation system towards a segmentation standard has been a critical issue. Kit et al. (2002) and Kit et al. (2003) attempt to integrate case-based learning with statistical models (e.g., n-gram) by extracting transformation rules from the training corpus for disambiguation via error correction; Gao et al. (2004) adopt a similar strategy for adaptive segmentation, with transformation templates (instead of case-based rules) to modify word boundaries (instead of individual words). The basic idea of example-based segmentation is very simple: existing pre-segmented strings in training corpus provide reliable examples for segmenting similar strings in input texts. In contrast to dictionary checking for locating"
I05-3021,P97-1041,0,\N,Missing
I05-4010,baroni-etal-2004-introducing,0,0.0324494,"sophisticated plain text format for data encoding and manipulation. It is particularly suitable for hierarchical linguistic data such as the hierarchicallyaligned bilingual corpus that we have produced. What’s more, converting data to XML format not only significantly reduces the complexity of data exchange among different computer systems but also enhances data transmission reliability and eases Web browsing. There have been many corpora that are annotated with XML, e.g., HCRC Map Task Corpus (Anderson et al., 1991), American National Corpus (Ide and Macleod, 2001), the La Republica corpus (Baroni et al., 2004). Below we present the XML schema for our subparagraph-aligned BLIS bitexts, with sample annotation, and necessary Web browsing. Indicating a text line break. Table 4: Anchors in a sample text item can be divided into a numbering item and the remaining content text in the line, as illustrated in Table 4. The Chinese counterpart of this text carries similar lines, if no missing line in any page of the pair. Unfortunately, missing lines are found in some BLIS pages, as exemplified in Figure 2. There is no guarantee that matching text lines one by one in sequence would carry out the expected alig"
I05-4010,J93-2003,0,0.0580449,"Missing"
I05-4010,H91-1026,0,0.11711,"stem in the legal text hierarchy. Basic methodology and practical techniques are reported in detail. The resultant bilingual corpus, 10.4M English words and 18.3M Chinese characters, is an authoritative and comprehensive text collection covering the specific and special domain of HK laws. It is particularly valuable to empirical MT research. This piece of work has also laid a foundation for exploring and harvesting English-Chinese bitexts in a larger volume from the Web. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al., 1999) and many others. The Web is being explored not only as a super corpus for NLP and linguistic research (Kilgarriff and Grefenstette, 2003) but also, more importantly to MT research, as a treasure for mining bitexts of various language pairs (Resnik, 1999; Chen and Nie, 2000; Nie and Cai, 2001; Nie and Chen, 2002; Resnik and Smith, 2003; Way and Gough, 2003). The Web has been the playground for many NLPers. More and more Web sites are found to have cloned their Web pages in"
I05-4010,P91-1023,0,0.178969,"stem in the legal text hierarchy. Basic methodology and practical techniques are reported in detail. The resultant bilingual corpus, 10.4M English words and 18.3M Chinese characters, is an authoritative and comprehensive text collection covering the specific and special domain of HK laws. It is particularly valuable to empirical MT research. This piece of work has also laid a foundation for exploring and harvesting English-Chinese bitexts in a larger volume from the Web. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al., 1999) and many others. The Web is being explored not only as a super corpus for NLP and linguistic research (Kilgarriff and Grefenstette, 2003) but also, more importantly to MT research, as a treasure for mining bitexts of various language pairs (Resnik, 1999; Chen and Nie, 2000; Nie and Cai, 2001; Nie and Chen, 2002; Resnik and Smith, 2003; Way and Gough, 2003). The Web has been the playground for many NLPers. More and more Web sites are found to have cloned their Web pages in"
I05-4010,ide-etal-2000-xces,0,0.0237299,"nacted in the English language only; and// (b) no authentic text of that Ordinance has been published in the Chinese language under section 4B(1).// (3) Nothing in subsection (1) shall require an Ordinance to be enacted and published in both official languages where the Chief Executive in Council- (Amended 26 of 1999 s.3)// a XML is applied to encode the text alignment outcomes output from the above alignment procedure. It has been a standard for data representation and exchange on the Web, and also accepted by the NLP community as a standard for linguistic data annotation and representation (Ide et al., 2000; Mengel and Lezius, 2000; Kim et al., 2001). There are a series of yearly NLPXML workshops for it since 2001. It provides a platform-independent flexible and sophisticated plain text format for data encoding and manipulation. It is particularly suitable for hierarchical linguistic data such as the hierarchicallyaligned bilingual corpus that we have produced. What’s more, converting data to XML format not only significantly reduces the complexity of data exchange among different computer systems but also enhances data transmission reliability and eases Web browsing. There have been many corpor"
I05-4010,P94-1012,0,0.161233,"ic methodology and practical techniques are reported in detail. The resultant bilingual corpus, 10.4M English words and 18.3M Chinese characters, is an authoritative and comprehensive text collection covering the specific and special domain of HK laws. It is particularly valuable to empirical MT research. This piece of work has also laid a foundation for exploring and harvesting English-Chinese bitexts in a larger volume from the Web. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al., 1999) and many others. The Web is being explored not only as a super corpus for NLP and linguistic research (Kilgarriff and Grefenstette, 2003) but also, more importantly to MT research, as a treasure for mining bitexts of various language pairs (Resnik, 1999; Chen and Nie, 2000; Nie and Cai, 2001; Nie and Chen, 2002; Resnik and Smith, 2003; Way and Gough, 2003). The Web has been the playground for many NLPers. More and more Web sites are found to have cloned their Web pages in several languages, aiming"
I05-4010,W97-0311,0,0.0569364,"Missing"
I05-4010,J00-2004,0,0.0330364,"Missing"
I05-4010,mengel-lezius-2000-xml,0,0.0303452,"ish language only; and// (b) no authentic text of that Ordinance has been published in the Chinese language under section 4B(1).// (3) Nothing in subsection (1) shall require an Ordinance to be enacted and published in both official languages where the Chief Executive in Council- (Amended 26 of 1999 s.3)// a XML is applied to encode the text alignment outcomes output from the above alignment procedure. It has been a standard for data representation and exchange on the Web, and also accepted by the NLP community as a standard for linguistic data annotation and representation (Ide et al., 2000; Mengel and Lezius, 2000; Kim et al., 2001). There are a series of yearly NLPXML workshops for it since 2001. It provides a platform-independent flexible and sophisticated plain text format for data encoding and manipulation. It is particularly suitable for hierarchical linguistic data such as the hierarchicallyaligned bilingual corpus that we have produced. What’s more, converting data to XML format not only significantly reduces the complexity of data exchange among different computer systems but also enhances data transmission reliability and eases Web browsing. There have been many corpora that are annotated with"
I05-4010,P99-1068,0,0.0340071,"tion for exploring and harvesting English-Chinese bitexts in a larger volume from the Web. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al., 1999) and many others. The Web is being explored not only as a super corpus for NLP and linguistic research (Kilgarriff and Grefenstette, 2003) but also, more importantly to MT research, as a treasure for mining bitexts of various language pairs (Resnik, 1999; Chen and Nie, 2000; Nie and Cai, 2001; Nie and Chen, 2002; Resnik and Smith, 2003; Way and Gough, 2003). The Web has been the playground for many NLPers. More and more Web sites are found to have cloned their Web pages in several languages, aiming at conveying information to audience in different languages. This gives rise to a huge volume of wonderful bilingual or multi-lingual resources freely available from the Web for research. What we need to do is to harvest the right resources for the right applications. 1 Introduction Bitexts, also referred to as parallel texts or bilingual corpora,"
I05-4010,J03-3002,0,0.0509315,"ume from the Web. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al., 1999) and many others. The Web is being explored not only as a super corpus for NLP and linguistic research (Kilgarriff and Grefenstette, 2003) but also, more importantly to MT research, as a treasure for mining bitexts of various language pairs (Resnik, 1999; Chen and Nie, 2000; Nie and Cai, 2001; Nie and Chen, 2002; Resnik and Smith, 2003; Way and Gough, 2003). The Web has been the playground for many NLPers. More and more Web sites are found to have cloned their Web pages in several languages, aiming at conveying information to audience in different languages. This gives rise to a huge volume of wonderful bilingual or multi-lingual resources freely available from the Web for research. What we need to do is to harvest the right resources for the right applications. 1 Introduction Bitexts, also referred to as parallel texts or bilingual corpora, collections of bilingual text pairs aligned at various levels of granularity, have"
I05-4010,J03-3004,0,0.0256318,"ng parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al., 1999) and many others. The Web is being explored not only as a super corpus for NLP and linguistic research (Kilgarriff and Grefenstette, 2003) but also, more importantly to MT research, as a treasure for mining bitexts of various language pairs (Resnik, 1999; Chen and Nie, 2000; Nie and Cai, 2001; Nie and Chen, 2002; Resnik and Smith, 2003; Way and Gough, 2003). The Web has been the playground for many NLPers. More and more Web sites are found to have cloned their Web pages in several languages, aiming at conveying information to audience in different languages. This gives rise to a huge volume of wonderful bilingual or multi-lingual resources freely available from the Web for research. What we need to do is to harvest the right resources for the right applications. 1 Introduction Bitexts, also referred to as parallel texts or bilingual corpora, collections of bilingual text pairs aligned at various levels of granularity, have been playing a critica"
I05-4010,J93-1004,0,\N,Missing
I05-4010,J03-3001,0,\N,Missing
I08-1002,A00-2032,0,0.0978954,"y. In this sense, one may consider them the discrete and continuous formulation of the same idea. 5 All evaluations will be represented in terms of word F-measure if not otherwise specified. A standard scoring tool with this metric can be found in SIGHAN website, http://www.sighan.org/bakeoff2003/score. However, to compare with related work, we will also adopt boundary F-measure Fb = 2Rb Pb /(Rb + Pb ), where the boundary recall Rb and boundary precision Pb are, respectively, the proportions of the correctly recognized boundaries to all boundaries in the goldstandard and a segmenter’s output (Ando and Lee, 2000). 11 Table 1: Bakeoff-3 Corpora Corpus Training(M) Test(K) AS 8.42 146 CityU 2.71 364 CTB 0.83 256 MSRA 2.17 173 Table 2: Performance with decoding algorithm (1) M. L.a GoodTraining corpus ness AS CityU CTB MSRA FSR .400 .454 .462 .432 DLG/d .592 .610 .604 .603 2 AV .568 .595 .596 .577 BE .559 .587 .592 .572 FSR .193 .251 .268 .235 DLG/d .331 .397 .409 .379 7 AV .399 .423 .430 .407 BE .390 .419 .428 .403 a M.L.: Maximal length allowable for word candidates. for computation. There are two ways to get this score: (1) computed by the goodness measure, which is applicable only if the measure allow"
I08-1002,O97-4005,0,0.455799,"tation of Chinese text. Four existing approaches to unsupervised segmentations or word extraction are considered as its special cases, each with its own goodness measurement to quantify word likelihood. The output by each approach will be evaluated using benchmark data sets of Bakeoff-32 (Levow, 2006). Note that unsupervised segmentation is different from, if not more complex than, word extraction, in that the former must carry out the segmentation task for a text, for which a segmentation (decoding) algorithm is indispensable, whereas the latter only acquires a word candidate list as output (Chang and Su, 1997; Zhang et al., 2000). This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework. Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores. Experiments show that description length gain outperforms other measures because of it"
I08-1002,J04-1004,0,0.548443,"gs. L(·) is the empirical description length of a corpus in bits that can be estimated by the Shannon-Fano code or Huffman code as below, following classic information theory (Shannon, 1948). X . L(X) = −|X| pˆ(x)log2 pˆ(x) (5) x∈V where |· |denotes string length, V is the character vocabulary of X and pˆ(x) x’s frequency in X. For a given word candidate w, we define gDLG (w) = DLG(w). In principle, a substring with a negative DLG do not bring any positive compression effect by itself. Thus only substrings with a positive DLG value are added into our word candidate list. Accessor Variety (AV) Feng et al. (2004) propose AV as a statistical criterion to measure how likely a substring is a word. It is reported to handle lowfrequent words particularly well. The AV of a substring xi..j is defined as AV (xi..j ) = min{Lav (xi..j ), Rav (xi..j )} (6) 3 Although there have been many existing works in this direction (Lua and Gan, 1994; Chien, 1997; Sun et al., 1998; Zhang et al., 2000; SUN et al., 2004), we have to skip the details of comparing MI due to the length limitation of this paper. However, our experiments with MI provide no evidence against the conclusions in this paper. 10 where the left and right"
I08-1002,Y03-1017,0,0.0160045,"iments with MI provide no evidence against the conclusions in this paper. 10 where the left and right accessor variety Lav (xi..j ) and Rav (xi..j ) are, respectively, the number of distinct predecessor and successor characters. For a similar reason as to FSR, the logarithm of AV is used as goodness measure, and only substrings with AV &gt; 1 are considered word candidates. That is, we have gAV (w) = logAV (w) for a word candidate w. Boundary Entropy (Branching Entropy, BE) It is proposed as a criterion for unsupervised segmentation in some existing works (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). The local entropy for a given xi..j , defined as X h(xi..j ) = − p(x|xi..j )log p(x|xi..j ), (7) x∈V indicates the average uncertainty after (or before) xi..j in the text, where p(x|xi..j ) is the co-occurrence probability for x and xi..j . Two types of h(xi..j ), namely hL (xi..j ) and hR (xi..j ), can be defined for the two directions to extend xi..j (Tung and Lee, 1994). Also, we can define hmin = min{hR , hL } in a similar way as in (6). In this study, only substrings with BE &gt; 0 are considered word candidates. For a candidate w, we have gBE (w) = hmin (w)4 ."
I08-1002,P06-2056,0,0.662748,"o evidence against the conclusions in this paper. 10 where the left and right accessor variety Lav (xi..j ) and Rav (xi..j ) are, respectively, the number of distinct predecessor and successor characters. For a similar reason as to FSR, the logarithm of AV is used as goodness measure, and only substrings with AV &gt; 1 are considered word candidates. That is, we have gAV (w) = logAV (w) for a word candidate w. Boundary Entropy (Branching Entropy, BE) It is proposed as a criterion for unsupervised segmentation in some existing works (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006). The local entropy for a given xi..j , defined as X h(xi..j ) = − p(x|xi..j )log p(x|xi..j ), (7) x∈V indicates the average uncertainty after (or before) xi..j in the text, where p(x|xi..j ) is the co-occurrence probability for x and xi..j . Two types of h(xi..j ), namely hL (xi..j ) and hR (xi..j ), can be defined for the two directions to extend xi..j (Tung and Lee, 1994). Also, we can define hmin = min{hR , hL } in a similar way as in (6). In this study, only substrings with BE &gt; 0 are considered word candidates. For a candidate w, we have gBE (w) = hmin (w)4 . 4 Evaluation The evaluation"
I08-1002,W99-0701,1,0.866648,"odness score. It works on T to output the best current word w∗ repeatedly with T =t∗ for the next round as follows, {w∗ , t∗ } = argmax g(w) (2) wt = T gF SR (w) = log(ˆ p(w)) (3) where pˆ(w) is w’s frequency in the corpus. This allows the arithmetic addition in (1). According to Zipf’s Law (Zipf, 1949), it approximates the use of the rank of w as its goodness, which would give it some statistical significance. For the sake of efficiency, only those substrings that occur more than once are considered qualified word candidates. Description Length Gain (DLG) The goodness measure is proposed in (Kit and Wilks, 1999) for compression-based unsupervised segmentation. The DLG from extracting all occurrences of xi xi+1 ...xj (also denoted as xi..j ) from a corpus X= x1 x2 ...xn as a word is defined as DLG(xi..j ) = L(X) − L(X[r → xi..j ] ⊕ xi..j ) (4) with each {w, g(w)} ∈ W . This algorithm will back off to forward maximal matching algorithm if the goodness function is set to word length. Thus the former may be regarded as a generalization of the latter. Symmetrically, it has an inverse version that works the other way around. 3 as the goodness for a word candidate, i.e., Goodness Measurement An unsupervised"
I08-1002,W06-0115,0,0.0334186,"are often involved in most existing works, and there has not been a comprehensive comparison of their performance in a unified way with available large-scale “gold standard” data sets, especially, multi-standard ones since Bakeoff-1 1 . In this paper we will propose a unified framework for unsupervised segmentation of Chinese text. Four existing approaches to unsupervised segmentations or word extraction are considered as its special cases, each with its own goodness measurement to quantify word likelihood. The output by each approach will be evaluated using benchmark data sets of Bakeoff-32 (Levow, 2006). Note that unsupervised segmentation is different from, if not more complex than, word extraction, in that the former must carry out the segmentation task for a text, for which a segmentation (decoding) algorithm is indispensable, whereas the latter only acquires a word candidate list as output (Chang and Su, 1997; Zhang et al., 2000). This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework. Assuming no prior knowledge about Chinese, this framework relies o"
I08-1002,P98-2206,0,0.400207,"te w, we define gDLG (w) = DLG(w). In principle, a substring with a negative DLG do not bring any positive compression effect by itself. Thus only substrings with a positive DLG value are added into our word candidate list. Accessor Variety (AV) Feng et al. (2004) propose AV as a statistical criterion to measure how likely a substring is a word. It is reported to handle lowfrequent words particularly well. The AV of a substring xi..j is defined as AV (xi..j ) = min{Lav (xi..j ), Rav (xi..j )} (6) 3 Although there have been many existing works in this direction (Lua and Gan, 1994; Chien, 1997; Sun et al., 1998; Zhang et al., 2000; SUN et al., 2004), we have to skip the details of comparing MI due to the length limitation of this paper. However, our experiments with MI provide no evidence against the conclusions in this paper. 10 where the left and right accessor variety Lav (xi..j ) and Rav (xi..j ) are, respectively, the number of distinct predecessor and successor characters. For a similar reason as to FSR, the logarithm of AV is used as goodness measure, and only substrings with AV &gt; 1 are considered word candidates. That is, we have gAV (w) = logAV (w) for a word candidate w. Boundary Entropy ("
I08-1002,W00-1219,0,0.0456081,"xt. Four existing approaches to unsupervised segmentations or word extraction are considered as its special cases, each with its own goodness measurement to quantify word likelihood. The output by each approach will be evaluated using benchmark data sets of Bakeoff-32 (Levow, 2006). Note that unsupervised segmentation is different from, if not more complex than, word extraction, in that the former must carry out the segmentation task for a text, for which a segmentation (decoding) algorithm is indispensable, whereas the latter only acquires a word candidate list as output (Chang and Su, 1997; Zhang et al., 2000). This paper reports our empirical evaluation and comparison of several popular goodness measures for unsupervised segmentation of Chinese texts using Bakeoff-3 data sets with a unified framework. Assuming no prior knowledge about Chinese, this framework relies on a goodness measure to identify word candidates from unlabeled texts and then applies a generalized decoding algorithm to find the optimal segmentation of a sentence into such candidates with the greatest sum of goodness scores. Experiments show that description length gain outperforms other measures because of its strength for identi"
I08-1002,Y06-1012,1,0.889942,"Missing"
I08-1002,C98-2201,0,\N,Missing
I08-4017,O97-4005,0,0.0750261,"erformance improvement on both word segmentation and NER for all tracks except CTB segmentation, as highlighted in Table 6. We are unable explain this yet, and can only attribute it to some unique text characteristics of the CTB segmented corpus. An unsupervised segmentation criterion provides a kind of global information over the whole text of a corpus (Zhao and Kit, 2007). Its effectiveness is certainly sensitive to text characteristics. Quite a number of other unsupervised segmentation criteria are available for word discovery in unlabeled texts, e.g., boundary entropy (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006) and descriptionlength-gain (DLG) (Kit and Wilks, 1999). We found that among them AV could help the CRFs model to achieve a better performance than others, although the overall unsupervised segmentation by DLG was slightly better than that by AV. Combining any two of these criteria did not give any further performance 110 5 Conclusion Without doubt our achievements in Bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength of CRFs learning but also to the effectiveness of our unsu"
I08-4017,W06-0116,0,0.0135173,"ndom fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chinese Language Processing Evaluation, at http://www.china-l"
I08-4017,I05-3017,0,0.152128,"segmenter. Table 3 lists the training corpora for the assistant CRFs segmenter and the ANERs for various open NER tests. The third group consists of feature templates generated from seven NE lists acquired from Chinese Wikipedia.4 The categories and numbers of these NE items are summarized in Table 4. 3 Evaluation Results The performance of both word segmentation and NER is measured in terms of the F-measure F = 2RP/(R + P ), where R and P are the recall and precision of segmentation or NER. We tested the techniques described above with the previous Bakeoffs’ data5 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006). The evaluation results for the closed tests of word segmentation are reported in Table 5 and those for the NER on two corpora of Bakeoff-3 are in the upper part of Table 7. ‘+/–AV’ indicates whether AV features are applied. For Bakeoff-4, we participated in all five closed tracks of word segmentation, namely, CityU, CKIP, CTB, NCC, and SXU, and in all closed and open NER tracks of CityU and MSRA.6 The evaluation 3 It consists of about 108K words of one to four characterslong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chi nese%20Information%20Processing/Source Code/Cha"
I08-4017,J04-1004,0,0.102102,"r, respectively. 107 ì E-ORG  O q S-LOC  ¯ ü Ñ O O O O In addition to these n-gram features, unsupervised segmentation outputs are also used as features, for the purpose of providing more word boundary information via global statistics derived from all unlabeled texts of the training and test corpora. The basic idea is to inform a supervised leaner of which substrings are recognized as word candidates by a given unsupervised segmentation criterion and how likely they are to be true words in terms of that criterion (Zhao and Kit, 2007; Kit and Zhao, 2007). We adopt the accessor variety (AV) (Feng et al., 2004a; Feng et al., 2004b) as our unsupervised segmentation criterion. It formulates an idea similar to linguist Harris’ (1955; 1970) for segmenting utterances of an unfamiliar language into morphemes to facilitate word extraction from Chinese raw texts. It is found more effective than other criteria in supporting CRFs learning of character tagging for word segmentation (Zhao and Kit, 2007). The AV value of a substring s is defined as AV (s) = min{Lav (s), Rav (s)}, where the left and right AV values Lav (s) and Rav (s) are defined, respectively, as the numbers of its distinct predecessor and succ"
I08-4017,Y03-1017,0,0.0536962,"nt on both word segmentation and NER for all tracks except CTB segmentation, as highlighted in Table 6. We are unable explain this yet, and can only attribute it to some unique text characteristics of the CTB segmented corpus. An unsupervised segmentation criterion provides a kind of global information over the whole text of a corpus (Zhao and Kit, 2007). Its effectiveness is certainly sensitive to text characteristics. Quite a number of other unsupervised segmentation criteria are available for word discovery in unlabeled texts, e.g., boundary entropy (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006) and descriptionlength-gain (DLG) (Kit and Wilks, 1999). We found that among them AV could help the CRFs model to achieve a better performance than others, although the overall unsupervised segmentation by DLG was slightly better than that by AV. Combining any two of these criteria did not give any further performance 110 5 Conclusion Without doubt our achievements in Bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength of CRFs learning but also to the effectiveness of our unsupervised segmentation ap"
I08-4017,P06-2056,0,0.0146146,"tion and NER for all tracks except CTB segmentation, as highlighted in Table 6. We are unable explain this yet, and can only attribute it to some unique text characteristics of the CTB segmented corpus. An unsupervised segmentation criterion provides a kind of global information over the whole text of a corpus (Zhao and Kit, 2007). Its effectiveness is certainly sensitive to text characteristics. Quite a number of other unsupervised segmentation criteria are available for word discovery in unlabeled texts, e.g., boundary entropy (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006) and descriptionlength-gain (DLG) (Kit and Wilks, 1999). We found that among them AV could help the CRFs model to achieve a better performance than others, although the overall unsupervised segmentation by DLG was slightly better than that by AV. Combining any two of these criteria did not give any further performance 110 5 Conclusion Without doubt our achievements in Bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength of CRFs learning but also to the effectiveness of our unsupervised segmentation approach. It is for the sake of"
I08-4017,W99-0701,1,0.674328,"ghted in Table 6. We are unable explain this yet, and can only attribute it to some unique text characteristics of the CTB segmented corpus. An unsupervised segmentation criterion provides a kind of global information over the whole text of a corpus (Zhao and Kit, 2007). Its effectiveness is certainly sensitive to text characteristics. Quite a number of other unsupervised segmentation criteria are available for word discovery in unlabeled texts, e.g., boundary entropy (Tung and Lee, 1994; Chang and Su, 1997; Huang and Powers, 2003; Jin and Tanaka-Ishii, 2006) and descriptionlength-gain (DLG) (Kit and Wilks, 1999). We found that among them AV could help the CRFs model to achieve a better performance than others, although the overall unsupervised segmentation by DLG was slightly better than that by AV. Combining any two of these criteria did not give any further performance 110 5 Conclusion Without doubt our achievements in Bakeoff-4 owes not only to the careful selection of character tag set and feature templates for exerting the strength of CRFs learning but also to the effectiveness of our unsupervised segmentation approach. It is for the sake of simplicity that similar sets of character tags and fea"
I08-4017,W06-0115,0,0.0295768,"le 3 lists the training corpora for the assistant CRFs segmenter and the ANERs for various open NER tests. The third group consists of feature templates generated from seven NE lists acquired from Chinese Wikipedia.4 The categories and numbers of these NE items are summarized in Table 4. 3 Evaluation Results The performance of both word segmentation and NER is measured in terms of the F-measure F = 2RP/(R + P ), where R and P are the recall and precision of segmentation or NER. We tested the techniques described above with the previous Bakeoffs’ data5 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006). The evaluation results for the closed tests of word segmentation are reported in Table 5 and those for the NER on two corpora of Bakeoff-3 are in the upper part of Table 7. ‘+/–AV’ indicates whether AV features are applied. For Bakeoff-4, we participated in all five closed tracks of word segmentation, namely, CityU, CKIP, CTB, NCC, and SXU, and in all closed and open NER tracks of CityU and MSRA.6 The evaluation 3 It consists of about 108K words of one to four characterslong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chi nese%20Information%20Processing/Source Code/Chapter 8/ Lexico"
I08-4017,I05-3025,0,0.514394,"rvised segmentation and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chinese Language Proc"
I08-4017,W03-1719,0,0.0324663,"sentation as the assistant segmenter. Table 3 lists the training corpora for the assistant CRFs segmenter and the ANERs for various open NER tests. The third group consists of feature templates generated from seven NE lists acquired from Chinese Wikipedia.4 The categories and numbers of these NE items are summarized in Table 4. 3 Evaluation Results The performance of both word segmentation and NER is measured in terms of the F-measure F = 2RP/(R + P ), where R and P are the recall and precision of segmentation or NER. We tested the techniques described above with the previous Bakeoffs’ data5 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006). The evaluation results for the closed tests of word segmentation are reported in Table 5 and those for the NER on two corpora of Bakeoff-3 are in the upper part of Table 7. ‘+/–AV’ indicates whether AV features are applied. For Bakeoff-4, we participated in all five closed tracks of word segmentation, namely, CityU, CKIP, CTB, NCC, and SXU, and in all closed and open NER tracks of CityU and MSRA.6 The evaluation 3 It consists of about 108K words of one to four characterslong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chi nese%20Information%20Processing/"
I08-4017,W06-0120,0,0.102785,"R trained on the MSRA NER training corpus of Bakeoff-3. This makes our official evaluation results extremely high but trivial, for a part of this corpus is used as the MSRA NER test corpus for Bakeoff-4. Presented here are the results without using this ANER. b Open2 is the result of Open1 using no NE list feature. results of word segmentation and NER for our system are presented in Tables 6 and 7, respectively. For the purpose of comparison, the word segmentation performance of our system on Bakeoff-4 data using the 2- and 4-tag sets and the best corresponding n-gram feature templates as in (Tsai et al., 2006; Low et al., 2005) are presented in Table 8.7 This comparison reconfirms the conclusion in (Zhao et CityU data sets in any other situation than the Bakeoff. 7 The templates for the 2-tag set, adopted from (Tsai et al., 2006), include C−2 , C−1 , C0 , C1 , C−3 C−1 , C−2 C0 , C−2 C−1 , C−1 C0 , C−1 C1 and C0 C1 . Those for the 4-tag set, adopted from (Xue, 2003) and (Low et al., 2005), include C−2 , C−1 , C0 , C1 , C2 , C−2 C−1 , C−1 C0 , C−1 C1 , C0 C1 and C1 C2 . 109 POOV .6960 .7013 .7719 .5182 .6223 .7912 .7649 .7761 .5984 .7159 ROOV .7168 .7216 .8141 .6280 .7116 .7495 .7404 .7730 .6179 .74"
I08-4017,O03-4002,0,0.860294,"ates unsupervised segmentation and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chi"
I08-4017,W06-0126,0,0.0390691,"we have opted for AV for Bakeoff-4. Table 9: Comparison of computational cost Tags 2 4 6 2 4 6 2 4 6 Templates AS CityU CTB Training time (Minutes) Tsai 112 52 16 Xue 206 79 28 Zhao 402 146 47 6 Feature numbers (×10 ) Tsai 13.2 7.3 3.1 Xue 16.1 9.0 3.9 Zhao 15.6 8.8 3.8 Memory cost (Giga bytes) Tsai 5.4 2.4 0.9 Xue 6.6 2.8 1.1 Zhao 6.4 2.7 1.0 MSRA 4.3 35 73 117 NE List Features for Open NER We realize that the NE lists available to us are far from sufficient for coping with all NEs in Bakeoff4. It is reasonable that using richer external NE lists gives a better NER performance in many cases (Zhang et al., 2006). Surprisingly, however, the NE list features used in our NER do not lead to any significant performance improvement, according to the evaluation results in Table 7. This is certainly another issue for our further inspection. 5.5 6.8 6.6 1.8 2.2 2.1 tainly, a possible way out of this problem is the computer hardware advancement, which is predicted by Moore’s Law (Moore, 1965) to be improving at an exponential rate in general, including processing speed and memory capacity. Specifically, CPU can be made twice faster every other year or even 18 months. It is predictable that computational cost w"
I08-4017,W06-0127,1,0.845739,"n and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chinese Language Processing Evaluation,"
I08-4017,Y06-1012,1,0.918116,"n and conditional random fields (CRFs) learning successfully, using similar character tags and feature templates for both word segmentation and NER. It ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open NER tasks in the Bakeoff. Tag set selection and unsupervised segmentation play a critical role in this success. 1 Introduction A number of recent studies show that character sequence labeling is a simple but effective formulation of Chinese word segmentation and name entity recognition for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a; Chen et al., 2006). Character tagging becomes a prevailing technique for this kind of labeling task for Chinese language processing, following the current trend of applying machine learning as a core technology in the field of natural language processing. In particular, when a full-fledged general-purpose sequence learning model such as CRFs is involved, the only work to do for a given application is to identify an ideal set of features and hyperparameters for the purpose 1 The Fourth International Chinese Language Processing Bakeoff & the First CIPS Chinese Language Processing Evaluation,"
I08-4017,W03-1726,0,\N,Missing
I11-1141,W10-1408,0,0.0393951,"Missing"
I11-1141,H91-1060,0,0.129578,"es, even if they are all properly normalized. Usually, when combining two heterogeneous models, a weighting scheme is needed to balance their unequal effect. For this, we introduce another parameter λ to our factorized parsing model as P(T, S) = P(L, S)λ P(C, S) (4) This is known as a product-of-experts (Hinton, 2002; Cohen and Smith, 2007), where a combined distribution is defined by multiplying several component distributions and renormalizing them. The parameter λ can be tuned with the Gold Section Search algorithm (Press et al., 2007) on a development set, using the F-measure of PARSEVAL (Black et al., 1991) as objective function for training. Model I Sequential dependency and local context have shown their strength in tagging disambiguation (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). However, in the high-performance PCFG-based parsers (Collins, 1997; Charniak, 2000; Petrov and Klein, 2007), none of these features can be used due to the generative nature of these parsing models. To address this problem and incorporate the advantages of POS tagging technique into parsing, we propose a discriminative lexicon model following the global linear model (Collins, 2002). Let"
I11-1141,E03-1005,0,0.442419,"Missing"
I11-1141,D08-1092,0,0.154245,"Missing"
I11-1141,P05-1022,0,0.846295,"ackward algorithm, which are similar to those for HMM (Baum and Eagon, 1967; Baum and Sell, 1968). Different from the conventional tagging systems, our lexical model does not generate the best tag sequence for the whole sentence, but a lattice of tags, on which the joint inference with the constituent model can be performed. In our model, feature functions fm (τi , S) are primarily binary, each of which maps the local context to 1 if its feature exists, or to 0 otherwise. The only exception is the first one. Using a similar design as in discriminative re-scoring parsing models (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008), we have the first feature f0 to be the logarithm of the probability of ti being wi ’s tag in S, as proposed in Petrov and Klein (2007), which is f0 (τi , S) = log q(ti , S).1 Beside f0 (τi , S), all other features are similar to those in the previous works (Ratnaparkhi, 1996; Toutanova et al., 2003; Shen et al., 2007), as listed in Table 1. The signature features prefix and suffix, as used in Toutanova et al. (2003), each return a substring of certain length from the current word. In our experiments, this length ranges from 1 to 8. Since these prefix and suffix features are bli"
I11-1141,D09-1087,0,0.0148075,"rate parsing. In fact, the research on POS tagging and chunking shows that the most important information for disambiguation at this level comes from local context (Toutanova et al., 2003), especially the surrounding words. Our current work is an attempt to fill this gap between tagging and parsing, by the way of enabling a parser to use rich contextual features in its lexical model. 6 Related Work Most successful parsing models are generative models. Therefore, a large portion of previous related work did not change the generative nature of the lexical models involved (Goldberg et al., 2009; Huang and Harper, 2009; Attia et al., 2010). The key idea of these approaches is basically to advance smoothing techniques for the distribution P(w|t). Goldberg et al. (2009) adopt a trigram HMM tagging model trained on unannotated data to help prediting tags of rare words, but only the emission probabilities are used in parsing. Huang and Harper (2009) propose a better way to smooth the lexical model in a PCFG-LA parser similar to the Berkeley parser. Some morphological features are also used to handle unknown words in Chinese. In their evaluation, however, they exclude all unary rules that cause one of the major"
I11-1141,P08-1067,0,0.21546,"similar to those for HMM (Baum and Eagon, 1967; Baum and Sell, 1968). Different from the conventional tagging systems, our lexical model does not generate the best tag sequence for the whole sentence, but a lattice of tags, on which the joint inference with the constituent model can be performed. In our model, feature functions fm (τi , S) are primarily binary, each of which maps the local context to 1 if its feature exists, or to 0 otherwise. The only exception is the first one. Using a similar design as in discriminative re-scoring parsing models (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008), we have the first feature f0 to be the logarithm of the probability of ti being wi ’s tag in S, as proposed in Petrov and Klein (2007), which is f0 (τi , S) = log q(ti , S).1 Beside f0 (τi , S), all other features are similar to those in the previous works (Ratnaparkhi, 1996; Toutanova et al., 2003; Shen et al., 2007), as listed in Table 1. The signature features prefix and suffix, as used in Toutanova et al. (2003), each return a substring of certain length from the current word. In our experiments, this length ranges from 1 to 8. Since these prefix and suffix features are blindly extracted"
I11-1141,J93-2004,0,0.0369504,"Missing"
I11-1141,N07-1051,0,0.826213,"2002; Cohen and Smith, 2007), where a combined distribution is defined by multiplying several component distributions and renormalizing them. The parameter λ can be tuned with the Gold Section Search algorithm (Press et al., 2007) on a development set, using the F-measure of PARSEVAL (Black et al., 1991) as objective function for training. Model I Sequential dependency and local context have shown their strength in tagging disambiguation (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). However, in the high-performance PCFG-based parsers (Collins, 1997; Charniak, 2000; Petrov and Klein, 2007), none of these features can be used due to the generative nature of these parsing models. To address this problem and incorporate the advantages of POS tagging technique into parsing, we propose a discriminative lexicon model following the global linear model (Collins, 2002). Let τ = {t0 , t1 , t2 ...tn } be a tag sequence for a sentence S = {w0 , w1 , w2 ...wn }, we define the score of a tagged sequence to be Accordingly, the parsing model can be decomposed into two factors: P(T, S) = P(C, S)P(L, S) (3) Q where P(C, S) = R∈T P(R)|R |is a constituent Q |r| model and P(L, S) = a lexical r∈T P("
I11-1141,D07-1022,0,0.136102,", to indicate the composition of a constituent. Let r denote a lexical rule and R a constituent rule, the probability defined in (1) can be rewritten as Y Y P(T, S) = P(R)|R| P(r)|r| (2) R∈T r∈T 3 3.1 2.2 Product of Experts The two separated models may score in different magnitudes, even if they are all properly normalized. Usually, when combining two heterogeneous models, a weighting scheme is needed to balance their unequal effect. For this, we introduce another parameter λ to our factorized parsing model as P(T, S) = P(L, S)λ P(C, S) (4) This is known as a product-of-experts (Hinton, 2002; Cohen and Smith, 2007), where a combined distribution is defined by multiplying several component distributions and renormalizing them. The parameter λ can be tuned with the Gold Section Search algorithm (Press et al., 2007) on a development set, using the F-measure of PARSEVAL (Black et al., 1991) as objective function for training. Model I Sequential dependency and local context have shown their strength in tagging disambiguation (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). However, in the high-performance PCFG-based parsers (Collins, 1997; Charniak, 2000; Petrov and Klein, 2007), non"
I11-1141,P02-1034,0,0.0528924,"model provides the flexibility of integrating various well-developed POS tagging techniques into parsing, and it is also easier for optimization, in contrast to a complex joint model. It is reasonable that a better lexical model is expected to have better effects on parsing. Lexical Model SCORE(τ, S) = θ · f(τ, S) = n X i=1 θ · f (τi , S) (5) where f(τ, S) is a global feature vector and θ a vector of associated weights. A global feature is defined through a collection of local features f (τi , S). We train θ on a treebank using an averaged perceptron algorithm similar to the one presented in Collins and Duffy (2002) and Collins (2002). The number of iterations needed is optimized on the development set. However, introducing sequential dependency into this lexical model would cause a severe efficiency problem with the joint inference for parsing. When the Viterbi algorithm is used to search for the best parse tree, its efficiency relies heavily on tree structure. The interdependency of adjacent POS tags actually changes the structure of the parse under operation. For example, to determine the best sub-tree for a non-terminal XP covering a span of words [wi ...wj ], we need to calculate the score for wi ’s"
I11-1141,P97-1003,0,0.165239,"a product-of-experts (Hinton, 2002; Cohen and Smith, 2007), where a combined distribution is defined by multiplying several component distributions and renormalizing them. The parameter λ can be tuned with the Gold Section Search algorithm (Press et al., 2007) on a development set, using the F-measure of PARSEVAL (Black et al., 1991) as objective function for training. Model I Sequential dependency and local context have shown their strength in tagging disambiguation (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). However, in the high-performance PCFG-based parsers (Collins, 1997; Charniak, 2000; Petrov and Klein, 2007), none of these features can be used due to the generative nature of these parsing models. To address this problem and incorporate the advantages of POS tagging technique into parsing, we propose a discriminative lexicon model following the global linear model (Collins, 2002). Let τ = {t0 , t1 , t2 ...tn } be a tag sequence for a sentence S = {w0 , w1 , w2 ...wn }, we define the score of a tagged sequence to be Accordingly, the parsing model can be decomposed into two factors: P(T, S) = P(C, S)P(L, S) (3) Q where P(C, S) = R∈T P(R)|R |is a constituent Q"
I11-1141,W02-1001,0,0.0798902,"(Black et al., 1991) as objective function for training. Model I Sequential dependency and local context have shown their strength in tagging disambiguation (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). However, in the high-performance PCFG-based parsers (Collins, 1997; Charniak, 2000; Petrov and Klein, 2007), none of these features can be used due to the generative nature of these parsing models. To address this problem and incorporate the advantages of POS tagging technique into parsing, we propose a discriminative lexicon model following the global linear model (Collins, 2002). Let τ = {t0 , t1 , t2 ...tn } be a tag sequence for a sentence S = {w0 , w1 , w2 ...wn }, we define the score of a tagged sequence to be Accordingly, the parsing model can be decomposed into two factors: P(T, S) = P(C, S)P(L, S) (3) Q where P(C, S) = R∈T P(R)|R |is a constituent Q |r| model and P(L, S) = a lexical r∈T P(r) model. The best parse is then selected by the joint inference with these two submodels. This factored-out lexical model provides the flexibility of integrating various well-developed POS tagging techniques into parsing, and it is also easier for optimization, in contrast t"
I11-1141,E09-1038,0,0.0240445,"s never enough for accurate parsing. In fact, the research on POS tagging and chunking shows that the most important information for disambiguation at this level comes from local context (Toutanova et al., 2003), especially the surrounding words. Our current work is an attempt to fill this gap between tagging and parsing, by the way of enabling a parser to use rich contextual features in its lexical model. 6 Related Work Most successful parsing models are generative models. Therefore, a large portion of previous related work did not change the generative nature of the lexical models involved (Goldberg et al., 2009; Huang and Harper, 2009; Attia et al., 2010). The key idea of these approaches is basically to advance smoothing techniques for the distribution P(w|t). Goldberg et al. (2009) adopt a trigram HMM tagging model trained on unannotated data to help prediting tags of rare words, but only the emission probabilities are used in parsing. Huang and Harper (2009) propose a better way to smooth the lexical model in a PCFG-LA parser similar to the Berkeley parser. Some morphological features are also used to handle unknown words in Chinese. In their evaluation, however, they exclude all unary rules that"
I11-1141,P06-1055,0,0.0193738,"1996; Toutanova et al., 2003; Shen et al., 2007), as listed in Table 1. The signature features prefix and suffix, as used in Toutanova et al. (2003), each return a substring of certain length from the current word. In our experiments, this length ranges from 1 to 8. Since these prefix and suffix features are blindly extracted with templates that can be applied to any language, our lexical model is more 1 In the lexical model, for ti = A (a non-terminal symbol), the score q(ti , S) is calculated by: q(ti , S) = P x O(Ax , i, i + 1)P(Ax → wi ) I(ROOT, 0, |S|) (8) where x is the latent variable (Petrov et al., 2006), and I(·) and O(·) are inside and outside scores. For more details of q(ti , S), please refer to Figure 3 in Petrov and Klein (2007). Word Tag Signature Feature Template [wi ] (i = −2, −1, 0, 1, 2) [wi−2 , wi−1 ] (i = 0, 1, 2) [w−1 , w1 ] [wi−2 , wi−1 , wi ] (i = 0, 1, 2) [ti ] (i = −2, −1, 1, 2) [ti−2 , ti−1 ] (i = 0, 3) punctuator digit prefix suffix &[t0 ] &[t0 ] &[t0 ] &[t0 ] &[t0 ] &[t0 ] &[t0 ] &[t0 ] &[t0 ] &[t0 ] Table 1: Feature templates of lexical model language-independent than those using a predefined language-specific affix list. For feature selection, we also follow other resea"
I11-1141,D10-1001,0,0.0183628,"inally, individual words in an input sentence are found to be very useful in the disambiguation for POS tagging and even for recognizing baselevel constituents. However, the structural parameterization of conventional parsing models cannot incorporate and utilize them effectively. Nevertheless, how to make use of all sorts of information available to enhance parsing is still a challenging research topic. Acknowledgement The research described in this paper was partially supported by the Research Grants Council (RGC) of HKSAR, China, through the GRF grant 9041597. References The recent work of Rush et al. (2010) integrates parsing and tagging under the framework of dual-decomposition. This approach has the advantage to combine heterogeneous models, and solves the complex combinatory optimization problem via Lagrangian relaxation. However, it has a inevitable defect of inefficiency, since it requires to parse and tag the input sentence repetitively (usually 10 times for each sentence). Comparatively, our approach is more efficient. Note that lexical model II takes only a linear time in the length of input sentence. Without systematic comparison, however, it is difficult to tell which approach can prov"
I11-1141,P07-1096,0,0.30748,"not generate the best tag sequence for the whole sentence, but a lattice of tags, on which the joint inference with the constituent model can be performed. In our model, feature functions fm (τi , S) are primarily binary, each of which maps the local context to 1 if its feature exists, or to 0 otherwise. The only exception is the first one. Using a similar design as in discriminative re-scoring parsing models (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008), we have the first feature f0 to be the logarithm of the probability of ti being wi ’s tag in S, as proposed in Petrov and Klein (2007), which is f0 (τi , S) = log q(ti , S).1 Beside f0 (τi , S), all other features are similar to those in the previous works (Ratnaparkhi, 1996; Toutanova et al., 2003; Shen et al., 2007), as listed in Table 1. The signature features prefix and suffix, as used in Toutanova et al. (2003), each return a substring of certain length from the current word. In our experiments, this length ranges from 1 to 8. Since these prefix and suffix features are blindly extracted with templates that can be applied to any language, our lexical model is more 1 In the lexical model, for ti = A (a non-terminal symbol"
I11-1141,E09-1087,0,0.0290104,"Missing"
I11-1141,W00-1308,0,0.791039,"bank (Marcus et al., 1993). For a long time, however, parsing seems to have been evolving in parallel to tagging without much interaction with each other. Generative parsers (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Petrov and Klein, 2007), owing to their generative nature, all include a lexical probability model in the form P (w|t). The information used to predict the POS tag of a word mainly comes from the word itself and/or the ancestors that derive this word in a tree. Local context, which is proved to be the most useful information source for tagging (Ratnaparkhi, 1996; Toutanova and Manning, 2000), is not efficiently utilized by these parsers. Another noticable fact is that these high-performance parsers cannot do a better job of POS tagging than most of the stateof-the-art taggers (see Section 4 for a comparison). This is quite against our intuition that a parser having access to syntactic and contextual information from all over an input sentence should outperform a tagger that is limited to utilizing only local context and sequential dependency. This is an observation in Charniak et al. (1996). But their explanation is that a parser is built to find phrase markers, not tags. Then an"
I11-1141,N03-1033,0,0.802293,"ur factorized parsing model as P(T, S) = P(L, S)λ P(C, S) (4) This is known as a product-of-experts (Hinton, 2002; Cohen and Smith, 2007), where a combined distribution is defined by multiplying several component distributions and renormalizing them. The parameter λ can be tuned with the Gold Section Search algorithm (Press et al., 2007) on a development set, using the F-measure of PARSEVAL (Black et al., 1991) as objective function for training. Model I Sequential dependency and local context have shown their strength in tagging disambiguation (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). However, in the high-performance PCFG-based parsers (Collins, 1997; Charniak, 2000; Petrov and Klein, 2007), none of these features can be used due to the generative nature of these parsing models. To address this problem and incorporate the advantages of POS tagging technique into parsing, we propose a discriminative lexicon model following the global linear model (Collins, 2002). Let τ = {t0 , t1 , t2 ...tn } be a tag sequence for a sentence S = {w0 , w1 , w2 ...wn }, we define the score of a tagged sequence to be Accordingly, the parsing model can be decomposed into two factors: P(T, S) ="
I11-1141,P04-1013,0,0.278454,"Missing"
I11-1141,A00-2018,0,\N,Missing
I11-1141,I08-4010,1,\N,Missing
P09-1007,J90-2002,0,0.510764,"Missing"
P09-1007,D08-1092,0,0.0385535,"from CTL, City University of Hong Kong. 1 It is a tradition to call an annotated syntactic corpus as treebank in parsing community. 55 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP target language can effectively make use of them by only considering the most related information extracted from the translated text. The basic idea to support this work is to make use of the semantic connection between different languages. In this sense, it is related to the work of (Merlo et al., 2002) and (Burkett and Klein, 2008). The former showed that complementary information about English verbs can be extracted from their translations in a second language (Chinese) and the use of multilingual features improves classification performance of the English verbs. The latter iteratively trained a model to maximize the marginal likelihood of tree pairs, with alignments treated as latent variables, and then jointly parsing bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performance for both monolingual parse"
P09-1007,I08-1012,0,0.573078,"n draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treeb"
P09-1007,D07-1098,0,0.0104809,"ting overlapped features as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm is to search a parsing action sequence with the maximal probability. Sdi = argmax Y p(di |di−1 di−2 ...), i where Sdi is the object parsing action sequence, p(di |di−1 ...) is the conditional probability, and di 58 Figure 1: A comparison before and after translation is i-th parsing action. We use a beam search algorithm to find the object parsing action sequence. Table 2: Features for Parsing 5 Exploiting the Translated Treebank in .f orm, n = 0, 1 i.f orm + i1 .f orm in .char2 + in+1 .char2 , n = −1, 0 i.char−1 + i1 .char−1 in .char−2 n = 0, 3 i1 .char"
P09-1007,P06-1072,0,0.011487,"nsufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural"
P09-1007,P08-1068,0,0.00863736,"ble. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural language processing tasks, it is basically new"
P09-1007,E03-1008,0,0.0270136,"a resources to enhance an existing parser, it is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are"
P09-1007,P06-1043,0,0.0262404,"an existing parser, it is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our metho"
P09-1007,P08-1061,0,0.0118388,"scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle golden-standard treebank, while all the previous works focus on the unlabeled data. Although cross-language technique has been used in other natural language processing tasks,"
P09-1007,D07-1013,0,0.0336862,"om the source language to the target one. In detail, a word-based decoding is used, which adopts a loglinear framework as in (Och and Ney, 2002) with only two features, translation model and language model, P exp[ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp[ i=1 λi hi (c, e)] Where h1 (c, e) = log(pγ (c|e)) is the translation model, which is converted from the bilingual lexicon, and h2 (c, e) = log(pθ (c)) 4 Dependency Parsing: Baseline 4.1 Learning Model and Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. 2 StarDict is an open source dictionary software, available at http://stardict.sourceforge.net/. 57 With notations defined in Table 1, a feature set as shown in Table 2 is adopted. Here, we explain some terms in Tables 1 and 2. We used a large scale feature selection approach as in (Zhao et al., 2009) to obtain the feature set in Table 2. Some feature notations in this paper are also borrowed from that work. The feature curroot returns the root of"
P09-1007,W05-1516,0,0.0299467,"Missing"
P09-1007,E06-1011,0,0.0426299,"Missing"
P09-1007,W03-3023,0,0.385266,"Missing"
P09-1007,P05-1012,0,0.211703,"d-by-word decoding, where not a parallel corpus but a bilingual lexicon is necessary, is adopted for the treebank translation. Using an ensemble method, the key information extracted from word pairs with dependency relations in the translated text is effectively integrated into the parser for the target language. The proposed method is evaluated in English and Chinese treebanks. It is shown that a translated English treebank helps a Chinese parser obtain a state-ofthe-art result. 1 Introduction Although supervised learning methods bring stateof-the-art outcome for dependency parser inferring (McDonald et al., 2005; Hall et al., 2007), a large enough data set is often required for specific parsing accuracy according to this type of methods. However, to annotate syntactic structure, either phrase- or dependency-based, is a costly job. Until now, the largest treebanks1 in various languages for syntax learning are with around one million words (or some other similar units). Limited data stand in the way of further performance enhancement. This is the case for each individual language at least. But, this is not the case as we observe all treebanks in different languages as a whole. For example, of ten treeb"
P09-1007,C08-1132,0,0.217068,"ted in (Wang et al., 2007). The experimental results in (McDonald and Nivre, 2007) show a negative impact on the parsing accuracy from too long dependency relation. For the proposed method, the improvement relative to dependency length is shown in Figure 2. From the figure, it is seen that our method gives observable better performance when dependency lengths are larger than 4. Although word order is changed, the results here show that the useful information from the translated treebank still help those long distance dependencies. 4 There is a slight exception: using the same data splitting, (Yu et al., 2008) reported UAS without p as 0.873 versus ours, 0.870. 61 chosen to generate some additional features to enhance the parser for the target language. The experimental results in English and Chinese treebanks show the proposed method is effective and helps the Chinese parser in this work achieve a state-of-the-art result. Note that our method is evaluated in two treebanks with a similar annotation style and it avoids using too many linguistic properties. Thus the method is in the hope of being used in other similarly annotated treebanks 5 . For an immediate example, we may adopt a translated Chine"
P09-1007,P02-1027,0,0.011287,"by a research fellowship from CTL, City University of Hong Kong. 1 It is a tradition to call an annotated syntactic corpus as treebank in parsing community. 55 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55–63, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP target language can effectively make use of them by only considering the most related information extracted from the translated text. The basic idea to support this work is to make use of the semantic connection between different languages. In this sense, it is related to the work of (Merlo et al., 2002) and (Burkett and Klein, 2008). The former showed that complementary information about English verbs can be extracted from their translations in a second language (Chinese) and the use of multilingual features improves classification performance of the English verbs. The latter iteratively trained a model to maximize the marginal likelihood of tree pairs, with alignments treated as latent variables, and then jointly parsing bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performa"
P09-1007,I08-3008,0,0.108101,"g bilingual sentences in a translation pair. The proposed parser using features from monolingual and mutual constraints helped its log-linear model to achieve better performance for both monolingual parsers and machine translation system. In this work, cross-language features will be also adopted as the latter work. However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. Among of existing works that we are aware of, we regard that the most similar one to ours is (Zeman and Resnik, 2008), who adapted a parser to a new language that is much poorer in linguistic resources than the source language. However, there are two main differences between their work and ours. The first is that they considered a pair of sufficiently related languages, Danish and Swedish, and made full use of the similar characteristics of two languages. Here we consider two quite different languages, English and Chinese. As fewer language properties are concerned, our approach holds the more possibility to be extended to other language pairs than theirs. The second is that a parallel corpus is required for"
P09-1007,W08-2127,1,0.799644,"g actions, a shift action and a reduce action are also defined to maintain the stack and the unprocessed sequence. In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to right and right dependents are attached to their heads as soon as possible (Hall et al., 2007). While memory-based and margin-based learning approaches such as support vector machines are popularly applied to shift-reduce parsing, we apply maximum entropy model as the learning model for efficient training and adopting overlapped features as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm i"
P09-1007,W03-3017,0,0.04529,"he first letter of POS tag of word coarse POS: the first two POS tags of word the left nearest verb The first character of a word The first two characters of a word The last character of a word The last two characters of a word ’s, i.e., ‘s.dprel’ means dependent label of character in the top of stack Feature combination, i.e., ‘s.char+i.char’ means both s.char and i.char work as a feature function. Although the former will be also used as comparison, the latter is chosen as the main parsing framework by this study for the sake of efficiency. In detail, a shift-reduce method is adopted as in (Nivre, 2003), where a classifier is used to make a parsing decision step by step. In each step, the classifier checks a word pair, namely, s, the top of a stack that consists of the processed words, and, i, the first word in the (input) unprocessed sequence, to determine if a dependent relation should be established between them. Besides two dependency arc building actions, a shift action and a reduce action are also defined to maintain the stack and the unprocessed sequence. In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to ri"
P09-1007,W09-1208,1,0.28587,"nd Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. 2 StarDict is an open source dictionary software, available at http://stardict.sourceforge.net/. 57 With notations defined in Table 1, a feature set as shown in Table 2 is adopted. Here, we explain some terms in Tables 1 and 2. We used a large scale feature selection approach as in (Zhao et al., 2009) to obtain the feature set in Table 2. Some feature notations in this paper are also borrowed from that work. The feature curroot returns the root of a partial parsing tree that includes a specified node. The feature charseq returns a character sequence whose members are collected from all identified children for a specified word. In Table 2, as for concatenating multiple substrings into a feature string, there are two ways, seq and bag. The former is to concatenate all substrings without do something special. The latter will remove all duplicated substrings, sort the rest and concatenate all"
P09-1007,P02-1038,0,0.0152292,"y reached in fact, as the following case is frequently encountered, multiple English words have to be translated into one Chinese word. To solve this problem, we use a policy that lets the output Chinese word only inherits the attached information of the highest syntactic head in the original multiple English words. 3.2 Translation A word-by-word statistical machine translation strategy is adopted to translate words attached with the respective dependency information from the source language to the target one. In detail, a word-based decoding is used, which adopts a loglinear framework as in (Och and Ney, 2002) with only two features, translation model and language model, P exp[ 2i=1 λi hi (c, e)] P (c|e) = P P2 c exp[ i=1 λi hi (c, e)] Where h1 (c, e) = log(pγ (c|e)) is the translation model, which is converted from the bilingual lexicon, and h2 (c, e) = log(pθ (c)) 4 Dependency Parsing: Baseline 4.1 Learning Model and Features is the language model, a word trigram model trained from the CTB. In our experiment, we set two weights λ1 = λ2 = 1. According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either grap"
P09-1007,P07-1078,0,0.0121418,"is related to domain adaption for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation metho"
P09-1007,E09-1100,1,0.164926,"as our work in (Zhao and Kit, 2008), especially, those character-level ones for Chinese parsing. Our implementation of maximum entropy adopts L-BFGS algorithm for parameter optimization as usual. 4.2 Parsing using a Beam Search Algorithm In Table 2, the feature preactn returns the previous parsing action type, and the subscript n stands for the action order before the current action. These are a group of Markovian features. Without this type of features, a shift-reduce parser may directly scan through an input sequence in linear time. Otherwise, following the work of (Duan et al., 2007) and (Zhao, 2009), the parsing algorithm is to search a parsing action sequence with the maximal probability. Sdi = argmax Y p(di |di−1 di−2 ...), i where Sdi is the object parsing action sequence, p(di |di−1 ...) is the conditional probability, and di 58 Figure 1: A comparison before and after translation is i-th parsing action. We use a beam search algorithm to find the object parsing action sequence. Table 2: Features for Parsing 5 Exploiting the Translated Treebank in .f orm, n = 0, 1 i.f orm + i1 .f orm in .char2 + in+1 .char2 , n = −1, 0 i.char−1 + i1 .char−1 in .char−2 n = 0, 3 i1 .char−2 + i2 .char−2 +"
P09-1007,D07-1111,0,0.00895734,"for parsing that has been draw some interests in recent years. Typical domain adaptation tasks often assume annotated data in new domain absent or insufficient and a large scale unlabeled data available. As unlabeled data are concerned, semi-supervised or unsupervised methods will be naturally adopted. In previous works, two basic types of methods can be identified to enhance an existing parser from additional resources. The first is usually focus on exploiting automatic generated labeled data from the unlabeled data (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Chen et al., 2008), the second is on combining supervised and unsupervised methods, and only unlabeled data are considered (Smith and Eisner, 2006; Wang and Schuurmans, 2008; Koo et al., 2008). Our purpose in this study is to obtain a further performance enhancement by exploiting treebanks in other languages. This is similar to the above first type of methods, some assistant data should be automatically generated for the subsequent processing. The differences are what type of data are concerned with and how they are produced. In our method, a machine translation method is applied to tackle g"
P09-1007,D07-1097,0,\N,Missing
P09-1007,D07-1096,0,\N,Missing
P12-2001,H91-1060,0,0.61529,"weights of all constituents in the parse. However, this definition has a systematic bias towards selecting a parse with as many constituents as possible 3 Train. Dev. Test. English Section 2-21 Section 22/24 Section 23 Chinese Art. 1-270,400-1151 Art. 301-325 Art. 271-300 Table 2: Experiment Setup for the highest weight. A pruning threshold ρ, similar to the one in Sagae and Lavie (2006), is therefore needed to restrain the number of constituents in a recombined parse. The parameters λi and ρ are tuned by the Powell’s method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fossum and Knight (2009), using Section 24 instead of Section 22 of WSJ treebank as development set. In this work, the lexical model of Chen and Kit (2011) is combined with our syntactic model under the framework of product-of-expe"
P12-2001,E03-1005,0,0.0627614,"Missing"
P12-2001,D08-1092,0,0.0650622,"Missing"
P12-2001,D07-1101,0,0.0487409,"Missing"
P12-2001,P05-1022,0,0.170695,"Missing"
P12-2001,A00-2018,0,0.502733,"Missing"
P12-2001,I11-1141,1,0.855453,"Missing"
P12-2001,P02-1034,0,0.0168741,"Missing"
P12-2001,W02-1001,0,0.00737433,"earch Grants Council of HKSAR, China, through the GRF Grant 9041597 (CityU 144410). t∈T (s) 1 http://code.google.com/p/gazaparser/ 1 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1–5, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics (1) PP IN NP NP DT VP QP VBN PP the $ 32 million realized from the sales will be ... ... a portion of begin(b) split(m) end(e) Figure 1: A part of a parse tree centered at NP → NP VP where g(t, s) is a scoring function to evaluate the event that t is the parse of s. Following Collins (2002), this scoring function is formulated in the linear form g(t, s) = θ · Ψ(t, s), (2) where Ψ(t, s) is a vector of features and θ the vector of their associated weights. To ensure tractability, this model is factorized as g(t, s) = X r∈t g(Q(r), s) = X θ · Φ(Q(r), s), (3) r∈t where g(Q(r), s) scores Q(r), a part centered at grammar rule instance r in t, and Φ(Q(r), s) is the vector of features for Q(r). Each Q(r) makes its own contribution to g(t, s). A part in a parse tree is illustrated in Figure 1. It consists of the center grammar rule instance NP → NP VP and a set of immediate neighbors, i."
P12-2001,C96-1058,0,0.0529178,"Missing"
P12-2001,P08-1109,0,0.0302928,"Missing"
P12-2001,N09-2064,0,0.0182209,"al span features in Taskar et al. (2004) and Petrov and Klein (2008b) Similar to the bigrams features in Collins (2000) Similar to the grandparent rules features in Collins (2000) VP VP VP VP Table 1: Examples of lexical and structural feature niak and Johnson, 2005; Petrov and Klein, 2007) is therefore adopted in our implementation for efficiency enhancement. A parallel decoding strategy is also developed to further improve the efficiency without loss of optimality. Interested readers can refer to Chen (2012) for more technical details of this algorithm. 3 Constituent Recombination Following Fossum and Knight (2009), our constituent weighting scheme for parser combination uses multiple outputs of independent parsers. Suppose each parser generates a k-best parse list for an input sentence, the weight of a candidate constituent c is defined as ω(c) = XX i λi δ(c, ti,k )f (ti,k ), (5) k where i is the index of an individual parser, λi the weight indicating the confidence of a parser, δ(c, ti,k ) a binary function indicating whether c is contained in ti,k , the k-th parse output from the ith parser, and f (ti,k ) the score of the k-th parse assigned by the i-th parser, as defined in Fossum and Knight (2009)."
P12-2001,P04-1013,0,0.0455078,"Missing"
P12-2001,D10-1002,0,0.0247378,"Missing"
P12-2001,P08-1067,0,0.0532684,"two categories, namely, lexical and structural. All features extracted from the part in Figure 1 are demonstrated in Table 1. Some back-off structural features are used for smoothing, which cannot be presented due to limited space. With only lexical features in a part, this parsing model backs off to a first-order one similar to those in the previous works. Adding structural features, each involving a least a neighboring rule instance, makes it a higher-order parsing model. y 2.2 Decoding The factorization of the parsing model allows us to develop an exact decoding algorithm for it. Following Huang (2008), this algorithm traverses a parse forest in a bottom-up manner. However, it determines and keeps the best derivation for every grammar rule instance instead of for each node. Because all structures above the current rule instance is not determined yet, the computation of its nonlocal structural features, e.g., parent and sibling features, has to be delayed until it joins an upper level structure. For example, when computing the score of a derivation under the center rule NP → NP VP in Figure 1, the algorithm will extract child features from its children NP → DT QP and VP → VBN PP. The parent"
P12-2001,P01-1042,0,0.0876691,"Missing"
P12-2001,P10-1001,0,0.0271573,"Missing"
P12-2001,E06-1011,0,0.0602363,"Missing"
P12-2001,P05-1012,0,0.149463,"Missing"
P12-2001,H05-1066,0,0.151099,"Missing"
P12-2001,N07-1051,0,0.34077,"to g(t, s). A part in a parse tree is illustrated in Figure 1. It consists of the center grammar rule instance NP → NP VP and a set of immediate neighbors, i.e., its parent PP → IN NP, its children NP → DT QP and VP → VBN PP, and its sibling IN → of. This set of neighboring rule instances forms a local structural context to provide useful information to determine the plausibility of the center rule instance. 2.1 Feature The feature vector Φ(Q(r), s) consists of a series of features {φi (Q(r), s))|i ≥ 0}. The first feature φ0 (Q(r), s) is calculated with a PCFG-based generative parsing model (Petrov and Klein, 2007), as defined in (4) below, where r is the grammar rule instance A → B C that covers the span from the b-th PPP φ0 (Q(r), s) = x to the e-th word, splitting at the m-th word, x, y and z are latent variables in the PCFG-based model, and I(·) and O(·) are the inside and outside probabilities, respectively. All other features φi (Q(r), s) are binary functions that indicate whether a configuration exists in Q(r) and s. These features are by their own nature in two categories, namely, lexical and structural. All features extracted from the part in Figure 1 are demonstrated in Table 1. Some back-off"
P12-2001,D08-1091,0,0.500509,"Missing"
P12-2001,N10-1003,0,0.0317613,"Missing"
P12-2001,N06-2033,0,0.0214114,"contained in ti,k , the k-th parse output from the ith parser, and f (ti,k ) the score of the k-th parse assigned by the i-th parser, as defined in Fossum and Knight (2009). The weight of a recombined parse is defined as the sum of weights of all constituents in the parse. However, this definition has a systematic bias towards selecting a parse with as many constituents as possible 3 Train. Dev. Test. English Section 2-21 Section 22/24 Section 23 Chinese Art. 1-270,400-1151 Art. 301-325 Art. 271-300 Table 2: Experiment Setup for the highest weight. A pruning threshold ρ, similar to the one in Sagae and Lavie (2006), is therefore needed to restrain the number of constituents in a recombined parse. The parameters λi and ρ are tuned by the Powell’s method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al., 1991) as objective. 4 Experiment Our parsing models are evaluated on both English and Chinese treebanks, i.e., the WSJ section of Penn Treebank 3.0 (LDC99T42) and the Chinese Treebank 5.1 (LDC2005T01U01). In order to compare with previous works, we opt for the same split as in Petrov and Klein (2007), as listed in Table 2. For parser combination, we follow the setting of Fo"
P12-2001,W04-3201,0,0.0800488,"Missing"
P12-2001,D09-1161,0,0.0379286,"Missing"
P12-2001,W08-2102,0,\N,Missing
P13-1061,ma-2006-champollion,0,0.110518,"tckit,[yansong]}@[student.]cityu.edu.hk Abstract approaches resort to word correspondences in a bilingual lexicon to match bilingual sentences. A few sentence alignment methods and tools have also been explored to combine the two. Moore (2002) proposes a multi-pass search procedure using both sentence length and an automaticallyderived bilingual lexicon. Hunalign (Varga et al., 2005) is another sentence aligner that combines sentence length and a lexicon. Without a lexicon, it backs off to a length-based algorithm and then automatically derives a lexicon from the alignment result. Soon after, Ma (2006) develops the lexicon-based aligner Champollion, assuming that different words have different importance in aligning two sentences. Nevertheless, most existing approaches to sentence alignment follow the monotonicity assumption that coupled sentences in bitexts appear in a similar sequential order in two languages and crossings are not entertained in general (Langlais et al., 1998; Wu, 2010). Consequently the task of sentence alignment becomes handily solvable by means of such basic techniques as dynamic programming. In many scenarios, however, this prerequisite monotonicity cannot be guarante"
P13-1061,C10-2081,0,0.0176133,"of bilingual sentences. The general idea is that the closer two sentences are in length, the more likely they are to align. A notable difference of their methods is that the former uses sentence 628 Type 1-1 1-0 Micro P 0.827 0.359 0.809 Moore R F1 0.828 0.827 0.329 0.343 0.807 0.808 P 0.999 0.330 0.961 Hunalign R 0.972 0.457 0.951 F1 0.986 0.383 0.956 NonmoAlign P R F1 0.987 0.987 0.987 0.729 0.729 0.729 0.976 0.976 0.976 Table 3: Performance of monotonic alignment in comparison with the baseline methods. namic programming algorithm is applied to produce final alignment. Following this work, Li et al. (2010) propose a revised version of Champollion, attempting to improve its speed without performance loss. For this purpose, the input bitexts are first divided into smaller aligned fragments before applying Champollion to derive finer-grained sentence pairs. In another related work by Deng et al. (2007), a generative model is proposed, accompanied by two specific alignment strategies, i.e., dynamic programming and divisive clustering. Although a non-monotonic search process that tolerates two successive chunks in reverse order is involved, their work is essentially targeted at monotonic alignment."
P13-1061,moore-2002-fast,0,0.180659,"age of initial 1-1 alignment. Figure 4: Demonstration of monolingual consistency. The horizontal axis is the similarity of English sentence pairs and the vertical is the similarity of the corresponding pairs in Chinese. Type NonmoAlign initAlign 3.4 Non-Monotonic Alignment To test our aligner with non-monotonic sequences of sentences, we have them randomly scrambled in our experimental data. This undoubtedly increases the difficulty of sentence alignment, especially for the traditional approaches critically relying on monotonicity. The baseline methods used for comparison are Moore’s aligner (Moore, 2002) and Hunalign (Varga et al., 2005). Hunalign is configured with the option [-realign], which triggers a three-step procedure: after an initial alignment, Hunalign heuristically enriches its dictionary using word cooccurrences in identified sentence pairs; then, it re-runs the alignment process using the updated Impact of Initial Alignment The 1-1 initial alignment plays the role of labeled instances for the semisupervised learning. It is of critical importance to the learning performance. As shown in Table 1, our alignment function predicts 1451 1-1 pairings by virtue of anchor strings, among"
P13-1061,P91-1022,0,0.655052,"Missing"
P13-1061,J93-2003,0,0.215014,"Missing"
P13-1061,P93-1002,0,0.241488,"st divided into smaller aligned fragments before applying Champollion to derive finer-grained sentence pairs. In another related work by Deng et al. (2007), a generative model is proposed, accompanied by two specific alignment strategies, i.e., dynamic programming and divisive clustering. Although a non-monotonic search process that tolerates two successive chunks in reverse order is involved, their work is essentially targeted at monotonic alignment. length in number of characters while the latter in number of tokens. Both use dynamic programming to search for the best alignment. As shown in Chen (1993) and Wu (1994), however, sentencelength based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. The subsequent stage of sentence alignment research is accompanied by the advent of a handful of well-designed alignment tools. Moore (2002) proposes a three-pass procedure to find final alignment. Its bitext input is initially aligned based on sentence length. This step generates a set of strictly-selected sentence pairs for use to train an IBM translation model 1 (Brown et al., 1993). Its final step realigns the bitext using both sent"
P13-1061,H05-1010,0,0.0416629,"owing optimization A = arg max X s.t. m X i=1 Xij ≤ 1, m X n X Xij Fij created. Each posting includes a sentence identifier, the in-sentence frequency and positions of this term. The positions of terms are intersected to find common anchor strings. The anchor strings, once found, are used to calculate the initial affinity Fˆij of two sentences using Dice’s coefficient (7) i=1 j=1 n X j=1 Xij ≤ 1, Xij ∈ {0, 1} 2|C1i ∩ C2j | Fˆij = |C1i |+ |C2j | This turns sentence alignment into a problem to be resolved by binary linear programming (BIP), which has been successfully applied to word alignment (Taskar et al., 2005). Given a scoring matrix, it guarantees an optimal solution. 2.4 (8) where C1i and C2j are the anchor sets in si and tj , respectively, and |· |is the cardinality of a set. Apart from using anchor strings, other avenues for the initialization are studied in the evaluation section below, i.e., using another aligner and an existing lexicon. Alignment Initialization Once the above alignment function is available, the initial alignment matrix Aˆ can be derived from an initial relation matrix Fˆ obtained by an available alignment method. This work resorts to another approach to initializing the rel"
P13-1061,P91-1023,0,0.385877,"onmoAlign initialized with Hunalign (marked as NonmoAlign Hun) is also tested. The experimental results are presented in Figure 6. It shows that both Moore’s aligner and Hunalign work relatively well on bitexts with a low degree of nonmonotonicity, but their performance drops dramatically when the non-monotonicity is increased. Despite the improvement at low non-monotonicity by seeding our aligner with Hunalign, its performance decreases likewise when the degree of non-monotonicity increases, due to the quality de4 Related Work The research of sentence alignment originates in the early 1990s. Gale and Church (1991) and Brown (1991) report the early works using length statistics of bilingual sentences. The general idea is that the closer two sentences are in length, the more likely they are to align. A notable difference of their methods is that the former uses sentence 628 Type 1-1 1-0 Micro P 0.827 0.359 0.809 Moore R F1 0.828 0.827 0.329 0.343 0.807 0.808 P 0.999 0.330 0.961 Hunalign R 0.972 0.457 0.951 F1 0.986 0.383 0.956 NonmoAlign P R F1 0.987 0.987 0.987 0.729 0.729 0.729 0.976 0.976 0.976 Table 3: Performance of monotonic alignment in comparison with the baseline methods. namic programming algor"
P13-1061,P94-1012,0,0.200707,"ation (Brown et al., 1993), bilingual lexicography (Klavans et al., 1990), and cross-language information retrieval (Nie et al., 1999). Its objective is to identify correspondences between bilingual sentences in given bitexts. As summarized by Wu (2010), existing sentence alignment techniques rely mainly on sentence length and bilingual lexical resource. Approaches based on the former perform effectively on cognate languages but not on the others. For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two IndoEuropean languages (Wu, 1994). Lexicon-based 622 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 622–630, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 1. Interpretation of words and expressions 1.娆婆娆⎍䘬慳佑 "" British citizen"" ( 劙 ⚳ ℔ 㮹 ) means a person who has the status of a British citizen under the British Nationality Act 1981 (1981 c. 61 U.K.) Ⱦ⍿劙⚳ᾅ嬟Ṣ⢓ȿĩŃųŪŵŪŴũġűųŰŵŦŤŵŦťġűŦųŴŰůĪġ㊯㟡㒂˪ĲĺĹĲ⸜劙⚳⚳ 䯵㱽Ẍ˫ĩĲĺĹĲġŤįġķĲġŖįŌįĪ℟㚱⍿劙⚳ᾅ嬟Ṣ⢓幓↮䘬Ṣ ""British Dependent Territories citizen"" (劙⚳Ⱄ⛇℔㮹) means a person who has or had the status of a British Dependent"
P13-1061,I05-4010,1,0.867117,"Missing"
P13-1061,C90-3031,0,0.195217,"Missing"
P13-1061,P98-1117,0,0.139781,"Missing"
P13-1061,P10-1085,0,0.0166828,"liable information to link bilingual sentences into pairs, and thus can serve as useful cues for sentence alignment. In fact, they can be treated as a special type of highly reliable “bilexicon”. The anchor strings used in this work are derived by searching the bitexts using word-level inverted indexing, a basic technique widely used in information retrieval (Baeza-Yates and Ribeiro-Neto, 2011). For each index term, a list of postings is 2.5 Monolingual Affinity Although various kinds of information from a monolingual corpus have been exploited to boost statistical machine translation models (Liu et al., 2010; Su et al., 2012), we have not yet been exposed to any attempt to leverage monolingual sentence affinity for sentence alignment. In our framework, an attempt to this can be made through the computation of W and V . Let us take W as an example, where the entry Wij represents the affinity of sentence si and sentence sj , and it is set to 0 for i = j in order to avoid self-reinforcement during optimization (Zhou et al., 2004). When two sentences in S or T are not too short, or their content is not divergent in meaning, their semantic similarity can be estimated in terms of common words. Motivate"
P13-1061,J93-1004,0,\N,Missing
P13-1061,P12-1048,0,\N,Missing
P13-1061,J93-1006,0,\N,Missing
P13-1061,C98-1113,0,\N,Missing
S10-1100,E06-1027,0,0.0251099,"accuracy, and 0.936 and 0.933 (ranked 2nd and 3rd) micro accuracy, respectively. 1 Introduction Sentiment analysis is always puzzled by the context-dependent sentiment words that one word brings positive, neutral or negative meanings in different contexts. Hatzivassiloglou and Mckeown (1997) predicated the polarity of adjectives by using the pairs of adjectives linked by consecutive or negation conjunctions. Turney and Littman (2003) determined the polarity of sentiment words by estimating the point-wise mutual information between sentiment words and a set of seed words with strong polarity. Andreevskaia and Bergler (2006) used a Sentiment Tag Extraction Program to extract sentimentbearing adjectives from WordNet. Esuli and Sebasian (2006) studied the context-dependent sentiment words in WordNet but ignored the inChunyu Kit2 2 City University of Hong Kong, Hong Kong ctckit@cityu.edu.hk stances in real context. Wu et al. (2008) applied collocation plus a SVM classifier in Chinese sentiment adjectives disambiguation. Xu et al. (2008) proposed a semi-supervised learning algorithm to learn new sentiment word and their contextdependent characteristics. Semeval-2 Task 18 is designed to provide a common framework and"
S10-1100,P97-1023,0,0.769044,"Missing"
S10-1100,W05-0408,0,\N,Missing
S10-1100,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
W02-1808,W97-1016,0,\N,Missing
W02-1808,W99-0701,1,\N,Missing
W02-1808,J96-3004,0,\N,Missing
W02-1808,W99-0707,0,\N,Missing
W02-1808,Y98-1021,0,\N,Missing
W02-1808,J97-4004,0,\N,Missing
W02-1808,C92-4173,1,\N,Missing
W02-1808,C94-2203,0,\N,Missing
W02-1808,J01-3001,0,\N,Missing
W02-1808,J96-4004,0,\N,Missing
W02-1808,J01-3002,0,\N,Missing
W02-1808,C92-1019,0,\N,Missing
W02-1808,P97-1041,0,\N,Missing
W02-1808,J00-3004,0,\N,Missing
W02-1808,W95-0109,0,\N,Missing
W02-1808,O92-1003,0,\N,Missing
W02-1808,O97-4005,0,\N,Missing
W02-1808,Y95-1016,0,\N,Missing
W03-1724,Y98-1021,0,0.0224333,"sed learning is an effective approach to disambiguation. The basic idea behind the case-based learning is to utilize existing resolutions for known ambiguous strings to do disambiguation if similar ambiguities occur again. This learning strategy can be implemented in two straightforward steps: transformation rules. An advantage of this approach is that the rules so derived carry out not only disambiguation but also error correction. This links our disambiguation strategy to the application of Brill’s (1993) transformation-based error-driven learning to Chinese word segmentation (Palmer, 1997; Hockenmaier and Brew, 1998). 4 System architecture The overall architecture of our word segmentation system is presented in Figure 1. 1. Collection of correct answers from the training corpus for ambiguous strings together with their contexts, resulting in a set of contextdependent transformation rules; 2. Application of appropriate rules to ambiguous strings. A transformation rule of this type is actually an example of segmentation, indicating how an ambiguous string is segmented within a particular context. It has the following general form: l r C α C : α → w 1 w2 · · · w k where α is the ambiguous string, C l and C r"
W03-1724,W02-1808,1,0.801617,"language model training and disambiguation rule learning, analyze the system’s performance, and discuss areas for further improvement, e.g., out-of-vocabulary (OOV) word discovery. 1 Introduction After about two decades of studies of Chinese word segmentation, ICWSB-1 (henceforth, the bakeoff) is the first effort to put different approaches and systems to the test and comparison on common datasets. We participated in the bakeoff with a segmentation system that is designed to integrate a general-purpose ngram model for probabilistic segmentation and a case- or example-based learning approach (Kit et al., 2002) for disambiguation. The ngram model, with words extracted from training corpora, is trained with the EM algorithm (Dempster et al., 1977) using unsegmented training corpora. Originally it was developed to enhance word segmentation accuracy so as to facilitate Chinese-English word alignment for our ongoing EBMT project, where only unsegmented texts are available for training. It is expected to be robust enough to handle novel texts, independent of any segmented texts for training. To simplify the EM training, we used the uni-gram model for the bakeoff and relied on the Viterbi algorithm (Viter"
W03-1724,P97-1041,0,0.103131,"s that case-based learning is an effective approach to disambiguation. The basic idea behind the case-based learning is to utilize existing resolutions for known ambiguous strings to do disambiguation if similar ambiguities occur again. This learning strategy can be implemented in two straightforward steps: transformation rules. An advantage of this approach is that the rules so derived carry out not only disambiguation but also error correction. This links our disambiguation strategy to the application of Brill’s (1993) transformation-based error-driven learning to Chinese word segmentation (Palmer, 1997; Hockenmaier and Brew, 1998). 4 System architecture The overall architecture of our word segmentation system is presented in Figure 1. 1. Collection of correct answers from the training corpus for ambiguous strings together with their contexts, resulting in a set of contextdependent transformation rules; 2. Application of appropriate rules to ambiguous strings. A transformation rule of this type is actually an example of segmentation, indicating how an ambiguous string is segmented within a particular context. It has the following general form: l r C α C : α → w 1 w2 · · · w k where α is the"
W08-2127,D07-1097,0,0.13653,"Missing"
W08-2127,W04-2705,0,0.0552221,"fficiency, we opt for the maximum entropy model with Gaussian prior as our learning model for both the syntactic and semantic dependency parsing. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1 . 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basically a word-by-word method with a projective constraint. In each step, the classifier checks a word pair, e.g., TOP, the top of a stack for processed words, and, NEXT, the first word in the unprocessed word sequence, in order to determine if a dependent label should be assigned to them. Besides two arc-building actions, a shift action and a reduce action are"
W08-2127,P05-1013,0,0.141092,"Missing"
W08-2127,W03-3017,0,0.0592114,"odel adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1 . 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basically a word-by-word method with a projective constraint. In each step, the classifier checks a word pair, e.g., TOP, the top of a stack for processed words, and, NEXT, the first word in the unprocessed word sequence, in order to determine if a dependent label should be assigned to them. Besides two arc-building actions, a shift action and a reduce action are also defined to meet the projective constraint, as follows. 1 These two dictionaries that we used are downloaded from CoNLL-2008 official website. 203 CoNLL 2008: P"
W08-2127,J05-1004,0,0.066516,"opy model with Gaussian prior as our learning model for both the syntactic and semantic dependency parsing. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual (Liu and Nocedal, 1989). No additional feature selection techniques are applied. Our system consists of three components to deal with syntactic and semantic dependency parsing and word sense determination, respectively. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al., 2004) or the PropBank (Palmer et al., 2005)1 . 2.1 Syntactic Dependency Parsing We use a shift-reduce scheme to implement syntactic dependency parsing as in (Nivre, 2003). It takes a step-wised, history- or transition-based approach. It is basically a word-by-word method with a projective constraint. In each step, the classifier checks a word pair, e.g., TOP, the top of a stack for processed words, and, NEXT, the first word in the unprocessed word sequence, in order to determine if a dependent label should be assigned to them. Besides two arc-building actions, a shift action and a reduce action are also defined to meet the projective c"
W08-2127,W08-2121,0,0.270756,"Missing"
W08-2127,W04-3212,0,0.090202,"ependency tree. c preddir: the direction to the current predicate candidate. d voice: if the syntactic head of p is be and p is not ended with -ing, then p is passive. e posSeq: PoS tag sequence of all syntactic children f dprelSeq: syntactic dependent label sequence of all syntactic children g dpTreeLevel: the level in the syntactic parse tree, counted from the leaf node. out. Many prepositions are also marked as predicate in the training corpus, but their arguments’ roles are ‘SU’, which are not counted the official evaluation. For argument, a dependency version of the pruning algorithm in (Xue and Palmer, 2004) is used to find, in an iterative way, the current syntactic head and its siblings in a parse tree in a constituentbased representation. In this representation, the head of a phrase governs all its sisters in the tree, as illustrated in the conversion of constituents to dependencies in (Lin, 1995). In our implementation, the following equivalent algorithm is applied to select argument candidates from a syntactic dependency parse tree. Initialization: Set the given predicate candidate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates"
W08-2127,C04-1186,0,\N,Missing
W09-1208,burchardt-etal-2006-salsa,0,0.0394038,"Missing"
W09-1208,kawahara-etal-2002-construction,0,0.0206595,"Missing"
W09-1208,W08-2121,0,0.200501,"Missing"
W09-1208,taule-etal-2008-ancora,0,0.0308401,"Missing"
W09-1208,W04-3212,0,0.0624688,"actly same way. When no constraint available, however, all word pairs in the an input sequence must be considered, leading to very poor efficiency in computation for no gain in effectiveness. Thus, the training sample needs to be pruned properly. As predicates overtly known in the share task, we only consider how to effectively prune argument candidates. We adopt five types of argument pruning strategies for seven languages. All of them assume that a syntactic dependency parsing tree is available. As for Chinese and English, we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004) as described in (Zhao and Kit, 2008). The pruning algorithm is readdressed as the following. Initialization: Set the given predicate candidate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates. (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached. Note that the given predicate candidate itself is excluded from the argument candidate list for Chinese, that is slightly different from English. The above pruning algorithm has been shown effective. However, it is still inefficient for a singlest"
W09-1208,W08-2127,1,0.844842,"40861 (CityU 1318/03H), CityU Strategic Research Grant 7002037, Projects 60673041 and 60873041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the ”863” National High-Tech Research and Development of China. 55 We opt for the maximum entropy model with Gaussian prior as our learning model for all classification subtasks in the shared task. Our implementation of the model adopts L-BFGS algorithm for parameter optimization as usual. No additional feature selection techniques are applied. Our system is basically improved from its early version for CoNLL-2008 (Zhao and Kit, 2008). By introducing a virtual root for every predicates, The job to determine both argument labels and predicate senses is formulated as a word-pair classification task in four languages, namely, Catalan, Spanish, Czech and Japanese. In other three languages, Chinese, English and German, a predicate sense classifier is individually trained before argument label classification. Note that traditionally (or you may say that most semantic parsing systems did so) argument identification and classification are handled in a two-stage pipeline, while ours always tackles them in one step, in addition, pre"
W09-1208,W09-1209,1,0.783826,"ust and stable results. The first is that two results for development and test sets in the same language are quite close. The second is about out-ofdomain (OOD) task. Though for each OOD task, we just used the same model trained from the respective language and did nothing to strengthen it, this does not hinder our system to obtain top results in Czech and English OOD tasks. In addition, the feature template sets from automatical selection procedure in this task were used for the joint task of this shared task, and also output top results according to the average score of semantic labeled F1 (Zhao et al., 2009). Development with Gold Development Test (official scores) Out-of-domain average 81.24 80.46 80.47 74.34 Catalan 81.52 80.66 80.32 Chinese 78.32 77.90 77.72 Czech 86.96 85.35 85.19 85.44 English 84.19 84.01 85.44 73.31 German 77.75 76.55 75.99 64.26 Japanese 78.67 78.41 78.15 Spanish 81.32 80.39 80.46 Table 6: Semantic labeled F1 Catalan Sense Argument Training memory (MB) Training time (Min.) Test time (Min.) Training memory (GB) Training time (Hours) Test time (Min.) 0.4 3.0 3.0 Chinese 418.0 11.0 0.7 3.7 13.8 144.0 Czech 3.2 24.9 27.1 English 136.0 2.5 0.2 3.8 12.4 88.0 German 63.0 1.7 0.03"
W09-3511,D08-1037,0,0.0396655,"set an intermediate layer to represent the source and target names by phonemes or phonetic tags (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Gao et al., 2004). Having been studied extensively though, the phonemes-based approaches cannot break its performance ceiling for two reasons (Li et al., 2004): (1) Languagedependent phoneme representation is not easy to obtain; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transliteration units for a better correspondence. There is 57 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57–60, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP 2.1 Training for the NEWS2009 task (Li et al., 2009) and present the experimented results. 2 For the purpose of modeling the training data, the characters from both the source and target name entities for training are split up for alignment, and then phrase extraction is conducted to fin"
W09-3511,P04-1021,0,0.498239,"Introduction To transliterate a foreign name into a target language, a direct instrument is to make use of existing rules for converting text to syllabus, or at least a phoneme base to support such transformation. Following this path, the well developed noisy channel model used for transliteration usually set an intermediate layer to represent the source and target names by phonemes or phonetic tags (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Gao et al., 2004). Having been studied extensively though, the phonemes-based approaches cannot break its performance ceiling for two reasons (Li et al., 2004): (1) Languagedependent phoneme representation is not easy to obtain; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transliteration units for a better correspondence. There is 57 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57–60, c Suntec, Singapore, 7 Au"
W09-3511,P07-1016,0,0.0181121,"teration usually set an intermediate layer to represent the source and target names by phonemes or phonetic tags (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Gao et al., 2004). Having been studied extensively though, the phonemes-based approaches cannot break its performance ceiling for two reasons (Li et al., 2004): (1) Languagedependent phoneme representation is not easy to obtain; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transliteration units for a better correspondence. There is 57 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57–60, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP 2.1 Training for the NEWS2009 task (Li et al., 2009) and present the experimented results. 2 For the purpose of modeling the training data, the characters from both the source and target name entities for training are split up for alignment, and then phrase ex"
W09-3511,W09-3501,0,0.0395223,"in; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transliteration units for a better correspondence. There is 57 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57–60, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP 2.1 Training for the NEWS2009 task (Li et al., 2009) and present the experimented results. 2 For the purpose of modeling the training data, the characters from both the source and target name entities for training are split up for alignment, and then phrase extraction is conducted to find the mapping pairs of character sequence. The alignment is performed by expectationmaximization (EM) iterations in the IBM model-4 SMT training using the GIZA++ toolkit1 . In some runs, however, e.g., English to Chinese and English to Korean transliteration, the character number of the source text is always more than that of the target text, the training conduc"
W09-3511,P02-1038,0,0.0501158,"ehn et al., 2003), with maximum length 10, which is tuned on development data, for both the sourceto-target and the target-to-source character alignment. Then two transliteration models, namely p(t|s) and p(s|t), are generated by such extraction for each transliteration run. Another component involved in the training is an n-gram language model. We set n = 3 and have it trained with the available data of the target language in question. Transliteration as SMT In order to transliterate effectively via a phrase based SMT process for our transliteration task, we opt for the log-linear framework (Och and Ney, 2002), a straight-forward architecture to have several feature models integrated together as P exp[ ni=1 λi hi (s, t)] Pn P (t|s) = P (1) t exp[ i=1 λi hi (s, t)] Then the transliteration task is to find the proper source and corresponding target chunks to maximize P (t|s) as t = argmax P (t|s) (2) t In (1), hi (s, t) is a feature model formulated as a probability functions on a pair of source and target texts in logarithmic form, and λi is a parameter to optimize its contribution. The two most important models in this framework are the translation model (i.e., the transliteration model in our case"
W09-3511,P03-1021,0,0.0113349,"ur task, we have tested these choices for p(s, t) on all our development data, arriving at a similar result. However, we opt to use both p(s|t) and p(t|s) if they give similar transliteration quality in some language pairs. Thus we take p(t|s) for our primary transliteration model for searching candidate corresponding character sequences, and p(s|t) as a supplement. In addition to the translation model feature, another feature for the language model can be described as hi (s, t) = log p(t) 2.2 Optimization Using the development sets for the NEWS2009 task, a minimum error rate training (MERT) (Och, 2003) is applied to tune the parameters for the corresponding feature models in (1). The training is performed with regard to the mean F-score, which is also called fuzziness in top-1, measuring on average how different the top transliteration candidate is from its closest reference. It is worth noting that a high mean F-score indicates a high accuracy of top candidates, thus a high mean reciprocal rank (MRR), which is used to quantify the overall performance of transliteration. (4) Usually the n-gram language model is used for its effectiveness and simplicity. 1 58 http://code.google.com/p/giza-pp"
W09-3511,W03-1508,0,0.106229,"rated in the decoding process. Our evaluated results indicate that this approach performs well in all standard runs in the NEWS2009 Machine Transliteration Shared Task. 1 Introduction To transliterate a foreign name into a target language, a direct instrument is to make use of existing rules for converting text to syllabus, or at least a phoneme base to support such transformation. Following this path, the well developed noisy channel model used for transliteration usually set an intermediate layer to represent the source and target names by phonemes or phonetic tags (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Gao et al., 2004). Having been studied extensively though, the phonemes-based approaches cannot break its performance ceiling for two reasons (Li et al., 2004): (1) Languagedependent phoneme representation is not easy to obtain; (2) The phonemic representation to source and target names usually causes error spread. Several approaches have been proposed for direct use of parallel texts for performance enhancement (Li et al., 2004; Li et al., 2007; Goldwasser and Roth, 2008). There is no straightforward mean for grouping characters or letters in the source or target language into better transl"
W09-3511,koen-2004-pharaoh,0,\N,Missing
W09-3511,J98-4003,0,\N,Missing
W10-1755,W05-0909,0,0.123348,"piloted a paradigm evolution to MT evaluation. It provides a computable solution to the task and turns it into an engineering problem of measuring text similarity and simulating human judgments of translation quality. Related studies in recent years have extensively revealed more essential characteristics of BLEU, including its strengths and weaknesses. This has aroused the proposal of different new evaluation metrics aimed at addressing such weaknesses so as to find some other hopefully better alternatives for the task. Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al., 2009) that seem to have already achieved considerably strong correlations with human judgments. Nevertheless, few metrics have really nurtured our understanding of possible parameters involved in our language comprehension and text quality judgment. This inadequacy limits, inevitably, the application of the existing metrics. The ATEC metric (Wong and Kit, 2008) was developed as a response to this inadequacy, with a focus to account for the process of human comprehension of sentences via two fundamental features of text, namely word choice and word order. It integrates"
W10-1755,W09-0441,0,0.017976,"valuation. It provides a computable solution to the task and turns it into an engineering problem of measuring text similarity and simulating human judgments of translation quality. Related studies in recent years have extensively revealed more essential characteristics of BLEU, including its strengths and weaknesses. This has aroused the proposal of different new evaluation metrics aimed at addressing such weaknesses so as to find some other hopefully better alternatives for the task. Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al., 2009) that seem to have already achieved considerably strong correlations with human judgments. Nevertheless, few metrics have really nurtured our understanding of possible parameters involved in our language comprehension and text quality judgment. This inadequacy limits, inevitably, the application of the existing metrics. The ATEC metric (Wong and Kit, 2008) was developed as a response to this inadequacy, with a focus to account for the process of human comprehension of sentences via two fundamental features of text, namely word choice and word order. It integrates various explicit measures for"
W10-1755,vilar-etal-2006-error,0,0.0485623,"uding its new features and optimization of parameters. In particular we will discuss how the design of this metric can complement the inadequacies of other metrics in terms of its treatment of word choice and word order and its utilization of multiple references in the evaluation process. 2 2.1 The ATEC Metric Word Choice In general, word is the basic meaning bearing unit of language. In a semantic theory such as Latent Semantic Analysis (LSA) (Landauer et al., 1998), lexical selection is even the sole consideration of the meaning of a text. A recent study of the major errors in MT outputs by Vilar et al. (2006) also reveals that different kinds of error related to word choices constitute a majority of error types. It is therefore of prime importance 1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2008/ 360 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 360–364, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics the evaluation set. In case of a high-frequency word whose tf-idf weight is less than 1, it is then rounded up to 1. In addition to matched words, unmatched words are also considered to have a role to play in"
W10-1755,wong-2010-semantic,1,0.786086,"s to diagnose the adequacy of word selection by an MT system. It is a general consensus that the performance of an evaluation metric can be improved by matching more words between MT outputs and human references. Linguistic resources like stemmer and WordNet are widely applied by many metrics for matching word stems and synonyms. ATEC is equipped with these two modules as well, and furthermore, with two measures for word similarity, including a WordNet-based (Wu and Palmer, 1994) and a corpus-based measure (Landauer et al., 1998) for matching word pairs of similar meanings. Our previous work (Wong, 2010) shows that the inclusion of semantically similar words results in a positive correlation gain comparable to the use of WordNet for synonym identification. In addition to increasing the number of legitimate matches, we also consider the importance of each match. Although most metrics score every matched word with equal weight, different words indeed contribute different amount of information to the meaning of a sentence. In Example 1 below, both C1 and C2 contain the same number of words matched with Ref, but the matches in C1 are more informative and therefore should be assigned higher weight"
W10-1755,P94-1019,0,0.0166513,"weighting to unmatched reference words so as to quantify the information missed in the MT outputs in question. for MT evaluation metrics to diagnose the adequacy of word selection by an MT system. It is a general consensus that the performance of an evaluation metric can be improved by matching more words between MT outputs and human references. Linguistic resources like stemmer and WordNet are widely applied by many metrics for matching word stems and synonyms. ATEC is equipped with these two modules as well, and furthermore, with two measures for word similarity, including a WordNet-based (Wu and Palmer, 1994) and a corpus-based measure (Landauer et al., 1998) for matching word pairs of similar meanings. Our previous work (Wong, 2010) shows that the inclusion of semantically similar words results in a positive correlation gain comparable to the use of WordNet for synonym identification. In addition to increasing the number of legitimate matches, we also consider the importance of each match. Although most metrics score every matched word with equal weight, different words indeed contribute different amount of information to the meaning of a sentence. In Example 1 below, both C1 and C2 contain the s"
W10-1755,P02-1040,0,0.0831549,"e ATEC metric for automatic MT evaluation, with parameters optimized for word choice and word order, the two fundamental features of language that the metric relies on. The former is assessed by matching at various linguistic levels and weighting the informativeness of both matched and unmatched words. The latter is quantified in term of word position and information flow. We also discuss those aspects of language not yet covered by other existing evaluation metrics but carefully considered in the formulation of our metric. 1 Introduction It is recognized that the proposal of the BLEU metric (Papineni et al., 2002) has piloted a paradigm evolution to MT evaluation. It provides a computable solution to the task and turns it into an engineering problem of measuring text similarity and simulating human judgments of translation quality. Related studies in recent years have extensively revealed more essential characteristics of BLEU, including its strengths and weaknesses. This has aroused the proposal of different new evaluation metrics aimed at addressing such weaknesses so as to find some other hopefully better alternatives for the task. Effort in this direction brings up some advanced metrics such as MET"
W10-2409,W02-1001,0,0.00994382,"ates are generated for each source name. 3 Algorithm 1 Averaged perceptron training Input: Candidate list with reference {LIST (xj , yj )nj=1 , yi∗ }N i=1 Output: Averaged parameters 1: ω ~ ← 0, ω ~ a ← 0, c ← 1 2: for t = 1 to T do 3: for i = 1 to N do 4: yˆi ← argmaxy∈LIST (xj ,yj ) ω ~ · Φ(xi , yi ) 5: if yˆi 6= yi∗ then 6: ω ~ ←ω ~ + Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi ) 7: ω ~a ← ω ~ a + c · {Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi )} 8: end if 9: c←c+1 10: end for 11: end for 12: return ω ~ −ω ~ a /c Reranking 3.1 Learning Framework For reranking training and prediction, we adopt the averaged perceptron (Collins, 2002) as our learning framework, which has a more stable performance than the non-averaged version. It is presented in Algorithm 1. Where ω ~ is the vector of parameters we want to optimize, x, y are the corresponding source (with different syllabification) and target graphemes in the candidate list, and Φ represents the feature vector in the pair of x and y. In this algorithm, reference yi∗ is the most appropriate output in the candidate list according to the true target named entity in the training data. We use the Mean-F score to identify which candidate can be the reference, by locating the one"
W10-2409,P04-1021,0,0.394968,"sult from the candidates. We only consider bi-grams when using this feature. Target grapheme chain feature, f (tii−2 ); This feature measures the appropriateness of the generated target graphemes on both character and syllables level. It performs in a similar way as the language model for SMT decoding. We use tri-gram syllables in this learning framework. 3.2 Multiple Features The following features are used in our reranking process: Paired source-to-target transition feature, f (< s, t >ii−1 ); Transliteration correspondence feature, f (si , ti ); This type of feature is firstly proposed in (Li et al., 2004), aiming at generating source and target graphemes simultaneously under a suitable constraint. We use this feature to restrict the synchronous transition of both source and target graphemes, measuring how well are those transitions, such as for “st”, This feature describes the mapping between source and target graphemes, similar to the transliteration options in the phrase table in our previous generation process, where s and 2 In this work, we use Pinyin as the phonetic representation for Chinese. 63 whether “s” transliterated by “斯” is followed by “t” transliterated by “特”. In order to deal"
W10-2409,P03-1021,0,0.012959,"62–65, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics try in it. After that, we use a phoneme resource2 to refine the phrase table by filtering out the wrongly extracted phrases and cleaning up the noise in it. In the decoding process, a dynamic pruning is performed when generating the hypothesis in each step, in which the threshold is variable according to the current searching space, for we need to obtain a good candidate list as precise as possible for the next stage. The parameter for each feature function in log-linear model is optimized by MERT training (Och, 2003). Finally, a maximum number of 50 candidates are generated for each source name. 3 Algorithm 1 Averaged perceptron training Input: Candidate list with reference {LIST (xj , yj )nj=1 , yi∗ }N i=1 Output: Averaged parameters 1: ω ~ ← 0, ω ~ a ← 0, c ← 1 2: for t = 1 to T do 3: for i = 1 to N do 4: yˆi ← argmaxy∈LIST (xj ,yj ) ω ~ · Φ(xi , yi ) 5: if yˆi 6= yi∗ then 6: ω ~ ←ω ~ + Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi ) 7: ω ~a ← ω ~ a + c · {Φ(x∗i , yi∗ ) − Φ(ˆ xi , yˆi )} 8: end if 9: c←c+1 10: end for 11: end for 12: return ω ~ −ω ~ a /c Reranking 3.1 Learning Framework For reranking training and predic"
W10-2409,W09-3506,0,0.0503016,"sult, it is found in (Song et al., 2009) that, in contrast to the ordinary top-1 accuracy (ACC) score, its recall rate, which is defined in terms of whether the correct answer is generated in the n-best output list, is rather high. This observation suggests that if we could rearrange those outputs into a better order, especially, push the correct one to the top, the overall performance could be enhanced significantly, without any further refinement of the original generation process. This reranking strategy is proved to be efficient in transliteration generation with a multi-engine approach (Oh et al., 2009). In this paper, we present our recent work on reranking the transliteration candidates via an online discriminative learning framework, namely, the averaged perceptron. Multiple features are incorporated into it for performance enhancement. The following sections will give the technical details of our method and present its results for NEWS2010 shared task for named entity transliteration. Effective transliteration of proper names via grapheme conversion needs to find transliteration patterns in training data, and then generate optimized candidates for testing samples accordingly. However, th"
W10-2409,W09-3511,1,0.92429,"it is still unreliable to rank the candidates simply by their statistical translation scores for the purpose of selecting the best one. In order to make a proper choice, the direct orthographic mapping requires a precise alignment and a better transliteration option selection. Thus, powerful algorithms for effective use of the parallel data is indispensable, especially when the available data is limited in volume. Interestingly, although an SMT based approach could not achieve a precise top-1 transliteration re2 Generation For the generation of transliteration candidates, we follow the work (Song et al., 2009), using a phrase-based SMT procedure with the log-linear model P exp[ ni=1 λi hi (s, t)] Pn P (t|s) = P t exp[ i=1 λi hi (s, t)] (1) for decoding. Originally we use two directional phrase1 tables, which are learned for both directions of source-to-target and target-to-source, containing different entries of transliteration options. In order to facilitate the decoding by exploiting all possible choices in a better way, we combine the forward and backward directed phrase tables together, and recalculate the probability for each en1 It herein refers to a character sequence as described in (Song e"
W10-4109,P07-1094,0,0.0298747,"ties between contexts. Experiment results show the output state sequence of HMM are highly correlated to the latent annotations of gold POS tags, in context of clustering similarity measures. The other experiments on a real application, unsupervised dependency parsing, reveal that the output sequence can replace the manually annotated tags without loss of accuracies. 1 Introduction Recently latent variable model has shown great potential in recovering the underlying structures. For example, the task of POS tagging is to recover the appropriate sequence structure given the input word sequence (Goldwater and Griffiths, 2007). One of the most popular example of latent models is Hidden Markov Model (HMM), which has been extensively studied for many years (Rabiner, 1989). The key problem of HMM is how to find an optimal hidden state number and the topology appropriately. Kwok-Ping Chan Department of Computer Science the University of Hong Kong Hong Kong kpchan@cs.hku.hk In most cases, the topology of HMM is predefined by exploiting the domain or empirical knowledge. This topology will be fixed during the whole process. Therefore how to select the optimal topology for a certain application or a set of training data i"
W10-4109,N06-1041,0,0.060561,"Missing"
W10-4109,C08-1042,0,0.0169919,"e-to-many clustering, because the same word play different roles under different contexts. We evaluate the similarity between the distribution of contexts, with the assumption that the context distribution implies syntactic pattern of the given word (Zelling, 1968; Weeds and Weir, 2003). With this clustering, more contextual information can be considered without increasing the model complexity. A relatively simple model is important for unsupervised task in terms of computational burden and data sparseness. This is the reason why we do not increase the order of HMM(Kaji and Kitsuregawa, 2008; Headden et al., 2008). With unsupervised algorithms, there are two aspects to be evaluated (Van Gael et al., 2009). Fist one is how good the outcome clusters are. We compare the HMM results with the manually POS tags and report the similarity measures based on information theory. On the other hand, we test how good the outputs act as an intermediate results. In many natural language tasks, the inputs are word class, not the actual lexical item, for reason of sparsity. In this paper, we choose the unsupervised dependency parsing as the application to investigate whether our clusters can replace the manual labeled t"
W10-4109,N09-1012,0,0.0580045,"Missing"
W10-4109,D07-1117,0,0.0310976,"Missing"
W10-4109,D07-1031,0,0.013513,"mber is initialized with a relatively large number. During the training, the states are merged or trimmed and ended with a small set of states. On the other hand, the top-down methods (Siddiqi et al., 2007) start from a small state set and split one or some states until no further improvement can be obtained. The bottom-up approaches require huge computational cost in deciding the states to be merged, which makes it impractical for applications with large state space. In this paper, we focus on the latter approaches. Another problem in HMM is that EM algorithm might yield local maximum value. Johnson (2007) points out that training HMM with EM gives poor results because it leads to a fairly flat distribution of hidden states when the empirical distribution is highly skewed. A multinomial prior, which favors sparse distribution, is a good choice for natural language tasks. In this paper, we proposed a new procedure for inferring the HMM topology and estimating its parameters simultaneously. Gibbs sampling has been used in infinite HMM (iHMM) (Beal et al., 2001; Fox et al., 2008; Van Gael et al., 2008) for inference. Unfortunately Gibbs sampling is slow and difficult to be converged. In this paper"
W10-4109,C08-1051,0,0.0222027,"the same cluster. It is a one-to-many clustering, because the same word play different roles under different contexts. We evaluate the similarity between the distribution of contexts, with the assumption that the context distribution implies syntactic pattern of the given word (Zelling, 1968; Weeds and Weir, 2003). With this clustering, more contextual information can be considered without increasing the model complexity. A relatively simple model is important for unsupervised task in terms of computational burden and data sparseness. This is the reason why we do not increase the order of HMM(Kaji and Kitsuregawa, 2008; Headden et al., 2008). With unsupervised algorithms, there are two aspects to be evaluated (Van Gael et al., 2009). Fist one is how good the outcome clusters are. We compare the HMM results with the manually POS tags and report the similarity measures based on information theory. On the other hand, we test how good the outputs act as an intermediate results. In many natural language tasks, the inputs are word class, not the actual lexical item, for reason of sparsity. In this paper, we choose the unsupervised dependency parsing as the application to investigate whether our clusters can repla"
W10-4109,P04-1061,0,0.0392902,"2005). 5.2 Dependency Parsing Evaluation The next experiment is to test the goodness of the outcome states of our model in the context of real tasks. In this work, we consider unsupervised dependency parsing for a fully unsupervised system. The dependency parsing is to extract the dependency graph whose nodes are the words of the given sentence. The dependency graph is a directed acyclic graph in which every edge links from a head word to its dependent. Because we work on unsupervised methods in this paper, we choose a simple generative head-outward model (Dependency Model with Valence, DMV) (Klein and Manning, 2004; Headden III et al., 2009) for parsing. The data through the experiment is restricted to the sentences up to length 10 (excluding punctuation). Because the main purpose is to test the HMM output rather than to improve the parsing performance, we select the original DMV model without extensions or modifications. Starting from the root, DMV generates the head, and then each head recursively generates its left and right dependents. In each direction, the possible dependents are repeatedly chosen until a STOP marker is seen. DMV use inside-outside algorithm for re-estimation. We choose the “harmo"
W10-4109,D07-1043,0,0.0995076,"Missing"
W10-4109,I05-3005,0,0.0686805,"Missing"
W10-4109,D09-1071,0,0.0245406,"Missing"
W10-4109,W03-1011,0,0.0364317,"he adaptive HMM model with Dirichlet prior. It involves a modification to the Baum-Welch algorithm. In each iteration, we replaced only one hidden state with two new states until convergence. To reduce the number of observation variables, the words are pre-clustered and represented by the exemplar within the same cluster. It is a one-to-many clustering, because the same word play different roles under different contexts. We evaluate the similarity between the distribution of contexts, with the assumption that the context distribution implies syntactic pattern of the given word (Zelling, 1968; Weeds and Weir, 2003). With this clustering, more contextual information can be considered without increasing the model complexity. A relatively simple model is important for unsupervised task in terms of computational burden and data sparseness. This is the reason why we do not increase the order of HMM(Kaji and Kitsuregawa, 2008; Headden et al., 2008). With unsupervised algorithms, there are two aspects to be evaluated (Van Gael et al., 2009). Fist one is how good the outcome clusters are. We compare the HMM results with the manually POS tags and report the similarity measures based on information theory. On the"
W10-4112,I05-3003,0,0.0310596,"of a sentence. In Chinese, it usually organizes two parts into a well-formed sentence, one with a subject and its adjunct, and the other with an object and/or complement (Luo et al., 1994). Accurate identification of predicate head is thus critical in determining the syntactic structure of a sentence. Moreover, a predicate head splitting a long sentence into two shorter parts can alleviate the complexity of syntactic analysis to a certain degree. This is particularly useful when long dependency relations are involved. Without doubt, this is also a difficult task in Chinese dependency parsing (Cheng et al., 2005). Predicate head identification also plays an important role in facilitating various tasks of natural language processing. For example, it enhances shallow parsing (Sun et al., 2000) and headdriven parsing (Collins, 1999), and also improves the precision of sentence similarity computation (Sui et al., 1998a). There is reason to expect it to be more widely applicable to other tasks, e.g. machine translation, information extraction, and question answering. In this paper, we propose an effective approach to automatically recognize predicate heads of Chinese sentences based on a preprocessing step"
W10-4112,J03-4003,0,\N,Missing
W10-4121,C04-1200,0,0.0221948,"ation and knowledge in subjective texts (Liu et al, 2008). This technique has wide and many real world applications, such as e-commerce, business intelligence, information monitoring, public opinion poll, e-learning, newspaper and publication compilation, and business management. For instance, a typical opinion mining system produces statistical results from online product reviews, which can be used by potential customers when deciding which model to choose, by manufacturers to find out the possible areas of improvement, and by dealers for sales plan evaluation (Yao et al, 2008). According to Kim and Hovy (2004), an opinion is composed of four parts, namely, topic, holder, sentiment, and claim, in which the holder expresses the claim including positive or negative sentiment towards the topic. For example, in the sentence I like this car, I is the holder, like is the positive sentiment, car is the topic, and the whole sentence is the claim. Research on Chinese opinion mining technology requires the support of annotated corpus for Chinese opinioned-subjective text. Since the corpus includes deep level information related to word segmentation, part-of-speech, syntax, semantics, opinioned elements, and s"
W10-4154,S07-1027,0,0.0127506,"b People Search (WePS and WePS2) provides a standard evaluation, which focuses on information extraction of personal named-entities in Web data (Artiles et al., 2007; Artiles et al., 2009; Sekine and Artiles, 2009). Generally speaking, both clusterbased techniques which cluster documents corresponding to one person with similar contexts, global features and document features (Han et al. 2004; Pedersen et al. 2005; Elmacioglu et al. 2007; Pedersen and Anagha 2007; Rao et al. 2007) and information extraction based techniques which recognizes/extracts the description features of one person name (Heyl and Neumann 2007; Chen et al. 2009) are adopted. Considering that these evaluations are only applied to English text, CIPS-SIGHAN 2010 bakeoff proposed the first evaluation campaign on Chinese person name disambiguation. In this evaluation, corresponding to given index person name string, the systems are required to recognize each identical person having the index string as substring and classify the document corresponding to each identical person into a group. This paper presents the design and implementation of HITSZ_CITYU system in this bakeoff. This system incorporates both recognition/extract technique a"
W10-4154,S07-1012,0,0.0355233,"Missing"
W10-4154,S07-1042,0,0.0266269,"Missing"
W10-4154,S07-1058,0,0.0132327,"ambiguation (Fleischman and Hovy 2004; Li et al. 2004; Niu et al. 2004; Bekkerman and McCallum 2005; Chen and Martin 2007; Song et al. 2009). To promote the research in this area, Web People Search (WePS and WePS2) provides a standard evaluation, which focuses on information extraction of personal named-entities in Web data (Artiles et al., 2007; Artiles et al., 2009; Sekine and Artiles, 2009). Generally speaking, both clusterbased techniques which cluster documents corresponding to one person with similar contexts, global features and document features (Han et al. 2004; Pedersen et al. 2005; Elmacioglu et al. 2007; Pedersen and Anagha 2007; Rao et al. 2007) and information extraction based techniques which recognizes/extracts the description features of one person name (Heyl and Neumann 2007; Chen et al. 2009) are adopted. Considering that these evaluations are only applied to English text, CIPS-SIGHAN 2010 bakeoff proposed the first evaluation campaign on Chinese person name disambiguation. In this evaluation, corresponding to given index person name string, the systems are required to recognize each identical person having the index string as substring and classify the document corresponding to each"
W12-6304,P08-1102,0,0.143622,"Missing"
W12-6304,P10-2040,0,0.046377,"Missing"
W12-6304,P11-1141,0,0.090252,"e probabilistic nature of this approach can lead to an even lower error rate in real applications. To the best of our knowledge, this is the first attempt on wide-coverage semisupervised automatic annotation of Chinese word structures. 2 POS-like tags called form classes. Also, rules in both systems are more or less syntactic. Computational linguists have also started rethinking the limitations of feature-based machine learning approaches to CWS and have called for morphology-based analysis of OOV words (Dong et al., 2010). There are a few pivotal works in this direction, such as Zhao (2009), Li (2011) and Li & Zhou (2012). Zhao (2009) has proposed a character-based dependency parsing model, based on the annotation of unlabeled inword character dependencies. While this is a valuable investigation, the deadlock of OOV word detection suggests that pure character-wise dependencies may be inadequate to model the morphological process. Li (2011) and Li & Zhou (2012) have proposed models of joint morphological and syntactical analysis, for constituent and dependency parsing, respectively. Both are based on the same annotation of word structures for CTB. Influenced by Packard (2000), they only ann"
W12-6304,J96-1002,0,0.0604926,"Missing"
W12-6304,D12-1132,0,0.185816,"c nature of this approach can lead to an even lower error rate in real applications. To the best of our knowledge, this is the first attempt on wide-coverage semisupervised automatic annotation of Chinese word structures. 2 POS-like tags called form classes. Also, rules in both systems are more or less syntactic. Computational linguists have also started rethinking the limitations of feature-based machine learning approaches to CWS and have called for morphology-based analysis of OOV words (Dong et al., 2010). There are a few pivotal works in this direction, such as Zhao (2009), Li (2011) and Li & Zhou (2012). Zhao (2009) has proposed a character-based dependency parsing model, based on the annotation of unlabeled inword character dependencies. While this is a valuable investigation, the deadlock of OOV word detection suggests that pure character-wise dependencies may be inadequate to model the morphological process. Li (2011) and Li & Zhou (2012) have proposed models of joint morphological and syntactical analysis, for constituent and dependency parsing, respectively. Both are based on the same annotation of word structures for CTB. Influenced by Packard (2000), they only annotated words that con"
W12-6304,zhao-etal-2010-large,1,0.898377,"Missing"
W12-6304,C04-1081,0,0.245815,"Missing"
W12-6304,C10-2139,0,0.0643083,"Missing"
W12-6304,P11-1139,0,0.0768812,"Missing"
W12-6304,D11-1090,0,0.0691408,"Missing"
W12-6304,C10-1132,0,0.0475425,"Missing"
W12-6304,W03-1728,0,0.0769864,"Missing"
W12-6304,P95-1026,0,0.409999,"Missing"
W12-6304,P08-1101,0,0.0536488,"Missing"
W12-6304,E09-1100,0,0.242904,"ected that the probabilistic nature of this approach can lead to an even lower error rate in real applications. To the best of our knowledge, this is the first attempt on wide-coverage semisupervised automatic annotation of Chinese word structures. 2 POS-like tags called form classes. Also, rules in both systems are more or less syntactic. Computational linguists have also started rethinking the limitations of feature-based machine learning approaches to CWS and have called for morphology-based analysis of OOV words (Dong et al., 2010). There are a few pivotal works in this direction, such as Zhao (2009), Li (2011) and Li & Zhou (2012). Zhao (2009) has proposed a character-based dependency parsing model, based on the annotation of unlabeled inword character dependencies. While this is a valuable investigation, the deadlock of OOV word detection suggests that pure character-wise dependencies may be inadequate to model the morphological process. Li (2011) and Li & Zhou (2012) have proposed models of joint morphological and syntactical analysis, for constituent and dependency parsing, respectively. Both are based on the same annotation of word structures for CTB. Influenced by Packard (2000), th"
W12-6304,I08-4017,1,0.793537,"Missing"
W12-6304,W10-4126,0,\N,Missing
W12-6304,W10-4101,0,\N,Missing
W13-2517,J90-2002,0,0.795094,"Missing"
W13-2517,P97-1063,0,0.152116,"Missing"
W13-2517,J03-3002,0,0.0689173,"igation and maintenance (Nie, 2010). The most common strategy is to create a parallel structure in terms of URL hierarchies, exploiting some known naming conventions for webpages of corresponding languages (Huang and Tilley, 2001; Nie, 2010). Following available structures and naming conventions, researchers have been exploring various means to mine parallel corpora from the web and a good number of such systems have demonstrated the feasibility and practicality in automatic acquisition of parallel corpora from bilingual and/or multilingual web sites, e.g., STRAND (Resnik, 1998; Resnik, 1999; Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTMiner (Chen and Nie, 2000), PTI (Chen et al., 2004), WPDE (Zhang et al., 2006), the DOM tree alignment model (Shi et al., 2006), PagePairGetter (YE et al., 2008) and Bitextor (Espl`a-Gomis and Forcada, 2010). Most of these systems are run in three steps: first, bilingual websites are identified and crawled; second, pairs of parallel webpages are extracted; and finally, the extracted pairs are validated (Kit and Ng, 2007). Among them, prior knowledge about parallel webpages, mostly in the form of ad hoc heuristics for identifying webpage languages or pre-define"
W13-2517,resnik-1998-parallel,0,0.0955583,"a way to facilitate both navigation and maintenance (Nie, 2010). The most common strategy is to create a parallel structure in terms of URL hierarchies, exploiting some known naming conventions for webpages of corresponding languages (Huang and Tilley, 2001; Nie, 2010). Following available structures and naming conventions, researchers have been exploring various means to mine parallel corpora from the web and a good number of such systems have demonstrated the feasibility and practicality in automatic acquisition of parallel corpora from bilingual and/or multilingual web sites, e.g., STRAND (Resnik, 1998; Resnik, 1999; Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTMiner (Chen and Nie, 2000), PTI (Chen et al., 2004), WPDE (Zhang et al., 2006), the DOM tree alignment model (Shi et al., 2006), PagePairGetter (YE et al., 2008) and Bitextor (Espl`a-Gomis and Forcada, 2010). Most of these systems are run in three steps: first, bilingual websites are identified and crawled; second, pairs of parallel webpages are extracted; and finally, the extracted pairs are validated (Kit and Ng, 2007). Among them, prior knowledge about parallel webpages, mostly in the form of ad hoc heuristics for iden"
W13-2517,H91-1026,0,0.566695,"Missing"
W13-2517,P09-1098,0,0.0409409,"Missing"
W13-2517,P99-1068,0,0.188587,"Missing"
W13-2517,P06-1062,0,0.0233649,"or webpages of corresponding languages (Huang and Tilley, 2001; Nie, 2010). Following available structures and naming conventions, researchers have been exploring various means to mine parallel corpora from the web and a good number of such systems have demonstrated the feasibility and practicality in automatic acquisition of parallel corpora from bilingual and/or multilingual web sites, e.g., STRAND (Resnik, 1998; Resnik, 1999; Resnik and Smith, 2003), BITS (Ma and Liberman, 1999), PTMiner (Chen and Nie, 2000), PTI (Chen et al., 2004), WPDE (Zhang et al., 2006), the DOM tree alignment model (Shi et al., 2006), PagePairGetter (YE et al., 2008) and Bitextor (Espl`a-Gomis and Forcada, 2010). Most of these systems are run in three steps: first, bilingual websites are identified and crawled; second, pairs of parallel webpages are extracted; and finally, the extracted pairs are validated (Kit and Ng, 2007). Among them, prior knowledge about parallel webpages, mostly in the form of ad hoc heuristics for identifying webpage languages or pre-defined patterns for matching or computing similarity between webpages, is commonly used for webpage pair extraction (Chen and Nie, 2000; Resnik and Smith, 2003; Zhang"
W13-2517,tsvetkov-wintner-2010-automatic,0,0.0145123,") and Bitextor (Espl`a-Gomis and Forcada, 2010). Most of these systems are run in three steps: first, bilingual websites are identified and crawled; second, pairs of parallel webpages are extracted; and finally, the extracted pairs are validated (Kit and Ng, 2007). Among them, prior knowledge about parallel webpages, mostly in the form of ad hoc heuristics for identifying webpage languages or pre-defined patterns for matching or computing similarity between webpages, is commonly used for webpage pair extraction (Chen and Nie, 2000; Resnik and Smith, 2003; Zhang et al., 2006; Shi et al., 2006; Yulia and Shuly, 2010; Tom´as et al., 2008). Specifically, these systems exploit search engines and heuristics across webpage anchors to locate candidate bilingual websites and then identify webpage pairs based on pre-defined URL matching patterns. However, ad hoc heuristics cannot exhaust all possible patterns. Many webpages do not even have any language label in their anchors, not to mention many untrustworthy labels. Also, using a limited set of pre138 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 138–143, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Li"
W13-2517,1999.mtsummit-1.79,0,\N,Missing
W99-0701,J93-2004,0,0.0635051,"Missing"
W99-0701,W93-0231,0,0.105862,"Missing"
W99-0701,H89-2010,0,0.0452752,"Missing"
Y08-1025,I05-3009,0,0.0191718,"all scale corpora. However, its disadvantages include inadequate coverage of pre-defined syntactic patterns, low transplantability to other domains or languages, and incapability of excluding non-term candidates consistent with the pre-defined patterns. Statistical Various kinds of statistical information can be utilized to support term extraction, e.g., frequency (Damerau, 1990), mutual information (Damerau, 1993), C-value (Frantzi and Ananiadou, 1996), NC-value (Frantzi et al., 1998), imp function (Nakagawa, 2001), KFIDF measure (Xu et al., 2002), standard deviation (Lin, 2004) and entropy (Chang, 2005), to name but a few. A multi-word term is assumed to carry a key concept and is thus expected to behave like an atomic text unit. Many of these statistical measures are applied to explore such unity or structural stability of a multi-word candidate, namely, its unithood. Besides, a bootstrapping approach is reported in Chen et al. (2003) to learn domain specific terms from unannotated texts on a subject. Wermter and Hahn (2005) identify multi-word terms among n-grams of words in a large biomedical corpus, measuring their termhood in terms of their paradigmatic modifiability. Although statistic"
Y08-1025,A94-1006,0,0.0702565,". 2. Previous Work Various approaches to ATR were developed in the past. From a methodological point of view, the existing approaches can be classified into the following categories. Linguistic A linguistic approach played a dominant role in the early research on ATR. As early as twenty years ago, Ananiadou (1988) studied the effectiveness of theoretically motivated linguistic knowledge (e.g. morphology) in term recognition. A common procedure involved in this kind of approach is to carry out part-of-speech tagging first and then some pre-defined syntactic patterns, e.g., noun-noun compounds (Dagan and Church, 1994; Wu and Hsu, 2002) and base noun phrases (Justeson and Katz, 1995), can be applied to identify term candidates. All word combinations that match none of the predefined patterns are filtered out. This approach was reported to achieve good results on small scale corpora. However, its disadvantages include inadequate coverage of pre-defined syntactic patterns, low transplantability to other domains or languages, and incapability of excluding non-term candidates consistent with the pre-defined patterns. Statistical Various kinds of statistical information can be utilized to support term extractio"
Y08-1025,C96-1009,0,0.0619863,"ed to identify term candidates. All word combinations that match none of the predefined patterns are filtered out. This approach was reported to achieve good results on small scale corpora. However, its disadvantages include inadequate coverage of pre-defined syntactic patterns, low transplantability to other domains or languages, and incapability of excluding non-term candidates consistent with the pre-defined patterns. Statistical Various kinds of statistical information can be utilized to support term extraction, e.g., frequency (Damerau, 1990), mutual information (Damerau, 1993), C-value (Frantzi and Ananiadou, 1996), NC-value (Frantzi et al., 1998), imp function (Nakagawa, 2001), KFIDF measure (Xu et al., 2002), standard deviation (Lin, 2004) and entropy (Chang, 2005), to name but a few. A multi-word term is assumed to carry a key concept and is thus expected to behave like an atomic text unit. Many of these statistical measures are applied to explore such unity or structural stability of a multi-word candidate, namely, its unithood. Besides, a bootstrapping approach is reported in Chen et al. (2003) to learn domain specific terms from unannotated texts on a subject. Wermter and Hahn (2005) identify mult"
Y08-1025,I05-4010,1,0.891304,"Missing"
Y08-1025,W00-0901,0,0.10709,"Missing"
Y08-1025,C02-2013,0,0.0327947,"us approaches to ATR were developed in the past. From a methodological point of view, the existing approaches can be classified into the following categories. Linguistic A linguistic approach played a dominant role in the early research on ATR. As early as twenty years ago, Ananiadou (1988) studied the effectiveness of theoretically motivated linguistic knowledge (e.g. morphology) in term recognition. A common procedure involved in this kind of approach is to carry out part-of-speech tagging first and then some pre-defined syntactic patterns, e.g., noun-noun compounds (Dagan and Church, 1994; Wu and Hsu, 2002) and base noun phrases (Justeson and Katz, 1995), can be applied to identify term candidates. All word combinations that match none of the predefined patterns are filtered out. This approach was reported to achieve good results on small scale corpora. However, its disadvantages include inadequate coverage of pre-defined syntactic patterns, low transplantability to other domains or languages, and incapability of excluding non-term candidates consistent with the pre-defined patterns. Statistical Various kinds of statistical information can be utilized to support term extraction, e.g., frequency"
Y08-1025,xu-etal-2002-domain,0,0.0245207,"out. This approach was reported to achieve good results on small scale corpora. However, its disadvantages include inadequate coverage of pre-defined syntactic patterns, low transplantability to other domains or languages, and incapability of excluding non-term candidates consistent with the pre-defined patterns. Statistical Various kinds of statistical information can be utilized to support term extraction, e.g., frequency (Damerau, 1990), mutual information (Damerau, 1993), C-value (Frantzi and Ananiadou, 1996), NC-value (Frantzi et al., 1998), imp function (Nakagawa, 2001), KFIDF measure (Xu et al., 2002), standard deviation (Lin, 2004) and entropy (Chang, 2005), to name but a few. A multi-word term is assumed to carry a key concept and is thus expected to behave like an atomic text unit. Many of these statistical measures are applied to explore such unity or structural stability of a multi-word candidate, namely, its unithood. Besides, a bootstrapping approach is reported in Chen et al. (2003) to learn domain specific terms from unannotated texts on a subject. Wermter and Hahn (2005) identify multi-word terms among n-grams of words in a large biomedical corpus, measuring their termhood in ter"
Y08-1025,C00-2136,0,0.0518776,"Missing"
Y08-1025,C94-2167,0,\N,Missing
zhao-etal-2010-large,C04-1081,0,\N,Missing
zhao-etal-2010-large,P01-1005,0,\N,Missing
zhao-etal-2010-large,O03-4002,0,\N,Missing
zhao-etal-2010-large,W06-0137,1,\N,Missing
zhao-etal-2010-large,W06-0115,0,\N,Missing
zhao-etal-2010-large,W06-0127,1,\N,Missing
zhao-etal-2010-large,I08-4017,1,\N,Missing
zhao-etal-2010-large,Y06-1012,1,\N,Missing
