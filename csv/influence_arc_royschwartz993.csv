2020.acl-main.43,N18-1205,0,0.0296558,"port this conjecture. 1 Figure 1: Hierarchy of state expressiveness for saturated RNNs and related models. The y axis represents increasing space complexity. ∅ means provably empty. Models are in bold with qualitative descriptions in gray. Introduction While neural networks are central to the performance of today’s strongest NLP systems, theoretical understanding of the formal properties of different kinds of networks is still limited. It is established, for example, that the Elman (1990) RNN is Turing-complete, given infinite precision and computation time (Siegelmann and Sontag, 1992, 1994; Chen et al., 2018). But tightening these unrealistic assumptions has serious implications for expressive power (Weiss et al., 2018), leaving a significant gap between classical theory and practice, which theorems in this paper attempt to address. Recently, Peng et al. (2018) introduced rational RNNs, a subclass of RNNs whose internal state can be computed by independent weighted finite automata (WFAs). Intuitively, such models have a computationally simpler recurrent update than conventional models like long short-term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997). Empirically, rational RNNs like th"
2020.acl-main.43,D19-1110,1,0.85501,"ower (Weiss et al., 2018), leaving a significant gap between classical theory and practice, which theorems in this paper attempt to address. Recently, Peng et al. (2018) introduced rational RNNs, a subclass of RNNs whose internal state can be computed by independent weighted finite automata (WFAs). Intuitively, such models have a computationally simpler recurrent update than conventional models like long short-term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997). Empirically, rational RNNs like the quasirecurrent neural network (QRNN; Bradbury et al., 2016) and unigram rational RNN (Dodge et al., 2019) perform comparably to the LSTM, with a smaller computational budget. Still, the underlying simplicity of rational models raises the question of whether their expressive power is fundamentally limited compared to other RNNs. In a separate line of work, Merrill (2019) introduced the saturated RNN1 as a formal model for analyzing the capacity of RNNs. A saturated RNN is a simplified network where all activation functions have been replaced by step functions. The saturated network may be seen intuitively as a “stable” version of its original RNN, in which the in1 Originally referred to as the asy"
2020.acl-main.43,W18-2501,0,0.0432401,"Missing"
2020.acl-main.43,2020.tacl-1.11,0,0.111331,"Missing"
2020.acl-main.43,D14-1181,0,\N,Missing
2020.acl-main.43,D18-1152,1,\N,Missing
2020.acl-main.43,W19-3905,0,\N,Missing
2020.acl-main.587,N18-1033,0,0.0501582,"achine Translation Datasets. We experiment with two machine translation datasets: • WMT14 EN-DE (Bojar et al., 2014).11 Following previous practice (Vaswani et al., 2017) we train on WMT14, and designate newstest2013 and newstest2014 as development and test data respectively. Our preprocessing follows that of Vaswani et al. (2017) and Ott et al. (2018). A shared source-target vocabulary is used, with 32k byte pair encoding types (BPE; Sennrich et al., 2016). • IWSLT14 DE-EN (Cettolo et al., 2014).12 It is based on TED talks, and is much smaller compared to WMT14. We use the preprocessing from Edunov et al. (2018). Following previous practice, we use separate vocabularies for the source and target, with around 9K and 7K BPE types respectively. Table 1 summarizes some statistics of the datasets. 10 Preliminary results show that mixing experts with fewer heads leads to underwhelming performance. We conjecture this is due to too strong a regularization effect (§3). 11 https://drive.google.com/a/ haopeng.name/uc?export=download&id=0B_ bZck-ksdkpM25jRUN2X2UxMm8 12 http://workshop2014.iwslt.org/. Data Train Dev. Test Vocab. WMT14 4.5M IWSLT14 160K 3K 7K 3K 7K 32K 9K/7K Table 1: Some statistics for WMT14 and"
2020.acl-main.587,K18-1056,0,0.102237,"CD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana et al., 2004; Liu et al., 2018; Dutt et al., 2017, inter alia). Recent efforts also explore MoE in sequence learning (Shazeer et al., 2017), and to promote diversity in text generation (He et al., 2018; Shen et al., 2019; Cho et al., 2019, inter alia). 7 Conclusion We presented MAE. It is inspired by a mixture-ofexperts perspective of multi-head attention. With a learned gating function, MAE activates different experts on different inputs. MAE is trained using a block coordinate descent algorithm, which alternates between updating the responsibilities of 6573 the experts and their parameters. Our experiments show that MAE outperforms the transformer baselines on machine translation and language modeling benchmarks. The analysis shows that MAE learns to activate different experts. The code i"
2020.acl-main.587,P19-2030,0,0.0202487,"etuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana e"
2020.acl-main.587,D19-1308,0,0.0935488,"from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana et al., 2004; Liu et al., 2018; Dutt et al., 2017, inter alia). Recent efforts also explore MoE in sequence learning (Shazeer et al., 2017), and to promote diversity in text generation (He et al., 2018; Shen et al., 2019; Cho et al., 2019, inter alia). 7 Conclusion We presented MAE. It is inspired by a mixture-ofexperts perspective of multi-head attention. With a learned gating function, MAE activates different experts on different inputs. MAE is trained using a block coordinate descent algorithm, which alternates between updating the responsibilities of 6573 the experts and their parameters. Our experiments show that MAE outperforms the transformer baselines on machine translation and language modeling benchmarks. The analysis shows that MAE learns to activate different experts. The code is publicly available at https://githu"
2020.acl-main.587,J90-1003,0,0.0989812,"less clear. gating weights and ignore the rest, instead of linearly combining them as in Eq. 6. We see from Table 5 a 0.3 BLEU decrease under this setting. In comparison, NO BCD has a larger performance decrease of 0.7 BLEU. NO BCD’s performance drop is similar to that of UNI -M AE -7, for which we randomly select an expert at each layer and average the performance over 5 runs. These results support the proposition that MAE specializes better when trained with BCD. Finally, we search for the tokens that are more likely to activate each expert. We compute the pointwise mutual information (PMI; Church and Hanks, 1990) between tokens and experts: PMI(tokeni , expertj ) = log p(tokeni , expertj ) . p(tokeni )p(expertj ) Table 6 lists the most indicative tokens of each expert, for the first layer. While some of the terms for some experts seem loosely related (e.g., bell, reuters, and computing for expert 2, it is hard to find clear patterns in most of them. 5.2 MAE’s Potential in Transfer Learning: A Case Study We now turn to evaluate another property of MAE: its potential for data-efficient transfer learning, by only updating the gating functions, freezing the experts. We consider the pretrain-then-finetune"
2020.acl-main.587,N19-1112,1,0.919486,"of h − 1 attention heads. MAE learns an input-dependent distribution of the experts (g). At each training step, a single expert is selected and updated (solid line); during the evaluation, experts’ outputs are linearly combined with weights produced by g. Introduction The transformer architecture and its variants achieve state-of-the-art performance across a variety of NLP tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019) suggest that typical transformer networks are overparameterized, in the sense that at test time, many of the heads, or even a full layer (Fan et al., 2020), can b"
2020.acl-main.587,W19-4828,0,0.0231298,"ormer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana et al., 2004; Liu et al., 2018; Dutt et al., 2017, inter alia). Recent efforts also explore MoE in sequence learning (Shazeer et al., 2017), and to promote diversity in text generation (He et al., 2018; Shen et al., 2019; Cho et al., 2019, inter alia). 7 Conclusion We presented MAE. It is inspired by a mixture-ofexperts perspective of multi-head attention. With a learned gating function, MAE activates different experts on different inputs. MAE"
2020.acl-main.587,2021.ccl-1.108,0,0.106237,"Missing"
2020.acl-main.587,D19-1223,0,0.0162426,"with F T G+ can be viable in lowresource transfer learning. 18 Here we reverse the translation direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims t"
2020.acl-main.587,P18-2059,0,0.0191773,"T G+ outperforms F TA LL. These results suggest that finetuning MAE with F T G+ can be viable in lowresource transfer learning. 18 Here we reverse the translation direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transforme"
2020.acl-main.587,N19-1313,0,0.0211404,"able in lowresource transfer learning. 18 Here we reverse the translation direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand"
2020.acl-main.587,P19-1032,0,0.0238442,"ggest that finetuning MAE with F T G+ can be viable in lowresource transfer learning. 18 Here we reverse the translation direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Anoth"
2020.acl-main.587,P19-1452,0,0.0184234,"Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana et al., 2004; Liu et al., 2018; Dutt et al., 2017, inter alia). Recent efforts also explore MoE in sequence learning (Shazeer et al., 2017), and to promote diversity in text generation (He et al., 2018; Shen et al., 2019; Cho et al., 2019, inter alia). 7 Conclusion We presented MAE. It is inspired by a mixture-ofexperts perspective of multi-head attention. With a learned gating function, MAE activates different experts on different inputs. MAE is trained using a block coordinate desc"
2020.acl-main.587,W18-6301,0,0.183155,"Figure 1: Illustration of MAE: a mixture of attentive experts. Each Hi box is an attention head in a given layer; there are h of them in total. Experts are groups of h − 1 attention heads. MAE learns an input-dependent distribution of the experts (g). At each training step, a single expert is selected and updated (solid line); during the evaluation, experts’ outputs are linearly combined with weights produced by g. Introduction The transformer architecture and its variants achieve state-of-the-art performance across a variety of NLP tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019)"
2020.acl-main.587,P02-1040,0,0.106804,"1 summarizes some statistics of the datasets. 10 Preliminary results show that mixing experts with fewer heads leads to underwhelming performance. We conjecture this is due to too strong a regularization effect (§3). 11 https://drive.google.com/a/ haopeng.name/uc?export=download&id=0B_ bZck-ksdkpM25jRUN2X2UxMm8 12 http://workshop2014.iwslt.org/. Data Train Dev. Test Vocab. WMT14 4.5M IWSLT14 160K 3K 7K 3K 7K 32K 9K/7K Table 1: Some statistics for WMT14 and IWSLT14 datasets. We use separate source and target vocabularies in IWSLT14 experiments. Evaluation. The models are evaluated using BLEU (Papineni et al., 2002). A beam search with beam size 5 is used. In the WMT14 experiments, we follow Vaswani et al. (2017), and apply a compound split postprocessing.13 Results. Table 2 summarizes WMT14 EN-DE translation test performance. The base and large sized transformer models are due to Vaswani et al. (2017). To control for compounding factors, we additionally compare to our implementation of the base sized model (BASE). It achieves slightly better performance than Vaswani et al. (2017), with a 0.3 BLEU edge. M AE -7 improves over the base transformer by 0.8 BLEU, obtaining similar performance to the large-siz"
2020.acl-main.587,N18-1202,0,0.110339,"Missing"
2020.acl-main.587,P19-1580,0,0.394215,"ion (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019) suggest that typical transformer networks are overparameterized, in the sense that at test time, many of the heads, or even a full layer (Fan et al., 2020), can be removed without significant loss in performance.2 In response to this observation, they propose to prune the unimportant attention heads in the model after it is trained, aiming for faster inference. In this paper, we ask whether, instead of reducing the model capacity, we can use it more effectively. We propose mixture of attentive experts (MAE). MAE retains all attention heads, and learns to activate diff"
2020.acl-main.587,N19-1407,0,0.220239,"heads. MAE learns an input-dependent distribution of the experts (g). At each training step, a single expert is selected and updated (solid line); during the evaluation, experts’ outputs are linearly combined with weights produced by g. Introduction The transformer architecture and its variants achieve state-of-the-art performance across a variety of NLP tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019) suggest that typical transformer networks are overparameterized, in the sense that at test time, many of the heads, or even a full layer (Fan et al., 2020), can be removed without si"
2020.acl-main.587,P16-1162,0,0.091906,"• UNI -M AE -6 mixes 28 6-attention-head experts, and is otherwise the same as UNI M AE -7. We refer the readers to Appendix A for implementation details. 4.2 Machine Translation Datasets. We experiment with two machine translation datasets: • WMT14 EN-DE (Bojar et al., 2014).11 Following previous practice (Vaswani et al., 2017) we train on WMT14, and designate newstest2013 and newstest2014 as development and test data respectively. Our preprocessing follows that of Vaswani et al. (2017) and Ott et al. (2018). A shared source-target vocabulary is used, with 32k byte pair encoding types (BPE; Sennrich et al., 2016). • IWSLT14 DE-EN (Cettolo et al., 2014).12 It is based on TED talks, and is much smaller compared to WMT14. We use the preprocessing from Edunov et al. (2018). Following previous practice, we use separate vocabularies for the source and target, with around 9K and 7K BPE types respectively. Table 1 summarizes some statistics of the datasets. 10 Preliminary results show that mixing experts with fewer heads leads to underwhelming performance. We conjecture this is due to too strong a regularization effect (§3). 11 https://drive.google.com/a/ haopeng.name/uc?export=download&id=0B_ bZck-ksdkpM25jR"
2020.acl-main.587,N18-2074,0,0.0321796,"direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a;"
2020.acl-main.587,D18-1548,0,0.02049,"ayer; there are h of them in total. Experts are groups of h − 1 attention heads. MAE learns an input-dependent distribution of the experts (g). At each training step, a single expert is selected and updated (solid line); during the evaluation, experts’ outputs are linearly combined with weights produced by g. Introduction The transformer architecture and its variants achieve state-of-the-art performance across a variety of NLP tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019) suggest that typical transformer networks are overparameterized, in the sense that at test time, many of the head"
2020.acl-main.587,W14-3302,0,\N,Missing
2020.acl-main.587,N19-1423,0,\N,Missing
2020.acl-main.593,D15-1075,0,0.0320946,"natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient baselines of increasing size (Figure 2). Each is a fine-tuned B"
2020.acl-main.593,2020.ngt-1.3,0,0.0718724,"Missing"
2020.acl-main.593,N10-1115,0,0.0467783,"uracy. In contrast, our model is overconfident in its prediction of some labels (business for AG, positive for SST), and underconfident in others (tech for AG, entailment for MNLI). These findings might indicate that while our method is designed to be globally calibrated, it is not necessarily calibrated for each label individually. Such observations relate to existing concerns regarding fairness when using calibrated classifiers (Pleiss et al., 2017). 7 Related Work Methods for making inference more efficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some"
2020.acl-main.593,N18-2017,1,0.889082,"Missing"
2020.acl-main.593,2020.emnlp-main.21,0,0.0574536,"r k is given as input a weighted sum of all the layers up to and including k, such that the weight is learned during fine-tuning (Peters et al., 2018).7 Calibration Classifiers’ confidence scores are not always reliable (Jiang et al., 2018). One way to mitigate this concern is to use calibration, which encourages the confidence level to correspond to the probability that the model is correct (DeGroot and Fienberg, 1983). In this paper we use temperature calibration, which is a simple technique that has been shown to work well in practice (Guo et al., 2017), in particular for BERT fine-tuning (Desai and Durrett, 2020). The method learns a single parameter, denoted temperature or T , and divides each of the logits {zi } by T before applying the softmax function: exp(zi /T ) pred = arg max P i j exp(zj /T ) We select T to maximize the log-likelihood of the development dataset. Note that temperature calibration is monotonic and thus does not influence predictions. It is only used in our model to make early-exit decisions. Discussion Our approach has several attractive properties. First, if mi is not sufficiently confident in its prediction, we reuse the computation and continue towards mi+1 without recomputin"
2020.acl-main.593,N19-1423,0,0.574981,"an early exit, avoiding the computation associated with successive (higher) layers (grayed out). Otherwise, the model continues to the next layer/classifier. The large increase in the size of artificial intelligence models often increases production costs (Amodei and Hernandez, 2018; Schwartz et al., 2019), and can also limit adoption on real-time devices. Compared to training, which is a one-time large investment, inference costs are incurred for every instance in production, and can thus add up ∗ No No Introduction 1 Prediction significantly. For instance, Microsoft reports that using BERT (Devlin et al., 2019) to process Bing queries requires more than 2,000 GPUs concurrently.2 We present a method to reduce the inference cost of today’s common models in NLP: fine-tuned contextual word representations. Our method exploits variation along two axes: models differ in size and cost, and instances vary in difficulty. Our method assesses the complexity of each test instance and matches it with the most efficient model in our “toolbelt.”3 As a result, some instances, which we refer to in this paper as “easy” or “simple,” can be solved by small models, leading to computational savings, while other instances"
2020.acl-main.593,D16-1139,0,0.0349806,"fficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: In"
2020.acl-main.593,D19-1224,1,0.921999,"1, which is applied to each confidence score to decide whether to exit early. Lower thresholds result in earlier exits, with 0 implying the most efficient classifier is always used. A threshold of 1 always uses the most expensive and accurate classifier. 5 Results A better speed/accuracy tradeoff. Figure 3 presents our test results.10 The blue line shows our model, where each point corresponds to an increasingly large confidence threshold. The leftmost 9 Preliminary experiments with other configurations, including ones with more layers, led to similar results. 10 For increased reproduciblity (Dodge et al., 2019a), we also report validation results in Appendix B. 6643 Layer n Layer n Prediction Layer n Prediction No Layer l Layer l Layer l Layer k Layer k Layer k Is confident? Layer j Layer j Is confident? Layer i Layer i Layer i Layer 0 Layer 0 Layer 0 Input Input Input Yes Early exit prediction No Layer j Prediction (a) Efficient Baseline (b) Standard Baseline Yes Early exit prediction No Is confident? Yes Early exit prediction (c) Our approach Figure 2: Illustration of our baselines. (2a) Efficient baseline: adding a single output layer to an intermediate layer, while not processing the remaining"
2020.acl-main.593,D19-1110,1,0.923288,"1, which is applied to each confidence score to decide whether to exit early. Lower thresholds result in earlier exits, with 0 implying the most efficient classifier is always used. A threshold of 1 always uses the most expensive and accurate classifier. 5 Results A better speed/accuracy tradeoff. Figure 3 presents our test results.10 The blue line shows our model, where each point corresponds to an increasingly large confidence threshold. The leftmost 9 Preliminary experiments with other configurations, including ones with more layers, led to similar results. 10 For increased reproduciblity (Dodge et al., 2019a), we also report validation results in Appendix B. 6643 Layer n Layer n Prediction Layer n Prediction No Layer l Layer l Layer l Layer k Layer k Layer k Is confident? Layer j Layer j Is confident? Layer i Layer i Layer i Layer 0 Layer 0 Layer 0 Input Input Input Yes Early exit prediction No Layer j Prediction (a) Efficient Baseline (b) Standard Baseline Yes Early exit prediction No Is confident? Yes Early exit prediction (c) Our approach Figure 2: Illustration of our baselines. (2a) Efficient baseline: adding a single output layer to an intermediate layer, while not processing the remaining"
2020.acl-main.593,P99-1059,0,0.149133,"and solved with high accuracy. In contrast, our model is overconfident in its prediction of some labels (business for AG, positive for SST), and underconfident in others (tech for AG, entailment for MNLI). These findings might indicate that while our method is designed to be globally calibrated, it is not necessarily calibrated for each label individually. Such observations relate to existing concerns regarding fairness when using calibrated classifiers (Pleiss et al., 2017). 7 Related Work Methods for making inference more efficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCu"
2020.acl-main.593,W18-2501,0,0.0401913,"Missing"
2020.acl-main.593,D18-1153,0,0.0765574,"riginal, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: Instances with different labels are predicted with different degrees of confidence. Figure 6: Comparing confidence levels and F1 scores of our most efficient classifier across datasets and labels. High confidence by the model is sometimes explained by “easy” classes that are predicted with high F1 (e.g., sports in AG). Other cases might stem from biases of the model which make it overconfident despite the label being harder than other labels (e.g., positive in SST)."
2020.acl-main.593,2020.acl-main.537,0,0.0823282,"y reducing training and/or inference time (Graves, 2016; Seo et al., 2018). Our method also puts less resources into some of the input, but does so at the document level rather than for individual tokens. A few concurrent works have explored similar ideas for dynamic early exits in the transformer model. Elbayad et al. (2020) and Dabre et al. (2020) introduced early stopping for sequence-tosequence tasks (e.g., machine translation). Bapna et al. (2020) modify the transformer architecture with “control symbols” which determine whether components are short-circuited to optimize budget. Finally, Liu et al. (2020) investigated several inference-time cost optimizations (including early stopping) in a multilingual setting. Several computer vision works explored similar ideas to the one in this paper. Wang et al. (2018) introduced a method for dynamically skipping convolutional layers. Bolukbasi et al. (2017) and Huang et al. (2018) learned early exit policies for computer vision architectures, observing substantial computational gains. 8 Conclusion We presented a method that improves the speed/accuracy tradeoff for inference using pretrained language models. Our method makes early exits for simple instan"
2020.acl-main.593,P11-1015,0,0.0160066,"ts, and the bottom set are NLI datasets. 4 Experiments To test our approach, we experiment with three text classification and two natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with"
2020.acl-main.593,N18-1202,0,0.0415422,"s function, we sum the losses of all classification layers, such that lower layers are trained to both be useful as feature generators for the higher layers, and as input to their respective classifiers. This also means that every output layer is trained to perform well on all instances. Importantly, we do not perform early exits during training, but only during inference. To encourage monotonicity in performance of the different classifiers, each classifier at layer k is given as input a weighted sum of all the layers up to and including k, such that the weight is learned during fine-tuning (Peters et al., 2018).7 Calibration Classifiers’ confidence scores are not always reliable (Jiang et al., 2018). One way to mitigate this concern is to use calibration, which encourages the confidence level to correspond to the probability that the model is correct (DeGroot and Fienberg, 1983). In this paper we use temperature calibration, which is a simple technique that has been shown to work well in practice (Guo et al., 2017), in particular for BERT fine-tuning (Desai and Durrett, 2020). The method learns a single parameter, denoted temperature or T , and divides each of the logits {zi } by T before applying t"
2020.acl-main.593,S18-2023,0,0.0641465,"Missing"
2020.acl-main.593,P19-1580,0,0.0284801,"o mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: Instances with different labels are predicted with different degrees of confidence. Figure 6: Comparing confidence levels and F1 scores of our most efficient classifier across datasets and labels. High confidence by the model is sometimes explained by “easy” classes that are predicted with high F1 (e.g., sports in AG). Other cases might stem from biases of the model which make it overconfident despite the label being harder t"
2020.acl-main.593,N18-1101,0,0.0507775,"n English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient baselines of increasing size (Figure 2). Each is a fine-tuned BERT model with a single output layer after"
2020.acl-main.593,J81-4005,0,0.691775,"Missing"
2020.acl-main.593,D13-1170,0,0.0050123,"pproach, we experiment with three text classification and two natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient ba"
2020.emnlp-main.746,D15-1075,0,0.585049,"dence) corresponds to easy-to-learn examples, the bottomleft corner (low variability, low confidence) corresponds to hard-to-learn examples, and examples on the right (with high variability) are ambiguous; all definitions are with respect to the RO BERTA-large model. The modal group in the data is formed by the easy-to-learn regions. For clarity we only plot 25K random samples from the SNLI train set. Fig. 8b in App. §C shows the same map in greater relief. The creation of large labeled datasets has fueled the advance of AI (Russakovsky et al., 2015; Antol et al., 2015) and NLP in particular (Bowman et al., 2015; Rajpurkar et al., 2016). The common belief is that the more abundant the labeled data, the higher the likelihood of learning diverse phenomena, which in turn leads to models that generalize well. In practice, however, out-of-distribution Work done at the Allen Institute for AI. ambiguou 0.4 0.2 Introduction ∗ 0.6 correct. 0.0 0.2 0.3 0.5 0.7 0.8 1.0 (OOD) generalization remains a challenge (Yogatama et al., 2019; Linzen, 2020); and, while recent large pretrained language models help, they fail to close this gap (Hendrycks et al., 2020). This urges a closer look at datasets, where not all exa"
2020.emnlp-main.746,P17-1152,0,0.0393322,"scussion (with empirical justifications) on connections between training dynamics measures and dropout-based (Srivastava et al., 2014), first-principles uncertainty estimates. These relations are further supported by previous work, which showed that deep ensembles provide well-calibrated uncertainty estimates (Lakshminarayanan et al., 2017; Gustafsson et al., 2019; Snoek et al., 2019). Generally, such approaches ensemble models trained from scratch; while ensembles of training checkpoints lose some diversity (Fort et al., 2019), they offer a cheaper alternative capturing some of the benefits (Chen et al., 2017a). Future work will involve investigation of such alternatives for building data maps. 7 Related Work Our work builds data maps using training dynamics measures for scoring data instances. Loss landscapes (Xing et al., 2018) are similar to training dynamics, but also consider variables from the stochastic optimization algorithm. Toneva et al. (2018) also use training dynamics to find train examples which are frequently “forgotten”, i.e., mis9282 classified during a later epoch of training, despite being classified correctly earlier; our correctness metric provides similar discrete scores, and"
2020.emnlp-main.746,N19-1423,0,0.0734792,"Missing"
2020.emnlp-main.746,D19-1224,1,0.892059,"Missing"
2020.emnlp-main.746,N18-2017,1,0.861687,"Missing"
2020.emnlp-main.746,2020.acl-main.244,0,0.0262881,"Missing"
2020.emnlp-main.746,N13-1132,0,0.0221471,"e ability of simple linear classifiers to predict them correctly. While AFLite, among others (Li and Vasconcelos, 2019; Gururangan et al., 2018), advocate removing “easy” instances from the dataset, our work shows that easy-to-learn instances can be useful. Similar intuitions have guided other work such as curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010; Lee and Grauman, 2011) where all examples are prioritized based on their “difficulty”. Other approaches have used training loss (Han et al., 2018; Arazo et al., 2019; Shen and Sanghavi, 2019), confidence (Hovy et al., 2013), and meta-learning (Ren et al., 2018), to differentiate instances within datasets. Perhaps our measures are the closest to those from Chang et al. (2017); they propose prediction variance and threshold closeness—which correspond to variability and confidence, respectively.18 However, they use these measures to reweight all instances, similar to sampling effective batches in online learning (Loshchilov and Hutter, 2016). Our work, instead, does a hard selection for the purpose of studying different groups within data. Our methods are also reminiscent of active learning methods (Settles, 2009;"
2020.emnlp-main.746,D17-1215,0,0.127475,"Missing"
2020.emnlp-main.746,W02-2015,0,0.109712,"oring data instances. Loss landscapes (Xing et al., 2018) are similar to training dynamics, but also consider variables from the stochastic optimization algorithm. Toneva et al. (2018) also use training dynamics to find train examples which are frequently “forgotten”, i.e., mis9282 classified during a later epoch of training, despite being classified correctly earlier; our correctness metric provides similar discrete scores, and results in models with better performance. Variants of such approaches address catastrophic forgetting, and are useful for analyzing data instances (Pan et al., 2020; Krymolowski, 2002). Prior work has proposed other criteria to score instances. AFLite (LeBras et al., 2020) is an adversarial filtering algorithm which ranks instances based on their “predictability”, i.e. the ability of simple linear classifiers to predict them correctly. While AFLite, among others (Li and Vasconcelos, 2019; Gururangan et al., 2018), advocate removing “easy” instances from the dataset, our work shows that easy-to-learn instances can be useful. Similar intuitions have guided other work such as curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010; Lee and Grauman"
2020.emnlp-main.746,2020.acl-main.465,0,0.0320741,"map in greater relief. The creation of large labeled datasets has fueled the advance of AI (Russakovsky et al., 2015; Antol et al., 2015) and NLP in particular (Bowman et al., 2015; Rajpurkar et al., 2016). The common belief is that the more abundant the labeled data, the higher the likelihood of learning diverse phenomena, which in turn leads to models that generalize well. In practice, however, out-of-distribution Work done at the Allen Institute for AI. ambiguou 0.4 0.2 Introduction ∗ 0.6 correct. 0.0 0.2 0.3 0.5 0.7 0.8 1.0 (OOD) generalization remains a challenge (Yogatama et al., 2019; Linzen, 2020); and, while recent large pretrained language models help, they fail to close this gap (Hendrycks et al., 2020). This urges a closer look at datasets, where not all examples might contribute equally towards learning (Vodrahalli et al., 2018). However, the scale of data can make this assessment challenging. How can we automatically characterize data instances with respect to their role in achieving good performance in- and out-of- distribution? Answering this question may take us a step closer to bridging the gap between dataset collection and broader task 9275 Proceedings of the 2020 Conferenc"
2020.emnlp-main.746,2021.ccl-1.108,0,0.106443,"Missing"
2020.emnlp-main.746,N19-1262,0,0.0341628,"Missing"
2020.emnlp-main.746,N18-1101,0,0.32608,"constructed using the RO BERTA-large model (Liu et al., 2019). The map reveals three distinct regions in the dataset: a region with instances whose true class probabilities fluctuate frequently during training (high variability), and are hence ambiguous for the model; a region with easy-to-learn instances that the model predicts correctly and consistently (high confidence, low variability); and a region with hard-to-learn instances with low confidence, low variability, many of which we find are mislabeled during annotation .1 Similar regions are observed across three other datasets: MultiNLI (Williams et al., 2018), WinoGrande (Sakaguchi et al., 2020) and SQuAD (Rajpurkar et al., 2016), with respect to respective RO BERTA-large classifiers. We further investigate the above regions by training models exclusively on examples from each region (§3). Training on ambiguous instances promotes generalization to OOD test sets, with little or no effect on in-distribution (ID) performance.2 Our data maps also reveal that datasets contain a majority of easy-to-learn instances, which are not as critical for ID or OOD performance, but without any such instances, training could fail to converge (§4). In §5, we show th"
2020.emnlp-main.746,W18-5446,0,0.0616166,"Missing"
2020.emnlp-main.746,N13-1086,0,0.0146549,", 2016). Our work, instead, does a hard selection for the purpose of studying different groups within data. Our methods are also reminiscent of active learning methods (Settles, 2009; Peris and Casacuberta, 2018; P.V.S and Meyer, 2019), such as uncertainty sampling (Lewis and Gale, 1994) which selects (unlabeled) data points, which a model trained on a small labeled subset, has least confidence in, or predicts as farthest (in vector space, based on cosine similarity) (Sener and Savarese, 2018; Wolf, 2011). Our approach uses labeled data for selection, similar to core-set selection approaches (Wei et al., 2013). Active learning approaches could be used in conjunction with data maps to create better datasets, similar to approaches proposed in Mishra et al. (2020). For instance, creating datasets 18 They also consider confidence intervals; our preliminary experiments, with and without, yielded similar results. with more ambiguous examples (with respect to a given model) could make it beneficial for OOD generalization. Data error detection also involves instance scoring. Influence functions (Koh and Liang, 2017), forgetting events (Toneva et al., 2018), cross validation (Chen et al., 2019), Shapely val"
2021.emnlp-main.133,2020.emnlp-main.576,0,0.239464,", 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully understood, saturated transformers can implement a counting mechanism similarly to LSTMs (Merrill et al., 2020). In practice, Bhattamishra et al. (2020) show transformers can learn tasks requiring counting, and that they struggle when more complicated structural representations are required. Ebrahimi et al. (2020) find that attention patterns of certain heads can emulate bounded stacks, but that this ability falls off sharply for longer sequences. Thus, the abilities of trained LSTMs and transformers appear to be predicted by the classes of problems solvable by their saturated counterparts. Merrill et al. (2020) conjecture that the saturated capacity might represent a class of tasks implicitly learnable by GD, but it is unclear a priori why t"
2021.emnlp-main.133,D19-1223,0,0.0631386,"Missing"
2021.emnlp-main.133,J93-2004,0,0.0741644,"t of Fig. 1 breaks down the growth trend by layer. Generally, the norm grows more quickly in later layers than in earlier √ ones, although always at a rate proportional to t.5 Next, in the bottom row of Fig. 1, we plot the cosine similarity between each parameter checkpoint θt+1 and its predecessor θt . This rapidly approaches 1, suggesting the “direction” of the parameters (θt /kθt k) converges. The trend in directional convergence looks similar across layers. We also train smaller transformer language models with 38M parameters on Wikitext-2 (Merity et al., 2016) and the Penn Treebank (PTB; Marcus et al., 1993). We consider two variants of the transformer: pre-norm and post-norm, which vary in the relative order of layer normalization and residual connections (cf. Xiong et al., 2020). Every model exhibits norm growth over training.6 Combined, these results provide evidence that the parameter norm of transformers tends to grow over the course of training. In the remainder of this paper, we will discuss the implications of this phenomenon for the linguistic biases of transformers, and then discuss potential causes of the trend rooted in the optimization dynamics. 4 Effect of Norm Growth §3 empirically"
2021.emnlp-main.133,W19-3901,1,0.912325,"are understandable in terms of formal languages and automata. Empirically, we find that internal 1 Introduction representations of pretrained transformers approxiTransformer-based models (Vaswani et al., 2017) mate their saturated counterparts, but for randomly like BERT (Devlin et al., 2019), XLNet (Yang et al., initialized transformers, they do not. This suggests that the norm growth implicit in training guides 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel transformers to approximate saturated networks, et al., 2019) have pushed the state of the art on an justifying studying the latter (Merrill, 2019) as a impressive array of NLP tasks. Overparameterized way to analyze the linguistic biases of NLP architransformers are known to be unversal approximators (Yun et al., 2020), suggesting their general- tectures and the structure of their representations. ization performance ought to rely on useful biases Past work (Merrill, 2019; Bhattamishra et al., or constraints imposed by the learning algorithm. 2020) reveals that saturation permits two useful Despite various attempts to study these biases in types of attention heads within a transformer: one 1766 Proceedings of the 2021 Conference on Empi"
2021.emnlp-main.133,2020.acl-main.43,1,0.830634,"f particular interest for Our main contribution is analyzing the effect NLP. We leverage the emergent discrete strucof norm growth on the representations within the ture in a saturated transformer to analyze the transformer (§4), which control the network’s gramrole of different attention heads, finding that matical generalization. With some light assumpsome focus locally on a small number of potions, we prove that any network where the paramesitions, while other heads compute global avter norm diverges during training approaches a saterages, allowing counting. We believe underurated network (Merrill et al., 2020): a restricted standing the interplay between these two capabilities may shed further light on the structure network variant whose discretized representations of computation within large transformers. are understandable in terms of formal languages and automata. Empirically, we find that internal 1 Introduction representations of pretrained transformers approxiTransformer-based models (Vaswani et al., 2017) mate their saturated counterparts, but for randomly like BERT (Devlin et al., 2019), XLNet (Yang et al., initialized transformers, they do not. This suggests that the norm growth implicit i"
2021.emnlp-main.133,W19-3905,0,0.0187455,"in its recurrent memory (Kirov and Frank, 2012). Stacks are useful for processing compositional structure in linguistic data 1767 2 The limit over f is taken pointwise. The range of sf is R. (Chomsky, 1956), e.g., for semantic parsing. However, a saturated LSTM does not have enough memory to simulate a stack (Merrill, 2019). Rather, saturated LSTMs resemble classical counter machines (Merrill, 2019): automata limited in their ability to model hierarchical structure (Merrill, 2020). Experiments suggest that LSTMs trained on synthetic tasks learn to implement counter memory (Weiss et al., 2018; Suzgun et al., 2019a), and that they fail on tasks requiring stacks and other deeper models of structure (Suzgun et al., 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully understood, saturated tran"
2021.emnlp-main.133,P18-2117,1,0.834048,"TM encoding a stack in its recurrent memory (Kirov and Frank, 2012). Stacks are useful for processing compositional structure in linguistic data 1767 2 The limit over f is taken pointwise. The range of sf is R. (Chomsky, 1956), e.g., for semantic parsing. However, a saturated LSTM does not have enough memory to simulate a stack (Merrill, 2019). Rather, saturated LSTMs resemble classical counter machines (Merrill, 2019): automata limited in their ability to model hierarchical structure (Merrill, 2020). Experiments suggest that LSTMs trained on synthetic tasks learn to implement counter memory (Weiss et al., 2018; Suzgun et al., 2019a), and that they fail on tasks requiring stacks and other deeper models of structure (Suzgun et al., 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully under"
2021.findings-emnlp.259,2021.naacl-main.9,1,0.759325,"sk, observing that vanilla BERTstyle often masks ungrounded words like “umm” or “yeah”. We share the same motivation to mask highly visual words. 6.3 Challenges in VQA generalization Visual understanding Language and vision tasks inherently demand deep understanding of both the text and the image. However, many works show that models can succeed on VQA datasets using strong language priors, and by relying on superficial cues, and there are still challenges to overcome for tasks with more compositional structure (Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2017; Agarwal et al., 2020; Bitton et al., 2021; Dancette et al., 2021). Balanced datasets such as VQA 2.0 (Goyal et al., 2017) and GQA (Hudson and Manning, 2019) have been presented to address these challenges. Novel models with richer visual representations (Zhang et al., 2021) were also presented, and some works tried to encourage the model to look at the “correct” image regions (Liu et al., 2021; Yang et al., 2020). Bias Yang et al. (2021) and Hendricks et al. (2018) have shown that attention-based visionlanguage models suffer from bias that misleads the attention module to focus on spurious correlations in training data, and leads to"
2021.findings-emnlp.259,2020.tacl-1.5,0,0.0636908,"Missing"
2021.findings-emnlp.259,D19-1514,0,0.0944787,"ngs might be that the model is evaluated mostly on retrieving objects, and had we tested it on other classes, its performance would have substantially decreased. To test this hypothesis, we inspect the same model’s performance on questions with answers from different semantic types. To do so, we experiment with the GQA dataset, which includes partitioning of the answers into different semantic types, including Objects, Relations (subject or object of a described relation, e.g., “what is the girl wearing?&quot;), and Attributes (the properties or position of an object). Many works (Lu et al., 2019; Tan and Bansal, 2019; Chen et al., 2020) assume that a VLP model should include an MLM component that is capable of predicting every masked token, including objects, properties, but also stop words and punctuation. Does a model that uses our Objects strategy, and The results for the semantic type partition are masks only objects, learn to complete words from presented in Table 3. Comparing between the other classes? If not, can such a pre-training strat- models trained with Objects and Baseline MLM egy be effective? masking strategies, the Objects masking strategy To examine this questions, we extend the experi-"
2021.findings-emnlp.342,D13-1170,0,0.00334594,"ons drawn with a larger budget. Here the high variance of UBB likely plays a role the stability of its predictions; while it may be unbiased, the lower variance estimators are more reliable. 4.1 5 Experimental Setup We proceed by performing a sensitivity analysis: we run 100 trials of random hyperparameter search (far more than is typically necessary to establish that one model outperforms another in current practice) for a CNN (Kim, 2014) and a linear bag-ofembedding (LBoE) (Yogatama and Smith, 2015). These models are trained on the Stanford sentiment treebank 5-way text classification task (Socher et al., 2013). We include details about the dataset (and a link to download it) in Appendix B. For all three estimators, the CNN has higher expected performance than the LBoE, for all n ≤ B.3 We then simulate a more practical scenario 3 See Appendix C for details. Figure 3 shows expected validation curves for B = 100 for all three estimators; with Conclusion Drawing reproducible conclusions from our experimental results is of paramount importance to NLP researchers, practitioners, and users of language technologies. Expected validation performance curves are tools for comparing the results of hyperparamete"
2021.findings-emnlp.342,2020.acl-main.246,0,0.0869282,"B). This is estimating what the maxiMSE strikes a balance between bias and varimum of n trials would be, in expectation; this is ance, displaying a classic bias-variance tradethus a statistical estimation problem. The formulaoff. We use expected validation performance to compare between different models, and antion was introduced by Dodge et al. (2019), who alyze how frequently each estimator leads to proposed a first estimator (defined as VnB in Equadrawing incorrect conclusions about which of tion 2). This estimator was later shown to be biased two models performs best. We find that the by Tang et al. (2020), who introduced an unbiased two biased estimators lead to the fewest incorestimator (defined as UnB in Equation 3) for the rect conclusions, which hints at the importance same expected maximum. of minimizing variance and MSE. In Section 2 we use tools from combinatorics to 1 Introduction derive both previously-introduced estimators, relate Drawing robust conclusions when comparing dif- them to each other, and show that they make two opposing assumptions; we show that changing only ferent methods in natural language processing is one of these assumptions instead of both leads to a central to s"
2021.findings-emnlp.342,D15-1251,1,0.829312,"a practitioner would care about: the frequency with which one draws conclusions that would be consistent with conclusions drawn with a larger budget. Here the high variance of UBB likely plays a role the stability of its predictions; while it may be unbiased, the lower variance estimators are more reliable. 4.1 5 Experimental Setup We proceed by performing a sensitivity analysis: we run 100 trials of random hyperparameter search (far more than is typically necessary to establish that one model outperforms another in current practice) for a CNN (Kim, 2014) and a linear bag-ofembedding (LBoE) (Yogatama and Smith, 2015). These models are trained on the Stanford sentiment treebank 5-way text classification task (Socher et al., 2013). We include details about the dataset (and a link to download it) in Appendix B. For all three estimators, the CNN has higher expected performance than the LBoE, for all n ≤ B.3 We then simulate a more practical scenario 3 See Appendix C for details. Figure 3 shows expected validation curves for B = 100 for all three estimators; with Conclusion Drawing reproducible conclusions from our experimental results is of paramount importance to NLP researchers, practitioners, and users of"
2021.naacl-main.9,P19-1164,1,0.793014,"s from the scene graph. Bottom: relations among the objects in the scene graph. First line at the top is the original QA pair, while the following 3 lines show our pertubated questions: replacing a single element in the question (a fence) with other options (a wall, men, an elephant), leading to a change in the output label. For each QA pair, the LXMERT predicted output is shown. Introduction the out-of-domain performance of these models is often severely deteriorated (Jia and Liang, 2017; Ribeiro et al., 2018; Gururangan et al., 2018; Geva et al., 2019; McCoy et al., 2019; Feng et al., 2019; Stanovsky et al., 2019). Recently, Kaushik et al. (2019) and Gardner et al. (2020) introduced the contrast sets approach to probe out-of-domain generalization. Contrast sets are constructed via minimal modifications to test inputs, such that their label is modified. For example, in Fig. 1, replacing “a fence” with “a wall”, changes the answer NLP benchmarks typically evaluate in-distribution generalization, where test sets are drawn i.i.d from a distribution similar to the training set. Recent works showed that high performance on test sets sampled in this manner is often achieved by exploiting systematic gaps, anno"
2021.naacl-main.9,2020.emnlp-main.158,0,0.0161646,"his is due to model architecture or dataset design. Bogin et al. (2020) claim that both of these models are prone to fail on compositional generalization because they do not decompose the problem into smaller sub-tasks. Our results support this claim. On the other hand, it is possible that a different dataset could prevent these models from finding shortcuts. Is there a dataset that can prevent all shortcuts? Our automatic method for creating contrast sets allows us to ask those questions, while we believe that future work in better training mechanisms, as suggested in Bogin et al. (2020) and Jin et al. (2020), could help in making more robust models. We proposed an automatic method for creating contrast sets for VQA datasets that use annotated scene graphs. We created contrast sets for the GQA dataset, which is designed to be compositional, balanced, and robust against statistical biases. We observed a large performance drop between the original and augmented sets. As our contrast sets Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on"
2021.naacl-main.9,D19-1514,0,0.122313,"Missing"
2021.naacl-main.9,D18-1009,1,0.746347,"Missing"
2021.naacl-main.9,2020.blackboxnlp-1.12,0,0.0341525,"t 94 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 94–105 June 6–11, 2021. ©2021 Association for Computational Linguistics 2 from “Yes” to “No”. Since such perturbations introduce minimal additional semantic complexity, robust models are expected to perform similarly on the test and contrast sets. However, a range of NLP models severely degrade in performance on contrast sets, hinting that they do not generalize well (Gardner et al., 2020). Except two recent exceptions for textual datasets (Li et al., 2020; Rosenman et al., 2020), contrast sets have so far been built manually, requiring extensive human effort and expertise. Automatic Contrast Set Construction To construct automatic contrast sets for GQA we first identify a large subset of questions requiring specific reasoning skills (§2.1). Using the scene graph representation, we perturb each question in a manner which changes its gold answer (§2.2). Finally, we validate the automatic process via crowdsourcing (§2.3). 2.1 Identifying Recurring Patterns in GQA The questions in the GQA dataset present a diverse set of modelling challenges, as e"
2021.naacl-main.9,P19-1472,0,0.0438219,"Missing"
C12-1147,P11-2086,0,0.0132508,"s no correlation, and −1 indicates anticorrelation. We also compute a significance p-value, which is the probability for obtaining a given correlation at random (Abdi 2007). Results show that the relative orderings obtained in the different settings are very much in concordance. The obtained Kendall τ correlation coefficients range between (0.46, 0.88). Interestingly, when excluding DMV, results are even more significant (correlation in (0.64, 0.88)). This corresponds to p-values smaller than 10−7 and smaller than 10−13 if we exclude DMV. 9 This is a commonly used measure in NLP (Lapata 2006; Brody and Kantor 2011). 2414 Relation between Learnability Measures. In order to explore the relations between the two learnability measures, we focused on pairs of orderings that use the same parser, but different learnability measures (|P |= 5 pairs). The Kendall τ values in this case range between (0.75, 0.82), which corresponds to p-values < 10−18 . Despite the high correlation between the measures, the biases discovered under the AccuracyLearnability measure are stronger than the ones discovered under the Rate-Learnability measure. This demonstrates the somewhat different perspectives obtained by using differe"
C12-1147,N09-1009,0,0.0254999,"ps Figure 3: The VSS’s with which we experiment. The possible annotations for each structure are marked using solid and dashed lines. alternatives2 . 4 Experimental Setup 4.1 The Parsers In this work we experiment with five parsers of different types. We briefly describe them. Dependency Model with Valence (DMV) (Klein and Manning 2004) is a generative parser that defines a probabilistic grammar for unlabeled dependency structures. This parser is widely used in the field of unsupervised dependency parsing, where the great majority of recent works are in fact elaborations of this model (e.g., (Cohen and Smith 2009; Headden III et al. 2009)). In our experiments we use a supervised version of this parser, by training it using maximum likelihood estimation (MLE). This approach was used in various previous works as an upper bound for the unsupervised model (Blunsom and Cohn 2010; Spitkovsky et al. 2011). Decoding is performed using the Viterbi algorithm3. MST Parser (McDonald et al. 2005)4 formulates dependency parsing as a search for a maximum spanning tree (MST). It uses online training and extends the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003) to learning with structured outputs."
C12-1147,de-marneffe-etal-2006-generating,0,0.0102724,"Missing"
C12-1147,N10-1115,0,0.0778492,"ing with structured outputs. Clear Parser (Choi and Nicolov 2009)5 is a fast transition-based parser that uses the robust risk minimization technique (Zhang et al. 2002). k-best ranking is used to prune the next state in decoding. Su Parser (Nivre 2009)6 is a transition-based parser and an extension of the MALT parser (Nivre et al. 2006). The parser starts by constructing arcs between adjacent words and then swaps the order of input words in order to learn more complex structures. It uses the stackeager algorithm, and is trained using various linear classifiers (including SVM). NonDir Parser (Goldberg and Elhadad 2010)7 is a non-directional, easy-first parser, which is greedy and deterministic. It first attempts to induce a non-directional version of the easiest arcs in 2 Some definitions of verb groups also include auxiliaries. We choose to exclude them from our definition since we use the PTB POS set, which distinguishes modals, but not auxiliaries, from other verbs. 3 http://www.cs.columbia.edu/~scohen/parser.html http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html http://code.google.com/p/clearparser/ 6 http://maltparser.org/ 7 http://www.cs.bgu.ac.il/~yoavg/software/easyfirst/ 4 5 2411 a depen"
C12-1147,P11-1067,1,0.473969,"al. 2012) and is described in detail in Section 3. While these examples are all taken from English, variation is found in any language for which sufficient resources are available (Zeman et al. 2012). Many previous works addressed the difficulties imposed by the lack of established standards for syntactic representation. Jiang and Liu (2009) adapted statistical tools trained with one annotation standard to another. Other works proposed to normalize the different representations into a standard scheme (Ide and Bunt 2010; Zeman et al. 2012). Parsing evaluation is also highly affected by VSS’s. Schwartz et al. (2011) suggested Neutral Edge Direction (NED), an evaluation measure for unsupervised dependency parsing that accepts more than one plausible annotation for dependency VSS’s. Tsarfaty et al. (2011) suggested a new evaluation measure for supervised dependency parsing to address representational variation. The measure is based on tree edit distance. Tsarfaty et al. (2012) extended this measure for comparing between annotations from different formalisms. The emphasis of all the above works was mainly to overcome the problems incurred by the lack of standard, and not to select the most advantageous anno"
C12-1147,W03-3023,0,0.0564775,", pages 2405–2422, COLING 2012, Mumbai, December 2012. 2405 1 Introduction The formal manner in which syntactic relations are represented is at the core of the study of grammar. Numerous representations have been proposed over the years for expressing similar syntactic relations. This diversity of representations is expressed in a variety of syntactic annotation schemes currently in use in NLP. Examples include, for constituency annotation, schemes by (Marcus et al. 1993; Sampson 1995; Nelson et al. 2002, inter alia) and for dependency annotation, schemes by (Collins 1999; Rambow et al. 2002; Yamada and Matsumoto 2003; Johansson and Nugues 2007, inter alia). Variation within the same formalism is expressed in structures that have several alternative annotations (henceforth Varying Syntactic Structure or VSS). In this work we focus on dependency structures, where some of the most basic structures are VSS’s. One example is prepositional phrases, which consist of a preposition followed by a noun phrase (e.g., “about everyone”). While some schemes select the preposition to head the NP (Collins 1999), others select the NP as the head of the preposition (Johansson and Nugues 2007) (see Figure 1). Other prominent"
C12-1147,zeman-etal-2012-hamledt,0,0.00944774,"Missing"
C12-1147,W12-3602,0,\N,Missing
C12-1147,nivre-etal-2006-maltparser,0,\N,Missing
C12-1147,E12-1006,0,\N,Missing
C12-1147,W10-1840,0,\N,Missing
C12-1147,J93-2004,0,\N,Missing
C12-1147,W11-0303,0,\N,Missing
C12-1147,W09-3803,0,\N,Missing
C12-1147,W04-2609,0,\N,Missing
C12-1147,N01-1021,0,\N,Missing
C12-1147,N09-1012,0,\N,Missing
C12-1147,J03-4003,0,\N,Missing
C12-1147,D10-1117,0,\N,Missing
C12-1147,P06-1033,0,\N,Missing
C12-1147,H05-1066,0,\N,Missing
C12-1147,J06-4002,0,\N,Missing
C12-1147,P09-1040,0,\N,Missing
C12-1147,P06-3004,0,\N,Missing
C12-1147,D11-1036,0,\N,Missing
C12-1147,D07-1112,0,\N,Missing
C12-1147,W07-2416,0,\N,Missing
C12-1147,P04-1061,0,\N,Missing
C12-1147,W04-1501,0,\N,Missing
C12-1147,rambow-etal-2002-dependency,0,\N,Missing
C14-1153,P13-1023,1,0.827372,"Missing"
C14-1153,J10-4006,0,0.112148,"Missing"
C14-1153,P99-1008,0,0.273486,"hy (MEG) (Sudre et al., 2012). See (Martin, 2007) for a detailed survey. This parallel evidence to the prominence of our categories provides substance for intriguing future research. 3 Symmetric Patterns Patterns. In this work, patterns are combinations of words and wildcards, which provide a structural phrase representation. Examples of patterns include “X and Y”, “X such as Y”, “X is a country”, etc. Patterns can be used to extract various relations between words. For example, patterns such as “X of a Y” (“basement of a building”) can be useful for detecting the meronymy (part-of) relation (Berland and Charniak, 1999). Symmetric patterns (e.g., “X and Y”, “France and Holland”), which we use in this paper, can be used to detect semantic similarity between words (Widdows and Dorow, 2002). Symmetric Patterns. Symmetric patterns are patterns that contain exactly two wildcards, and where these wildcards are interchangeable. Examples of symmetric patterns include “X and Y”, “X or Y” and “X as well as Y”. Previous works have shown that word pairs that participate in symmetric patterns bare strong semantic resemblance, and consequently, that these patterns can be used to cluster words into semantic categories, whe"
C14-1153,N12-2002,0,0.0199936,"egories expressed by languages, e.g., objects, actions, and properties. We follow human development, acquiring coarse-grained categories and distinctions before detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete “things” (Langacker, 2008, Ch. 4), roughly corresponding to nouns – the main participants in linguistic clauses – that are universally present in the semantics of virtually all languages (Dixon, 2005). Most works on noun classification to semantic categories require large amounts of human annotation to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or rely on language-specific resources such as WordNet (Evans and Orˇasan, 2000; Orˇasan and Evans, 2007). Such heavy supervision is labor intensive and makes these models domain and language dependent. Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can compensate for the lack of input from the senses in text corpora. Our model therefore performs semantic category classification using only a small number of labeled seed words per category. The experiments we conduct show that such weak supervision is sufficient to construct a"
C14-1153,J92-4003,0,0.553962,"milarity measures is described in Section 5.2. SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path lengt"
C14-1153,W00-0717,0,0.0358593,"008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different l"
C14-1153,P06-1038,1,0.918323,"ns Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with known classes to the rest of the concepts. For this aim we apply an iterative variant of the k-Nearest Neighbors algorithm (denoted wit"
C14-1153,P08-1079,1,0.838024,"ossible is the concept of “flexible patterns”, which are composed of high frequency words (HFW) and content words (CW). Every word in the language is defined as either HFW or CW, based on the number of times this word appears in a large corpus. This clustering procedure is applied by traversing a large corpus, and marking words that appear with corpus frequency higher than a predefined threshold t1 as HFWs, and words with corpus frequency lower than t2 as CWs.1 1 We follow (Davidov and Rappoport, 2006) and set t1 = 10−5 , t2 = 10−3 . Note that some words are marked both as HFW and as CW. See (Davidov and Rappoport, 2008) for discussion. 1614 The resulting clusters have a desired property: HFWs are comprised mostly of function words (prepositions, determiners, etc.) while CWs are comprised mostly of content words (nouns, verbs, adjectives and adverbs). This coarse grained clustering is useful for pattern extraction from plain text, since language patterns tend to use fixed function words, while content words change from one instance of the pattern to another (Davidov and Rappoport, 2006). Flexible patterns are extracted by traversing a large corpus and, based on the clustering of words to CWs and HFWs, extract"
C14-1153,P97-1023,0,0.667401,"Missing"
C14-1153,C92-2082,0,0.325937,"Missing"
C14-1153,Y09-1024,0,0.0307906,"Categories. Several works tackled the task of semantic classification, mostly focusing on animacy, concreteness and countability. The vast majority of these works are either supervised (Hatzivassiloglou and McKeown, 1997; Baldwin and Bond, 2003; Peng and Araki, 2005; Øvrelid, 2005; Nagata et al., 2006; Xing et al., 2010; Kwong, 2011; Bowman and Chopra, 2012) or make use of external, language-specific resources such as WordNet (Orˇasan and Evans, 2001; Orˇasan and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a small set of labeled seed words. Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences in instances of hand-crafted patterns such as “X who Y” and “X and his Y”. While their model uses patterns that are tailored to the animacy and gender categories, our model uses automatically induced patterns and is thus applicable to a range of semantic categories. Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais, 1997) based classification features. They used their model to classify nouns into the concrete/abstract category using 40 labeled seed words . Unl"
C14-1153,P08-1068,0,0.264369,"ord embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different label propagation algorithm, while still using the symmetric pattern f"
C14-1153,P08-1119,0,0.805739,"s include “X and Y”, “X as well as Y” and “neither X nor Y”. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with known classes to the res"
C14-1153,Y11-1007,0,0.0612062,"Missing"
C14-1153,N04-1043,0,0.0368862,"2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a simple contextual preference similarity correlates with similarity in semantic categorization better than symmetric pattern features. The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word similarity measure for building our graph. 5.1.2 Label Propagation Baselines In this type of baselines, we replace I-k-NN with a different label propagation algorithm,"
C14-1153,D13-1006,0,0.0883615,"uages, e.g., objects, actions, and properties. We follow human development, acquiring coarse-grained categories and distinctions before detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete “things” (Langacker, 2008, Ch. 4), roughly corresponding to nouns – the main participants in linguistic clauses – that are universally present in the semantics of virtually all languages (Dixon, 2005). Most works on noun classification to semantic categories require large amounts of human annotation to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or rely on language-specific resources such as WordNet (Evans and Orˇasan, 2000; Orˇasan and Evans, 2007). Such heavy supervision is labor intensive and makes these models domain and language dependent. Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can compensate for the lack of input from the senses in text corpora. Our model therefore performs semantic category classification using only a small number of labeled seed words per category. The experiments we conduct show that such weak supervision is sufficient to construct a high quality classifi"
C14-1153,P06-2077,0,0.0664839,"Missing"
C14-1153,W01-0716,0,0.133972,"Missing"
C14-1153,I05-2018,0,0.0860965,"Missing"
C14-1153,W97-0313,0,0.188417,"on. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label 1620 seeds to achieve good performance; in contrast, our work performs very well with a randomly selected set of labeled seed words. 8 Conclusion We presented a minimally supervised model for noun classification into coarse grained semantic categories. Our model"
C14-1153,D13-1193,1,0.840639,". Previous works have shown that word pairs that participate in symmetric patterns bare strong semantic resemblance, and consequently, that these patterns can be used to cluster words into semantic categories, where a high precision, but low coverage (recall) solution is good enough (Dorow et al., 2005; Davidov and Rappoport, 2006). A key observation of this paper is that symmetric patterns can be also used for semantic classification, where recall is as important as precision. Flexible Patterns. It has been shown in previous work (Davidov and Rappoport, 2006; Turney, 2008; Tsur et al., 2010; Schwartz et al., 2013) that patterns can be extracted from plain text in a fully unsupervised manner. The key idea that makes this procedure possible is the concept of “flexible patterns”, which are composed of high frequency words (HFW) and content words (CW). Every word in the language is defined as either HFW or CW, based on the number of times this word appears in a large corpus. This clustering procedure is applied by traversing a large corpus, and marking words that appear with corpus frequency higher than a predefined threshold t1 as HFWs, and words with corpus frequency lower than t2 as CWs.1 1 We follow (D"
C14-1153,P06-1101,0,0.046761,"abeled words that are used for propagation. Our model, on the other hand, does not require any seed selection procedure, and utilizes a randomly selected set of labeled seed words. Lexical Acquisition. Another line of work focused on the acquisition of semantic categories. In this setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for precision. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the"
C14-1153,D13-1170,0,0.00602652,"th a more sophisticated label propagation algorithm. 5.1.1 Classification Features Baselines In this set of baselines, we use different methods for building our graph. Concretely, instead of adding edges for pairs of words that appear in the same symmetric pattern, we use word similarity measures based on different feature sets as described below. The process of building the graph using the baseline word similarity measures is described in Section 5.2. SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,7 a deep network based word embedding method, which has been used to produce state-of-the-art results in several NLP tasks, including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine similarity between two word embeddings as a word similarity measure. Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al., 1992).8 This clustering, in which words share a cluster if they tend to appear in the same lexical context, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller"
C14-1153,D11-1063,0,0.0331411,"ternal, language-specific resources such as WordNet (Orˇasan and Evans, 2001; Orˇasan and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a small set of labeled seed words. Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences in instances of hand-crafted patterns such as “X who Y” and “X and his Y”. While their model uses patterns that are tailored to the animacy and gender categories, our model uses automatically induced patterns and is thus applicable to a range of semantic categories. Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais, 1997) based classification features. They used their model to classify nouns into the concrete/abstract category using 40 labeled seed words . Unlike our model, which requires only a small set of labeled seeds, their algorithm is actually heavily supervised, requiring thousands of labeled examples for selecting the seed set of labeled words that are used for propagation. Our model, on the other hand, does not require any seed selection procedure, and utilizes a randomly selected set of labeled seed words. Lexical Acquisit"
C14-1153,P09-1050,0,0.0137145,"categories. In this setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for precision. Our model tackles a different task, namely the classification of words according to a given category where both recall and precision are to be optimized. Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model automatically induces symmetric patterns, obtaining high quality results without relying on any type of language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label 1620 seeds to achieve good performance; in contrast, our work performs very well with a randomly selected set of labeled seed wo"
C14-1153,C02-1114,0,0.757091,"ples of symmetric patterns include “X and Y”, “X as well as Y” and “neither X nor Y”. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 1612 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1612–1623, Dublin, Ireland, August 23-29 2014. Works that apply symmetric patterns in their model generally require expert knowledge in the form of a pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain and language independent. Our model addresses semantic classification in a transductive setup. It takes advantage of word similarity scores that are computed based on symmetric pattern features, and propagates information from concepts with"
C14-1153,W03-1010,0,\N,Missing
D13-1193,P99-1008,0,0.0386137,"several tweets into a longer document is appealing since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms"
D13-1193,W04-3205,0,0.0274065,"rovement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2"
D13-1193,P06-1038,1,0.320317,"ori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent aut"
D13-1193,P08-1027,1,0.902015,"ms. As a result, flexible patterns can pick up fine-grained differences between authors’ styles. Unlike other types of pattern features, 1886 Word Frequency. Flexible patterns are composed of high frequency words (HFW) and content words (CW). Every word in the corpus is defined as either HFW or CW. This clustering is performed by counting the number of times each word appears in the corpus of size s. A word that appears more than 10−4 ×s times in a corpus is considered HFW. A word that appears less than 10−3 ×s times in a corpus is considered CW. Some words may serve both as HFWs and CWs (see Davidov and Rappoport (2008b) for discussion). Structure of a Flexible Pattern. Flexible patterns start and end with an HFW. A sequence of zero or more CWs separates consecutive HFWs. At least one CW must appear in every pattern.10 For efficiency, at most six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW ” 10 Omitting this treats word n-grams as flexible patterns. Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible patter"
D13-1193,P08-1079,1,0.301097,"ms. As a result, flexible patterns can pick up fine-grained differences between authors’ styles. Unlike other types of pattern features, 1886 Word Frequency. Flexible patterns are composed of high frequency words (HFW) and content words (CW). Every word in the corpus is defined as either HFW or CW. This clustering is performed by counting the number of times each word appears in the corpus of size s. A word that appears more than 10−4 ×s times in a corpus is considered HFW. A word that appears less than 10−3 ×s times in a corpus is considered CW. Some words may serve both as HFWs and CWs (see Davidov and Rappoport (2008b) for discussion). Structure of a Flexible Pattern. Flexible patterns start and end with an HFW. A sequence of zero or more CWs separates consecutive HFWs. At least one CW must appear in every pattern.10 For efficiency, at most six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW ” 10 Omitting this treats word n-grams as flexible patterns. Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible patter"
D13-1193,P07-1030,1,0.833997,"were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of au1889 thorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We"
D13-1193,W10-2914,1,0.899099,"six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW ” 10 Omitting this treats word n-grams as flexible patterns. Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible pattern sequence. For example, (1) is matched by (2). Partial Flexible Patterns. A flexible pattern may appear in a given tweet with additional words not originally found in the flexible pattern, and/or with only a subset of the HFWs (Davidov et al., 2010a). For example, (3) is a partial match of (1), since the word “great” is not part of the original flexible pattern. Similarly, (4) is another partial match of (1), since (a) the word “good” is not part of the original flexible pattern and (b) the second occurrence of the word “the” does not appear in (4) (missing word is ). marked by 3. “TheHFW greatHFW kingCW ofHFW theHFW ring” 4. “TheHFW goodHFW kingCW ofHFW Spain” We use such cases as features with lower weight, proportional to the number of found HFWs in the 0.5×nf ound ). For example, (1) receives a tweet (w = nexpected weight of 1 (comp"
D13-1193,C10-2028,1,0.724989,"six HFWs (and as a result, five CW sequences) may appear in a flexible pattern. Examples of flexible patterns include 1. “theHFW CW ofHFW theHFW ” 10 Omitting this treats word n-grams as flexible patterns. Flexible Pattern Features. Flexible patterns can serve as binary classification features; a tweet matches a given flexible pattern if it contains the flexible pattern sequence. For example, (1) is matched by (2). Partial Flexible Patterns. A flexible pattern may appear in a given tweet with additional words not originally found in the flexible pattern, and/or with only a subset of the HFWs (Davidov et al., 2010a). For example, (3) is a partial match of (1), since the word “great” is not part of the original flexible pattern. Similarly, (4) is another partial match of (1), since (a) the word “good” is not part of the original flexible pattern and (b) the second occurrence of the word “the” does not appear in (4) (missing word is ). marked by 3. “TheHFW greatHFW kingCW ofHFW theHFW ring” 4. “TheHFW goodHFW kingCW ofHFW Spain” We use such cases as features with lower weight, proportional to the number of found HFWs in the 0.5×nf ound ). For example, (1) receives a tweet (w = nexpected weight of 1 (comp"
D13-1193,C92-2082,0,0.0741925,"aracter n-gram and word n-grams. They experimented with 10 authors of Greek text, and also joined several tweets into a single document. Joining several tweets into a longer document is appealing since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used a"
D13-1193,U11-1008,0,0.0528424,"ve mainly been evaluated against long texts such as theater plays (Mendenhall, 1887), essays (Yule, 1939; Mosteller and Wallace, 1964), biblical books (Mealand, 1995; Koppel et al., 2011a) and book chapters (Argamon et al., 2007; Koppel et al., 2007). In recent year, many works focused on web data such as emails (De Vel et al., 2001; Koppel and Schler, 2003; Abbasi and Chen, 2008), web forum messages (Abbasi and Chen, 2005; Solorio et al., 2011), blogs (Koppel et al., 2006; Koppel et al., 2011b) and chat messages (Abbasi and Chen, 2008). Some works focused on SMS messages (Mohan et al., 2010; Ishihara, 2011). Authorship Attribution on Twitter. The performance of authorship attribution systems on short texts is affected by several factors (Stamatatos, 2009). These factors include the number of candidate authors, the training set size and the size of the test document. Very few authorship attribution works experimented with Twitter. Unlike our work, all used a single group of authors (group sizes varied between 3-50). Layton et al. (2010) used the SCAP methodology (Frantzeskou et al., 2007) with character ngram features. They experimented with 50 authors and compared different numbers of tweets per"
D13-1193,P11-1136,1,0.600788,"Missing"
D13-1193,P08-1119,0,0.0645394,"Missing"
D13-1193,W06-1657,0,0.00835382,"res are especially useful for authorship attribution on micro-messages since they are relatively tolerant to typos and non-standard use of punctuation (Stamatatos, 2009). These are common in the nonformal style generally applied in social media services. Consider the example of misspelling “Britney” as “Brittney”. The misspelled name shares the 4-grams “Brit” and “tney” with the correct name. As a result, these features provide information about the author’s style (or at least her topic of interest), which is not available through lexical features. Following standard practice, we use 4-grams (Sanderson and Guenter, 2006; Layton et al., 2010; Koppel et al., 2011b). White spaces are considered characters (i.e., a character n-gram may be composed of letters from two different words). A single white-space is appended to the beginning and the end of each tweet. For efficiency, we consider only character n-gram features that appear at least tcng times in the training set of at least one author (see Section 5). Word n-grams. We hypothesize that word n-gram features would be useful for authorship attribution on micro-messages. We assume that under a strict length restriction, many authors would prefer using short, r"
D13-1193,I11-1018,0,0.248104,"of flavors of the authorship attribution task. 1 Introduction Research in authorship attribution has developed substantially over the last decade (Stamatatos, 2009). The vast majority of such research has been dedicated towards finding the author of long texts, ranging from single passages to book chapters. In recent years, the growing popularity of social media has created special interest, both theoretical and computational, in short texts. This has led to many recent authorship attribution projects that experimented with web data such as emails (Abbasi and Chen, 2008), web forum messages (Solorio et al., 2011) and blogs (Koppel et al., 2011b). This paper addresses the question to what extent the authors of very short texts can be identified. To answer this question, we experiment with Twitter tweets. Twitter messages (tweets) are limited to 140 characters. This restriction imposes major difficulties on Moshe Koppel Department of Computer Science Bar Ilan University koppel@macs.biu.ac.il authorship attribution systems, since authorship attribution methods that work well on long texts are often not as useful when applied to short texts (Burrows, 2002; Sanderson and Guenter, 2006). Nonetheless, tweets"
D13-1193,C08-1114,0,0.0355457,"ks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal compound relations (Davidov and Rappoport, 2008a), sentiment analysis (Davidov et al., 2010b) and detection of sarcasm (Tsur et al., 2010). 9 Conclusion The main goal of this paper is to measure to what extent authors of micro-messages can be identified. We have shown that authors of very short texts can be successfully identified in an array of au1889 thorship attribution settings reported for long documents. This is the first work on micro-messages to address some of these settings. We introduced the"
D13-1193,C02-1114,0,0.0215657,"aling since it can lead to substantial improvement of the classification results, as demonstrated by the works above. However, this approach requires the test data to contain several tweets that are known a-priori to be written by the same author. This assumption is not always realistic. In our paper, we intentionally focus on a single tweet as document size. Flexible Patterns. Patterns were introduced by (Hearst, 1992), who used hand crafted patterns to discover hyponyms. Hard coded patterns were used for many tasks, such as discovering meronymy (Berland and Charniak, 1999), noun categories (Widdows and Dorow, 2002), verb relations (Chklovski and Pantel, 2004) and semantic class learning (Kozareva et al., 2008). Patterns were first extracted in a fully unsupervised manner (“flexible patterns”) by (Davidov and Rappoport, 2006), who used flexible patterns in order to establish noun categories, and (Bicic¸i and Yuret, 2006) who used them for analogy question answering. Ever since, flexible patterns were used as features for various tasks such as extraction of semantic relationships (Davidov et al., 2007; Turney, 2008b; Bollegala et al., 2009), detection of synonyms (Turney, 2008a), disambiguation of nominal"
D18-1009,D15-1075,0,0.770689,"s placed in the kennel next to a woman’s feet. b) washes her face with the shampoo. c) walks into frame and walks towards the dog. d) tried to cut her face, so she is trying to do something very close to her face. Table 1: Examples from Swag; the correct answer is bolded. Adversarial Filtering ensures that stylistic models find all options equally appealing. linguistic entailment (Chierchia and McConnellGinet, 2000). Whereas the dominant entailment paradigm asks if two natural language sentences (the ‘premise’ and the ‘hypothesis’) describe the same set of possible worlds (Dagan et al., 2006; Bowman et al., 2015), here we focus on whether a (multiple-choice) ending describes a possible (future) world that can be anticipated from the situation described in the premise, even when it is not strictly entailed. Making such inference necessitates a rich understanding about everyday physical situations, including object affordances (Gibson, 1979) and frame semantics (Baker et al., 1998). A first step toward grounded commonsense inference with today’s deep learning machinery is to create a large-scale dataset. However, recent work has shown that human-written datasets are susceptible to annotation artifacts:"
D18-1009,P18-2103,0,0.0716038,"Missing"
D18-1009,P09-1068,0,0.0706243,"Missing"
D18-1009,P17-1152,0,0.0414444,"oped for the NLI task. d. Dual Bag-of-Words For this baseline, we treat each sentence as a bag-of-embeddings (c, vi ). We model the probability of picking an ending i using a bilinear model: softmaxi (cWviT ).8 e. Dual pretrained sentence encoders Here, we obtain representations from SkipThoughts or InferSent for each span, and compute their pairwise compatibility using either 1) a bilinear model or 2) an MLP from their concatenated representations. f. SNLI inference Here, we consider two models that do well on SNLI (Bowman et al., 2015): Decomposable Attention (Parikh et al., 2016) and ESIM (Chen et al., 2017). We use pretrained versions of these models (with ELMo embeddings) on SNLI to obtain 3-way entailment, neutral, and contradiction probabilities for each example. We then train a log-linear model using these 3-way probabilities as features. g. SNLI models (retrained) Here, we train ESIM and Decomposable Attention on our dataset: we simply change the output layer size to 1 (the potential of an ending vi ) with a softmax over i. for our dataset take the following form: given a sentence and a noun phrase as context c = (s, n), as well as a list of possible verb phrase endings V = {v1 , . . . , v4"
D18-1009,N18-2017,1,0.89719,"Missing"
D18-1009,D17-1070,0,0.0192329,"multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans. In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality. Related Work Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about. Conclusion We propose a new challenge of physically situated commonsense inference that broadens the scope of natural language inference (NL"
D18-1009,P18-1152,1,0.872355,"Missing"
D18-1009,P17-1025,1,0.857241,"Missing"
D18-1009,E17-2068,0,0.0297031,"rbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018). We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data. See the appendix for more details. 4.1 Binary models Unary models The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage. a. fastText (Joulin et al., 2017): This library models a single span of text as a bag of n-grams, and tries to predict the probability of an ending being correct or incorrect independently.7 b. Pretrained sentence encoders We consider two types of pretrained RNN sentence encoders, SkipThoughts (Kiros et al., 2015) and InferSent 4.3 Other models We also considered the following models: h. Length: Although length was used by the adversarial classifier, we want to verify that human validation didn’t reintroduce a length bias. For this baseline, we always choose the shortest ending. i. ConceptNet As our task requires world knowle"
D18-1009,D16-1244,0,0.117864,"Missing"
D18-1009,P17-1117,0,0.0508501,"Missing"
D18-1009,I17-1011,1,0.777985,"t to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans. In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality. Related Work Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reas"
D18-1009,N18-2102,0,0.04485,"Missing"
D18-1009,P16-1137,0,0.0647538,"Missing"
D18-1009,D14-1162,0,0.0811429,"our dataset take the following form: given a sentence and a noun phrase as context c = (s, n), as well as a list of possible verb phrase endings V = {v1 , . . . , v4 }, a model fθ must select a verb ˆi that hopefully matches igold : ˆi = argmax fθ (s, n, vi ) (4) i To study the amount of bias in our dataset, we also consider models that take as input just the ending verb phrase vi , or the entire second sentence (n, vi ). For our learned models, we train f by minimizing multi-class cross-entropy. We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018). We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data. See the appendix for more details. 4.1 Binary models Unary models The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage. a"
D18-1009,P11-2057,0,0.064,"Missing"
D18-1009,N18-1202,0,0.0627976,"gold : ˆi = argmax fθ (s, n, vi ) (4) i To study the amount of bias in our dataset, we also consider models that take as input just the ending verb phrase vi , or the entire second sentence (n, vi ). For our learned models, we train f by minimizing multi-class cross-entropy. We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018). We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data. See the appendix for more details. 4.1 Binary models Unary models The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage. a. fastText (Joulin et al., 2017): This library models a single span of text as a bag of n-grams, and tries to predict the probability of an ending being correct or incorrect independently.7 b. Pretrained sentence encoders W"
D18-1009,W17-2810,0,0.0592611,"Missing"
D18-1009,marelli-etal-2014-sick,0,0.0427973,"owledge graph or a neural model. In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans. In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality. Related Work Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requi"
D18-1009,S18-2023,0,0.0923282,"Missing"
D18-1009,N16-1098,0,0.0815222,"LMo features), but the large gap between machine and human performance suggests that more is required to solve the dataset. As models are developed for commonsense inference, and more broadly as the field of NLP advances, we note that AF can be used again to create a more adversarial version of Swag using better language models and AF models. 6 Commonsense NLI Several datasets have been introduced to study NLI beyond linguistic entailment: for inferring likely causes and endings given a sentence (COPA; Roemmele et al., 2011), for choosing the most sensible ending to a short story (RocStories; Mostafazadeh et al., 2016; Sharma et al., 2018), and for predicting likelihood of a hypothesis by regressing to an ordinal label (JOCI; (Zhang et al., 2017)). These datasets are relatively small: 1k examples for COPA and 10k cloze examples for RocStories.13 JOCI increases the scale by generating the hypotheses using a knowledge graph or a neural model. In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison betwee"
D18-1009,P16-1144,0,0.0335514,"al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about. Conclusion We propose a new challenge of physically situated commonsense inference that broadens the scope of natural language inference (NLI) with commonsense reasoning. To support research toward commonsense NLI, we create a large-scale dataset Swag with 113k multiple-choice questions. Our dataset is constructed using Adversarial Filtering (AF), a new paradigm for robust and cost-effective dataset construction that allows datasets to be constructed at scale while automatically reducing annotation artifacts that ca"
D18-1009,W17-1609,0,0.0793366,"Missing"
D18-1009,D17-1247,1,0.862848,"Missing"
D18-1009,D17-1099,1,0.847025,"Missing"
D18-1009,W16-0204,0,0.0509438,"Missing"
D18-1009,K17-1004,1,0.868554,"Missing"
D18-1009,P18-2119,0,0.0417135,"e gap between machine and human performance suggests that more is required to solve the dataset. As models are developed for commonsense inference, and more broadly as the field of NLP advances, we note that AF can be used again to create a more adversarial version of Swag using better language models and AF models. 6 Commonsense NLI Several datasets have been introduced to study NLI beyond linguistic entailment: for inferring likely causes and endings given a sentence (COPA; Roemmele et al., 2011), for choosing the most sensible ending to a short story (RocStories; Mostafazadeh et al., 2016; Sharma et al., 2018), and for predicting likelihood of a hypothesis by regressing to an ordinal label (JOCI; (Zhang et al., 2017)). These datasets are relatively small: 1k examples for COPA and 10k cloze examples for RocStories.13 JOCI increases the scale by generating the hypotheses using a knowledge graph or a neural model. In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans."
D18-1009,D17-1323,0,0.0268507,"This work was supported by the National Science Foundation Graduate Research Fellowship (DGE-1256082), the NSF grant (IIS1524371, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the IARPA DIVA program through D17PC00343, and gifts by Google and Facebook. The views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government. Reducing gender/racial bias Prior work has sought to reduce demographic biases in word embeddings (Zhang et al., 2018) as well as in image recognition models (Zhao et al., 2017). Our work has focused on producing a dataset with minimal annotation artifacts, which in turn helps to avoid some gender and racial biases that stem from elicitation (Rudinger et al., 2017). However, it is not perfect in this regard, particularly due to biases in movies (Schofield and Mehr, 2016; Sap et al., 2017). Our methodology could potentially be extended to construct datasets free of (possibly intersectional) gender or racial bias. References Physical knowledge Prior work has studied learning grounded knowledge about objects and verbs: from knowledge bases (Li et al., 2016), syntax pars"
D18-1009,P17-1076,0,0.0159913,"estions (73k training, 20k validation, 20k test) and is derived from pairs of consecutive video captions from ActivityNet Captions (Krishna et al., 2017; Heilbron et al., 2015) and the Large Scale Movie Description Challenge (LSMDC; Rohrbach et al., 2017). The two datasets are slightly different in nature and allow us to achieve broader coverage: ActivityNet contains 20k YouTube clips containing one of 203 activity types (such as doing gymnastics or playing guitar); LSMDC consists of 128k movie captions (audio descriptions and scripts). For each pair of captions, we use a constituency parser (Stern et al., 2017) to split the second sentence into noun and verb phrases (Figure 1).2 Each question has a human-verified gold ending and 3 distractors. 3 A solution to annotation artifacts In this section, we outline the construction of Swag. We seek dataset diversity while minimizing annotation artifacts, conditional stylistic patterns such as length and word-preference biases. For many NLI datasets, these biases have been shown to allow shallow models (e.g. bag-of-words) obtain artificially high performance. To avoid introducing easily “gamed” patterns, we present Adversarial Filtering (AF), a generallyappl"
D18-1009,W18-5446,0,0.122748,"Missing"
D18-1009,N18-1101,0,0.0773261,"e task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans. In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality. Related Work Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018). Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components 13 For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories. 100 7 Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about. Conclusion We"
D18-1009,P98-1013,0,\N,Missing
D18-1009,C98-1013,0,\N,Missing
D18-1009,P17-2097,0,\N,Missing
D18-1152,N18-1205,0,0.0924699,"997; Cho et al., 2014), extensive efforts have been devoted to developing alternatives (Balduzzi and Ghifary, 2016; Miao et al., 2016; Zoph and Le, 2017; Lee et al., 2017; Lei et al., 2017a; Vaswani et al., 2017; Gehring et al., 2017, inter alia). Departing from the above approaches, this work derives RNN architectures drawing inspiration from WFSAs. Another line of work studied the connections between WFSAs and RNNs in terms of modeling capacity, both empirically (Kolen, 1993; Giles et al., 1992; Weiss et al., 2018, inter alia) and theoretically (Cleeremans et al., 1989; Visser et al., 2001; Chen et al., 2018, inter alia). 8 Conclusion We presented rational recurrences, a new construction to study the recurrent updates in RNNs, drawing inspiration from WFSAs. We showed that rational recurrences are in frequent use by several recently proposed recurrent neural architectures, providing new understanding of them. Based on such connections, we discussed approaches to deriving novel neural architectures from WFSAs. Our empirical results demonstrate the potential of doing so. We publicly release our implementation at https://github.com/ Noahs-ARK/rational-recurrences. Acknowledgments We thank Jason Eisn"
D18-1152,Q15-1031,0,0.0161797,"(F) is especially useful in small data setups. As in the language modeling experiments, RRNN(B)m+ underperforms all other models in most cases, and in particular RRNN(B). These results provide evidence that replacing the real semiring in rational models might be challenging. We leave further exploration to future work. 7 Related Work Weighted finite state automata. WFSAs were once popular among many sequential tasks (Mohri et al., 2002; Kumar and Byrne, 2003; Cortes et al., 2004; Pardo and Birmingham, 2005; Moore et al., 2006, inter alia), and are still successful in morphology (Dreyer, 2011; Cotterell et al., 2015; Rastogi et al., 2016, inter alia). Compared to neural networks, WFSAs are better understood theoreti10 http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html cally and arguably more interpretable. They were recently revisited in combination with the former in, e.g., text generation (Ghazvininejad et al., 2016, 2017; Lin et al., 2017) and automatic music accompaniment (Forsyth, 2016). Recurrent neural networks. RNNs (Elman, 1990; Jordan, 1989) prove to be strong models for sequential data (Siegelmann and Sontag, 1995). Besides the perhaps most notable gated variants (Hochreiter and Schmidhu"
D18-1152,C10-2028,0,0.025046,"Missing"
D18-1152,N16-1024,1,0.394928,"d with final weights. Arrows represent transitions, labeled by the symbols ↵ they consume, and the weights as a function of ↵. Arcs not drawn are assumed to have weight ¯0. For brevity, 8↵ means 8↵ 2 ⌃, with ⌃ being the alphabet. Introduction Neural models, and in particular gated variants of recurrent neural networks (RNNs, e.g., Hochreiter and Schmidhuber, 1997; Cho et al., 2014), have become a core building block for stateof-the-art approaches in NLP (Goldberg, 2016). While these models empirically outperform classical NLP methods on many tasks (Zaremba et al., 2014; Bahdanau et al., 2015; Dyer et al., 2016; Peng et al., 2017, inter alia), they typically lack the intuition offered by classical models, making it hard to understand the roles played by each of their components. In this work we show that many neural models are more interpretable than previously thought, by drawing connections to weighted finite state automata (WFSAs). We study several recently proposed RNN architectures and show that one can use WFSAs to characterize their recurrent updates. We call such models rational recurrences (§3).1 Analyzing recurrences in terms of WFSAs provides a new view of existing models and facilitates"
D18-1152,P02-1001,0,0.0885187,"inearity: ut = Wu vt . By Proposition 7 and Corollary 9: Corollary 10. The recurrences of single-layer SRU, T-RNN, and SCRN architectures are rational. It is slightly more complicated to analyze the recurrences of the QRNN, T-LSTM, and T-GRU. Although their hidden states ct are updated in the same way as Equation 5c, the input representations and gates may depend on previous inputs. For example, in T-LSTM and T-GRU, the forget gate is a function of two consecutive inputs: ft = (Vf vt 1 + Wf vt + bf ) . (10) QRNNs are similar, but may depend on up to K tokens, due to the K-window convolutions. Eisner (2002) discuss finite state machines for second (or higher) order probabilistic sequence models. Following the same intuition, we sketch the construction of WFSAs corresponding to QRNNs with 2-window convolutions in Appendix A, and summarize the key results here: Proposition 11. The recurrences of single-layer T-GRU, T-LSTM, and QRNN are rational. In particular, a single-layer d-dimensional QRNN using K-window convolutions can be recovered by a set of d WFSAs, each with O(2 |⌃|K 1 ) states. The size of WFSAs needed to recover QRNN grows exponentially in the window size. Therefore, at least for QRNNs"
D18-1152,D16-1126,0,0.00432377,"ture work. 7 Related Work Weighted finite state automata. WFSAs were once popular among many sequential tasks (Mohri et al., 2002; Kumar and Byrne, 2003; Cortes et al., 2004; Pardo and Birmingham, 2005; Moore et al., 2006, inter alia), and are still successful in morphology (Dreyer, 2011; Cotterell et al., 2015; Rastogi et al., 2016, inter alia). Compared to neural networks, WFSAs are better understood theoreti10 http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html cally and arguably more interpretable. They were recently revisited in combination with the former in, e.g., text generation (Ghazvininejad et al., 2016, 2017; Lin et al., 2017) and automatic music accompaniment (Forsyth, 2016). Recurrent neural networks. RNNs (Elman, 1990; Jordan, 1989) prove to be strong models for sequential data (Siegelmann and Sontag, 1995). Besides the perhaps most notable gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), extensive efforts have been devoted to developing alternatives (Balduzzi and Ghifary, 2016; Miao et al., 2016; Zoph and Le, 2017; Lee et al., 2017; Lei et al., 2017a; Vaswani et al., 2017; Gehring et al., 2017, inter alia). Departing from the above approaches, this work derives RNN a"
D18-1152,P17-4008,0,0.00774605,"Missing"
D18-1152,N03-1019,0,0.041538,"ificant differences between RRNN (F) and RRNN (C), while both outperform others. This may suggest that the interpolation of unigram and bigram features by RRNN(F) is especially useful in small data setups. As in the language modeling experiments, RRNN(B)m+ underperforms all other models in most cases, and in particular RRNN(B). These results provide evidence that replacing the real semiring in rational models might be challenging. We leave further exploration to future work. 7 Related Work Weighted finite state automata. WFSAs were once popular among many sequential tasks (Mohri et al., 2002; Kumar and Byrne, 2003; Cortes et al., 2004; Pardo and Birmingham, 2005; Moore et al., 2006, inter alia), and are still successful in morphology (Dreyer, 2011; Cotterell et al., 2015; Rastogi et al., 2016, inter alia). Compared to neural networks, WFSAs are better understood theoreti10 http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html cally and arguably more interpretable. They were recently revisited in combination with the former in, e.g., text generation (Ghazvininejad et al., 2016, 2017; Lin et al., 2017) and automatic music accompaniment (Forsyth, 2016). Recurrent neural networks. RNNs (Elman, 1990; Jor"
D18-1152,D15-1180,0,0.0163074,"om a string kernel perspective (Lei et al., 2017a). Here we review its nonlinear bigram version: (1) 8↵/⌘1 (↵) 8↵/µ2,2 (↵) (11) Due to the self-loop over state q1 , t can be seen as a weighted sum of the µ1 terms up to xt (Equaltion 13). The second product term in Equation 11 then provides multiplicative interactions between µ2 , and the weighted sum of µ1 s. In this sense, it captures nonconsecutive bigram features. ct 8↵/µ1,1 (↵) (14b) where the ut s are computed similarly to Equa(2) tion 5b, and ct is used as output for onward computation. Different strategies to computing t were explored (Lei et al., 2015, 2016). When t is a constant, or depends only on xt , e.g., t = (W vt +b ), the ith dimension of Equations 14 Beyond Elementwise Operations So far we have discussed rational recurrences for models using elementwise recurrent updates (e.g., Equation 5c). This section uses an existing model as an example, to study a rational recurrence that is not elementwise. We focus on the input switched affine network (ISAN; Foerster et al., 2017). Aiming for efficiency and interpretability, it does not use any explicit nonlinearity; its affine transformation parameters depend only on the input: ct = W x t"
D18-1152,N16-1153,0,0.0171468,"Missing"
D18-1152,P04-1035,0,0.00902813,"Missing"
D18-1152,P17-1186,1,0.82596,"s. Arrows represent transitions, labeled by the symbols ↵ they consume, and the weights as a function of ↵. Arcs not drawn are assumed to have weight ¯0. For brevity, 8↵ means 8↵ 2 ⌃, with ⌃ being the alphabet. Introduction Neural models, and in particular gated variants of recurrent neural networks (RNNs, e.g., Hochreiter and Schmidhuber, 1997; Cho et al., 2014), have become a core building block for stateof-the-art approaches in NLP (Goldberg, 2016). While these models empirically outperform classical NLP methods on many tasks (Zaremba et al., 2014; Bahdanau et al., 2015; Dyer et al., 2016; Peng et al., 2017, inter alia), they typically lack the intuition offered by classical models, making it hard to understand the roles played by each of their components. In this work we show that many neural models are more interpretable than previously thought, by drawing connections to weighted finite state automata (WFSAs). We study several recently proposed RNN architectures and show that one can use WFSAs to characterize their recurrent updates. We call such models rational recurrences (§3).1 Analyzing recurrences in terms of WFSAs provides a new view of existing models and facilitates the development of"
D18-1152,D18-1477,0,0.0319395,"SRU; Lei et al., 2017b) aim to speed up the recurrent computation. To do so, they drop the matrix multiplication dependence on the previous hidden state, resulting in similar recurrences to that in Example 6.5 Other works start from different motivations, but land on similar recurrences, e.g., strongly-typed RNNs (TRNN; Balduzzi and Ghifary, 2016) and its gated 4 We restrict that both operations take constant time and space, to exclude the use of arbitrarily complex semirings (§4.3). 5 The SRU architecture discussed through this work is based on Lei et al. (2017b). In a later updated version, Lei et al. (2018) introduce diagonal matrix multiplication interaction in the hidden state updates, inspired by (Li et al., 2018), which yields a recurrence not obviously rational. variants (T-LSTM and T-GRU), and structurally constrained RNNs (SCRN; Mikolov et al., 2014). The analysis in §3.1 directly applies to SRU, T-RNN, and SCRN. In fact, Example 6 presents a slightly more complicated version of them. In these models, input representations are computed without the bias term or any nonlinearity: ut = Wu vt . By Proposition 7 and Corollary 9: Corollary 10. The recurrences of single-layer SRU, T-RNN, and SCR"
D18-1152,P18-1173,1,0.895246,"Missing"
D18-1152,D14-1162,0,0.117975,"Missing"
D18-1152,J93-2004,0,0.0611805,"ng (§5.1). We also compare to an LSTM baseline. Aiming to control for comfounding factors, we do not use highway connections in any of the models.6 In the interest of space, the full architectures and hyperparameters are detailed in Appendices D and E. 6.2 RRNN (C) RRNN (F) RRNN (B) RRNN (B)m+ RRNN (C) RRNN (F) Table 3: Language modeling perplexity on PTB test set (lower is better). LSTM numbers are taken from Lei et al. (2017b). ` denotes the number of layers. Bold font indicates best performance. Language Modeling Dataset and implementation. We experiment with the Penn Treebank corpus (PTB; Marcus et al., 1993). We use the preprocessing and splits from Mikolov et al. (2010), resulting in a vocabulary size of 10K and 1M tokens. Following standard practice, we treat the training data as one long sequence, split into mini batches, and train using BPTT truncated to 35 time steps (Williams and Peng, 1990). The input embeddings and output softmax weights are tied (Press and Wolf, 2017). Results. Following Collins et al. (2017) and Melis et al. (2018), we compare models controlling for parameter budget. Table 3 summarizes language modeling perplexities on PTB test set. The middle block compares all models"
D18-1152,E17-2025,0,0.00583644,"s better). LSTM numbers are taken from Lei et al. (2017b). ` denotes the number of layers. Bold font indicates best performance. Language Modeling Dataset and implementation. We experiment with the Penn Treebank corpus (PTB; Marcus et al., 1993). We use the preprocessing and splits from Mikolov et al. (2010), resulting in a vocabulary size of 10K and 1M tokens. Following standard practice, we treat the training data as one long sequence, split into mini batches, and train using BPTT truncated to 35 time steps (Williams and Peng, 1990). The input embeddings and output softmax weights are tied (Press and Wolf, 2017). Results. Following Collins et al. (2017) and Melis et al. (2018), we compare models controlling for parameter budget. Table 3 summarizes language modeling perplexities on PTB test set. The middle block compares all models with two layers and 10M trainable parameters. RRNN (B) and RRNN (C) achieve roughly the same performance; interpolating both unigram and bigram features, RRNN(F) outperforms others by more than 2.9 test perplexity. For the three-layer and 24M setting (the bottom block), we observe similar trends, except that RRNN(C) slightly underperforms RRNN(B). Here RRNN(F) outperforms o"
D18-1152,N16-1076,0,0.0132168,"in small data setups. As in the language modeling experiments, RRNN(B)m+ underperforms all other models in most cases, and in particular RRNN(B). These results provide evidence that replacing the real semiring in rational models might be challenging. We leave further exploration to future work. 7 Related Work Weighted finite state automata. WFSAs were once popular among many sequential tasks (Mohri et al., 2002; Kumar and Byrne, 2003; Cortes et al., 2004; Pardo and Birmingham, 2005; Moore et al., 2006, inter alia), and are still successful in morphology (Dreyer, 2011; Cotterell et al., 2015; Rastogi et al., 2016, inter alia). Compared to neural networks, WFSAs are better understood theoreti10 http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html cally and arguably more interpretable. They were recently revisited in combination with the former in, e.g., text generation (Ghazvininejad et al., 2016, 2017; Lin et al., 2017) and automatic music accompaniment (Forsyth, 2016). Recurrent neural networks. RNNs (Elman, 1990; Jordan, 1989) prove to be strong models for sequential data (Siegelmann and Sontag, 1995). Besides the perhaps most notable gated variants (Hochreiter and Schmidhuber, 1997; Cho et al.,"
D18-1152,P18-1028,1,0.48448,"lly lack the intuition offered by classical models, making it hard to understand the roles played by each of their components. In this work we show that many neural models are more interpretable than previously thought, by drawing connections to weighted finite state automata (WFSAs). We study several recently proposed RNN architectures and show that one can use WFSAs to characterize their recurrent updates. We call such models rational recurrences (§3).1 Analyzing recurrences in terms of WFSAs provides a new view of existing models and facilitates the development of new ones. In recent work, Schwartz et al. (2018) introduced SoPa, an RNN constructed from WFSAs, and thus rational by our definition. They also showed that a single-layer max-pooled CNN (LeCun, 1998) can be simulated by a set of simple WFSAs (one per output dimension), and accordingly are also rational. In this paper we broaden such efforts, and show that rational recurrences are in frequent use (Mikolov et al., 2014; Balduzzi and Ghifary, 2016; Lei et al., 2016, 2017a,b; Bradbury et al., 2017; Foerster et al., 2017). For instance, we will show in §4 that the WFSA diagrammed 1 Where the term regular is used with unweighted FSAs (e.g., regul"
D18-1152,D13-1170,0,0.0014574,"t compared models outperform the LSTM baselines, whose numbers are taken from Lei et al. (2017b).7 6.3 Text Classification Implementation. We use unidirectional 2-layer architectures for all compared models. To build the classifiers, we feed the final RNN hidden states into a 2-layer tanh-MLP. Further implementation details are described in Appendix E. Datasets. We experiment with four binary text classification datasets, described below. • Amazon (electronic product review corpus; McAuley and Leskovec, 2013).8 We focus on the positive and negative reviews. • SST (Stanford sentiment treebank; Socher et al., 2013).9 We focus on the binary classification task. SST provides labels for syntactic phrases; we experiment with a more realistic setup, and 7 Melis et al. (2018) point out that carefully tuning LSTMs can achieve much stronger performance, at the cost of exceptionally large amounts of computational resources for tuning. 8 http://riejohnson.com/cnn_data.html 9 nlp.stanford.edu/sentiment/index.html 1210 Model Amazon LSTM 91.2±0.3 85.1±0.6 93.3±0.6 82.4±1.5 RRNN (B) RRNN (C) RRNN (B)m+ RRNN (F) SST subj CR 92.4±0.1 85.8±0.3 93.9±0.4 84.1±1.0 92.8±0.2 84.8±0.4 93.8±0.6 84.5±0.9 89.2±3.1 84.9±0.4 92.6±"
D18-1152,P18-2117,0,0.0463808,"and Sontag, 1995). Besides the perhaps most notable gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), extensive efforts have been devoted to developing alternatives (Balduzzi and Ghifary, 2016; Miao et al., 2016; Zoph and Le, 2017; Lee et al., 2017; Lei et al., 2017a; Vaswani et al., 2017; Gehring et al., 2017, inter alia). Departing from the above approaches, this work derives RNN architectures drawing inspiration from WFSAs. Another line of work studied the connections between WFSAs and RNNs in terms of modeling capacity, both empirically (Kolen, 1993; Giles et al., 1992; Weiss et al., 2018, inter alia) and theoretically (Cleeremans et al., 1989; Visser et al., 2001; Chen et al., 2018, inter alia). 8 Conclusion We presented rational recurrences, a new construction to study the recurrent updates in RNNs, drawing inspiration from WFSAs. We showed that rational recurrences are in frequent use by several recently proposed recurrent neural architectures, providing new understanding of them. Based on such connections, we discussed approaches to deriving novel neural architectures from WFSAs. Our empirical results demonstrate the potential of doing so. We publicly release our implement"
D19-1005,D19-1522,0,0.0518018,"Missing"
D19-1005,D17-1284,1,0.863563,"Missing"
D19-1005,D18-1454,0,0.0247236,"nguage models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Dev"
D19-1005,W09-2415,0,0.0815158,"Missing"
D19-1005,D11-1072,0,0.332131,"Missing"
D19-1005,D17-1195,1,0.70636,"ng based on the smaller BERTBASE model, the experiments demonstrate improved masked language model perplexity and ability to recall facts over BERTLARGE . The extrinsic evaluations demonstrate improvements for challenging relationship extraction, entity typing and word sense disambiguation datasets, and often outperform other contemporaneous attempts to incorporate external knowledge into BERT. 2 Entity-aware language models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a"
D19-1005,P18-1224,0,0.201073,"ns in the input text and use an entity linker to retrieve relevant entity embeddings from a KB to form knowledge enhanced entity-span representations. Then, the model recontextualizes the entity-span representations with word-toentity attention to allow long range interactions between contextual word representations and all entity spans in the context. The entire KAR is inserted between two layers in the middle of a pretrained model such as BERT. In contrast to previous approaches that integrate external knowledge into task-specific models with task supervision (e.g., Yang and Mitchell, 2017; Chen et al., 2018), our approach learns the entity linkers with self-supervision on unlabeled data. This results in general purpose knowledge enhanced representations that can be applied to a wide range of downstream tasks. Our approach has several other benefits. First, it leaves the top layers of the original model unchanged so we may retain the output loss layers and fine-tune on unlabeled corpora while training the KAR. This also allows us to simply swap out BERT for KnowBert in any downstream application. Second, by taking advantage of the existing high capacity layers in the original model, the KAR is lig"
D19-1005,K18-1050,0,0.171803,"tential mention from among the available candidates. It first runs mention-span self-attention to compute Se = TransformerBlock(S). (3) and max-margin, (2) LEL = max(0, γ − ψmg ) + X max(0, γ + ψmk ), The span self-attention is identical to the typical transformer layer, exception that the self-attention is between mention-span vectors instead of word piece vectors. This allows KnowBert to incorporate global information into each linking decision so that it can take advantage of entity-entity cooccurrence and resolve which of several overlapping candidate mentions should be linked.1 Following Kolitsas et al. (2018), Se is used to score each of the candidate entities while incorporating the candidate entity prior from the KB. Each candidate span m has an associated mention-span (5) emk 6=emg formulations (see Sec. 4.1 for details). Knowledge enhanced entity-span representations KnowBert next injects the KB entity information into the mention-span representations computed from BERT vectors (sem ) to form entityspan representations. For a given span m, we first disregard all candidate entities with score ψ below a fixed threshold, and softmax normalize the remaining scores:  exp(ψmk )   , ψmk ≥ δ  P ex"
D19-1005,D14-1110,0,0.0324097,"knowledge graph. These methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi"
D19-1005,D17-1018,0,0.0321533,"back to the BERT dimension (7) resulting in H0i . vector sem (computed via Eq. 2), Mm candidate entities with embeddings emk (from the KB), and prior probabilities pmk . We compute Mm scores using the prior and dot product between the entityspan vectors and entity embeddings, jected to the entity dimension (E, typically 200 or 300, see Sec. 4.1) with a linear projection, proj Hi proj = Hi W1 proj + b1 . (1) Then, the KAR computes C mention-span representations sm ∈ RE , one for each candidate mention, by pooling over all word pieces in a mentionspan using the self-attentive span pooling from Lee et al. (2017). The mention-spans are stacked into a matrix S ∈ RC×E . ψmk = MLP(pmk , sem · emk ), with a two-layer MLP (100 hidden dimensions). If entity linking (EL) supervision is available, we can compute a loss with the gold entity emg . The exact form of the loss depends on the KB, and we use both log-likelihood,   X exp(ψmg ) LEL = − log P , (4) k exp(ψmk ) m Entity linker The entity linker is responsible for performing entity disambiguation for each potential mention from among the available candidates. It first runs mention-span self-attention to compute Se = TransformerBlock(S). (3) and max-mar"
D19-1005,P18-1009,0,0.0644275,"Missing"
D19-1005,Q15-1023,1,0.851813,"input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors into entity linkers can be a powerful signal, so we optionally allow for the candidate selector to return an associated prior probability for each entity candidate. In some cases, it is beneficial to over-generate potential candidates and add a special NULL entity to each candidate list, thereby allowing the linker to discriminate between actual links and false positive candidates. In this work, the entity candidate selectors are fixed but their output is passed to a learned context dependent entity linker to disambiguate the candidate mentions. Fina"
D19-1005,P19-1598,1,0.857562,"eam performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 43–54, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics et al., 2017). Our approach is agnostic to the details of the entity embedding method and as a result is able to use any of these methods. KB, subject to a small set of requirements (see Se"
D19-1005,K16-1006,0,0.0381149,"performance as they must balance specializing for the entity linking task and learning general purpose representations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for fu"
D19-1005,N19-1423,0,0.721409,"nd-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confere"
D19-1005,P18-1076,0,0.131211,"revious work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We buil"
D19-1005,D17-1169,0,0.0298708,"projected word piece representations and knowledge enhanced entityspan vectors. As introduced by Vaswani et al. (2017), the contextual embeddings Hi are used for the query, key, and value in multi-headed selfattention. The word-to-entity-span attention in proj KnowBert substitutes Hi for the query, and S0e for both the key and value: 0 proj Hi proj 0 3.4 Our training regime incrementally pretrains increasingly larger portions of KnowBert before fine-tuning all trainable parameters in a multitask setting with any available EL supervision. It is similar in spirit to the “chain-thaw” approach in Felbo et al. (2017), and is summarized in Alg. 1. We assume access to a pretrained BERT model and one or more KBs with their entity candidate selectors. To add the first KB, we begin by pretraining entity embeddings (if not already provided from another source), then freeze them in all subsequent training, including task-specific finetuning. If EL supervision is available, it is used to pretrain the KB specific EL parameters, while freezing the remainder of the network. Finally, the entire network is fine-tuned to convergence by minimizing 0 = MLP(MultiHeadAttn(Hi , S e , S e )). This allows each word piece to a"
D19-1005,D17-1277,0,0.337604,"ese methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear funct"
D19-1005,H94-1046,0,0.0558172,"dings. We used TuckER (Balazevic et al., 2019) to compute 200dimensional vectors for each synset and lemma using the relationship graph. Then, we extracted the gloss for each synset and used an off-theshelf state-of-the-art sentence embedding method (Subramanian et al., 2018) to produce 2048dimensional vectors. These are concatenated to the TuckER embeddings. To reduce the dimensionality for use in KnowBert, the frozen 2248dimensional embeddings are projected to 200dimensions with a learned linear transformation. For supervision, we combined the SemCor word sense disambiguation (WSD) dataset (Miller et al., 1994) with all lemma example usages from WordNet6 and link directly to synsets. The loss function is Eq. 4. At train time, we did not provide gold lemmas or POS tags, so KnowBert must learn to implicitly model coarse grained POS tags to disambiguate each word. At test time when evaluating we restricted candidate entities to just those matching the gold lemma and POS tag, consistent with the standard WSD evaluation. 4.2 Intrinsic Evaluation Perplexity Table 1 compares masked LM perplexity for KnowBert with BERTBASE and BERTLARGE . To rule out minor differences due to our data preparation, the BERT m"
D19-1005,E17-1010,0,0.0327968,"ameters are actually used. In contrast, BERTLARGE uses the majority of its 336M parameters for each input. Integrated EL It is also possible to evaluate the performance of the integrated entity linkers inside KnowBert using diagnostic probes without any further fine-tuning. As these were trained in a multitask setting primarily with raw text, we do not a priori expect high performance as they must balance specializing for the entity linking task and learning general purpose representations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This se"
D19-1005,D14-1162,0,0.0943202,"ing KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorporating structured knowledge into these models. 3 KnowBert KnowBert incorporates knowledge bases into BERT using the Knowledge Attention and Recontextualization component (KAR). We start by describing the BERT and KB components. We then move to introducing KAR. Finally, we describe the training procedure, including the multitask training regime for jointly training KnowBert and an entity linker. Entity embeddings Entity embeddi"
D19-1005,N18-1202,1,0.886558,"supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural"
D19-1005,P19-1219,0,0.0221389,"n adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorp"
D19-1005,N19-1128,0,0.0605837,"Missing"
D19-1005,P19-1132,0,0.0588253,"Missing"
D19-1005,P16-1123,0,0.0556875,"Missing"
D19-1005,D17-1120,0,0.0301197,"resentations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and t"
D19-1005,D14-1167,0,0.0467978,"using the Knowledge Attention and Recontextualization component (KAR). We start by describing the BERT and KB components. We then move to introducing KAR. Finally, we describe the training procedure, including the multitask training regime for jointly training KnowBert and an entity linker. Entity embeddings Entity embedding methods produce continuous vector representations from external knowledge sources. Knowledge graphbased methods optimize the score of observed triples in a knowledge graph. These methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We"
D19-1005,P16-1162,0,0.0394323,"embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors"
D19-1005,P19-1279,0,0.0283521,"it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and two contemporaneous papers that add similar types of knowledge to BERT. ERNIE (Zhang et al., 2019) uses TAGME (Ferragina and Scaiella, 2010) to link entities to Wikidata, retrieves the associated entity embeddings, and fuses them into BERTBASE by fine-tuning. Soares et al. (2019) learns relationship representations by fine-tuning BERTLARGE with large scale “matching the blanks” (MTB) pretraining using entity linked text. 50 System ELMo† BERTBASE † BERTLARGE † BERTLARGE †† KnowBert-W+W Accuracy System 57.7 65.4 65.5 69.5 70.9 UFET BERTBASE ERNIE KnowBert-W+W P R F1 68.8 76.4 78.4 78.6 53.3 71.0 72.9 73.7 60.1 73.6 75.6 76.1 Table 7: Test set results for entity typing using the nine general types from (Choi et al., 2018). Table 6: Test set results for the WiC dataset (v1.0). † Pilehvar and Camacho-Collados (2019) †† Wang et al. (2019a) Entity typing We also evaluated Kn"
D19-1005,spitkovsky-chang-2012-cross,0,0.314947,"eral and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors into entity linkers can be a powerful signal, so we optionally allow for the candidate selector to return an associated prior probability for each entity candidate. In some cases, it is beneficial to over-generate potential candidates and add a special NULL entity to each candidate list, thereby allowing the linker to discriminate between actual links and false positive candidates. In this work, the entity candidate"
D19-1005,P19-1226,0,0.0248349,"tive language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorporating structured k"
D19-1005,D18-1455,0,0.0713268,"Missing"
D19-1005,P17-1132,0,0.0797122,"plicitly model entity spans in the input text and use an entity linker to retrieve relevant entity embeddings from a KB to form knowledge enhanced entity-span representations. Then, the model recontextualizes the entity-span representations with word-toentity attention to allow long range interactions between contextual word representations and all entity spans in the context. The entire KAR is inserted between two layers in the middle of a pretrained model such as BERT. In contrast to previous approaches that integrate external knowledge into task-specific models with task supervision (e.g., Yang and Mitchell, 2017; Chen et al., 2018), our approach learns the entity linkers with self-supervision on unlabeled data. This results in general purpose knowledge enhanced representations that can be applied to a wide range of downstream tasks. Our approach has several other benefits. First, it leaves the top layers of the original model unchanged so we may retain the output loss layers and fine-tune on unlabeled corpora while training the KAR. This also allows us to simply swap out BERT for KnowBert in any downstream application. Second, by taking advantage of the existing high capacity layers in the original m"
D19-1005,D17-1197,0,0.0344531,"luate KnowBert with a mix of intrinsic and extrinsic tasks. Despite being based on the smaller BERTBASE model, the experiments demonstrate improved masked language model perplexity and ability to recall facts over BERTLARGE . The extrinsic evaluations demonstrate improvements for challenging relationship extraction, entity typing and word sense disambiguation datasets, and often outperform other contemporaneous attempts to incorporate external knowledge into BERT. 2 Entity-aware language models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more"
D19-1005,D18-1244,0,0.0614284,"Missing"
D19-1005,D17-1004,0,0.0721139,"Missing"
D19-1005,P19-1139,0,0.110116,"is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and two contemporaneous papers that add similar types of knowledge to BERT. ERNIE (Zhang et al., 2019) uses TAGME (Ferragina and Scaiella, 2010) to link entities to Wikidata, retrieves the associated entity embeddings, and fuses them into BERTBASE by fine-tuning. Soares et al. (2019) learns relationship representations by fine-tuning BERTLARGE with large scale “matching the blanks” (MTB) pretraining using entity linked text. 50 System ELMo† BERTBASE † BERTLARGE † BERTLARGE †† KnowBert-W+W Accuracy System 57.7 65.4 65.5 69.5 70.9 UFET BERTBASE ERNIE KnowBert-W+W P R F1 68.8 76.4 78.4 78.6 53.3 71.0 72.9 73.7 60.1 73.6 75.6 76.1 Table 7: Test set results for entity typing using the nine general"
D19-1110,P07-1056,0,0.0182093,"sso regularization, using increasingly large regularization strengths, resulting in increasingly compact models. As the goal of our experiments is to demonstrate the ability of our approach to reduce the number of parameters, we only consider rational baselines: the same rational RNNs trained without group lasso.6 We manually tune the number and sizes of the baselines WFSAs, and then compare the tradeoff curve between model size and accuracy. We describe our experiments below. For more details, see Appendix A. Data We experiment with the Amazon reviews binary sentiment classification dataset (Blitzer et al., 2007), composed of 22 product categories. We examine the standard dataset (original mix) comprised of a mixture of data from the different categories (Johnson and Zhang, 2015).7 We also examine three of the largest individual categories as separate datasets (kitchen, dvd, and books), following Johnson and Zhang (2015). The three category datasets do not overlap with each other (though they do with original mix), and are significantly different in size (see Appendix A), so we can see how our approach behaves with different amounts of training data. Implementation details To classify text, we concate"
D19-1110,N19-1423,0,0.0339076,"elease our code.1 1 (1) w1 (2) w1 (3) w1 (2) w2 (3) w2 (1) w3 (2) w3 (3) w3 Base structure (1) wˆ1 (2) (1) wˆ2 (2) (1) wˆ3 wˆ1 wˆ2 (0) (0) (0) (0) Learned structure Figure 1: Our approach learns a sparse structure (right hand side) of a base rational RNN (left hand side) where each hidden unit corresponds to a WFSA (in this example, three hidden units, represented by the three rows). Grayed-out, dashed states are removed from the model, while retained states are marked in bold green. Introduction State-of-the-art neural models for NLP are heavily parameterized, requiring hundreds of millions (Devlin et al., 2019) and even billions (Radford et al., 2019) of parameters. While overparameterized models can sometimes be easier to train (Livni et al., 2014), they may also introduce memory problems on small devices and lead to increased carbon emission (Strubell et al., 2019; Schwartz et al., 2019). In feature-based NLP, structured-sparse regularization, in particular the group lasso (Yuan and 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs (1) w2 Lin, 2006), has been proposed as a method to reduce model size while preserving performance (Martins et al., 2011). But, in neural NLP, some of"
D19-1110,N15-1011,0,0.0230283,"bility of our approach to reduce the number of parameters, we only consider rational baselines: the same rational RNNs trained without group lasso.6 We manually tune the number and sizes of the baselines WFSAs, and then compare the tradeoff curve between model size and accuracy. We describe our experiments below. For more details, see Appendix A. Data We experiment with the Amazon reviews binary sentiment classification dataset (Blitzer et al., 2007), composed of 22 product categories. We examine the standard dataset (original mix) comprised of a mixture of data from the different categories (Johnson and Zhang, 2015).7 We also examine three of the largest individual categories as separate datasets (kitchen, dvd, and books), following Johnson and Zhang (2015). The three category datasets do not overlap with each other (though they do with original mix), and are significantly different in size (see Appendix A), so we can see how our approach behaves with different amounts of training data. Implementation details To classify text, we concatenate the scores computed by each WFSA, then feed this d-dimensional vector of scores into a linear binary classifier. We use log loss. We experiment with both type-level"
D19-1110,P18-1028,1,0.90485,"rameters (other than, perhaps, division into layers), so the use of structured sparsity at first may appear incongruous. In this paper we show that group lasso can be successfully applied to neural NLP models. We focus on a family of neural models for which the hidden state exhibits a natural structure: rational RNNs (Peng et al., 2018). In a rational RNN, the value of each hidden dimension is the score of a weighted finite-state automaton (WFSA) on (a prefix of) the input vector sequence. This property offers a natural grouping of the transition function parameters for each WFSA. As shown by Schwartz et al. (2018) and Peng et al. (2018), a variety of state-of-the-art neural architectures are rational (Lei et al., 2017; Bradbury et al., 2017; Foerster et al., 2017, inter alia), so learning parameterefficient rational RNNs is of practical value. We 1179 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1179–1184, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics also take advantage of the natural interpretation of rational RNNs as “soft” patterns (Schw"
D19-1110,P19-1355,0,0.0173307,"t hand side) where each hidden unit corresponds to a WFSA (in this example, three hidden units, represented by the three rows). Grayed-out, dashed states are removed from the model, while retained states are marked in bold green. Introduction State-of-the-art neural models for NLP are heavily parameterized, requiring hundreds of millions (Devlin et al., 2019) and even billions (Radford et al., 2019) of parameters. While overparameterized models can sometimes be easier to train (Livni et al., 2014), they may also introduce memory problems on small devices and lead to increased carbon emission (Strubell et al., 2019; Schwartz et al., 2019). In feature-based NLP, structured-sparse regularization, in particular the group lasso (Yuan and 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs (1) w2 Lin, 2006), has been proposed as a method to reduce model size while preserving performance (Martins et al., 2011). But, in neural NLP, some of the most widely used models—LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014)—do not have an obvious, intuitive notion of “structure” in their parameters (other than, perhaps, division into layers), so the use of structured sparsity at firs"
D19-1110,D11-1139,1,0.775047,"quiring hundreds of millions (Devlin et al., 2019) and even billions (Radford et al., 2019) of parameters. While overparameterized models can sometimes be easier to train (Livni et al., 2014), they may also introduce memory problems on small devices and lead to increased carbon emission (Strubell et al., 2019; Schwartz et al., 2019). In feature-based NLP, structured-sparse regularization, in particular the group lasso (Yuan and 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs (1) w2 Lin, 2006), has been proposed as a method to reduce model size while preserving performance (Martins et al., 2011). But, in neural NLP, some of the most widely used models—LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014)—do not have an obvious, intuitive notion of “structure” in their parameters (other than, perhaps, division into layers), so the use of structured sparsity at first may appear incongruous. In this paper we show that group lasso can be successfully applied to neural NLP models. We focus on a family of neural models for which the hidden state exhibits a natural structure: rational RNNs (Peng et al., 2018). In a rational RNN, the value of each hidden dimension is the score"
D19-1110,D18-1152,1,0.906466,"Carnegie Mellon University, Pittsburgh, PA, USA ♦ Allen Institute for Artificial Intelligence, Seattle, WA, USA ♠ Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA jessed@cs.cmu.edu roys@allenai.org {hapeng,nasmith}@cs.washington.edu Abstract Neural models for NLP typically use large numbers of parameters to reach state-of-theart performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of models that is closely connected to weighted finitestate automata (WFSAs). We take advantage of rational RNNs’ natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment analysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parameters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of rational RNNs. We sh"
D19-1110,D14-1162,0,0.0997069,"Missing"
D19-1224,Q17-1033,0,0.0166996,"tion. They recommend detailing experimental results found throughout the research process in a time-stamped document, as is done in other experimental science fields. Our work formalizes these issues and provides an ac2192 tionable set of recommendations to address them. Reproducibility issues relating to standard data splits (Schwartz et al., 2011; Gorman and Bedrick, 2019; Recht et al., 2019a,b) have surfaced in a number of areas. Shuffling standard training, validation, and test set splits led to a drop in performance, and in a number of cases the inability to reproduce rankings of models. Dror et al. (2017) studied reproducibility in the context of consistency among multiple comparisons. Limited community standards exist for documenting datasets and models. To address this, Gebru et al. (2018) recommend pairing new datasets with a “datasheet” which includes information such as how the data was collected, how it was cleaned, and the motivation behind building the dataset. Similarly, Mitchell et al. (2019) advocate for including a “model card” with trained models which document training data, model assumptions, and intended use, among other things. Our recommendations in §5 are meant to document r"
D19-1224,W18-2501,0,0.0308221,"tivity analysis in Zhang and Wallace (2015). We run 50 trials of random hyperparameter search for each classifier. Our results (Fig. 1) confirm previous findings (Zhang and Wallace, 2015): under a budget of fewer than 10 hyperparameter 11 https://github.com/allenai/ show-your-work 0.90 0.88 0.86 0.84 0.82 0.80 0.78 GloVe + ELMo (FT) GloVe + ELMo (FR) GloVe 0.76 For each experiment, we document the hyperparameter search space, hardware, average runtime, number of samples, and links to model implementations. We use public implementations for all models in our experiments, primarily in AllenNLP (Gardner et al., 2018). We use Tune (Liaw et al., 2018) to run parallel evaluations of uniformly sampled hyperparameter values. 4.2 SST (binary) 0.92 Expected validation accuracy match their reported performance. We find these budget estimates vary drastically. Consistently, we see that the best model is a function of the budget. We publicly release the search space and training configurations used for each case study. 11 Note that we do not report test performance in our experiments, as our purpose is not to establish a benchmark level for a model, but to demonstrate the utility of expected validation performance"
D19-1224,P19-1267,0,0.0285409,"ectly attributing empirical gains to modeling choices when they came from other sources such as hyperparameter tuning. Sculley et al. (2018) list examples of similar evaluation issues, and suggest encouraging stronger standards for empirical evaluation. They recommend detailing experimental results found throughout the research process in a time-stamped document, as is done in other experimental science fields. Our work formalizes these issues and provides an ac2192 tionable set of recommendations to address them. Reproducibility issues relating to standard data splits (Schwartz et al., 2011; Gorman and Bedrick, 2019; Recht et al., 2019a,b) have surfaced in a number of areas. Shuffling standard training, validation, and test set splits led to a drop in performance, and in a number of cases the inability to reproduce rankings of models. Dror et al. (2017) studied reproducibility in the context of consistency among multiple comparisons. Limited community standards exist for documenting datasets and models. To address this, Gebru et al. (2018) recommend pairing new datasets with a “datasheet” which includes information such as how the data was collected, how it was cleaned, and the motivation behind building"
D19-1224,P17-1152,0,0.031975,"the hyperparameters they did list in addition to standard choices (such as the learning rate). In neither case do they report the method used to tune the hyperparameters, and we suspect they tuned them manually. Our experiments here are meant give an idea of the budget that would be required to reproduce their results or to apply their models to other datasets under random hyperparameter value selection. SciTail When introducing the SciTail textual entailment dataset, Khot et al. (2018) compared four models: an n-gram baseline, which measures word-overlap as an indicator of entailment, ESIM (Chen et al., 2017), a sequence-based entailment model, DAM (Parikh et al., 2016), a bagof-words entailment model, and their proposed model, DGEM (Khot et al., 2018), a graph-based structured entailment model. Their conclusion was that DGEM outperforms the other models. 12 Peters et al. (2019) use a BCN with frozen embeddings and a BiLSTM BCN for fine-tuning. We conducted experiments with both a BCN and a BiLSTM with frozen and finetuned embeddings, and found our conclusions to be consistent. We report the full hyperparameter search space, which matched Peters et al. (2019) as closely as their reporting allowed,"
D19-1224,N19-1423,0,0.0419714,"ng to a model on a leaderboard is difficult if they only report test scores. For example, on the GLUE benchmark (Wang et al., 2018), the differences in test set performance between the top performing models can be on the order of a tenth of a percent, while the difference between test and validation performance might be one percent or larger. Verifying that a new implementation matches established performance requires submitting to the leaderboard, wasting test evaluations. Thus, we recommend leaderboards report validation performance for models evaluated on test sets. As an example, consider Devlin et al. (2019), which introduced BERT and reported state-of-theart results on the GLUE benchmark. The authors provide some details about the experimental setup, but do not report a specific budget. Subsequent work which extended BERT (Phang et al., 2018) included distributions of validation results, and we highlight this as a positive example of how to report experimental results. To achieve comparable test performance to Devlin et al. (2019), the authors report the best of twenty or one hundred random initializations. Their validation performance reporting not only illuminates the budget required to fine-t"
D19-1224,D16-1244,0,0.0858124,"Missing"
D19-1224,D14-1162,0,0.0820497,"s K times (where K is large), and crete random variables, 2188 4.1 Experimental Details Validating Previous Findings We start by applying our technique on a text classification task in order to confirm a well-established observation (Yogatama and Smith, 2015): logistic regression has reasonable performance with minimal hyperparameter tuning, but a well-tuned convolutional neural network (CNN) can perform better. We experiment with the fine-grained Stanford Sentiment Treebank text classification dataset (Socher et al., 2013). For the CNN classifier, we embed the text with 50-dim GloVe vectors (Pennington et al., 2014), feed the vectors to a ConvNet encoder, and feed the output representation into a softmax classification layer. We use the scikit-learn implementation of logistic regression with bag-of-word counts and a linear classification layer. The hyperparameter spaces HCNN and HLR are detailed in Appendix B. For logistic regression we used bounds suggested by Yogatama and Smith (2015), which include term weighting, ngrams, stopwords, and learning rate. For the CNN we follow the hyperparameter sensitivity analysis in Zhang and Wallace (2015). We run 50 trials of random hyperparameter search for each cla"
D19-1224,W19-4302,1,0.871645,"plot shows that for each approach (GloVe, ELMo frozen, and ELMo fine-tuned), there exists a budget for which it is preferable. search trials, logistic regression achieves a higher expected validation accuracy than the CNN. As the budget increases, the CNN gradually improves to a higher overall expected validation accuracy. For all budgets, logistic regression has lower variance, so may be a more suitable approach for fast prototyping. 4.3 Contextual Representations We next explore how computational budget affects the performance of contextual embedding models (Peters et al., 2018). Recently, Peters et al. (2019) compared two methods for using contextual representations for downstream tasks: feature extraction, where features are fixed after pretraining and passed into a task-specific model, or fine-tuning, where they are updated during task training. Peters et al. (2019) found that feature extraction is preferable to fine-tuning ELMo embeddings. Here we set to explore whether this conclusion depends on the experimental budget. Closely following their experimental setup, in Fig. 2 we show the expected performance of the biattentive classification network (BCN; McCann et al., 2017) with three embedding"
D19-1224,N18-1202,0,0.150615,"e mance (V ∗ ) matches a desired performance level. the mean and variance of the best validation perWe present three examples that demonstrate these formance. The bootstrap (Efron and Tibshirani, use cases. First, we reproduce previous findings 1994) is a general method which can be used to that compared different models for text classifiestimate statistics that do not have a closed form. cation. Second, we explore the time vs. perforThe bootstrap process is as follows: draw N i.i.d. mance tradeoff of models that use contextual word samples (in our case, N model evaluations). From embeddings (Peters et al., 2018). Third, from these N points, sample n points (with replacetwo previously published papers, we examine the ment), and compute the statistic of interest (e.g., budget required for our expected performance to the max). Do this K times (where K is large), and crete random variables, 2188 4.1 Experimental Details Validating Previous Findings We start by applying our technique on a text classification task in order to confirm a well-established observation (Yogatama and Smith, 2015): logistic regression has reasonable performance with minimal hyperparameter tuning, but a well-tuned convolutional ne"
D19-1224,P11-1067,1,0.744768,"rning, including incorrectly attributing empirical gains to modeling choices when they came from other sources such as hyperparameter tuning. Sculley et al. (2018) list examples of similar evaluation issues, and suggest encouraging stronger standards for empirical evaluation. They recommend detailing experimental results found throughout the research process in a time-stamped document, as is done in other experimental science fields. Our work formalizes these issues and provides an ac2192 tionable set of recommendations to address them. Reproducibility issues relating to standard data splits (Schwartz et al., 2011; Gorman and Bedrick, 2019; Recht et al., 2019a,b) have surfaced in a number of areas. Shuffling standard training, validation, and test set splits led to a drop in performance, and in a number of cases the inability to reproduce rankings of models. Dror et al. (2017) studied reproducibility in the context of consistency among multiple comparisons. Limited community standards exist for documenting datasets and models. To address this, Gebru et al. (2018) recommend pairing new datasets with a “datasheet” which includes information such as how the data was collected, how it was cleaned, and the"
D19-1224,D13-1170,0,0.00488383,"atistic of interest (e.g., budget required for our expected performance to the max). Do this K times (where K is large), and crete random variables, 2188 4.1 Experimental Details Validating Previous Findings We start by applying our technique on a text classification task in order to confirm a well-established observation (Yogatama and Smith, 2015): logistic regression has reasonable performance with minimal hyperparameter tuning, but a well-tuned convolutional neural network (CNN) can perform better. We experiment with the fine-grained Stanford Sentiment Treebank text classification dataset (Socher et al., 2013). For the CNN classifier, we embed the text with 50-dim GloVe vectors (Pennington et al., 2014), feed the vectors to a ConvNet encoder, and feed the output representation into a softmax classification layer. We use the scikit-learn implementation of logistic regression with bag-of-word counts and a linear classification layer. The hyperparameter spaces HCNN and HLR are detailed in Appendix B. For logistic regression we used bounds suggested by Yogatama and Smith (2015), which include term weighting, ngrams, stopwords, and learning rate. For the CNN we follow the hyperparameter sensitivity anal"
D19-1224,P19-1355,0,0.0224817,"t performing model is a function of the computational budget. In §4.4 we show how our approach can be used to estimate the budget that went into obtaining previous results; in one example, we see a too-small budget for baselines, while in another we estimate a budget of about 18 GPU days was used (but not reported). Previous work on reporting validation performance used the bootstrap to approximate the mean and variance of the best performing model (Lucic et al., 2018); in §3.2 we show that our ap2 Recent work has also called attention to the environmental cost of intensive model exploration (Strubell et al., 2019). 3 We use the term performance as a general evaluation measure, e.g., accuracy, F1 , etc. 4 We leave forecasting performance with larger budgets n > N to future work. proach computes these values with strictly less error than the bootstrap. We conclude by presenting a set of recommendations for researchers that will improve scientific reporting over current practice. We emphasize this work is about reporting, not about running additional experiments (which undoubtedly can improve evidence in comparisons among models). Our reporting recommendations aim at reproducibility and improved understan"
D19-1224,W18-5446,0,0.0244779,"ling for the amount of training data, which is an established norm in NLP research. helped to mitigate the so-called “replication crisis” happening in fields such as psychology and medicine (Ioannidis, 2005; Gelman and Loken, 2014). Unfortunately, leaderboards can create additional reproducibility issues (Rogers, 2019). First, leaderboards obscure the budget that was used to tune hyperparameters, and thus the amount of work required to apply a model to a new dataset. Second, comparing to a model on a leaderboard is difficult if they only report test scores. For example, on the GLUE benchmark (Wang et al., 2018), the differences in test set performance between the top performing models can be on the order of a tenth of a percent, while the difference between test and validation performance might be one percent or larger. Verifying that a new implementation matches established performance requires submitting to the leaderboard, wasting test evaluations. Thus, we recommend leaderboards report validation performance for models evaluated on test sets. As an example, consider Devlin et al. (2019), which introduced BERT and reported state-of-theart results on the GLUE benchmark. The authors provide some de"
D19-1224,D15-1251,1,0.874752,"aw N i.i.d. mance tradeoff of models that use contextual word samples (in our case, N model evaluations). From embeddings (Peters et al., 2018). Third, from these N points, sample n points (with replacetwo previously published papers, we examine the ment), and compute the statistic of interest (e.g., budget required for our expected performance to the max). Do this K times (where K is large), and crete random variables, 2188 4.1 Experimental Details Validating Previous Findings We start by applying our technique on a text classification task in order to confirm a well-established observation (Yogatama and Smith, 2015): logistic regression has reasonable performance with minimal hyperparameter tuning, but a well-tuned convolutional neural network (CNN) can perform better. We experiment with the fine-grained Stanford Sentiment Treebank text classification dataset (Socher et al., 2013). For the CNN classifier, we embed the text with 50-dim GloVe vectors (Pennington et al., 2014), feed the vectors to a ConvNet encoder, and feed the output representation into a softmax classification layer. We use the scikit-learn implementation of logistic regression with bag-of-word counts and a linear classification layer. T"
D19-1224,D16-1264,0,0.0282532,"deviation, within the observed range of values. It takes about 18 days (55 hyperparameter trials) for the expected performance to match the reported results. Our results (Fig. 3) show that the different models require different budgets to reach their reported performance in expectation, ranging from 2 (ngram) to 20 (DGEM). Moreover, providing a large budget for each approach improves performance substantially over reported numbers. Finally, under different computation budgets, the top performing model changes (though the neural models are similar). SQuAD Next, we turn our attention to SQuAD (Rajpurkar et al., 2016) and report performance of the commonly-used BiDAF model (Seo et al., 2017). The set of hyperparameters we tune covers those mentioned in addition to standard choices (details in Appendix D). We see in Fig. 4 that we require a budget of 18 GPU days in order for the expected maximum validation performance to match the value reported in the original paper. This suggests that some combination of prior intuition and extensive hyperparameter tuning were used by the original authors, though neither were reported. 5 Recommendations Experimental results checklist The findings discussed in this paper a"
D19-1224,D17-1035,0,0.0292507,"dotted lines). https://github.com/allenai/allentune 2185 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2185–2194, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tained purely through more intensive hyperparameter tuning (Melis et al., 2018; Lipton and Steinhardt, 2018).2 Moreover, recent investigations into “state-ofthe-art” claims have found competing methods to only be comparable, without clear superiority, even against baselines (Reimers and Gurevych, 2017; Lucic et al., 2018; Li and Talwalkar, 2019); this has exposed the need for reporting more than a single point estimate of performance. Echoing calls for more rigorous scientific practice in machine learning (Lipton and Steinhardt, 2018; Sculley et al., 2018), we draw attention to the weaknesses in current reporting practices and propose solutions which would allow for fairer comparisons and improved reproducibility. Our primary technical contribution is the introduction of a tool for reporting validation results in an easily interpretable way: expected validation performance of the best mode"
D19-1376,N18-1202,0,0.0596875,"Missing"
D19-1376,J91-3004,0,0.442397,"w that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no marginal inference, which can be computationally expensive (Jelinek and Lafferty, 1991; Chelba and Jelinek, 1998; Roark, 2001; Dyer et al., 2016; Buys and Blunsom, 2018; Kim et al., 2019, inter alia). Second, the attention can be syntactically informed, in the sense that the attention component can optionally be supervised using syntactic annotations, either through pretraining or by joint training with the LM (§2.2). Last, PaLM can derive an unsupervised constituency parser (§2.2), whose parameters are estimated purely using the language modeling objective. To demonstrate the empirical benefits of PaLM, we experiment with language modeling (§3). PaLM outperforms the AWD-LSTM m"
D19-1376,N19-1114,0,0.0226965,"age model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no marginal inference, which can be computationally expensive (Jelinek and Lafferty, 1991; Chelba and Jelinek, 1998; Roark, 2001; Dyer et al., 2016; Buys and Blunsom, 2018; Kim et al., 2019, inter alia). Second, the attention can be syntactically informed, in the sense that the attention component can optionally be supervised using syntactic annotations, either through pretraining or by joint training with the LM (§2.2). Last, PaLM can derive an unsupervised constituency parser (§2.2), whose parameters are estimated purely using the language modeling objective. To demonstrate the empirical benefits of PaLM, we experiment with language modeling (§3). PaLM outperforms the AWD-LSTM model (Merity et al., 2018) on both the Penn Treebank 3644 Proceedings of the 2019 Conference on Empi"
D19-1376,D17-1018,0,0.0285876,"t step t, PaLM attends over the spans ending at t − 1, up to a maximum length m, i.e., t−1 {[i, t − 1]}i=t−m .3 Essentially, this can be seen as splitting the prefix span [t − m, t − 1] into two, and attending over the one on the right. Such a span attention mechanism is inspired by the top-down greedy span parser of Stern et al. (2017), which recursively divides phrases. In §2.2, we will use a similar algorithm to derive a constituency parser from the span attention weights. Bidirectional span representation with rational RNNs. Meaningful span representations are crucial in span-based tasks (Lee et al., 2017; Peng et al., 2018c; Swayamdipta et al., 2018, inter 1 We experiment with a strong LSTM implementation for language modeling (Merity et al., 2018), see §3. 2 Standard token-based self-attention naturally relates to dependency structures through head selection (Strubell et al., 2018). In a left-to-right factored language model, dependencies are less natural if we want to allow a child to precede its parent. 3 m is set to 20. This reduces the number of considered spans from O(n2 ) to O(mn). Besides practical concerns, it makes less sense if a phrase goes beyond one single sentence (the average"
D19-1376,J93-2004,0,0.0649163,"2.2). Last, PaLM can derive an unsupervised constituency parser (§2.2), whose parameters are estimated purely using the language modeling objective. To demonstrate the empirical benefits of PaLM, we experiment with language modeling (§3). PaLM outperforms the AWD-LSTM model (Merity et al., 2018) on both the Penn Treebank 3644 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3644–3651, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (PTB; Marcus et al., 1993) and WikiText-2 (Merity et al., 2017) datasets by small but consistent margins in the unsupervised setup. When the parser is trained jointly with the language model, we see additional perplexity reductions in both cases. Our implementation is available at https: //github.com/Noahs-ARK/PaLM. 2 PaLM—Parser and Language Model We describe PaLM in detail. At its core is an attention component, gathering the representations of preceding spans at each time step. Similar to self-attention, PaLM can be implemented on top of RNN encoders (Parikh et al., 2016), or as it is (Vaswani et al., 2017). Here we"
D19-1376,P17-1076,0,0.333111,"akanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no marginal inference, which can be computationally expensive (Jelinek and Lafferty, 1991; Chelba and Jelinek, 1998; Roark, 2001; Dyer et al., 2016; Buys and Blunsom, 2018; Kim et al., 2019, inter alia). Second, the attention can be syntactically informed, in the sense that the attention component can optionally be supervised using syntactic annotations, either through pretraining or by joint training with the LM (§2.2). Last, PaLM can derive an unsupervised cons"
D19-1376,D18-1548,0,0.147359,"Peters et al., 2018; Radford et al., 2019), which benefit many NLP tasks such as text classification (Howard and Ruder, 2018) and dataset creation (Zellers et al., 2018). Language models are typically trained on large amounts of raw text, and therefore do not explicitly encode any notion of structural information. Structures in the form of syntactic trees have been shown to benefit both classical NLP models (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no marginal inference, which"
D19-1376,D18-1412,1,0.910151,"cluding, most notably, contextual embeddings (Peters et al., 2018; Radford et al., 2019), which benefit many NLP tasks such as text classification (Howard and Ruder, 2018) and dataset creation (Zellers et al., 2018). Language models are typically trained on large amounts of raw text, and therefore do not explicitly encode any notion of structural information. Structures in the form of syntactic trees have been shown to benefit both classical NLP models (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2"
D19-1376,D18-1489,0,0.017797,"yields no improvement over the baseline. Unsupervised constituency parsing. We evaluate the parser component of PaLM-U on WSJ40. It uses the same data as in language modeling, but filters out sentences longer than 40 tokens after 10 Preliminary experiments show that including the span attention after the last layer yields similar empirical results, but is more sensitive to hyperparameters. 11 We use the WSJ portion of PTB for parsing annotations. 12 We set scores to m, m − 1, . . . , 1, before the softmax. 13 Several recent works report better language modeling perplexity (Yang et al., 2019; Takase et al., 2018; Dai et al., 2019, inter alia). Their contribution is orthogonal to ours and not head-to-head comparable to the models in the table. # Params. Dev. Test AWD-LSTM 24M 60.0 57.3 PRPN ON-LSTM 25M 58.3 62.0 56.2 PaLM-U PaLM-RB PaLM-S 24M 24M 24M 58.6 60.1 57.9 56.4 57.5 55.5 Table 1: PTB language modeling perplexity (lower is better). Bold fonts indicate best performance.13 Model # Params. Dev. Test AWD-LSTM 33M 68.6 65.8 PaLM-U PaLM-S 36M 36M 68.4 65.5 65.4 63.2 Table 2: WikiText-2 language modeling perplexity (lower is better). Bold fonts indicate best performance. punctuation removal. The mode"
D19-1376,D16-1244,0,0.109695,"Missing"
D19-1376,D18-1152,1,0.938629,"textual embeddings (Peters et al., 2018; Radford et al., 2019), which benefit many NLP tasks such as text classification (Howard and Ruder, 2018) and dataset creation (Zellers et al., 2018). Language models are typically trained on large amounts of raw text, and therefore do not explicitly encode any notion of structural information. Structures in the form of syntactic trees have been shown to benefit both classical NLP models (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no m"
D19-1376,D18-1009,1,0.821835,"e taking the top scoring attended span (red solid line) and the prefix leading to it. It then recursively splits the two sub-spans using the same procedure (line 3). Finally, spans of length two are trivially split into terminal nodes (line 4). Introduction Recent language models have shown very strong data-fitting performance (Jozefowicz et al., 2016; Merity et al., 2018). They offer useful products including, most notably, contextual embeddings (Peters et al., 2018; Radford et al., 2019), which benefit many NLP tasks such as text classification (Howard and Ruder, 2018) and dataset creation (Zellers et al., 2018). Language models are typically trained on large amounts of raw text, and therefore do not explicitly encode any notion of structural information. Structures in the form of syntactic trees have been shown to benefit both classical NLP models (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language"
K15-1026,D13-1167,0,0.0182713,"tation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by"
K15-1026,N09-1003,0,0.646382,"f-words models that encode co-occurrence statistics directly in features; (b) NN models that implement the bag-of-words approach in their objective; and (c) models that go beyond the bag-ofwords assumption. Similarity vs. Association Most recent VSM research does not distinguish between association and similarity in a principled way, although notable exceptions exist. Turney (2012) constructed two VSMs with the explicit goal of capturing either similarity or association. A classifier that uses the output of these models was able to predict whether two concepts are associated, similar or both. Agirre et al. (2009) partitioned the wordsim353 dataset into two subsets, one focused on similarity and the other on association. They demonstrated the importance of the association/similarity distinction by showing that some VSMs perform relatively well on one subset while others perform comparatively better on the other. Recently, Hill et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset"
K15-1026,P13-1174,0,0.0338768,"Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Table 1: The six most frequent pattern candidates that co"
K15-1026,P14-1023,0,0.21428,"Missing"
K15-1026,S13-1005,0,0.0664835,"Missing"
K15-1026,N15-1100,0,0.342972,"d similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. This algorithm starts by defining an SP templa"
K15-1026,D14-1162,0,0.108133,"Abstract has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Particularly, very little infor"
K15-1026,C92-2082,0,0.395839,"l et al. (2014) presented the SimLex999 dataset consisting of 999 word pairs judged by humans for similarity only. The participating words belong to a variety of POS tags and concreteness levels, arguably providing a more realistic sample of the English lexicon. Using their dataset the authors show the tendency of VSMs that take the bag-of-words approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used the"
K15-1026,J15-4004,1,0.60606,"Missing"
K15-1026,P08-1119,0,0.0131529,"onymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language"
K15-1026,E14-1051,0,0.0131748,"two important characteristics. First, they encode co-occurrence statistics from an input corpus directly into the word vector features. Second, they consider very little information on the syntactic and semantic relations between the represented word and its context items. Instead, a bag-of-words approach is taken. Recently, there is a surge of work focusing on Neural Network (NN) algorithms for word representations learning (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Collobert et al., 2011; Dhillon et al., 2011; Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Like the more traditional models, these works also take the bag-of-words approach, encoding only shallow co-occurrence information between linguistic items. However, they encode this information into their objective, often a language model, rather than directly into the features. Consider, for example, the successful word2vec model (Mikolov et al., 2013a). Its continuous-bagof-words architecture is designed to predict a word given its past and future context. The resulted objective function is: T X log p(wt |wt−c , . . . , wt−1 , wt+1 , . . . , wt+c ) max t=1 where"
K15-1026,P14-2050,0,0.364695,"−c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is particularly suitable for capturing word similarity. In experiments we show the superiority of our model over six models of the above three families: (a) bag-of-words models that encode co-occurrence statistics directly in features; (b) NN"
K15-1026,P14-2086,0,0.150344,"Missing"
K15-1026,D13-1193,1,0.834039,"approach to capture association much better than similarity. This observation motivates our work. Symmetric Patterns. Patterns (symmetric or not) were found useful in a variety of NLP tasks, including identification of word relations such as hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Patterns have also been applied to 260 Candidate “X of Y” “X the Y” “X to Y” “X and Y” “X in Y” “X of the Y” tackle sentence level tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand cra"
K15-1026,C14-1153,1,0.851528,"l tasks such as identification of sarcasm (Tsur et al., 2010), sentiment analysis (Davidov et al., 2010) and authorship attribution (Schwartz et al., 2013). Symmetric patterns (SPs) were employed in various NLP tasks to capture different aspects of word similarity. Widdows and Dorow (2002) used SPs for the task of lexical acquisition. Dorow et al. (2005) and Davidov and Rappoport (2006) used them to perform unsupervised clustering of words. Kozareva et al. (2008) used SPs to classify proper names (e.g., fish names, singer names). Feng et al. (2013) used SPs to build a connotation lexicon, and Schwartz et al. (2014) used SPs to perform minimally supervised classification of words into semantic categories. While some of these works used a hand crafted set of SPs (Widdows and Dorow, 2002; Dorow et al., 2005; Kozareva et al., 2008; Feng et al., 2013), Davidov and Rappoport (2006) introduced a fully unsupervised algorithm for the extraction of SPs. Here we apply their algorithm in order to reduce the required human supervision and demonstrate the language independence of our approach. Table 1: The six most frequent pattern candidates that contain exactly two wildcards and 1-3 words in our corpus. terns inclu"
K15-1026,N03-1033,0,0.0542377,"re 7 and 10. 6 www.cl.cam.ac.uk/˜fh295/simlex.html 7 code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (ρ) between the ranking derived from m’s scores and the one derived from the human scores. Model GloVe BOW CBOW Dep NNSE skip-gram Spearman’s ρ 0.35 0.423 0.43 0.436 0.455 0.462 SP(−) SP(+) 0.434 0.517 Joint (SP"
K15-1026,N13-1090,0,0.67305,"oiri@ie.technion.ac.il Abstract has become a key tool in NLP. Most approaches to word representation follow the distributional hypothesis (Harris, 1954), which states that words that co-occur in similar contexts are likely to have similar meanings. VSMs differ in the way they exploit word cooccurrence statistics. Earlier works (see (Turney et al., 2010)) encode this information directly in the features of the word vector representation. More Recently, Neural Networks have become prominent in word representation learning (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014, inter alia). Most of these models aim to learn word vectors that maximize a language model objective, thus capturing the tendencies of the represented words to co-occur in the training corpus. VSM approaches have resulted in highly useful word embeddings, obtaining high quality results on various semantic tasks (Baroni et al., 2014). Interestingly, the impressive results of these models are achieved despite the shallow linguistic information most of them consider, which is limited to the tendency of words to co-occur together in a pre-specified context window. Parti"
K15-1026,C08-1114,0,0.0274924,"that cooccur in SPs are semantically similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In thi"
K15-1026,J13-3004,0,0.0208148,"lly similar (Section 2). In this work we use symmetric patterns to represent words. Our hypothesis is that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2"
K15-1026,C12-1118,0,0.122551,"t=1 where T is the number of words in the corpus, and c is a pre-determined window size. Another word2vec architecture, skip-gram, aims to predict the past and future context given a word. Its objective is: T X X log p(wt+j |wt ) max t=1 −c≤j≤c,j6=0 In both cases the objective function relates to the co-occurrence of words within a context window. A small number of works went beyond the bagof-words assumption, considering deeper relationships between linguistic items. The Strudel system (Baroni et al., 2010) represents a word using the clusters of lexico-syntactic patterns in which it occurs. Murphy et al. (2012) represented words through their co-occurrence with other words in syntactic dependency relations, and then used the Non-Negative Sparse Embedding (NNSE) method to reduce the dimension of the resulted representation. Levy and Goldberg (2014) extended the skip-gram word2vec model with negative sampling (Mikolov et al., 2013b) by basing the word co-occurrence window on the dependency parse tree of the sentence. Bollegala et al. (2015) replaced bag-of-words contexts with various patterns (lexical, POS and dependency). We introduce a symmetric pattern based approach to word representation which is"
K15-1026,nivre-etal-2006-maltparser,0,0.00942273,"tml 7 code.google.com/p/word2vec/source/ browse/trunk/demo-train-big-model-v1.sh 263 5. NNSE. The NNSE model (Murphy et al., 2012). As no full implementation of this model is available online, we use the off-the-shelf embeddings available at the authors’ website,17 taking the full document and dependency model with 2500 dimensions. Embeddings were computed using a dataset about twice as big as our corpus. 6. Dep. The modified, dependency-based, skipgram model (Levy and Goldberg, 2014). To generate dependency links, we use the Stanford POS Tagger (Toutanova et al., 2003)18 and the MALT parser (Nivre et al., 2006).19 We follow the parameters suggested by the authors. 5.3 Evaluation For evaluation we follow the standard VSM literature: the score assigned to each pair of words by a model m is the cosine similarity between the vectors induced by m for the participating words. m’s quality is evaluated by computing the Spearman correlation coefficient score (ρ) between the ranking derived from m’s scores and the one derived from the human scores. Model GloVe BOW CBOW Dep NNSE skip-gram Spearman’s ρ 0.35 0.423 0.43 0.436 0.455 0.462 SP(−) SP(+) 0.434 0.517 Joint (SP(+) , skip-gram) Average Human Score 0.563"
K15-1026,C02-1114,0,0.129662,"ty, we propose an alternative, pattern-based, approach to word representation. In previous work patterns were used to represent a variety of semantic relations, including hyponymy (Hearst, 1992), meronymy (Berland and Charniak, 1999) and antonymy (Lin et al., 2003). Here, in order to capture similarity between words, we use Symmetric patterns (SPs), such as “X and Y” and “X as well as Y”, where each of the words in the pair can take either the X or the Y position. Symmetric patterns have shown useful for representing similarity between words in various NLP tasks including lexical acquisition (Widdows and Dorow, 2002), word clustering (Davidov and Rappoport, 2006) and classification of words to semantic categories (Schwartz et al., 2014). However, to the best of our knowledge, they have not been applied to vector space word representation. Our representation is constructed in the following way (Section 3). For each word w, we construct a vector v of size V , where V is the size of the lexicon. Each element in v represents the cooccurrence in SPs of w with another word in the lexicon, which results in a sparse word representation. Unlike most previous works that applied SPs to NLP tasks, we do not use a har"
K15-1026,D12-1111,0,0.166728,"that such representation would reflect word similarity (i.e., that similar vectors would represent similar words). Our experiments show that this is indeed the case. Antonyms. A useful property of our model is its ability to control the representation of antonym pairs. Outside the VSM literature several works identified antonyms using word co-occurrence statistics, manually and automatically induced patterns, the WordNet lexicon and thesauri (Lin et al., 2003; Turney, 2008; Wang et al., 2010; Mohammad et al., 2013; Schulte im Walde and Koper, 2013; Roth and Schulte im Walde, 2014). Recently, Yih et al. (2012), Chang et al. (2013) and Ono et al. (2015) proposed word representation methods that assign dissimilar vectors to antonyms. Unlike our unsupervised model, which uses plain text only, these works used the WordNet lexicon and a thesaurus. Symmetric Patterns Extraction. Most works that used SPs manually constructed a set of such patterns. The most prominent patterns in these works are “X and Y” and “X or Y” (Widdows and Dorow, 2002; Feng et al., 2013). In this work we follow (Davidov and Rappoport, 2006) and apply an unsupervised algorithm for the automatic extraction of SPs from plain text. Thi"
K15-1026,C10-2028,1,\N,Missing
K15-1026,P06-1038,1,\N,Missing
K15-1026,P99-1008,0,\N,Missing
K17-1004,D14-1155,1,0.38453,"ons reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatically affect the way people write.1 1 Ending She feels flattered and asks John on a date. The girl found this charming, and gave him a second chance. John was happy about being rejected. Table 1: Examples of stories from the story cloze task. The table shows a story prefix with three contrastive endings: The original ending, a coherent ending and a incoherent one. since different tasks likely engage different cognitive processes (Campbell and Pennebaker, 2003; Banerjee et al., 2014).2 We show that similar writing tasks with different constraints on the author can lead to measurable differences in her writing style. As a case study, we present experiments based on the recently introduced ROC story cloze task (Mostafazadeh et al., 2016a). In this task, authors were asked to write five-sentence self-contained stories, henceforth original stories. Then, each original story was given to a different author, who was shown only the first four sentences as a story context, and asked to write two contrasting story endings: a right (coherent) ending, and a wrong (incoherent) ending"
K17-1004,P82-1020,0,0.851485,"Missing"
K17-1004,N12-1033,0,0.00560131,"yle has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state, and also her concrete response. They also provide valuable lessons for designing new NLP datasets. 10 style and physical health. Psychological Science 14(1):60–65. Danqi Chen, Jason Bol"
K17-1004,D15-1075,0,0.02051,"ples compared to 3,742 in the story cloze task), this indicates that simple instructions may help alleviate the effects of writing style found in this paper. Another way to avoid such effects is to have people rate naturally occurring sentences by parameters such as coherence (or, conversely, the level of surprise), rather than asking them to generate new text. 8 Machine reading. The story cloze task, which is the focus of this paper, is part of a wide set of machine reading/comprehension challenges published in the last few years. These include datasets like bAbI (Weston et al., 2016), SNLI (Bowman et al., 2015), CNN/DailyMail (Hermann et al., 2015), LAMBADA (Paperno et al., 2016) and SQuAD (Rajpurkar et al., 2016). While these works have presented resources for researchers, it is often the case that these datasets suffer from methodological problems caused by applying noisy automatic tools to generate them (Chen et al., 2016).11 In this paper, we have pointed to another methodological challenge in designing machine reading tasks: different writing tasks used to generated the data affect writing style, confounding classification problems. 9 Conclusion Different writing tasks assigned to an author res"
K17-1004,P17-2097,0,0.437985,"Missing"
K17-1004,P14-2072,0,0.0173871,"Missing"
K17-1004,N16-1098,0,0.194848,"Washington, Seattle, WA, USA 2 Allen Institute for Artificial Intelligence, Seattle, WA, USA {roysch,msap,ikonstas,lzilles,yejin,nasmith}@cs.washington.edu Story Prefix John liked a girl at his work. He tried to get her attention by acting silly. She told him to grow up. John confesses he was trying to make her like him more. Abstract A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatical"
K17-1004,W17-0906,0,0.0269252,"Missing"
K17-1004,W16-2505,0,0.160323,"Washington, Seattle, WA, USA 2 Allen Institute for Artificial Intelligence, Seattle, WA, USA {roysch,msap,ikonstas,lzilles,yejin,nasmith}@cs.washington.edu Story Prefix John liked a girl at his work. He tried to get her attention by acting silly. She told him to grow up. John confesses he was trying to make her like him more. Abstract A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatical"
K17-1004,D16-1264,0,0.0242504,"15), LAMBADA Background: The Story Cloze Task To understand how different writing tasks affect writing style, we focus on the story cloze task (Mostafazadeh et al., 2016a). While this task was developed to facilitate representation and learning of commonsense story understanding, its design included a few key choices which make it ideal for our study. We describe the task below. ROC stories. The ROC story corpus consists of 49,255 five-sentence stories, collected on Ama3 Recently, additional 53K stories were released, which results in roughly 100K stories. 16 (Paperno et al., 2016) and SQuAD (Rajpurkar et al., 2016), for which results improved dramatically over similar or much shorter periods of time. This suggests that this task is challenging and that high performance is hard to achieve. In addition, Mostafazadeh et al. (2016a) made substantial efforts to ensure the quality of this dataset. First, each pair of endings was written by the same author, which ensured that style differences between authors could not be used to solve the task. Furthermore, Mostafazadeh et al. implemented nine baselines for the task, using surface level features as well as narrative-informed ones, and showed that each of them"
K17-1004,P11-1077,0,0.0110115,"eve state of the art results on the story cloze task. The findings presented in this paper have cognitive implications, as they motivate further research Related Work Writing style. Writing style has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state,"
K17-1004,W11-1515,1,0.178023,"Missing"
K17-1004,P11-1032,1,0.0248668,"Writing tasks can even have a long-term effect, as writing emotional texts was observed to benefit both physical and mental health (LepDesign of NLP tasks. Our study also provides important insights for the future design of NLP tasks. The story cloze task was very carefully designed. Many factors, such as topic diversity and 21 and Wallace, 1963; Pennebaker and King, 1999; Schwartz et al., 2013b). The line of work that most resembles our work is the detection of deceptive text. Several researchers have used stylometric features to predict deception (Newman et al., 2003; Hancock et al., 2007; Ott et al., 2011; Feng et al., 2012). Some works even showed that gender affects a person’s writing style when lying (P´erez-Rosas and Mihalcea, 2014a,b). In this work, we have shown that an even more subtle writing task—writing coherent and incoherent story endings—imposes different styles on the author. temporal and causal relation diversity, were controlled for (Mostafazadeh et al., 2016a). The authors also made sure each pair of endings was written by the same author, partly in order to avoid author-specific style effects. Nonetheless, despite these efforts, several significant style differences can be fo"
K17-1004,P16-1144,0,0.0591898,"Missing"
K17-1004,W17-0907,1,0.671362,"Missing"
K17-1004,D13-1193,1,0.264354,"periment 2 Table 5: The top 5 most heavily weighted features for predicting right vs. wrong endings (5a) and original vs. new (right) endings (5b). length is the sentence length feature (see Section 4). ore and Smyth, 2002; Frattaroli, 2006). Campbell and Pennebaker (2003) also showed that the health benefits of writing emotional text are accompanied by changes in writing style, mostly in the use of pronouns. Another line of work has shown that writing style is affected by mental state. First, an author’s personality traits (e.g., depression, neuroticism, narcissism) affect her writing style (Schwartz et al., 2013a; Ireland and Mehl, 2014). Second, temporary changes, such as a romantic relationship (Ireland et al., 2011; Bowen et al., 2016), work collaboration (Tausczik, 2009; Gonzales et al., 2009), or negotiation (Ireland and Henderson, 2014) may also affect writing style. Finally, writing style can also change from one sentence to another, for instance between positive and negative text (Davidov et al., 2010) or when writing sarcastic text (Tsur et al., 2010). This large body of work indicates a tight connection between writing tasks, mental states, and variation in writing style. This connection hi"
K17-1004,W07-0602,0,0.0529888,"Writing style. Writing style has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state, and also her concrete response. They also provide valuable lessons for designing new NLP datasets. 10 style and physical health. Psychological Science 14(1):60–65"
K17-1004,P13-1093,0,0.0486373,"Missing"
K17-1004,N12-1097,1,0.0996929,"Missing"
K17-1004,P12-2034,1,\N,Missing
K17-1004,P16-1223,0,\N,Missing
K17-1013,W13-3520,0,0.0499698,"eled conjll. If both are used, the label is simply conj=conjlr+conjll.7 Consequently, the individual context bags we use in all experiments are: subj, obj, comp, nummod, appos, nmod, acl, amod, prep, adv, compound, conjlr, conjll. 4.2 Training and Evaluation We run the algorithm for context configuration selection only once, with the SGNS training setup described below. Our main evaluation setup is presented below, but the learned configurations are tested in additional setups, detailed in Sect. 5. Training Data Our training corpus is the cleaned and tokenised English Polyglot Wikipedia data (Al-Rfou et al., 2013),8 consisting of approxi7 Given the coordination structure boys and girls, conjlr training pairs are (boys, girls_conj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of t"
K17-1013,P14-2131,0,0.0737811,"v et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still th"
K17-1013,P14-1023,0,0.0288632,"alian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and ve"
K17-1013,C10-1011,0,0.0100926,"nj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of the UD treebank annotated with UPOS tags. The data were then parsed with UD using the graph-based Mate parser v3.61 (Bohnet, 2010)10 with standard settings on TRAIN + DEV of the UD treebank. Evaluation We experiment with the verb pair (222 pairs), adjective pair (111 pairs), and noun pair (666 pairs) portions of SimLex-999. We report Spearman’s ρ correlation between the ranks derived from the scores of the evaluated models and the human scores. Our evaluation setup is borrowed from Levy et al. (2015): we perform 2-fold cross-validation, where the context configurations are optimised on a development set, separate from the unseen test data. Unless stated otherwise, the reported scores are always the averages of the 2 runs"
K17-1013,D14-1082,0,0.0395504,"at the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, rea"
K17-1013,W14-1618,0,0.0925589,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,P06-1038,1,0.647241,"sequential position of each context word. Given the example from Fig. 1, POSIT with the window size 2 extracts the following contexts for discovers: Australian_-2, scientist_-1, stars_+2, with_+1. - DEPS-All: All dependency links without any context selection, extracted from dependency-parsed data with prepositional arc collapsing. - COORD: Coordination-based contexts are used as fast lightweight contexts for improved representations of adjectives and verbs (Schwartz et al., 2016). This is in fact the conjlr context bag, a subset of DEPS-All. - SP: Contexts based on symmetric patterns (SPs, (Davidov and Rappoport, 2006; Schwartz et al., 2015)). For example, if the word X and the word 9 10 Context Group Adj Verb Noun conjlr (A+N+V) obj (N+V) prep (N+V) amod (A+N) compound (N) adv (V) nummod (-) 0.415 -0.028 0.188 0.479 -0.124 0.197 -0.142 0.281 0.309 0.344 0.058 -0.019 0.342 -0.065 0.401 0.390 0.387 0.398 0.416 0.104 0.029 Table 1: 2-fold cross-validation results for an illustrative selection of individual context bags. Results are presented for the noun, verb and adjective subsets of SimLex-999. Values in parentheses denote the class-specific initial pools to which each context is selected based on its ρ sc"
K17-1013,de-marneffe-etal-2014-universal,0,0.0293418,"Missing"
K17-1013,W08-1301,0,0.125848,"Missing"
K17-1013,N15-1184,0,0.0343743,"searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scopre stelle con telescopio nmod amod Australian nsubj scientist dobj discovers case stars with telescope prep:with Figure 1: Ext"
K17-1013,C12-1059,0,0.0242772,"ed in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS configuration is outscored by SP, but the margin is negligible. We als"
K17-1013,D15-1242,0,0.0909931,"bs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case do"
K17-1013,Q15-1016,0,0.665137,"trate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, S"
K17-1013,N15-1142,0,0.0798687,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,D15-1161,0,0.115421,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,P15-1145,0,0.038685,"ore useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Sci"
K17-1013,P13-2109,0,0.0606688,"Missing"
K17-1013,P13-2017,0,0.0177128,"rch algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based configuration space can be developed for multiple languages based on the same design principles. Indeed, in this work we show that the optimal configurations for English translate to improved representations in two additional languages, German and Italian. And so, given a (UD-)parsed training corpus, for each target word w with modifiers m1 , . . . , mk and a head h, the word w is paired with context elements m1 _r1 , . . . , mk"
K17-1013,N15-1050,0,0.0247132,"its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as contexts for V and A si"
K17-1013,N16-1118,0,0.353877,"ill considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when l"
K17-1013,P14-2050,0,0.0802645,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,Q17-1022,1,0.874088,"Missing"
K17-1013,J07-2002,0,0.313892,"Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended"
K17-1013,D14-1162,0,0.0756258,"figurations across languages: these configurations improve the SGNS performance when trained with German or Italian corpora and evaluated on class-specific subsets of the multilingual SimLex-999 (Leviant and Reichart, 2015), without any language-specific tuning. 2 Related Work Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic"
K17-1013,petrov-etal-2012-universal,0,0.227836,"path of the best-scoring lower-level configuration even if its score is lower than that of its origin. As we do not observe any significant improvement with this variant, we opt for the faster and simpler one. 5 https://bitbucket.org/yoavgo/word2vecf 6 SGNS for all models was trained using stochastic gradient descent and standard settings: 15 negative samples, global learning rate: 0.025, subsampling rate: 1e − 4, 15 epochs. Universal Dependencies as Labels The adopted UD scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the universal POS tagset (Petrov et al., 2012). It is straightforward to “translate” previous annotation schemes to UD (de Marneffe et al., 2014). Providing a consistently annotated inventory of categories for similar syntactic constructions across languages, the UD scheme facilitates representation learning in languages other than English, as shown in (Vuli´c and Korhonen, 2016; Vuli´c, 2017). Individual Context Bags Standard post-parsing steps are performed in order to obtain an initial list of individual context bags for our algorithm: (1) Prepositional arcs are collapsed ((Levy and Goldberg, 2014a; Vuli´c and Korhonen, 2016), see Fig."
K17-1013,P93-1034,0,0.74216,"Missing"
K17-1013,K15-1026,1,0.948135,"for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In thi"
K17-1013,N16-1060,1,0.312769,"). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In this work, we propose a simple yet effective framework for selecting context configurations, which yields improved representations for verbs, adjectives, and nouns. We s"
K17-1013,N03-1033,0,0.0161394,"ining Setup We first test whether the context configurations learned in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS co"
K17-1013,P10-1040,0,0.0566914,"ning time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not"
K17-1013,Q14-1020,0,0.0199341,"ified by similar adjectives”. In another example, “two verbs are similar if they are used as predicates of similar nominal subjects” (the nsubj and nsubjpass dependency relations). First, we have to define an expressive context configuration space that contains potential training configurations and is effectively decomposed so that useful configurations may be sought algorithmically. We can then continue by designing a search algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based conf"
K17-1013,E17-2065,1,0.823839,"Missing"
K17-1013,P16-2084,1,0.761929,"Missing"
K17-1013,Q15-1025,0,0.0546541,"dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope n"
K17-1013,D12-1086,0,0.0288956,"epresent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as"
K17-1013,P14-2089,0,0.0430435,"ctures, a particular dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers"
N16-1060,D14-1034,1,0.809186,"djectives. A number of evaluation sets consisting of word pairs scored by humans for semantic relations (mostly association and similarity) are in use for VSM evaluation. These include: RG-65 (Rubenstein and Goodenough, 1965), MC-30 (Miller and Charles, 1991), WordSim353 (Finkelstein et al., 2001), MEN (Bruni 1 Coor ∪ CoorC = DepAll, Coor ∩ CoorC = ∅ et al., 2014) and SimLex999 (Hill et al., 2014).2 Nouns are dominant in almost all of these datasets. For example, RG-65, MC-30 and WordSim353 consist of noun pairs almost exclusively. A few datasets contain pairs of verbs (Yang and Powers, 2006; Baker et al., 2014). The MEN dataset, although dominated by nouns, also contains verbs and adjectives. Nonetheless, the human judgment scores in these datasets reflect relatedness between words. In contrast, the recent SimLex999 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the"
N16-1060,P14-1023,0,0.0791358,"word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extracted from plain text. 4 Experiments Model. We keep the VSM fixed throughout our experiments, changing only the context type. This methodology allows us to evaluate the impact of different contexts on the VSM performance, as context choice is the only modeling decision that changes across experimental conditions. Our VSM is the word2vec skip-gram model (w2vSG, Mikolov et al. (2013a)), which obtains state-ofthe-art results on a variety of NLP tasks (Baroni et al., 2014). We employ the word2vec toolkit.4 For all context types other than BOW we use the word2vec package of (Levy and Goldberg, 2014),5 which augments the standard word2vec toolkit with code that allows arbitrary context definition. Experimental Setup. We experiment with the verb pair (222 pairs) and adjective pair (111 pairs) portions of SimLex999 (Hill et al., 2014). We report the Spearman ρ correlation between the ranks derived from the scores of the evaluated models and the human scores provided in SimLex999.6 We train the w2v-SG model with five different context types: (a) BOW contexts (SG-BOW"
N16-1060,P06-1038,1,0.911956,"Missing"
N16-1060,P07-1030,1,0.829956,"Missing"
N16-1060,C10-2028,1,0.689434,"Missing"
N16-1060,P13-1174,0,0.0126494,"Missing"
N16-1060,C92-2082,0,0.0548764,"Missing"
N16-1060,J15-4004,1,0.897757,"Missing"
N16-1060,P08-1119,0,0.0717551,"Missing"
N16-1060,P14-2050,0,0.60595,"2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Me"
N16-1060,N15-1098,0,0.0104165,"n, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyponymy and entailment. Rubinstein et al. (2015) showed that while state-of-the-art embeddings are successful at capturing taxonomic information (e.g., cow is an animal), they are much less successful in capturing attributive properties (bananas are yellow). In (Schwartz et al., 2015), we showed that word embeddings are unable to distinguish between pairs of words with opposite meanings (antonyms, e.g., good/bad). In this paper we study the difficulties of bag-of-words based word embeddings in representing verb similarity."
N16-1060,N16-1118,0,0.0192828,"4), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyp"
N16-1060,N13-1090,0,0.150625,"pairs of verbs (Yang and Powers, 2006; Baker et al., 2014). The MEN dataset, although dominated by nouns, also contains verbs and adjectives. Nonetheless, the human judgment scores in these datasets reflect relatedness between words. In contrast, the recent SimLex999 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extract"
N16-1060,H05-1105,0,0.0834407,"Missing"
N16-1060,P06-1033,0,0.0393585,"n our corpus. Indeed, due to the significant overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP cont"
N16-1060,W09-3811,0,0.0137806,"based model of Schwartz et al. (2015), with (SRR15) and without (SRR15− ) its antonym detection method. The two rightmost columns present the run time of the w2v-SG models in minutes (Time) and the number of context instances used by the model (#Cont.).10 For each SimLex999 portion, the score of the best w2v-SG model across context types is highlighted in bold font. ated by the word2vec script.7 Models (b)-(d) require the dependency parse trees of the corpus as input. To generate these trees, we employ the Stanford POS Tagger (Toutanova et al., 2003)8 and the stack version of the MALT parser (Nivre et al., 2009).9 The SP contexts are generated using the SPs extracted by the DR 06 algorithm from our training corpus (see Section 3). For BOW contexts, we experiment with three window sizes (2, 5 and 10) and report the best results (window size of 2 across conditions). For dependency based contexts we follow the standard convention in the literature: we consider the immediate heads and modifiers of the represented word. All models are trained with 500 dimensions, the default value of the word2vec script. Other hyperparameters were also set to the default values of the code packages. Results. Table 1 prese"
N16-1060,J07-2002,0,0.121164,"9 dataset (Hill et al., 2014) contains word similarity scores for nouns (666 pairs), verbs (222 pairs) and adjectives (111 pairs). We use this dataset to study the effect of context type on VSM performance in a verb and adjective similarity prediction task. Context Type in Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recen"
N16-1060,D14-1162,0,0.116226,"Missing"
N16-1060,P15-2119,1,0.320289,"ect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2 500 For a comprehensive list see: wordvectors.org/ mantic information. Levy et al. (2015) showed that word embeddings do not capture semantic relations such as hyponymy and entailment. Rubinstein et al. (2015) showed that while state-of-the-art embeddings are successful at capturing taxonomic information (e.g., cow is an animal), they are much less successful in capturing attributive properties (bananas are yellow). In (Schwartz et al., 2015), we showed that word embeddings are unable to distinguish between pairs of words with opposite meanings (antonyms, e.g., good/bad). In this paper we study the difficulties of bag-of-words based word embeddings in representing verb similarity. 3 Symmetric Patterns (SPs) Lexico-syntactic patterns are templates of text that contain both words and wildcards (Hears"
N16-1060,P11-1067,1,0.763494,"due to the significant overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extract"
N16-1060,C12-1147,1,0.173989,"overlap between SPs and Coors, the former have been proposed as a simple model of the latter (Nakov and Hearst, 2005).3 Despite their tight connection, SPs sometimes fail to properly identify the components of Coors. For example, while SPs are instrumental in capturing shallow Coors, they fail in capturing coordination between phrases. Consider the sentence John 3 Note though that the exact syntactic annotation of coordination is debatable both in the linguistic community (Tesnière, 1959; Hudson, 1980; Mel’ˇcuk, 1988) and also in the NLP community (Nilsson et al., 2006; Schwartz et al., 2011; Schwartz et al., 2012). walked and Mary ran: the SP “X and Y” captures the phrase walked and Mary, while the Coor links the heads of the connected phrases (“walked” and “ran”). SPs, on the other hand, can go beyond Coors and capture other types of symmetric structures like “from X to Y” and “X rather than Y”. Our experiments reveal that both SPs and Coors are highly useful contexts for verb and adjective representation, at least with respect to word similarity. Interestingly, Coor contexts, extracted using a supervised dependency parser, are less effective than SP contexts, which are extracted from plain text. 4 Ex"
N16-1060,D13-1193,1,0.55537,"Missing"
N16-1060,C14-1153,1,0.901785,"Missing"
N16-1060,K15-1026,1,0.754717,"ts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which we find to be most useful for predicting word similarity. Limitations of Word Embeddings. Recently, a few papers examined the limitations of word embedding models in representing different types of se2"
N16-1060,N03-1033,0,0.0501082,"s (see text). The bottom lines present the results of the count SP-based model of Schwartz et al. (2015), with (SRR15) and without (SRR15− ) its antonym detection method. The two rightmost columns present the run time of the w2v-SG models in minutes (Time) and the number of context instances used by the model (#Cont.).10 For each SimLex999 portion, the score of the best w2v-SG model across context types is highlighted in bold font. ated by the word2vec script.7 Models (b)-(d) require the dependency parse trees of the corpus as input. To generate these trees, we employ the Stanford POS Tagger (Toutanova et al., 2003)8 and the stack version of the MALT parser (Nivre et al., 2009).9 The SP contexts are generated using the SPs extracted by the DR 06 algorithm from our training corpus (see Section 3). For BOW contexts, we experiment with three window sizes (2, 5 and 10) and report the best results (window size of 2 across conditions). For dependency based contexts we follow the standard convention in the literature: we consider the immediate heads and modifiers of the represented word. All models are trained with 500 dimensions, the default value of the word2vec script. Other hyperparameters were also set to"
N16-1060,J06-3003,0,0.0359954,"Word Embeddings. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination"
N16-1060,C08-1114,0,0.0254482,"gs. Most VSMs (e.g., (Collobert et al., 2011; Mikolov et al., 2013b; Pennington et al., 2014)) define the context of a target word to be the words in its physical proximity (bag-of-words contexts). Dependency contexts, consisting of the words connected to the target word by dependency links (Grefenstette, 1994; Padó and Lapata, 2007; Levy and Goldberg, 2014), are another well researched alternative. These works did not recognize the importance of syntactic coordination contexts (Coor). Patterns have also been suggested as VSM contexts, but mostly for representing pairs of words (Turney, 2006; Turney, 2008). While this approach has been successful for extracting various types of word relations, using patterns to represent single words is useful for downstream applications. Recently, Schwartz et al. (2015) explored the value of symmetric pattern contexts for word representation, an idea this paper develops further. A recently published approach (Melamud et al., 2016) also explored the effect of the type of context on the performance of word embedding models. Nonetheless, while they also explored bag-of-words and dependency contexts, they did not experiment with SPs or coordination contexts, which"
N16-1060,C02-1114,0,0.196931,"Missing"
N18-1149,D14-1162,0,0.0903683,"Missing"
N18-1149,D13-1181,0,\N,Missing
N18-1149,N18-1022,1,\N,Missing
N18-2017,D15-1075,1,0.69331,". This suggests that, despite recently reported progress, natural language inference remains an open problem. Introduction Natural language inference (NLI; also known as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced (Dagan et al., 2006). Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h. The problem of acquiring large amounts of labeled inference data was addressed by Bowman et al. (2015), who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI (Williams et al., 2018) datasets. In this process, crowd workers are presented with F h is definitely true given p h might be true given p h is definitely not true given p 2 Annotation Artifacts are Common We conjecture that the framing of the annotation task has a significant effect on the language generation choices that crowd workers make when authoring hypotheses, producing certain patterns in the data. We call these patterns annotation artifacts. T"
N18-2017,P17-2097,0,0.0400512,"ICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across multiple NLI datasets. Indeed, annotation artifacts are not unique to the NLI datasets, and the danger of such biases should be carefully considered wh"
N18-2017,P16-1223,0,0.0889145,"Missing"
N18-2017,N18-2123,0,0.0357975,"Missing"
N18-2017,D17-1070,0,0.0753495,"Missing"
N18-2017,N16-1098,0,0.0292386,"yMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across multiple NLI datasets. Indeed, annotation artifacts are not unique to the NLI datasets, and the danger of such biases should be carefully considered when annotating new datasets. to use them for evaluating NLI models (in addition to the original benc"
N18-2017,D16-1244,0,0.193867,"Missing"
N18-2017,S18-2023,0,0.153649,"Missing"
N18-2017,D16-1264,0,0.144407,"Missing"
N18-2017,W17-1609,0,0.0464368,"Missing"
N18-2017,D17-1215,0,0.24022,"Missing"
N18-2017,K17-1004,1,0.376046,"f discourse markers such as because. Once again, we observe that the example from the SNLI annotation guidelines does just that, by adding the purpose clause to catch a stick (Table 3). Contradiction. Negation words such as nobody, no, never and nothing are strong indicators of contradiction.3 Other (non-negative) words appear to be part of heuristics for contradicting whatever information is displayed in the premise; sleeping contradicts any activity, and naked (further down the list) contradicts any description of clothing. 3 Similar findings were observed in the ROC story cloze annotation (Schwartz et al., 2017). 109 Model DAM ESIM DIIN Full SNLI Hard Easy MultiNLI Matched Full Hard Easy MultiNLI Mismatched Full Hard Easy 84.7 85.8 86.5 69.4 71.3 72.7 72.0 74.1 77.0 72.1 73.1 76.5 92.4 92.6 93.4 55.8 59.3 64.1 85.3 86.2 87.6 56.2 58.9 64.4 85.7 85.2 86.8 Table 5: Performance of high-performing NLI models on the full, Hard, and Easy NLI test sets. 4 Re-evaluating NLI Models not be as straightforward to eliminate annotation artifacts once the dataset has been collected. First, after removing the Easy examples, Hard examples might not necessarily be artifact-free. For instance, removing all contradictin"
N18-2017,E17-2068,0,0.0173405,"9 35.2 52.3 Table 2: Performance of a premise-oblivious text classifier on NLI. The MultiNLI benchmark contains two test sets: matched (in-domain examples) and mismatched (out-of-domain examples). A majority baseline is presented for reference. 3.1 To see whether the use of certain words is indicative of the inference class, we compute the pointwise mutual information (PMI) between each word and class in the training set: To determine the degree to which such artifacts exist, we train a model to predict the label of a given hypothesis without seeing the premise. Specifically, we use fastText (Joulin et al., 2017), an off-the-shelf text classifier that models text as a bag of words and bigrams, to predict the entailment label of the hypothesis.1 This classifier is completely oblivious to the premise. Table 2 shows that a significant portion of each test set can be correctly classified without looking at the premise, well beyond the most-frequentclass baseline.2 Our finding demonstrates that it is possible to perform well on these datasets without modeling natural language inference. 3 Lexical Choice PMI(word, class) = log p(word, class) p(word, ·)p(·, class) We apply add-100 smoothing to the raw statis"
N18-2017,N18-1101,1,0.645347,"wn as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced (Dagan et al., 2006). Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h. The problem of acquiring large amounts of labeled inference data was addressed by Bowman et al. (2015), who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI (Williams et al., 2018) datasets. In this process, crowd workers are presented with F h is definitely true given p h might be true given p h is definitely not true given p 2 Annotation Artifacts are Common We conjecture that the framing of the annotation task has a significant effect on the language generation choices that crowd workers make when authoring hypotheses, producing certain patterns in the data. We call these patterns annotation artifacts. These authors contributed equally to this work. 107 Proceedings of NAACL-HLT 2018, pages 107–112 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Compu"
N18-2017,S14-2055,0,0.0249089,"that such models rely heavily on annotation artifacts in the hypothesis to make their predictions. A natural question to ask is whether it is possible to select a set of NLI training and test samples which do not contain easy-to-exploit artifacts. One solution might be to filter Easy examples from the training set, retaining only Hard examples. However, initial experiments suggest that it might 5 Discussion We reflect on our results and relate them to other work that also analyzes annotation artifacts in NLP datasets, drawing three main conclusions. Many datasets contain annotation artifacts. Lai and Hockenmaier (2014) demonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark"
N18-2017,P12-2031,0,0.0251786,"emonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across"
N18-2017,P16-2041,1,0.826417,"atasets contain annotation artifacts. Lai and Hockenmaier (2014) demonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this wo"
N18-2017,N15-1098,1,0.787039,"lopment of additional challenging benchmarks that expose the true performance levels of state-of-the-art NLI models. Acknowledgments This research was supported in part by the DARPA CwC program through ARO (W911NF15-1-0543) and a hardware gift from NVIDIA Corporation. SB acknowledges gift support from Google and Tencent Holdings and support from Samsung Research. References Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016. Analyzing the behavior of visual question answering models. In Proc. of EMNLP. https: //aclweb.org/anthology/D16-1203. Supervised models leverage annotation artifacts. Levy et al. (2015) demonstrated that supervised lexical inference models rely heavily on artifacts in the datasets, particularly the tendency of some words to serve as prototypical hypernyms. Agrawal et al. (2016); Jabri et al. (2016); Goyal et al. (2017) all showed that state-of-the-art visual question answering (Antol et al., 2015) systems leverage annotation biases in the dataset. Cirik et al. (2018) find that complex models for referring expression recognition achieve high performance without any text input. In parallel to this work, Dasgupta et al. (2018) found that the InferSent model (Conneau et al., 201"
N18-2017,D16-1203,0,\N,Missing
N18-2017,P18-1224,0,\N,Missing
N19-1225,D18-1316,0,0.0245864,"tandard dataset (e.g., SQuAD) and “Challenge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. Introduction NLP research progresses through the construction of dataset-benchmarks and the development of systems whose performance on them can be fairly compared. A recent pattern involves challenges to benchmarks:1 manipulations to input data that result in severe degradation of system performance, but not human performance. These challenges have been used as evidence that current systems are brittle (Belinkov and Bisk, 2018; Mudrakarta et al., 2018; Zhao et al., 2018; Glockner et al., 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018, 1 inter alia). For instance, Naik et al. (2018) generated natural language inference challenge data by applying simple textual transformations to existing examples from MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). Similarly, Jia and Liang (2017) built an adversarial evaluation dataset for reading comprehension based on SQuAD (Rajpurkar et al., 2016). What should we conclude when a system fails on a challenge dataset? In some cases, a challenge might exploit blind spots in the design of the original datase"
N19-1225,P18-2006,0,0.0150295,"ge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. Introduction NLP research progresses through the construction of dataset-benchmarks and the development of systems whose performance on them can be fairly compared. A recent pattern involves challenges to benchmarks:1 manipulations to input data that result in severe degradation of system performance, but not human performance. These challenges have been used as evidence that current systems are brittle (Belinkov and Bisk, 2018; Mudrakarta et al., 2018; Zhao et al., 2018; Glockner et al., 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018, 1 inter alia). For instance, Naik et al. (2018) generated natural language inference challenge data by applying simple textual transformations to existing examples from MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). Similarly, Jia and Liang (2017) built an adversarial evaluation dataset for reading comprehension based on SQuAD (Rajpurkar et al., 2016). What should we conclude when a system fails on a challenge dataset? In some cases, a challenge might exploit blind spots in the design of the original dataset (dataset weakness). In others, the challenge"
N19-1225,W18-2501,1,0.884509,"Missing"
N19-1225,P18-2103,0,0.183604,"g., SQuAD) and “Challenge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. Introduction NLP research progresses through the construction of dataset-benchmarks and the development of systems whose performance on them can be fairly compared. A recent pattern involves challenges to benchmarks:1 manipulations to input data that result in severe degradation of system performance, but not human performance. These challenges have been used as evidence that current systems are brittle (Belinkov and Bisk, 2018; Mudrakarta et al., 2018; Zhao et al., 2018; Glockner et al., 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018, 1 inter alia). For instance, Naik et al. (2018) generated natural language inference challenge data by applying simple textual transformations to existing examples from MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). Similarly, Jia and Liang (2017) built an adversarial evaluation dataset for reading comprehension based on SQuAD (Rajpurkar et al., 2016). What should we conclude when a system fails on a challenge dataset? In some cases, a challenge might exploit blind spots in the design of the original dataset (dataset weakness). I"
N19-1225,N18-2017,1,0.895702,"Missing"
N19-1225,D17-1215,0,0.436908,"hat particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model’s specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI “stress tests” (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves. 1 Figure 1: An illustration of the standard challenge evaluation procedure (e.g., Jia and Liang, 2017) and our proposed analysis method. “Original” refers to the a standard dataset (e.g., SQuAD) and “Challenge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. Introd"
N19-1225,C18-1198,0,0.180841,"Missing"
N19-1225,D15-1075,0,\N,Missing
N19-1225,D16-1244,0,\N,Missing
N19-1225,P17-1152,0,\N,Missing
N19-1225,P18-1079,0,\N,Missing
N19-1225,N18-1101,0,\N,Missing
P11-1067,N10-1083,0,0.0100955,"Missing"
P11-1067,P10-1131,0,0.00560222,"ing logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-super"
P11-1067,W04-1501,0,0.0673155,"in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex"
P11-1067,D10-1117,0,0.186649,"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 2004; Cohen et al., 2008; Headden et al., 2009; Spitkovsky et al., 2010a; Gillenwater et al., 2010; Berg-Kirkpatrick et al., 2010; Blunsom and Cohn, 2010, inter alia). Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated. However, there are cases in which there is no linguistic consensus as to what the correct annotation is (K¨ubler et al., 2009). Examples include which verb is the head in a verb group structure (e.g., “can” or “eat” in “can eat”), and which ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguis"
P11-1067,N09-1009,0,0.0289598,"blematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of researc"
P11-1067,P09-1041,0,0.00977741,"re. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent sta"
P11-1067,P10-2036,0,0.0356408,"Missing"
P11-1067,C08-1042,0,0.0740417,"Missing"
P11-1067,N09-1012,0,0.26345,"Missing"
P11-1067,W07-2416,0,0.0665149,"note that these controversies are reflected in the evaluation of this task, resulting in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s sc"
P11-1067,P04-1061,0,0.712703,"lizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substa"
P11-1067,J93-2004,0,0.0435294,"Missing"
P11-1067,D10-1120,0,0.0191741,"Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and ve"
P11-1067,nivre-etal-2006-maltparser,0,0.00840752,"applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, w3 w2 (a) w1 w3 w2 w1 (b) Figure 1: A dependency structure on the words w1 , w2 , w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-flip of w2 →w1 . thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (2005) compared two different conversion schemes in German supervised constituency parsing and found one to have positive influence on parsing quality. Dependen"
P11-1067,P06-1033,0,0.363717,"iner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, w3 w2 (a) w1 w3 w2 w1 (b) Figure 1: A dependency structure on the words w1 , w2 , w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-flip of w2 →w1 . thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (200"
P11-1067,rambow-etal-2002-dependency,0,0.0689242,", if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). 667 Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possib"
P11-1067,P06-1072,0,0.0773069,"Missing"
P11-1067,N10-1116,0,0.195398,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,P10-1130,0,0.232129,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,W10-2902,0,0.312419,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,W05-1516,0,0.038097,"Missing"
P11-1067,W06-2904,0,0.0476608,"Missing"
P11-1067,W03-3023,0,0.776919,"”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). 667 Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possible, since different papers use different gold standard annotations even when they are all deriv"
P11-1067,J03-4003,0,\N,Missing
P11-1067,D07-1112,0,\N,Missing
P11-1067,D07-1096,0,\N,Missing
P15-2119,P14-2050,0,0.0210063,"nt is considered large) . We model this task as a learning problem, in which concepts have a feature representation based on a state-of-the-art DM. A property-predictor is then trained to predict, for any given concept, whether the property applies to it or not (in a binary classification setup), or the strength of affiliation between the property and the concept (in a regression setup). By evaluating the performance of these predictors, we assess the degree to which the property is captured by the DM. We experiment with four state-of-the-art DMs (Baroni and Lenci, 2010; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). Our results show that all DMs, quite successful in many semantic tasks, fail when it comes to predicting attributive properties of concepts. For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties. This result, which may be attributed to an essential difference between taxonomic and attributive properties, demonstrates possible limitations of the distributional hypothesis, at least in terms of the information captured by c"
P15-2119,J10-4006,0,0.77946,"r not (e.g., whether or not the concept elephant is considered large) . We model this task as a learning problem, in which concepts have a feature representation based on a state-of-the-art DM. A property-predictor is then trained to predict, for any given concept, whether the property applies to it or not (in a binary classification setup), or the strength of affiliation between the property and the concept (in a regression setup). By evaluating the performance of these predictors, we assess the degree to which the property is captured by the DM. We experiment with four state-of-the-art DMs (Baroni and Lenci, 2010; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). Our results show that all DMs, quite successful in many semantic tasks, fail when it comes to predicting attributive properties of concepts. For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties. This result, which may be attributed to an essential difference between taxonomic and attributive properties, demonstrates possible limitations of the distributional hypothesis, at"
P15-2119,D14-1162,0,0.0774181,"We model this task as a learning problem, in which concepts have a feature representation based on a state-of-the-art DM. A property-predictor is then trained to predict, for any given concept, whether the property applies to it or not (in a binary classification setup), or the strength of affiliation between the property and the concept (in a regression setup). By evaluating the performance of these predictors, we assess the degree to which the property is captured by the DM. We experiment with four state-of-the-art DMs (Baroni and Lenci, 2010; Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). Our results show that all DMs, quite successful in many semantic tasks, fail when it comes to predicting attributive properties of concepts. For example, in the classification task, the best performing DM achieves an averaged F-score of only 0.37, contrasted with an average F-score of 0.73 achieved by the same model for taxonomic properties. This result, which may be attributed to an essential difference between taxonomic and attributive properties, demonstrates possible limitations of the distributional hypothesis, at least in terms of the information captured by current stateof-the-art DMs"
P15-2119,P14-1023,0,0.294874,"nal hypothesis may not be equally applicable to all types of semantic information. 1 Introduction The Distributional Hypothesis states that the meaning of words can be inferred from their linguistic environment (Harris, 1954). This hypothesis lies at the heart of distributional models (DMs), which approximate the meaning of words by considering the statistics of their co-occurrence with other words in the lexicon. DMs have shown impressive results in many semantic tasks, such as predicting the similarity of two words, grouping words into semantic categories, and solving analogy questions (see Baroni et al. (2014) for a recent survey). They are also used as a source of semantic information by many downstream applications, including syntactic parsing (Socher et al., 2013), image annotation (Klein et al., 2014), and semantic frame identification (Hermann et al., 2014). 726 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 726–730, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Binary Classification. For each property p, we take concepts"
P15-2119,C14-1153,1,0.884029,"Missing"
P15-2119,P13-1045,0,0.0198942,"can be inferred from their linguistic environment (Harris, 1954). This hypothesis lies at the heart of distributional models (DMs), which approximate the meaning of words by considering the statistics of their co-occurrence with other words in the lexicon. DMs have shown impressive results in many semantic tasks, such as predicting the similarity of two words, grouping words into semantic categories, and solving analogy questions (see Baroni et al. (2014) for a recent survey). They are also used as a source of semantic information by many downstream applications, including syntactic parsing (Socher et al., 2013), image annotation (Klein et al., 2014), and semantic frame identification (Hermann et al., 2014). 726 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 726–730, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Binary Classification. For each property p, we take concepts for which p applies to be positive instances, and concepts for which it does not as negative instances. For example, the property is loud is positive for a trum"
P15-2119,J15-4004,0,\N,Missing
P15-2119,P14-1136,0,\N,Missing
P18-1028,N18-1205,0,0.0616202,"e variants, the long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU; Cho et al., 2014), have become ubiquitous in NLP algorithms (Goldberg, 2016). Recently, several works introduced simpler versions of RNNs, such as recurrent additive networks (Lee et al., 2017) and Quasi-RNNs (Bradbury et al., 2017). Like SoPa, these models can be seen as points along the bridge between RNNs and CNNs. Other works have studied the expressive power of RNNs, in particular in the context of WFSAs or HMMs (Cleeremans et al., 1989; Giles et al., 1992; Visser et al., 2001; Chen et al., 2018). In this work we relate CNNs to WFSAs, showing that a one-layer CNN with max-pooling can be simulated by a collection of linear-chain WFSAs. Convolutional neural networks. CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting"
P18-1028,Q15-1031,0,0.0610826,"the resulting score and the original model score is considered p’s contribution. We then consider the highest contributing patterns, and attach each one with its highest scoring phrase in that document. Table 3 shows example texts along with their most positive and negative contributing phrases. 8 Related Work Weighted finite-state automata. WFSAs and hidden Markov models19 were once popular in automatic speech recognition (Hetherington, 2004; Moore et al., 2006; Hoffmeister et al., 2012) 19 HMMs are a special case of WFSAs (Mohri et al., 2002). and remain popular in morphology (Dreyer, 2011; Cotterell et al., 2015). Most closely related to this work, neural networks have been combined with weighted finite-state transducers to do morphological reinflection (Rastogi et al., 2016). These prior works learn a single FSA or FST, whereas our model learns a collection of simple but complementary FSAs, together encoding a sequence. We are the first to incorporate neural networks both before WFSAs (in their transition scoring functions), and after (in the function that turns their vector of scores into a final prediction), to produce an expressive model that remains interpretable. Recurrent neural networks. The a"
P18-1028,P08-1079,0,0.0278546,"–6 times as many parameters. Figure 3 shows a comparison of all models on the SST and Amazon datasets with varying training set sizes. SoPa is substantially outperforming all baselines, in particular BiLSTM, on small datasets (100 samples). This suggests that SoPa is better fit to learn from small datasets. Ablation analysis. Table 1 also shows an ablation of the differences between SoPa and CNN: max-product semiring with sigmoid vs. max-sum semiring with identity, self-loops, and ✏-transitions. The last line is equivalent to a CNN with 16 Some words may serve as both words and wildcards. See Davidov and Rappoport (2008) for discussion. 17 The number of patterns and their length are hyperparameters tuned on the development data (see Appendix A). 300 Model ROC SST Amazon Highest Scoring Phrases Hard DAN BiLSTM CNN 62.2 (4K) 64.3 (91K) 65.2 (844K) 64.3 (155K) SoPa 66.5 (255K) 85.6 (255K) 90.5 (256K) SoPams1 SoPams1 {sl} SoPams1 {✏} SoPams1 {sl, ✏} 64.4 63.2 64.3 64.0 75.5 (6K) 83.1 (91K) 84.8 (1.5M) 82.2 (62K) 84.8 84.6 83.6 85.0 88.5 (67K) 85.4 (91K) 90.8 (844K) 90.2 (305K) 90.0 89.8 89.7 89.5 Classification Accuracy Table 1: Test classification accuracy (and the number of parameters used). The bottom part"
P18-1028,C10-2028,0,0.0574624,"le method to interpret SoPa (Section 7). This method applies equally well to CNNs. We release our code at https://github.com/ Noahs-ARK/soft_patterns. 2 Background Surface patterns. Patterns (Hearst, 1992) are particularly useful tool in NLP (Lin et al., 2003; Etzioni et al., 2005; Schwartz et al., 2015). The most basic definition of a pattern is a sequence of words and wildcards (e.g., “X is a Y”), which can either be manually defined or extracted from a corpus using cooccurrence statistics. Patterns can then be matched against a specific text span by replacing wildcards with concrete words. Davidov et al. (2010) introduced a flexible notion of patterns, which supports partial matching of the pattern with a given text by skipping some of the words in the pattern, or introducing new words. In their framework, when a sequence of text partially matches a pattern, hard-coded partial scores are assigned to the pattern match. Here, we represent patterns as WFSAs with neural weights, and support these partial matches in a soft manner. WFSAs. We review weighted finite-state automata with ✏-transitions before we move on to our special case in Section 3. A WFSA-✏ with d states over a vocabulary V is formally de"
P18-1028,D14-1181,0,0.0420492,"iting this theoretical restriction is in practice, especially when SoPa is used as a component in a larger network. We defer the investigation of the exact computational properties of SoPa to future work. In the next section, we show that SoPa is an extension of a one-layer CNN, and hence more expressive. 4 SoPa as a CNN Extension A convolutional neural network (CNN; LeCun, 1998) moves a fixed-size sliding window over the document, producing a vector representation for each window. These representations are then often summed, averaged, or max-pooled to produce a document-level representation (Kim, 2014; Yin and Sch¨utze, 2015). In this section, we show that SoPa is an extension of one-layer, max-pooled CNNs. To recover a CNN from a soft pattern with d + 1 states, we first remove self-loops and ✏-transitions, 9 Rational series generalize recognizers of regular languages, which are the special case of the Boolean semiring. 298 max-pooled END states pattern1 states START states pattern2 states word vectors Fielding’s funniest and most likeable book in years Figure 2: State activations of two patterns as they score a document. pattern1 (length three) matches on “in years”. pattern2 (length five"
P18-1028,P02-1001,0,0.0973942,"ontributed equally. funny, magical great a What START 1 2 X 3 ! 4 END ✏ Figure 1: A representation of a surface pattern as a six-state automaton. Self-loops allow for repeatedly inserting words (e.g., “funny”). ✏-transitions allow for dropping words (e.g., “a”). is able to capture a soft notion of surface patterns (e.g., “what a great X !”; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words (see Figure 1). From a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms (Mohri, 1997; Eisner, 2002) that SoPa can take advantage of. SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA (Section 3). While the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end. SoPa then represents a document with a vector that is the aggregate of the scores computed by matching each of the patterns with each span in the document. Because SoPa defines a hidden state that depends on the input token and the previous state, it can be thought of as a simple type of RNN. We show that SoPa is an extension of a onelayer"
P18-1028,W16-5901,0,0.0193662,"nt-level score. 3 SoPa: A Weighted Finite-State Automaton RNN We introduce SoPa, a WFSA-based RNN, which is designed to represent text as collection of surface pattern occurrences. We start by showing how a single pattern can be represented as a WFSA-✏ (Section 3.1). Then we describe how to score a complete document using a pattern (Section 3.2), and how multiple patterns can be used to encode a document (Section 3.3). Finally, we show that SoPa can be seen as a simple variant of an RNN (Section 3.4). 1 The semiring parsing view (Goodman, 1999) has produced unexpected connections in the past (Eisner, 2016). We experiment with max-product and max-sum semirings, but note that our model could be easily updated to use any semiring. 2 In our case, we also use a sparse transition matrix (Section 3.1), which further reduces our runtime to O(dn). 296 3.1 Patterns as WFSAs We describe how a pattern can be represented as a WFSA-✏. We first assume a single pattern. A pattern is a WFSA-✏, but we impose hard constraints on its shape, and its transition weights are given by differentiable functions that have the power to capture concrete words, wildcards, and everything in between. Our model is designed to b"
P18-1028,J99-4004,0,0.124536,"tion 3.2 how phrase-level scores can be summarized into a document-level score. 3 SoPa: A Weighted Finite-State Automaton RNN We introduce SoPa, a WFSA-based RNN, which is designed to represent text as collection of surface pattern occurrences. We start by showing how a single pattern can be represented as a WFSA-✏ (Section 3.1). Then we describe how to score a complete document using a pattern (Section 3.2), and how multiple patterns can be used to encode a document (Section 3.3). Finally, we show that SoPa can be seen as a simple variant of an RNN (Section 3.4). 1 The semiring parsing view (Goodman, 1999) has produced unexpected connections in the past (Eisner, 2016). We experiment with max-product and max-sum semirings, but note that our model could be easily updated to use any semiring. 2 In our case, we also use a sparse transition matrix (Section 3.1), which further reduces our runtime to O(dn). 296 3.1 Patterns as WFSAs We describe how a pattern can be represented as a WFSA-✏. We first assume a single pattern. A pattern is a WFSA-✏, but we impose hard constraints on its shape, and its transition weights are given by differentiable functions that have the power to capture concrete words, w"
P18-1028,C92-2082,0,0.352422,"Ns and CNNs, presenting SoPa (for Soft Patterns), a model that lies in between them. SoPa is a neural version of a weighted finitestate automaton (WFSA), with a restricted set of transitions. Linguistically, SoPa is appealing as it ⇤ The first two authors contributed equally. funny, magical great a What START 1 2 X 3 ! 4 END ✏ Figure 1: A representation of a surface pattern as a six-state automaton. Self-loops allow for repeatedly inserting words (e.g., “funny”). ✏-transitions allow for dropping words (e.g., “a”). is able to capture a soft notion of surface patterns (e.g., “what a great X !”; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words (see Figure 1). From a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms (Mohri, 1997; Eisner, 2002) that SoPa can take advantage of. SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA (Section 3). While the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end. SoPa then represents a document with a vector that is the aggregate of the scores co"
P18-1028,P15-1162,0,0.0497016,"Missing"
P18-1028,D15-1180,0,0.0555662,"WFSAs, showing that a one-layer CNN with max-pooling can be simulated by a collection of linear-chain WFSAs. Convolutional neural networks. CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016). Neural networks and patterns. Some works used patterns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used"
P18-1028,D16-1011,0,0.0215851,"The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features. Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016). Li et al. (2016) and K´ad´ar et al. (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions. Finally, several works presented methods to visualize deep CNNs (Zeiler and Fergus, 2014; Simonyan et al., 2014; Yosinski et al., 2015), focusing on visualizing the different layers of the network, mainly in the context of image and video understanding. We believe these two types of research approaches are complementary: inventing general purpose visualization tools for existing black-box models on the one hand, and on the other, designi"
P18-1028,J97-2003,0,0.355747,"two authors contributed equally. funny, magical great a What START 1 2 X 3 ! 4 END ✏ Figure 1: A representation of a surface pattern as a six-state automaton. Self-loops allow for repeatedly inserting words (e.g., “funny”). ✏-transitions allow for dropping words (e.g., “a”). is able to capture a soft notion of surface patterns (e.g., “what a great X !”; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words (see Figure 1). From a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms (Mohri, 1997; Eisner, 2002) that SoPa can take advantage of. SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA (Section 3). While the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end. SoPa then represents a document with a vector that is the aggregate of the scores computed by matching each of the patterns with each span in the document. Because SoPa defines a hidden state that depends on the input token and the previous state, it can be thought of as a simple type of RNN. We show that SoPa is an extensio"
P18-1028,N16-1098,0,0.0681027,"Missing"
P18-1028,D16-1085,0,0.0167498,"t a one-layer CNN with max-pooling can be simulated by a collection of linear-chain WFSAs. Convolutional neural networks. CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016). Neural networks and patterns. Some works used patterns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that a"
P18-1028,D14-1162,0,0.0887451,"vx + bi ), if j = i + 1 &gt; : 0, otherwise, (3) where ui and wi are vectors of parameters, ai and bi are scalar parameters, vx is a fixed pre-trained word vector for x,4 and E is an encoding function, typically the identity function or sigmoid. ✏-transitions are also parameterized, but don’t consume a token and depend only on the current state: ( E(ci ), if j = i + 1 [T(✏)]i,j = (4) 0, otherwise, where ci is a scalar parameter.5 As we have only 3 To ensure that we start in the START state and end in the END state, we fix ⇡ = [1, 0, . . . , 0] and ⌘ = [0, . . . , 0, 1]. 4 We use GloVe 300d 840B (Pennington et al., 2014). 5 Adding ✏-transitions to WFSAs does not increase their three non-zero diagonals in total, the matrix multiplications in Equation 2 can be implemented using vector operations, and the overall runtimes of Forward and Viterbi are reduced to O(dn).6 Words vs. wildcards. Traditional hard patterns distinguish between words and wildcards. Our model does not explicitly capture the notion of either, but the transition weight function can be interpreted in those terms. Each transition is a logistic regression over the next word vector vx . For example, for a main path out of state i, T has two parame"
P18-1028,N16-1076,0,0.123297,"t scoring phrase in that document. Table 3 shows example texts along with their most positive and negative contributing phrases. 8 Related Work Weighted finite-state automata. WFSAs and hidden Markov models19 were once popular in automatic speech recognition (Hetherington, 2004; Moore et al., 2006; Hoffmeister et al., 2012) 19 HMMs are a special case of WFSAs (Mohri et al., 2002). and remain popular in morphology (Dreyer, 2011; Cotterell et al., 2015). Most closely related to this work, neural networks have been combined with weighted finite-state transducers to do morphological reinflection (Rastogi et al., 2016). These prior works learn a single FSA or FST, whereas our model learns a collection of simple but complementary FSAs, together encoding a sequence. We are the first to incorporate neural networks both before WFSAs (in their transition scoring functions), and after (in the function that turns their vector of scores into a final prediction), to produce an expressive model that remains interpretable. Recurrent neural networks. The ability of RNNs to represent arbitrarily long sequences of embedded tokens has made them attractive to NLP researchers. The most notable variants, the long short-term"
P18-1028,D13-1170,0,0.00747955,"Pa is restricted to operations that follow the semiring laws.11 As a model that is more flexible than a one-layer CNN, but (arguably) less expressive than many RNNs, SoPa lies somewhere on the continuum between these two approaches. Continuing to study the bridge between CNNs and RNNs is an exciting direction for future research. 5 Experiments To evaluate SoPa, we apply it to text classification tasks. Below we describe our datasets and baselines. More details can be found in Appendix A. Datasets. We experiment with three binary classification datasets. • SST. The Stanford Sentiment Treebank (Socher et al., 2013)12 contains roughly 10K movie reviews from Rotten Tomatoes,13 labeled on a scale of 1–5. We consider the binary task, which considers 1 and 2 as negative, and 4 and 5 as positive (ignoring 3s). It is worth noting that this dataset also contains syntactic phrase level annotations, providing a sentiment label to parts of 11 The max-sum semiring corresponds to a linear filter with max-pooling. Other semirings could potentially model more interesting interactions, but we leave this to future work. 12 https://nlp.stanford.edu/sentiment/ index.html 13 http://www.rottentomatoes.com 299 sentences. In"
P18-1028,D15-1243,0,0.016976,"large magnitude and is close to the word vector for some word y (e.g., wi ⇡ 100vy ), and bi is a large negative bias (e.g., bi ⇡ 100), then the transition is essentially matching the specific word y. Whereas if wi has small magnitude (wi ⇡ 0) and bi is a large positive bias (e.g., bi ⇡ 100), then the transition is ignoring the current token and matching a wildcard.7 The transition could also be something in between, for instance by focusing on specific dimensions of a word’s meaning encoded in the vector, such as POS or semantic features like animacy or concreteness (Rubinstein et al., 2015; Tsvetkov et al., 2015). 3.2 Scoring Documents So far we described how to calculate how well a pattern matches a token span exactly (consuming the whole span). To score a complete document, we prefer a score that aggregates over all matches on subspans of the document (similar to “search” instead of “match” in regular expression parlance). We still assume a single pattern. Either the Forward algorithm can be used to calculate the Pexpected count of the pattern in the document, 1ijn pspan (xi:j ), or Viterbi to calculate sdoc (x) = max1ijn sspan (xi:j ), the score of the highest-scoring match. In short document"
P18-1028,N16-3020,0,0.0173392,"terns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features. Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016). Li et al. (2016) and K´ad´ar et al. (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions. Fi"
P18-1028,P15-2119,1,0.853947,"rs, wi and bi . If wi has large magnitude and is close to the word vector for some word y (e.g., wi ⇡ 100vy ), and bi is a large negative bias (e.g., bi ⇡ 100), then the transition is essentially matching the specific word y. Whereas if wi has small magnitude (wi ⇡ 0) and bi is a large positive bias (e.g., bi ⇡ 100), then the transition is ignoring the current token and matching a wildcard.7 The transition could also be something in between, for instance by focusing on specific dimensions of a word’s meaning encoded in the vector, such as POS or semantic features like animacy or concreteness (Rubinstein et al., 2015; Tsvetkov et al., 2015). 3.2 Scoring Documents So far we described how to calculate how well a pattern matches a token span exactly (consuming the whole span). To score a complete document, we prefer a score that aggregates over all matches on subspans of the document (similar to “search” instead of “match” in regular expression parlance). We still assume a single pattern. Either the Forward algorithm can be used to calculate the Pexpected count of the pattern in the document, 1ijn pspan (xi:j ), or Viterbi to calculate sdoc (x) = max1ijn sspan (xi:j ), the score of the highest-scoring"
P18-1028,K15-1026,1,0.84818,"tional LSTM and a CNN. Our model performs on par with or better than all baselines on all tasks (Section 6). Moreover, when training with smaller datasets, SoPa is particularly useful, outperforming all models by substantial margins. Finally, building on the connections discovered in this paper, we offer a new, simple method to interpret SoPa (Section 7). This method applies equally well to CNNs. We release our code at https://github.com/ Noahs-ARK/soft_patterns. 2 Background Surface patterns. Patterns (Hearst, 1992) are particularly useful tool in NLP (Lin et al., 2003; Etzioni et al., 2005; Schwartz et al., 2015). The most basic definition of a pattern is a sequence of words and wildcards (e.g., “X is a Y”), which can either be manually defined or extracted from a corpus using cooccurrence statistics. Patterns can then be matched against a specific text span by replacing wildcards with concrete words. Davidov et al. (2010) introduced a flexible notion of patterns, which supports partial matching of the pattern with a given text by skipping some of the words in the pattern, or introducing new words. In their framework, when a sequence of text partially matches a pattern, hard-coded partial scores are a"
P18-1028,N16-1060,1,0.85755,"networks. CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016). Neural networks and patterns. Some works used patterns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing"
P18-1028,K17-1004,1,0.830825,"2013)14 contains electronics product reviews, a subset of a larger review dataset. Each document in the dataset contains a review and a summary. Following Yogatama et al. (2015), we only use the reviews part, focusing on positive and negative reviews. The number of training/development/test samples is 20K/5K/25K. • ROC. The ROC story cloze task (Mostafazadeh et al., 2016) is a story understanding task.15 The task is composed of four-sentence story prefixes, followed by two competing endings: one that makes the joint five-sentence story coherent, and another that makes it incoherent. Following Schwartz et al. (2017), we treat it as a style detection task: we treat all “right” endings as positive samples and all “wrong” ones as negative, and we ignore the story prefix. We split the development set into train and development (of sizes 3,366 and 374 sentences, respectively), and take the test set as-is (3,742 sentences). Reduced training data. In order to test our model’s ability to learn from small datasets, we also randomly sample 100, 500, 1,000 and 2,500 SST training instances and 100, 500, 1,000, 2,500, 5,000, and 10,000 Amazon training instances. Development and test sets remain the same. Baselines. W"
P18-1028,P16-1226,0,0.0186872,"s like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016). Neural networks and patterns. Some works used patterns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regulariz"
P18-1028,D10-1102,0,0.0265513,"o interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features. Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016). Li et al. (2016) and K´ad´ar et al. (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions. Finally, several works presented methods to visualize deep CNNs (Zeiler and Fergus, 2014; Simonyan et al., 2014; Yosinski et al., 2015), focusing on visualizing the different layers of the network, mainly in the context of image and video understanding. We believe these two types of research approaches are complementary: inventing general purpose visualization tools for existing black-box models on the one hand, and on"
P18-1028,K15-1021,0,0.0334286,"Missing"
P18-1028,D15-1251,1,0.848162,"e interesting interactions, but we leave this to future work. 12 https://nlp.stanford.edu/sentiment/ index.html 13 http://www.rottentomatoes.com 299 sentences. In order to experiment in a realistic setup, we only consider the complete sentences, and ignore syntactic annotations at train or test time. The number of training/development/test sentences in the dataset is 6,920/872/1,821. • Amazon. The Amazon Review Corpus (McAuley and Leskovec, 2013)14 contains electronics product reviews, a subset of a larger review dataset. Each document in the dataset contains a review and a summary. Following Yogatama et al. (2015), we only use the reviews part, focusing on positive and negative reviews. The number of training/development/test samples is 20K/5K/25K. • ROC. The ROC story cloze task (Mostafazadeh et al., 2016) is a story understanding task.15 The task is composed of four-sentence story prefixes, followed by two competing endings: one that makes the joint five-sentence story coherent, and another that makes it incoherent. Following Schwartz et al. (2017), we treat it as a style detection task: we treat all “right” endings as positive samples and all “wrong” ones as negative, and we ignore the story prefix."
P18-1028,P14-1074,1,0.820344,"embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features. Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016). Li et al. (2016) and K´ad´ar et al. (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions. Finally, several works presented methods to visualize deep CNNs (Zeiler and Fergus, 2014; Simonyan et al."
P18-1028,C16-1329,0,0.0191724,"), and take the test set as-is (3,742 sentences). Reduced training data. In order to test our model’s ability to learn from small datasets, we also randomly sample 100, 500, 1,000 and 2,500 SST training instances and 100, 500, 1,000, 2,500, 5,000, and 10,000 Amazon training instances. Development and test sets remain the same. Baselines. We compare to four baselines: a BiLSTM, a one-layer CNN, DAN (a simple alternative to RNNs) and a feature-based classifier trained with hard-pattern features. • BiLSTM. Bidirectional LSTMs have been successfully used in the past for text classification tasks (Zhou et al., 2016). We learn a one-layer BiLSTM representation of the document, and feed the average of all hidden states to an MLP. • CNN. CNNs are particularly useful for text classification (Kim, 2014). We train a one-layer CNN with max-pooling, and feed the resulting representation to an MLP. 14 http://riejohnson.com/cnn_data.html 15 http://cs.rochester.edu/nlp/ rocstories/ • DAN. We learn a deep averaging network with word dropout (Iyyer et al., 2015), a simple but strong text-classification baseline. • Hard. We train a logistic regression classifier with hard-pattern features. Following Tsur et al. (2010)"
W17-0907,P82-1020,0,0.813873,"Missing"
W17-0907,W07-0602,0,0.0823614,". (2017). 2 System Description We design a system that predicts, given a pair of story endings, which is the right one and which is the wrong one. Our system applies a linear classifier guided by several types of features to solve the task. We describe the system in detail below. 52 Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 52–55, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 2.1 Model (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). We add the following classification features to capture style differences between the two endings. These features are computed on the story endings alone (right or wrong), and do not consider, either at train or at test time, the first four (shared) sentences of each story. We train a binary logistic regression classifier to distinguish between right and wrong stories. We use the set of right stories as positive samples and the set of wrong stories as negative samples. At test time, for a given pair, we consider the classification results of both candidates. If our cla"
W17-0907,N16-1098,0,0.0403179,"Missing"
W17-0907,W11-1515,1,0.892048,"Missing"
W17-0907,P11-1077,0,0.0656157,"rring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of 21,582. Only during training, we apply a (1) The intuition is that a correct ending should be unsurprising (to the model) given the four preceding sentences of the story (the numerator), controlling for the inherent surprise of the words in that ending (the denominator).1 Stylistic features. We hypothesize that right and wrong endings might be distinguishable using style features. We adopt style features that have been shown useful in the past in tasks such as detection of age (Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender 2 www.nltk.org/api/nltk.tokenize.html www.tensorflow.org 4 We train on both the Spring 2016 and the Winter 2017 datasets, a total of roughly 100K stories. 3 1 Note that taking the logarithm of the expression in Equation 1 gives the pointwise mutual information between the story and the ending, under the language model. 53 Model DSSM (Mostafazadeh et al., 2016) LexVec (Salle et al., 2016) RNNLM features Stylistic features Combined (Style + RNNLM) Human judgment Acc. 0.585 0.599 0.677 0.724 0.752 1.000 style on the authors, which is expressed in the different style"
W17-0907,D13-1193,1,0.0470041,", we keep them. If not, the label whose posterior probability is lower is reversed. We describe the classification features below. 2.2 • Length. The number of words in the sentence. Features We use two types of features, designed to capture different aspects of the problem. We use neural language model features to leverage corpus level word distributions, specifically longer term sequence probabilities. We use stylistic features to capture differences in writing between coherent story endings and incoherent ones. • Word n-grams. We use sequences of 1– 5 words. Following Tsur et al. (2010) and Schwartz et al. (2013), we distinguish between high frequency and low frequency words. Specifically, we replace content words, which are often low frequency, with their part-of-speech tags (Nouns, Verbs, Adjectives, and Adverbs). Language model features. We experiment with state-of-the-art text comprehension models, specifically an LSTM (Hochreiter and Schmidhuber, 1997) recurrent neural network language model (RNNLM; Mikolov et al., 2010). Our RNNLM is used to generate two different probabilities: pθ (ending), which is the language model probability of the fifth sentence alone and pθ (ending |story), which is the"
W17-0907,K17-1004,1,0.671994,"Missing"
W18-3024,P17-1080,0,0.0468882,"t LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016b; Adi et al., 2017; Belinkov et al., 2017, among others). Validation Test 0 10 20 30 # Epochs 40 50 Figure 4: Model validation and test accuracy over time during training. Validation improves faster than test, indicating that the model exploits linguistic properties of the data during training. Figure 4 shows that models trained on the unigram and language datasets converge to high validation accuracy faster than high test accuracy. This suggests that models trained on data with linguistic attributes first learn to do well on the training data by exploiting the properties of language and not truly memorizing. Perhaps the model genera"
W18-3024,D16-1248,0,0.206153,"ical structure of language 1 This distribution is adversarial with respect to the Zipfian and natural language training sets. 180 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 180–186 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics paths for the model to minimize its loss, thus offering more training signal than the uniform case, in which the only way to reduce the loss is to learn the memorization function. We further inspect how the LSTM solves the memorization task, and find that some hidden units count the number of inputs. Shi et al. (2016a) analyzed LSTM encoder-decoder translation models and found that similar counting neurons regulate the length of generated translations. Since LSTMs better memorize (and thus better count) on language data than on non-language data, and counting plays a role in encoder-decoder models, our work could also lead to improved training for sequence-to-sequence models in non-language applications (e.g., Schwaller et al., 2017). vocabulary. Our goal is to evaluate the memorization ability of the LSTM, so we freeze the weights of the embedding matrix and the linear output projection during training."
W18-3024,D16-1159,0,0.258984,"ical structure of language 1 This distribution is adversarial with respect to the Zipfian and natural language training sets. 180 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 180–186 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics paths for the model to minimize its loss, thus offering more training signal than the uniform case, in which the only way to reduce the loss is to learn the memorization function. We further inspect how the LSTM solves the memorization task, and find that some hidden units count the number of inputs. Shi et al. (2016a) analyzed LSTM encoder-decoder translation models and found that similar counting neurons regulate the length of generated translations. Since LSTMs better memorize (and thus better count) on language data than on non-language data, and counting plays a role in encoder-decoder models, our work could also lead to improved training for sequence-to-sequence models in non-language applications (e.g., Schwaller et al., 2017). vocabulary. Our goal is to evaluate the memorization ability of the LSTM, so we freeze the weights of the embedding matrix and the linear output projection during training."
W18-3024,P18-2117,0,0.0961116,"counting frequent words or phrases. In the uniform setting, the model has only one path to success: true memorization, and it cannot find an effective way to reduce the loss. In other words, linguistic structure and the patterns of language may provide additional signals that correlate with the label and facilitate learning the memorization task. 183 Accuracy 100 90 80 70 60 50 40 30 20 10 0 ture of language. Blevins et al. (2018) show that the internal representations of LSTMs encode syntactic information, even when trained without explicit syntactic supervision. Also related is the work of Weiss et al. (2018), who demonstrate that LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016"
W18-3024,N18-1108,0,0.051157,"Missing"
W18-3024,P82-1020,0,0.756047,"Missing"
W18-3024,N16-1082,0,0.0475318,"t the internal representations of LSTMs encode syntactic information, even when trained without explicit syntactic supervision. Also related is the work of Weiss et al. (2018), who demonstrate that LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016b; Adi et al., 2017; Belinkov et al., 2017, among others). Validation Test 0 10 20 30 # Epochs 40 50 Figure 4: Model validation and test accuracy over time during training. Validation improves faster than test, indicating that the model exploits linguistic properties of the data during training. Figure 4 shows that models trained on the unigram and language datasets converge to high validation accuracy faster than high test accuracy. Th"
W18-3024,Q16-1037,0,0.0939639,"Missing"
W18-3024,J93-2004,0,0.0609389,"set the forget gate to 0 and the input gate to 1).3 All input sequences at train and test time are of equal length. To explore the effect of sequence length on LSTM task performance, we experiment with different input sequence lengths (10, 20, 40, 60, . . . , 300). 3 • In the uniform setup, each token in the training dataset is randomly sampled from a uniform distribution over the vocabulary. • In the unigram setup, we modify the uniform data by integrating the Zipfian token frequencies found in natural language data. The input sequences are taken from a modified version of the Penn Treebank (Marcus et al., 1993) with randomly permuted tokens. • In the 5gram, 10gram, and 50gram settings, we seek to augment the unigram setting with Markovian dependencies. We generate the dataset by grouping the tokens of the Penn Treebank into 5, 10, or 50-length chunks and randomly permuting these chunks. Experimental Setup We modify the linguistic properties of the training data and observe the effects on model performance. Further details are found in Appendix A, and we release code for reproducing our results.4 • In the language setup, we assess the effect of using real language. The input sequences here are taken"
