2020.acl-main.399,C18-1139,0,0.0407377,"on’s stance, and (3) generating the conclusion’s text with the inferred stance 4334 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4334–4345 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and the inferred target. In this paper, we focus on the first step by proposing two computational approaches for conclusion target inference. As sketched in Figure 1, we hypothesize that the conclusion target is related to the targets of the argument’s premises. To obtain premise targets, we train a state-of-the-art sequence labeling model (Akbik et al., 2018) on target-annotated claims (Bar-Haim et al., 2017). Since the exact relation of premise and conclusion targets is unknown, we develop two complementary inference approaches: One approach ranks premise targets based on their likelihood of being a conclusion target. The other one employs a triplet neural network (Hoffer and Ailon, 2015) that generates a conclusion target embedding from the premise targets in a learned target embedding space. A unique facet of the latter is the integration of the network with a knowledge base of targets (built from any training set), namely, the approach returns"
2020.acl-main.399,C16-1324,1,0.903957,"Missing"
2020.acl-main.399,D18-1403,0,0.0175808,"model of Wang and Ling (2016) as a baseline in our experiments. General research on summarization is manifold and beyond the scope of this work. For a survey, we refer the reader to Gambhir and Gupta (2017). In recent work, we summarize the core of an argument to be used as a snippet in the context of argument search by a two-sentence extract (Alshomary et al., 2020) and Egan et al. (2016) create abstractive summaries of the main points in a debate. We hypothesize a dependency between the target and stance of a conclusion and those of the premises. At a high level, this resembles the work of Angelidis and Lapata (2018) where aspects and sentiments are modeled for the extractive summarization of opinions. We focus on the inference of conclusion targets in this work. Our approach builds upon ideas of Bar-Haim et al. (2017), who classify the stance of premises to a conclusion. To do so, they identify and relate targets in these components, and model stance with sentiment. We do not explicitly tackle stance inference here, because our focus is a conclusion’s target. To identify premise targets, we first train a state-of-the art sequence tagger using contextualized word embeddings (Akbik et al., 2018) on the cor"
2020.acl-main.399,E17-1024,0,0.396839,"promotes equality. (3) Conclusion text generation Stance classification Parents who left school at a young age Premise target identification Premises Abstract con pro pro (2) Stance inference Raising the school leaving age promotes equal opportunities. pro Conclusion Figure 1: Illustration of our full model of generating an argument’s conclusion from its premises. This paper focuses on the identification and inference of targets. Introduction The conclusion (or claim) of a natural language argument conveys a pro or con stance towards some target, such as a controversial concept or statement (Bar-Haim et al., 2017). It is inferred from a set of premises. Conclusions are key to understanding arguments, and hence, critical for any downstream application that processes argumentation. The task of identifying conclusions has been studied intensively in the context of argument mining (Stab and Gurevych, 2014) and automatic essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al K"
2020.acl-main.399,P16-2085,0,0.179685,"(Stab and Gurevych, 2014) and automatic essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conc"
2020.acl-main.399,Q17-1010,0,0.00569372,"ic Evaluation In this section, we report on empirical experiments, along with their results, performed to evaluate our approaches to target inference. were not explicitly phrased, our target identifier did not annotate any token. Hence, we eliminated those cases from the test set.4 Approaches For the premise target ranking approach, we trained LambdaMART (Burges, 2010) on each training set with 1000 estimators and a learning rate of 0.02. We refer to this approach below as Premise Targets (ranking). For target embedding learning, we used the pretrained FastText embeddings with 300 dimensions (Bojanowski et al., 2017) to initially represent each target. To obtain a knowledge base of candidate targets, we applied the target identifier to all conclusions of all training sets.5 The resulting lexicon contains 1,780 targets, each is represented by its FastText embedding. We implemented the triplet neural network as three feed-forward neural networks, each with two layers and shared weights. We call this approach Target Embedding (learning). The simple hybrid of both approaches introduced above is denoted Hybrid (ranking & embedding). To evaluate target inference, we use the iDebate Dataset and the two essay dat"
2020.acl-main.399,W16-2815,0,0.119572,"cific inference schemes for user-defined content (Green, 2017), modeled rhetorical aspects in synthesis (Wachsmuth et al., 4335 2018), and composed arguments that follow a strategy (El Baff et al., 2019). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is an implicit premise, usually the warrant (or major premise) that clarifies how a conclusion is inferred from the given premises (Walton et al., 2008). Motivated by the importance of finding the thesis, Boltuzic and Šnajder (2016) study how to identify such enthymemes given the other components. Similarly, Habernal et al. (2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (201"
2020.acl-main.399,W18-6509,1,0.816119,"(2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (2019) extended this approach by planning and retrieval mechanisms. With a comparable intention, Chen et al. (2018) modified the bias of news headlines from right-toleft or vice versa. Closest to our work is the approach of Wang and Ling (2016) whose sequenceto-sequence model generates summaries for opinionated and argumentative text. Like us, the authors face the problem of varying numbers of input components, and tackle this using an importancebased sampling method. For their evaluation, they crawled arguments from idebate.org. We use this dataset in our experiments. Unfortunately, their manual evaluation considers opinionated text only, leaving the semantic adequacy of the generated argument summaries u"
2020.acl-main.399,P19-1255,0,0.052996,"4). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conclusion’s target from the premises, (2) inferring the conclusion’s stance, a"
2020.acl-main.399,W16-2816,0,0.0652267,"m the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conclusion’s target from the premises, (2) inferring the conclusion’s stance, and (3) generating the conclusion’s text with the inferred stance 4334 Proceedings of the 58th Annual M"
2020.acl-main.399,P18-1021,0,0.0466629,"2008). Motivated by the importance of finding the thesis, Boltuzic and Šnajder (2016) study how to identify such enthymemes given the other components. Similarly, Habernal et al. (2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (2019) extended this approach by planning and retrieval mechanisms. With a comparable intention, Chen et al. (2018) modified the bias of news headlines from right-toleft or vice versa. Closest to our work is the approach of Wang and Ling (2016) whose sequenceto-sequence model generates summaries for opinionated and argumentative text. Like us, the authors face the problem of varying numbers of input components, and tackle this using an importancebased sampling method. For their evaluation, they crawled arguments from idebate.org. W"
2020.acl-main.399,W19-8607,1,0.902062,"Missing"
2020.acl-main.399,W07-0734,0,0.0271692,"Missing"
2020.acl-main.399,P02-1040,0,0.107234,"Missing"
2020.acl-main.399,D15-1110,0,0.0217774,"earch on student essay assessment (Burstein and Marcu, 2003). Falakmasir et al. (2014) show the importance of essay conclusions in applications, whereas Jabbari et al. (2016) specifically target an essay’s overall conclusion, i.e., its thesis (also known as major, main, or central claim). Given the importance of theses, we dedicate one experiment particularly targeting them below. The classification of argument components (as theses, conclusions, premises, etc.) is a core task in argument mining (Stede and Schneider, 2018) and has been approached for different genres (Stab and Gurevych, 2014; Peldszus and Stede, 2015). As Habernal and Gurevych (2015) observe, though, real-world arguments often leave the conclusion implicit, particularly where it is clear in the context of a discussion. In genres such as news editorials, conclusions may even be left out on purpose, in order to persuade readers in a “hidden” manner (Al Khatib et al., 2016). If an implicit conclusion is needed, it hence needs to be synthesized. Argumentation synthesis research is on the rise. Early argument generation approaches relied on rule-based discourse planning techniques (Zukerman et al., 2000). Later, Reisert et al. (2015) generalize"
2020.acl-main.399,D15-1255,0,0.162101,"t conveys a pro or con stance towards some target, such as a controversial concept or statement (Bar-Haim et al., 2017). It is inferred from a set of premises. Conclusions are key to understanding arguments, and hence, critical for any downstream application that processes argumentation. The task of identifying conclusions has been studied intensively in the context of argument mining (Stab and Gurevych, 2014) and automatic essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online deb"
2020.acl-main.399,W16-2804,0,0.150749,"). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is an implicit premise, usually the warrant (or major premise) that clarifies how a conclusion is inferred from the given premises (Walton et al., 2008). Motivated by the importance of finding the thesis, Boltuzic and Šnajder (2016) study how to identify such enthymemes given the other components. Similarly, Habernal et al. (2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (2019) extended this approach by planning and retrieval mechanisms. With a comparable intention, Chen et al. (2018) modified the bias of news headlines from right-toleft or vice versa. Closest to our wo"
2020.acl-main.399,N18-1175,1,0.737726,"Wachsmuth et al., 4335 2018), and composed arguments that follow a strategy (El Baff et al., 2019). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is an implicit premise, usually the warrant (or major premise) that clarifies how a conclusion is inferred from the given premises (Walton et al., 2008). Motivated by the importance of finding the thesis, Boltuzic and Šnajder (2016) study how to identify such enthymemes given the other components. Similarly, Habernal et al. (2018) present the task of identifying the correct warrant from two options, and Rajendran et al. (2016) aim to generate the premise connecting an aspect-related opinion to an overall opinion. Instead of missing premises, we aim to synthesize (parts of) an argument’s conclusion. For any text generation task, a candidate technique is sequence-to-sequence models (Sutskever et al., 2014). Relevant in the given context, Hua and Wang (2018) used such models to generate counterarguments, and Hua et al. (2019) extended this approach by planning and retrieval mechanisms. With a comparable intention, Chen et"
2020.acl-main.399,W15-0507,0,0.35911,"essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conclusion’s target from the premises, (2)"
2020.acl-main.399,P15-4019,0,0.0256367,"ersuade readers in a “hidden” manner (Al Khatib et al., 2016). If an implicit conclusion is needed, it hence needs to be synthesized. Argumentation synthesis research is on the rise. Early argument generation approaches relied on rule-based discourse planning techniques (Zukerman et al., 2000). Later, Reisert et al. (2015) generalized target-stance relations from claims and used them to automatically create new arguments. The relations were curated manually, though. An approach that finds the best conclusion for generation among a set of candidate claims was presented by Yanase et al. (2015). Sato et al. (2015) built upon this approach to phrase texts with multiple arguments. Others recycled targets and predicates of claims in new claims (Bilu and Slonim, 2016), generated arguments with specific inference schemes for user-defined content (Green, 2017), modeled rhetorical aspects in synthesis (Wachsmuth et al., 4335 2018), and composed arguments that follow a strategy (El Baff et al., 2019). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is an implicit premise"
2020.acl-main.399,W00-1408,0,0.278256,"rent genres (Stab and Gurevych, 2014; Peldszus and Stede, 2015). As Habernal and Gurevych (2015) observe, though, real-world arguments often leave the conclusion implicit, particularly where it is clear in the context of a discussion. In genres such as news editorials, conclusions may even be left out on purpose, in order to persuade readers in a “hidden” manner (Al Khatib et al., 2016). If an implicit conclusion is needed, it hence needs to be synthesized. Argumentation synthesis research is on the rise. Early argument generation approaches relied on rule-based discourse planning techniques (Zukerman et al., 2000). Later, Reisert et al. (2015) generalized target-stance relations from claims and used them to automatically create new arguments. The relations were curated manually, though. An approach that finds the best conclusion for generation among a set of candidate claims was presented by Yanase et al. (2015). Sato et al. (2015) built upon this approach to phrase texts with multiple arguments. Others recycled targets and predicates of claims in new claims (Bilu and Slonim, 2016), generated arguments with specific inference schemes for user-defined content (Green, 2017), modeled rhetorical aspects in"
2020.acl-main.399,P17-1099,0,0.0254025,"zation. For generation, we used three LSTM layers with hidden size 150 and a pretrained embedding of size 300. Extra features of the original approach were left out, as they did not help much in our case. We trained the model with batch size 48 and learning rate 0.1 using the Adagrad optimizer (Duchi et al., 2011). For translation, we followed Wang and Ling. To identify targets in the generated summaries, we employed our target identifier. We refer to this baseline as Seq2Seq. To test our hypothesis on the relation of premise and conclusion targets, we extended Seq2Seq by a pointer generator (See et al., 2017) and an extra binary feature that encodes whether a token belongs to a target or not, allowing the model to learn this relation. We call this Seq2Seq (w/ premise targets). On the other hand, we complemented our approaches with simpler variants, in order to check whether learning is needed. Instead of premise tar3 4 5.1 Premise Target Identification We implemented the target identifier as a BiLSTMCRF with hidden layer size 256, using the pretrained contextual string embedding model of Akbik et al. (2018). We trained the model on the training set of the Claim Stance Dataset with batch size 16 an"
2020.acl-main.399,D14-1006,0,0.209591,"of our full model of generating an argument’s conclusion from its premises. This paper focuses on the identification and inference of targets. Introduction The conclusion (or claim) of a natural language argument conveys a pro or con stance towards some target, such as a controversial concept or statement (Bar-Haim et al., 2017). It is inferred from a set of premises. Conclusions are key to understanding arguments, and hence, critical for any downstream application that processes argumentation. The task of identifying conclusions has been studied intensively in the context of argument mining (Stab and Gurevych, 2014) and automatic essay assessment (Falakmasir et al., 2014). In genres other than essays, however, conclusions often remain implicit, since they are clear from the context of a discussion (Habernal and Gurevych, 2015) or hidden on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), n"
2020.acl-main.399,C18-1318,1,0.924044,"Missing"
2020.acl-main.399,N16-1007,0,0.368999,"on purpose for rhetorical reasons, as is often the case in news editorials (Al Khatib et al., 2016). This alters the task entirely to become a synthesis task: Given an argument’s premises, generate its conclusion. As detailed in Section 2, research on argumentation synthesis is still limited. Existing approaches focus on generating single claims (Bilu and Slonim, 2016), new arguments (Reisert et al., 2015), counterarguments (Hua et al., 2019), or argumentative texts (Wachsmuth et al., 2018). Closer to conclusion generation, Egan et al. (2016) summarized the main points of online debates, and Wang and Ling (2016) worked on identifying the main claim of an argument through abstractive summarization. To our knowledge, however, no approach so far reconstructs an argument’s conclusion from its premises. In general, we consider the synthesis task outlined above. Conceptually, we decompose this task into three steps, as depicted in Figure 1: (1) inferring the conclusion’s target from the premises, (2) inferring the conclusion’s stance, and (3) generating the conclusion’s text with the inferred stance 4334 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4334–434"
2020.acl-main.399,W15-0512,0,0.0267417,"purpose, in order to persuade readers in a “hidden” manner (Al Khatib et al., 2016). If an implicit conclusion is needed, it hence needs to be synthesized. Argumentation synthesis research is on the rise. Early argument generation approaches relied on rule-based discourse planning techniques (Zukerman et al., 2000). Later, Reisert et al. (2015) generalized target-stance relations from claims and used them to automatically create new arguments. The relations were curated manually, though. An approach that finds the best conclusion for generation among a set of candidate claims was presented by Yanase et al. (2015). Sato et al. (2015) built upon this approach to phrase texts with multiple arguments. Others recycled targets and predicates of claims in new claims (Bilu and Slonim, 2016), generated arguments with specific inference schemes for user-defined content (Green, 2017), modeled rhetorical aspects in synthesis (Wachsmuth et al., 4335 2018), and composed arguments that follow a strategy (El Baff et al., 2019). All these methods synthesize new argumentative content. In contrast, we aim for the missing components of given arguments. As such, our task resembles enthymeme reconstruction. An enthymeme is"
2020.acl-main.511,P16-1150,0,0.415226,"ata on an interval scale. However, assessors rarely perceive labels as equidistant, thus producing only ordinal data. This leads to a misuse of statistical tests and results in low statistical power of subsequent analyses. (2) Absolute rating is difficult for assessors without prior domain knowledge, since they may be unsure which label to assign. This results in noisy, inconsistent, and unreliable data. As an alternative, preference rating (i.e., a relative comparison by showing two arguments to an assessor and letting them declare their preference towards one of them) has been considered by Habernal and Gurevych (2016), who compile an exhaustive set of pairwise comparisons to infer labels for argument convincingness. For 1,052 arguments on 32 issues, each of the over 16,000 total comparisons was annotated by five different crowd workers on MTurk. While no α statistics are provided, the authors do conclude that preference ratings in a crowdsourced setting are sufficiently accurate, since the best-ranked rater for each pair achieves 0.935 accuracy compared to a gold label. The indicated reliability of pairwise annotation for argument quality is further corroborated by Toledo et al. (2019), who compile a large"
2020.acl-main.511,Q18-1026,0,0.17038,"s promising based on the reported reliability, it creates the need for a model that infers score labels from the collected comparison data. Habernal and Gurevych propose the use of PageRank (Page et al., 1999). This is problematic, since cycles in the comparison graph may form rank sinks, distorting the latent rankings. Habernal and Gurevych deal with this problem by constructing a directed acyclic graph (DAG) from the collected data prior to applying PageRank, assuming that argument convincingness exhibits the property of total order. However, no prior evidence for this property is apparent. Simpson and Gurevych (2018) note further problems with PageRank and propose the use of Gaussian process preference learning instead, demonstrating a high scalability. However, for a practical approach, an effective strategy to minimize the number of needed comparisons is warranted, since, to build the DAG, exhaustive comparison data is required. This is inefficient;  n at worst 2 comparisons have to be obtained for n arguments. Also, no data was collected on how the PageRank method performs on incomplete or sparse comparison data. Chen et al. (2013) also propose an online sampling strategy based on the Bradley-Terry mo"
2020.acl-main.511,W15-4631,0,0.171934,"n issue in crowdsourced settings, where judgments can be collected in abundance for a comparatively cheap price. However, the problem of annotation quality is more severe here: argument quality might be even more difficult to judge without prior domain-specific knowledge, creating the need for annotation frameworks that can still maintain a sufficiently high data quality. Judging from the agreement scores given by Wachsmuth et al. and Potthast et al., obtaining reliable data using classic graded scales proves infeasible, an effect that should be even more pronounced in a crowdsourced setting. Swanson et al. (2015) measure an arguments’ quality as the amount of context or inference required for it to be understood, describing an annotation setup where assessors judge seven individual quality dimensions on a 0-1-slider. Recruiting assessors on Amazon Mechanical Turk (MTurk), they use intra-class correlation to estimate inter-rater agreement, with an average value of 0.42 over all topics, thus also indicating a poor reliability (Portney et al., 2009). They further observe a correlation with sentence length, prompting them to remove all sentences shorter than four words. All three studies indicate that abs"
2020.acl-main.511,D19-1564,0,0.0842744,"onsidered by Habernal and Gurevych (2016), who compile an exhaustive set of pairwise comparisons to infer labels for argument convincingness. For 1,052 arguments on 32 issues, each of the over 16,000 total comparisons was annotated by five different crowd workers on MTurk. While no α statistics are provided, the authors do conclude that preference ratings in a crowdsourced setting are sufficiently accurate, since the best-ranked rater for each pair achieves 0.935 accuracy compared to a gold label. The indicated reliability of pairwise annotation for argument quality is further corroborated by Toledo et al. (2019), who compile a large dataset of about 14,000 annotated argument pairs, and absolute ratings in the 0-1-range for about 6,300 arguments. Pairwise annotations were made in regard to the overall quality of arguments, operationalized as “Which of the two arguments would have been preferred by most people to support/contest the 5773 topic?” Using a strict quality control, they show that the annotated relations consistently reproduce the direction implied by absolute ratings. Yet, annotating quality as a single feature is problematic, since (1) it is hard to capture the multi-facet nature of argume"
2020.acl-main.511,E17-1017,1,0.66167,"l, argumentation generation, and question answering, compiling labeled data for argument quality remains an important prerequisite, yet, also a difficult problem. Most commonly, human assessors have been presented with one argument at a time and then asked to assign labels on a graded quality scale h0, 1, 2i with label descriptions such as (0) “low quality”, (1) “medium quality” and (2) “high quality” for guidance. In previous work, this was usually done concurrently for multiple orthogonal sub-dimensions of argument quality; judging the overall quality of an argument has been deemed complex (Wachsmuth et al., 2017). But on closer inspection, even the more specialized quality dimensions considered are difficult to be assessed as evidenced by the low reliability scores reported. Especially crowdsourcing suffers from assessors often having different reference frames to base their judgments on and task instructions being nondescript and therefore unhelpful in ensuring consistency. Employing experts, however, not only comes at a significantly higher cost per label; despite their expertise, even experts did not achieve more reliable judgments. We pursue an alternative approach: stochastic transitivity modelin"
2021.argmining-1.19,D14-1226,0,0.0748302,"Missing"
2021.argmining-1.19,D17-1218,0,0.0271879,"as the duced by Bar-Haim et al. (2020a) is key point anal- summary of an argument (Petasis and Karkaletsis, ysis where the goal is to map a collection of argu1 https://2021.argmining.org/shared_task_ibm, last accessed: ments to a set of salient key points (say, high-level 2021-08-08 2 arguments) to provide a quantitative summary of The code is available under https://github.com/webis-de/ these arguments. ArgMining-21 184 Proceedings of The 8th Workshop on Argument Mining, pages 184–189 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics 2016; Daxenberger et al., 2017). Wang and Ling (2016) used a sequence-to-sequence model for the abstractive summarization of arguments from online debate portals. A complementary task of generating conclusions as informative argument summaries was introduced by Syed et al. (2021). Similar to Alshomary et al. (2020b) who inferred a conclusion’s target with a triplet neural network, we rely on contrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main su"
2021.argmining-1.19,W16-2816,0,0.0243861,"ontrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an extractive summary. All these works represent the single-document summarization paradigm where only one argument is summarized at a time, whereas the given shared task is a multi-document summarization setting. The first approaches to multi-document argument summarization aimed to identify the main points of online discussions. Among these, Egan et al. (2016) grouped verb frames into pattern clusters that serve as input to a structured summarization pipeline, whereas Misra et al. (2016) proposed a more condensed approach by directly extracting argumentative sentences, summarized by similarity clustering. Bar-Haim et al. (2020a) continued this line of research by introducing the notion of key points and contributing the ArgsKP corpus, a collection of arguments mapped to manually-created key points. These key points are concise and selfcontained sentences that capture the gist of the arguments. Later, Bar-Haim et al. (2020b) proposed a quantitative"
2021.argmining-1.19,W04-1013,0,0.0128467,"ph-based Summarization Aspect Clustering 19.8 18.9 3.5 4.7 18.0 17.1 For the graph-based summarization model, we employed Spacy (Honnibal et al., 2020) to split the arguments into sentences. Similar to (Bar-Haim et al., 2020b), only sentences with a minimum of 5 and a maximum of 20 tokens, and not starting with a pronoun, were used for building the graph. Argument quality scores for each sentence were obtained from Project Debater’s API (Toledo et al., 2019)3 . We selected the thresholds for the parameters d, qual and match in Equation 1 as 0.2, 0.8 and 0.4 respectively, optimizing for ROUGE (Lin, 2004). In particular, we computed ROUGE-L between the ground-truth key points and the top 10 ranked sentences as our predictions, averaged over all the topic and stance combinations in the training split. We excluded sentences with a matching score higher than 0.8 with the selected candidates to minimize redundancy. For aspect clustering, we created 15 clusters per topic and stance combination. After greedy approximation of the candidate sentences, we removed redundant ones using a threshold of 0.65 for the normalized BERTScore (Zhang et al., 2020) with the previously selected candidates. Table 1:"
2021.argmining-1.19,D19-1387,0,0.0191228,"ss, matching findings of Syed et al. (2021). For the aspect clustering, we observe that the key points are more focused on specific aspects such as “disease” (for Pro) and “effectiveness” (for Con). In a real-world application, this may provide the flexibility to choose key points by aspects of interest to the end-user, especially with further improvement of aspect tagger by avoiding non-essential extracted phrases as “mandatory”. Hence, given the task of generating a quantitative summary of a collection of arguments, we believe that the graph-based summary provides We employed RoBERTa-large (Liu and Lapata, 2019) for encoding the tokens of the two inputs of key point matching to the siamese neural network, which acts as a mean-pooling layer and projects the encoder outputs (matrix of token embeddings) into a sentence embedding of size 768. We used Sentence-BERT (Reimers and Gurevych, 2019) to train our model for 10 epochs, with batch size 32, and maximum input length of 70, leaving all other parameters to their defaults. For automatic evaluation, we computed both strict and relaxed mean Average Precision (mAP) following Friedman et al. (2021). In cases where there is no majority label for matching, th"
2021.argmining-1.19,W16-3636,0,0.0248737,"aph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an extractive summary. All these works represent the single-document summarization paradigm where only one argument is summarized at a time, whereas the given shared task is a multi-document summarization setting. The first approaches to multi-document argument summarization aimed to identify the main points of online discussions. Among these, Egan et al. (2016) grouped verb frames into pattern clusters that serve as input to a structured summarization pipeline, whereas Misra et al. (2016) proposed a more condensed approach by directly extracting argumentative sentences, summarized by similarity clustering. Bar-Haim et al. (2020a) continued this line of research by introducing the notion of key points and contributing the ArgsKP corpus, a collection of arguments mapped to manually-created key points. These key points are concise and selfcontained sentences that capture the gist of the arguments. Later, Bar-Haim et al. (2020b) proposed a quantitative argument summarization framework that automatically extracts key points from a set of arguments. Building upon this research, our"
2021.argmining-1.19,W16-2811,0,0.045117,"Missing"
2021.argmining-1.19,D19-1410,0,0.0185596,"oints by aspects of interest to the end-user, especially with further improvement of aspect tagger by avoiding non-essential extracted phrases as “mandatory”. Hence, given the task of generating a quantitative summary of a collection of arguments, we believe that the graph-based summary provides We employed RoBERTa-large (Liu and Lapata, 2019) for encoding the tokens of the two inputs of key point matching to the siamese neural network, which acts as a mean-pooling layer and projects the encoder outputs (matrix of token embeddings) into a sentence embedding of size 768. We used Sentence-BERT (Reimers and Gurevych, 2019) to train our model for 10 epochs, with batch size 32, and maximum input length of 70, leaving all other parameters to their defaults. For automatic evaluation, we computed both strict and relaxed mean Average Precision (mAP) following Friedman et al. (2021). In cases where there is no majority label for matching, the relaxed mAP considers them to be a match while the strict mAP considers them as not matching. In the development phase, we trained our model on the training split and evaluated on the validation split provided by the organizers. The strict and relaxed mAP on the validation set we"
2021.argmining-1.19,2021.naacl-main.34,0,0.0187054,"sentence to the final set of key points if its maximum matching score with the already selected candidates is below a certain threshold. Aspect Clustering Extracting key points is conceptually similar to identifying aspects (Bar-Haim Graph-based Summarization Following the et al., 2020a), which inspired our clustering apwork of Alshomary et al. (2020a), we first construct proach that selects representative sentences from an undirected graph with the arguments’ sentences multiple aspect clusters as the final key points. We as nodes. As a filtering step, we compute argument employ the tagger of Schiller et al. (2021) to exquality scores for each sentence as Toledo et al. tract the arguments’ aspects (on average, 2.1 as(2019) and exclude low-quality arguments from pects per argument). To tackle the lack of diversity, the graph. Next, we employ our key point match- we follow Heinisch and Cimiano (2021) and creing model (Section 4.1) to compute the edge weight ate k diverse aspect clusters by projecting the exbetween two nodes as the pairwise matching score tracted aspect phrases to an embedding space. Next, of the corresponding sentences. Only nodes with a we model the candidate selection of argument sensco"
2021.argmining-1.19,2020.coling-main.470,1,0.82388,"Missing"
2021.argmining-1.19,2021.findings-acl.306,1,0.802537,"Missing"
2021.argmining-1.19,D19-1564,0,0.0281845,"selection of argument senscore above a defined threshold are connected via tences as the set cover problem. Specifically, the 186 Approach R-1 R-2 R-L 5.2 Graph-based Summarization Aspect Clustering 19.8 18.9 3.5 4.7 18.0 17.1 For the graph-based summarization model, we employed Spacy (Honnibal et al., 2020) to split the arguments into sentences. Similar to (Bar-Haim et al., 2020b), only sentences with a minimum of 5 and a maximum of 20 tokens, and not starting with a pronoun, were used for building the graph. Argument quality scores for each sentence were obtained from Project Debater’s API (Toledo et al., 2019)3 . We selected the thresholds for the parameters d, qual and match in Equation 1 as 0.2, 0.8 and 0.4 respectively, optimizing for ROUGE (Lin, 2004). In particular, we computed ROUGE-L between the ground-truth key points and the top 10 ranked sentences as our predictions, averaged over all the topic and stance combinations in the training split. We excluded sentences with a matching score higher than 0.8 with the selected candidates to minimize redundancy. For aspect clustering, we created 15 clusters per topic and stance combination. After greedy approximation of the candidate sentences, we r"
2021.argmining-1.19,N16-1007,0,0.0282445,"t al. (2020a) is key point anal- summary of an argument (Petasis and Karkaletsis, ysis where the goal is to map a collection of argu1 https://2021.argmining.org/shared_task_ibm, last accessed: ments to a set of salient key points (say, high-level 2021-08-08 2 arguments) to provide a quantitative summary of The code is available under https://github.com/webis-de/ these arguments. ArgMining-21 184 Proceedings of The 8th Workshop on Argument Mining, pages 184–189 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics 2016; Daxenberger et al., 2017). Wang and Ling (2016) used a sequence-to-sequence model for the abstractive summarization of arguments from online debate portals. A complementary task of generating conclusions as informative argument summaries was introduced by Syed et al. (2021). Similar to Alshomary et al. (2020b) who inferred a conclusion’s target with a triplet neural network, we rely on contrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an"
2021.argmining-1.4,W14-2109,0,0.0294538,"suggests listing the image both as a pro and a con. In the future, however, additional considerations of argumentative quality (especially in terms of clarity) might suggest to omit such images completely, or to show them separately. Topic relevance The image content is related to the query topic. This criterion corresponds to the notion of relevance in keyword-based image retrieval (Shanbehzadeh et al., 2000). Argumentativeness The image can be used to support a stance regarding the query topic. This criterion corresponds to the notion of a context-dependent claim in textual argument mining (Aharoni et al., 2014). Stance relevance The image can be used to support the predicted stance within the query topic. This criterion corresponds to the categorization into pros and cons in standard argument search (Wachsmuth et al., 2017b). Since stance relevance entails argumentativeness, which in turn entails topic relevance, we refer to these three as “levels” of relevance. Though previous work focused on stance relevance only (e.g., Stab et al., 2018), an analysis on all three levels provides more insight into the errors made and is especially warranted for “argumentative images.” Figure 2 illustrates the diff"
2021.argmining-1.4,goldhahn-etal-2012-building,0,0.00947894,"this term is more associated with “doing it poorly” than with “being against it.” (2) Positive-Negative This method exploits the fact that stance is often reflected through expressions of sentiment, for example, as used in counterargument retrieval (Wachsmuth et al., 2018). For each stance, we generate up to five queries by appending the top positive (for pro) or negative terms (for con) of the 8000 entries in the MPQA subjectivity lexicon (Wilson et al., 2005) as ranked by their co-occurrence with the query according to the Leipzig Corpora Collection’s English corpus (120 million sentences, Goldhahn et al., 2012). (3) Pros-Cons This method employs argument search engines to identify terms typical for certain topic-stance-combinations. E.g., in arguments retrieved for nuclear energy, “CO2 neutrality” occurs more often in pro arguments than in con ones, whereas “radiation” occurs more often in con arguments than in pro ones. Based on work in anomaly detection (Afgani et al., 2008), this method calculates the specificity of a term t to a stance s, δ(t, s), as their contribution to the Kullback-Leibler divergence of the term distributions between the two stances: (1) Good-Anti Conceivably the single most"
2021.argmining-1.4,P18-1023,1,0.763553,"st for a stance, then the second of each, and so on. We devise three methods: (1) appending always the same stance-indicating terms to the user’s query, (2) appending sentiment-indicating terms that co-occur with the query’s terms, and (3) appending topic-specific stance-indicating terms obtained from a text argument search engine. for some topics this term is more associated with “doing it poorly” than with “being against it.” (2) Positive-Negative This method exploits the fact that stance is often reflected through expressions of sentiment, for example, as used in counterargument retrieval (Wachsmuth et al., 2018). For each stance, we generate up to five queries by appending the top positive (for pro) or negative terms (for con) of the 8000 entries in the MPQA subjectivity lexicon (Wilson et al., 2005) as ranked by their co-occurrence with the query according to the Leipzig Corpora Collection’s English corpus (120 million sentences, Goldhahn et al., 2012). (3) Pros-Cons This method employs argument search engines to identify terms typical for certain topic-stance-combinations. E.g., in arguments retrieved for nuclear energy, “CO2 neutrality” occurs more often in pro arguments than in con ones, whereas"
2021.argmining-1.4,walker-etal-2012-corpus,0,0.0426073,"ence (Berger, 2010), or the recent departure of Western military forces from Afghanistan. Argument search Based on argument mining from texts (cf. Peldszus and Stede, 2013), argument search engines aim to support decisionmaking and persuasion. Conceptually, a query to an argument search engine may either name an issue without a stance (Stab et al., 2018) (e.g., nuclear energy), or represent a conclusion for which supporting and attacking premises are to be retrieved (e.g., nuclear energy mitigates climate change). The first collection of arguments from the web is the Internet Argument Corpus (Walker et al., 2012), containing 400,000 posts from an online debate portal. The first argument search engine, args.me, indexes a similar dataset of arguments (Wachsmuth et al., 2017b). Not relying on retrieval in collections of arguments, ArgumenText (Stab et al., 2018) first searches for documents relevant to a user’s query in generic web crawls, and then mines arguments on the fly within retrieved documents. Regarding the evaluation of argument search engines, judging the topic relevance of a retrieved text alone is insufficient, it must also be argumentative (Potthast et al., 2019; Bondarenko et al., 2021). R"
2021.argmining-1.4,H05-1044,0,0.0300411,"hat co-occur with the query’s terms, and (3) appending topic-specific stance-indicating terms obtained from a text argument search engine. for some topics this term is more associated with “doing it poorly” than with “being against it.” (2) Positive-Negative This method exploits the fact that stance is often reflected through expressions of sentiment, for example, as used in counterargument retrieval (Wachsmuth et al., 2018). For each stance, we generate up to five queries by appending the top positive (for pro) or negative terms (for con) of the 8000 entries in the MPQA subjectivity lexicon (Wilson et al., 2005) as ranked by their co-occurrence with the query according to the Leipzig Corpora Collection’s English corpus (120 million sentences, Goldhahn et al., 2012). (3) Pros-Cons This method employs argument search engines to identify terms typical for certain topic-stance-combinations. E.g., in arguments retrieved for nuclear energy, “CO2 neutrality” occurs more often in pro arguments than in con ones, whereas “radiation” occurs more often in con arguments than in pro ones. Based on work in anomaly detection (Afgani et al., 2008), this method calculates the specificity of a term t to a stance s, δ(t"
2021.argmining-1.4,N18-5005,0,0.0353157,"Missing"
2021.argmining-1.4,E17-1017,1,0.796036,"Missing"
2021.argmining-1.4,W17-5106,1,0.914024,"nformation and express, underline, or popularize an opinion (Dove, 2012), thereby taking the form of subjective statements (Dunaway, 2018). Some images express both a premise and a conclusion, making them full arguments (Roque, 2012; Grancea, 2017). Other images may provide contextual information only and need to be combined with a conclusion to form an argument. In this regard, a recent SemEval task distinguished a total of 22 persuasion techniques in memes alone (Dimitrov et al., 2021). Moreover, argument quality dimensions like acceptability, credibility, emotional appeal, and sufficiency (Wachsmuth et al., 2017a) all apply to arguments that include images as well. And as a kind of visual argumentation scheme (a “stereotypical pattern of human reasoning”; Walton et al., 2008), some images are frequently adapted to different topics (Heiskanen, 2017). Social groups even create their own symbolisms and use them to express opinions (e.g., fringe web communities; Zannettou et al., 2018). The potentially high emotional impact of images to a vast audience (Adler-Nissen et al., 2020) can cause changes in the social discourse and eventually politics (Woods and Hahner, 2019). Examples include photos of a drown"
2021.emnlp-main.795,S17-2001,0,0.0198183,"used n-gram features or word embeddings. Further, it gained recognition in argument mining, as demonstrated by Sobhani et al. (2015). Xu et al. (2019) introduce reason comparing networks (RCN) that identify agreement and disagreement between utterances towards a topic. They leverage reason information to cope with non-dialogic utterances. Since the S3C task authors hypothesize that textual similarity between arguments may be sufficient, the task bears structural similarity towards semantic textual similarity, which has often been a topic of shared tasks (Agirre et al., 2013; Xu et al., 2015; Cer et al., 2017), and for which many datasets can be found (Dolan and Brockett, 2005; Ganitkevitch et al., 2013). S3C Shared Task The S3C dataset (Stein et al., 2021) is derived from the args.me corpus (Ajjour et al., 2019) and comprises pairs of arguments from several thousand debates about one of two topics, namely abortion and gay marriage. The arguments have been retrieved from online debate portals. Argument pairs were sampled from single arguments that occurred within the same debate context. Binary labels for pairs were inferred according to whether or not the two arguments take the same stance. Two ta"
2021.emnlp-main.795,I05-5002,0,0.0449291,"recognition in argument mining, as demonstrated by Sobhani et al. (2015). Xu et al. (2019) introduce reason comparing networks (RCN) that identify agreement and disagreement between utterances towards a topic. They leverage reason information to cope with non-dialogic utterances. Since the S3C task authors hypothesize that textual similarity between arguments may be sufficient, the task bears structural similarity towards semantic textual similarity, which has often been a topic of shared tasks (Agirre et al., 2013; Xu et al., 2015; Cer et al., 2017), and for which many datasets can be found (Dolan and Brockett, 2005; Ganitkevitch et al., 2013). S3C Shared Task The S3C dataset (Stein et al., 2021) is derived from the args.me corpus (Ajjour et al., 2019) and comprises pairs of arguments from several thousand debates about one of two topics, namely abortion and gay marriage. The arguments have been retrieved from online debate portals. Argument pairs were sampled from single arguments that occurred within the same debate context. Binary labels for pairs were inferred according to whether or not the two arguments take the same stance. Two tasks have been defined based on this data: within, where training and"
2021.emnlp-main.795,C16-1250,0,0.0501212,"Missing"
2021.emnlp-main.795,N13-1092,0,0.0371629,"Missing"
2021.emnlp-main.795,D13-1191,0,0.0133071,"domain, so to speak, in a topic-agnostic fashion”.1 Successful S3C can, for instance, help to quickly identify coherent posts in social media debates, or to quantify groups of posts with opposing stances. To advance S3C as a task in the argument mining community, this paper makes three main contributions: (1) Development of new transformer-based approaches 1 More details on the task at https://sameside.webis.de Related Work Stance Classification S3C has been introduced as a shared task by Stein et al. (2021). Prior work on stance classification, such as that of Somasundaran and Wiebe (2010), Gottipati et al. (2013), and Sridhar et al. (2015), focuses more on detecting the stance towards a certain topic and only marginally the direct comparison between two arguments. Sridhar et al. (2014) describe a collective stance classification approach using both linguistic and structural features to predict the stance of many posts in an online debate forum. It uses a weighted graph to model author and post relations and predicts the stance with a set of logic rules. Rosenthal and McKeown (2015) use the conversational structure of online discussion forums to detect agreement and disagreement, and Walker et al. (201"
2021.emnlp-main.795,P15-1012,0,0.020855,"pic-agnostic fashion”.1 Successful S3C can, for instance, help to quickly identify coherent posts in social media debates, or to quantify groups of posts with opposing stances. To advance S3C as a task in the argument mining community, this paper makes three main contributions: (1) Development of new transformer-based approaches 1 More details on the task at https://sameside.webis.de Related Work Stance Classification S3C has been introduced as a shared task by Stein et al. (2021). Prior work on stance classification, such as that of Somasundaran and Wiebe (2010), Gottipati et al. (2013), and Sridhar et al. (2015), focuses more on detecting the stance towards a certain topic and only marginally the direct comparison between two arguments. Sridhar et al. (2014) describe a collective stance classification approach using both linguistic and structural features to predict the stance of many posts in an online debate forum. It uses a weighted graph to model author and post relations and predicts the stance with a set of logic rules. Rosenthal and McKeown (2015) use the conversational structure of online discussion forums to detect agreement and disagreement, and Walker et al. (2012) exploit the dialogic str"
2021.emnlp-main.795,W14-2715,0,0.0362634,"osts with opposing stances. To advance S3C as a task in the argument mining community, this paper makes three main contributions: (1) Development of new transformer-based approaches 1 More details on the task at https://sameside.webis.de Related Work Stance Classification S3C has been introduced as a shared task by Stein et al. (2021). Prior work on stance classification, such as that of Somasundaran and Wiebe (2010), Gottipati et al. (2013), and Sridhar et al. (2015), focuses more on detecting the stance towards a certain topic and only marginally the direct comparison between two arguments. Sridhar et al. (2014) describe a collective stance classification approach using both linguistic and structural features to predict the stance of many posts in an online debate forum. It uses a weighted graph to model author and post relations and predicts the stance with a set of logic rules. Rosenthal and McKeown (2015) use the conversational structure of online discussion forums to detect agreement and disagreement, and Walker et al. (2012) exploit the dialogic structure in online debates to outperform content-based models. As opinionated language in social media typically expresses a stance towards a topic, it"
2021.emnlp-main.795,N12-1072,0,0.0397668,"pati et al. (2013), and Sridhar et al. (2015), focuses more on detecting the stance towards a certain topic and only marginally the direct comparison between two arguments. Sridhar et al. (2014) describe a collective stance classification approach using both linguistic and structural features to predict the stance of many posts in an online debate forum. It uses a weighted graph to model author and post relations and predicts the stance with a set of logic rules. Rosenthal and McKeown (2015) use the conversational structure of online discussion forums to detect agreement and disagreement, and Walker et al. (2012) exploit the dialogic structure in online debates to outperform content-based models. As opinionated language in social media typically expresses a stance towards a topic, it allows us to infer the connection between stance classification and target-dependent sentiment classification, as demonstrated by Wang and Cardie (2014) and Ebrahimi et al. (2016). Stance classification in tweets was also a target of the SemEval-2016 2 Code and data: https://github.com/webis-de/EMNLP-21 10130 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10130–10138 c Novemb"
2021.emnlp-main.795,W14-2617,0,0.0198845,"osts in an online debate forum. It uses a weighted graph to model author and post relations and predicts the stance with a set of logic rules. Rosenthal and McKeown (2015) use the conversational structure of online discussion forums to detect agreement and disagreement, and Walker et al. (2012) exploit the dialogic structure in online debates to outperform content-based models. As opinionated language in social media typically expresses a stance towards a topic, it allows us to infer the connection between stance classification and target-dependent sentiment classification, as demonstrated by Wang and Cardie (2014) and Ebrahimi et al. (2016). Stance classification in tweets was also a target of the SemEval-2016 2 Code and data: https://github.com/webis-de/EMNLP-21 10130 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10130–10138 c November 7–11, 2021. 2021 Association for Computational Linguistics Task 6 (Mohammad et al., 2016), where most teams used n-gram features or word embeddings. Further, it gained recognition in argument mining, as demonstrated by Sobhani et al. (2015). Xu et al. (2019) introduce reason comparing networks (RCN) that identify agreement"
2021.emnlp-main.795,P19-1460,0,0.0147937,"rget-dependent sentiment classification, as demonstrated by Wang and Cardie (2014) and Ebrahimi et al. (2016). Stance classification in tweets was also a target of the SemEval-2016 2 Code and data: https://github.com/webis-de/EMNLP-21 10130 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10130–10138 c November 7–11, 2021. 2021 Association for Computational Linguistics Task 6 (Mohammad et al., 2016), where most teams used n-gram features or word embeddings. Further, it gained recognition in argument mining, as demonstrated by Sobhani et al. (2015). Xu et al. (2019) introduce reason comparing networks (RCN) that identify agreement and disagreement between utterances towards a topic. They leverage reason information to cope with non-dialogic utterances. Since the S3C task authors hypothesize that textual similarity between arguments may be sufficient, the task bears structural similarity towards semantic textual similarity, which has often been a topic of shared tasks (Agirre et al., 2013; Xu et al., 2015; Cer et al., 2017), and for which many datasets can be found (Dolan and Brockett, 2005; Ganitkevitch et al., 2013). S3C Shared Task The S3C dataset (Ste"
2021.emnlp-main.795,S15-2001,0,0.186331,"shion”.1 Successful S3C can, for instance, help to quickly identify coherent posts in social media debates, or to quantify groups of posts with opposing stances. To advance S3C as a task in the argument mining community, this paper makes three main contributions: (1) Development of new transformer-based approaches 1 More details on the task at https://sameside.webis.de Related Work Stance Classification S3C has been introduced as a shared task by Stein et al. (2021). Prior work on stance classification, such as that of Somasundaran and Wiebe (2010), Gottipati et al. (2013), and Sridhar et al. (2015), focuses more on detecting the stance towards a certain topic and only marginally the direct comparison between two arguments. Sridhar et al. (2014) describe a collective stance classification approach using both linguistic and structural features to predict the stance of many posts in an online debate forum. It uses a weighted graph to model author and post relations and predicts the stance with a set of logic rules. Rosenthal and McKeown (2015) use the conversational structure of online discussion forums to detect agreement and disagreement, and Walker et al. (2012) exploit the dialogic str"
2021.findings-acl.159,2021.eacl-main.17,1,0.722278,"th for single claims and complete arguments. Bilu et al. (2015) composed opposing claims combining rules with classifiers, whereas Hidey and McKeown (2019) tackled an analog task with neural methods. Alshomary et al. (2020) reconstructed implicit claims from argument premises using triplet neural networks, and Gretz et al. (2020) explored ability of 2 Our assumption is that each sentence represents a premise supporting the main claim mentioned in the title of the post 3 Code and resources can be found under https:// github.com/webis-de/ACL-21 1817 GPT-2 to generate claims on topics. Recently, Alshomary et al. (2021) studied how to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack"
2021.findings-acl.159,2020.acl-main.399,1,0.844959,"or detecting premise attackability, achieving state-of-the-art effectiveness. • A new approach to counter-argument generation that identifies and attacks weak premises. • Empirical evidence of the impact of considering specific attackable premises in the argument when generating a counter-argument. 2 Related Work Recently, text generation has gained much interest in computational argumentation, both for single claims and complete arguments. Bilu et al. (2015) composed opposing claims combining rules with classifiers, whereas Hidey and McKeown (2019) tackled an analog task with neural methods. Alshomary et al. (2020) reconstructed implicit claims from argument premises using triplet neural networks, and Gretz et al. (2020) explored ability of 2 Our assumption is that each sentence represents a premise supporting the main claim mentioned in the title of the post 3 Code and resources can be found under https:// github.com/webis-de/ACL-21 1817 GPT-2 to generate claims on topics. Recently, Alshomary et al. (2021) studied how to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full argument"
2021.findings-acl.159,W15-0511,0,0.0814234,"from Reddit changemyview. The italicized premise part was quoted by the user who stated the counter-argument. Introduction Following Walton (2009), a counter-argument can be defined as an attack on a specific argument by arguing against either its claim (called rebuttal), the validity of reasoning of its premises toward its claim (undercut), or the validity of one of its premises (undermining). Not only the mining and retrieval of counter-arguments have been studied (Peldszus and Stede, 2015; Wachsmuth et al., 2018b), recent works also tackled the generation of counter-arguments. Among these, Bilu et al. (2015) and Hidey and McKeown (2019) studied the task of contrastive claim generation, the former in a partly rule-based manner, the latter data-driven. Moreover, Hua and Wang (2019) proposed a neural counter-argument generation approach. So far, however, research focused only on rebutting a given argument, ignoring the other aforementioned types. We expand this research by studying to what extent argument undermining can be utilized in counterargument generation. In argument undermining, the validity of some premises is questioned. Such a phenomenon can be observed often in online discussions on soc"
2021.findings-acl.159,D19-1291,0,0.0264544,"fs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mining counter-considerations from text (Peldszus and Stede, 2015), and retrieving counter-arguments (Wachsmuth et al., 2018b; Orbach et al., 2019). However, only the two works of Hua and Wang (2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied from an argument quality pers"
2021.findings-acl.159,W18-6509,1,0.838201,"ning rules with classifiers, whereas Hidey and McKeown (2019) tackled an analog task with neural methods. Alshomary et al. (2020) reconstructed implicit claims from argument premises using triplet neural networks, and Gretz et al. (2020) explored ability of 2 Our assumption is that each sentence represents a premise supporting the main claim mentioned in the title of the post 3 Code and resources can be found under https:// github.com/webis-de/ACL-21 1817 GPT-2 to generate claims on topics. Recently, Alshomary et al. (2021) studied how to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mini"
2021.findings-acl.159,D17-1144,0,0.0285889,"w to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mining counter-considerations from text (Peldszus and Stede, 2015), and retrieving counter-arguments (Wachsmuth et al., 2018b; Orbach et al., 2019). However, only the two works of Hua and Wang (2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied fro"
2021.findings-acl.159,W19-8607,1,0.896693,"Missing"
2021.findings-acl.159,2020.findings-emnlp.47,0,0.0247972,"nt generation that identifies and attacks weak premises. • Empirical evidence of the impact of considering specific attackable premises in the argument when generating a counter-argument. 2 Related Work Recently, text generation has gained much interest in computational argumentation, both for single claims and complete arguments. Bilu et al. (2015) composed opposing claims combining rules with classifiers, whereas Hidey and McKeown (2019) tackled an analog task with neural methods. Alshomary et al. (2020) reconstructed implicit claims from argument premises using triplet neural networks, and Gretz et al. (2020) explored ability of 2 Our assumption is that each sentence represents a premise supporting the main claim mentioned in the title of the post 3 Code and resources can be found under https:// github.com/webis-de/ACL-21 1817 GPT-2 to generate claims on topics. Recently, Alshomary et al. (2021) studied how to encode specific beliefs into generated claims, whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et"
2021.findings-acl.159,N19-1174,0,0.0473113,"Missing"
2021.findings-acl.159,P18-1021,0,0.0237511,"l argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mining counter-considerations from text (Peldszus and Stede, 2015), and retrieving counter-arguments (Wachsmuth et al., 2018b; Orbach et al., 2019). However, only the two works of Hua and Wang (2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied from an argument quality perspective. That is, a premise is attackable when it lacks specific quality criteria. A significant body of research has studied argument quality assessment, with a comprehensive survey of quality criteria presented in Wachsm"
2021.findings-acl.159,D19-1055,0,0.209842,"defined as an attack on a specific argument by arguing against either its claim (called rebuttal), the validity of reasoning of its premises toward its claim (undercut), or the validity of one of its premises (undermining). Not only the mining and retrieval of counter-arguments have been studied (Peldszus and Stede, 2015; Wachsmuth et al., 2018b), recent works also tackled the generation of counter-arguments. Among these, Bilu et al. (2015) and Hidey and McKeown (2019) studied the task of contrastive claim generation, the former in a partly rule-based manner, the latter data-driven. Moreover, Hua and Wang (2019) proposed a neural counter-argument generation approach. So far, however, research focused only on rebutting a given argument, ignoring the other aforementioned types. We expand this research by studying to what extent argument undermining can be utilized in counterargument generation. In argument undermining, the validity of some premises is questioned. Such a phenomenon can be observed often in online discussions on social media. For example, in the discussion excerpt in Table 1, taken from the Reddit forum changemyview,1 a user contests the whole stated argument (claim and premises) by refe"
2021.findings-acl.159,2020.emnlp-main.1,0,0.246547,"o tackle the task of counter-argument generation by attacking one of the weak premises of an argument. We hypothesize that identifying a weak premise is key to effective counter-argument generation—especially when the argument is of high complexity, comprising multiple interlinked claims and premises, which makes it hard to comprehend the argument as a single unit. Figure 1 illustrates our two-step pipeline approach: it first detects premises that may be attackable and then generates a counter-argument targeting one or more of these premises. To identify weak premises, we build on the work of Jo et al. (2020), who classify attackable sentences using BERT. Unlike the authors, we rank premises based on their attackability concerning the argument’s main claim, utilizing the learning-to-rank approach of Han et al. (2020). For the second step, similar to Wolf et al. (2019), we fine-tune a pre-trained transformer-based language model (Radford et al., 2018), in a multitask learning setting: next-token classification and counter-argument classification. In our experiments, we make use of the changemyview (CMV) dataset of Jo et al. (2020), where each instance is a post consisting of a title (say, an argume"
2021.findings-acl.159,D19-1561,0,0.0281432,"Missing"
2021.findings-acl.159,2020.acl-main.633,0,0.0107266,"whereas Chen et al. (2018) flipped the bias of claim-like news headlines using style transfer. Sato et al. (2015) generated full arguments in a largely rule-based way. Building on the model of rhetorical argumentation strategies by Wachsmuth et al. (2018a), El Baff et al. (2019) modeled argument synthesis as a language modeling task, and Schiller et al. (2020) studied the neural generation of arguments on a topic with controlled aspects and stance. Unlike all these, we deal with counter-arguments. Research exists for mining attack relations (Cocarascu and Toni, 2017; Chakrabarty et al., 2019; Orbach et al., 2020), mining counter-considerations from text (Peldszus and Stede, 2015), and retrieving counter-arguments (Wachsmuth et al., 2018b; Orbach et al., 2019). However, only the two works of Hua and Wang (2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied from an argument quality perspective. That is, a pr"
2021.findings-acl.159,W15-0513,0,0.184859,"n’t an example of homophobia... Table 1: An example argument (claim + premises) and a counter-argument in response to it, taken from Reddit changemyview. The italicized premise part was quoted by the user who stated the counter-argument. Introduction Following Walton (2009), a counter-argument can be defined as an attack on a specific argument by arguing against either its claim (called rebuttal), the validity of reasoning of its premises toward its claim (undercut), or the validity of one of its premises (undermining). Not only the mining and retrieval of counter-arguments have been studied (Peldszus and Stede, 2015; Wachsmuth et al., 2018b), recent works also tackled the generation of counter-arguments. Among these, Bilu et al. (2015) and Hidey and McKeown (2019) studied the task of contrastive claim generation, the former in a partly rule-based manner, the latter data-driven. Moreover, Hua and Wang (2019) proposed a neural counter-argument generation approach. So far, however, research focused only on rebutting a given argument, ignoring the other aforementioned types. We expand this research by studying to what extent argument undermining can be utilized in counterargument generation. In argument unde"
2021.findings-acl.159,E17-1017,1,0.833237,"(2018, 2019) realy generated such arguments. Their latest neural approach takes an argument or claim as input and generates a counter-argument rebutting it. Differently, we consider countering an argument by attacking one of its premises, known as undermining (Walton, 2009). Part of our approach is to identify attackable premises, which can be studied from an argument quality perspective. That is, a premise is attackable when it lacks specific quality criteria. A significant body of research has studied argument quality assessment, with a comprehensive survey of quality criteria presented in Wachsmuth et al. (2017). Implicitly, we target criteria such as a premise’s acceptability or relevance. Still, we follow Jo et al. (2020) in deriving attackability from the sentences of posts that users attack in the Reddit forum CMV. These sentences represent premises supporting the claim encoded in a post’s title. The authors experimented with different features that potentially reflect weaknesses in the premises. Their best model for identifying attackable premises is a BERT-based classifier. We use their data to learn weak premise identification, but we address it as a learning-to-rank task. As for text generati"
2021.findings-acl.159,C18-1318,1,0.889685,"Missing"
2021.findings-acl.159,P18-1023,1,0.928982,"ia... Table 1: An example argument (claim + premises) and a counter-argument in response to it, taken from Reddit changemyview. The italicized premise part was quoted by the user who stated the counter-argument. Introduction Following Walton (2009), a counter-argument can be defined as an attack on a specific argument by arguing against either its claim (called rebuttal), the validity of reasoning of its premises toward its claim (undercut), or the validity of one of its premises (undermining). Not only the mining and retrieval of counter-arguments have been studied (Peldszus and Stede, 2015; Wachsmuth et al., 2018b), recent works also tackled the generation of counter-arguments. Among these, Bilu et al. (2015) and Hidey and McKeown (2019) studied the task of contrastive claim generation, the former in a partly rule-based manner, the latter data-driven. Moreover, Hua and Wang (2019) proposed a neural counter-argument generation approach. So far, however, research focused only on rebutting a given argument, ignoring the other aforementioned types. We expand this research by studying to what extent argument undermining can be utilized in counterargument generation. In argument undermining, the validity of"
2021.findings-acl.159,2020.emnlp-demos.6,0,0.0738354,"Missing"
2021.findings-acl.451,D16-1245,0,0.0451648,"Missing"
2021.findings-acl.451,L16-1586,0,0.0231091,"to improve data discovery. 6 https://zenodo.org/ https://datacite.org/ 8 http://schema.org/Dataset 9 This project has been discontinued: https://www.microsoft. com/en-us/research/project/microsoft-academic-graph/ 7 The task of extracting structured information from scientific publications has been tackled many times. Gupta and Manning (2011) extract key aspects of scientific papers, including focus, technique, and domain from the ACL Anthology. Mesbah et al. (2018), Luan et al. (2018), and Jain et al. (2020) propose approaches to identify entities and their relations in scientific documents. Gábor et al. (2016) creates an annotated corpus for concepts and semantic relations based on the ACL Anthology. Duck et al. (2016) employed text mining to process dataset and software mentions in biological and medical publications from PubMed Central. Boland et al. (2012) identify references to datasets in social science publications. Closely related to our work, software mentions in scientific documents can be mined using a Grobid library module,10 e.g., to give research software more credit. In 2019, a shared task focusing on the tasks of dataset review mining and extraction of scientific methods and fields w"
2021.findings-acl.451,I11-1001,0,0.0423591,"h as CKAN, integrate this functionality by default. In addition, improvements on tracking citations of data are being actively developed. Projects, like Semantic Scholar, Microsoft Academic Graph,9 and Google Dataset Search, along with Google Scholar, couple data and publications to improve data discovery. 6 https://zenodo.org/ https://datacite.org/ 8 http://schema.org/Dataset 9 This project has been discontinued: https://www.microsoft. com/en-us/research/project/microsoft-academic-graph/ 7 The task of extracting structured information from scientific publications has been tackled many times. Gupta and Manning (2011) extract key aspects of scientific papers, including focus, technique, and domain from the ACL Anthology. Mesbah et al. (2018), Luan et al. (2018), and Jain et al. (2020) propose approaches to identify entities and their relations in scientific documents. Gábor et al. (2016) creates an annotated corpus for concepts and semantic relations based on the ACL Anthology. Duck et al. (2016) employed text mining to process dataset and software mentions in biological and medical publications from PubMed Central. Boland et al. (2012) identify references to datasets in social science publications. Closel"
2021.findings-acl.451,2020.acl-main.670,0,0.0317915,"Missing"
2021.findings-acl.451,D18-1360,0,0.0117576,"Semantic Scholar, Microsoft Academic Graph,9 and Google Dataset Search, along with Google Scholar, couple data and publications to improve data discovery. 6 https://zenodo.org/ https://datacite.org/ 8 http://schema.org/Dataset 9 This project has been discontinued: https://www.microsoft. com/en-us/research/project/microsoft-academic-graph/ 7 The task of extracting structured information from scientific publications has been tackled many times. Gupta and Manning (2011) extract key aspects of scientific papers, including focus, technique, and domain from the ACL Anthology. Mesbah et al. (2018), Luan et al. (2018), and Jain et al. (2020) propose approaches to identify entities and their relations in scientific documents. Gábor et al. (2016) creates an annotated corpus for concepts and semantic relations based on the ACL Anthology. Duck et al. (2016) employed text mining to process dataset and software mentions in biological and medical publications from PubMed Central. Boland et al. (2012) identify references to datasets in social science publications. Closely related to our work, software mentions in scientific documents can be mined using a Grobid library module,10 e.g., to give research software mor"
2021.findings-emnlp.53,S17-2001,0,0.0151302,"tein et al. (2021) as well as the authorship verification problem (Koppel and Schler, 2004), our underlying hypothesis is that the more complex single sentiment problem may be able to be simplified to the semantic similarity of sentiment text pairs. This can then reduce the demand for topic-specific sentiment vocabulary usage (Hammer et al., 2015; Labille et al., 2017). As there is no prior work about same sentiment classification, our work uses well-known approaches from semantic text similarity (STS) about which several shared tasks have been organized (Agirre et al., 2013; Xu et al., 2015; Cer et al., 2017) and a variety of datasets (Dolan and Brockett, 2005; Ganitkevitch et al., 2013) have been compiled. While prior approaches have employed syntactic, structural, and semantic similarity, to evaluate sentence similarity, single models have gained more popularity in recent times. Mueller and Thyagarajan (2016) show the application of siamese recurrent networks for sentence similarity. With the introduction of contextualized word embeddings, Ranasinghe et al. (2019) evaluate their impact on STS methods compared to traditional word embeddings in different languages and domains. 3 The Same Sentiment"
2021.findings-emnlp.53,N19-1423,0,0.165806,"rent models and forms of data representation. In recent years, sentiment analysis is increasingly being performed using deep learning approaches (Zhang et al., 2018). Johnson and Zhang (2017) designed a deep pyramid CNN which could efficiently represent long-range associations in text and thus more global information for better sentiment classification. Howard and Ruder (2018) have developed ULMFiT, a simple efficient transfer learning method that achieves improvements for various NLP Tasks such as sentiment classification. Another model that performs well on sentiment classification is BERT (Devlin et al., 2019), where pre-trained language models can be fine-tuned without substantial effort to suit different tasks. Sun et al. (2019) showed that decreasing the learning rate layer-wise and further pre-training enhance the performance of BERT. Another approach from Xie et al. (2019) improves the performance of BERT with the usage of data augmentation. It was shown that another current language model XLNet (Yang et al., 2019) achieves the best results for the sentiment classification task. Based on the idea of the same side stance classification task by Stein et al. (2021) as well as the authorship verif"
2021.findings-emnlp.53,I05-5002,0,0.087556,"verification problem (Koppel and Schler, 2004), our underlying hypothesis is that the more complex single sentiment problem may be able to be simplified to the semantic similarity of sentiment text pairs. This can then reduce the demand for topic-specific sentiment vocabulary usage (Hammer et al., 2015; Labille et al., 2017). As there is no prior work about same sentiment classification, our work uses well-known approaches from semantic text similarity (STS) about which several shared tasks have been organized (Agirre et al., 2013; Xu et al., 2015; Cer et al., 2017) and a variety of datasets (Dolan and Brockett, 2005; Ganitkevitch et al., 2013) have been compiled. While prior approaches have employed syntactic, structural, and semantic similarity, to evaluate sentence similarity, single models have gained more popularity in recent times. Mueller and Thyagarajan (2016) show the application of siamese recurrent networks for sentence similarity. With the introduction of contextualized word embeddings, Ranasinghe et al. (2019) evaluate their impact on STS methods compared to traditional word embeddings in different languages and domains. 3 The Same Sentiment Problem In the following, we will introduce our mod"
2021.findings-emnlp.53,N13-1092,0,0.0380852,"Missing"
2021.findings-emnlp.53,P18-1031,0,0.0193585,"ta: https://github.com/webis-de/EMNLP-21 584 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 584–590 November 7–11, 2021. ©2021 Association for Computational Linguistics much research to improve accuracy using different models and forms of data representation. In recent years, sentiment analysis is increasingly being performed using deep learning approaches (Zhang et al., 2018). Johnson and Zhang (2017) designed a deep pyramid CNN which could efficiently represent long-range associations in text and thus more global information for better sentiment classification. Howard and Ruder (2018) have developed ULMFiT, a simple efficient transfer learning method that achieves improvements for various NLP Tasks such as sentiment classification. Another model that performs well on sentiment classification is BERT (Devlin et al., 2019), where pre-trained language models can be fine-tuned without substantial effort to suit different tasks. Sun et al. (2019) showed that decreasing the learning rate layer-wise and further pre-training enhance the performance of BERT. Another approach from Xie et al. (2019) improves the performance of BERT with the usage of data augmentation. It was shown th"
2021.findings-emnlp.53,P17-1052,0,0.0261317,"roduct, brand, or service (Tedmori and Awajan, 2019). It has importance for businesses, in campaigns, and the financial sector, among others, and as a result, it has undergone 1 Code and data: https://github.com/webis-de/EMNLP-21 584 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 584–590 November 7–11, 2021. ©2021 Association for Computational Linguistics much research to improve accuracy using different models and forms of data representation. In recent years, sentiment analysis is increasingly being performed using deep learning approaches (Zhang et al., 2018). Johnson and Zhang (2017) designed a deep pyramid CNN which could efficiently represent long-range associations in text and thus more global information for better sentiment classification. Howard and Ruder (2018) have developed ULMFiT, a simple efficient transfer learning method that achieves improvements for various NLP Tasks such as sentiment classification. Another model that performs well on sentiment classification is BERT (Devlin et al., 2019), where pre-trained language models can be fine-tuned without substantial effort to suit different tasks. Sun et al. (2019) showed that decreasing the learning rate layer-"
2021.findings-emnlp.53,D18-2012,0,0.0141894,"n and is officially being provided as several JSON files from which we only used general business information, such as category, and the customer reviews with text and ratings. It contains 6,685,900 user reviews about 192,127 businesses,4 in 22 main categories.5 Businesses are mostly assigned a single main category with related subcategories and seldom overlap. Previous general examinations by Asghar (2016) show extreme variance of the number of reviews and businesses between categories. The reviews required no further textual preprocessing as transformer models use a SentencePiece tokenizer (Kudo and Richardson, 2018) to handle arbitrary text input. It should be noted that those models can only handle some predefined sequence lengths, so text sequences after tokenization will be truncated to fit. With a sequence length of 512, we were able to sufficiently cover most review pairs, as the average number of tokens was about 150 for a single review. Training Data Generation: For the sequence pair classification, we matched random pairs of reviews about the same business. The star rating of 1 to 5 was translated into binary labels, good or bad, with reviews being considered good if their ranking was above 3 sta"
2021.findings-emnlp.53,W16-1617,0,0.0365296,"Missing"
2021.findings-emnlp.53,D19-1018,0,0.0227037,"ion with a sigmoid binary crossentropy loss function as it performed better than two outputs for classes same or not same. 3.2 Data Acquisition and Preparation For our analysis, we required texts with clear stances or sentiments, with both positive and negative samples about the same topic. As we wanted to do cross-topic comparisons, multiple topics with enough samples for standalone training or finetuning of a model were necessary. Those requirements were fulfilled by the sentiment datasets from the business reviews of the Yelp Dataset Challenge (Asghar, 2016) and Ama585 zon product reviews (Ni et al., 2019).2 The IMDb dataset3 commonly used in sentiment analysis was not useful as it only contained both a single positive and negative review per movie, and was, therefore, more suited for sentiment vocabulary analysis. We chose to focus on the Yelp business review dataset as it contains a variety of categories for cross evaluations and qualitatively better review texts compared to Amazon. The dataset is a snapshot with reviews not older than 14 days at its time of creation and is officially being provided as several JSON files from which we only used general business information, such as category,"
2021.findings-emnlp.53,D14-1162,0,0.0871729,"Missing"
2021.findings-emnlp.53,R19-1115,0,0.0127255,"n approaches from semantic text similarity (STS) about which several shared tasks have been organized (Agirre et al., 2013; Xu et al., 2015; Cer et al., 2017) and a variety of datasets (Dolan and Brockett, 2005; Ganitkevitch et al., 2013) have been compiled. While prior approaches have employed syntactic, structural, and semantic similarity, to evaluate sentence similarity, single models have gained more popularity in recent times. Mueller and Thyagarajan (2016) show the application of siamese recurrent networks for sentence similarity. With the introduction of contextualized word embeddings, Ranasinghe et al. (2019) evaluate their impact on STS methods compared to traditional word embeddings in different languages and domains. 3 The Same Sentiment Problem In the following, we will introduce our model for same sentiment prediction and explain how to prepare training and test data. 3.1 Sequence Pair Classification Model Our approach is based on the sequence pair classification task using the well-known transformer language models. The classification model employs the standard pre-trained BERT model architecture (Devlin et al., 2019) with an additional classification layer, consisting of a dropout of 0.1 an"
2021.findings-emnlp.53,S15-2001,0,0.0528128,"Missing"
barron-cedeno-etal-2010-corpus,clough-etal-2002-building,0,\N,Missing
barron-cedeno-etal-2010-corpus,P02-1020,0,\N,Missing
C10-2115,ambati-etal-2010-active,0,0.0315161,"Missing"
C10-2115,N03-1003,0,0.0248306,"Missing"
C10-2115,clough-etal-2002-building,0,0.519814,"Missing"
C14-1091,C08-3010,0,\N,Missing
C14-1091,W10-0404,0,\N,Missing
C14-1091,P05-3009,0,\N,Missing
C14-1091,P06-1032,0,\N,Missing
C14-1091,P13-4024,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
K18-2001,K18-2015,0,0.053009,"Missing"
K18-2001,Q17-1010,0,0.211935,"Missing"
K18-2001,K18-2010,0,0.0386566,"Missing"
K18-2001,K18-2017,0,0.075361,"Missing"
K18-2001,W06-2920,0,0.453112,"Missing"
K18-2001,K18-2025,0,0.0365994,"Missing"
K18-2001,K18-2005,0,0.120251,"Missing"
K18-2001,K18-2013,1,0.806044,"Missing"
K18-2001,K18-2026,0,0.0321915,"Missing"
K18-2001,K18-2012,0,0.0235436,"above are all intrinsic measures: they evaluate the grammatical analysis task per se, with the hope that better scores correspond to output that is more useful for downstream NLP applications. Nevertheless, such correlations are not automatically granted. We thus seek to complement our task with an extrinsic evaluation, where the output of parsing systems is exploited by applications like biological event extraction, opinion analysis and negation scope resolution. This optional track involves English only. It is organized in collaboration with the EPE initiative;7 for details see Fares et al. (2018). Syntactic Word Alignment The higher segmentation level is based on the notion of syntactic word. Some languages contain multi-word tokens (MWT) that are regarded as contractions of multiple syntactic words. For example, the German token zum is a contraction of the preposition zu “to” and the article dem “the”. Syntactic words constitute independent nodes in dependency trees. As shown by the example, it is not required that the MWT is a pure concatenation of the participating words; the simple token alignment thus does not work when MWTs 4 TIRA: The System Submission Platform Similarly to our"
K18-2001,K18-2003,0,0.040574,"Missing"
K18-2001,K18-2006,0,0.0774162,"Missing"
K18-2001,K18-2014,0,0.0664725,"Missing"
K18-2001,K18-2008,0,0.0697052,"Missing"
K18-2001,L16-1262,1,0.910778,"Missing"
K18-2001,W17-0411,1,0.849881,"and in the system output before comparing them. In the end-to-end evaluation of our task, LAS is re-defined as the harmonic mean (F1 ) of precision P and recall R, where P = #correctRelations #systemNodes (1) R= #correctRelations #goldNodes (2) LAS = 2P R P +R (3) Note that attachment of all nodes including punctuation is evaluated. LAS is computed separately for each of the 82 test files and a macro-average of all these scores is used to rank the systems. 3.2 MLAS: Morphology-Aware Labeled Attachment Score MLAS aims at cross-linguistic comparability of the scores. It is an extension of CLAS (Nivre and Fang, 2017), which was tested experimentally in the 2017 task. CLAS focuses on dependencies between content words and disregards attachment of function words; in MLAS, function words are not ignored, but they are treated as features of content words. In addition, part-of-speech tags and morphological features are evaluated, too. 3.3 BLEX: Bilexical Dependency Score BLEX is similar to MLAS in that it focuses on relations between content words. Instead of morphological features, it incorporates lemmatization in the evaluation. It is thus closer to semantic content and evaluates two aspects of UD annota5 ar"
K18-2001,K18-2022,0,0.0296323,"Missing"
K18-2001,K18-2011,1,0.844373,"Missing"
K18-2001,W17-0412,1,0.901947,"Missing"
K18-2001,L16-1680,1,0.90044,"Missing"
K18-2001,K17-3009,1,0.858784,"Missing"
K18-2001,tiedemann-2012-parallel,0,0.0674866,"at follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers provided with large amounts of freely available data. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. We provided dependency-annotated training and test data, and also large quantities of crawled raw texts. Other language resources are available from third-party servers and we only referred to the respective download sites. 2.1 Training Data: UD 2.2 Training and development data came from the Universal Dependencies (UD) 2.2 collection (Nivre et al., 2018). This year, the official UD release immediately followed the test phase of the shared task. The training and development data were available to the"
K18-2001,K18-2016,0,0.0988933,"Missing"
K18-2001,K18-2019,0,0.110064,"Missing"
K18-2001,K18-2007,0,0.0602044,"Missing"
K18-2001,K18-2004,0,0.103154,"Missing"
P13-1119,C10-2115,1,\N,Missing
P13-1119,P02-1020,0,\N,Missing
reyes-etal-2010-evaluating,E06-2031,0,\N,Missing
reyes-etal-2010-evaluating,W04-2214,0,\N,Missing
reyes-etal-2010-evaluating,P05-3029,0,\N,Missing
reyes-etal-2010-evaluating,W02-1011,0,\N,Missing
reyes-etal-2010-evaluating,strapparava-valitutti-2004-wordnet,0,\N,Missing
reyes-etal-2010-evaluating,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
S15-2097,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S15-2097,H05-1044,0,\N,Missing
S15-2097,P11-1016,0,\N,Missing
S15-2097,R13-1007,0,\N,Missing
S15-2097,J92-4003,0,\N,Missing
S15-2097,C10-2005,0,\N,Missing
S15-2097,S13-2065,0,\N,Missing
S15-2097,D11-1052,0,\N,Missing
S15-2097,P02-1053,0,\N,Missing
S15-2097,W10-0204,0,\N,Missing
S15-2097,W02-1011,0,\N,Missing
S15-2097,S14-2009,0,\N,Missing
S15-2097,S14-2111,0,\N,Missing
S15-2097,S13-2054,0,\N,Missing
S15-2097,P14-1146,0,\N,Missing
S15-2097,S15-2078,0,\N,Missing
S15-2097,N13-1039,0,\N,Missing
S15-2097,S13-2052,0,\N,Missing
W17-4508,D15-1044,0,\N,Missing
W17-4508,N16-1012,0,\N,Missing
W17-4508,P17-1099,0,\N,Missing
W17-4508,D15-1229,0,\N,Missing
W17-5106,walker-etal-2012-corpus,0,\N,Missing
W17-5106,P12-2041,0,\N,Missing
W17-5106,W14-2105,0,\N,Missing
W17-5106,W14-2107,0,\N,Missing
W17-5106,D15-1255,0,\N,Missing
W17-5106,W15-0514,0,\N,Missing
W17-5106,D15-1110,0,\N,Missing
W17-5106,N16-1007,0,\N,Missing
W17-5106,P16-1150,0,\N,Missing
W17-5106,W17-5115,1,\N,Missing
W18-6538,P18-1063,0,0.0194833,"e, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first sentence or some key points are combined together to train the mo"
W18-6538,N16-1012,0,0.032176,"ation ranks among the oldest synthesis tasks of computer science, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first"
W18-6538,K16-1028,0,0.0457146,"Missing"
W18-6538,P17-1099,0,0.0441586,"of computer science, addressing the nowadays almost stereotypical problem of information overload. Traditionally, the task has been tackled within natural language processing and information retrieval by extracting phrases from a to-besummarized text. However, the task draws increasing attention from the machine learning community. Owing to advances in theory, algorithms, and hardware, the training of complex models has become feasible that abstract over the to-be-summarized text. Here, deep generative models have delivered some impressive results (Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Chen and Bansal, 2018). Since these models need substantially large amounts of training data in order to understand and generate natural language text, the availability of suitable corpora is important. The most commonly used datasets for abstractive summarization, namely the Gigaword corpus (Graff and Cieri, 2003) and the CNN Dailymail news dataset (Hermann et al., 2015), comprise short articles from the news domain, representing only one of the many genres of written text. Target summaries in both these corpora are extractive where either the first sentence or some key points are combined"
W18-6538,W17-4508,1,0.896057,"Missing"
W19-8666,W19-8665,0,0.0128685,"valuator script to compute ROUGE scores. Each software and evaluator run on the test set was manually reviewed by organizers for errors and data leakage. After a successful review, https://tac.nist.gov/ 524 5 Evaluating models on TIRA using ROUGE was allowed even after the submission deadline. Thus, a participant’s technical paper may have a variation of the same model with different ROUGE scores, but was not manually evaluated. the scores were shared on a public leaderboard.6 Two participants provided their system descriptions. We did not receive any description for the tldr-bottom-up model. Gehrmann et al. (2019) leveraged fine-tuned language models to generate abstractive summaries. They argue that excessive copying facilitated by the copy-attention mechanism hinders paraphrasing and information compression (abstraction). As part of the TL;DR challenge, they compared two summarization approaches (pseudo-self-attn and transf-seq2seq) demonstrating the effectiveness of transfer learning at generating abstractive summaries. Our manual evaluation confirms that these models generate concise and coherent summaries. Tackling the same problem of excessive copying in pointer-generator models, Choi et al. (201"
W19-8666,W10-0722,0,0.0151451,"ey U for pairwise comparison using Bonferroni correction. 2 unified-pgn 2 38 unified-vae-pgn 4 30 transf-seq2seq 4 27 pseudo-self-attn 12 35 tldr-bottom-up 2 25 seq2seq-baseline 79 14 ts .28 up Sufficiency 2 3 Avg. 60 66 69 53 73 7 Text quality 1 2 2.11 6 2.13 9 2.20 0 1.97 2 2.30 1 1.11 73 68 62 5 8 28 21 8 90 2.52 3 Avg. 26 29 95 90 71 6 1.78 1.78 2.70 2.67 2.29 1.11 0 15 85 2.57 Table 3: Sufficiency and text quality score distribution in the majority category. the models. Furthermore, it may help to identify if non-expert annotators can still produce reliable judgments without a guideline. Gillick and Liu (2010) cautioned that workers have difficulties distinguishing the content of a summary from its text quality. With that in mind, we devised two orthogonal three-level rating scales. With respect to sufficiency, workers could rate a summary as insufficient (incomplete and unrelated to the source text), as barely acceptable (missing the main point, but capturing relevant secondary information), or as sufficient (capturing the main point of the text). In terms of text quality, we distinguished the levels badly written (incoherent or major errors), needs improvement (minor errors breaking the flow, but"
W19-8666,hovy-etal-2006-automated,0,0.0683851,"summarization technology for social media, focusing on abstractive summarization. This paper reports the results of the challenge and describes our manual evaluation of the submissions. Finally, we discuss the expected properties of a good summary after analyzing the comments provided by human annotators. 1 c Hinrich Schützed 2 Related Work Shared tasks on automatic summarization were first introduced at the Document Understanding Conferences (DUC).3 In addition to new summarization technology, equal emphasis was given to formulating strong evaluation measures. Methods such as basic elements (Hovy et al., 2006), pyramid (Nenkova and Passonneau, 2004), and ROUGE (Lin, 2004) were introduced for automatically evaluating the content selection capabilities of the participating systems. Furthermore, Dang (2005) presented the first guideline for manually judging summary quality. In 2008, DUC became a summarization track at the Text Analysis Conference 2 TL;DR, short for “too long; didn’t read”, is a cliché reply bemoaning a post’s excessive length, and has given rise to a practice of adding a summary at the end of long posts, introduced by that same letter sequence or variants thereof. 3 https://www-nlpir."
W19-8666,D18-1208,0,0.0297649,"of news. This can be attributed to the ease of obtaining large amounts of news articles alongside suitable summary ground truth, greatly simplifying the corpus construction. However, the summaries found in the currently widely used corpora are either only highlights directly extracted from news articles, offering little abstraction and no coherent text, or headlines, which are short and not necessarily summaries, albeit occasionally abstractive. Furthermore, the common structure of news articles1 introduces bias, since the lead paragraph usually already captures the most relevant information (Kedzie et al., 2018). To foster the development of robust summarization technology, we need to venture off the beaten track and explore more diverse domains. In this regard, the recently published Webis-TLDR-17 corpus (Völske et al., 2017) provides for the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than"
W19-8666,N04-1019,0,0.0522061,"social media, focusing on abstractive summarization. This paper reports the results of the challenge and describes our manual evaluation of the submissions. Finally, we discuss the expected properties of a good summary after analyzing the comments provided by human annotators. 1 c Hinrich Schützed 2 Related Work Shared tasks on automatic summarization were first introduced at the Document Understanding Conferences (DUC).3 In addition to new summarization technology, equal emphasis was given to formulating strong evaluation measures. Methods such as basic elements (Hovy et al., 2006), pyramid (Nenkova and Passonneau, 2004), and ROUGE (Lin, 2004) were introduced for automatically evaluating the content selection capabilities of the participating systems. Furthermore, Dang (2005) presented the first guideline for manually judging summary quality. In 2008, DUC became a summarization track at the Text Analysis Conference 2 TL;DR, short for “too long; didn’t read”, is a cliché reply bemoaning a post’s excessive length, and has given rise to a practice of adding a summary at the end of long posts, introduced by that same letter sequence or variants thereof. 3 https://www-nlpir.nist.gov/projects/duc/index.html https:/"
W19-8666,D15-1044,0,0.0377217,"st, the example from the WebisTLDR-17 corpus exhibits higher abstraction, abbreviations and composition of multiple facts into single phrases. (TAC)4 with evaluation as an independent task (Automatically Evaluating Summaries of Peers, AESOP). Most of these efforts were limited to extractive summarization on comparably small datasets from specific domains, such as biomedical records, newswire articles, and opinions, since neural text generation had not yet become mainstream, rendering abstractive summarization much more difficult. The first attempt at abstractive summarization was presented by Rush et al. (2015), which resulted in a subsequent surge in neural summarization research yielding promising results—we refer to Shi et al. (2018) for a comprehensive review. However, as most recent models have been evaluated exclusively on news corpora, our knowledge of their full capabilities is still superficial. Through the TL;DR challenge, we hope to close this gap. 4 3 Survey of Submissions Out of 16 registered participants, we received 5 submissions from 3 participants (2 from industry). In addition, we provided a seq2seq-baseline model with 2 layers, bi-LSTM, 256 hidden units and no attention. Participa"
W19-8666,E17-2007,0,0.0375653,"Missing"
W19-8666,P17-1099,0,0.0391249,"e diversity, the unified-vae-pgn model uses a VAE for generating summaries of the extracted important sentences. This multi-stage architecture preserves a substantial amount of key information while generating acceptable summaries as revealed in our manual evaluation. We refer readers to the system description papers for further details. 4 Evaluation via crowdsourcing to evaluate both the sufficiency and the text quality of a generated abstractive summary. Below, after reviewing both approaches, we report on the results of the participating systems. 4.1 We begin with a novelty analysis as per See et al. (2017), calculating the fraction of n-grams in the summary that are absent from the text as its novelty (Table 2). The ground truth has the highest novelty, underlining the abstractive nature of selfauthored summaries. Next, we used ROUGE (Lin, 2004) for automatic evaluation and report the F1scores.7 From Table 2 it is difficult to draw any conclusions just by looking at ROUGE scores. Furthermore, a key issue of ROUGE is that it does not provide any upper bounds for the quality of a summarization system (Schluter, 2017), thus warranting an extensive manual evaluation of the systems. Model ROUGE 1 2"
W19-8666,W18-6538,1,0.848865,"r the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than generally found in news articles. Table 1 shows a comparison of the nature of ground truth summaries in the news and the social media domain. With permission from its creators, we used this corpus to organize the TL;DR challenge (Syed et al., 2018), inviting summarization researchers to test existing models as well as new ones. To ensure reproducibility as well as blind and semi-automatic evaluation, we adopted the cloud-based evaluation platform TIRA (Potthast et al., 2019). In addition to the automatic ROUGE metrics, we evaluate the submissions manually for summary effectiveness and text quality via crowdsourcing. In this paper, we report our findings, discuss what annotators consider when scoring summaries, and outline future directions for abstractive summarization research. With most summarization research focused on the news domai"
W19-8666,W17-4508,1,0.852321,"widely used corpora are either only highlights directly extracted from news articles, offering little abstraction and no coherent text, or headlines, which are short and not necessarily summaries, albeit occasionally abstractive. Furthermore, the common structure of news articles1 introduces bias, since the lead paragraph usually already captures the most relevant information (Kedzie et al., 2018). To foster the development of robust summarization technology, we need to venture off the beaten track and explore more diverse domains. In this regard, the recently published Webis-TLDR-17 corpus (Völske et al., 2017) provides for the first English summarization corpus from the domain of 1 Adobe Research d Martin Potthasta LMU Munich social media, consisting of 3 million posts alongside so-called TL;DR summaries.2 The summaries found in this corpus are true summaries provided by the authors of a post, they often abstract over a subject matter, and they cover a much wider range of topics than generally found in news articles. Table 1 shows a comparison of the nature of ground truth summaries in the news and the social media domain. With permission from its creators, we used this corpus to organize the TL;DR"
