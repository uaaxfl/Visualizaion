2018.gwc-1.22,P09-4002,1,0.667287,"allows for adding new types of lexicographic files and annotation with semantic domains. The former facilitates wordnet editing (e.g. the extension includes verb classes used in plWordNet), while the latter supports applications. The domains are based on WordNet Domains (Bentivogli et al., 2004), but we plan to manually modify and expand this classification. 6.2 Portuguese Wordnet As WordnetLoom is getting consolidated, it can be used to help the construction of wordnets other than just plWordNet. This is what is happening with the MultiWordnet of Portuguese, a quality wordnet for Portuguese (Branco et al., 2009). This Portuguese wordnet is a project started in 2004 as a branch of Multi-WordNet (Pianta et al., 2002), which until now gathered seven different languages (English, Hebrew, Italian, Latin, Portuguese, Romanian and Spanish), and was one of the first consistent initiatives pursuing the goal of establishing a multilingual wordnet that remains open for further languages. The wordnets in these languages, were transitively aligned with each other by resorting to its alignment to Princeton WordNet, whose format all are following, and thus having English as the pivot language. This pilot applicatio"
2018.gwc-1.22,E12-1059,0,0.0602801,"Missing"
2018.gwc-1.22,henrich-hinrichs-2010-gernedit,0,0.0319185,"). A web-based system sloWTool (Fišer and Novak, 2011, Fišer and Sagot, 2015) offers good UI and visual wordnet browsing and editing. However, presentation is always limited to a small fragment of the wordnet graph (up to two links distance) and there is no means for neither viewing larger parts, nor comparing different parts. Visualisation of wordnet graphs in most tools follows a radial pattern: a synset in focus is presented in the middle and all links, irrespectively of their types are placed radially around the central element, e.g. sloWTool or WordTies (Pedersen et al., 2012). GernEdiT (Henrich and Hinrichs, 2010) offers visualisation of the wordnet structure in the range selected by the user, but it is hierarchical and focused mainly on hypernymy. Moreover the visual presentation does not allow for direct editing of the structures. WordnetLoom introduced elaborated presentation of the relation graph and direct visual editing (Piasecki et al., 2013). As it is an open tool, it was used as a basis for the solution presented in this paper. 3 Basic Assumptions WordnetLoom 1.0 has been used for plWordNet development since 2005 and proved to be a generally useful system. Thus, although software architecture"
2018.gwc-1.22,R13-1058,1,0.838614,"inning of the plWordNet project in the year 2005, we developed a wordnet editing system, called WordnetLoom in order to avoid problems with manual editing of wordnet representation. It was based on a database and Graphical User Interface (GUI), and separated users from the internal representation of the wordnet. As plWordNet was developed by a team of linguists, it was important to provide distributed access to the system. WordnetLoom has been constructed in a way providing support for the corpus-based wordnet de1 A triple: lemma, Part of Speech, sense id. velopment method used for plWordNet (Maziarz et al., 2013); i.e. enabling close association between editors’ decisions and language data, the use of substitution tests and application of semiautomatic methods as tools for editors. An unique feature of WordnetLoom is the possibility to simultaneously browse and edit wordnet graphs directly on the screen. Nevertheless, WordnetLoom was based on a quite inefficient thick client model, as well as it had restricted expressiveness of the applied wordnet representation and limited possibilities to adapt UI to the format extensions. Moreover, WordnetLoom was initially designed to support a monolingual wordnet"
2018.gwc-1.22,P15-4013,0,0.0311776,"Missing"
2019.gwc-1.32,C12-3044,0,0.303075,"Missing"
2019.gwc-1.32,N15-1165,0,0.0692974,"Missing"
2019.gwc-1.32,L18-1382,1,0.870611,"Missing"
2019.gwc-1.32,W18-3016,1,0.723499,"Missing"
2020.coling-main.354,N15-1165,0,0.0715513,"Missing"
2020.coling-main.354,S12-1019,0,0.0854188,"Missing"
2020.coling-main.354,D14-1162,0,0.099326,"Missing"
2020.iwltp-1.6,W16-3412,1,0.809538,"The contents of the input files in different formats are first extracted, followed by automated language identification which allows the different text files to be grouped by language.14 Within each file, the text is then split into separate sentences, to allow further processes to apply. Each sentence is then pre-processed, which mainly includes tokenisation and truecasing; these operations are performed with scripts that are part of the Moses toolkit15 (Koehn et al., 2007). All document pairs with content in different languages are then automatically aligned with the DOCAL document aligner (Etchegoyhen and Azpeitia, 2016). For all document pairs whose alignment score indicates that the documents are a translation of each other, sentence alignment is then performed on the content, retrieving translations at the sentence level.16 From the aligned sentences a translation memory in TMX format 1.4b is then gener• NationalOrganisations: This group includes all registered users of the NRS from a specific country and resources shared with this group are accessible to all registered users of the NRS based in that Member State. • NationalOrganisations+EuropeanCommission: This group includes all registered users of the N"
2020.iwltp-1.6,P07-2045,0,0.00526572,"le documents containing translations in two or more languages. This is the most complex scenario and its main steps are summarised below.13 The contents of the input files in different formats are first extracted, followed by automated language identification which allows the different text files to be grouped by language.14 Within each file, the text is then split into separate sentences, to allow further processes to apply. Each sentence is then pre-processed, which mainly includes tokenisation and truecasing; these operations are performed with scripts that are part of the Moses toolkit15 (Koehn et al., 2007). All document pairs with content in different languages are then automatically aligned with the DOCAL document aligner (Etchegoyhen and Azpeitia, 2016). For all document pairs whose alignment score indicates that the documents are a translation of each other, sentence alignment is then performed on the content, retrieving translations at the sentence level.16 From the aligned sentences a translation memory in TMX format 1.4b is then gener• NationalOrganisations: This group includes all registered users of the NRS from a specific country and resources shared with this group are accessible to a"
2020.iwltp-1.6,L18-1213,1,0.837898,"Missing"
2020.lrec-1.106,barreto-etal-2006-open,1,0.649652,"tokens, which only partly coincide with the texts in BDCam˜oes (6 texts, with about 159,000 words). LT Corpus—Corpus de Textos Liter´arios (G´en´ereux et al., 2012) is a literary corpus containing 70 documents published between the mid-19th century and the 1940’s of the 20th century. While similar in design to and complementing BDCam˜oes, it covers a shorter time span, has less variety of genres, fewer authors, and is smaller in size, with 1.8 million words, which only partly coincide with the texts in BDCam˜oes (23 texts, with about 897,000 words). CINTIL—Corpus Internacional do Portuguˆes (Barreto et al., 2006) is a linguistically interpreted corpus with 1 million tokens, mostly from anonymized excerpts of news articles, but also including some works of fiction, and transcriptions of formal and informal speech. It is annotated with a variety of manually verified linguistic information, including morphological information and part-of-speech tags. Its texts are only from a recent period and it lacks some metadata items, such as information on the author, that would be necessary for some types of studies. 3 http://beta.clul.ul.pt/teitok/cta/ 3. 3.1. Corpus description Documents gathering methodology Th"
2020.lrec-1.106,E06-2024,1,0.653703,"vailable in Figure 1: XML structure of a document in BDCam˜oes the Digital Library of Cam˜oes, making them available for various types of studies. Researchers interested in a particular set of authors, genre or time period will then be able to take the BDCam˜oes corpus as a resource in which the relevant documents may be found. 3.3. Metadata and linguistic annotation All the documents written in Modern Portuguese, or which are older but whose edition has been transcribed into that orthographic norm, have been automatically parsed with state-of-the-art language processing tools for Portuguese (Branco and Silva, 2006), and thus annotated with linguistic information that follows from the design of these tools and that can be found in detail in their guidelines and documentation (Branco et al., 2015). This subcorpus forms the BDCam˜oes Treebank with 4,495,379 tokens, of which 3,456,396 belong to the public domain part and 1,038,983 belong to restricted part of the corpus. The resulting linguistic annotation comprises part-ofspeech tags (e.g. PREP, ADV, etc.), morphology (lemmas for words from the open categories; gender and number for words from nominal categories; tense, aspect, person and number for verbs)"
2020.lrec-1.106,de-marneffe-etal-2014-universal,0,0.0296811,"Missing"
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.lrec-1.598,2019.gwc-1.32,1,0.722388,"n was performed only for a small sample of the relations. Both these wordnets are very large, by virtue of the automatic processes used to gather their contents, but on the 4860 flip side they lack the thorough validation only granted by a manual process. Another wordnet for Portuguese, WordNet.PT (Marrafa et al., 2006), presents opposite characteristics. Developed at CLUL, the Center for Linguistics of the University of Lisbon, it has been manually built, but is reported to contain only 19,000 expressions. Besides this, this wordnet is not distributed for reuse. To the best of our knowledge (Branco et al., 2019; De Paiva et al., 2016), MWN.PT differs from other wordnets for Portuguese in that it is the only high quality, manually validated and cross-lingually integrated, large-scale wordnet available for this language. 3. Projection The wordnet being presented in this paper has been built according to the expand model along two major devlopment campaigns. A first construction phase took place in the context of the MultiWordNet project (Pianta et al., 2002), leading to an initial, small sized version that is part of the collection of wordnets that resulted from that project. That inaugural version co"
2020.lrec-1.598,C12-3044,0,0.0334482,"Missing"
2020.lrec-1.598,2016.gwc-1.12,0,0.0477374,"Missing"
2020.lrec-1.598,magnini-cavaglia-2000-integrating,0,0.184054,"Missing"
2020.lrec-1.622,D15-1075,0,0.020107,"out of the current paper. These reports may, however, be found at (NLX, 2019). GIST (Choi and Lee, 2018) was the best system on the ARCT, with 0.712 accuracy, 0.175 over the baseline. The reference paper described the development score, the use of a distributed score and the best hyper-parameters. The system consists of LSTMs neural networks and makes use of 10 Note that three metrics are not shown in the table, namely Infrastructure, Empirical run-time and Hyper-parameter search trials, as none of the systems reported them. transfer learning from the natural language inference corpora SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). The source code was made available through a public code repository (GitHub). Although the supporting library requirements were described in the repository, the specific versions of the libraries were not reported. This made the reproduction difficult, due to conflicts with the Theano GPU library (Theano Development Team, 2016) which forced us to resort to a CPU-compatible library instead, with which we were able to run the system (though certainly taking much longer than it would have on a GPU). It took one person less than one working day to reproduce t"
2020.lrec-1.622,S18-1192,0,0.0460678,"Missing"
2020.lrec-1.622,P17-1152,0,0.017888,"hich we attribute to non-deterministic steps in the process, such as weight initialization. Although the reference paper lacked a description of the infrastructure, the number of trials, and the hyper-parameter bounds and choice method, we consider the GIST system to be reproducible, mainly due to the availability of the source code. BLCU NLP (Zhao et al., 2018) was the second-best system on ARCT, with 0.606 accuracy, 0.106 points below the first system. The reference paper reported the development score, the score distribution and the best hyper-parameters. The system is an ensemble of ESIM (Chen et al., 2017) models, which are enhanced LSTM networks that incorporate syntactic information. Source code was made available through a public code repository (GitHub). Although the source code contained a minor problem in its instructions (pointing to a different script for execution) no other problems occur regarding running the source code. The reference paper mentions the use of the best five models for an ensemble majority vote, however, it does not mention the total number of models from which the five best models were selected. We decided to run the system ten times and choose the best five models t"
2020.lrec-1.622,S18-1194,0,0.0225858,"n paper, 7 gave only a summary of the system and 1 system (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU)"
2020.lrec-1.622,S18-1122,0,0.277472,", the trial, the training and development sets with gold labels were made available. In the second phase, the test with the unlabeled test set instances was made available. Competing with the baseline provided by a naive 50-50 random classifier, 21 systems participated.7 The ranking and scores of the systems are presented in Table 2. An upper bound for performance was found by having humans solving the task for 10 instances. A set of 173 crowdsourced participants set human accuracy at 0.798.8 The naive baseline of 50-50 random choice achieved an accuracy of 0.527. The top three systems, GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018) and ECNU (Tian et al., 2018), achieved an accuracy of 0.712, 0.606 and 0.604, respectively. These results lead the ARCT organizers to conclude that the identification of warrants for arguments is a feasible NLP task. It is important to note that the organizers mentioned the existence of artifacts in the data set that could bias the classifiers, such as negation cues, and provided a solution to fix this problem on the existing corpus. However, these fixes were not implemented for ARCT, and their impact on the results was not fully realised until after the ARCT was"
2020.lrec-1.622,N19-1423,0,0.0865159,"18 Task 12, the Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018b), where the top 3 systems, with very good, close to human performance, were included. In a nutshell, the task consists of a binary decision among two input candidate sentences of which only one is fit to be a premise in the input argument. We were able to reproduce most systems, though with varying degrees of difficulty, and provide a detailed report for each case. On the other hand, we take notice of a problem that was found by Niven and Kao (2019) in the original data set of ARCT when it was used with BERT (Devlin et al., 2019), and of spurious statistical cues that have been shown to bias the results obtained. As a cleaned ARCT data set that fixes this bias problem has been released, we also run the reproduced systems on this revised data set. Given the sharp drop in performance that resulted — to very modest scores below to naive random choice baseline —, the present paper leads to a reassessment of the state-ofthe-art for this task. While highlighting the importance of reproducing previous work, we believe it will help also to foster the revival of the ARCT task. 2. Related Work Scientific reproduction. The chall"
2020.lrec-1.622,S18-1189,0,0.0132018,"oduce From the 21 participating systems, 13 were accompanied with a description paper, 7 gave only a summary of the system and 1 system (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems"
2020.lrec-1.622,D19-1224,0,0.0803662,"de available; and (iii) subjective attributes such as the use of toy problems, paper readability and algorithm difficulty, among others. The attributes that showed a significant positive impact on raising the level of reproducibility were: existence of a formal proof, paper readability, algorithm difficulty, availability of pseudo-code, primary topic, whether the hyper-parameters are specified, amount of computation needed, whether the authors reply to questions, number of equations and number of tables. Machine Learning. Upon assessing the comparability of different Machine Learning systems, Dodge et al. (2019) claim that simply reporting the score on the test set, as it is common practice, is insufficient to deem some systems better than others. In order to address this issue, a reproducibility checklist (based on the NeurIPS Machine Learning Reproducibility Checklist3 ) is presented along with a metric to measure the expected validation performance. Through this checklist, it is possible to arrive at recommendations to enhance the level of reproducibility and replicability. Regarding the reproducibility of experimental results, it is recommended to include the description of the computing infrastr"
2020.lrec-1.622,P13-1166,0,0.40265,"Missing"
2020.lrec-1.622,N18-1175,0,0.372026,"co et al., 2017): Replication, the practice of independently implementing scientific experiments to validate specific findings, is the cornerstone of discovering scientific truth. Related to replication is reproducibility, which is the calculation of quantitative scientific results by independent scientists using the original data sets and methods. The goal of this paper is twofold. On the one hand, we report on the challenges faced and the insights gained from our endeavour in reproducing several systems submitted to the SemEval-2018 Task 12, the Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018b), where the top 3 systems, with very good, close to human performance, were included. In a nutshell, the task consists of a binary decision among two input candidate sentences of which only one is fit to be a premise in the input argument. We were able to reproduce most systems, though with varying degrees of difficulty, and provide a detailed report for each case. On the other hand, we take notice of a problem that was found by Niven and Kao (2019) in the original data set of ARCT when it was used with BERT (Devlin et al., 2019), and of spurious statistical cues that have been shown to bias"
2020.lrec-1.622,S18-1121,0,0.234483,"co et al., 2017): Replication, the practice of independently implementing scientific experiments to validate specific findings, is the cornerstone of discovering scientific truth. Related to replication is reproducibility, which is the calculation of quantitative scientific results by independent scientists using the original data sets and methods. The goal of this paper is twofold. On the one hand, we report on the challenges faced and the insights gained from our endeavour in reproducing several systems submitted to the SemEval-2018 Task 12, the Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018b), where the top 3 systems, with very good, close to human performance, were included. In a nutshell, the task consists of a binary decision among two input candidate sentences of which only one is fit to be a premise in the input argument. We were able to reproduce most systems, though with varying degrees of difficulty, and provide a detailed report for each case. On the other hand, we take notice of a problem that was found by Niven and Kao (2019) in the original data set of ARCT when it was used with BERT (Devlin et al., 2019), and of spurious statistical cues that have been shown to bias"
2020.lrec-1.622,S18-1190,0,0.0615341,"Missing"
2020.lrec-1.622,S18-1182,0,0.15718,"m (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional neural network (CNN), and 1 a"
2020.lrec-1.622,S18-1193,0,0.019482,"ary of the system and 1 system (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional ne"
2020.lrec-1.622,S18-1188,0,0.017024,"show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional neural network (CNN), and 1 a supportvector machine (SVM) classifier. A total of 12 systems report using pre-trained distributional semantic models, the majority o"
2020.lrec-1.622,S18-1183,0,0.0142222,". Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional neural network (CNN), and 1 a supportvector machine (SVM) classifier. A"
2020.lrec-1.622,P14-5010,0,0.00247417,"od; and best: hyper-parameter settings for the best model. dependencies such as numpy, but these dependencies, and their precise versions, had to be determined by trial and error as the paper and source code documentation did not specify that information and using the most recent versions yielded errors. After several version regressions, we settled for a working Tensorflow (1.0.0) and Keras (2.2.4) version. The Python version was also not reported, thus we used 3.6.9 (the most recent stable version at the time of running our experiment). The system makes use of the Stanford CoreNLP pipeline (Manning et al., 2014) to parse its input but, again, the precise version is not specified and had to be determined through inspection of the source code. The system relies on pre-trained Word2Vec embeddings, but their source was not described so we assumed them to be the standard GoogleNews pre-trained vectors (Mikolov et al., 2013). The source code implements several models, but since the documentation does not specify which are used in the ensemble, these had to be determined by inspecting the source code.11 The experiments were ran on 2 Intel(R) Xeon(R) Gold 6152 CPU’s. We obtained a score of 0.583, 0.021 point"
2020.lrec-1.622,C18-1097,0,0.0195849,". Keywords: Reproduction, Replication, Argument Mining, Argument Reasoning, Argument Comprehension, ARCT, SemEval. 1. Introduction The ability to repeat experiments and to reproduce their results is a cornerstone of scientific work and it is necessary to properly validate the findings that are published. Several examples of failed replication efforts, however, have been documented in different scientific fields (Branco et al., 2017). For Natural Language Processing (NLP), failure to reproduce results has been reported for WordNet similarity measures (Fokkens et al., 2013), sentiment analysis (Moore and Rayson, 2018), PoS tagging and named entity recognition (Reimers and Gurevych, 2017), among others. The importance of open challenges that foster the reproduction of research results has been underlined (Fokkens et al., 2013; Branco, 2013) as part of the solution for the so-called replication crisis (Hutson, 2018). The community gathered around the science and technology of language has responded through initiatives such as the 4REAL workshops (Branco et al., 2017; Branco et al., 2018), and the forthcoming ReproLang cooperative shared task (ReproLang, 2019) of the LREC2020 conference. The discussion regard"
2020.lrec-1.622,S18-1185,0,0.137288,"ur system”. 4.1. Selecting what to reproduce From the 21 participating systems, 13 were accompanied with a description paper, 7 gave only a summary of the system and 1 system (Joker) did not provide any description. Only 5 participants made the source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php"
2020.lrec-1.622,P19-1459,0,0.125652,"ined from our endeavour in reproducing several systems submitted to the SemEval-2018 Task 12, the Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018b), where the top 3 systems, with very good, close to human performance, were included. In a nutshell, the task consists of a binary decision among two input candidate sentences of which only one is fit to be a premise in the input argument. We were able to reproduce most systems, though with varying degrees of difficulty, and provide a detailed report for each case. On the other hand, we take notice of a problem that was found by Niven and Kao (2019) in the original data set of ARCT when it was used with BERT (Devlin et al., 2019), and of spurious statistical cues that have been shown to bias the results obtained. As a cleaned ARCT data set that fixes this bias problem has been released, we also run the reproduced systems on this revised data set. Given the sharp drop in performance that resulted — to very modest scores below to naive random choice baseline —, the present paper leads to a reassessment of the state-ofthe-art for this task. While highlighting the importance of reproducing previous work, we believe it will help also to foste"
2020.lrec-1.622,D14-1162,0,0.0895797,"Missing"
2020.lrec-1.622,D17-1035,0,0.120817,"asoning, Argument Comprehension, ARCT, SemEval. 1. Introduction The ability to repeat experiments and to reproduce their results is a cornerstone of scientific work and it is necessary to properly validate the findings that are published. Several examples of failed replication efforts, however, have been documented in different scientific fields (Branco et al., 2017). For Natural Language Processing (NLP), failure to reproduce results has been reported for WordNet similarity measures (Fokkens et al., 2013), sentiment analysis (Moore and Rayson, 2018), PoS tagging and named entity recognition (Reimers and Gurevych, 2017), among others. The importance of open challenges that foster the reproduction of research results has been underlined (Fokkens et al., 2013; Branco, 2013) as part of the solution for the so-called replication crisis (Hutson, 2018). The community gathered around the science and technology of language has responded through initiatives such as the 4REAL workshops (Branco et al., 2017; Branco et al., 2018), and the forthcoming ReproLang cooperative shared task (ReproLang, 2019) of the LREC2020 conference. The discussion regarding the reproduction of scientific experiments faces an additional, non"
2020.lrec-1.622,S18-1184,0,0.15679,"els were made available. In the second phase, the test with the unlabeled test set instances was made available. Competing with the baseline provided by a naive 50-50 random classifier, 21 systems participated.7 The ranking and scores of the systems are presented in Table 2. An upper bound for performance was found by having humans solving the task for 10 instances. A set of 173 crowdsourced participants set human accuracy at 0.798.8 The naive baseline of 50-50 random choice achieved an accuracy of 0.527. The top three systems, GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018) and ECNU (Tian et al., 2018), achieved an accuracy of 0.712, 0.606 and 0.604, respectively. These results lead the ARCT organizers to conclude that the identification of warrants for arguments is a feasible NLP task. It is important to note that the organizers mentioned the existence of artifacts in the data set that could bias the classifiers, such as negation cues, and provided a solution to fix this problem on the existing corpus. However, these fixes were not implemented for ARCT, and their impact on the results was not fully realised until after the ARCT was concluded. We will come back to this issue in Section 5. 4"
2020.lrec-1.622,N18-1101,0,0.167395,"ports may, however, be found at (NLX, 2019). GIST (Choi and Lee, 2018) was the best system on the ARCT, with 0.712 accuracy, 0.175 over the baseline. The reference paper described the development score, the use of a distributed score and the best hyper-parameters. The system consists of LSTMs neural networks and makes use of 10 Note that three metrics are not shown in the table, namely Infrastructure, Empirical run-time and Hyper-parameter search trials, as none of the systems reported them. transfer learning from the natural language inference corpora SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). The source code was made available through a public code repository (GitHub). Although the supporting library requirements were described in the repository, the specific versions of the libraries were not reported. This made the reproduction difficult, due to conflicts with the Theano GPU library (Theano Development Team, 2016) which forced us to resort to a CPU-compatible library instead, with which we were able to run the system (though certainly taking much longer than it would have on a GPU). It took one person less than one working day to reproduce this system. Running the system with t"
2020.lrec-1.622,S18-1040,0,0.0126281,"e source code available. Overall, the participating systems show little variability regarding the machine learning methods resorted to, given that all the systems were based on neural networks, except one classifier that used support-vector machines. As for the 7 GIST (Choi and Lee, 2018), BLCU NLP (Zhao et al., 2018), ECNU (Tian et al., 2018), NLITrans (Niven and Kao, 2018), Joker, YNU Deep (Ding and Zhou, 2018), mingyan, ArcNet, UniMelb (Joshi et al., 2018), TRANSRW (Chen et al., 2018), lyb3b (Li and Zhou, 2018), SNU iDS (Kim et al., 2018), ArgEnsGRU, ITNLP-ARC (Liu et al., 2018), YNU-HPCC (Zhang et al., 2018), TakeLab (Brassard et al., 2018), HHU (Liebeck et al., 2018), Deepfinder, ART, RW2C and ztangfdu. Systems that lack a citation did not provide a reference paper. 8 If participants with extensive prior knowledge of the task are used, human accuracy jumps to 0.909, with 30 participants solving the task perfectly. 9 http://alt.qcri.org/semeval2018/index. php?id=papers underlying model, 14 systems used a long short-term memory (LSTM) model, 1 a gated recurrent unit (GRU) model, 2 a convolutional neural network (CNN), and 1 a supportvector machine (SVM) classifier. A total of 12 systems report usi"
2020.lrec-1.622,S18-1186,0,0.039955,"Missing"
2020.lrec-1.680,P18-2005,0,0.0604696,"Missing"
2020.lrec-1.680,P18-3010,0,0.0204747,"Missing"
2020.lrec-1.680,P18-1246,0,0.0988842,"Missing"
2020.lrec-1.680,D16-1034,0,0.0641556,"Missing"
2020.lrec-1.680,P18-1197,0,0.0391306,"Missing"
2020.lrec-1.680,P18-1031,0,0.0492424,"Missing"
2020.lrec-1.680,W18-3026,0,0.0278988,"Missing"
2020.lrec-1.680,P17-2014,0,0.0241643,"Missing"
2020.lrec-1.680,S18-1112,0,0.196409,"Missing"
2020.lrec-1.680,W18-0515,0,0.0590939,"Missing"
2021.emnlp-main.113,2020.acl-main.703,0,0.486964,"s 1504–1521 c November 7–11, 2021. 2021 Association for Computational Linguistics lect four prominent commonsense reasoning tasks, namely Argument Reasoning Comprehension Task (ARCT) (Habernal et al., 2018), AI2 Reasoning Challenge (ARC) (Clark et al., 2018), Physical IQA (PIQA) (Bisk et al., 2020), and CommonsenseQA (CSQA) (Talmor et al., 2019)). We adopt the successful Transformer architecture and resort to prominent pre-trained language models: the encoder-only RoBERTa (Liu et al., 2019b), the decoder-only GPT-2 (Radford et al., 2019), the encoder-decoder T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), and the neurosymbolic COMET(BART) (Hwang et al., 2021)). And we set on to gain insight with stress experiments that seek to find evidence that hopefully permits to advance in answering questions like the following: Are the models taking into account the actual input as a whole? Or could the models be only looking at certain parts of the input, therefore not performing the underlying task but some derivative. How robust are the models? Can they withstand adversarial attacks on the basis of powerful generalization they gained during training, or are they brittle and crumble under attack? How w"
2021.emnlp-main.113,2021.ccl-1.108,0,0.0825837,"Missing"
2021.emnlp-main.113,2020.findings-emnlp.341,0,0.02943,"Missing"
2021.emnlp-main.113,2020.emnlp-demos.16,0,0.0987421,"Missing"
2021.emnlp-main.113,P19-1459,0,0.280322,"tate-of-the-art NLP deep learnto the problem at stake or just taking advaning models genuinely grasp the underlying tasks tage of incidental shortcuts in the data items. they are handling is a debated topic. Recently, an The results obtained indicate that most increasing number of published experiments indidatasets experimented with are problematic, cate that models may be latching at spurious cues with models resorting to non-robust features present in the data (Zech et al., 2018; Geirhos et al., and appearing not to be learning and generalizing towards the overall tasks intended to be 2020; Niven and Kao, 2019), implying that they conveyed or exemplified by the datasets. will severely lack generalization capacity when presented with out-of-distribution data. 1 Introduction As they become more refined, pre-trained language models are continuously closing the gap to Reasoning helps humans to cope with their experience, whether in a complex situation such as de- humans in commonsense reasoning tasks (Zhou vising a plan to solve a pandemic or in a simple, et al., 2020; Tamborrino et al., 2020; Lourie et al., intuitive inference like what will happen to a cof- 2021). In light of the skepticism about the"
2021.emnlp-main.113,S18-2023,0,0.0581942,"Missing"
barreto-etal-2006-open,A00-1031,0,\N,Missing
barreto-etal-2006-open,branco-silva-2004-evaluating,1,\N,Missing
branco-etal-2008-lx,branco-silva-2004-evaluating,1,\N,Missing
branco-etal-2010-developing,W00-1908,0,\N,Missing
branco-etal-2010-developing,J93-2004,0,\N,Missing
branco-etal-2010-developing,brants-2000-inter,0,\N,Missing
branco-etal-2010-developing,C02-2025,0,\N,Missing
branco-etal-2010-developing,P06-4017,0,\N,Missing
branco-etal-2010-developing,W08-2224,1,\N,Missing
branco-etal-2010-developing,J08-4004,0,\N,Missing
branco-etal-2010-developing,J05-1004,0,\N,Missing
branco-etal-2010-developing,simov-etal-2002-building,0,\N,Missing
branco-etal-2012-propbank,J93-2004,0,\N,Missing
branco-etal-2012-propbank,S07-1018,0,\N,Missing
branco-etal-2012-propbank,W02-1502,0,\N,Missing
branco-etal-2012-propbank,W05-0620,0,\N,Missing
branco-etal-2012-propbank,J05-1004,0,\N,Missing
C00-1016,C88-1026,0,0.120438,"Missing"
C00-1016,C90-3022,0,0.0498472,"Missing"
C00-1016,C88-1060,0,0.121139,"Missing"
C00-1016,E91-1008,0,0.161195,"Missing"
C00-1016,C86-1156,0,\N,Missing
C00-1016,E95-1025,0,\N,Missing
C00-1016,E95-1002,0,\N,Missing
C00-1016,P90-1014,0,\N,Missing
C00-1016,P89-1032,0,\N,Missing
C96-1027,E95-1002,0,0.0683342,"Missing"
C98-1027,E91-1008,0,\N,Missing
C98-1027,E95-1002,0,\N,Missing
C98-1027,C88-1060,0,\N,Missing
C98-1027,P90-1014,0,\N,Missing
C98-1027,J95-2003,0,\N,Missing
C98-1027,P89-1032,0,\N,Missing
C98-1027,C88-1026,0,\N,Missing
C98-1027,C90-3022,0,\N,Missing
costa-branco-2012-timebankpt,W01-1315,0,\N,Missing
costa-branco-2012-timebankpt,S10-1076,0,\N,Missing
costa-branco-2012-timebankpt,W01-1311,0,\N,Missing
costa-branco-2012-timebankpt,S07-1098,0,\N,Missing
costa-branco-2012-timebankpt,P10-1069,1,\N,Missing
costa-branco-2012-timebankpt,S10-1010,0,\N,Missing
costa-branco-2012-timebankpt,D08-1073,0,\N,Missing
E06-2024,A00-1031,0,0.0895081,"Missing"
E06-2024,C90-3030,0,0.0434006,"Missing"
E06-2024,branco-silva-2004-evaluating,1,\N,Missing
E12-1027,barreto-etal-2006-open,1,0.824336,"and simple past forms (both imperfective). It compares the number of hits a for fazia “did” to the number of b . hits b for estava a fazer “was doing”: a+b Assuming the progressive construction is a function from processes to states (see Section 2), the higher this value, the more likely the verb can occur with the interpretation of a process. Extracting the Aspectual Indicators 2 We extracted the 4,000 most common verbs from a 180 million word corpus of Portuguese newspaper text, CETEMP´ublico. Because this corpus is not annotated, we used a part-of-speech tagger and morphological analyzer (Barreto et al., 2006; Silva, 2007) to detect verbs and to obtain their dictionary form. We then used an inflection We expect this frequency to be indicative of states because states can appear in the imperfective past tense with their interpretation unchanged, whereas non-stative events have their interpretation shifted to a stative one in that context (e.g. they get a habitual reading). In order to refer to an event occurring in the past with an on-going interpretation, non-stative verbs require the progressive construction to be used in Portuguese, whereas states do not. Therefore, states should occur more free"
E12-1027,P09-4002,1,0.843521,"classification, our approach is to check whether incorporating that kind of information into existing solutions for this problem can improve their performance. TimeML annotated data, such as those used for TempEval, can be used to train machine learned classifiers. These can then be augmented with attributes encoding aspectual type information and their performance compared to the original classifiers. Additionally, we work with Portuguese data. This is because our work is part of an effort to implement a temporal processing system for Portuguese. We briefly describe the data next. 269 tool (Branco et al., 2009) to generate the specific verb forms that are used in the queries. They are mostly third person singular forms of several different tenses. The indicators that we used are ratios of Google Hits. They compare two queries. Several indicators were tested. We provide examples with the verb fazer “do” for the queries being compared by each indicator. The name of each indicator reflects the aspectual type being tested, i.e. states should present high values for State Indicators 1 and 2, processes should show high values for Process Indicators 1–4, etc. &lt;s>Em Washington, &lt;TIMEX3 tid=""t53"" type=""DATE"""
E12-1027,W04-3205,0,0.0324881,"992) is one of the earliest studies where specific textual patterns are used to extract lexicosemantic information from very large corpora. The author’s goal was to extract hyponymy relations. With the same goal, Kozareva et al. (2008) apply similar textual patterns to the web. The web has been used as a corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. Ravichandran and Hovy (2002), Etzioni et al. (2004), etc. Some of this work is specially relevant to the problem of temporal information processing. VerbOcean (Chklovski and Pantel, 2004) is a database of web mined relations between verbs. Among other kinds of relations, it includes typical precedence relations, e.g. sleeping happens before waking up. This type of information has in fact been used by some of the participating systems of TempEval-2 (Ha et al., 2010), with good results. More generally, there is a large body of work focusing on lexical acquisition from corpora. Just as an example, Mayol et al. (2005) learn subcategorization frames of verbs from large amounts of data. Relevant to our work is that of Siegel and McKeown (2000). The authors guess the aspectual type o"
E12-1027,P10-1069,1,0.871675,"Missing"
E12-1027,costa-branco-2012-timebankpt,1,0.841334,"Missing"
E12-1027,S10-1076,0,0.0233761,"en used as a corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. Ravichandran and Hovy (2002), Etzioni et al. (2004), etc. Some of this work is specially relevant to the problem of temporal information processing. VerbOcean (Chklovski and Pantel, 2004) is a database of web mined relations between verbs. Among other kinds of relations, it includes typical precedence relations, e.g. sleeping happens before waking up. This type of information has in fact been used by some of the participating systems of TempEval-2 (Ha et al., 2010), with good results. More generally, there is a large body of work focusing on lexical acquisition from corpora. Just as an example, Mayol et al. (2005) learn subcategorization frames of verbs from large amounts of data. Relevant to our work is that of Siegel and McKeown (2000). The authors guess the aspectual type of verbs by searching for specific patterns in a one million word corpus that has been syntactically parsed. They extract several linguistic indicators and combine them with machine learning algorithms. The indicators that they extract are naturally different from ours, since they h"
E12-1027,C92-2082,0,0.165894,"ed expression. This methodology is not perfect, since multiple occurrences of the queried expression in the same web page are not reflected in the hit count, and in many cases the hit counts reported by search engines are just estimates and might not be very accurate. Additionally, uncarefully formulated queries can match expressions that are syntactically and semantically very different from what was intended. In any case, it has the advantages of being based on a very large amount of data and not requiring any manual annotation, which can introduce errors. 3.1 The Web as a Very Large Corpus Hearst (1992) is one of the earliest studies where specific textual patterns are used to extract lexicosemantic information from very large corpora. The author’s goal was to extract hyponymy relations. With the same goal, Kozareva et al. (2008) apply similar textual patterns to the web. The web has been used as a corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. Ravichandran and Hovy (2002), Etzioni et al. (2004), etc. Some of this work is specially relevant to the problem of temporal information processing. VerbOcean (Chkl"
E12-1027,S07-1098,0,0.448323,"l) value is 0 and its maximum (actual) value is 1. 5 Evaluation As mentioned before, in order to assess the usefulness of these aspectual indicators for the tasks of temporal relation classification, we checked whether they can improve machine learned classifiers trained for this problem. We next describe the classifiers that were used as the bases for comparison. 5.1 Experimental Setup In order to obtain bases for comparison, we trained machine learned classifiers on the Portuguese corpus TimeBankPT, that is adapted from the TempEval data (see Section 4.1). We took inspiration in the work of Hepple et al. (2007). 271 This was one of the participating systems of TempEval. It used machine learning algorithms implemented in Weka (Witten and Frank, 1999). For our experiments, we used Weka’s implementation of the C4.5 algorithm, trees.J48 (Quinlan, 1993), the RIPPER algorithm as implemented by Weka’s rules.JRip (Cohen, 1995), a nearest neighbors classifier, lazy.KStar (Cleary and Trigg, 1995), a Na¨ıve Bayes classifier, namely Weka’s bayes.NaiveBayes (John and Langley, 1995), and a support vector classifier, Weka’s functions.SMO (Platt, 1998) . We chose these algorithms as they are representative of a wid"
E12-1027,P08-1119,0,0.0219243,"just estimates and might not be very accurate. Additionally, uncarefully formulated queries can match expressions that are syntactically and semantically very different from what was intended. In any case, it has the advantages of being based on a very large amount of data and not requiring any manual annotation, which can introduce errors. 3.1 The Web as a Very Large Corpus Hearst (1992) is one of the earliest studies where specific textual patterns are used to extract lexicosemantic information from very large corpora. The author’s goal was to extract hyponymy relations. With the same goal, Kozareva et al. (2008) apply similar textual patterns to the web. The web has been used as a corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. Ravichandran and Hovy (2002), Etzioni et al. (2004), etc. Some of this work is specially relevant to the problem of temporal information processing. VerbOcean (Chklovski and Pantel, 2004) is a database of web mined relations between verbs. Among other kinds of relations, it includes typical precedence relations, e.g. sleeping happens before waking up. This type of information has in fact been"
E12-1027,S07-1046,0,0.074261,"Missing"
E12-1027,J88-2003,0,0.503061,"a picture, it is the moment when the picture is ready; in the case of to explode, it is the moment of the explosion). There are several reasons to think aspectual 267 type is relevant to temporal information processing. First, these distinctions are related to how long events last: culminations are punctual, whereas states can be very prolonged in time. States are thus more likely to temporally overlap other temporal entities than culminations, for instance. Second, there are grammatical consequences on how events are anchored in time. Consider the following examples, from Ritchie (1979) and Moens and Steedman (1988): (1) When they built the 59th Street bridge, they used the best materials. (2) When they built that bridge, I was still a young lad. The situation of building the bridge is a culminated processed, composed by the process of actively building a bridge followed by the culmination of the bridge being finished. In sentence (1), the event described in the main clause (that of using the best materials) is a process, but in sentence (2) it is a state (the state of being a young lad). Even though the two clauses in each sentence are connected by when, the temporal relations holding between the events"
E12-1027,S07-1108,0,0.0711251,"Missing"
E12-1027,P02-1006,0,0.0335785,"ery large amount of data and not requiring any manual annotation, which can introduce errors. 3.1 The Web as a Very Large Corpus Hearst (1992) is one of the earliest studies where specific textual patterns are used to extract lexicosemantic information from very large corpora. The author’s goal was to extract hyponymy relations. With the same goal, Kozareva et al. (2008) apply similar textual patterns to the web. The web has been used as a corpus by many other authors with the purpose of extracting syntactic or semantic properties of words or relations between them, e.g. Ravichandran and Hovy (2002), Etzioni et al. (2004), etc. Some of this work is specially relevant to the problem of temporal information processing. VerbOcean (Chklovski and Pantel, 2004) is a database of web mined relations between verbs. Among other kinds of relations, it includes typical precedence relations, e.g. sleeping happens before waking up. This type of information has in fact been used by some of the participating systems of TempEval-2 (Ha et al., 2010), with good results. More generally, there is a large body of work focusing on lexical acquisition from corpora. Just as an example, Mayol et al. (2005) learn"
E12-1027,J00-4004,0,0.660211,"al information processing. VerbOcean (Chklovski and Pantel, 2004) is a database of web mined relations between verbs. Among other kinds of relations, it includes typical precedence relations, e.g. sleeping happens before waking up. This type of information has in fact been used by some of the participating systems of TempEval-2 (Ha et al., 2010), with good results. More generally, there is a large body of work focusing on lexical acquisition from corpora. Just as an example, Mayol et al. (2005) learn subcategorization frames of verbs from large amounts of data. Relevant to our work is that of Siegel and McKeown (2000). The authors guess the aspectual type of verbs by searching for specific patterns in a one million word corpus that has been syntactically parsed. They extract several linguistic indicators and combine them with machine learning algorithms. The indicators that they extract are naturally different from ours, since they have access to syntactic structure and we do not, but our data are based on a much larger corpus. 3.2 Textual Patterns as Indicators of Aspectual Type Because of aspectual shift phenomena (see Section 2), full syntactic parsing is necessary in order to determine the aspectual ty"
E12-1027,S07-1014,0,0.110387,"Missing"
E12-1027,S10-1010,0,\N,Missing
E14-2021,razmara-kosseim-2008-answering,0,0.00986981,"Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to group candidate answers that co-occur more often in the collection. 81 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–84, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 4 Systems that take advantage from semantic content to answer List questions (Cardoso et al., 2009), (Hartrumpf and Leveling, 2010), (Dornescu, 2009) achieved good results although all information should be stored in the database. This approach"
E14-2021,P06-1147,0,0.0340654,"and Principe and East Timor. When the information that is needed is nontrivial and it is found spread over several texts, a lot of human effort is required to gather the various separate pieces of data into the desired result, which is not an easy task. Ideally, they would prefer to quickly get a precise answer and go on to 2 Related Work List QA is an emerging topic and few approaches have been developed. The most common approach is to take a QA system for factoid questions and extend it to answer List questions. Some pioneering systems using this approach are (Gaizauskas et al., 2005) and (Wu and Strzalkowski, 2006), which show a low performance. Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct an"
E14-2021,C04-1187,0,0.010333,"tems using this approach are (Gaizauskas et al., 2005) and (Wu and Strzalkowski, 2006), which show a low performance. Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to group candidate answers that co-occur more often in the collection. 81 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–84, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 4 Systems that take advantage from semantic content to answer List questions (Cardoso et al., 2009), (Hartrumpf and Leveli"
goncalves-etal-2012-treebanking,silva-etal-2010-top,1,\N,Missing
goncalves-etal-2012-treebanking,W08-2224,1,\N,Missing
goncalves-etal-2012-treebanking,branco-etal-2010-developing,1,\N,Missing
J02-1001,P98-1027,1,0.700351,"ral language, and the formal tools he developed for the analysis of phase quantification, we showed in Branco (2000) that the four binding constraints can be seen as the effect of four binding quantifiers. These phase quantifiers can be viewed as being expressed by the nominals of the four binding classes, and they quantify over the reference markers organized in the obliqueness order. A full-fledged account of the empirical support and justification for these results, and of their implications, is beyond the scope of this article. For an abridged presentation of the core argument, see Branco (1998). 10 As there are different grammatical frameworks, binding constraints have been specified under different versions. Some differences between versions are due just to this fact that binding constraints are supposed to be accommodated into different grammatical frameworks; some other differences, however, are real differences of specification in the sense that different variants may not have the same empirical coverage or be aimed at predicting the same (un)grammatical constructions. In the Appendix, we present a common and fairly well empirically tested version of binding theory given the cur"
J02-1001,C88-1021,0,0.0892432,"ntegration into grammar processing. The ultimate reason for this is to be found in the original exhaustive coindexation rationale for their specification and verification. As an alternative, we propose an approach which, while permitting a unification-based specification of binding constraints, allows for a verification methodology that helps to overcome previous drawbacks. This alternative approach is based on the rationale that anaphoric nominals can be viewed as binding machines. 1. Introduction Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedents of anaphors divide into filters and preferences. The former exclude impossible antecedents and help to circumscribe the set of antecedent candidates; the latter help to pick the most likely candidate, which will be proposed as the antecedent. Binding constraints are a significant subset of such filters. As they delimit the relative positioning of anaphors and their possible antecedents"
J02-1001,C88-1026,0,0.389125,"on procedure, based on exhaustive coindexation, dates back to Chomsky (1980, Appendix; 1981, Section 3.2.3). The basics of this approach can be outlined as follows: After the grammatical parsing of a sentence with n NPs has been completed, for every parse tree t: a. Indexation: Generate a new, annotated tree by assigning indices to the NPs in t. b. Filtering: Store this annotated tree if the indexation of NPs respects binding constraints; otherwise, delete it. c. Iteration: Repeat (a)–(b) until all type-different assignments of n possibly different indices have been exhausted. As discussed in Correa (1988), this procedure is grossly inefficient: its complexity was shown in Fong (1990) to be of exponential order. Moreover, this approach is conceptually awkward, given that a submodule of the grammar, the set of binding constraints, is not operative during grammatical processing, but functions as an extragrammatical add-on.3 This proposal also disregards the need to interface grammar with systems for reference processing. The input for such systems will not be a grammatical representation to be refined vis-`a-vis the preferences for anaphor resolution, but a forest of differently labeled trees tha"
J02-1001,E95-1025,0,0.032513,"antecedent can be related in accordance with the mode of anaphora determined by the reference-processing system. This semantic relation between anaphorically related reference markers can be represented simply as another DRS condition in the CONDS value. This makes possible a mainstream DRT representation for the resolved anaphoric link, thus building on the substantial number of already worked out solutions available in the literature for DRT-based semantic representation of anaphora.16 This specification of binding theory for HPSG was tested with a computational implementation using ProFIT (Erbach 1995). In this implementation, the relational constraints corresponding to binding principles were straightforwardly encoded by means of Prolog predicates associated to the lexical clauses of anaphoric expressions, and defined in terms of simple auxiliary predicates ensuring the component operations of list appending, list difference, and so on. It is worth noting that some of these predicates have arguments—for example, the LIST-U value, whose value is computed when the whole relevant grammatical representation is built up. This is a consequence of packing nonlocal information in such lists. As in"
J02-1001,P90-1014,0,0.711888,"dix; 1981, Section 3.2.3). The basics of this approach can be outlined as follows: After the grammatical parsing of a sentence with n NPs has been completed, for every parse tree t: a. Indexation: Generate a new, annotated tree by assigning indices to the NPs in t. b. Filtering: Store this annotated tree if the indexation of NPs respects binding constraints; otherwise, delete it. c. Iteration: Repeat (a)–(b) until all type-different assignments of n possibly different indices have been exhausted. As discussed in Correa (1988), this procedure is grossly inefficient: its complexity was shown in Fong (1990) to be of exponential order. Moreover, this approach is conceptually awkward, given that a submodule of the grammar, the set of binding constraints, is not operative during grammatical processing, but functions as an extragrammatical add-on.3 This proposal also disregards the need to interface grammar with systems for reference processing. The input for such systems will not be a grammatical representation to be refined vis-`a-vis the preferences for anaphor resolution, but a forest of differently labeled trees that have to be internally searched and compared with each other by anaphor resolve"
J02-1001,E95-1002,0,0.0306984,"straints can be integrated into grammar easily and in a principled manner. In what follows, we outline how these constraints can be specified and handled in a unification-based grammatical framework such as HPSG. 11 This rationale is in line with the insights of Johnson and Klein (1990) concerning the processing of the semantics of nominals. 12 See Kamp and Reyle (1993) for the notion of reference marker. 10 Branco Binding Machines As a proposal for that integration, we designed an extension to the Underspecified Discourse Representation Theory (UDRT) semantics component for HPSG developed by Frank and Reyle (1995). This component is encoded as the value of the feature CONT(ENT), which is now extended with the feature ANAPH(ORA); see (4). This new feature keeps information about the anaphoric potential of the corresponding nominal n: its subfeature ANTEC(EDENTS) keeps a record of how that potential is updated when the anaphor enters a grammatical construction; and its subfeature R(EFERENCE)-MARK(ER) indicates the reference marker of n, to be contributed to the context. Similarly, and still assuming Pollard and Sag’s (1994) feature geometry as a starting point, the NONLOC value is also extended with a ne"
J02-1001,C90-3022,0,0.0826883,"Missing"
J02-1001,P89-1032,0,0.241614,"d in subsequent elaboration on this issue by Merlo (1993). 4 C-command is a configurational version of the command relation where x c-commands y iff the first branching node that dominates x dominates y (Barker and Pullum 1990). 3 Computational Linguistics Volume 28, Number 1 b. Assign: Take the first index i of the stack copied into the NPj node, and annotate NPj with j = i. c. Collect: Add index j to A in each sister node of NPj . When a local domain border is crossed: d. Reset: Reset B to A ∪ B. This algorithm has been given two different implementations, one by Correa (1988), the other by Ingria and Stallard (1989). Further elaboration by Giorgi, Pianesi, and Satta (1990) and Pianesi (1991) offers a variant in terms of formal language techniques, where the stack copied into pronouns contains the antecedent candidates excluded by Principle B. The “do-it-while-parsing” approach of Correa’s implementation has the advantage of discarding a special-purpose postgrammatical module for binding. Nevertheless, this solution turns out to be dependent on a top-down parsing strategy. On the other hand, while Ingria and Stallard’s implementation is independent of the parsing strategy adopted, its independence comes a"
J02-1001,C88-1060,0,0.170969,"Missing"
J02-1001,J94-4002,0,0.0119285,"ale for their specification and verification. As an alternative, we propose an approach which, while permitting a unification-based specification of binding constraints, allows for a verification methodology that helps to overcome previous drawbacks. This alternative approach is based on the rationale that anaphoric nominals can be viewed as binding machines. 1. Introduction Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedents of anaphors divide into filters and preferences. The former exclude impossible antecedents and help to circumscribe the set of antecedent candidates; the latter help to pick the most likely candidate, which will be proposed as the antecedent. Binding constraints are a significant subset of such filters. As they delimit the relative positioning of anaphors and their possible antecedents in grammatical geometry, these constraints are crucial to restricting the search space for antecedents and enhancing the p"
J02-1001,P98-2143,0,0.0435921,"Missing"
J02-1001,E91-1008,0,0.0341957,"al version of the command relation where x c-commands y iff the first branching node that dominates x dominates y (Barker and Pullum 1990). 3 Computational Linguistics Volume 28, Number 1 b. Assign: Take the first index i of the stack copied into the NPj node, and annotate NPj with j = i. c. Collect: Add index j to A in each sister node of NPj . When a local domain border is crossed: d. Reset: Reset B to A ∪ B. This algorithm has been given two different implementations, one by Correa (1988), the other by Ingria and Stallard (1989). Further elaboration by Giorgi, Pianesi, and Satta (1990) and Pianesi (1991) offers a variant in terms of formal language techniques, where the stack copied into pronouns contains the antecedent candidates excluded by Principle B. The “do-it-while-parsing” approach of Correa’s implementation has the advantage of discarding a special-purpose postgrammatical module for binding. Nevertheless, this solution turns out to be dependent on a top-down parsing strategy. On the other hand, while Ingria and Stallard’s implementation is independent of the parsing strategy adopted, its independence comes at the cost of still requiring a special-purpose postgrammatical parsing modul"
J02-1001,A88-1003,0,0.074136,"ocessing. The ultimate reason for this is to be found in the original exhaustive coindexation rationale for their specification and verification. As an alternative, we propose an approach which, while permitting a unification-based specification of binding constraints, allows for a verification methodology that helps to overcome previous drawbacks. This alternative approach is based on the rationale that anaphoric nominals can be viewed as binding machines. 1. Introduction Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedents of anaphors divide into filters and preferences. The former exclude impossible antecedents and help to circumscribe the set of antecedent candidates; the latter help to pick the most likely candidate, which will be proposed as the antecedent. Binding constraints are a significant subset of such filters. As they delimit the relative positioning of anaphors and their possible antecedents in grammatical geometry"
J02-1001,C86-1156,0,\N,Missing
J02-1001,C98-2138,0,\N,Missing
J02-1001,C98-1027,1,\N,Missing
L16-1001,J10-3004,0,0.0171198,"nal or books, or transcription of oral conversations. Furthermore, corpora with interrogatives are extremely rare, and most of them contain interrogatives that are artificially produced by manipulation over sentences that were originally declarative ones. The multilinguality of this corpus adds also to its importance. In the last years, some corpora were collected composed by chat conversations over the internet. These corpora typically contain informal conversations about personal topics (Forsyth and Martell, 2007). Other corpora are more focused on technical issues such as the LINUX corpus (Elsner and Charniak, 2010), the IPHONE/PHYSICS/PYTHON corpus (Adams, 2008) and the Ubuntu chat corpus (Uthus and Aha, 2013). These corpora differ from the one presented here as they include large amounts of utterances produce under social interaction, although the chats used as sources for these corpora were initially intended only for technical support. In all the referred corpora, the conversation threads involve several participants using an informal register. In almost all the cases (except for the Ubuntu corpus) the language addressed in these corpora is limited to English. The corpus was collected selecting the d"
L16-1094,costa-etal-2014-translation,0,0.0331141,"Missing"
L16-1094,W11-2123,0,0.0171962,"(Silva et al., 2015) developed under the QTLeap project. For the PBSMT system, we used the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 5 We would like to thank Eleftherios Avramidis and Lukas Poustka for making the LibreOffice corpus available to us. 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). All PBSMT systems were tuned on the IT1 corpus using minimum error rate training (MERT) (Och, 2003). Their language models were built on a 2,121,382 sentence corpus (target side of the full EP+FAPESP corpora) using the KenLM (Heafield, 2011) 5-gram language model. The stack size was limited to 100 hypotheses during decoding. For each of the two systems (TectoMT and PBSMT), we performed four baseline experiments (using the EP1, EP2, FAPESP, or IT1 corpus as the training dataset) and five experiments which exploited three different strategies for enlarging the small in-domain training dataset (IT1) by adding: (1) a larger out-of-domain dataset (IT1+EP2 or IT1+FAPESP), (2) quasi in-domain data (IT1+TERM), and (3) a combination of both (IT1+EP2+TERM or IT1+FAPESP+TERM). 594 Group TectoMT better rated than PBSMT TectoMT and PBSMT rate"
L16-1094,N03-1017,0,0.0218682,"ortuguese translation using two different approaches: a hybrid MT system (TectoMT) and a PBSMT system in the Moses toolkit. All models were tested on the same dataset (IT2). For the TectoMT system, we used the English to Portuguese TectoMT system (Silva et al., 2015) developed under the QTLeap project. For the PBSMT system, we used the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 5 We would like to thank Eleftherios Avramidis and Lukas Poustka for making the LibreOffice corpus available to us. 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). All PBSMT systems were tuned on the IT1 corpus using minimum error rate training (MERT) (Och, 2003). Their language models were built on a 2,121,382 sentence corpus (target side of the full EP+FAPESP corpora) using the KenLM (Heafield, 2011) 5-gram language model. The stack size was limited to 100 hypotheses during decoding. For each of the two systems (TectoMT and PBSMT), we performed four baseline experiments (using the EP1, EP2, FAPESP, or IT1 corpus as the training dataset) and five experiments which exploited three different strategies for enlarging the small in-domain training dataset"
L16-1094,P07-2045,0,0.0302813,"and-crafted rules for the synthesis phase of the hybrid MT system being developed. This QTLeap corpus consists of recorded interactions of real users with experts to obtain technical support via chat, which were translated by professional translators into the eight languages of the project.2 We perform a series of MT experiments using two sysˇ tems: (1) the TectoMT (Zabokrtsk´ y et al., 2008) adapted to English-Portuguese translation (Silva et al., 2015), as a hybrid system with hybrid analysis, rule-based synthesis and statistically based transfer, and (2) the standard PBSMT system in Moses (Koehn et al., 2007). We vary the training datasets exploring three different strategies to overcome the problem of small amount of training data: (1) adding larger out-of-domain dataset, (2) adding in-domain bilingual terminology, (3) adding both. The main contribution of this paper is the in-depth error analysis, showing the error patterns in each of the systems (TectoMT and PBSMT) when trained on the same datasets, thus directly contrasting those two approaches and showing the main advantages of the hybrid MT system. We further propose a set of rules for improving the synthesis stage in the TectoMT system to e"
L16-1094,2009.mtsummit-papers.7,0,0.0147301,"ransforming the translated t-tree into an a-tree and then linearise the a-tree into a plain surface form of the output sentence. These modules are language-specific and take care of word order, agreement (e.g. subject-predicate agreement or noun-adjective agreement), insertion of grammatical words (such as prepositions, articles, particles, etc.), inflections, and capitalisation. 2.2. English to Portuguese Machine Translation The studies concerning EN-PT MT are very scarce and mostly report on results of the PBSMT systems. The best results (BLEU = 0.55) were obtained on the JRC-Acquis corpus (Koehn et al., 2009), followed by the results obtained using a significantly smaller FAPESP corpus (Aziz and Specia, 2011) of scientific news texts (BLEU = 0.46). The PBSMT systems trained on Europarl corpus (and interpolated with models trained on datasets from the same domain as the test datasets) and tested on domain-specific corpora – TED talks and TAP (Portuguese airline) magazine – achieved significantly lower BLEU scores, 0.20 and 0.19 respectively (Costa et al., 2014). Google Translate achieved better, but still not very high, BLEU scores (0.28 and 0.26, respectively) on the same task (Costa et al., 2014)"
L16-1094,W04-3250,0,0.112153,"ESP IT1 TERM / / / / / / 162,350 / / / 2,000 / / 2,000 / 162,350 2,000 / / 2,000 14,025 / 2,000 14,025 162,350 2,000 178,375 Total 1,960,407 162,350 162, 350 2,000 164,350 164,350 16,025 178,375 14,025 BLEU score TectoMT PBSMT 19.34 18.99 17.76 17.53 19.43 19.20 20.77 21.55 19.77 20.79 20.53 *22.64 *21.89 *22.73 *21.04 *22.25 *21.63 *23.53 Table 2: Number of sentence pairs (terms and MWE in the case of the TERM corpus) used for the training and the achieved BLEU score. Best results of each system are shown in bold. Results of the systems which significantly (using paired bootstrap resampling (Koehn, 2004)) outperformed all four baselines are shown with an ‘*’. and a small portion of LibreOffice terminology (995 terms).5 Several examples from each corpus are given in Table 1. 3.2. Experimental Setup We performed a series of MT experiments for English to Portuguese translation using two different approaches: a hybrid MT system (TectoMT) and a PBSMT system in the Moses toolkit. All models were tested on the same dataset (IT2). For the TectoMT system, we used the English to Portuguese TectoMT system (Silva et al., 2015) developed under the QTLeap project. For the PBSMT system, we used the GIZA++ i"
L16-1094,2005.mtsummit-papers.11,0,0.121199,"l., 2014). There have been two studies comparing a hybrid and a PBSMT system for EN-PT language pair (Silva et al., 2015; ˇ Stajner et al., 2015), reporting the performances of the two approaches as comparable. None of those studies, however, performs an error analysis to directly compare the errors made by those systems. 3. Machine Translation Experiments The corpora used in MT experiments, experimental setup and the results of the automatic evaluation of all MT systems are presented in the next three subsections. 3.1. Corpora We used six corpora in this study: 1. EP1 – Full Europarl corpus (Koehn, 2005) with English on the source side and Portuguese on the target side (1,960,407 sentence pairs). 2. EP2 – A smaller portion of Europarl corpus which has the same size as the FAPESP corpus (162,350 sentence pairs). 3. FAPESP – A Portuguese-English bilingual collection of the online issue of the scientific news Brazilian magazine “Revista Pesquisa FAPESP”3 (Aziz and Specia, 2011). 4. IT1 – An in-domain IT corpus with 2,000 sentence pairs (1,000 questions and 1,000 answers) compiled under the QTLeap project (QTLeap corpus, batch 1). This corpus was used as the training dataset (or a part of the tra"
L16-1094,W10-1730,0,0.652819,"Missing"
L16-1094,J03-1002,0,0.0100679,"Missing"
L16-1094,P03-1021,0,0.0177018,"oses toolkit. All models were tested on the same dataset (IT2). For the TectoMT system, we used the English to Portuguese TectoMT system (Silva et al., 2015) developed under the QTLeap project. For the PBSMT system, we used the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 5 We would like to thank Eleftherios Avramidis and Lukas Poustka for making the LibreOffice corpus available to us. 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). All PBSMT systems were tuned on the IT1 corpus using minimum error rate training (MERT) (Och, 2003). Their language models were built on a 2,121,382 sentence corpus (target side of the full EP+FAPESP corpora) using the KenLM (Heafield, 2011) 5-gram language model. The stack size was limited to 100 hypotheses during decoding. For each of the two systems (TectoMT and PBSMT), we performed four baseline experiments (using the EP1, EP2, FAPESP, or IT1 corpus as the training dataset) and five experiments which exploited three different strategies for enlarging the small in-domain training dataset (IT1) by adding: (1) a larger out-of-domain dataset (IT1+EP2 or IT1+FAPESP), (2) quasi in-domain data"
L16-1094,P02-1040,0,0.101156,"c¸a˜ o” instead of “ir a codificac¸a˜ o” “escolha a canal” instead of “escolha o canal” “junto de sua perfil” instead of “junto ao seu perfil” “bot˜oes que diz” instead of “dizem” Open Network Broadcast → “Difus˜ao de rede de abrir”, instead of “Abrir Difus˜ao de Rede” Table 4: Classification of adequacy and fluency errors. 3.3. Results of the Translation Experiments The training datasets of each of the nine experiments (performed for both MT systems) are presented in Table 2, together with the results of the automatic evaluation of both systems (TectoMT and PBSMT) in terms of the BLEU score (Papineni et al., 2002). Our results indicated that addition of a larger out-of-domain corpus only improves the performance of the PBSMT system, while it deteriorates the performance of the TectoMT system. The TectoMT achieves best improvements with addition of the bilingual terminology only. 4. Hybrid vs Statistical MT System It is known that BLEU scores cannot be used for comparing two MT systems with different architectures (TectoMT and PBSMT in our case), but only for comparing different versions of the same system (either PBSMT or TectoMT in our case). Therefore, we focused on the results of the IT1+TERM experi"
L16-1094,W15-4101,1,0.850808,"the EN-PT language pair exists for the IT domain, a small ENPT corpus was compiled under the QTLeap project in order to enable in-domain examples and guide the hand-crafted rules for the synthesis phase of the hybrid MT system being developed. This QTLeap corpus consists of recorded interactions of real users with experts to obtain technical support via chat, which were translated by professional translators into the eight languages of the project.2 We perform a series of MT experiments using two sysˇ tems: (1) the TectoMT (Zabokrtsk´ y et al., 2008) adapted to English-Portuguese translation (Silva et al., 2015), as a hybrid system with hybrid analysis, rule-based synthesis and statistically based transfer, and (2) the standard PBSMT system in Moses (Koehn et al., 2007). We vary the training datasets exploring three different strategies to overcome the problem of small amount of training data: (1) adding larger out-of-domain dataset, (2) adding in-domain bilingual terminology, (3) adding both. The main contribution of this paper is the in-depth error analysis, showing the error patterns in each of the systems (TectoMT and PBSMT) when trained on the same datasets, thus directly contrasting those two a"
L16-1094,W15-5713,1,0.813691,"Missing"
L16-1094,W08-0325,0,0.55815,"Missing"
L16-1246,ballesteros-nivre-2012-maltoptimizer-system,0,0.0495771,"Missing"
L16-1246,barreto-etal-2006-open,1,0.760318,"Missing"
L16-1246,W06-2920,0,0.120533,"Missing"
L16-1246,de-marneffe-etal-2006-generating,0,0.156842,"Missing"
L16-1246,de-marneffe-etal-2014-universal,0,0.0285395,"Missing"
L16-1246,E06-1011,0,0.0407046,"ices and Parsing Dependency banks usually represent the dependency structure of a sentence through a tree whose root is the main verb of the sentence, although there are some phenomena whose linguistic analysis might be best represented through a graph structure that is not a tree, like relative structures or the coordination with shared dependents, which is represented using multiple heads. The reason why dependency banks are usually restricted to tree structures is not linguistic, but algorithmic, since finding a parse, if such non-tree structures are allowed, can be an intractable problem (McDonald and Pereira, 2006). Though there has been some research done on extending 1554 Figure 3: A complex predicate: Deveriam ficar em aquele pedestal em o qual foram postas sem nos deixarem aproximar. / They should stay in that pedestal in which they were placed on without letting us approach. Figure 4: Emphatic duplication: Eles v˜ao l´a amanh˜a a o quartel. / Tomorrow they will go there to the barracks. parsing algorithms to handle these more complex structures (McDonald and Pereira, 2006), the available dependency parsers cannot handle them. As such, these sentences are removed from the corpus before training and"
L16-1246,H05-1066,0,0.0238118,"Missing"
L16-1246,petrov-etal-2012-universal,0,0.0360236,"Missing"
L16-1246,P13-4001,0,0.0435072,"Missing"
L16-1246,zeman-2008-reusable,0,0.049108,"Missing"
L16-1438,W13-2201,0,0.0355127,"Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010). The analysis phase (which converts the input sentence into the a-layer and then the t-layer) and the synthesis phase (which converts the translated t-layer representation to the a-layer and then to the output surface string) are mostly rule-based. They use a modular structure, allowing for its components to be inherited, and adapted, for different languages. Figure 1: TectoMT architecture. The English-Czech TectoMT system achieved very good results on the Workshop on Statistical Machine Translation 2013 shared task (Bojar et al., 2013). Those results, together with the widespread concerns that the state-of-theart PBSMT may be reaching a performance ceiling, make us believe that the use of the linguistically rich MT approaches, such as TectoMT, could be a good way forward towards higher quality MT. In order to adapt the English-Czech TectoMT to a new language pair, it is only necessary to adapt the synthesis phase of the original system and train a transfer model (handled by the tree-to-tree maximum entropy translation model (Mareˇcek et al., 2010)) which takes around two weeks to train over the whole Europarl corpus. The bl"
L16-1438,E06-2024,1,0.810503,"d the formeme to help separating the lexical from the syntactic information. For example, the formeme of the semantic noun in a subject position is represented as n:sub. For the morphological categories the grammateme tectogrammatical representation is ˇ used (Zabokrtsk´ y et al., 2008). For the Portuguese synthesis adaptation, the process begun by adapting the existing English blocks (Popel and ˇ Zabokrtsk` y, 2010), with the corresponding Portuguese linguistic phenomena. The next step was to create new blocks for Portuguese-specific phenomena. We resorted to the existing tools developed by (Branco and Silva, 2006) whenever possible, due to the higher accuracy over the available tools within the original TectoMT system. For verbal conjugation and for nominal inflection, we used the LX-Conjugator and LX-Inflector to generate surface forms (described in more details in Section 3.1.). We created new TectoMT blocks to call and incorporate these tools into the TectoMT pipeline. The English to Portuguese TectoMT system was being improved iteratively, controlling for the BLEU score and human error analysis of 1,000 sentences in each step. The set of 1,000 sentences (QTLEAP-IT1) was compiled under the QTLeap pr"
L16-1438,P07-2045,0,0.00679985,"r the translation from English to Portuguese, and discuss the findings of the conducted error analysis. As the test corpus, we use 1, 000 parallel sentences from the IT domain (QTLEAP-IT2), compiled under the QTLeap project in a similar way as the 1, 000 parallel sentences used for the development of the synthesis phase (none of the 1, 000 test sentences can be found among the 1, 000 sentences used for development). For the automatic evaluation, we use the BLEU score (Papineni et al., 2002) and compare our TectoMT system with the standard phrase-based SMT system built using the Moses toolkit (Koehn et al., 2007). Both systems were trained over the full Europarl corpus (Koehn, 2005), consisting of 1,960,407 sentence pairs. The PBSMT was tuned using the QTLEAP-IT1 dataset (the same dataset which was used for the development of the rules for the synthesis phase of the TectoMT system). As can be seen from the results presented in Table 1, our adaptation of TectoMT system to EN→PT translation task significantly outperforms the standard PBSMT system on the IT test set (the difference is statistically significant at a 0.05 level of significance, using the paired bootstrap resampling (Koehn, 2004)). Comparin"
L16-1438,W04-3250,0,0.08534,"it (Koehn et al., 2007). Both systems were trained over the full Europarl corpus (Koehn, 2005), consisting of 1,960,407 sentence pairs. The PBSMT was tuned using the QTLEAP-IT1 dataset (the same dataset which was used for the development of the rules for the synthesis phase of the TectoMT system). As can be seen from the results presented in Table 1, our adaptation of TectoMT system to EN→PT translation task significantly outperforms the standard PBSMT system on the IT test set (the difference is statistically significant at a 0.05 level of significance, using the paired bootstrap resampling (Koehn, 2004)). Comparing to other languages pairs translation (Rosa et al., 2015) using TectoMT, the translation from English to Portuguese falls within the same range of score values. Other Blocks CopyTtree performs a deep-copy of the current transfer ttrees into an a-tree. CliticExceptions handles the clitics in Portuguese. MarkSubject fills the Afun label (analytical functions which correspond to syntactic functions such as subject, predicate, object and attribute) with the Sb attribute marking a subject. The values are read from the formeme. For Portuguese, if the formeme is a possessive noun or a nou"
L16-1438,2005.mtsummit-papers.11,0,0.651924,"in the case of domain-specific MT for which there is not enough parallel data for training a statistical machine translation system. Keywords: Hybrid Machine Translation, Tecto MT, Portuguese Synthesis 1. Introduction Phrase-based statistical machine translation (PBSMT) is considered as state-of-the-art MT approach whenever sufficiently large parallel (or comparable) datasets for training are available. However, for many language pairs and translation directions (English to Portuguese among them) large training datasets only exists for few domains, such as parliamentary discussions (Europarl (Koehn, 2005)) or legal documents (JRC-Acquis corpus (Steinberger et al., 2006)). In such cases, it is assumed that a rule-based or a hybrid MT system lead to better results as it can better overcome the data sparsity and generalise over the unseen word forms (especially useful in the case of morphologically rich languages). The main concern is, however, that adaptation of a rule-based or a hybrid MT system (its rules) to a new language pair or a new domain may require considerable time and effort. In this paper, we focus on a hybrid MT system (TectoMT ˇ (Popel and Zabokrtsk` y, 2010)) and show that the ad"
L16-1438,W10-1730,0,0.655214,"Missing"
L16-1438,P02-1040,0,0.0968438,"report on the automatic evaluation of the full translation pipeline (which contains analysis, transfer and synthesis phases) for the translation from English to Portuguese, and discuss the findings of the conducted error analysis. As the test corpus, we use 1, 000 parallel sentences from the IT domain (QTLEAP-IT2), compiled under the QTLeap project in a similar way as the 1, 000 parallel sentences used for the development of the synthesis phase (none of the 1, 000 test sentences can be found among the 1, 000 sentences used for development). For the automatic evaluation, we use the BLEU score (Papineni et al., 2002) and compare our TectoMT system with the standard phrase-based SMT system built using the Moses toolkit (Koehn et al., 2007). Both systems were trained over the full Europarl corpus (Koehn, 2005), consisting of 1,960,407 sentence pairs. The PBSMT was tuned using the QTLEAP-IT1 dataset (the same dataset which was used for the development of the rules for the synthesis phase of the TectoMT system). As can be seen from the results presented in Table 1, our adaptation of TectoMT system to EN→PT translation task significantly outperforms the standard PBSMT system on the IT test set (the difference"
L16-1438,W15-5711,0,0.420349,"Missing"
L16-1438,steinberger-etal-2006-jrc,0,0.0351353,"not enough parallel data for training a statistical machine translation system. Keywords: Hybrid Machine Translation, Tecto MT, Portuguese Synthesis 1. Introduction Phrase-based statistical machine translation (PBSMT) is considered as state-of-the-art MT approach whenever sufficiently large parallel (or comparable) datasets for training are available. However, for many language pairs and translation directions (English to Portuguese among them) large training datasets only exists for few domains, such as parliamentary discussions (Europarl (Koehn, 2005)) or legal documents (JRC-Acquis corpus (Steinberger et al., 2006)). In such cases, it is assumed that a rule-based or a hybrid MT system lead to better results as it can better overcome the data sparsity and generalise over the unseen word forms (especially useful in the case of morphologically rich languages). The main concern is, however, that adaptation of a rule-based or a hybrid MT system (its rules) to a new language pair or a new domain may require considerable time and effort. In this paper, we focus on a hybrid MT system (TectoMT ˇ (Popel and Zabokrtsk` y, 2010)) and show that the adaptation of the existing TectoMT system for English-Czech language"
L16-1438,W08-0325,0,0.135757,"Missing"
L16-1438,zeman-2008-reusable,0,0.640337,"for the gender concordance. ImposeAttrAgr sets the gender, number and person according to their governing nouns. SecondPersonPoliteness sets politeness person for Portuguese (third person). This occurs when the lemma is represented as a PersPron (all personal and possessive pronouns, including reflexive pronouns) and the corresponding t-node is in the second person. 3.1.6. Inflect Blocks GenerateWordforms generates the corresponding word forms for each lemma by using the LX-Conjugator for the verb nodes and the LX-Inflector for the adjective and nouns nodes. This block also uses the Interset (Zeman, 2008) partof-speech, number, mood, tense, person and lemma. This block also handles the inflection for the superlative degree. GeneratePronouns generates pronouns by using the formeme and the Interset person, gender and number. This block also handles the possessive, dative, accusative and oblique case pronouns. 3.1.7. An example of the a-trees and t-trees is given in the Figure 2. This example also illustrates the use of AddGender and AddPrepos blocks. From the AddGender block the node ’imagem’ gets the tag ’fem’ which will make possible to the block AddPrepos to add the preposition ’a’. In this s"
L16-1441,E09-1005,1,0.672807,"ne translation reported in the literature had resorted to. While we acknowledged that our work was only in a preliminary state – our reported gains were minimal and based on a very controlled evaluation trained on a small, in-domain corpus – we found any improvement at all to be a good outcome, and recognized the need to continue our experimentation on a larger, open-domain corpus in the future. 3. Integrating WSD Output into the Machine Translation Pipeline Our chosen WSD algorithm is UKB, a collection of tools and algorithms for performing graph-based WSD over a pre-existing knowledge base (Agirre and Soroa, 2009; Agirre et al., 2014). Based on the graph-based WSD method pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008), UKB allows for WordNet-style knowledge bases to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between those nodes. By calculating the probability of a ‘random walk’ over the graph from a target word’s node ending on any other node in the graph – with the nodes (senses)"
L16-1441,J14-1003,1,0.88208,"Missing"
L16-1441,P14-1023,0,0.0242498,"calculated from an embedded space obtained using a shallow neural network language model (NNLM) (Mikolov et al., 2013), which have become an important tool in NLP and for semantics in particular in recent years. We used a ‘Skipgram’ model, in which each current word is used as the input of a log-linear classifier with a continuous projection layer that predicts the previous and subsequent words within a defined context window. Skip-grams have been shown to be one of the most accurate models available in a variety of semantics-based NLP tasks, such as word similarity and semantic-relatedness (Baroni et al., 2014). In order to extract the domain-specific thesaurus, an automatically-built corpus of 109 million words was built to 2778 provide informative context to the Skip-gram model, comprising 209,000 articles and documents about computer science and information technology extracted from Wikipedia, plus KDE and OpenOffice manuals. The Skip-gram model is then able to learn – following Harris’ (1954) distributional hypothesis of a word’s semantic features being related to its co-occurrence patterns – word representations as dense scalar vectors of 300 dimensions, with each of these dimensions representi"
L16-1441,P05-1048,0,0.267881,"d token may have multiple distinct meanings. To use a classic example, the word ‘bank’ could be interpreted in the sense of the financial institution or as the slope of land at the side of a river. Some successful approaches to WSD in recent years have been ‘knowledge-based’, with classes of words stored in lexical ontologies such as WordNet (Fellbaum, 1998) where the collective meanings of open-class words (nouns, verbs, adjectives and adverbs) are grouped together as ‘synsets’. While it has long been assumed that an optimally successful MT system must incorporate some kind of WSD component (Carpuat and Wu, 2005), attempts to integrate WSD components into machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by various complete reformulations of the disambiguation process (Carpuat and Wu, 2007; Xiong and Zhang, 2014) – some of which yielded small improvements in translation quality – but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This paper provides furthe"
L16-1441,2007.tmi-papers.6,0,0.197209,"WordNet (Fellbaum, 1998) where the collective meanings of open-class words (nouns, verbs, adjectives and adverbs) are grouped together as ‘synsets’. While it has long been assumed that an optimally successful MT system must incorporate some kind of WSD component (Carpuat and Wu, 2005), attempts to integrate WSD components into machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by various complete reformulations of the disambiguation process (Carpuat and Wu, 2007; Xiong and Zhang, 2014) – some of which yielded small improvements in translation quality – but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This paper provides further evidence to support our previous work (Neale et al., 2015), which experimented in including the output from WSD as contextual features in maxent-based translation models in search of improved performance for machine translation from English to Portuguese. Training our transfer model using a much larger dataset – approximately 1.9"
L16-1441,P07-1005,0,0.0604865,"Missing"
L16-1441,W07-0719,0,0.083017,"Missing"
L16-1441,H05-1052,0,0.0109657,"were minimal and based on a very controlled evaluation trained on a small, in-domain corpus – we found any improvement at all to be a good outcome, and recognized the need to continue our experimentation on a larger, open-domain corpus in the future. 3. Integrating WSD Output into the Machine Translation Pipeline Our chosen WSD algorithm is UKB, a collection of tools and algorithms for performing graph-based WSD over a pre-existing knowledge base (Agirre and Soroa, 2009; Agirre et al., 2014). Based on the graph-based WSD method pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008), UKB allows for WordNet-style knowledge bases to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between those nodes. By calculating the probability of a ‘random walk’ over the graph from a target word’s node ending on any other node in the graph – with the nodes (senses) ‘recommending’ each other and being more or less important based on the importance of the other nodes which recommend them – the most appr"
L16-1441,W15-5708,1,0.469514,"to machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by various complete reformulations of the disambiguation process (Carpuat and Wu, 2007; Xiong and Zhang, 2014) – some of which yielded small improvements in translation quality – but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This paper provides further evidence to support our previous work (Neale et al., 2015), which experimented in including the output from WSD as contextual features in maxent-based translation models in search of improved performance for machine translation from English to Portuguese. Training our transfer model using a much larger dataset – approximately 1.9 million English-Portuguese aligned sentences from Europarl – we find that the very small gains reported previously are now statistically significant, confirming our original hypothesis that adding word senses provided by WSD tools as contextual features of a translation model can improve machine translation performance witho"
L16-1441,L16-1483,1,0.857045,"Missing"
L16-1441,W15-4101,1,0.356594,"nslation, TectoMT breaks down source language input and reconstructs target language output according to four layers of representation: the word layer (raw text), the morphological layer, the analytical layer (shallowsyntax) and the tectogrammatical layer (deep-syntax). The three scenarios needed for machine translation – one for analysis (of the source language), one for transfer (of tectogrammatical nodes from source to target language) and one for synthesis (of the target language) – are constructed from a combination of blocks, for which a pipeline for Portuguese has already been created (Silva et al., 2015). To integrate word senses into the machine translation pipeline as this paper describes, new combinations of blocks were introduced in the analysis scenario to first convert input sentences into the context format needed to perform WSD using UKB – which we run over a graph-based representation of the English WordNet (Fellbaum, 1998). Another block runs UKB over these sentence-level contexts, returning the appropriate 8-digit synset identifiers for each target word and mapping them back onto the respective word in the source language input. This process happens at the analytical layer, where s"
L16-1441,P14-1137,0,0.0837546,"98) where the collective meanings of open-class words (nouns, verbs, adjectives and adverbs) are grouped together as ‘synsets’. While it has long been assumed that an optimally successful MT system must incorporate some kind of WSD component (Carpuat and Wu, 2005), attempts to integrate WSD components into machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by various complete reformulations of the disambiguation process (Carpuat and Wu, 2007; Xiong and Zhang, 2014) – some of which yielded small improvements in translation quality – but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This paper provides further evidence to support our previous work (Neale et al., 2015), which experimented in including the output from WSD as contextual features in maxent-based translation models in search of improved performance for machine translation from English to Portuguese. Training our transfer model using a much larger dataset – approximately 1.9 million English-Portugue"
L16-1441,agirre-soroa-2008-using,1,\N,Missing
L16-1483,agerri-etal-2014-ixa,0,0.0139978,"us into a pivot language, English, and this into the remaining six languages. The current annotated corpus covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, s"
L16-1483,E09-1005,1,0.857409,"ese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the LKB used for this processing. Bulgarian The basic version of Bulgarian WSD is implemented on the assumption of one sense per discourse and bigram statistics. Czech Two different approaches were used for Czech WSD. The first approach based on the work of Duˇsek et al. (2015) focuses on verbal WSD. The second approach followed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the"
L16-1483,barreto-etal-2006-open,1,0.822658,"Missing"
L16-1483,bojar-etal-2012-joy,1,0.897675,"Missing"
L16-1483,E06-2024,1,0.746535,"It offers the “disambiguate” and “candidates” service endpoints. The former takes the spotted text input and it returns the DBpedia resource page for each entity. The later is similar to disambiguate, but returns a ranked list of candidates. Portuguese The NED module for Portuguese, LX-NED, uses DBpedia Spotlight to find links to resources about entities identified in pre-processed input text. It creates a process to run a Portuguese extraction of DBpedia Spotlight on a local server, then takes an input text pre-processed with lemmas, Part of Speech tags and named entities using the LX-Suite (Branco and Silva, 2006) and converts it to the ’spotted’ format understood by Spotlight. This spotted input text is then disambiguated using DBpedia Spotlight, returning among other information links to existing Portuguese DBpedia resource pages for each named entity discovered. 3024 3.3. Word-sense disambiguation Basque, English and Spanish ixa-pipe-wsd-ukb is based on UKB, a collection of programs for performing graphbased WSD (Agirre and Soroa, 2009). It applies the socalled Personalized PageRank on a Lexical Knowledge Base (LKB) to rank the vertices of the LKB and thus perform disambiguation. WordNet 3.0 is the"
L16-1483,W02-2004,0,0.181331,"Missing"
L16-1483,W02-1001,0,0.104764,"covers the first 2,000 sentences of the QTLeap corpus, which have been used to train the MT systems in the project. 3. Annotation Tools In this section, we describe the NERC, NED, WSD and coreference tools used to annotate the corpora. We have chosen the tools based on their performance and their ease of use. We also describe the annotation formats. 3.1. Named-entity recognition and classification Basque, English and Spanish ixa-pipe-nerc is a multilingual NERC tagger, part of IXA pipes (Agerri et al., 2014). Every model has been trained with the averaged Perceptron algorithm as described in Collins (2002) and as implemented in Apache OpenNLP. The datasets used for training the models are the following: Egunkaria dataset for Basque, a combination of Ontonotes 4.0, CoNLL 2003 and MUC 7 for English, and CoNLL 2002 for Spanish. Bulgarian The Bulgarian NERC is a rule-based module. It uses a gazetteer with names categorized in four types: Person, Location, Organization, Other. The identification of new names is based on two factors – sure positions in the text and classifying contextual information, such as, titles for persons, types of geographical objects or organizations, etc. The disambiguation"
L16-1483,W15-2111,1,0.864612,"Missing"
L16-1483,hajic-etal-2012-announcing,1,0.885975,"Missing"
L16-1483,W03-1901,0,0.0627303,"n to comparing the cores and the morphological information (gender and number) of the two expressions. As such, we found it easier to directly implement equivalent tests in-code instead of having to feed the extracted features to the Weka J48 classifier proper. 3.5. Annotation formats Basque, Bulgarian, Czech, English and Spanish These corpora are annotated in the NAF format. The NAF format (Fokkens et al., 2014) is a linguistic annotation format designed for complex NLP pipelines that combines strengths of the Linguistic Annotation Framework (LAF) and the NLP Interchange Formats described by Ide and Romary (2003). Because of its layered extensible format, it can easily be incorporated in a variety of NLP modules that may require different linguistic information as their input. Portuguese The corpus for Portuguese is divided into 4 text files - the raw corpus, and one file for the output of each of the three tools used to process it (WSD, NED and coreference). For each of the three tools output is provided in a standoff annotation format, consisting of one token per line (ID of each token in a markable pair in the case of the coreference tool), the appropriate output element of the respective tools (wo"
L16-1483,2005.mtsummit-papers.11,0,0.506663,"ces, the less language-specific differences will remain between the representations of the meaning of the source and target texts. As a result, chances of success are expected to increase considerably by MT systems that are based on deeper semantic engineering approaches. Following this assumption, one of the approaches taken by the QTLeap project1 is to enrich MT training resources with lexico-semantic information. In this work, we present a solid effort to build multilingual parallel corpora annotated at multiple semantic levels. Our overall goal is to enrich two parallel corpora, Europarl (Koehn, 2005) and the QTLeap corpus (Agirre et al., 2015b), with token, lemma, part-of-speech (POS), namedentity recognition and classification (NERC), named-entity disambiguation (NED), word-sense disambiguation (WSD) and coreference for six languages covered in the QTLeap project, namely, Basque (EU), Bulgarian (BG), Czech (CS), English (EN), Portuguese (PT) and Spanish (ES). Specifically, this paper presents the first release of such corpora, which includes NERC, NED, WSD and coreferencelevel annotation for these six languages. Additionally, some languages have extra annotations, such as wikification (E"
L16-1483,W11-1902,0,0.0321161,"ce in Czech. English and Spanish ixa-pipe-coref is loosely based on the Stanford Multi Sieve Pass system (Lee et al., 2013). The system consists of a number of rule-based sieves. Each sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pai"
L16-1483,J13-4004,0,0.0359801,"owed for the annotation is a straightforward way of achieving compatibility with English WordNet IDs. Since the Czech corpus contains the same sentences as the English corpus, the English WordNet ID annotation from this corpus is projected onto Czech words using GIZA++ word alignment. Portuguese The Portuguese WSD tool, LX-WSD, is also based on UKB. The LKB from which UKB returns word senses within the pipeline has been generated from an extraction of the Portuguese MultiWordNet6 . 3.4. Coreference Basque ixa-pipe-coref-eu is an adaptation of the Stanford Deterministic Coreference Resolution (Lee et al., 2013), which gives state-of-the art performance for English. The original system applies a succession of ten independent deterministic coreference models or sieves. During the adaptation process, firstly, a baseline system has been created which receives as input texts processed by Basque analysis tools and uses specifically adapted static lists to identify language dependent features like gender, animacy or number. Afterwards, improvements over the baseline system have been applied, adapting and replacing some of the original sieves (Soraluze et al., 2015), taking into account that morphosyntactic"
L16-1483,S07-1008,0,0.017093,"Missing"
L16-1483,S07-1006,0,0.117234,"Missing"
L16-1483,W11-1901,0,0.087359,"Missing"
L16-1483,D10-1048,0,0.0133306,"h sieve pass is applied in a deterministic manner, reusing the information generated by the previous sieve and the mention processing. The order in which the sieves are applied favors a highest precision approach and aims at improving the recall with the subsequent application of each of the sieve passes. This is illustrated by the evaluation results of the CoNLL 2011 Coreference Evaluation task (Lee et al., 2013; Lee et al., 2011), in which the Stanford system obtained the best results. The results show a pattern which has also been shown in other results reported with other evaluation sets (Raghunathan et al., 2010), namely, the fact that a large part of the performance of the multi-pass sieve system is based on a set of significant sieves. Thus, this module so far focuses on a subset of sieves only, namely, Speaker Match, Exact Match, Precise Constructs, Strict Head Match and Pronoun Match (Lee et al., 2013). Portuguese For the Portuguese coreference tool, a decision tree classifier was experimented with. Given a pair of expressions, the classifier returns a true or false value that indicates whether those expressions are coreferent. The classifier was trained over the Summit Corpus (Collovini et al., 2"
L16-1483,P14-5003,1,0.902631,"Missing"
L16-1483,tiedemann-2012-parallel,0,0.0371283,"with corresponding document IDs. Then, sentence boundaries were identified and aligned (for further collection and processing information, see Koehn (2005)). The Europarl corpus consists of monolingual data as well as bilingual parallel data with English as pivot language. In our effort, we have annotated the BG, CS, ES and PT parts of the corpus separately while the EN side of the ESEN language pair was used as pivot language to link all six languages. Given that Europarl does not include Basque, we annotated an alternative publicly available Basque-English parallel corpus, the GNOME corpus (Tiedemann, 2012), which includes GNOME localization files. 2.2. QTLeap corpus The QTLeap corpus consists of 4,000 pairs of questions and respective answers in the domain of IT troubleshooting http://hdl.handle.net/11234/1-1477 4 http://www.statmt.org/europarl/ 3023 for both hardware and software, distributed in four 1,000pair batches (Gaudio et al., 2016). This material was collected using a real-life, commercial online support service via chat. The QTLeap corpus is a unique resource in that it is a multilingual data set with parallel utterances in different languages (Basque, Bulgarian, Czech, Dutch, English"
L16-1483,E14-4045,0,0.0404676,"Missing"
L16-1483,M95-1005,0,0.280516,"Missing"
L18-1022,L18-1722,1,0.810597,"xistence of the individual WordNets gets obfuscated. Given these ensembles appear as a convenient one-stop reference, even when only one particular WordNet in them is needed, the former are the favored reference. Citations to individual WordNets are thus vanishing, and with them the incentives and the research productivity indicators that researchers and funding entities need in order to support the continued research on other languages/WordNets. As a first possible step contributing towards mitigating these funneling effects, we have proposed the undertaking of a Pluricentric Global Wordnet (Branco et al., 2018). But our goal here is not to motivate and present this notion. Rather, WordNet is being offered just as one illustrative case — among possibly many existing ones — of asymmetric biases that may be specific to our field, and to each one of our research topics, and that need to be addressed with specific responses that go on a par with an increased attention to reproduction and replication of results. Such specific responses are needed to secure and enhance diversity in a wide range of dimensions in our research activities. We need more language diversity in every aspect of our procedures in ou"
L18-1382,D14-1067,0,0.0256371,"rted on here is being distributed and where it can be obtained for free under a most permissive license. Keywords: distributional semantics, word embeddings, word analogy, lexical similarity, conceptual categorization, Portuguese. 1. Introduction Distributional semantics assumes that the frequency of contexts in which expressions occur helps to capture important syntactic and semantic properties of these expressions (Garvin, 1962). Exploring distributional models has led to advances in a range of natural language processing tasks, from dialog systems (Chen et al., 2014) to question answering (Bordes et al., 2014), among many others. A distributional model is a major language resource for any language as it is instrumental to enhance the performance of many applications and processing tasks for that language. Recent work concerned with distributional models for Portuguese contributed with the creation and public release of a free distributional model for this language that resorts to a considerably large corpus, for the training, and to a standard analogy test set, for the evaluation and tuning of this model (Rodrigues et al., 2016). Since then that corpus has been greatly expanded by our group and is"
L18-1382,W15-3904,0,0.151494,"Missing"
L18-1382,W17-6615,0,0.114178,"Missing"
L18-1382,D15-1176,0,0.082531,"Missing"
L18-1382,W13-3520,0,0.0535469,"Missing"
L18-1382,P15-2001,0,0.0174787,"butional semantics space, we resorted to a range of evaluation data sets. These include the analogy test set LX-4WAnalogies already developed and used by (Rodrigues et al., 2016), but also several more, namely the datasets for Portuguese recently made available by Querido et al. (2017): LXWordSim-353, LX-SimLex-999, LX-Rare Word Similarity, LX-ESSLLI 2008, LX-Battig and the LX-AP. These evaluation datasets were obtained by translating into Portuguese similar ones existing for English, which have been translated also into other languages (Hassan and Mihalcea, 2009) (Joubarne and Inkpen, 2011) (Camacho-Collados et al., 2015) (Freitas et al., 2016) (Cinkov´a, 2016). Given the latter are mainstream datasets, and that these datasets for Portuguese are the most prominent ones existing for this language, this allows thus to develop word embeddings that are mainstream for Portuguese and are comparable with those in English provided large enough data is available for their training. In the next Section 4., we present in more detail the datasets we used. 4. Datasets To the best of our knowledge, the largest raw text corpus ever gathered for the Portuguese language, with 1.7 billion tokens, and publicly described in a pub"
L18-1382,D09-1124,0,0.0240524,"s. For the evaluation and tuning of the resulting distributional semantics space, we resorted to a range of evaluation data sets. These include the analogy test set LX-4WAnalogies already developed and used by (Rodrigues et al., 2016), but also several more, namely the datasets for Portuguese recently made available by Querido et al. (2017): LXWordSim-353, LX-SimLex-999, LX-Rare Word Similarity, LX-ESSLLI 2008, LX-Battig and the LX-AP. These evaluation datasets were obtained by translating into Portuguese similar ones existing for English, which have been translated also into other languages (Hassan and Mihalcea, 2009) (Joubarne and Inkpen, 2011) (Camacho-Collados et al., 2015) (Freitas et al., 2016) (Cinkov´a, 2016). Given the latter are mainstream datasets, and that these datasets for Portuguese are the most prominent ones existing for this language, this allows thus to develop word embeddings that are mainstream for Portuguese and are comparable with those in English provided large enough data is available for their training. In the next Section 4., we present in more detail the datasets we used. 4. Datasets To the best of our knowledge, the largest raw text corpus ever gathered for the Portuguese langua"
L18-1382,N13-1090,0,0.211803,"Missing"
L18-1382,D14-1162,0,0.0933062,"Missing"
L18-1513,S12-1051,0,0.046219,"he performance of resolvers for segments of other types. Keywords: semantic text similarity, paraphrase detection, duplicate question detection 1. Introduction Semantic Text Similarity (STS) is a Natural Language Processing (NLP) task whereby a system, given two input text segments, assigns to them a similarity score in a discrete or continuous scale that ranges from representing total similarity—for semantically equivalent segments—to representing total dissimilarity—for segments that are semantically independent. The STS task has been part of the SemEval competitive shared tasks since 2012 (Agirre et al., 2012), together with other challenges for a wide variety of other tasks, such as plagiarism detection, sentiment analysis or relation extraction, to name but a few. More recently, SemEval embraced STS challenges that concern more focused tasks, like paraphrase detection, which consists of a binary decision on whether two input sentences are paraphrases of each other and, starting in 2016, a task on Duplicate Question Detection (DQD) (Nakov et al., 2016). DQD appears as a special case of paraphrase detection, where the focus is on interrogative sentences: this task consists of a binary decision on w"
L18-1513,K15-1013,0,0.0166638,"question should be directed to the already existing question. Duplicate questions are manually flagged by the users, but this effort quickly becomes unwieldy as the site grows in popularity, driving the need for automatic procedures for DQD. Though the interest in DQD may be seen as relatively recent, there is an accumulated body of lessons learned about this task and the expected performance of systems tackling it, some of them being quite in line with what is known about data-driven approaches in general, while some others are more specific for this task. From existing work on DQD, such as (Bogdanova et al., 2015) (Rodrigues et al., 2017) and (Saedi et al., 2017), one learned that (i) training and evaluating over a specific domain with less data, rather than over a generic one with more data, will likely lead to better performance; (ii) training on as much data as possible, gathered from all different domains, and evaluating on a specific domain yields little more than random choice performance; (iii) when training on data sets of interrogative sentences, differences in the average length or in the level of grammaticality of sentences have little impact on performance; (iv) the differences in performan"
L18-1513,S16-1083,0,0.074292,"total dissimilarity—for segments that are semantically independent. The STS task has been part of the SemEval competitive shared tasks since 2012 (Agirre et al., 2012), together with other challenges for a wide variety of other tasks, such as plagiarism detection, sentiment analysis or relation extraction, to name but a few. More recently, SemEval embraced STS challenges that concern more focused tasks, like paraphrase detection, which consists of a binary decision on whether two input sentences are paraphrases of each other and, starting in 2016, a task on Duplicate Question Detection (DQD) (Nakov et al., 2016). DQD appears as a special case of paraphrase detection, where the focus is on interrogative sentences: this task consists of a binary decision on whether two input interrogatives sentences are a duplicate of each other. The motivation for the increasing interest in DQD, and the inclusion in SemEval of challenges dedicated to DQD, comes from the increasing popularity of on-line Community Question Answering (CQA) forums, such as Stack Exchange1 or Quora2 . These forums are quite open in allowing any user to post questions (and answer questions from other users) but from this arises a potential"
L18-1513,S17-1030,1,0.922196,"ed to the already existing question. Duplicate questions are manually flagged by the users, but this effort quickly becomes unwieldy as the site grows in popularity, driving the need for automatic procedures for DQD. Though the interest in DQD may be seen as relatively recent, there is an accumulated body of lessons learned about this task and the expected performance of systems tackling it, some of them being quite in line with what is known about data-driven approaches in general, while some others are more specific for this task. From existing work on DQD, such as (Bogdanova et al., 2015) (Rodrigues et al., 2017) and (Saedi et al., 2017), one learned that (i) training and evaluating over a specific domain with less data, rather than over a generic one with more data, will likely lead to better performance; (ii) training on as much data as possible, gathered from all different domains, and evaluating on a specific domain yields little more than random choice performance; (iii) when training on data sets of interrogative sentences, differences in the average length or in the level of grammaticality of sentences have little impact on performance; (iv) the differences in performance between the major type"
L18-1513,I11-1112,0,0.0280301,"ly competitive performance for DQD, in order to explain away a possible justification for the difference in performance between the two types of sentences based on the putative weakness of the methods used vis a vis interrogatives. In this paper we provide a short summary of each approach and direct the reader to the articles cited above for further information. Jaccard The Jaccard index is a straightforward statistic based on the count of common of n-grams between the two segments being compared. It is used as a simple baseline that previous work has shown to nonetheless be very competitive (Wu et al., 2011), especially for small sized data sets below 30,000 pairs (Saedi et al., 2017). All n-grams, with n ranging from 1 to 4, are used. SVM Support Vector Machine classifiers have been used with success in many NLP tasks and are able to cope with a great variety of features. The set of features used in this work is formed by (i) two vectors with the one-hot encodings of n-gram occurrences in each segment; (ii) the Jaccard index scores for 1, 2, 3 and 4grams; (iii) the counts of negative words (e.g. never, nothing, etc.) in each segment; (iv) the number of nouns that are common to both segments; and"
L18-1513,I05-5002,0,0.296278,"Missing"
L18-1513,S15-2001,0,0.0483771,"Missing"
L18-1722,P13-1133,0,0.0859157,"eton wordnet (Fellbaum, 1998), is publicly available to be reused. It is designed to browse through synsets and senses in any wordnet compatible with the Princeton format. However, like the previous example, it does not support connections among wordnets of different languages. • Visuwords (Critchfield, 2017) is an online graphical wordnet browser that employs colors and shapes to distinguish between synsets in various parts of speeches and types of semantic relations. Although very user friendly, this browser does not support multilingual wordnet browsing either. • Open Multilingual Wordnet (Bond and Foster, 2013) connects a large number of wordnets from different languages while using the English Princeton wordnet as the pivot one. The wordnets it resorts to have permissive licences for derivatives and redistribution and searching through the browser shows results in all their languages. However the source code of the browser itself is not available to be reused, and it is a browser that in any case offers no options to peruse wordnets on the basis of, direct or transitive, semantic relations. • Multi-WordNet (Pianta et al., 2002) is a well-known project aiming at aligning wordnets of different langua"
L18-1722,P09-4002,1,0.687014,"Missing"
L18-1722,elkateb-etal-2006-building,0,0.101522,"Missing"
L18-1722,finthammer-cramer-2008-exploring,0,0.0818219,"Missing"
L18-1722,isahara-etal-2008-development,0,0.0641716,"Missing"
L18-1722,C10-2097,0,0.0366018,"Missing"
L18-1722,Y11-1027,0,0.0349276,"Missing"
L18-1722,W09-3420,0,0.0585876,"Missing"
L18-1722,tufis-etal-2008-racais,0,0.0755107,"Missing"
L18-1722,W13-4302,0,0.0329857,"Missing"
P09-4002,A00-1031,0,0.0423582,"Missing"
P09-4002,branco-etal-2008-lx,1,\N,Missing
P10-1069,N07-1053,0,0.0732118,"Missing"
P10-1069,S07-1098,0,0.684091,"Missing"
P10-1069,J03-1002,0,0.00279005,"e Google Translator Toolkit.3 This tool combines machine translation with a translation memory. A human translator corrected the proposed translations manually. After that, we had the three collections of documents (the TimeML data, the English unannotated data and the Portuguese unannotated data) aligned by paragraphs (we just kept the line breaks from the original collection in the other collections). In this way, for each paragraph in the Portuguese data we know all the corresponding TimeML tags in the original English paragraph. We tried using machine translation software (we used GIZA++ (Och and Ney, 2003)) to perform word alignment on the unannotated texts, which would have enabled us to transpose the TimeML annotations automatically. However, word alignment algorithms have suboptimal accuracy, so the results would have to be checked manually. Therefore we abandoned this idea, and instead we simply placed the different TimeML markup in the correct positions manually. This is possible since the TempEval-1 corpus is not very large. A small script was developed to place all relevant TimeML markup at the end of each paragraph in the Portuguese text, and then each tag was manually repositioned. Not"
P10-1069,W09-2418,0,0.0257003,"Missing"
P10-1069,S07-1014,0,\N,Missing
P18-1164,D16-1162,0,0.0193314,"ng only admissible translation pairs. These data thus suggest that our improved model has a good capability of capturing the translation equivalence between source and target word embeddings. 5 ment quality by inducing the NMT model to capture more favorable pairs of words that are translation equivalents of each other under the effect of the bridging mechanism. Recently there have been also studies towards leveraging word alignments from SMT models. Mi et al. (2016) and Liu et al. (2016) use preobtained word alignments to guide the NMT attention model in the learning of favorable word pairs. Arthur et al. (2016) leverage a pre-obtained word dictionary to constrain the prediction of target words. Despite these approaches having a somewhat similar motivation of using pairs of translation equivalents to benefit the NMT translation, in our new bridging approach we do not use extra resources in the NMT model, but let the model itself learn the similarity of word pairs from the training data. 4 Besides, there exist also studies on the learning of cross-lingual embeddings for machine translation. Mikolov et al. (2013) propose to first learn distributed representation of words from large monolingual data, an"
P18-1164,P10-4002,0,0.0355367,"ry words were mapped to the special token UNK. The dimension of word embedding was 620 and the size of the hidden layer was 1000. All other settings were the same as in Bahdanau et al. (2015). The maximum length of sentences that we used to train the NMT model in our experiments was set to 50, for both the Chinese and English sides. Additionally, during decoding, we used the beam-search algorithm and set the beam size to 10. The model parameters were selected according to the maximum BLEU points on the development set. We compared our proposed models against the following two systems: • cdec (Dyer et al., 2010): this is an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target side of the training data. • Rather than using a concrete source word embedding xt∗ in Equation 3, we could also use a weighted sum of source word embeddings, P i.e. j αtj hj . However, our preliminary experiments showed that the performance gap between these two methods is very small. Therefore, we use xt∗ to calculate the new training objective as shown in Equation 3 in all experiments. 3 • RNNSearch*: this is an attention-based NMT system,"
P18-1164,P17-1064,1,0.787515,"te the respec1772 System 40 BLEU Score 35 RNNSearch* 30 25 cdec RNNSearch* Direct Link 20 (0,10] (10,20] (20,30] (30,40] (40,50] (50,100] Direct bridging Length of Source Sentence Figure 6: BLEU scores for the translation of sentences with different lengths. tive BLEU scores, which are presented in Figure 6. These results indicate that our improved system outperforms RNNSearch* for all the sentence lengths. They also reveal that the performance drops substantially when the length of the input sentence increases. This trend is consistent with the findings in (Cho et al., 2014; Tu et al., 2016; Li et al., 2017). One also observes that the NMT systems perform very badly on sentences of length over 50, when compared to the performance of the baseline SMT system (cdec). We think that the degradation of NMT systems performance over long sentences is due to the following reasons: (1) during training, the maximum source sentence length limit is set to 50, thus making the learned models not ready to cope well with sentences over this maximum length limit; (2) for long input sentences, NMT systems tend to stop early in the generation of the translation. 4.3 Analysis of Over and Under Translation To assess t"
P18-1164,C16-1291,0,0.0538199,"sformation matrix of our direct bridging method are very consistent with those obtained from the SMT lexical table, containing only admissible translation pairs. These data thus suggest that our improved model has a good capability of capturing the translation equivalence between source and target word embeddings. 5 ment quality by inducing the NMT model to capture more favorable pairs of words that are translation equivalents of each other under the effect of the bridging mechanism. Recently there have been also studies towards leveraging word alignments from SMT models. Mi et al. (2016) and Liu et al. (2016) use preobtained word alignments to guide the NMT attention model in the learning of favorable word pairs. Arthur et al. (2016) leverage a pre-obtained word dictionary to constrain the prediction of target words. Despite these approaches having a somewhat similar motivation of using pairs of translation equivalents to benefit the NMT translation, in our new bridging approach we do not use extra resources in the NMT model, but let the model itself learn the similarity of word pairs from the training data. 4 Besides, there exist also studies on the learning of cross-lingual embeddings for machin"
P18-1164,2015.iwslt-evaluation.11,0,0.0385175,"redicted word. Seeking to shorten the distance between source and target word embeddings, in what we term bridging, is the key insight for the advances presented in this paper. improve quality of both sentence translation, in general, and alignment and translation of individual source words with target words, in particular. 1 Introduction Neural machine translation (NMT) is an endto-end approach to machine translation that has achieved competitive results vis-a-vis statistical machine translation (SMT) on various language pairs (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014; Luong and Manning, 2015). In NMT, the sequence-to-sequence (seq2seq) model learns word embeddings for both source and target words synchronously. However, as illustrated in Figure 1, source and target word embeddings are at the two ends of a long information processing procedure. The individual associations between them will gradually become loose due to the separation of source-side hidden states (represented by h1 , . . . , hT in Fig. 1) and a target1767 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1767–1776 c Melbourne, Australia, July 15 - 20, 2018."
P18-1164,D15-1166,0,0.130766,"e insight of shortening the distance between source and target embeddings in the seq2seq processing chain, in the present paper we propose more strategies to bridge source and target word embeddings and with better results. 6 Related Work Since the pioneer work of Bahdanau et al. (2015) to jointly learning alignment and translation in NMT, many effective approaches have been proposed to further improve the alignment quality. The attention model plays a crucial role in the alignment quality and thus its enhancement has continuously attracted further efforts. To obtain better attention focuses, Luong et al. (2015) propose global and local attention models; and Cohn et al. (2016) extend the attentional model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. In contrast, we did not delve into the attention model or sought to redesign it in our new bridging proposal. And yet we achieve enhanced alignConclusion We have presented three models to bridge source and target word embeddings for NMT. The three models seek to shorten the distance between source and target word embeddings along the exte"
P18-1164,D16-1249,0,0.142173,"h a source word fj , a transformation matrix W is learned with the hope that the discrepancy of W xi and yj tends to be zero. Accordingly, we update 1769 the objective function of training for a single sentence with its following extended formulation: L(θ) = − Ty X (log p(yt |y<t , x) − kW xt∗ − yt k2 ) t=1 (3) where log p(yt |y<t , x) is the original objective function of the NMT model, and the term kW xt∗ − yt k2 measures and penalizes the difference between target word yt and its aligned source word xt∗ , i.e. the one with the highest attention weight, as computed in Equation 2. Similar to Mi et al. (2016), we view the two parts of the loss in Equation 3 as equally important. At this juncture, it is worth noting the following: • Our direct bridging model is an extension of the source-side bridging model, where the source word embeddings are part of the final annotation vector of the encoder. We have also tried to place the auxiliary object function directly on the NMT baseline model. However, our empirical study showed that the combined objective consistently worsens the translation quality. We blame this on that the learned word embeddings on two sides by the baseline model are too heterogeneo"
P18-1164,J03-1002,0,0.0313851,"Missing"
P18-1164,P02-1040,0,0.09988,"Missing"
P18-1164,2006.amta-papers.25,0,0.107933,"Missing"
P18-1164,N03-1033,0,0.109773,"Missing"
P18-1164,P16-1008,0,0.0277452,"length and compute the respec1772 System 40 BLEU Score 35 RNNSearch* 30 25 cdec RNNSearch* Direct Link 20 (0,10] (10,20] (20,30] (30,40] (40,50] (50,100] Direct bridging Length of Source Sentence Figure 6: BLEU scores for the translation of sentences with different lengths. tive BLEU scores, which are presented in Figure 6. These results indicate that our improved system outperforms RNNSearch* for all the sentence lengths. They also reveal that the performance drops substantially when the length of the input sentence increases. This trend is consistent with the findings in (Cho et al., 2014; Tu et al., 2016; Li et al., 2017). One also observes that the NMT systems perform very badly on sentences of length over 50, when compared to the performance of the baseline SMT system (cdec). We think that the degradation of NMT systems performance over long sentences is due to the following reasons: (1) during training, the maximum source sentence length limit is set to 50, thus making the learned models not ready to cope well with sentences over this maximum length limit; (2) for long input sentences, NMT systems tend to stop early in the generation of the translation. 4.3 Analysis of Over and Under Trans"
P98-1027,E91-1008,0,\N,Missing
P98-1027,E95-1002,0,\N,Missing
P98-1027,C88-1060,0,\N,Missing
P98-1027,P90-1014,0,\N,Missing
P98-1027,J95-2003,0,\N,Missing
P98-1027,P89-1032,0,\N,Missing
P98-1027,C88-1026,0,\N,Missing
P98-1027,C90-3022,0,\N,Missing
R19-1120,N09-1003,0,0.16458,"Missing"
R19-1120,E09-1005,0,0.0205942,"ferent vector sizes by Saedi et al. (2018), namely 100, 300, 850, 1000 and 3000, with 850 performing the best. For the sake of comparability with the other models we resort to, namely the text-based ones, we set a vector size of 300 for the matrix factorisation embedding technique. stein and Goodenough, 1965). For semantic relatedness, WordSim-353-Relatedness (252) (Agirre et al., 2009), MEN (3000) (Bruni et al., 2012) and MTURK-771 (771) (Halawi et al., 2012) were used. The results with WordNet embeddings are displayed in Table 1.8 4.3 Similarity Random walk The random walk was based on UKB (Agirre and Soroa, 2009; Agirre et al., 2014; Goikoetxea et al., 2015), which performs a random walk through edges on graphs and in each step writes a word in the node into an artificial text. With the resulting corpus, a two-layer neural network model (Skip-Gram) (Mikolov et al., 2013b) was trained to predict for each vocabulary word its neighbouring words, thus generating in one of the layers the resulting word embedding vectors. We restricted the original technique to use only the information from the graph and to ignore the glosses. The random walk was applied to the same WordNet graph (60k vocabulary) described"
R19-1120,E12-1004,0,0.0166904,"while the information encoded in Small World of Words are the associations between concepts evoked and collected from laypersons. Even when motivated in the first place by (psycho-)linguistic research goals, these repositories of lexical knowledge have been extraordinarily important for language technology. They have been instrumental for major advances in language processing tasks and applications such as word sense disambiguation, part-of-speech tagging, named entity recognition, sentiment analysis (e.g. Li and Jurafsky (2015)), parsing (e.g. Socher et al. (2013)), textual entailment (e.g. Baroni et al. (2012)), discourse analysis (e.g. Ji and Eisenstein (2014)), among many others.1 In our experiments, we resort to these two major representatives of inference- and feature-based lexical networks, namely WordNet2 and SWOW3 . 3 Graph embedding methods As for methods to convert graphs into embedding, we are resorting also to one outstanding representative per major family of techniques. Following the recent comprehensive survey by (Cai et al., 2017), graph embeddings methods divide into those that represent a whole graph as a single vector and those that output a vector for each node in the graph. For"
R19-1120,P17-1080,0,0.0295882,"mation has consisted mostly of large collections of raw text, and thus ultimately on the frequencies of co-occurrence of words with other neighbouring words in certain windows of context, (Mikolov et al., 2013a; Pennington et al., 2014; Mikolov et al., 2018) among others. A few research trends have been gaining momentum concerning the application of neural networks to natural language technology, and a fortiori in what concerns distributional semantics. On the one hand, there has been a growing interest in the linguistic information that may be ultimately encoded in vectorial representations (Belinkov et al., 2017; Conneau et al., 2018), also relating to their eventual ”universality”, in view of possibly transferring these representations from one language processing task or application to another (Shi et al., 2016; C´ıfka and Bojar, 2018). On the other hand, growing attention has been devoted to sources of information for word embeddings other than what may be conveyed and extracted from co-occurrences in text. This includes information that is encoded in sophisticated lexical collections of data that are carefully crafted and densely loaded with accurate information on lexical semantics (Goikoetxea e"
R19-1120,P12-1015,0,0.0282148,"ollowed by an L2-norm to normalise each line of MG , and finally, a Principal Compo1044 nent Analysis (PCA) was applied to reduce the dimension of the vectors. This procedure was evaluated with different vector sizes by Saedi et al. (2018), namely 100, 300, 850, 1000 and 3000, with 850 performing the best. For the sake of comparability with the other models we resort to, namely the text-based ones, we set a vector size of 300 for the matrix factorisation embedding technique. stein and Goodenough, 1965). For semantic relatedness, WordSim-353-Relatedness (252) (Agirre et al., 2009), MEN (3000) (Bruni et al., 2012) and MTURK-771 (771) (Halawi et al., 2012) were used. The results with WordNet embeddings are displayed in Table 1.8 4.3 Similarity Random walk The random walk was based on UKB (Agirre and Soroa, 2009; Agirre et al., 2014; Goikoetxea et al., 2015), which performs a random walk through edges on graphs and in each step writes a word in the node into an artificial text. With the resulting corpus, a two-layer neural network model (Skip-Gram) (Mikolov et al., 2013b) was trained to predict for each vocabulary word its neighbouring words, thus generating in one of the layers the resulting word embedd"
R19-1120,J06-1003,0,0.182808,"for text-based embeddings (600B) — whose collection can be obtained with quite affordable costs in the case of SWOW, the graph that is informing the top-performing embeddings. 7 distance of their respective vectors in a semantic space, but also on the basis of the distance of the respective concepts in the semantic network itself. There has been a research tradition on this issue whose major proposals include (Jiang and Conrath, 1997; Lin, 1998; Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Resnik, 1999) a.o., which received nice comparative assessments in (Ferlez and Gams, 2004) and (Budanitsky and Hirst, 2006). The focus of the present paper, though, is rather on vectorial representations and semantic distances based on them. Related Work There have been some publications pioneering the issue of obtaining word embeddings from lexical semantic networks. Each has focused though on a particular graph embedding technique or in a particular lexical graph, and thus a systematic study of graph embeddings under comparable settings was not undertaken, and a fortiori a comparative assessment of their strengths with regards textbased ones is also lacking. The application of Katz index for matrix factorisation"
R19-1120,P18-1198,0,0.0195448,"stly of large collections of raw text, and thus ultimately on the frequencies of co-occurrence of words with other neighbouring words in certain windows of context, (Mikolov et al., 2013a; Pennington et al., 2014; Mikolov et al., 2018) among others. A few research trends have been gaining momentum concerning the application of neural networks to natural language technology, and a fortiori in what concerns distributional semantics. On the one hand, there has been a growing interest in the linguistic information that may be ultimately encoded in vectorial representations (Belinkov et al., 2017; Conneau et al., 2018), also relating to their eventual ”universality”, in view of possibly transferring these representations from one language processing task or application to another (Shi et al., 2016; C´ıfka and Bojar, 2018). On the other hand, growing attention has been devoted to sources of information for word embeddings other than what may be conveyed and extracted from co-occurrences in text. This includes information that is encoded in sophisticated lexical collections of data that are carefully crafted and densely loaded with accurate information on lexical semantics (Goikoetxea et al., 2015; Saedi et a"
R19-1120,P18-1126,0,0.0357942,"Missing"
R19-1120,C16-1175,0,0.0346386,"Missing"
R19-1120,N15-1165,0,0.060158,"Missing"
R19-1120,D07-1061,0,0.0585626,"he graph embedding SME technique based on edge reconstruction was pioneered by Bordes et al. (2014), who applied it to a small WordNet subset restricted to 1-hop relations, which we expanded in the experiments reported here. The random walk methods for graph embeddings were experimented with by Goikoetxea et al. (2015) over full WordNet. This however does not represent a ”purely” graph-based approach given the raw text in the glosses was also used. In our implementation here, the embeddings were based solely on the information in the graph. In this connection, it is worthy of note the work by Hughes and Ramage (2007), which resorts also to random graph walks over WordNet. Differently, from the goal here, its goal was to obtain word-specific stationary probability distributions — such that the semantic affinity of two words is based on the similarity of their probability distributions —, rather than to obtain vectorial representations for words. It is also worth mentioning that the task of determining the semantic similarity between two words can be performed not only on the basis of the 8 Conclusions This paper reports on the insights gained on word embeddings with an experimental space that systemically"
R19-1120,P14-1002,0,0.0336511,"Words are the associations between concepts evoked and collected from laypersons. Even when motivated in the first place by (psycho-)linguistic research goals, these repositories of lexical knowledge have been extraordinarily important for language technology. They have been instrumental for major advances in language processing tasks and applications such as word sense disambiguation, part-of-speech tagging, named entity recognition, sentiment analysis (e.g. Li and Jurafsky (2015)), parsing (e.g. Socher et al. (2013)), textual entailment (e.g. Baroni et al. (2012)), discourse analysis (e.g. Ji and Eisenstein (2014)), among many others.1 In our experiments, we resort to these two major representatives of inference- and feature-based lexical networks, namely WordNet2 and SWOW3 . 3 Graph embedding methods As for methods to convert graphs into embedding, we are resorting also to one outstanding representative per major family of techniques. Following the recent comprehensive survey by (Cai et al., 2017), graph embeddings methods divide into those that represent a whole graph as a single vector and those that output a vector for each node in the graph. For our experiments, we are interested in the latter, fo"
R19-1120,O97-1002,0,0.501878,"lutions consistently outperforming mainstream text-based ones by a substantial margin. It is of note that this is obtained with data sets of a much smaller size (12k) than the ones used for text-based embeddings (600B) — whose collection can be obtained with quite affordable costs in the case of SWOW, the graph that is informing the top-performing embeddings. 7 distance of their respective vectors in a semantic space, but also on the basis of the distance of the respective concepts in the semantic network itself. There has been a research tradition on this issue whose major proposals include (Jiang and Conrath, 1997; Lin, 1998; Leacock and Chodorow, 1998; Hirst and St-Onge, 1998; Resnik, 1999) a.o., which received nice comparative assessments in (Ferlez and Gams, 2004) and (Budanitsky and Hirst, 2006). The focus of the present paper, though, is rather on vectorial representations and semantic distances based on them. Related Work There have been some publications pioneering the issue of obtaining word embeddings from lexical semantic networks. Each has focused though on a particular graph embedding technique or in a particular lexical graph, and thus a systematic study of graph embeddings under comparabl"
R19-1120,L18-1008,0,0.0129744,"on distributional semantics, with its vector space models of meaning, has been a driving factor for research on natural language semantics. When focusing on the meaning of words under this approach, information on lexical semantics has been sought to be encoded into appropriate vectorial representations, also known as word embeddings. The source for this information has consisted mostly of large collections of raw text, and thus ultimately on the frequencies of co-occurrence of words with other neighbouring words in certain windows of context, (Mikolov et al., 2013a; Pennington et al., 2014; Mikolov et al., 2018) among others. A few research trends have been gaining momentum concerning the application of neural networks to natural language technology, and a fortiori in what concerns distributional semantics. On the one hand, there has been a growing interest in the linguistic information that may be ultimately encoded in vectorial representations (Belinkov et al., 2017; Conneau et al., 2018), also relating to their eventual ”universality”, in view of possibly transferring these representations from one language processing task or application to another (Shi et al., 2016; C´ıfka and Bojar, 2018). On th"
R19-1120,D14-1162,0,0.0857614,"uage processing, interest on distributional semantics, with its vector space models of meaning, has been a driving factor for research on natural language semantics. When focusing on the meaning of words under this approach, information on lexical semantics has been sought to be encoded into appropriate vectorial representations, also known as word embeddings. The source for this information has consisted mostly of large collections of raw text, and thus ultimately on the frequencies of co-occurrence of words with other neighbouring words in certain windows of context, (Mikolov et al., 2013a; Pennington et al., 2014; Mikolov et al., 2018) among others. A few research trends have been gaining momentum concerning the application of neural networks to natural language technology, and a fortiori in what concerns distributional semantics. On the one hand, there has been a growing interest in the linguistic information that may be ultimately encoded in vectorial representations (Belinkov et al., 2017; Conneau et al., 2018), also relating to their eventual ”universality”, in view of possibly transferring these representations from one language processing task or application to another (Shi et al., 2016; C´ıfka"
R19-1120,P14-2050,0,0.0555371,"Missing"
R19-1120,D15-1200,0,0.0263718,"constructed on the basis of lexical intuitions systematically handled by human experts, while the information encoded in Small World of Words are the associations between concepts evoked and collected from laypersons. Even when motivated in the first place by (psycho-)linguistic research goals, these repositories of lexical knowledge have been extraordinarily important for language technology. They have been instrumental for major advances in language processing tasks and applications such as word sense disambiguation, part-of-speech tagging, named entity recognition, sentiment analysis (e.g. Li and Jurafsky (2015)), parsing (e.g. Socher et al. (2013)), textual entailment (e.g. Baroni et al. (2012)), discourse analysis (e.g. Ji and Eisenstein (2014)), among many others.1 In our experiments, we resort to these two major representatives of inference- and feature-based lexical networks, namely WordNet2 and SWOW3 . 3 Graph embedding methods As for methods to convert graphs into embedding, we are resorting also to one outstanding representative per major family of techniques. Following the recent comprehensive survey by (Cai et al., 2017), graph embeddings methods divide into those that represent a whole gra"
R19-1120,W18-3016,1,0.693831,"al., 2018), also relating to their eventual ”universality”, in view of possibly transferring these representations from one language processing task or application to another (Shi et al., 2016; C´ıfka and Bojar, 2018). On the other hand, growing attention has been devoted to sources of information for word embeddings other than what may be conveyed and extracted from co-occurrences in text. This includes information that is encoded in sophisticated lexical collections of data that are carefully crafted and densely loaded with accurate information on lexical semantics (Goikoetxea et al., 2015; Saedi et al., 2018). The results reported in the present paper lies at the intersection of those research goals. In particular, we aim here to gain a better insight into these two sources of lexical information, and the quality of the resulting word embeddings, by assessing how graph-based word embeddings compare to mainstream text-based ones. To pursue this objective, we explore an experimental space that takes into account lexical semantic networks of essentially different types as well as different sorts of methods, with different strengths, to convert those graphs into embeddings. In the experimental space t"
R19-1120,D16-1159,0,0.0312668,"; Pennington et al., 2014; Mikolov et al., 2018) among others. A few research trends have been gaining momentum concerning the application of neural networks to natural language technology, and a fortiori in what concerns distributional semantics. On the one hand, there has been a growing interest in the linguistic information that may be ultimately encoded in vectorial representations (Belinkov et al., 2017; Conneau et al., 2018), also relating to their eventual ”universality”, in view of possibly transferring these representations from one language processing task or application to another (Shi et al., 2016; C´ıfka and Bojar, 2018). On the other hand, growing attention has been devoted to sources of information for word embeddings other than what may be conveyed and extracted from co-occurrences in text. This includes information that is encoded in sophisticated lexical collections of data that are carefully crafted and densely loaded with accurate information on lexical semantics (Goikoetxea et al., 2015; Saedi et al., 2018). The results reported in the present paper lies at the intersection of those research goals. In particular, we aim here to gain a better insight into these two sources of l"
R19-1120,P13-1045,0,0.00960853,"uitions systematically handled by human experts, while the information encoded in Small World of Words are the associations between concepts evoked and collected from laypersons. Even when motivated in the first place by (psycho-)linguistic research goals, these repositories of lexical knowledge have been extraordinarily important for language technology. They have been instrumental for major advances in language processing tasks and applications such as word sense disambiguation, part-of-speech tagging, named entity recognition, sentiment analysis (e.g. Li and Jurafsky (2015)), parsing (e.g. Socher et al. (2013)), textual entailment (e.g. Baroni et al. (2012)), discourse analysis (e.g. Ji and Eisenstein (2014)), among many others.1 In our experiments, we resort to these two major representatives of inference- and feature-based lexical networks, namely WordNet2 and SWOW3 . 3 Graph embedding methods As for methods to convert graphs into embedding, we are resorting also to one outstanding representative per major family of techniques. Following the recent comprehensive survey by (Cai et al., 2017), graph embeddings methods divide into those that represent a whole graph as a single vector and those that"
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
S17-1030,N13-1090,0,0.00847526,"Missing"
S17-1030,S16-1103,0,0.0247811,"Missing"
S17-1030,D16-1244,0,0.0446491,"Missing"
S17-1030,P06-4018,0,0.0108849,"Missing"
S17-1030,P15-1173,0,0.0244336,"Missing"
S17-1030,I11-1112,0,0.429916,"Missing"
silva-etal-2010-top,varadi-etal-2008-clarin,0,\N,Missing
silva-etal-2010-top,J03-4003,0,\N,Missing
silva-etal-2010-top,P06-1055,0,\N,Missing
silva-etal-2010-top,branco-etal-2008-lx,1,\N,Missing
W07-1208,branco-silva-2004-evaluating,1,0.898477,"Missing"
W07-1208,hughes-etal-2006-reconsidering,0,0.0314724,". The way P (s|Li ) is calculated is also the standard way to do it, namely assuming independence and just multiplying the probabilities of character ci given the preceding n-1 characters (using n-grams), for all characters in the input (estimated from n-gram counts in the training set). For our experiments, we implemented the algorithm described in (Dunning, 1994). Other common strategies were also used, like prepending n−1 special characters to the input string to harmonize calculations, summing logs of probabilities instead of multiplying them to avoid un4 See (Sibun and Reynar, 1996) and (Hughes et al., 2006) for surveys. 62 derﬂow errors, and using Laplace smoothing to reserve probability mass to events not seen in training. 5.2 5.2.1 Calibrating the implementation Detection of languages First of all, we want to check that the language identiﬁcation methods we are using, and have implemented, are in fact reliable to identify diﬀerent languages. Hence, we run the classiﬁer on three languages showing strikingly diﬀerent characters and character sequences. This is a deliberately easy test to get insight into the appropriate setting of the two parameters at stake here, size of of the n-gram in the tr"
W08-2204,W04-1901,0,0.0164514,". As such, they are very similar across different natural languages (modulo predicate names). This is also true of MRS. Furthermore, semantic representations hide grammar implementation. As such, they are the preferred grammar’s interface for applications, that do not need any knowledge of the grammatical properties of Portuguese and may not need to look at syntactic analysis. The MRS format is also used with several other computational HPSGs, for other languages. Several applications (e.g. Machine Translation) have been used with other HPSGs that communicate with these grammars via the MRSs (Bond et al., 2004). These applications can be easily integrated with grammars for different languages that also use MRS: they are almost completely language independent. 3 Design Features Given the foundational options, LXGram adheres to a number of important design features. Bidirectionality LXGram is bidirectional. The formalism employed is completely declarative. It can be used for parsing (yielding syntactic analyses and semantic representations from natural language input) and also for generation (yielding natural language from meaning representations). As such it can be useful for a wide range of applicat"
W08-2204,W02-1210,0,0.0268929,"sion Semantics (MRS; Copestake et al. (2005)) for the representation of meaning. LXGram is developed in the Linguistic Knowledge Builder (LKB) system (Copestake, 2002), a development environment for constraint-based grammars. This environment provides a GUI, debugging tools and very efficient algorithms for parsing and generation with the grammars developed there (Malouf et al., 2000; Carroll et al., 1999). Several broad-coverage grammars have been developed in the LKB. Currently, the largest ones are for English (Copestake and Flickinger, 2000), German (Müller and Kasper, 2000) and Japanese (Siegel and Bender, 2002). The grammars developed with the LKB are also supported by the PET parser (Callmeier, 2000), which allows for faster parsing times due to the fact that the grammars are compiled into a binary format in a first step. As the LKB grammars for other languages, LXGram is in active development, and it is intended to be a broad-coverage, open-domain grammar for Portuguese. At the same time, it produces detailed representations of meaning in tandem with syntactic structures, making it useful for a wide range of applications. In Section 2, we describe the framework foundations of the grammar. The majo"
W08-2204,copestake-flickinger-2000-open,0,\N,Missing
W08-2224,W08-2220,0,0.031295,"slated into Portuguese. Because the LXGram generates many different analyses (mainly due to PP attachment ambiguities), the preferred analysis was selected manually. It was required to extend LXGram’s lexicon and inventory of syntax rules to be able to get a reasonable performance on the shared task data. Eventually, our system was able to produce an analysis for 20 out of the 30 sentences of the shared task data. 299 300 Branco and Costa 1 Introduction This paper describes the participation of the Portuguese grammar LXGram in the Shared Task of STEP 2008 “Comparing Semantic Representations” (Bos, 2008). This Shared Task was held in the University of Venice on 22–24 September 2008, with the purpose of comparing semantic representations produced by different natural language processing systems. This task had seven participating teams. Each team contributed with a small text (up to five sentences long) to be processed by all the systems. LXGram is a hand-built, general purpose computational grammar for the deep linguistic processing of Portuguese. It is developed under the grammatical framework of Head-Driven Phrase Structure Grammar, HPSG (Pollard and Sag, 1987, 1994; Sag et al., 2003) and us"
W08-2224,W08-2204,1,0.88171,"Missing"
W08-2224,C94-2144,0,0.00853191,"s Minimal Recursion Semantics, MRS (Copestake et al., 2005) for the representation of meaning. This grammar implementation is undertaken with the LKB (Copestake, 2002) grammar development environment and its evaluation and regression testing is done via [incr tsdb()] (Oepen, 2001). It is also intended to be compatible with the PET parser (Callmeier, 2000). The LinGO Grammar Matrix (version 0.9), an open-source kit for the rapid development of grammars based on HPSG and MRS, was used as the initial code upon which to build LXGram. The grammar is implemented in the LKB using the T DL formalism (Krieger and Schäfer, 1994), based on unification and on typed feature structures, and whose types are organized in a multiple inheritance hierarchy. For more information, please refer to a detailed implementation report (Branco and Costa, 2008a) or on pages 31–43 of this volume (Branco and Costa, 2008b). A free version of the grammar can also be obtained at http://nlx.di.fc.ul.pt/lxgram, under an ELDA research license. Section 2 introduces the main features of the Minimal Recursion Semantics format, which is employed in the semantic representations produced by LXGram. In Section 3, the sample text that the LXGram team"
W09-4406,W06-2609,0,0.411441,"our experiments a simple grammar for each language was create that extracts all the sentences where the verb ”to be” appears as the main verb. For Dutch we obtained a sub-corpus composed by 4,829, 120 of which are definitions, with a ratio of 39:1. For Portuguese we obtained a sub-corpus composed by 1,360 sentences, 121 of which are definitions, with a ratio of about 10:1. Commonly used features are: bag-of-word, ngrams [17] (either of part-of-speech or of base forms), the position of the definition inside the document [12], the presence of determiners in the definiens and in the definiendum [8]. Other relevant properties can be the 1 www.lt4el.eu presence of named entities [8] or data from en external source such as encyclopedic data, wordnet, etc. [22]. Some features work well with a corpus but not so well in a different corpus, resulting in the impossibility to use the learner with different corpora. The use of the position of a definition-bearing sentence in [8] is an example of a feature that is corpus dependent. The same issue arise when lexical information is used as feature. In order to avoid such limitation we represented instances as n-grams of POS. From both the corpora th"
W09-4406,C92-2082,0,0.190665,"Missing"
W09-4406,W04-1807,0,0.282899,"Missing"
W09-4406,C04-1199,0,0.0739727,"formation using the LX-Suite [23] a set of tools for the shallow processing of Portuguese with state of the art performance. In order to prepare the data set for to be used in our experiments a simple grammar for each language was create that extracts all the sentences where the verb ”to be” appears as the main verb. For Dutch we obtained a sub-corpus composed by 4,829, 120 of which are definitions, with a ratio of 39:1. For Portuguese we obtained a sub-corpus composed by 1,360 sentences, 121 of which are definitions, with a ratio of about 10:1. Commonly used features are: bag-of-word, ngrams [17] (either of part-of-speech or of base forms), the position of the definition inside the document [12], the presence of determiners in the definiens and in the definiendum [8]. Other relevant properties can be the 1 www.lt4el.eu presence of named entities [8] or data from en external source such as encyclopedic data, wordnet, etc. [22]. Some features work well with a corpus but not so well in a different corpus, resulting in the impossibility to use the learner with different corpora. The use of the position of a definition-bearing sentence in [8] is an example of a feature that is corpus depen"
W09-4406,saggion-2004-identifying,0,0.0175304,"ined a sub-corpus composed by 4,829, 120 of which are definitions, with a ratio of 39:1. For Portuguese we obtained a sub-corpus composed by 1,360 sentences, 121 of which are definitions, with a ratio of about 10:1. Commonly used features are: bag-of-word, ngrams [17] (either of part-of-speech or of base forms), the position of the definition inside the document [12], the presence of determiners in the definiens and in the definiendum [8]. Other relevant properties can be the 1 www.lt4el.eu presence of named entities [8] or data from en external source such as encyclopedic data, wordnet, etc. [22]. Some features work well with a corpus but not so well in a different corpus, resulting in the impossibility to use the learner with different corpora. The use of the position of a definition-bearing sentence in [8] is an example of a feature that is corpus dependent. The same issue arise when lexical information is used as feature. In order to avoid such limitation we represented instances as n-grams of POS. From both the corpora the 100 most frequent n-grams were extracted and were used as features. Each sentence was represented as an array where cells record the number of occurrences of th"
W11-4524,E06-2024,1,0.81908,"as. E´ importante destacar a diferenc¸a entre as classes: N´ umero e Medida. Quando e´ realizada a anotac¸a˜ o N´ umero como tipo de resposta espera-se qualquer n´umero como resposta. No caso da classe Medida a resposta esperada e´ mais complexa que simplesmente um n´umero, mas sim uma medida. Alguns exemplos de respostas mensur´aveis s˜ao: “Qual a altura...?, Quantos quilos...?, Qual o tamanho...?, Qual a velocidade...?, Qual a distˆancia...?”, entre outras. 3. Experimentos Para a realizac¸a˜ o dos experimentos, primeiramente, o corpus foi processado usando as ferramentas: LX-Suite [Branco e Silva 2006] e LX-Ner [Ferreira et al. 2007]. O LX-Suite e´ um tagger para o Portuguˆes que fornece informac¸o˜ es no n´ıvel da palavra como: etiqueta part-of-speech, forma can´onica (lema), gˆenero e n´umero e flex˜ao verbal. O LX-Ner e´ um sistema de reconhecimento de entidades mencionadas que realiza marcac¸o˜ es em entidades dos tipos: pessoa (PER), organizac¸a˜ o (ORG), evento (EVT), localizac¸a˜ o (LOC), trabalho/obra (WRK) e outros (MSC). Todas as ferramentas est˜ao dispon´ıveis online2 . 2 http://lxcenter.di.fc.ul.pt/ 190 Usando as anotac¸o˜ es fornecidas pelas ferramentas descritas acima foram d"
W12-3409,W05-1008,0,0.0342651,"e features it uses, are described in Section 4. Section 5 covers empirical evaluation and comparison with other approaches. Finally, Section 6 concludes with some final remarks. 2 Background and Related Work The construction of a hand-crafted lexicon for a deep grammar is a time-consuming task requiring trained linguists. More importantly, such lexica are invariably incomplete since they often do not cover specialized domains and are slow to incorporate new words. Accordingly, much research in this area has been focused on automatic lexical acquisition (Brent, 1991; Briscoe and Carroll, 1997; Baldwin, 2005). That is, approaches that try to discover all the lexical types a given unknown word may occur with, thus effectively creating a new lexical entry. However, at run-time, it is still up to the grammar using the newly acquired lexical entry to choose which of those lexical types is the correct one for each particular occurrence of that word; and, ultimately, one can only acquire the lexicon entries for those words that are present in the corpus. Thus, any system that is constantly exposed to new text—e.g. parsing text from the Web—will eventually come across some unknown word that has not yet b"
W12-3409,C94-1024,0,0.107758,"Missing"
W12-3409,J99-2004,0,0.0857291,"Missing"
W12-3409,A00-1031,0,0.00867165,".5 The second feature tree, labeled with D as root, encodes the target word—again marked with an asterisk—and the word it is dependent on. In the example shown in Figure 2, since the target word is the main verb of the sentence, the feature tree has no other nodes apart from that of the target word. 5 Evaluation The following evaluation results were obtained following a standard 10-fold cross-validation approach, where the folds were taken from a random shuffle of the sentences in the corpus. We compare the performance of our tree kernel (TK) approach with two other automatic annotators, TnT (Brants, 2000) and SVMTool (Gim´enez and M`arquez, 2004). TnT is a statistical POS tagger, well known for its efficiency—in terms of training and tagging speed—and for achieving state-of-the-art results despite having a quite simple underlying 5 POS tags in the example: V (verb), PREP (preposition) and CN (common noun). C(de, viagem) ADV(dia, de) PRD(dia, segundo) SP(dia, o) PRD(golfinhos, primeiros) SP(golfinhos, os) C(a, dia) TMP(encontr´amos, a) DO(encontr´amos, golfinhos) Figure 1: Dependency representation D H * DO TMP * V CN PREP V encontr´amos we-found golfinhos dolphins a by encontr´amos we-found Fi"
W12-3409,P91-1027,0,0.0999595,"lassifier. The classifier itself, and the features it uses, are described in Section 4. Section 5 covers empirical evaluation and comparison with other approaches. Finally, Section 6 concludes with some final remarks. 2 Background and Related Work The construction of a hand-crafted lexicon for a deep grammar is a time-consuming task requiring trained linguists. More importantly, such lexica are invariably incomplete since they often do not cover specialized domains and are slow to incorporate new words. Accordingly, much research in this area has been focused on automatic lexical acquisition (Brent, 1991; Briscoe and Carroll, 1997; Baldwin, 2005). That is, approaches that try to discover all the lexical types a given unknown word may occur with, thus effectively creating a new lexical entry. However, at run-time, it is still up to the grammar using the newly acquired lexical entry to choose which of those lexical types is the correct one for each particular occurrence of that word; and, ultimately, one can only acquire the lexicon entries for those words that are present in the corpus. Thus, any system that is constantly exposed to new text—e.g. parsing text from the Web—will eventually come"
W12-3409,A97-1052,0,0.0583635,"e classifier itself, and the features it uses, are described in Section 4. Section 5 covers empirical evaluation and comparison with other approaches. Finally, Section 6 concludes with some final remarks. 2 Background and Related Work The construction of a hand-crafted lexicon for a deep grammar is a time-consuming task requiring trained linguists. More importantly, such lexica are invariably incomplete since they often do not cover specialized domains and are slow to incorporate new words. Accordingly, much research in this area has been focused on automatic lexical acquisition (Brent, 1991; Briscoe and Carroll, 1997; Baldwin, 2005). That is, approaches that try to discover all the lexical types a given unknown word may occur with, thus effectively creating a new lexical entry. However, at run-time, it is still up to the grammar using the newly acquired lexical entry to choose which of those lexical types is the correct one for each particular occurrence of that word; and, ultimately, one can only acquire the lexicon entries for those words that are present in the corpus. Thus, any system that is constantly exposed to new text—e.g. parsing text from the Web—will eventually come across some unknown word th"
W12-3409,J07-4004,0,0.0898872,"Missing"
W12-3409,P02-1034,0,0.14639,"Missing"
W12-3409,gimenez-marquez-2004-svmtool,0,0.0510017,"Missing"
W12-3409,E06-1015,0,0.0185769,"est.2 This grammar-supported approach to corpus annotation ensures that the various linguistic annotation layers—morphological, syntactic and semantic—are consistent. The corpus that was used is composed mostly by a subset of the sentences in CETEMP´ublico, a corpus of plain text excerpts from the P´ublico newspaper. After running LXGram and manually disambiguating the parse forests, we were left with a dataset consisting of 5,422 sentences annotated with all the linguistic information provided by LXGram. 4 Classifier and Feature Extraction For training and classification we use SVM-light-TK (Moschitti, 2006), an extension to the widely-used SVM-light (Joachims, 1999) software for SVMs that adds a function implementing the tree kernel introduced in Section 2.2. With SVM-light-TK one can 2 In our setup, two annotators work in a double-blind scheme, where those cases where they disagree are adjudicated by a third annotator. Inter-annotator agreement is 0.86. directly provide one or more tree structures as features (using the standard parenthesis representation of trees) together with the numeric feature vectors that are already accepted by SVM-light. Given that the task at stake is a multi-class cla"
W12-3409,H91-1067,0,\N,Missing
W13-0106,W06-3913,0,0.0317532,"ation from text. Evaluation campaigns like the two TempEval challenges (Verhagen et al., 2010) have brought an increased interest to this topic. The two TempEval challenges focused on ordering the events and the dates and times mentioned in text. Since then, temporal processing has expanded beyond the problems presented in TempEval, like for instance the work of Pan et al. (2011), which is about learning event durations. Temporal information processing is important and related to a number of applications, including event co-reference resolution (Bejan and Harabagiu, 2010), question answering (Ahn et al., 2006; Saquete et al., 2004; Tao et al., 2010) and information extraction (Ling and Weld, 2010). Another application is learning narrative event chains or scripts (Chambers and Jurafsky, 2008b; Regneri et al., 2010), which are “sequences of events that describe some stereotypical human activity” (i.e. eating at a restaurant involves looking at the menu, then ordering food, etc.). This paper focuses on assessing the impact of temporal reasoning on the problem of temporal information extraction. We will show that simple classifiers trained for the TempEval tasks can be improved by extending their fea"
W13-0106,P10-1143,0,0.026408,"n renewed interest in extracting temporal information from text. Evaluation campaigns like the two TempEval challenges (Verhagen et al., 2010) have brought an increased interest to this topic. The two TempEval challenges focused on ordering the events and the dates and times mentioned in text. Since then, temporal processing has expanded beyond the problems presented in TempEval, like for instance the work of Pan et al. (2011), which is about learning event durations. Temporal information processing is important and related to a number of applications, including event co-reference resolution (Bejan and Harabagiu, 2010), question answering (Ahn et al., 2006; Saquete et al., 2004; Tao et al., 2010) and information extraction (Ling and Weld, 2010). Another application is learning narrative event chains or scripts (Chambers and Jurafsky, 2008b; Regneri et al., 2010), which are “sequences of events that describe some stereotypical human activity” (i.e. eating at a restaurant involves looking at the menu, then ordering food, etc.). This paper focuses on assessing the impact of temporal reasoning on the problem of temporal information extraction. We will show that simple classifiers trained for the TempEval tasks"
W13-0106,W06-1623,0,0.0319344,"l relations in isolation, and therefore it is not guaranteed that the resulting ordering is globally consistent. Yoshikawa et al. (2009) and Ling and Weld (2010) overcome this limitation using Markov logic networks (Richardson and Domingos, 2006), or MLNs, which learn probabilities attached to first-order formulas. One participant of the second TempEval used a similar approach (Ha et al., 2010). Denis and Muller (2011) cast the problem of learning temporal orderings from texts as a constraint optimization problem. They search for a solution using Integer Linear Programming (ILP), similarly to Bramsen et al. (2006), and Chambers and Jurafsky (2008a). Because ILP is costly (it is NP-hard), the latter two only consider before and after relations. Most of these approaches are similar to ours in that they can use knowledge about one TempEval task to solve the other tasks. However, these studies do not report on the full set of logical constraints 3 <TIMEX3 tid=""t190"" type=""TIME"" value=""1998-02-06T22:19:00"" functionInDocument=""CREATION TIME"">06/02/1998 22:19:00</TIMEX3> <s>WASHINGTON - A economia <EVENT eid=""e1"">criou</EVENT> empregos a um ritmo surpreendentemente robusto em <TIMEX3 tid=""t191"" type=""DATE"" va"
W13-0106,D08-1073,0,0.0559441,"Missing"
W13-0106,P08-1090,0,0.0947053,"Missing"
W13-0106,costa-branco-2012-timebankpt,1,0.756541,"s” as part of a symbolic solution to this challenge (Pus¸cas¸u, 2007). This world-knowledge component includes rules for reasoning about time. Closest to our work is that of Tatu and Srikanth (2008). The authors employ information about task B and temporal reasoning as a source of classifier features for task C only. This is more limited than our approach: we also explore the other tasks as sources of knowledge, besides task B, and we also experiment with solutions for the other tasks, not just task C. 4 Annotation Scheme and Data For the experiments reported in this paper we used TimeBankPT (Costa and Branco, 2012), which is an adaptation to Portuguese of the English data used in the first TempEval. These data were produced by translating the English data used in the first TempEval and then adapting the annotations so that they conform to the new language. Figure 2 shows a sample of that corpus. As before, that figure is simplified. For instance, the full annotation for the first event event term in that example is: <EVENT eid=""e1"" class= ""OCCURRENCE"" stem=""criar"" aspect=""NONE"" tense=""PPI"" polarity=""POS"" pos=""VERB"">criou</EVENT>. TimeBankPT is similar in size to the English TempEval data. It contains 60"
W13-0106,S10-1076,0,0.0138381,"machine learning methods have become dominant to solve the problem of temporally ordering entities mentioned in text. One major limitation of machine learning methods is that they are typically used to classify temporal relations in isolation, and therefore it is not guaranteed that the resulting ordering is globally consistent. Yoshikawa et al. (2009) and Ling and Weld (2010) overcome this limitation using Markov logic networks (Richardson and Domingos, 2006), or MLNs, which learn probabilities attached to first-order formulas. One participant of the second TempEval used a similar approach (Ha et al., 2010). Denis and Muller (2011) cast the problem of learning temporal orderings from texts as a constraint optimization problem. They search for a solution using Integer Linear Programming (ILP), similarly to Bramsen et al. (2006), and Chambers and Jurafsky (2008a). Because ILP is costly (it is NP-hard), the latter two only consider before and after relations. Most of these approaches are similar to ours in that they can use knowledge about one TempEval task to solve the other tasks. However, these studies do not report on the full set of logical constraints 3 <TIMEX3 tid=""t190"" type=""TIME"" value=""1"
W13-0106,S07-1098,0,0.343449,"which therefore precedes FUTURE REF, etc.). So, in additional to the representation of times and dates as time intervals, we employ a layer of ad-hoc rules. The variety of temporal expressions makes it impossible to provide a full account of the implemented rules in this paper, but they are listed in full in Costa (2013). 6 Experiment and Results Our goal is to test the features introduced in Section 5. Our methodology is to extend existing classifiers for the problem of temporal relation classification with these features, and check whether their performance improves. For the first TempEval, Hepple et al. (2007) used simple classifiers that use the annotations present in the annotated data as features. They trained Weka (Witten and Frank, 1999) classifiers with these features and obtained competitive results. 10-fold cross-validation on the training data was employed to evaluate different combinations of features. For our baselines, we use the same approach as Hepple et al. (2007), with the Portuguese data mentioned above in Section 4. 6.1 Experimental Setup The classifier features used in the baselines are also similar to the ones used by Hepple et al. (2007). The event features correspond to attrib"
W13-0106,W01-1315,0,0.0145031,"ede this event. That is, the possible relation type for the relation represented with the TLINK named l1 is constrained—it cannot be AFTER. What this means is that, in this example, solving task B can, at least partially, solve task A. The information obtained by solving task B can be utilized in order to improve the solutions for task A. 3 Related Work The literature on automated temporal reasoning includes important pieces of work such as Allen (1984); Vilain et al. (1990); Freksa (1992). A lot of the work in this area has focused on finding efficient methods to compute temporal inferences. Katz and Arosio (2001) used a temporal reasoning system to compare the temporal annotations of two annotators. In a similar spirit, Setzer and Gaizauskas (2001) first compute the deductive closure of annotated temporal relations so that they can then assess annotator agreement with standard precision and recall measures. Verhagen (2005) uses temporal closure as a means to aid TimeML annotation, that is as part of a mixed-initiative approach to annotation. He reports that closing a set of manually annotated temporal relations more than quadruples the number of temporal relations in TimeBank (Pustejovsky et al., 2003"
W13-0106,P06-1095,0,0.14196,"annotators. In a similar spirit, Setzer and Gaizauskas (2001) first compute the deductive closure of annotated temporal relations so that they can then assess annotator agreement with standard precision and recall measures. Verhagen (2005) uses temporal closure as a means to aid TimeML annotation, that is as part of a mixed-initiative approach to annotation. He reports that closing a set of manually annotated temporal relations more than quadruples the number of temporal relations in TimeBank (Pustejovsky et al., 2003), a corpus that is the source of the data used for the TempEval challenges. Mani et al. (2006) use temporal reasoning as an oversampling method to increase the amount of training data. Even though this is an interesting idea, the authors recognized in subsequent work that there were methodological problems in this work which invalidate the results (Mani et al., 2007). Since the advent of TimeBank and the TempEval challenges, machine learning methods have become dominant to solve the problem of temporally ordering entities mentioned in text. One major limitation of machine learning methods is that they are typically used to classify temporal relations in isolation, and therefore it is n"
W13-0106,J11-4005,0,0.0147673,"cal inference can be used to improve—sometimes dramatically— existing machine learned classifiers for the problem of temporal relation classification. 1 Introduction Recent years have seen renewed interest in extracting temporal information from text. Evaluation campaigns like the two TempEval challenges (Verhagen et al., 2010) have brought an increased interest to this topic. The two TempEval challenges focused on ordering the events and the dates and times mentioned in text. Since then, temporal processing has expanded beyond the problems presented in TempEval, like for instance the work of Pan et al. (2011), which is about learning event durations. Temporal information processing is important and related to a number of applications, including event co-reference resolution (Bejan and Harabagiu, 2010), question answering (Ahn et al., 2006; Saquete et al., 2004; Tao et al., 2010) and information extraction (Ling and Weld, 2010). Another application is learning narrative event chains or scripts (Chambers and Jurafsky, 2008b; Regneri et al., 2010), which are “sequences of events that describe some stereotypical human activity” (i.e. eating at a restaurant involves looking at the menu, then ordering f"
W13-0106,S07-1108,0,0.0793874,"Missing"
W13-0106,P10-1100,0,0.0130855,"nd the dates and times mentioned in text. Since then, temporal processing has expanded beyond the problems presented in TempEval, like for instance the work of Pan et al. (2011), which is about learning event durations. Temporal information processing is important and related to a number of applications, including event co-reference resolution (Bejan and Harabagiu, 2010), question answering (Ahn et al., 2006; Saquete et al., 2004; Tao et al., 2010) and information extraction (Ling and Weld, 2010). Another application is learning narrative event chains or scripts (Chambers and Jurafsky, 2008b; Regneri et al., 2010), which are “sequences of events that describe some stereotypical human activity” (i.e. eating at a restaurant involves looking at the menu, then ordering food, etc.). This paper focuses on assessing the impact of temporal reasoning on the problem of temporal information extraction. We will show that simple classifiers trained for the TempEval tasks can be improved by extending their feature set with features that can be computed with automated reasoning. 2 Temporal Information Processing The two TempEval challenges made available annotated data sets for the training and evaluation of temporal"
W13-0106,P04-1072,0,0.0977665,"Missing"
W13-0106,W01-1311,0,0.0566698,"AFTER. What this means is that, in this example, solving task B can, at least partially, solve task A. The information obtained by solving task B can be utilized in order to improve the solutions for task A. 3 Related Work The literature on automated temporal reasoning includes important pieces of work such as Allen (1984); Vilain et al. (1990); Freksa (1992). A lot of the work in this area has focused on finding efficient methods to compute temporal inferences. Katz and Arosio (2001) used a temporal reasoning system to compare the temporal annotations of two annotators. In a similar spirit, Setzer and Gaizauskas (2001) first compute the deductive closure of annotated temporal relations so that they can then assess annotator agreement with standard precision and recall measures. Verhagen (2005) uses temporal closure as a means to aid TimeML annotation, that is as part of a mixed-initiative approach to annotation. He reports that closing a set of manually annotated temporal relations more than quadruples the number of temporal relations in TimeBank (Pustejovsky et al., 2003), a corpus that is the source of the data used for the TempEval challenges. Mani et al. (2006) use temporal reasoning as an oversampling"
W13-0106,C08-1108,0,0.0253046,"al relations are consistent. This is a limitation of our approach that is not present in some of the above mentioned work. However, our approach is not sensitive to the size of the training data, since the reasoning rules are hand-coded. With MLNs, even though the rules are also designed by humans, the weight of each rule still has to be learned in training. One participant of the first TempEval used “world-knowledge axioms” as part of a symbolic solution to this challenge (Pus¸cas¸u, 2007). This world-knowledge component includes rules for reasoning about time. Closest to our work is that of Tatu and Srikanth (2008). The authors employ information about task B and temporal reasoning as a source of classifier features for task C only. This is more limited than our approach: we also explore the other tasks as sources of knowledge, besides task B, and we also experiment with solutions for the other tasks, not just task C. 4 Annotation Scheme and Data For the experiments reported in this paper we used TimeBankPT (Costa and Branco, 2012), which is an adaptation to Portuguese of the English data used in the first TempEval. These data were produced by translating the English data used in the first TempEval and"
W13-0106,P09-1046,0,0.0195183,"amount of training data. Even though this is an interesting idea, the authors recognized in subsequent work that there were methodological problems in this work which invalidate the results (Mani et al., 2007). Since the advent of TimeBank and the TempEval challenges, machine learning methods have become dominant to solve the problem of temporally ordering entities mentioned in text. One major limitation of machine learning methods is that they are typically used to classify temporal relations in isolation, and therefore it is not guaranteed that the resulting ordering is globally consistent. Yoshikawa et al. (2009) and Ling and Weld (2010) overcome this limitation using Markov logic networks (Richardson and Domingos, 2006), or MLNs, which learn probabilities attached to first-order formulas. One participant of the second TempEval used a similar approach (Ha et al., 2010). Denis and Muller (2011) cast the problem of learning temporal orderings from texts as a constraint optimization problem. They search for a solution using Integer Linear Programming (ILP), similarly to Bramsen et al. (2006), and Chambers and Jurafsky (2008a). Because ILP is costly (it is NP-hard), the latter two only consider before and"
W13-0106,S10-1010,0,\N,Missing
W15-0208,E09-1005,0,0.149443,"ll also be addressed. 1 Introduction Annotated corpora are a cornerstone of Natural Language Processing (NLP), supporting the analysis of large quantities of text across a wide variety of contexts (Leech, 2004) and the development and evaluation of processing tools. There has been an increased interest in “high quality linguistic annotations of corpora” at the semantic level, with word senses in particular being “both elusive and central to many areas of NLP” (Passonneau et al., 2012). Sense annotated corpora are useful, for example, as training data for Word Sense Disambiguation (WSD) tools (Agirre and Soroa, 2009), many of which are based on the Princeton WordNet approach to the lexical semantics of nouns, verbs, adjectives and adverbs (Fellbaum, 1998). This format is widely used to build sense-annotated corpora in a variety of languages—examples include parallel corpora such as the English/Italian MultiSemCor (Bentivogli and Pianta, 2002) and corpora in languages such as Japanese, Bulgarian, German, Polish and many more (Global WordNet Association, 2013). Despite the need for these corpora to train and test new and developing WSD approaches (Wu et al., 2007), tools for manual sense-annotation are not"
W15-0208,bentivogli-pianta-2002-opportunistic,0,0.0509746,"stic annotations of corpora” at the semantic level, with word senses in particular being “both elusive and central to many areas of NLP” (Passonneau et al., 2012). Sense annotated corpora are useful, for example, as training data for Word Sense Disambiguation (WSD) tools (Agirre and Soroa, 2009), many of which are based on the Princeton WordNet approach to the lexical semantics of nouns, verbs, adjectives and adverbs (Fellbaum, 1998). This format is widely used to build sense-annotated corpora in a variety of languages—examples include parallel corpora such as the English/Italian MultiSemCor (Bentivogli and Pianta, 2002) and corpora in languages such as Japanese, Bulgarian, German, Polish and many more (Global WordNet Association, 2013). Despite the need for these corpora to train and test new and developing WSD approaches (Wu et al., 2007), tools for manual sense-annotation are not easy to come by. Finding any information at all about such tools is difficult, and those that are described are often done so in the context of the specific purposes for which they were developed. For example, the tools used to manually annotate the English MASC Corpus (Passonneau et al., 2012) and Chinese Word Sense Annotated Cor"
W15-0208,E06-2024,1,0.761276,"on-style WordNet. We are using this tool to produce a gold-standard corpus annotated with senses from our Portuguese WordNet for use in our own WSD tasks, and in this paper describe how its usability and flexibility make it well-suited to similar manual annotation tasks using source texts and WordNet-based lexicons in a variety of different languages. 2 Importing Text The current implementation of LX-SenseAnnotator is designed for the import of text files that have already been tagged and morphologically analyzed (in particular, POS-tagged and lemmatized) in an existing pipeline of NLP tools (Branco and Silva, 2006). POS-tagging in particular makes the separation of the input text according to which words are and are not sense-taggable (as described in the next section) very straightforward. It is of course assumed that the preprocessed tags in the input text have been verified and are correct. A goal for LX-SenseAnnotator is to support the import of source text in a variety of different formats. The code that currently reads and interprets input text is stored in a stand-alone C++ function, making it easy for the tool to be tuned to allow texts pre-processed using different types of tagsets to be import"
W15-0208,P08-4004,0,0.0601329,"Missing"
W15-0208,passonneau-etal-2012-masc,0,0.0844132,"f LX-SenseAnnotator, including the support of a variety of languages and the handling of pre-processed texts with different tagsets, will also be addressed. 1 Introduction Annotated corpora are a cornerstone of Natural Language Processing (NLP), supporting the analysis of large quantities of text across a wide variety of contexts (Leech, 2004) and the development and evaluation of processing tools. There has been an increased interest in “high quality linguistic annotations of corpora” at the semantic level, with word senses in particular being “both elusive and central to many areas of NLP” (Passonneau et al., 2012). Sense annotated corpora are useful, for example, as training data for Word Sense Disambiguation (WSD) tools (Agirre and Soroa, 2009), many of which are based on the Princeton WordNet approach to the lexical semantics of nouns, verbs, adjectives and adverbs (Fellbaum, 1998). This format is widely used to build sense-annotated corpora in a variety of languages—examples include parallel corpora such as the English/Italian MultiSemCor (Bentivogli and Pianta, 2002) and corpora in languages such as Japanese, Bulgarian, German, Polish and many more (Global WordNet Association, 2013). Despite the ne"
W15-0208,W07-1521,0,0.0172516,"rd Sense Disambiguation (WSD) tools (Agirre and Soroa, 2009), many of which are based on the Princeton WordNet approach to the lexical semantics of nouns, verbs, adjectives and adverbs (Fellbaum, 1998). This format is widely used to build sense-annotated corpora in a variety of languages—examples include parallel corpora such as the English/Italian MultiSemCor (Bentivogli and Pianta, 2002) and corpora in languages such as Japanese, Bulgarian, German, Polish and many more (Global WordNet Association, 2013). Despite the need for these corpora to train and test new and developing WSD approaches (Wu et al., 2007), tools for manual sense-annotation are not easy to come by. Finding any information at all about such tools is difficult, and those that are described are often done so in the context of the specific purposes for which they were developed. For example, the tools used to manually annotate the English MASC Corpus (Passonneau et al., 2012) and Chinese Word Sense Annotated Corpus (Wu et al., 2007) both seem intrinsically tied to those particular corpora. Such examples demonstrate the need for a more open, flexible solution for manual word-sense annotation that is more “readily adaptable to differ"
W15-4101,E06-2024,1,0.776568,"Europarl (Koehn, 2005). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. Analysis Analysis proceeds in two stages. The first stage is a shallow syntactic analysis that takes us from the surface string to what in the Treex framework is called the a-layer (analytical layer), which is a grammatical dependency graph. The second stage is a deep syntactic analysis that takes us from the a-layer to the t-layer (tectogrammatical layer). 2.1.1 Getting the a-layer We resort to LX-Suite (Branco and Silva, 2006), a set of pre-existing shallow processing tools for Portuguese that include a sentence segmenter, a tokenizer, a POS tagger, a morphological analyser and a dependency parser, all with state-of-the-art performance. Treex blocks were created to call and interface with these tools. After running the shallow processing tools, the dependency output of the parser is converted into Universal Dependencies (UD, (de Marneffe et al., 2014)). These dependencies are then converted into the a-layer tree (a-tree) in a second step. Both steps are implemented as rule-based Treex blocks. Taking this two-tiered"
W15-4101,de-marneffe-etal-2014-universal,0,0.0825014,"Missing"
W15-4101,2005.mtsummit-papers.11,0,0.0109181,"pro-dropped pronouns.1 2.2 Transfer Transfer is handled by a tree-to-tree maximum entropy translation model (Mareˇcek et al., 2010) working at the deep syntactic level of Tecto trees. This transfer model assumes that the source and target trees are isomorphic. This limitation is rarely a problem since at the Tecto level, as one would expect from a deep syntactic representation, the source and target trees are often isomorphic. Since the trees are isomorphic, the model is concerned only with learning mappings between t-tree nodes. The model was trained over 1.9 million sentences from Europarl (Koehn, 2005). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. Analysis Analysis proceeds in two stages. The first stage is a shallow syntactic analysis that takes us from the surface string to what in the Treex framework is called the a-layer (analytical layer), which is a grammatical dependency graph. The second stage is a deep syntactic analysis that takes us from the a-layer to the t-layer (tectogrammatical layer). 2.1.1 Getting the a-layer We resort to LX-Suite (Branco and Silva, 2006"
W15-4101,W10-1730,0,0.561457,"Missing"
W15-4101,W08-0325,0,0.305184,"as hierarchical methods and tree-to-tree mappings, but these methods have been unable to clearly improve on the phrase-based state-of-the-art. There is a growing opinion that the previous approaches to SMT may be reaching a performance ceiling and that pushing beyond it will require approaches that are more linguistically informed and that are able to bring semantics into the process. 2 Translation pipeline Our pipeline is built upon the Treex system (Popel ˇ and Zabokrtsk´ y, 2010), a modular NLP framework used mostly for MT and the most recent ˇ incarnation of the TectoMT system (Zabokrtsk´ y et al., 2008). Treex uses an analysis-transfersynthesis architecture, with transfer being done at the deep syntactic level, where a Tectogrammatical (Tecto) formal description is used. 1 Proceedings of the ACL 2015 Fourth Workshop on Hybrid Approaches to Translation (HyTra), pages 1–5, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics 2.1.2 The choice of Treex as the supporting framework was motivated by several reasons. Firstly, Treex is a tried and tested framework that has been shown to achieve very good results in English to Czech translation, on a par with phrasebased SMT"
W15-4101,P07-2045,0,\N,Missing
W15-4101,W13-2201,0,\N,Missing
W15-5503,agirre-soroa-2008-using,0,0.0220661,"using the smaller, languagespecific Portuguese MultiWordNet (MultiWordNet, nd) as the underlying lexical knowledge base (LKB) for the WSD, and 2) translating open-class words in the input text from Portuguese to English in order to run WSD using the much larger English WordNet as the underlying LKB. The contributions from our results are twofold: Progress in knowledge-based WSD has largely been driven by the development of graph-based disambiguation methods, as pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008). Graph-based methods allow LKBs such as WordNets to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between nodes. The strength of the edge between two nodes, corresponding to the relationship or dependency between two synsets, can then be calculated using semantic similarity measures such as the Lesk algorithm (Lesk, 1986). • Performing graph-based WSD using a smaller, language-specific LKB (Portuguese MultiWordNet) provides better results than translating terms to English in order"
W15-5503,E09-1005,0,0.237514,"e some related work (Section 2), before describing an implementation of graphbased WSD for Portuguese (Section 3). Next, we present our evaluation of the two approaches to WSD in Portuguese, using a gold-standard, human-annotated corpus for comparison (Section 4). Finally, we discuss the possible ramifications of our findings in the context of LOD (Section 5), before presenting our conclusions (Section 6). 2 2.1 For WSD tasks, graph-based representations of LKBs can then be used to choose the most likely sense of a word in a given context, based on the dependencies between nodes in the graph (Agirre and Soroa, 2009). Algorithms such as PageRank (Brin and Page, 1998) allow for the weights and probabilities of directed links between target words and words in their local context to be spread over the entirety of the graph (Agirre and Soroa, 2009). Nodes (senses) ‘recommend’ each other based on their own importance – with the importance of any given node being higher or lower depending on the importance of other nodes which recommend it – and then follow a ‘random walk’ over the rest of the graph based on the importance of the nodes to whose edges they are attached (Mihalcea, 2005; Agirre and Soroa, 2009). A"
W15-5503,S07-1008,0,0.0635913,"Missing"
W15-5503,J14-1003,0,0.0222562,"Missing"
W15-5503,H05-1052,0,0.0794691,"evenson et al., 2011; Preiss and Stevenson, 2013). in Portuguese; 1) using the smaller, languagespecific Portuguese MultiWordNet (MultiWordNet, nd) as the underlying lexical knowledge base (LKB) for the WSD, and 2) translating open-class words in the input text from Portuguese to English in order to run WSD using the much larger English WordNet as the underlying LKB. The contributions from our results are twofold: Progress in knowledge-based WSD has largely been driven by the development of graph-based disambiguation methods, as pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008). Graph-based methods allow LKBs such as WordNets to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between nodes. The strength of the edge between two nodes, corresponding to the relationship or dependency between two synsets, can then be calculated using semantic similarity measures such as the Lesk algorithm (Lesk, 1986). • Performing graph-based WSD using a smaller, language-specific LKB (Portuguese Mult"
W15-5503,atserias-etal-2004-spanish,0,0.0475901,"some tasks (Agirre et al., 2014). Related Work Knowledge and graph-based WSD While WSD has traditionally delivered its best results using supervised and unsupervised machine learning methods, domain-specific knowledgebased WSD can now perform as well or better than a more generic, supervised machine learningbased WSD approach (Agirre et al., 2009). For example, in the medical domain good results have been obtained in WSD tasks by creating an 7 2.2 in the context of WSD. Agirre and Soroa (2009) evaluated their graph-based WSD algorithm using the Spanish WordNet of approximately 67,000 senses (Atserias et al., 2004) as their LKB. They obtained promising results that approach those reported using the supervised ‘most frequent sense’ (MFS) baseline system for the SemEval-2007 Task 09 dataset (M`arquez et al., 2007). More recently, graph-based WSD performed over Spanish Babelnet senses as the LKB was shown to improve over the MFS baseline in the Multilingual Word Sense Disambiguation task at SemEval2013 (Navigli et al., 2013). These results are encouraging for the case of Portuguese, demonstrating that knowledge-based WSD produces good results using LKBs specific to similar languages. For Portuguese, it wou"
W15-5503,barreto-etal-2006-open,1,0.765626,"Missing"
W15-5503,E06-2024,1,0.741074,"when building the graph, which our own experimentation and previous reporting of results using UKB (Agirre and Soroa, 2009; Agirre et al., 2014) have both shown to result in more accurate WSD. UKB first accepts input texts in a ‘context’ format, where each sentence in a text is treated as an individual context containing the target word and all other open-class words (nouns, verbs, adjectives and adverbs) from the original sentence. This context file can be easily extracted and arranged from input texts pre-tagged with lemmas and partof-speech (PoS) tags, which we produce using the LX-Suite (Branco and Silva, 2006), a collection of shallow processing tools for Portuguese. UKB then performs WSD for each sentence in the context file, using a PageRank-based (Brin and Page, 1998) random walk to return the probability of each node (synset) in a given graph being semantically related to a target word, and returning the appropriate synset identifier for the most likely node. It is this use of the words surrounding a target word in the context file – which are also included as nodes in the graph and whose relevance thus affects the final decision on which sense to assign – that separates UKB from similar algori"
W15-5503,S07-1006,0,0.0236199,"with the terminology that is accounted for being specific to the language in question. A glance at the original and translated context files used in our comparison shows that in many Table 2 compares the performance of UKB over the Portuguese MultiWordNet with the results obtained by Agirre et al. (2014), who most recently reported on the performance of UKB as F1 over four different datasets – the Senseval-2 (Palmer et al., 2001), Senseval-3 (Snyder and Palmer, 2004), Semeval-2007 fine-grained (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) and Semeval-2007 coarse-graned (Navigli et al., 2007) English all-words tasks. Although the results they present cover various disambiguation options within UKB, we focus here on the results they obtained using the ppr w2w UKB method (as we have). We also assume that they continue using version 3.0 of the English WordNet (complete with information on the semantic relationships between glosses) as their underlying LKB, as they have reported in previous evaluations (Agirre and Soroa, 2009). This combination of UKB option and underlying LKB is comparable with our own evaluation of UKB over the Portuguese MultiWordNet. The 19,700 verified synsets fr"
W15-5503,W15-0208,1,0.830706,"roximately 1 million tokens manually annotated with lemmas, part-of-speech, inflection, and named entities, which are compatible with the input and output formats of the tools in the LXSuite. The corpus contains data from both written sources and transcriptions of spoken Portuguese – we have used the data from the written part, sourced mainly from newspaper articles and short novels and comprising approximately 700,000 tokens, of which 193,443 are open class words. Word senses were manually chosen and assigned to open-class words by a team of human annotators using the LX-SenseAnnotator tool (Neale et al., 2015), a graphical user interface for assigning senses from WordNet-style lexicons to pretagged input texts. The lexicon from which annotators were able to choose senses was the same Portuguese MultiWordNet (approximately 19,700 verified synsets) used in the evaluation. Because annotators were only able to select from the words and synets present in the Portuguese MultiWordNet, not all of the open-class words in the corpus were able to be annotated. 4.2 Performance for Portuguese Running the UKB algorithm over the manually disambiuated CINTIL corpus, we can see how well the two approaches – disambi"
W15-5503,P07-1006,0,0.021086,"Missing"
W15-5503,S01-1005,0,0.0598606,"ese that have much more specific categories in English (N´obrega and Pardo, 2014). While their coverage may be less due to their smaller size, language-specific LKBs limit such problems, with the terminology that is accounted for being specific to the language in question. A glance at the original and translated context files used in our comparison shows that in many Table 2 compares the performance of UKB over the Portuguese MultiWordNet with the results obtained by Agirre et al. (2014), who most recently reported on the performance of UKB as F1 over four different datasets – the Senseval-2 (Palmer et al., 2001), Senseval-3 (Snyder and Palmer, 2004), Semeval-2007 fine-grained (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) and Semeval-2007 coarse-graned (Navigli et al., 2007) English all-words tasks. Although the results they present cover various disambiguation options within UKB, we focus here on the results they obtained using the ppr w2w UKB method (as we have). We also assume that they continue using version 3.0 of the English WordNet (complete with information on the semantic relationships between glosses) as their underlying LKB, as they have reported in previous evaluatio"
W15-5503,P06-3010,0,0.0205559,"described in this paper, we use UKB, a collection of tools and algorithms (Agirre and Soroa, 2009; Agirre et al., 2014) for performing graph-based WSD over a pre-existing knowledge base. We use UKB for two reasons: Current state of WSD in Portuguese Portuguese-specific WSD has also followed the knowledge-based trend. Early work focused on the automatic generation of disambiguation rules based on representations of meaning in preannotated corpora (Specia et al., 2005), before exploring hybrid approaches that leverage the relationships between different knowledge sources to support such rules (Specia, 2006; Specia et al., 2007). More recent work has focused on graph-based methods, leveraging WordNets as LKBs (N´obrega and Pardo, 2014). However, this work assumes that translating Portuguese terms into English and then querying the English WordNet is sufficient for representing most of the senses found in Portuguese texts. Spanish, which shares a degree of similarity with Portuguese, has been more widely explored • UKB includes tools for automatically creating graph-based representations of LKBs in WordNet-style formats. • The algorithm used by UKB for performing WSD over the graph itself has bee"
W15-5503,S07-1016,0,0.0143434,"ler size, language-specific LKBs limit such problems, with the terminology that is accounted for being specific to the language in question. A glance at the original and translated context files used in our comparison shows that in many Table 2 compares the performance of UKB over the Portuguese MultiWordNet with the results obtained by Agirre et al. (2014), who most recently reported on the performance of UKB as F1 over four different datasets – the Senseval-2 (Palmer et al., 2001), Senseval-3 (Snyder and Palmer, 2004), Semeval-2007 fine-grained (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) and Semeval-2007 coarse-graned (Navigli et al., 2007) English all-words tasks. Although the results they present cover various disambiguation options within UKB, we focus here on the results they obtained using the ppr w2w UKB method (as we have). We also assume that they continue using version 3.0 of the English WordNet (complete with information on the semantic relationships between glosses) as their underlying LKB, as they have reported in previous evaluations (Agirre and Soroa, 2009). This combination of UKB option and underlying LKB is comparable with our own evaluation of UKB over the P"
W15-5503,N13-3001,0,0.0198652,"example, the word ‘bank’ could be interpreted in the sense of the financial institution or as the slope of land at the side of a river, depending on the context in which it is used. Target words are disambiguated based on their context (determined based on the words surrounding 6 Proceedings of the Second Workshop on Natural Language Processing and Linked Open Data, pages 6–15, Hissar, Bulgaria, 11 September 2015. LKB from the Unified Medical Language System (UMLS) Metathesarurus, a collection of more than one million biomedical concepts and five million concept names (Stevenson et al., 2011; Preiss and Stevenson, 2013). in Portuguese; 1) using the smaller, languagespecific Portuguese MultiWordNet (MultiWordNet, nd) as the underlying lexical knowledge base (LKB) for the WSD, and 2) translating open-class words in the input text from Portuguese to English in order to run WSD using the much larger English WordNet as the underlying LKB. The contributions from our results are twofold: Progress in knowledge-based WSD has largely been driven by the development of graph-based disambiguation methods, as pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli"
W15-5503,W04-0811,0,0.0572555,"ategories in English (N´obrega and Pardo, 2014). While their coverage may be less due to their smaller size, language-specific LKBs limit such problems, with the terminology that is accounted for being specific to the language in question. A glance at the original and translated context files used in our comparison shows that in many Table 2 compares the performance of UKB over the Portuguese MultiWordNet with the results obtained by Agirre et al. (2014), who most recently reported on the performance of UKB as F1 over four different datasets – the Senseval-2 (Palmer et al., 2001), Senseval-3 (Snyder and Palmer, 2004), Semeval-2007 fine-grained (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) and Semeval-2007 coarse-graned (Navigli et al., 2007) English all-words tasks. Although the results they present cover various disambiguation options within UKB, we focus here on the results they obtained using the ppr w2w UKB method (as we have). We also assume that they continue using version 3.0 of the English WordNet (complete with information on the semantic relationships between glosses) as their underlying LKB, as they have reported in previous evaluations (Agirre and Soroa, 2009). This comb"
W15-5708,E09-1005,0,0.355315,"into which we implement it, and the two approaches we have taken to making use of the information output by the WSD process: 1) forcing information into the input sentences (directly affecting the alignment of words before the translation model is trained), and 2) including information as features in a maxent-based translation model (which does not affect word alignment but rather directly influences the training of the translation model). 3.1 WSD algorithm - UKB To perform WSD we use UKB, a collection of tools and algorithms for performing graph-based WSD over a pre-existing knowledge base (Agirre and Soroa, 2009; Agirre et al., 2014). Graph-based WSD, as pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008), allows knowledge bases such as WordNets to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between nodes. The strength of the edge between two nodes, corresponding to the relationship or dependency between two synsets, can then be calculated using semantic similarity measures such as th"
W15-5708,J14-1003,0,0.10501,"Missing"
W15-5708,P07-1020,0,0.0135736,". In TectoMT, for each word in the source language that has more than one possible translation in the target language a maxent model exists to determine the probability of any of those translations being correct based on contextual features such as neighbouring words – words with only one translation have no ambiguity, and hence no need of a maxent model. For statistical machine translation systems, previous research suggests that maxent-based translation models are an effective way of leveraging the context provided by the neighbouring words of source sentences (Ittycheriah and Roukos, 2007; Bangalore et al., 2007). In order for maxent models to be created, analysis must have been performed on both the source and target languages, in order for the models to be trained based on aligned parallel treebanks of sentences represented as tectogrammatical (deep-syntax) trees. The maxent model for each word is trained using a list of ‘samples’, which are themselves vectors between contextual features in the source language‘node’ (the tectogrammatical representation of the given word) and an output label (e.g. the lemma of the given word). Contextual features might include information (such as lemmas) from neighb"
W15-5708,P05-1048,0,0.724477,"‘knowledge-based’, with those classes of words stored in lexical ontologies such as WordNet (Fellbaum, 1998), where the collective meanings of open-class words (nouns, verbs, adjectives and adverbs) are grouped together as ‘synsets’. For tasks such as machine translation, ambiguous terms are a major potential source of errors, as identical words with different meanings will normally have different target translations (Xiong and Zhang, 2014). Thus, it has long been assumed that in order for a machine translation system to be optimally successful, it must incorporate some kind of WSD component (Carpuat and Wu, 2005). Most attempts to integrate WSD components into machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by a complete reformulation of the disambiguation process as a multi-word ‘phrase sense’ disambiguation approach, yeilding some improvements in translation quality (Carpuat and Wu, 2007). More recently, a ‘word sense induction’ approach that assigns word senses without the need for predefined sense inventories (such as WordNets) has been explore"
W15-5708,2007.tmi-papers.6,0,0.460294,"2014). Thus, it has long been assumed that in order for a machine translation system to be optimally successful, it must incorporate some kind of WSD component (Carpuat and Wu, 2005). Most attempts to integrate WSD components into machine translation systems have met with mixed – and usually limited – success. Early attempts at ‘projecting’ word senses directly into a machine translation system (Carpuat and Wu, 2005) were followed by a complete reformulation of the disambiguation process as a multi-word ‘phrase sense’ disambiguation approach, yeilding some improvements in translation quality (Carpuat and Wu, 2007). More recently, a ‘word sense induction’ approach that assigns word senses without the need for predefined sense inventories (such as WordNets) has been explored (Xiong and Zhang, 2014), but the question of whether pure word senses from traditional, knowledge-based WSD approaches can be useful for machine translation still remains. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 64 Proceedings of the 1st Deep Machine Translation Wo"
W15-5708,P07-1005,0,0.282073,"Missing"
W15-5708,W07-0719,0,0.594846,"Missing"
W15-5708,N07-1008,0,0.0172616,"are therefore not independent. In TectoMT, for each word in the source language that has more than one possible translation in the target language a maxent model exists to determine the probability of any of those translations being correct based on contextual features such as neighbouring words – words with only one translation have no ambiguity, and hence no need of a maxent model. For statistical machine translation systems, previous research suggests that maxent-based translation models are an effective way of leveraging the context provided by the neighbouring words of source sentences (Ittycheriah and Roukos, 2007; Bangalore et al., 2007). In order for maxent models to be created, analysis must have been performed on both the source and target languages, in order for the models to be trained based on aligned parallel treebanks of sentences represented as tectogrammatical (deep-syntax) trees. The maxent model for each word is trained using a list of ‘samples’, which are themselves vectors between contextual features in the source language‘node’ (the tectogrammatical representation of the given word) and an output label (e.g. the lemma of the given word). Contextual features might include information (su"
W15-5708,D09-1040,0,0.0314399,"synset identifiers of the parent nodes of words as features in the maxent-based translation model – all of which produce results significantly below our baseline machine translation system. While these results seem counterintuitive – more information should provide more constraints on the probabilities of alignments and pairings between words being made – we interpret them as showing that the extra data we introduce to the translation model with these approaches has resulted in too much sparsity, rather than constraint. It would be interesting in future work to explore whether a paraphrasing (Marton et al., 2009) or synonym-based approach as opposed to a strictly word sense-based approach might yield different outcomes. While the work we report in this paper is in a preliminary state, the small improvement achieved by adding synset identifiers as features of single nodes in a maxent-based translation model does represent a step in the right direction, and merits further discussion and experimentation. The results reported here are based on a very controlled evaluation, trained on a small, in-domain corpus. We acknowledge that training on large, open domain corpora such as Europarl might produce differ"
W15-5708,H05-1052,0,0.0889289,"rcing information into the input sentences (directly affecting the alignment of words before the translation model is trained), and 2) including information as features in a maxent-based translation model (which does not affect word alignment but rather directly influences the training of the translation model). 3.1 WSD algorithm - UKB To perform WSD we use UKB, a collection of tools and algorithms for performing graph-based WSD over a pre-existing knowledge base (Agirre and Soroa, 2009; Agirre et al., 2014). Graph-based WSD, as pioneered by a number of researchers (Navigli and Velardi, 2005; Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Agirre and Soroa, 2008), allows knowledge bases such as WordNets to be represented as weighted graphs, where word senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between nodes. The strength of the edge between two nodes, corresponding to the relationship or dependency between two synsets, can then be calculated using semantic similarity measures such as the Lesk algorithm (Lesk, 1986). UKB uses graph-based representations of knowledge bases to choose the most likely sense of a"
W15-5708,P14-1137,0,0.519755,"corporation of traditional WSD into machine translation. In this paper, we present preliminary results that suggest that incorporating output from WSD as contextual features in a maxent-based translation model yields a slight improvement in the quality of machine translation and is potentially a step in the right direction, in contrast to other approaches to introducing word senses into a machine translation system which significantly impede its performance. 1 Introduction Ambiguity is a common problem in language, caused by the phenomena of identical words having multiple, distinct meanings (Xiong and Zhang, 2014). To use a classic example, the word ‘bank’ could be interpreted in the sense of the financial institution or as the slope of land at the side of a river, depending on the context in which it is used. In natural language processing (NLP), word sense disambiguation (WSD) refers to the process of solving this problem by determining the ‘sense’ or meaning of a word when used in a particular context (Agirre and Edmonds, 2006). In computational terms, WSD is a classification task, where the context in which a target word is used provides evidence that helps to determine which class of words – sense"
W15-5713,2003.mtsummit-systems.1,0,0.0960608,", we also conduct a human evaluation and error analysis of their output sentences. The remainder of the paper is organised as follows: Section 2 introduces studies that are relevant to our work; Section 3 describes the corpora, MT systems, experimental setup, goals and evaluation procedures; Section 4 presents and discusses the results of both automatic and human evaluation; and Section 5 summarises the findings of this study and gives directions for future work. 2 Related Work The rule-based machine translation (MT) systems, such as Systran (Toma, 1977), ETAP-3 (Boguslavsky, 1995), and Lucy (Alonso and Thurmair, 2003), required linguistic expertise to operate and were difficult This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 106 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 106–115, Praha, Czech Republic, 3–4 September 2015. to adapt to different languages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine trans"
W15-5713,D11-1033,0,0.0237704,"s data for a particular language pair, another problem arises if the SMT system is needed for a different domain, as the training data may not cover the specific vocabulary or sentence constructions present in the targeted domain. In order to address this problem, many domain-adaptation techniques for SMT have been proposed, ranging from simply adding out-of-domain data to the small amount of in-domain data for training (Foster and Kuhn, 2007) to more sophisticated techniques, such as selecting only particular sentences from the out-of-domain data which are most similar to the in-domain data (Axelrod et al., 2011) or are similar to the sentences with the lowest translation quality (Banerjee et al., 2015). Hybrid MT systems, in turn, aim to exploit the best of both SMT and rule-based approaches, usually either by combining rule-based transfer with statistical language models in the synthesis phase (Habash and Dorr, 2002), or by combining rule-based with statistical approaches at different points of the Vauquois ˇ triangle, as the TectoMT system (Zabokrtsk´ y et al., 2008) that we use in this study. 2.1 English-Portuguese MT The English-Portuguese translation model built using the standard PBSMT system i"
W15-5713,E06-2024,1,0.763003,". Figure 2: An example of the a-trees and t-trees in the TectoMT system (the input EN sentence: “Try pressing the F11 key.” translated into the output PT sentence: “Tente carregar na tecla f11.”) 109 After the transfer of the English t-trees into Portuguese t-trees, the synthesis phase constructs a flat surface form of the sentence from the Portuguese t-tree. This is achieved using additional rule-based blocks which take care of word reordering, insertion of negations, prepositions, conjuctions, correct agreement, compound verb forms, etc. The synthesis stage for Portuguese uses the LX-Suite (Branco and Silva, 2006) to perform such tasks. The expected advantage of the TectoMT system over the standard PBSMT system is that the TectoMT translates t-tree nodes (and not the inflected forms) and should thus be able to generalise over the unseen morphological forms. This is particularly important for translation into morphologically rich languages (such as Portuguese) where data sparseness presents a problem for a purely statistically driven MT systems. 3.2.2 PBSMT In all experiments, we use the same PBSMT model (Koehn et al., 2007), GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), a"
W15-5713,C88-1016,0,0.487963,"nslation (MT) systems, such as Systran (Toma, 1977), ETAP-3 (Boguslavsky, 1995), and Lucy (Alonso and Thurmair, 2003), required linguistic expertise to operate and were difficult This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 106 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 106–115, Praha, Czech Republic, 3–4 September 2015. to adapt to different languages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine translation (SMT) systems. Later, the word-based SMT models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to addr"
W15-5713,J90-2002,0,0.847136,"s, such as Systran (Toma, 1977), ETAP-3 (Boguslavsky, 1995), and Lucy (Alonso and Thurmair, 2003), required linguistic expertise to operate and were difficult This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 106 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 106–115, Praha, Czech Republic, 3–4 September 2015. to adapt to different languages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine translation (SMT) systems. Later, the word-based SMT models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues usi"
W15-5713,P03-2041,0,0.0179195,"ion or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues using three different approaches: a tree-to-string translation, where linguistic information is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley et al., 2004), and a tree-to-tree translation, where linguistic information is applied on both source and target side (Eisner, 2003). However, for the majority of language pairs, phrase-based SMT systems still produce better results. The main limitation of SMT systems is that they require large amounts of parallel (or at least comparable) training data, which is hard to obtain for language pairs not covered by the Europarl corpora (Koehn, 2005). Even if Europarl contains data for a particular language pair, another problem arises if the SMT system is needed for a different domain, as the training data may not cover the specific vocabulary or sentence constructions present in the targeted domain. In order to address this pr"
W15-5713,fishel-etal-2012-terra,0,0.0227633,"ages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine translation (SMT) systems. Later, the word-based SMT models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues using three different approaches: a tree-to-string translation, where linguistic information is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley et al., 2004), and a tree-to-tree translation, where linguistic information is applied on both source and target side (Eisner, 2003). However, for the majority of language pairs, phrase-based SMT systems still produce better results. The main limitation of SMT systems is that they r"
W15-5713,W07-0717,0,0.033265,"unts of parallel (or at least comparable) training data, which is hard to obtain for language pairs not covered by the Europarl corpora (Koehn, 2005). Even if Europarl contains data for a particular language pair, another problem arises if the SMT system is needed for a different domain, as the training data may not cover the specific vocabulary or sentence constructions present in the targeted domain. In order to address this problem, many domain-adaptation techniques for SMT have been proposed, ranging from simply adding out-of-domain data to the small amount of in-domain data for training (Foster and Kuhn, 2007) to more sophisticated techniques, such as selecting only particular sentences from the out-of-domain data which are most similar to the in-domain data (Axelrod et al., 2011) or are similar to the sentences with the lowest translation quality (Banerjee et al., 2015). Hybrid MT systems, in turn, aim to exploit the best of both SMT and rule-based approaches, usually either by combining rule-based transfer with statistical language models in the synthesis phase (Habash and Dorr, 2002), or by combining rule-based with statistical approaches at different points of the Vauquois ˇ triangle, as the Te"
W15-5713,habash-dorr-2002-handling,0,0.141261,"Missing"
W15-5713,W06-3601,0,0.0301905,"models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues using three different approaches: a tree-to-string translation, where linguistic information is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley et al., 2004), and a tree-to-tree translation, where linguistic information is applied on both source and target side (Eisner, 2003). However, for the majority of language pairs, phrase-based SMT systems still produce better results. The main limitation of SMT systems is that they require large amounts of parallel (or at least comparable) training data, which is hard to obtain for language pairs not covered by the Europarl corpora (Koehn, 2005). Even if Europarl contains data for a particula"
W15-5713,N03-1017,0,0.00875554,"m over the standard PBSMT system is that the TectoMT translates t-tree nodes (and not the inflected forms) and should thus be able to generalise over the unseen morphological forms. This is particularly important for translation into morphologically rich languages (such as Portuguese) where data sparseness presents a problem for a purely statistically driven MT systems. 3.2.2 PBSMT In all experiments, we use the same PBSMT model (Koehn et al., 2007), GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics as described by Koehn et al. (2003). We tune the systems using MERT (Minimum Error Rate Training (Och, 2003)) and build a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the whole target side (Portuguese) of the English to Portuguese Europarl corpus (Koehn, 2005), which contains 1,960,407 sentences. 3.3 Experiments In all experiments, the PBSMT system uses the in-domain IT1 corpus for tuning, and the language model (LM) is trained on all sentences in the Portuguese side of the Europarl corpus (EP)6 . All experiments (in both TectoMT and PBSMT systems) are evaluated on the same test dataset"
W15-5713,2009.mtsummit-papers.7,0,0.508415,"Missing"
W15-5713,W04-3250,0,0.288249,".99 20.77 21.55 21.89 22.73 20.97 *21.08 21.16 21.66 22.20 22.16 Table 2: Translation experiments setup – type and the size of the corpora used (the number of sentence pairs for the IT1, IT2, and EP corpora, and the number of unigram or multiword expression pairs in the case of the TERM corpus), and the results of the automatic evaluation (the results of the systems which significantly outperformed both baselines are shown in bold; the ‘*’ marks the result which is significantly lower than the result for the BaselineIT; statistical significance is calculated using paired bootstrap resampling (Koehn, 2004)) 4.2 Human Evaluation Results The results of our human evaluation of the fluency and adequacy of the output are presented in Table 3. For each sentence we additionally calculate the Total score (for each annotator separately) as the rounded arithmetic mean of its Fluency and Adequacy scores. The TectoMT system achieved significantly higher adequacy score and total score than the PBSMT system. The mean and median value of the fluency score in the TectoMT system was higher than in the PBSMT system, but the reported difference was not statistically significant (at a 0.05 level of significance us"
W15-5713,2005.mtsummit-papers.11,0,0.402738,"tion is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley et al., 2004), and a tree-to-tree translation, where linguistic information is applied on both source and target side (Eisner, 2003). However, for the majority of language pairs, phrase-based SMT systems still produce better results. The main limitation of SMT systems is that they require large amounts of parallel (or at least comparable) training data, which is hard to obtain for language pairs not covered by the Europarl corpora (Koehn, 2005). Even if Europarl contains data for a particular language pair, another problem arises if the SMT system is needed for a different domain, as the training data may not cover the specific vocabulary or sentence constructions present in the targeted domain. In order to address this problem, many domain-adaptation techniques for SMT have been proposed, ranging from simply adding out-of-domain data to the small amount of in-domain data for training (Foster and Kuhn, 2007) to more sophisticated techniques, such as selecting only particular sentences from the out-of-domain data which are most simil"
W15-5713,W09-0424,0,0.0214156,"umbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 106 Proceedings of the 1st Deep Machine Translation Workshop (DMTW 2015), pages 106–115, Praha, Czech Republic, 3–4 September 2015. to adapt to different languages. The emergence of the word-based IBM models (Brown et al., 1988; Brown et al., 1990; Brown et al., 1993) heralded a new approach to MT – statistical machine translation (SMT) systems. Later, the word-based SMT models were replaced by better-performing phrase-based (Koehn et al., 2007) or hierarchical phrase-based (Li et al., 2009) SMT systems. However, it was noticed that those shallow SMT approaches which do not use any deeper linguistic information or syntax are not able to capture long-distance dependences and may lead to problems with word order and grammatical and semantic cohesion (Fishel et al., 2012). Shallow syntax-based SMT systems tried to address those issues using three different approaches: a tree-to-string translation, where linguistic information is applied only on the source side (Huang et al., 2006); a string-to-tree translation, where linguistic information is applied only on the target side (Galley"
W15-5713,W10-1730,0,0.546275,"Missing"
W15-5713,H05-1066,0,0.22079,"Missing"
W15-5713,J03-1002,0,0.00549994,"anco and Silva, 2006) to perform such tasks. The expected advantage of the TectoMT system over the standard PBSMT system is that the TectoMT translates t-tree nodes (and not the inflected forms) and should thus be able to generalise over the unseen morphological forms. This is particularly important for translation into morphologically rich languages (such as Portuguese) where data sparseness presents a problem for a purely statistically driven MT systems. 3.2.2 PBSMT In all experiments, we use the same PBSMT model (Koehn et al., 2007), GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics as described by Koehn et al. (2003). We tune the systems using MERT (Minimum Error Rate Training (Och, 2003)) and build a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the whole target side (Portuguese) of the English to Portuguese Europarl corpus (Koehn, 2005), which contains 1,960,407 sentences. 3.3 Experiments In all experiments, the PBSMT system uses the in-domain IT1 corpus for tuning, and the language model (LM) is trained on all sentences in the Portuguese side of the Europarl corpus (EP)6 . Al"
W15-5713,P03-1021,0,0.0278888,"not the inflected forms) and should thus be able to generalise over the unseen morphological forms. This is particularly important for translation into morphologically rich languages (such as Portuguese) where data sparseness presents a problem for a purely statistically driven MT systems. 3.2.2 PBSMT In all experiments, we use the same PBSMT model (Koehn et al., 2007), GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics as described by Koehn et al. (2003). We tune the systems using MERT (Minimum Error Rate Training (Och, 2003)) and build a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the whole target side (Portuguese) of the English to Portuguese Europarl corpus (Koehn, 2005), which contains 1,960,407 sentences. 3.3 Experiments In all experiments, the PBSMT system uses the in-domain IT1 corpus for tuning, and the language model (LM) is trained on all sentences in the Portuguese side of the Europarl corpus (EP)6 . All experiments (in both TectoMT and PBSMT systems) are evaluated on the same test dataset (IT2). In order to obtain two baselines for each MT approach (TectoMT and"
W15-5713,P02-1040,0,0.108547,"Missing"
W15-5713,W14-1007,0,0.0475046,"Missing"
W15-5713,W07-1709,0,0.0681898,"Missing"
W15-5713,W08-0325,0,0.269184,"te este per´ıodo de sess˜oes. Table 1: Examples from the corpora 3. IT2 – Another in-domain IT corpus, with 1,000 sentence pairs (answers only) compiled under the QTLeap project, and comparable with the IT1 corpus.3 4. TERM – A parallel corpus of IT terminology (unigrams or multiword expressions), which consists of the Microsoft Terminology Collection4 (13,030 terms) and a small portion of LibreOffice terminology5 (995 terms). Examples from each corpora are presented in Table 1. 3.2 Systems This section describes the two MT systems used for the experiments. 3.2.1 TectoMT ˇ TectoMT (Zabokrtsk´ y et al., 2008) is a structural MT system which uses two layers of structural description, the shallow a-layer and the deep t-layer, performing the transfer on the t-layer (Figure 1). It encompasses three phases along the Vauquois triangle: analysis (which transforms the input sentence into the a-layer and t-layer in a two-step process), transfer (at the t-layer), and synthesis (which converts the translated t-layer representation to the a-layer and then to the output surface string). The analysis and synthesis phases are hybrid, while the transfer phase is mostly statistical, based on the Maximum Entropy co"
W15-5713,P07-2045,0,\N,Missing
W15-5713,D07-1096,0,\N,Missing
W16-2332,N03-1017,0,0.00891535,"provide several translation options for each node along with their estimated probability. The best options are then selected using a Hidden Markov Tree Model (HMTM) ˇ with a target-language tree model (Zabokrtsk´ y and Popel, 2009). For this specific task, where we need to work on a specific domain, an extended version of TectoMT was used allowing interpolation of multiple TMs (Rosa et al., 2015). Moses All the systems submitted that were based on Moses have been trained on a phrase-based model by Giza++ or mGiza with “grow-diag-finaland” symmetrization and “msd-bidirectional-fe” reordering (Koehn et al., 2003). For the language pairs where big quantities of domain-specific monolingual data were available along with the generic domain data, separate language models (domain-specific and generic) were interpolated against our ICT domain-specific development set. For LM training and interpolation, the SRILM toolkit (Stolcke, 2002) was used. The method of truecasing has been adopted for several language pairs where it proved useful. 3 TectoMT The deep translation is based on the TectoMT system, an open-source MT system based on the Treex platform for general natural-language processing. TectoMT uses a c"
W16-2332,C10-3009,0,0.0136209,"obabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms e"
W16-2332,W16-2334,1,0.789389,"Missing"
W16-2332,2005.mtsummit-papers.11,0,0.0123228,"PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first"
W16-2332,P14-5010,0,0.00318097,"ed Moses with the following factors: ENWordForm-BGLemma|Lemma|BGPOStag, where ENWordForm-BGLemma is an English word form when there is no appropriate Bulgarian one, or the Bulgarian lemma; BGPOStag is the appropriate Bulgarian tag representing grammatical features like number, tense, etc. adaptation and MERT training. Batch2 domain corpus was used for testing during development. The Moses system, EU-Moses, uses factored models to allow lemma-based word-alignment. After word alignment, the rest of the training process is based on lowercased word-forms and standard parameters: Stanford CoreNLP (Manning et al., 2014) and Eustagger (Alegria et al., 2002) tools are used for tokenization and lemmatization, MGIZA for word alignment with the ”growdiag-final-and” symmetrization heuristic, a maximum length of 75 tokens per sentence and 5 tokens per phrase, translation probabilities in both directions, lexical weightings in both directions, a phrase length penalty, a ”phrase-mslr-fe” lexicalized reordering model and a target language model. As for the language model, a 5-gram model was trained. The weights for the different components were adjusted to optimize BLEU using MERT tuning over the Batch1 development se"
W16-2332,W15-4101,1,0.800257,"was performed by the Moses tokenizer. No lemmatization or compound splitting was used and the casing was obtained with the Moses truecaser. For the training, a phrase-based model was used with a language model order of 5, with Kneser-Ney smoothing, which was interpolated using the SRILM tool. The word alignment was done with Giza++ on full forms and the final tuning was done using MERT. The Europarl corpus was used for the training data, both as monolingual data for training language models and as parallel data for training the phrase-table. Regarding the English-to-Portuguese TectoMT system (Silva et al., 2015)(Rodrigues et al., 2016a), PT-Treex, in order to get the a-layer the Portuguese system resorted to LX-Suite (Branco and Silva, 2006), a set of pre-existing shallow processing tools for Portuguese that include a sentence segmenter, a tokenizer, a POS tagger, a morphological analyser and a dependency parser, all with state-of-the-art performance. Treex blocks were created to be called and interfaced with these tools. After running the shallow processing tools, the dependency output of the parser is converted into Universal Dependencies (UD) (de Marneffe et al., 2014). These dependencies are then"
W16-2332,W10-1730,1,0.894585,"Missing"
W16-2332,W15-5712,1,0.718195,"ansfer, and synthesis 4 Basque Both English-Basque submissions are trained on the same training corpora. That is, the PaCO2eneu corpus for translation and language modeling, and the in-domain Batch1 corpus for domain 436 tors retrieved from POS tagged, lemmatized parallel corpora; and BG-DeepMoses — a system that also is based on standard factored Moses but the translation is done in two steps: (1) semanticsbased translation of the source language text to a mixed source-target language text which is then (2) translated to the target language via Moses. The latter system builds on Simov et al. (2015). As training data for both systems the following corpora were used: the Setimes parallel corpus, the Europarl parallel corpus and a corpus created on the basis of the documentation of LibreOffice. The corpora are linguistically processed with the IXA2 pipeline for the English part and the BTB pipeline for the Bulgarian. The analyses include POS tagging, lemmatization and WSD, using the UKB system,3 which provides graph-based methods for Word Sense Disambiguation and lexical similarity measurements. For the BG-Moses system, the following factors have been constructed: WordForm|Lemma|POStag. Fo"
W16-2332,H05-1066,0,0.184414,"Missing"
W16-2332,P14-5003,0,0.0466518,"Missing"
W16-2332,2006.jeptalnrecital-invite.2,1,0.754357,"Missing"
W16-2332,tiedemann-2012-parallel,0,0.0377489,"extracted from Wikipedia are used to identify domain terms before syntactic analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used to identify the elements that should be maintained untranslated on the output. Both systems were trained using the same training corpora: the 7th version of the Europarl corpus was used for both translation and language modDutch The Moses system for Dutch was trained on the third version of the Europarl corpus (Koehn, 2005) and the in-domain KDE4 Localization data (Tiedemann, 2012). Words are aligned with GIZA++ and tuning was done with MERT. The applied heuristics for the Dutch baselines were set to “grow-diag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn,"
W16-2332,L16-1094,1,0.833385,"ag-final-and” alignment and “msdbidirectional-fe” reordering. For the creation of the language models, IRSTLM was used to train a 5-gram language model with Kneser-Ney smoothing on the monolingual part of the training corpora. For the TectoMT system, the analysis of Dutch input uses the Alpino system (Noord, 2006), a 438 of tectogrammatical trees. Two separate models were trained and interpolated, the first model with over 1.9 million sentences from Europarl (Koehn, 2005) and the second model composed of the Batch1, the Microsoft Terminology Collection and ˇ the LibreOffice localization data (Stajner et al., 2016). Each pair of parallel sentences, one in English and one in Portuguese, are analyzed by Treex up to the t-layer level, where each pair of trees are fed into the model. The TectoMT synthesis (Rodrigues et al., 2016b) included other two lexical-semanticsrelated modules, the HideIT and gazetteers. The HideIT module handles entities that do not require translation such as URLs and shell commands. The gazetteers are specialized lexicons that handle the translation of named entities from the ITdomain such as menu items and button names. Finally, synset IDs were used as additional contextual feature"
W16-2332,P09-2037,1,0.925288,"uage-specific additions and distinguishes two levels of syntactic description: and Spanish, Charles University in Prague for Czech, by University of Groningen for Dutch, by University of Lisbon for Portuguese and by IICTBAS of the Bulgarian Academy of Sciences for Bulgarian. For each language two different systems were submitted, corresponding to different phases of the project, namely a phrase-based MT system built using Moses (Koehn et al., 2007), and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was imˇ plemented using TectoMT (Zabokrtsk´ y and Popel, 2009). For Bulgarian, its second MT system is not based on TectoMT, but on exploiting deep factors in Moses. All 12 systems are constrained, that is trained only on the data provided by the WMT16 IT-task organizers. We present briefly the Moses common setting and the TectoMT structure and then more detailed information for each language system are provided. In the last Section, results based on BLEU and TrueSkill are given and discussed. 2 • Surface dependency syntax (a-layer) – surface dependency trees containing all the tokens in the sentence. • Deep syntax (t-layer) – dependency trees that conta"
W16-2332,W08-0325,0,0.300026,"Missing"
W16-2332,L16-1438,1,0.826183,"Missing"
W16-2332,zeman-2008-reusable,0,0.0263149,"djusted to optimize BLEU using MERT tuning over the Batch1 development set, with an n-best list of size 100. For the TectoMT system, EU-Treex existing tools were used in order to get the a-layer. Eustagger is a robust and wide coverage morphological analyzer and POS tagger. The dependency parser is based on the MATE-tools (Bjrkelund et al., 2010). Basque models have been trained using the Basque Dependency Treebank (BDT) corpus (Aduriz et al., 2003). Transformation from the a-level analysis into t-level is partially performed with language-independent blocks thanks to the support of Interset (Zeman, 2008). The English-to-Basque TectoMT system uses the PaCo2 and the Batch1 corpora to train two separate translation models, and they are used to create an interpolated list of translation candidates. In addition to that, the terminological equivalences extracted from the localization PO files (VLC, LO and KDE) as well as the domain terms extracted from Wikipedia are used to identify domain terms before syntactical analysis and to ensure domain translation on transfer. Finally, an extra module to treat non linguistic elements (URLs, shell commands, ...) has been used, to identify the elements that s"
W16-2332,W15-5711,1,0.91741,"Missing"
W16-2332,de-marneffe-etal-2014-universal,0,\N,Missing
W16-2332,E06-2024,1,\N,Missing
W16-2332,P07-2045,0,\N,Missing
W16-2332,W13-2208,0,\N,Missing
W16-2332,bojar-etal-2012-joy,1,\N,Missing
W16-2332,L16-1441,1,\N,Missing
W16-6405,W12-3132,0,0.045366,"Missing"
W16-6405,W15-3009,0,0.0248943,"Missing"
W16-6405,W08-0325,0,0.0248875,"sults with simple techniques, e.g. force-translating domain-specific expressions. In such approaches, multiword entries are translated as if they were a single token-with-spaces, failing to represent the internal structure which makes TectoMT a powerful translation engine. In this work we enrich source and target multiword terms with syntactic structure, and seamlessly integrate them in the tree-based transfer phase of TectoMT. Our experiments on the IT domain using the Microsoft terminological resource show improvement in Spanish, Basque and Portuguese. 1 Introduction ˇ ˇ TectoMT (Zabokrtsk´ y et al., 2008; Popel and Zabokrtsk´ y, 2010) has emerged as an architecture to develop deep-transfer systems, where the translation step is done a deep level of analysis, in contrast to methods based on surface sequences of words. TectoMT combines linguistic knowledge and statistical techniques, particularly during transfer, and it aims at transfer on the so-called tectogrammatical layer (Hajiˇcov´a, 2000), a layer of deep syntactic dependency trees. In domain adaptation of machine translation, a typical scenario is as follows: there is an MT system trained on large general-domain data, and there is a bili"
W16-6405,zesch-etal-2008-extracting,0,0.0608747,"Missing"
W18-2801,W10-0603,0,0.0151401,"tic representations of words; and that (ii) lexical semantics can be captured by cooccurrence statistics, the assumption underlying semantics space models of the lexicon. 3 the sake of the comparability of the performance scores obtained. In an initial period, different authors sought to explore the experimental space of the task by focusing on different ways to set up the features. Devereux et al. (2010) find that choosing the set of verbs used for the semantic features under an automatic approach can lead to predictions that are equally good as when using the manually selected set of verbs. Jelodar et al. (2010) use the same set of 25 features to represent a word, but instead of basing the features on co-occurrence counts they resort to relatedness measures based on WordNet. Fernandino et al. (2015) use instead a set of features with 5 sensory-motor experience based attributes (sound, color, visual motion, shape, and manipulation). The relatedness scores between the stimulus word and the attributes are based on human ratings instead of corpus data. Subsequently, as distributional semantics became increasingly popular, authors moved from feature-based representations of the meaning of words to experim"
W18-2801,W18-0107,0,0.645643,"dings. They find the best results with dependency-based embeddings, where words inside the context window are extended with grammatical functions. Binder et al. (2016) use word representations based on 65 experiential attributes with relatedness scores crowdsourced from over 1,700 participants. Xu et al. (2016) present BrainBench, a workbench to test embedding models on both behavioral and brain imaging data sets. Anderson et al. (2017) use a linguistic model based on word2vec embeddings and a visual model built with a deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-based word2vec (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016). These authors found that dependencyRelated work Several authors have addr"
W18-2801,P14-2050,0,0.0395971,"ic model based on word2vec embeddings and a visual model built with a deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-based word2vec (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016). These authors found that dependencyRelated work Several authors have addressed this brain activation prediction task, keeping up with its basic assumptions and resorting to the same data sets for 1 The verbs are: approach, break, clean, drive, eat, enter, fear, fill, hear, lift, listen, manipulate, move, near, open, push, ride, rub, run, say, see, smell, taste, touch, and wear. 2 The 60 pairs are composed of 5 items from each of the 12 concrete semantic categories (animals, body parts, buildings, building parts, clothing, furni"
W18-2801,Q17-1002,0,0.0150808,"meaning of words to experiment with different vector based representation models (aka word embeddings). Murphy et al. (2012) compare different corpusbased models to derive word embeddings. They find the best results with dependency-based embeddings, where words inside the context window are extended with grammatical functions. Binder et al. (2016) use word representations based on 65 experiential attributes with relatedness scores crowdsourced from over 1,700 participants. Xu et al. (2016) present BrainBench, a workbench to test embedding models on both behavioral and brain imaging data sets. Anderson et al. (2017) use a linguistic model based on word2vec embeddings and a visual model built with a deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-b"
W18-2801,S12-1019,0,0.0231684,"he features on co-occurrence counts they resort to relatedness measures based on WordNet. Fernandino et al. (2015) use instead a set of features with 5 sensory-motor experience based attributes (sound, color, visual motion, shape, and manipulation). The relatedness scores between the stimulus word and the attributes are based on human ratings instead of corpus data. Subsequently, as distributional semantics became increasingly popular, authors moved from feature-based representations of the meaning of words to experiment with different vector based representation models (aka word embeddings). Murphy et al. (2012) compare different corpusbased models to derive word embeddings. They find the best results with dependency-based embeddings, where words inside the context window are extended with grammatical functions. Binder et al. (2016) use word representations based on 65 experiential attributes with relatedness scores crowdsourced from over 1,700 participants. Xu et al. (2016) present BrainBench, a workbench to test embedding models on both behavioral and brain imaging data sets. Anderson et al. (2017) use a linguistic model based on word2vec embeddings and a visual model built with a deep convolutiona"
W18-2801,D14-1162,0,0.103859,"ings and a visual model built with a deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-based word2vec (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016). These authors found that dependencyRelated work Several authors have addressed this brain activation prediction task, keeping up with its basic assumptions and resorting to the same data sets for 1 The verbs are: approach, break, clean, drive, eat, enter, fear, fill, hear, lift, listen, manipulate, move, near, open, push, ride, rub, run, say, see, smell, taste, touch, and wear. 2 The 60 pairs are composed of 5 items from each of the 12 concrete semantic categories (animals, body parts, buildings, building parts, clothing, furniture, insects, kitchen items, too"
W18-2801,W18-3016,1,0.632324,"erent sorts. While most approaches to this task have resorted to feature-based models or to semantic spaces (aka word embeddings), here we address the task of prediciting the brain activation triggred by nouns rather by using a semantic network, thus providing further evidence for the cognitive plausibility of this approach to model lexical meaning. In this paper, we report on the competitive results of resolving the brain activation task by taking a mainstream lexical semantics network, WordNet (Fellbaum, 1998), and resorting to intermediate word embeddings obatined with a novel methodology (Saedi et al., 2018) for generating semantic spaces from semantic networks. The task of taking a semantic representation of a noun and predicting the brain activity triggered by it in terms of fMRI spatial patterns was pioneered by Mitchell et al. (2008). That seminal work used word co-occurrence features to represent the meaning of the nouns. Even though the task does not impose any specific type of semantic representation, the vast majority of subsequent approaches resort to featurebased models or to semantic spaces (aka word embeddings). We address this task, with competitive results, by using instead a semant"
W18-2801,W10-0609,0,0.0214274,"ith all individual accuracies significantly above chance. These results support the plausibility of the two key assumptions underlying the task, namely that (i) brain activation patterns can be predicted from semantic representations of words; and that (ii) lexical semantics can be captured by cooccurrence statistics, the assumption underlying semantics space models of the lexicon. 3 the sake of the comparability of the performance scores obtained. In an initial period, different authors sought to explore the experimental space of the task by focusing on different ways to set up the features. Devereux et al. (2010) find that choosing the set of verbs used for the semantic features under an automatic approach can lead to predictions that are equally good as when using the manually selected set of verbs. Jelodar et al. (2010) use the same set of 25 features to represent a word, but instead of basing the features on co-occurrence counts they resort to relatedness measures based on WordNet. Fernandino et al. (2015) use instead a set of features with 5 sensory-motor experience based attributes (sound, color, visual motion, shape, and manipulation). The relatedness scores between the stimulus word and the att"
W18-2801,P16-2068,0,0.0228355,"deep convolutional neural network on the Google Images data set. Recently, Abnar et al. (2018) evaluated 8 different embeddings regarding their usefulness in predicting neural activation patterns: the cooccurrence embeddings of (Mitchell et al., 2008); the experiential embeddings of (Binder et al., 2016); the non-distributional feature-based embeddings of (Faruqui and Dyer, 2015); and 5 different distributional embeddings, namely word2vec (Mikolov et al., 2013), Fasttext (Bojanowski et al., 2016), dependency-based word2vec (Levy and Goldberg, 2014), GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016). These authors found that dependencyRelated work Several authors have addressed this brain activation prediction task, keeping up with its basic assumptions and resorting to the same data sets for 1 The verbs are: approach, break, clean, drive, eat, enter, fear, fill, hear, lift, listen, manipulate, move, near, open, push, ride, rub, run, say, see, smell, taste, touch, and wear. 2 The 60 pairs are composed of 5 items from each of the 12 concrete semantic categories (animals, body parts, buildings, building parts, clothing, furniture, insects, kitchen items, tools, vegetables, vehicles, and ot"
W18-2801,D16-1213,0,0.0363587,"Missing"
W18-3016,E12-1004,0,0.298302,"from laypersons; and word2vec is built on the basis of the co-occurrence frequency of lexical units in a collection of documents. Even when motivated in the first place by psycholinguistic research goals, these repositories of lexical knowledge have been extraordinarily important for language technology. They have been instrumental for major advances in language processing tasks and applications such as word sense disambiguation, part-of-speech tagging, named entity recognition, sentiment analysis (e.g. (Li and Jurafsky, 2015)), parsing (e.g. (Socher et al., 2013)), textual entailment (e.g. (Baroni et al., 2012)), discourse analysis (e.g. (Ji and Eisenstein, 2014)), among many others.1 The proliferation of different types of representation for the same object of research is common in science, and searching for a unified rendering of a given research domain has been a major goal in many disciplines. To a large extent, such search focuses on finding ways of converting from one type of representation into another. Once this is made possible, it brings not only the theoretical satisfaction of getting a better unified insight into the research object, but also important instrumental rewards of reapplying"
W18-3016,C92-2082,0,0.305341,"sed our best settings, with a random 60k subgraph, and our second best settings, with the best model with a specific 13k subgraph, cf. Subsection 4.3. 5 Related work From semantic spaces to semantic networks: There has been a long research tradition on semantic networks enhanced with information extracted from text, including distributional vectors, which in the limit may encompass semantic networks obtained from semantic spaces. As a way of illustration, among many others, this includes the work on semantic relations determined from patterns based on regular expressions, either hand crafted (Hearst, 1992), or learned from corpora (Snow et al., 2005); work on semantic relations predicted by classifiers running over distributional vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014); work on semantic relations obtained with deep learning that integrates distributional information and patterns of grammatical dependency relations (Shwartz et al., 2016), including the hard task of distinguishing synonymy from antonymy (Nguyen et al., 2017); etc. While being highly relevant for a unified account of lexical semantics, this line of research addresses the conversion direction, from se"
W18-3016,P12-1015,0,0.0374131,"rdNet under n POS categories. Word ambiguity rate of the whole WordNet is 1.3. 126 4.4 displayed in Table 4.12 Testing data and metrics To assess the robustness of the results obtained, experiments were undertaken with: (i) yet another evaluation metric, namely Pearson’s correlation coefficient; (ii) further evaluation data sets for semantic similarity, namely RG1965 (Rubenstein and Goodenough, 1965) and Wordsim-353Similarity (Agirre et al., 2009); (iii) and testing over another task, namely semantic relatedness, with the evaluation data sets Wordsim-353Relatedness (Agirre et al., 2009), MEN (Bruni et al., 2012) and MTurk-771 (Halawi et al., 2012). In these experiments we used our best settings, with a random 60k subgraph, and our second best settings, with the best model with a specific 13k subgraph, cf. Subsection 4.3. 5 Related work From semantic spaces to semantic networks: There has been a long research tradition on semantic networks enhanced with information extracted from text, including distributional vectors, which in the limit may encompass semantic networks obtained from semantic spaces. As a way of illustration, among many others, this includes the work on semantic relations determined fr"
W18-3016,J06-1003,0,0.147804,". Distances in a semantic graph: The task of determining the semantic similarity between two words can be performed not only on the basis of the distance of their respective vectors in a semantic space, but also on the basis of the distance of the respective concepts in a lexical semantic network, like WordNet. There has been a long research tradition on this issue whose major proposals include (Jiang and Conrath, 1997), (Lin, 1998), (Leacock and Chodorow, 1998), (Hirst and St-Onge, 1998),(Resnik, 1999), among others, which received nice comparative assessments in (Ferlez and Gams, 2004) and (Budanitsky and Hirst, 2006), including their correlation with human judgments. In this context, it is worth of note the work by (Hughes and Ramage, 2007), which resorts to random graph walks over WordNet edges. Differently from our approach, its goal is to obtain word-specific stationary probability distributions — such that the semantic affinity of two words is based on the similarity of their probability distributions —, rather than to obtain vectorial representations for words in a shared distributional sethe relevant semantic relations; (Nickel and Kiela, 2017) that experimented with computing embeddings not in Eucl"
W18-3016,N15-1059,0,0.0240857,"not the major focus of this paper. From semantic networks to semantic spaces: Work towards the conversion direction that is of interest here is more recent. As a way of illustration, among others, one can mention (Faruqui et al., 2015), which explored retrofitting to refine distributional representations using relational information, and (Yu and Dredze, 2014), which focused also on refining word embeddings with lexical knowledge, but which are not addressing the goal of obtaining semantic spaces solely on the basis of semantic networks as we do here. That is the aim also of recent work like (Camacho-Collados et al., 2015) who improve the embeddings built from data sets made of selected Wikipedia pages by resorting to the local, oneedge relations of each relevant word in the WordNet graph. Further recent works worth mentioning include (Vendrov et al., 2015) that resorted to order embeddings, which however do not preserve distance and/or do not preserve directionality under Additional metric: The evaluation scores obtained over SimLex-999 with the Pearson’s coefficient are basically aligned with the scores already obtained with Spearman’s coefficient, confirming the superiority of the WordNet embeddings. Additio"
W18-3016,D07-1061,0,0.116955,"he basis of the distance of their respective vectors in a semantic space, but also on the basis of the distance of the respective concepts in a lexical semantic network, like WordNet. There has been a long research tradition on this issue whose major proposals include (Jiang and Conrath, 1997), (Lin, 1998), (Leacock and Chodorow, 1998), (Hirst and St-Onge, 1998),(Resnik, 1999), among others, which received nice comparative assessments in (Ferlez and Gams, 2004) and (Budanitsky and Hirst, 2006), including their correlation with human judgments. In this context, it is worth of note the work by (Hughes and Ramage, 2007), which resorts to random graph walks over WordNet edges. Differently from our approach, its goal is to obtain word-specific stationary probability distributions — such that the semantic affinity of two words is based on the similarity of their probability distributions —, rather than to obtain vectorial representations for words in a shared distributional sethe relevant semantic relations; (Nickel and Kiela, 2017) that experimented with computing embeddings not in Euclidean but in hyperbolic space, namely the Poincaré ball model. A shortcoming with these proposals is that their outcome is not"
W18-3016,P14-1002,0,0.103507,"Missing"
W18-3016,C16-1175,0,0.105528,"Missing"
W18-3016,O97-1002,0,0.577729,"dNet; this “artificial text” is a partial and contingent reflection of the semantic network and is used to obtain distributional vectors by resorting to typical word embeddings techniques based on text. Distances in a semantic graph: The task of determining the semantic similarity between two words can be performed not only on the basis of the distance of their respective vectors in a semantic space, but also on the basis of the distance of the respective concepts in a lexical semantic network, like WordNet. There has been a long research tradition on this issue whose major proposals include (Jiang and Conrath, 1997), (Lin, 1998), (Leacock and Chodorow, 1998), (Hirst and St-Onge, 1998),(Resnik, 1999), among others, which received nice comparative assessments in (Ferlez and Gams, 2004) and (Budanitsky and Hirst, 2006), including their correlation with human judgments. In this context, it is worth of note the work by (Hughes and Ramage, 2007), which resorts to random graph walks over WordNet edges. Differently from our approach, its goal is to obtain word-specific stationary probability distributions — such that the semantic affinity of two words is based on the similarity of their probability distributions"
W18-3016,N15-1184,0,0.0646122,"Missing"
W18-3016,D15-1200,0,0.0594503,"Missing"
W18-3016,N15-1165,0,0.155556,"Missing"
W18-3016,W18-2801,1,0.596089,"relations; (Nickel and Kiela, 2017) that experimented with computing embeddings not in Euclidean but in hyperbolic space, namely the Poincaré ball model. A shortcoming with these proposals is that their outcome is not easily plugged into neural models. Also they are not fit to evaluation on external tasks, like the semantic similarity task, with their evaluation being rather based on their ability to complete missing edges from ontological graphs. In contrats, an example of the sutability of wnet2vec to be plugged into neural models and of its application in a downstream task is reported in (Rodrigues et al., 2018), where these embeddings support the predicition of brain activation based on neural networks. There has been also a long tradition of research on learning vector embeddings from multirelational data of which, among many others, one can refer (Bordes et al., 2013), (Lin et al., 2015), and (Nickel et al., 2016). Though to a large extent these are generic approaches for graph to vectors conversion, also here the major focus has been on exploring these models on their ability to complete missing relations in knowledge bases rather than to experiment them on natural language processing and lexical"
W18-3016,C14-1097,0,0.0248139,"Missing"
W18-3016,P16-1226,0,0.0237672,"Missing"
W18-3016,P13-1045,0,0.0575887,"Missing"
W18-3016,C14-1212,0,0.0189738,"emantic networks: There has been a long research tradition on semantic networks enhanced with information extracted from text, including distributional vectors, which in the limit may encompass semantic networks obtained from semantic spaces. As a way of illustration, among many others, this includes the work on semantic relations determined from patterns based on regular expressions, either hand crafted (Hearst, 1992), or learned from corpora (Snow et al., 2005); work on semantic relations predicted by classifiers running over distributional vectors (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014); work on semantic relations obtained with deep learning that integrates distributional information and patterns of grammatical dependency relations (Shwartz et al., 2016), including the hard task of distinguishing synonymy from antonymy (Nguyen et al., 2017); etc. While being highly relevant for a unified account of lexical semantics, this line of research addresses the conversion direction, from semantic spaces to semantic networks, that is not the major focus of this paper. From semantic networks to semantic spaces: Work towards the conversion direction that is of interest here is more rece"
W18-3016,P14-2089,0,0.0322672,"ard task of distinguishing synonymy from antonymy (Nguyen et al., 2017); etc. While being highly relevant for a unified account of lexical semantics, this line of research addresses the conversion direction, from semantic spaces to semantic networks, that is not the major focus of this paper. From semantic networks to semantic spaces: Work towards the conversion direction that is of interest here is more recent. As a way of illustration, among others, one can mention (Faruqui et al., 2015), which explored retrofitting to refine distributional representations using relational information, and (Yu and Dredze, 2014), which focused also on refining word embeddings with lexical knowledge, but which are not addressing the goal of obtaining semantic spaces solely on the basis of semantic networks as we do here. That is the aim also of recent work like (Camacho-Collados et al., 2015) who improve the embeddings built from data sets made of selected Wikipedia pages by resorting to the local, oneedge relations of each relevant word in the WordNet graph. Further recent works worth mentioning include (Vendrov et al., 2015) that resorted to order embeddings, which however do not preserve distance and/or do not pres"
W18-4602,J82-3001,0,0.73967,"sing algorithm, on the one hand, and on the other hand by other factors relevant given the small size of the input problems at stake — namely by the size and shape of the grammar. Accordingly, natural language grammar is very likely to be of a context-sensitive type, with its companion parser of exponential complexity. This position is fully articulated in (Berwick and Weinberg, 1982).14 The LFG15 framework (Kaplan and Bresnan, 1982) is a research program that lends itself to be classified as a grammar framework admitting context-sensitive grammars for natural languages (Bresnan et al., 1982; Berwick, 1982).16 6 Final remarks The programs of research on natural language grammar described above adopt different ways to accommodate results from research on the computational complexity of the recognition problem. Given the Chomsky complexity hierarchy for computable solutions, they fill the whole spectrum of hypothesis ranging from the position that the grammars of natural languages are regular to the positions that they are context-sensitive, also including the claim that they are basically context-free. What these research programs and the argumentation supporting them bring to light is that, impo"
W18-4602,P02-1056,0,0.0594481,"icated in the previous subsection are not necessarily in conflict. The complementarity nature of the two has actually been explored under the rationale that less complex solutions should be used as much as possible until the point where resorting to more complex solutions turns out to be unavoidable with respect to the eventual nature of the sub-problems to be solved. Regular methods have been applied to shallow linguistic processing, whose outcome feeds augmented context-free grammars in charge of deep linguistic processing, responsible for yielding fully-fledged grammatical representations (Crysmann et al., 2002). Nevertheless, when it comes to the accommodation of the results presented in the previous section, the largest divide is perhaps not so much between these two research programs as it is between them and a third, to be presented in the next subsection below. 5.3 The complexity of the recognition problem in a trade off The two approaches described in the two subsections above result from different perspectives on empirical data supporting arguments on the complexity level. A third line of research calls instead for putting into perspective the complexity metric used. In particular, it is noted"
W18-4602,J84-3003,0,0.715078,"atch a pattern xy k z (for a proof, see (Sipser, 2013, p.80)). The intended proof that E’ (and hence E, i.e. the English language) is not regular has its grip in case E’ is considered to be infinite: see Section 5.2 below on the empirical grounds to eventually dispute this. 4 The validity of the argument given the experimentally elicited data obtained to sustain it was strongly challenged however (Liberman, 2004; Coleman et al., 2004; Pinker and Jackendoff, 2005). An overview can be found in (Fitch et al., 2012a). 5 For extended overviews and critical assessment, see (Pullum and Gazdar, 1982; Pullum, 1984; Partee et al., 1993). 13 Jan s¨ait das mer em Hans es huus haend wele h¨alfe aastriiche. Jan said that we the Hans-DAT the house-ACC have wanted help paint Jan said that we have wanted to help Hans paint the house. Jan s¨ait das mer d’chind em Hans es huus haend wele laa h¨alfe aastriiche. Jan said that we the children-ACC the Hans-DAT the house-ACC have wanted let help paint Jan said that we have wanted to let the children help Hans paint the house. ... Based on these examples, and letting A = {d’chind , ...} B = {em Hans , ...} C = {laa , ...} D = {h¨alfe , ...} be finite sets of accusativ"
W18-4602,1985.tmi-1.17,0,0.467793,"ee, resorting to data from English comparatives (Chomsky, 1963), Mohawk noun-stem incorporation (Postal, 1964), ”respectively” constructions (Bar-Hillel and Shamir, 1964; Langendoen, 1977), Dutch embedded verb phrases (Huybregts, 1976; Huybregts, 1984; Bresnan et al., 1982), number Pi (Elster, 1978), English ”such that” clauses (Higginbotham, 1984), or English sluicing clauses (Langendoen and Postal, 1985). Those that were to be eventually retained as the best arguments are based on reduplication in noun formation in Bambara (Culy, 1985), and on Swiss German embedded infinitival verb phrases (Shieber, 1985).5 The argument based on Swiss German data is as follows. Consider the following sequence of example sentences built by successively embedding verb phrases in subordinate clauses (-DAT and -ACC signal dative and accusative case, respectively): 3 The proof that an bn is not regular resorts to the following Pumping Lemma for Regular Languages: Let L be a regular language. Then there exists a constant c (which depends on L) such that for every string w in L of length l ≥ c, we can break w into three subsequences w = xyz, such that y is not an empty string, the length of xy is less than c + 1, and"
Y96-1003,C96-1027,1,0.87585,"ct solution to this problem. (9) does not rule out constructions like (2), where subject-oriented z-pronouns, like ziji, are o-bound by antecedents which are not subjects. We retain, nevertheless, Principle Z as stated in (9), and notice that the ill-formedness of such constructions can be explained on the basis of an independent principled account, without resorting to any specific, provisional or not, stipulation in order to assure subject-orientedness. We adopt the proposal that the obliqueness hierarchy relevant to Binding Theory may have a non-linear ordering, independently motivated in (Branco 1996). This solution builds on (Manning and Sag 1995) proposal for dissociating argument structure (coded in the new ARG-S 25 feature) and grammatical relations (coded in the previous SUBCAT feature), and for checking binding principles in the former and subcategorization principles in the latter. Following (Branco 1996), obliqueness hierarchies may be given a non-linear ordering where subjects are the only o-commanders of any other argument, both in single and multiclausal constructions: (12) • • • argln Accordingly, in languages with subject-orientedness, hence with non-linear obliqueness hierarc"
