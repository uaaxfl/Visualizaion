2016.gwc-1.20,bizzoni-etal-2014-making,0,0.0606451,"Missing"
2020.iwclul-1.7,W17-0606,1,0.637422,"the language about one hundred years ago, which were only lately published (Munkácsi and Kálmán, 1986; Kannisto, 2013). These dictionaries contain words from all the dialects, also from those that are now extinct. There are also some modern dictionaries of the Northern Mansi dialect available (Rombandeeva, 2005; Rombandeeva and Kuzakova, 1982). These dictionaries form the base for the vocabulary used in our application, all of which come from the Northern dialect of Mansi. The vocabulary used in the application is built on an online Mansi dictionary that contains approximately 20,000 entries (Horváth et al., 2017). The Mansi forms were retrieved from the PDF versions of Rombandeeva’s and Kuzakova’s, as well as Rombandeeva’s dictionaries (Rombandeeva, 2005; Rombandeeva and Kuzakova, 1982) by means of optical character recognition, then lexical entries from diﬀerent sources were merged. The Mansi lexemes are supplemented with the Russian translation given by the dictionaries, and Hungarian and English translations were provided by linguists. Thus, the potential learners of Mansi can choose whether they want to learn Mansi with Russian, English or Hungarian as the source language. Table 2 contains the num"
2020.lrec-1.290,P16-1141,0,0.0744999,"Missing"
2020.lrec-1.290,D16-1229,0,0.0444983,"Missing"
2020.lrec-1.290,oravecz-etal-2014-hungarian,0,0.0263381,"Missing"
2020.lrec-1.290,D14-1162,0,0.0833188,"Missing"
2020.lrec-1.290,D17-1317,0,0.484879,"specific concepts in a large corpus may prove to be beneficial as such a study might be able to identify notable changes in the politicalsocial discourse of this era (Xu and Kemp, 2015; Jatowt and Duh, 2014; Kulkarni et al., 2014; Hamilton et al., 2016b; Hamilton et al., 2016a; Garg et al., 2018). As for propaganda texts, despite the fact that the quantitative and qualitative analyses of propaganda discourse are important from an NLP and a political-historical point of view, very few studies seem to provide a systematic analysis of a huge amount of propaganda texts (Propaganda Analysis, 1938; Rashkin et al., 2017; Barr´on-Cede˜no 4 http://clara.nytud.hu/mnsz2-dev/ et al., 2019; Vincze et al., 2019). This is a quite significant research gap concerning the Hungarian language. We are aware of only one paper, focusing specifically on the features of a Hungarian totalitarian discourse by using NLP methods (Vincze et al., 2019). 3. 3.1. Dataset Source of the Data The P´art´elet journal was the official journal of the governing party, the Central Leadership of the Magyar Szocialista Munk´asp´art or MSZMP (in English: Hungarian Socialist Workers’ Party, 1956-1989). It consists of 33 volumes with 12 issues eve"
2020.lrec-1.290,R13-1099,1,0.914482,"Missing"
2020.rdsm-1.6,W12-3809,0,0.0410364,"Missing"
2020.rdsm-1.6,N16-1138,0,0.017718,"38; Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Vincze et al., 2019; Kmetty et al., 2020; Szab´o et al., 2020). Recently, several databases have been compiled for the English language which contain both fake and real news. For instance, the dataset described in Vlachos and Riedel (2014) consists of a total of 221 statements, along with their veracity value. The Liar corpus contains approximately 13,000 short political statements (Wang, 2017). The Emergent Corpus consists of 300 statements and 2500 news related to the semantic content of the statements, along with their veracity value (Ferreira and Vlachos, 2016). Our study is most similar in vein to Rubin et al. (2016), which compares satirical news to real news, collected from different websites. P´erez-Rosas et al. (2018) used crowdsourcing to generate fake news on the basis of real news and then carried out machine learning experiments to separate the two types. There are also a few studies that identify clickbaits (see e.g. Biyani et al. (2016) for English and Karadzhov et al. (2017) for Bulgarian). The above studies focus on the phenomena of deception and bias almost exclusively in sources written in English, so the reported research findings an"
2020.rdsm-1.6,N16-1047,0,0.0483718,"Missing"
2020.rdsm-1.6,N09-1057,0,0.0265672,"pain (Online), December 12, 2020. 2 Related work Most of the authors address the issue of automatic fact extraction and binary classification verification task based on machine learning methods and annotated datasets. There are several studies addressing the phenomena of deception and bias in different types of discourse. For instance, most of the authors analyze the phenomena of deception and bias either in speech text (Fetzer, 2008; Fraser, 2010; Scheithauer, 2007; Simon-Vandenbergen et al., 2007) or address the issue of automatic fact extraction and binary classification verification task (Greene and Resnik, 2009; Rubin et al., 2015; Wang, 2017; Thorne and Vlachos, 2018; Thorne et al., 2018; Graves, 2018). In addition, uncertainty detectors have mostly been developed in the biological and medical domains (Szarvas et al., 2012). Also, a few studies seem to address the issue of the systematic analysis of a huge amount of propaganda texts (Propaganda Analysis, 1938; Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Vincze et al., 2019; Kmetty et al., 2020; Szab´o et al., 2020). Recently, several databases have been compiled for the English language which contain both fake and real news. For instance, t"
2020.rdsm-1.6,karadzhov-etal-2017-built,0,0.0252296,"ts (Wang, 2017). The Emergent Corpus consists of 300 statements and 2500 news related to the semantic content of the statements, along with their veracity value (Ferreira and Vlachos, 2016). Our study is most similar in vein to Rubin et al. (2016), which compares satirical news to real news, collected from different websites. P´erez-Rosas et al. (2018) used crowdsourcing to generate fake news on the basis of real news and then carried out machine learning experiments to separate the two types. There are also a few studies that identify clickbaits (see e.g. Biyani et al. (2016) for English and Karadzhov et al. (2017) for Bulgarian). The above studies focus on the phenomena of deception and bias almost exclusively in sources written in English, so the reported research findings and models are mostly limited to the English language. The same goes for the currently existing datasets. Santos et al. (2020) forms an exception, which, however, aims at the distinction of Brazilian Portuguese fake and real news. To the best of our knowledge, our recent research work is the first attempt at the automatic detection of Hungarian fake news. The main contributions of our paper are the following: • We present a novel da"
2020.rdsm-1.6,2020.lrec-1.290,1,0.828064,"Missing"
2020.rdsm-1.6,P09-2078,0,0.124689,"Missing"
2020.rdsm-1.6,C18-1287,0,0.0365817,"Missing"
2020.rdsm-1.6,D17-1317,0,0.0135134,"f deception and bias either in speech text (Fetzer, 2008; Fraser, 2010; Scheithauer, 2007; Simon-Vandenbergen et al., 2007) or address the issue of automatic fact extraction and binary classification verification task (Greene and Resnik, 2009; Rubin et al., 2015; Wang, 2017; Thorne and Vlachos, 2018; Thorne et al., 2018; Graves, 2018). In addition, uncertainty detectors have mostly been developed in the biological and medical domains (Szarvas et al., 2012). Also, a few studies seem to address the issue of the systematic analysis of a huge amount of propaganda texts (Propaganda Analysis, 1938; Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Vincze et al., 2019; Kmetty et al., 2020; Szab´o et al., 2020). Recently, several databases have been compiled for the English language which contain both fake and real news. For instance, the dataset described in Vlachos and Riedel (2014) consists of a total of 221 statements, along with their veracity value. The Liar corpus contains approximately 13,000 short political statements (Wang, 2017). The Emergent Corpus consists of 300 statements and 2500 news related to the semantic content of the statements, along with their veracity value (Ferreira and Vlachos, 20"
2020.rdsm-1.6,W16-0802,0,0.0384995,"al., 2019; Kmetty et al., 2020; Szab´o et al., 2020). Recently, several databases have been compiled for the English language which contain both fake and real news. For instance, the dataset described in Vlachos and Riedel (2014) consists of a total of 221 statements, along with their veracity value. The Liar corpus contains approximately 13,000 short political statements (Wang, 2017). The Emergent Corpus consists of 300 statements and 2500 news related to the semantic content of the statements, along with their veracity value (Ferreira and Vlachos, 2016). Our study is most similar in vein to Rubin et al. (2016), which compares satirical news to real news, collected from different websites. P´erez-Rosas et al. (2018) used crowdsourcing to generate fake news on the basis of real news and then carried out machine learning experiments to separate the two types. There are also a few studies that identify clickbaits (see e.g. Biyani et al. (2016) for English and Karadzhov et al. (2017) for Bulgarian). The above studies focus on the phenomena of deception and bias almost exclusively in sources written in English, so the reported research findings and models are mostly limited to the English language. The s"
2020.rdsm-1.6,2020.lrec-1.176,0,0.0337582,"collected from different websites. P´erez-Rosas et al. (2018) used crowdsourcing to generate fake news on the basis of real news and then carried out machine learning experiments to separate the two types. There are also a few studies that identify clickbaits (see e.g. Biyani et al. (2016) for English and Karadzhov et al. (2017) for Bulgarian). The above studies focus on the phenomena of deception and bias almost exclusively in sources written in English, so the reported research findings and models are mostly limited to the English language. The same goes for the currently existing datasets. Santos et al. (2020) forms an exception, which, however, aims at the distinction of Brazilian Portuguese fake and real news. To the best of our knowledge, our recent research work is the first attempt at the automatic detection of Hungarian fake news. The main contributions of our paper are the following: • We present a novel dataset for detecting fake news in Hungarian; • We define a rich feature set of linguistic parameters for detecting different characteristics of different types of Hungarian news examined; • We carry out a detailed statistical analysis of linguistic parameters that may distinguish real news,"
2020.rdsm-1.6,C18-1283,0,0.0126122,"the authors address the issue of automatic fact extraction and binary classification verification task based on machine learning methods and annotated datasets. There are several studies addressing the phenomena of deception and bias in different types of discourse. For instance, most of the authors analyze the phenomena of deception and bias either in speech text (Fetzer, 2008; Fraser, 2010; Scheithauer, 2007; Simon-Vandenbergen et al., 2007) or address the issue of automatic fact extraction and binary classification verification task (Greene and Resnik, 2009; Rubin et al., 2015; Wang, 2017; Thorne and Vlachos, 2018; Thorne et al., 2018; Graves, 2018). In addition, uncertainty detectors have mostly been developed in the biological and medical domains (Szarvas et al., 2012). Also, a few studies seem to address the issue of the systematic analysis of a huge amount of propaganda texts (Propaganda Analysis, 1938; Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Vincze et al., 2019; Kmetty et al., 2020; Szab´o et al., 2020). Recently, several databases have been compiled for the English language which contain both fake and real news. For instance, the dataset described in Vlachos and Riedel (2014) consists"
2020.rdsm-1.6,N18-1074,0,0.0114103,"sue of automatic fact extraction and binary classification verification task based on machine learning methods and annotated datasets. There are several studies addressing the phenomena of deception and bias in different types of discourse. For instance, most of the authors analyze the phenomena of deception and bias either in speech text (Fetzer, 2008; Fraser, 2010; Scheithauer, 2007; Simon-Vandenbergen et al., 2007) or address the issue of automatic fact extraction and binary classification verification task (Greene and Resnik, 2009; Rubin et al., 2015; Wang, 2017; Thorne and Vlachos, 2018; Thorne et al., 2018; Graves, 2018). In addition, uncertainty detectors have mostly been developed in the biological and medical domains (Szarvas et al., 2012). Also, a few studies seem to address the issue of the systematic analysis of a huge amount of propaganda texts (Propaganda Analysis, 1938; Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Vincze et al., 2019; Kmetty et al., 2020; Szab´o et al., 2020). Recently, several databases have been compiled for the English language which contain both fake and real news. For instance, the dataset described in Vlachos and Riedel (2014) consists of a total of 221 st"
2020.rdsm-1.6,I13-1044,1,0.800013,"semantic-pragmatic features because at the same time, morphological characteristics of the tokens proved to be also essential. Apart from morphological features, the frequency distribution of certain types of semantic contents also seem to be significantly different in the three subcorpora. For instance, there is a significant difference of the frequency of uncertainty markers such as condition, weasel, peacock and hedge between clickbait news and both of the other corpora. While the first type belongs to semantic uncertainty, the latter three are types of uncertainty at the discourse level (Vincze, 2013). In the case of semantic uncertainty, the lexical content (meaning) of the uncertainty marker (cue) is responsible for uncertainty, e.g. may, possible, believe etc. (Vincze, 2014b). In contrast to semantic uncertainty (Szarvas et al., 2012), in the case of discourse level uncertainty, “the missing or intentionally omitted information is not related to the propositional content of the utterance but to other factors”, e.g. for cues like some, often, much etc. Bias evoked by discourse-level uncertainty might be viewed as a characteristic feature of clickbait news. 5.2 Results of machine learning"
2020.rdsm-1.6,C14-1174,1,0.840188,"an is a pro-drop language, meaning that pronominal subjects and objects might not be overt in the clause; the number and frequency of adverbials; the number and frequency of coordinations and subordinations. • Semantic features: the number and frequency of negation words; the number and frequency of content words and function words; the number of frequency of public verbs, private verbs and suasive verbs based on a Hungarian translation of lists found in Quirk et al. (1985). Uncertainty features: the number and frequency of words belonging to several classes of linguistic uncertainty based on Vincze (2014a). Sentiment features: the number and frequency of positive and negative words based on a list of sentiment phrases. We applied two different Hungarian dictionaries for sentiment analysis: one list was a translation of Liu (2012), while the other one contained Hungarian slang words (Szab´o, 2015), respectively. Emotion features: the number and frequency of words belonging to the emotions described in Szab´o et al. (2016). For list-based semantic features we used a simple dictionary-based method. Thus, if a lemma in our corpora matched with any item in our lists, it was counted as a hit. 2 htt"
2020.rdsm-1.6,W14-2508,0,0.0267603,"ng, 2017; Thorne and Vlachos, 2018; Thorne et al., 2018; Graves, 2018). In addition, uncertainty detectors have mostly been developed in the biological and medical domains (Szarvas et al., 2012). Also, a few studies seem to address the issue of the systematic analysis of a huge amount of propaganda texts (Propaganda Analysis, 1938; Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Vincze et al., 2019; Kmetty et al., 2020; Szab´o et al., 2020). Recently, several databases have been compiled for the English language which contain both fake and real news. For instance, the dataset described in Vlachos and Riedel (2014) consists of a total of 221 statements, along with their veracity value. The Liar corpus contains approximately 13,000 short political statements (Wang, 2017). The Emergent Corpus consists of 300 statements and 2500 news related to the semantic content of the statements, along with their veracity value (Ferreira and Vlachos, 2016). Our study is most similar in vein to Rubin et al. (2016), which compares satirical news to real news, collected from different websites. P´erez-Rosas et al. (2018) used crowdsourcing to generate fake news on the basis of real news and then carried out machine learni"
2020.rdsm-1.6,P17-2067,0,0.0203787,"ork Most of the authors address the issue of automatic fact extraction and binary classification verification task based on machine learning methods and annotated datasets. There are several studies addressing the phenomena of deception and bias in different types of discourse. For instance, most of the authors analyze the phenomena of deception and bias either in speech text (Fetzer, 2008; Fraser, 2010; Scheithauer, 2007; Simon-Vandenbergen et al., 2007) or address the issue of automatic fact extraction and binary classification verification task (Greene and Resnik, 2009; Rubin et al., 2015; Wang, 2017; Thorne and Vlachos, 2018; Thorne et al., 2018; Graves, 2018). In addition, uncertainty detectors have mostly been developed in the biological and medical domains (Szarvas et al., 2012). Also, a few studies seem to address the issue of the systematic analysis of a huge amount of propaganda texts (Propaganda Analysis, 1938; Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Vincze et al., 2019; Kmetty et al., 2020; Szab´o et al., 2020). Recently, several databases have been compiled for the English language which contain both fake and real news. For instance, the dataset described in Vlachos"
2020.rdsm-1.6,R13-1099,1,0.869371,"shed during the time period between 2017 and 2019. For each type of text, we collected 60 documents. The news were randomly selected but we made sure that a given topic should not be included in the corpus more than one time. The basic data of our corpus are presented in Table 1. 4 Experiments In this section, we present our methods for identifying the three types of news with machine learning methods as well as providing statistical significance analysis for the linguistic features defined for the task. 4.1 Feature set As a first step, we automatically preprocessed the texts with magyarlanc (Zsibrita et al., 2013), a toolkit written in JAVA for the linguistic processing of Hungarian texts. With this tool, the text was first split into sentences, then tokenised and lemmatised, and finally morphologically and syntactically analyzed. Based on the output of magyarlanc, we extracted a high number of linguistic features. Our feature set consists of the following features: • Basic statistical features: the number of sentences; the number of words; the number and rate of lemmas; the average sentence length. • Morphological features: Part-of-speech (or POS) features: the number and frequency of nouns, proper no"
C10-1125,W09-2903,0,0.0331449,"onship: in dependency grammars, the new role QUASI ARGUMENT might be proposed for this purpose. 3 Related work Light verb constructions – as a subtype of multiword expressions – have been paid special attention in NLP literature. Sag et al. (2001) classify them as a subtype of lexicalized phrases and flexible expressions. They are usually distinguished from productive or literal verb + noun constructions on the one hand and idiomatic verb + noun expressions on the other hand: e.g. Fazly and Stevenson (2007) use statistical measures in order to classify subtypes of verb + noun combinations and Diab and Bhutada (2009) developed a chunking method for classifying multiword expressions. Identifying multiword expressions in general and light verb constructions in particular is not unequivocal since constructions with similar syntactic structure (e.g. verb + noun combinations) can belong to different subclasses on the productivity scale (i.e. productive combinations, light verb constructions and idioms). That is why welldesigned and tagged corpora of multiword expressions are invaluable resources for training and testing algorithms that are able to identify multiword expressions. For instance, Gr´egoire (2007)"
C10-1125,W07-1103,0,0.0693639,"Missing"
C10-1125,W06-2408,0,0.247395,"nd light verb constructions in particular is not unequivocal since constructions with similar syntactic structure (e.g. verb + noun combinations) can belong to different subclasses on the productivity scale (i.e. productive combinations, light verb constructions and idioms). That is why welldesigned and tagged corpora of multiword expressions are invaluable resources for training and testing algorithms that are able to identify multiword expressions. For instance, Gr´egoire (2007) describes the design and implementation of a lexicon of Dutch multiword expressions. Focusing on multiword verbs, Kaalep and Muischnek (2006; 2008) present an Estonian database and a corpus and Krenn (2008) describes a database of German PP-verb combinations. The Prague Dependency Treebank also contains annotation for light verb constructions (Cinkov´a and Kol´aˇrov´a, 2005) and NomBank (Meyers et al., 2004b) provides the argument structure of common nouns, paying attention to those occurring in support verb constructions as well. On the other hand, Zarrieß and Kuhn (2009) make use of translational correspondences when identifying multiword expressions (among them, light verb constructions). A further example of corpus-based ident"
C10-1125,1995.mtsummit-1.1,0,0.189784,"Missing"
C10-1125,W04-0413,0,0.0894358,"structions has been built for Hungarian, which is presented together with statistical data in section 5. Finally, it is shown how NLP applications in the fields of machine translation and information extraction can profit from the implementation of an algorithm capable of identifying light verb constructions (section 6). 2 Light verb constructions in NLP In natural language processing, one of the most challenging tasks is the proper treatment of col1 There might be slight theoretical differences in the usage of these terms – e.g. semantically empty support verbs are called light verbs in e.g. Meyers et al. (2004a), that is, the term support verb is a hypernym of light verb. However, these differences are not analyzed in detail in this paper. 1110 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1110–1118, Beijing, August 2010 locations, which term comprises light verb constructions as well. Every multiword expression is considered to be a collocation if its members often co-occur and its form is fixed to some extent (Siepmann, 2005; Siepmann, 2006; Sag et al., 2001; Oravecz et al., 2004; V´aradi, 2006). Collocations are frequent in language use and th"
C10-1125,W04-2705,0,0.21991,"Missing"
C10-1125,W04-0401,0,0.0712132,"ion to those occurring in support verb constructions as well. On the other hand, Zarrieß and Kuhn (2009) make use of translational correspondences when identifying multiword expressions (among them, light verb constructions). A further example of corpus-based identification of light verb constructions in English is described in Tan et al. (2006). Light verb constructions are considered to be semi-productive, that is, certain verbs tend to cooccur with nouns belonging to a given semantic class. A statistical method is applied to measure the acceptability of possible light verb constructions in Stevenson et al. (2004), which correlates reasonably well with human judgments. 4 Identifying light verb constructions A database of light verb constructions and an annotated corpus might be of great help in the automatic recognition of light verb constructions. They can serve as a training database when implementing an algorithm for identifying those constructions. The recognition of light verb constructions cannot be solely based on syntactic patterns for other (productive or idiomatic) combinations may exhibit the same verb + noun scheme (see section 1111 2). However, in agglutinative languages such as Hungarian,"
C10-1125,W06-2407,0,0.172513,") describes a database of German PP-verb combinations. The Prague Dependency Treebank also contains annotation for light verb constructions (Cinkov´a and Kol´aˇrov´a, 2005) and NomBank (Meyers et al., 2004b) provides the argument structure of common nouns, paying attention to those occurring in support verb constructions as well. On the other hand, Zarrieß and Kuhn (2009) make use of translational correspondences when identifying multiword expressions (among them, light verb constructions). A further example of corpus-based identification of light verb constructions in English is described in Tan et al. (2006). Light verb constructions are considered to be semi-productive, that is, certain verbs tend to cooccur with nouns belonging to a given semantic class. A statistical method is applied to measure the acceptability of possible light verb constructions in Stevenson et al. (2004), which correlates reasonably well with human judgments. 4 Identifying light verb constructions A database of light verb constructions and an annotated corpus might be of great help in the automatic recognition of light verb constructions. They can serve as a training database when implementing an algorithm for identifying"
C10-1125,W05-1106,0,0.0357234,"Missing"
C10-1125,W06-2410,0,0.0394635,"Missing"
C10-1125,W09-2904,0,0.0771688,"expressions. For instance, Gr´egoire (2007) describes the design and implementation of a lexicon of Dutch multiword expressions. Focusing on multiword verbs, Kaalep and Muischnek (2006; 2008) present an Estonian database and a corpus and Krenn (2008) describes a database of German PP-verb combinations. The Prague Dependency Treebank also contains annotation for light verb constructions (Cinkov´a and Kol´aˇrov´a, 2005) and NomBank (Meyers et al., 2004b) provides the argument structure of common nouns, paying attention to those occurring in support verb constructions as well. On the other hand, Zarrieß and Kuhn (2009) make use of translational correspondences when identifying multiword expressions (among them, light verb constructions). A further example of corpus-based identification of light verb constructions in English is described in Tan et al. (2006). Light verb constructions are considered to be semi-productive, that is, certain verbs tend to cooccur with nouns belonging to a given semantic class. A statistical method is applied to measure the acceptability of possible light verb constructions in Stevenson et al. (2004), which correlates reasonably well with human judgments. 4 Identifying light verb"
C10-1125,W07-1102,0,\N,Missing
C10-1125,vincze-etal-2010-hungarian,1,\N,Missing
C14-1132,C10-1011,0,0.0650955,"Missing"
C14-1132,C12-1052,1,0.930312,"r on the gold standard constituency data and converting the output into dependency format. convDep: training the Bohnet parser without dependency labels on the silver standard data. conjunctions or adverbs – that require manual checking even if automatic conversion from constituency to dependency is applied. 5 Pre- or Post Conversion? It is well known that for English, converting a constituency parser’s output to dependency format (post conversion) can achieve competitive ULA scores to a dependency parser’s output trained on automatically converted trees (pre conversion) (Petrov et al., 2010; Farkas and Bohnet, 2012). One of the possible reasons for this may be that English is a configurational language, hence constituency parsers are expected to perform better here. In this paper, we investigate whether this is true for Hungarian, which is the prototype of morphologically rich languages with free word order. We employed the product-of-grammars procedure (Petrov, 2010) of the Berkeleyparser (Petrov et al., 2006), where grammars are trained on the same dataset but with different initialization setups, which leads to different grammars. We trained 8 grammars and used tree-level inference. The output of the"
C14-1132,E12-1007,1,0.924075,"n. The differences between them originate from the characteristics of constituent and dependency syntax. 3 Converting Constituency Trees to Dependency Trees In this section, we present our methods to convert constituency trees to dependency trees and we also discuss the most typical sources of errors during conversion. 3.1 Conversion rules In order to convert constituency trees to dependency trees, we used a rule based system. Sentences with virtual dependency nodes were omitted, as they are not annotated in the constituent treebank and their treatment in dependency trees is also problematic (Farkas et al., 2012; Seeker et al., 2012). As a result, we worked with 7,372 sentences and 162,960 tokens. First, we determined the head of each clause (CP) and the relations between CPs in complex sentences. In most cases the head of the CP is a finite verb, if the CP contains no finite verb, the head is the either an infinitive verb or a participle, if none of these are present in the CP, the head can be a nominal expression. The relations between the CP heads make up the base of the dependency structure using ROOT relation for the sentence’s main verb, COORD for coordination and ATT for subordination, as well"
C14-1132,P06-1055,0,0.0599368,"output to dependency format (post conversion) can achieve competitive ULA scores to a dependency parser’s output trained on automatically converted trees (pre conversion) (Petrov et al., 2010; Farkas and Bohnet, 2012). One of the possible reasons for this may be that English is a configurational language, hence constituency parsers are expected to perform better here. In this paper, we investigate whether this is true for Hungarian, which is the prototype of morphologically rich languages with free word order. We employed the product-of-grammars procedure (Petrov, 2010) of the Berkeleyparser (Petrov et al., 2006), where grammars are trained on the same dataset but with different initialization setups, which leads to different grammars. We trained 8 grammars and used tree-level inference. The output of the parser was then automatically converted to dependency format, based on the rules described in Section 3 (BerkeleyConv). Second, we used the silver standard dependency treebank for training the Bohnet parser (convDep). Since our constituency parser did not produce grammatical functions for the nodes, we trained the Bohnet parser on unlabeled dependency trees in order to ensure a fair comparison here ("
C14-1132,D10-1069,0,0.154876,"the quality of a rule-based automatic conversion from constituency to dependency trees, to compare the two sets of manual annotations and also the output of constituency and dependency parsers trained on converted and gold standard dependency trees. We investigate the effect of automatic conversions related to the two parsing paradigms as well. It is well known that for English, the automatic conversion of a constituency parser’s output to dependency format can achieve competitive unlabeled attachment scores (ULA) to a dependency parser’s output trained on automatically converted trees1 (cf. Petrov et al. (2010)). One of the possible explanations for this is that English is a configurational language, hence constituency parsers have advantages over dependency parsers here. We check whether this hypothesis holds for Hungarian too, which is the prototype of free word order languages. In this paper, we compare three pairs of dependency analyses in order to evaluate the usefulness of converted trees. First, we examine the errors of the conversion itself by comparing the converted dependency trees with the manually annotated gold standard ones. Second, we argue for the importance of training parsers on go"
C14-1132,C12-2105,1,0.858462,"tween them originate from the characteristics of constituent and dependency syntax. 3 Converting Constituency Trees to Dependency Trees In this section, we present our methods to convert constituency trees to dependency trees and we also discuss the most typical sources of errors during conversion. 3.1 Conversion rules In order to convert constituency trees to dependency trees, we used a rule based system. Sentences with virtual dependency nodes were omitted, as they are not annotated in the constituent treebank and their treatment in dependency trees is also problematic (Farkas et al., 2012; Seeker et al., 2012). As a result, we worked with 7,372 sentences and 162,960 tokens. First, we determined the head of each clause (CP) and the relations between CPs in complex sentences. In most cases the head of the CP is a finite verb, if the CP contains no finite verb, the head is the either an infinitive verb or a participle, if none of these are present in the CP, the head can be a nominal expression. The relations between the CP heads make up the base of the dependency structure using ROOT relation for the sentence’s main verb, COORD for coordination and ATT for subordination, as well as CONJ in the case o"
C14-1132,N10-1003,0,\N,Missing
C14-1132,W13-4917,1,\N,Missing
C14-1132,vincze-etal-2010-hungarian,1,\N,Missing
C14-1174,W07-1011,0,0.0866667,"anguage texts (Morante and Sporleder, 2012). As indicated above, most earlier research on uncertainty detection focused on the English language. As for the domains of the texts, newspapers (Saur´ı and Pustejovsky, 2009), biological or medical texts (Szarvas et al., 2012; Morante et al., 2009; Farkas et al., 2010; Kim et al., 2008), Wikipedia articles (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012) and most recently social media texts (Wei et al., 2013) have been selected for the experiments. Systems for uncertainty detection were originally rule-based (Light et al., 2004; Chapman et al., 2007) but recently, they exploit machine learning methods, usually applying a supervised approach (see ¨ ur and Radev (2009), Szarvas et al. (2012) e.g. Medlock and Briscoe (2007), Morante et al. (2009), Ozg¨ and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)). In harmony with the latest tendencies, our system here is also based on supervised machine learning techniques, which employs a rich feature set of lexical, morphological, syntactic and semantic features and also exploits contextual features. Supervised machine learning methods require annotated corpora. There have been seve"
C14-1174,R13-2007,0,0.332843,"Missing"
C14-1174,W10-3001,1,0.929877,"Missing"
C14-1174,P09-2044,0,0.410845,"cues in biological papers and Wikipedia articles written in English (Farkas et al., 2010). Moreover, a special issue of the journal Computational Linguistics (Vol. 38, No. 2) was recently dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). As indicated above, most earlier research on uncertainty detection focused on the English language. As for the domains of the texts, newspapers (Saur´ı and Pustejovsky, 2009), biological or medical texts (Szarvas et al., 2012; Morante et al., 2009; Farkas et al., 2010; Kim et al., 2008), Wikipedia articles (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012) and most recently social media texts (Wei et al., 2013) have been selected for the experiments. Systems for uncertainty detection were originally rule-based (Light et al., 2004; Chapman et al., 2007) but recently, they exploit machine learning methods, usually applying a supervised approach (see ¨ ur and Radev (2009), Szarvas et al. (2012) e.g. Medlock and Briscoe (2007), Morante et al. (2009), Ozg¨ and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)). In harmony with the latest tendencies, our system here is also based on supervised"
C14-1174,konstantinova-etal-2012-review,0,0.151885,"Missing"
C14-1174,W04-3103,0,0.178933,"egation in natural language texts (Morante and Sporleder, 2012). As indicated above, most earlier research on uncertainty detection focused on the English language. As for the domains of the texts, newspapers (Saur´ı and Pustejovsky, 2009), biological or medical texts (Szarvas et al., 2012; Morante et al., 2009; Farkas et al., 2010; Kim et al., 2008), Wikipedia articles (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012) and most recently social media texts (Wei et al., 2013) have been selected for the experiments. Systems for uncertainty detection were originally rule-based (Light et al., 2004; Chapman et al., 2007) but recently, they exploit machine learning methods, usually applying a supervised approach (see ¨ ur and Radev (2009), Szarvas et al. (2012) e.g. Medlock and Briscoe (2007), Morante et al. (2009), Ozg¨ and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)). In harmony with the latest tendencies, our system here is also based on supervised machine learning techniques, which employs a rich feature set of lexical, morphological, syntactic and semantic features and also exploits contextual features. Supervised machine learning methods require annotated corpor"
C14-1174,P07-1125,0,0.225238,"e texts, newspapers (Saur´ı and Pustejovsky, 2009), biological or medical texts (Szarvas et al., 2012; Morante et al., 2009; Farkas et al., 2010; Kim et al., 2008), Wikipedia articles (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012) and most recently social media texts (Wei et al., 2013) have been selected for the experiments. Systems for uncertainty detection were originally rule-based (Light et al., 2004; Chapman et al., 2007) but recently, they exploit machine learning methods, usually applying a supervised approach (see ¨ ur and Radev (2009), Szarvas et al. (2012) e.g. Medlock and Briscoe (2007), Morante et al. (2009), Ozg¨ and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)). In harmony with the latest tendencies, our system here is also based on supervised machine learning techniques, which employs a rich feature set of lexical, morphological, syntactic and semantic features and also exploits contextual features. Supervised machine learning methods require annotated corpora. There have been several corpora annotated for uncertainty in different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et"
C14-1174,J12-2001,0,0.437175,"edia. The system is based on sequence labeling and makes use of a rich feature set including orthographic, lexical, morphological, syntactic and semantic features as well. Having access to annotated data from two domains, we also focus on the domain specificities of uncertainty detection by comparing results obtained in indomain and cross-domain settings. Our results show that the domain of the text has significant influence on uncertainty detection. 1 Introduction Uncertainty detection has become one of the most intensively studied problems of natural language processing (NLP) in these days (Morante and Sporleder, 2012). For several NLP applications, it is essential to distinguish between factual and nonfactual, i.e. negated or uncertain information: for instance, in medical information retrieval, it must be known whether the patient definitely suffers, probably suffers or does not suffer from an illness. This type of information can only be revealed from the texts of the documents if reliable uncertainty detectors are available, which are able to identify linguistic markers of uncertainty, i.e. cues within the text. To the best of our knowledge, uncertainty detectors have been mostly developed for the Engli"
C14-1174,W09-1203,0,0.0349289,"Missing"
C14-1174,W10-3112,0,0.403561,"al. (2009), Ozg¨ and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)). In harmony with the latest tendencies, our system here is also based on supervised machine learning techniques, which employs a rich feature set of lexical, morphological, syntactic and semantic features and also exploits contextual features. Supervised machine learning methods require annotated corpora. There have been several corpora annotated for uncertainty in different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuner et al., 2009), news media (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia (Farkas et al., 2010), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and social media (Wei et al., 2013). For our experiments, however, we make use of the first Hungarian uncertainty corpus created for the purpose of this study. 3 Experiments In this section, we present our methodology to detect uncertainty cues in Hungarian. We first describe the uncertainty categories applied and report some statistics on the corpus. Then we describe our machine le"
C14-1174,D09-1145,0,0.0514818,"n the English language. As for the domains of the texts, newspapers (Saur´ı and Pustejovsky, 2009), biological or medical texts (Szarvas et al., 2012; Morante et al., 2009; Farkas et al., 2010; Kim et al., 2008), Wikipedia articles (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012) and most recently social media texts (Wei et al., 2013) have been selected for the experiments. Systems for uncertainty detection were originally rule-based (Light et al., 2004; Chapman et al., 2007) but recently, they exploit machine learning methods, usually applying a supervised approach (see ¨ ur and Radev (2009), Szarvas et al. (2012) e.g. Medlock and Briscoe (2007), Morante et al. (2009), Ozg¨ and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)). In harmony with the latest tendencies, our system here is also based on supervised machine learning techniques, which employs a rich feature set of lexical, morphological, syntactic and semantic features and also exploits contextual features. Supervised machine learning methods require annotated corpora. There have been several corpora annotated for uncertainty in different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008"
C14-1174,J12-2004,1,0.923543,"Missing"
C14-1174,W08-0606,1,0.940658,"Missing"
C14-1174,I13-1044,1,0.80615,"hUnCertainty Corpus For the purpose of this study, we manually annotated texts from two domains. First, we randomly selected 1,081 paragraphs from the Hungarian Wikipedia dump. This selection contains 9,722 sentences and 180,000 tokens. Second, we downloaded 300 pieces of criminal news from a Hungarian news portal (http://www.hvg.hu), which altogether consist of 5,481 sentences and 94,000 tokens. In total, the hUnCertainty corpus consists of 15,203 sentences and 274,000 tokens. During annotation, we followed the categorization of uncertainty phenomena as described in Szarvas et al. (2012) and Vincze (2013) with some slight modifications, due to the morphologically rich nature of Hungarian (for instance, modal auxiliaries like may correspond to a derivational suffix in Hungarian, which required that in the case of j¨ohet “may come” the whole word was annotated as uncertain, not just the suffix -het). Here we just briefly summarize uncertainty categories that were annotated – for a detailed discussion, please refer to Szarvas et al. (2012) and Vincze (2013). Linguistic uncertainty is traditionally connected to modality and the semantics of the sentence. For instance, the sentence It may be rainin"
C14-1174,P13-2011,0,0.361457,"ecial issue of the journal Computational Linguistics (Vol. 38, No. 2) was recently dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). As indicated above, most earlier research on uncertainty detection focused on the English language. As for the domains of the texts, newspapers (Saur´ı and Pustejovsky, 2009), biological or medical texts (Szarvas et al., 2012; Morante et al., 2009; Farkas et al., 2010; Kim et al., 2008), Wikipedia articles (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012) and most recently social media texts (Wei et al., 2013) have been selected for the experiments. Systems for uncertainty detection were originally rule-based (Light et al., 2004; Chapman et al., 2007) but recently, they exploit machine learning methods, usually applying a supervised approach (see ¨ ur and Radev (2009), Szarvas et al. (2012) e.g. Medlock and Briscoe (2007), Morante et al. (2009), Ozg¨ and the systems of the CoNLL-2010 Shared Task (Farkas et al., 2010)). In harmony with the latest tendencies, our system here is also based on supervised machine learning techniques, which employs a rich feature set of lexical, morphological, syntactic"
C14-1174,R13-1099,1,0.784568,"Missing"
E12-1007,E03-1012,0,0.43377,"Missing"
E12-1007,C10-1011,0,0.333815,"ide the issue of the internal structure of NPs, most sentence-level syntactic information in Hungarian is conveyed ´ Kiss, by morphology, not by configuration (E. 2002). A large part of the methodology for syntactic parsing has been developed for English. However, parsing non-configurational and less configurational languages requires different techniques. In this study, we present results on Hungarian dependency parsing and we investigate this general issue in the case of English and Hungarian. We employed three state-of-the-art data-driven parsers (Nivre et al., 2004; McDonald et al., 2005; Bohnet, 2010), which achieved (un)labeled attachment scores on Hungarian not so different from the corresponding English scores (and even higher on certain domains/subcorpora). Our investigations show that the feature representation used by the data-driven parsers is so rich that they can – without any modification – effectively learn a reasonable model for non-configurational languages as well. We also conducted a systematic and comparative error analysis of the system’s outputs for Hungarian and English. This analysis highlights the challenges of parsing Hungarian and suggests that the further improvemen"
E12-1007,W06-2920,0,0.200297,"Missing"
E12-1007,D07-1101,0,0.0214338,"on-based system, which uses an arc-eager system along with support vector machines to learn the scoring function for transitions and which uses greedy, deterministic onebest search at parsing time. As one of the graphbased parsers, we employed the MST parser (McDonald et al., 2005) with a second-order feature decoder. It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mate parser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). Its decoder works on labeled edges, i.e. it uses a single-step approach for obtaining labeled dependency trees. Mate uses a rich and 4 The JAVA implementation of the morphological analyser and the slightly modified POS tagger along with trained models are available at http://www.inf.u-szeged. hu/rgai/magyarlanc. 58 corpus newspaper short business fiction law computer composition #sent. 9189 8616 9279 8347 8653 22248 length 21.6 23.6 12.6 27.3 21.9 13.7 CPOS 97.2 98.0 96.9 98.3 96.4 96.7 DPOS 96.5 97.7 95.8 98.1 95.8 95.6 ULA 88.0 (90.0) 93.8 (94.8) 87.7 (89.4) 90.6 (90.7) 91.3 (92.8) 92.7 (9"
E12-1007,C96-1058,0,0.0840122,"ds inside of constituents to be consecutive and it naturally represent discontinuous constructions, which are frequent in languages where grammatical relations are often signaled by morphology instead of word order (McDonald and Nivre, 2011). The two main efficient approaches for dependency parsing are the graph-based and the transition-based parsers. The graph-based models look for the highest scoring directed spanning tree in the complete graph whose nodes are the words of the sentence in question. They solve the machine learning problem of finding the optimal scoring function of subgraphs (Eisner, 1996; McDonald et al., 2005). The transition-based approaches parse a sentence in a single left-to-right pass over the words. The next transition in these systems is predicted by a classifier that is based on history-related features (Kudo and Matsumoto, 2002; Nivre et al., 2004). Although the available treebanks for Hungarian are relatively big (82K sentences) and fully manually annotated, the studies on parsing Hungarian are rather limited. The Szeged (Constituency) Treebank (Csendes et al., 2005) con56 sists of six domains – namely, short business news, newspaper, law, literature, compositions"
E12-1007,D07-1097,0,0.0321724,"are efficiently identified by our morphological analyser. The most frequent morphological features which cannot be disambiguated at the word level are related to suffixes with multiple functions or the word itself cannot be unambiguously segmented into morphemes. Although the number of such ambiguous cases is low, they form important features for the parser, thus we will focus on the more accurate handling of these cases in future work. Comparison to CoNLL-2007 results: The best performing participant of the CoNLL-2007 Shared Task (Nivre et al., 2007) achieved an ULA of 83.6 and LAS of 80.3 (Hall et al., 2007) on the Hungarian corpus. The difference between the top performing English and Hungarian systems were 8.14 ULA and 9.3 LAS. The results reported in 2007 were significantly lower and the gap between English and Hungarian is higher than our current values. To locate the sources of difference we carried out other experiments with Mate on the CoNLL-2007 dataset using the gold-standard POS tags (the shared task used gold-standard POS tags for evaluation). First we trained and evaluated Mate on the original CoNLL-2007 datasets, where it achieved ULA 84.3 and LAS 80.0. Then we used the sentences of"
E12-1007,W02-2016,0,0.0111191,"The two main efficient approaches for dependency parsing are the graph-based and the transition-based parsers. The graph-based models look for the highest scoring directed spanning tree in the complete graph whose nodes are the words of the sentence in question. They solve the machine learning problem of finding the optimal scoring function of subgraphs (Eisner, 1996; McDonald et al., 2005). The transition-based approaches parse a sentence in a single left-to-right pass over the words. The next transition in these systems is predicted by a classifier that is based on history-related features (Kudo and Matsumoto, 2002; Nivre et al., 2004). Although the available treebanks for Hungarian are relatively big (82K sentences) and fully manually annotated, the studies on parsing Hungarian are rather limited. The Szeged (Constituency) Treebank (Csendes et al., 2005) con56 sists of six domains – namely, short business news, newspaper, law, literature, compositions and informatics – and it is manually annotated for the possible alternatives of words’ morphological analyses, the disambiguated analysis and constituency trees. We are aware of only two articles on phrase-structure parsers which were trained and evaluate"
E12-1007,J11-1007,0,0.0114111,"). This train of thought indicates that the cross-lingual comparison of final parser scores should be conducted very carefully. 3 Related work We decided to focus on dependency parsing in this study as it is a superior framework for nonconfigurational languages. It has gained interest in natural language processing recently because the representation itself does not require the words inside of constituents to be consecutive and it naturally represent discontinuous constructions, which are frequent in languages where grammatical relations are often signaled by morphology instead of word order (McDonald and Nivre, 2011). The two main efficient approaches for dependency parsing are the graph-based and the transition-based parsers. The graph-based models look for the highest scoring directed spanning tree in the complete graph whose nodes are the words of the sentence in question. They solve the machine learning problem of finding the optimal scoring function of subgraphs (Eisner, 1996; McDonald et al., 2005). The transition-based approaches parse a sentence in a single left-to-right pass over the words. The next transition in these systems is predicted by a classifier that is based on history-related features"
E12-1007,H05-1066,0,0.74539,"tence level. Leaving aside the issue of the internal structure of NPs, most sentence-level syntactic information in Hungarian is conveyed ´ Kiss, by morphology, not by configuration (E. 2002). A large part of the methodology for syntactic parsing has been developed for English. However, parsing non-configurational and less configurational languages requires different techniques. In this study, we present results on Hungarian dependency parsing and we investigate this general issue in the case of English and Hungarian. We employed three state-of-the-art data-driven parsers (Nivre et al., 2004; McDonald et al., 2005; Bohnet, 2010), which achieved (un)labeled attachment scores on Hungarian not so different from the corresponding English scores (and even higher on certain domains/subcorpora). Our investigations show that the feature representation used by the data-driven parsers is so rich that they can – without any modification – effectively learn a reasonable model for non-configurational languages as well. We also conducted a systematic and comparative error analysis of the system’s outputs for Hungarian and English. This analysis highlights the challenges of parsing Hungarian and suggests that the fur"
E12-1007,P05-1013,0,0.021246,"he internal structures of Named Entities and we imagine a pipeline where a Named Entity Recogniser precedes the parsing step. Empty copula: In the verbless clauses (predicative nouns or adjectives) the Szeged Dependency Treebank introduces virtual nodes (16,000 items in the corpus). This solution means that a similar tree structure is ascribed to the same sentence in the present third person singular and all the other tenses / persons. A further argument for the use of a virtual node is that the virtual node is always present at the syntactic level 2 Using the transitive closure definition of Nivre and Nilsson (2005). 57 corpus Hungarian English dev test dev test Malt ULA LAS 88.3 (89.9) 85.7 (87.9) 88.7 (90.2) 86.1 (88.2) 87.8 (89.1) 84.5 (86.1) 88.8 (89.9) 86.2 (87.6) MST ULA LAS 86.9 (88.5) 80.9 (82.9) 87.5 (89.0) 81.6 (83.5) 89.4 (91.2) 86.1 (87.7) 90.7 (91.8) 87.7 (89.2) Mate ULA LAS 89.7 (91.1) 86.8 (89.0) 90.1 (91.5) 87.2 (89.4) 91.6 (92.7) 88.5 (90.0) 92.6 (93.4) 90.3 (91.5) Table 1: Results achieved by the three parsers on the (full) Hungarian (Szeged Dependency Treebank) and English (CoNLL-2009) datasets. The scores in brackets are achieved with gold-standard POS tagging. since it is overt in al"
E12-1007,W04-2407,0,0.0175033,"Missing"
E12-1007,2004.eamt-1.17,0,0.0785525,"Missing"
E12-1007,N03-1033,0,0.00635951,"egedTreebank. Tools: We employed a finite state automatabased morphological analyser constructed from the morphdb.hu lexical resource (Tr´on et al., 2006) and we used the MSD-style morphological code system of the Szeged TreeBank (Alexin et al., 2003). The output of the morphological analyser is a set of possible lemma–morphological analysis pairs. This set of possible morphological analyses for a word form is then used as possible alternatives – instead of open and closed tag sets – in a standard sequential POS tagger. Here, we applied the Conditional Random Fields-based Stanford POS tagger (Toutanova et al., 2003) and carried out 5-fold-cross POS training/tagging inside the subcorpora.4 For the English experiments we used the predicted POS tags provided for the CoNLL-2009 shared task (Hajiˇc et al., 2009). As the dependency parser we employed three state-of-the-art data-driven parsers, a transitionbased parser (Malt) and two graph-based parsers (MST and Mate parsers). The Malt parser (Nivre et al., 2004) is a transition-based system, which uses an arc-eager system along with support vector machines to learn the scoring function for transitions and which uses greedy, deterministic onebest search at pars"
E12-1007,tron-etal-2006-morphdb,0,0.468383,"Missing"
E12-1007,vincze-etal-2010-hungarian,1,0.867873,"Missing"
E12-1007,W09-1201,0,\N,Missing
E12-1007,D07-1096,0,\N,Missing
E17-1034,C10-1011,0,0.0394206,"tions. We empirical evaluations which we introduce in this were particularly interested in the utility of the section. two representations for dependency parsing. We trained two models of the MarMot morphological 5.1 On the Accuracy of Automatic tagger (Mueller et al., 2013) using the two morConverters phological representation in 10-fold cross-tagging Most of the UD treebanks are the result of autoon our manually corrected 1800 sentences. Then matic conversion from a dependency treebank of we trained and evaluated the Bohnet dependency originally different principles. The accuracy of parser (Bohnet, 2010) on the train/test split of the these automatic converters is unknown, i.e. we do UD repository v3.0 utilizing the two different prenot know how much information was lost or how dicted morphological descriptions. We used the much noise was introduced by the converters. To default parameters for both the MarMot and the empirically investigate this in the case of HungarBohnet parser. ian UD, we compared the converted and the manuTable 1 presents unlabeled (UAS) and labeled ally corrected, i.e. gold standard, trees of the 1800 (LAS) attachment scores achieved by the parser sentences. on the test"
E17-1034,W04-1903,0,0.0503916,". Recently, a further extension on the UD relations has been proposed: enhanced English dependencies are described in Schuster and Manning (2016). Our UD principles introduced in this paper follow the central UD guidelines (Nivre, 2015) and we did our best to align with the existing guidelines for other morphologically rich languages as well. On the other hand, there are several Hungarian-specific phenomena that required changes and extensions of the original UD principles. The only available manually annotated treehttp://universaldependencies.org/ 357 bank for Hungarian is the Szeged Corpus (Csendes et al., 2004) and Szeged Dependency Treebank (Vincze et al., 2010). It contains approximately 82,000 sentences and 1.5 million tokens, all manually annotated for POS-tagging and constituency and dependency syntax. We developed an automatic tool that converts the morphological descriptions of the Szeged Corpus to universal morphology tags and the dependency trees of the Szeged Treebank to universal dependencies. 3 the possessor on the possessed noun but use determiners for this purpose (cf. my car but az autóm (the car-1SGPOSS)). Moreover, the number of the possessed can be marked on the noun in elliptical"
E17-1034,L16-1248,0,0.0132852,"ed datasets available for 40 languages, including English, German, French, Hungarian and Irish, among others1 . In these datasets, the very same tagsets are applied at the morphological and 1 Up to now, several papers have been published on the general principles behind UD (Nivre, 2015; Nivre et al., 2016) or on specific treebanks. For instance, there are UD treebanks available for agglutinative languages such as Finnish (Haverinen et al., 2014; Pyysalo et al., 2015), Estonian (Muischnek et al., 2016) and Japanese (Tanaka et al., 2016), for Slavic languages (Zeman, 2015) and spoken Slovenian (Dobrovoljc and Nivre, 2016) and for Nordic languages such as Norwegian (Øvrelid and Hohle, 2016), Danish (Johannsen et al., 2014) and Swedish (Nivre, 2014), together with several other languages (Persian (Seraji et al., 2016) and Basque (Aranzabe et al., 2014), just to name a few). Recently, a further extension on the UD relations has been proposed: enhanced English dependencies are described in Schuster and Manning (2016). Our UD principles introduced in this paper follow the central UD guidelines (Nivre, 2015) and we did our best to align with the existing guidelines for other morphologically rich languages as well. O"
E17-1034,J13-1005,1,0.880652,"farkas}@inf.u-szeged.hu Abstract plementation of multilingual morphological and syntactic parsers from a computational linguistic point of view. Furthermore, it can enhance studies on linguistic typology and contrastive linguistics. From the viewpoint of syntactic parsing, the languages of the world are usually categorized according to their level of morphological richness (which is negatively correlated with configurationality). At one end, there is English, a strongly configurational language while there is Hungarian at the other end of the spectrum with rich morphology and free word order (Fraser et al., 2013). In this paper, we present how UD principles were adapted to Hungarian, with special emphasis on Hungarian-specific phenomena. Hungarian is one of the prototypical morphologically rich languages thus our UD principles can provide important best practices for the universalization of other morphologically rich languages. The UD guidelines for Hungarian were motivated by both linguistic considerations and data-driven observations. We developed a converter from the existing Szeged Dependency Treebank (Vincze et al., 2010) to UD and manually corrected 1,800 sentences from the newspaper domain. The"
E17-1034,D07-1013,0,0.0285171,"l and syntactic annotations have been constantly developed in the international NLP community. For instance, the MSD morphological coding system was developed for a set of Eastern European languages (Erjavec, 2012), within the MULTEXTEAST project. Interset functions as an interlingua for several morphological coding systems, which can convert different tagsets to the same morphological representation (Zeman, 2008). There have also been some attempts to define a common set of parts-of-speech: Rambow et al. (2006) defined a multilingual tagset for part-of-speech (POS) tagging and parsing, while McDonald and Nivre (2007) identified eight POS tags based on data from the CoNLL-2007 Shared Task (Nivre et al., 2007). Petrov et al. (2012) offered a tagset of 12 POS tags and applied this tagset to 22 languages. Now, Universal Dependencies (UD) is the latest standardized tagset that we are aware of. UD is an international project that aims at developing a unified annotation scheme for dependency syntax and morphology in a language-independent framework (Nivre, 2015). Hungarian was among the first 10 languages of the project, participating also in the first official release in January 2015. In the latest release (Ver"
E17-1034,D13-1032,0,0.0217888,"iences gained during We carried out experiments for investigating the manual correction could reinforce the linguiswhether is there any difference between using the tic conversion rules and the manually corrected original MSD (Vincze et al., 2014) and the new gold standard corpus provides the opportunity for universal morphological (UM) descriptions. We empirical evaluations which we introduce in this were particularly interested in the utility of the section. two representations for dependency parsing. We trained two models of the MarMot morphological 5.1 On the Accuracy of Automatic tagger (Mueller et al., 2013) using the two morConverters phological representation in 10-fold cross-tagging Most of the UD treebanks are the result of autoon our manually corrected 1800 sentences. Then matic conversion from a dependency treebank of we trained and evaluated the Bohnet dependency originally different principles. The accuracy of parser (Bohnet, 2010) on the train/test split of the these automatic converters is unknown, i.e. we do UD repository v3.0 utilizing the two different prenot know how much information was lost or how dicted morphological descriptions. We used the much noise was introduced by the conv"
E17-1034,W13-4917,1,0.920355,"Missing"
E17-1034,L16-1247,0,0.0834687,"also in the first official release in January 2015. In the latest release (Version 1.3, May 2016), there are annotated datasets available for 40 languages, including English, German, French, Hungarian and Irish, among others1 . In these datasets, the very same tagsets are applied at the morphological and 1 Up to now, several papers have been published on the general principles behind UD (Nivre, 2015; Nivre et al., 2016) or on specific treebanks. For instance, there are UD treebanks available for agglutinative languages such as Finnish (Haverinen et al., 2014; Pyysalo et al., 2015), Estonian (Muischnek et al., 2016) and Japanese (Tanaka et al., 2016), for Slavic languages (Zeman, 2015) and spoken Slovenian (Dobrovoljc and Nivre, 2016) and for Nordic languages such as Norwegian (Øvrelid and Hohle, 2016), Danish (Johannsen et al., 2014) and Swedish (Nivre, 2014), together with several other languages (Persian (Seraji et al., 2016) and Basque (Aranzabe et al., 2014), just to name a few). Recently, a further extension on the UD relations has been proposed: enhanced English dependencies are described in Schuster and Manning (2016). Our UD principles introduced in this paper follow the central UD guidelines (N"
E17-1034,W14-6111,0,0.0406033,"ion and the added value of language-specific, i.e. non-universal, annotations. Our results reveal that converting to universal dependencies is not necessarily trivial, moreover, using languagespecific morphological features may have an impact on overall performance. 1 Introduction Morphological tagging and syntactic parsing are key components in most natural language processing (NLP) applications. Linguistic resources and parsers for morphological and syntactic analysis have been developed for several languages, see e.g. the shared tasks on morphologically rich languages (Seddah et al., 2013; Seddah et al., 2014). However, the comparison of results achieved for different languages is not straightforward as most languages and databases apply a unique tagset, moreover, they were annotated following different guidelines. In order to overcome these issues, the project Universal Dependencies and Morphology (UD) has recently been initiated within the NLP community (Nivre, 2015). The main goal of the UD project is to develop a “universal”, i.e. a language-independent morphological and syntactic representation which can contribute to the im356 Proceedings of the 15th Conference of the European Chapter of the"
E17-1034,L16-1261,0,0.0248178,"January 2015. In the latest release (Version 1.3, May 2016), there are annotated datasets available for 40 languages, including English, German, French, Hungarian and Irish, among others1 . In these datasets, the very same tagsets are applied at the morphological and 1 Up to now, several papers have been published on the general principles behind UD (Nivre, 2015; Nivre et al., 2016) or on specific treebanks. For instance, there are UD treebanks available for agglutinative languages such as Finnish (Haverinen et al., 2014; Pyysalo et al., 2015), Estonian (Muischnek et al., 2016) and Japanese (Tanaka et al., 2016), for Slavic languages (Zeman, 2015) and spoken Slovenian (Dobrovoljc and Nivre, 2016) and for Nordic languages such as Norwegian (Øvrelid and Hohle, 2016), Danish (Johannsen et al., 2014) and Swedish (Nivre, 2014), together with several other languages (Persian (Seraji et al., 2016) and Basque (Aranzabe et al., 2014), just to name a few). Recently, a further extension on the UD relations has been proposed: enhanced English dependencies are described in Schuster and Manning (2016). Our UD principles introduced in this paper follow the central UD guidelines (Nivre, 2015) and we did our best to"
E17-1034,L16-1250,0,0.0191574,"h, Hungarian and Irish, among others1 . In these datasets, the very same tagsets are applied at the morphological and 1 Up to now, several papers have been published on the general principles behind UD (Nivre, 2015; Nivre et al., 2016) or on specific treebanks. For instance, there are UD treebanks available for agglutinative languages such as Finnish (Haverinen et al., 2014; Pyysalo et al., 2015), Estonian (Muischnek et al., 2016) and Japanese (Tanaka et al., 2016), for Slavic languages (Zeman, 2015) and spoken Slovenian (Dobrovoljc and Nivre, 2016) and for Nordic languages such as Norwegian (Øvrelid and Hohle, 2016), Danish (Johannsen et al., 2014) and Swedish (Nivre, 2014), together with several other languages (Persian (Seraji et al., 2016) and Basque (Aranzabe et al., 2014), just to name a few). Recently, a further extension on the UD relations has been proposed: enhanced English dependencies are described in Schuster and Manning (2016). Our UD principles introduced in this paper follow the central UD guidelines (Nivre, 2015) and we did our best to align with the existing guidelines for other morphologically rich languages as well. On the other hand, there are several Hungarian-specific phenomena that"
E17-1034,petrov-etal-2012-universal,0,0.0372414,"phological coding system was developed for a set of Eastern European languages (Erjavec, 2012), within the MULTEXTEAST project. Interset functions as an interlingua for several morphological coding systems, which can convert different tagsets to the same morphological representation (Zeman, 2008). There have also been some attempts to define a common set of parts-of-speech: Rambow et al. (2006) defined a multilingual tagset for part-of-speech (POS) tagging and parsing, while McDonald and Nivre (2007) identified eight POS tags based on data from the CoNLL-2007 Shared Task (Nivre et al., 2007). Petrov et al. (2012) offered a tagset of 12 POS tags and applied this tagset to 22 languages. Now, Universal Dependencies (UD) is the latest standardized tagset that we are aware of. UD is an international project that aims at developing a unified annotation scheme for dependency syntax and morphology in a language-independent framework (Nivre, 2015). Hungarian was among the first 10 languages of the project, participating also in the first official release in January 2015. In the latest release (Version 1.3, May 2016), there are annotated datasets available for 40 languages, including English, German, French, Hu"
E17-1034,W15-1821,0,0.0517461,"ges of the project, participating also in the first official release in January 2015. In the latest release (Version 1.3, May 2016), there are annotated datasets available for 40 languages, including English, German, French, Hungarian and Irish, among others1 . In these datasets, the very same tagsets are applied at the morphological and 1 Up to now, several papers have been published on the general principles behind UD (Nivre, 2015; Nivre et al., 2016) or on specific treebanks. For instance, there are UD treebanks available for agglutinative languages such as Finnish (Haverinen et al., 2014; Pyysalo et al., 2015), Estonian (Muischnek et al., 2016) and Japanese (Tanaka et al., 2016), for Slavic languages (Zeman, 2015) and spoken Slovenian (Dobrovoljc and Nivre, 2016) and for Nordic languages such as Norwegian (Øvrelid and Hohle, 2016), Danish (Johannsen et al., 2014) and Swedish (Nivre, 2014), together with several other languages (Persian (Seraji et al., 2016) and Basque (Aranzabe et al., 2014), just to name a few). Recently, a further extension on the UD relations has been proposed: enhanced English dependencies are described in Schuster and Manning (2016). Our UD principles introduced in this paper"
E17-1034,zeman-2008-reusable,0,0.0370556,"ssing the added value of language-specific information at the morphological and syntactic layers along with the interaction of these two. 2 Related Work Standardized tagsets for both morphological and syntactic annotations have been constantly developed in the international NLP community. For instance, the MSD morphological coding system was developed for a set of Eastern European languages (Erjavec, 2012), within the MULTEXTEAST project. Interset functions as an interlingua for several morphological coding systems, which can convert different tagsets to the same morphological representation (Zeman, 2008). There have also been some attempts to define a common set of parts-of-speech: Rambow et al. (2006) defined a multilingual tagset for part-of-speech (POS) tagging and parsing, while McDonald and Nivre (2007) identified eight POS tags based on data from the CoNLL-2007 Shared Task (Nivre et al., 2007). Petrov et al. (2012) offered a tagset of 12 POS tags and applied this tagset to 22 languages. Now, Universal Dependencies (UD) is the latest standardized tagset that we are aware of. UD is an international project that aims at developing a unified annotation scheme for dependency syntax and morph"
E17-1034,rambow-etal-2006-parallel,0,0.0128733,"yers along with the interaction of these two. 2 Related Work Standardized tagsets for both morphological and syntactic annotations have been constantly developed in the international NLP community. For instance, the MSD morphological coding system was developed for a set of Eastern European languages (Erjavec, 2012), within the MULTEXTEAST project. Interset functions as an interlingua for several morphological coding systems, which can convert different tagsets to the same morphological representation (Zeman, 2008). There have also been some attempts to define a common set of parts-of-speech: Rambow et al. (2006) defined a multilingual tagset for part-of-speech (POS) tagging and parsing, while McDonald and Nivre (2007) identified eight POS tags based on data from the CoNLL-2007 Shared Task (Nivre et al., 2007). Petrov et al. (2012) offered a tagset of 12 POS tags and applied this tagset to 22 languages. Now, Universal Dependencies (UD) is the latest standardized tagset that we are aware of. UD is an international project that aims at developing a unified annotation scheme for dependency syntax and morphology in a language-independent framework (Nivre, 2015). Hungarian was among the first 10 languages"
E17-1034,L16-1376,0,0.0409195,"Missing"
I13-1024,C10-1011,0,0.171803,"Missing"
I13-1024,W07-1106,0,0.0517789,"step after syntactic parsing, the special relation of the nominal and the verbal component should be marked, i.e. certain syntactic labels are overwritten. Second, we execute parsing in a way that the training dataset already contains LVC-specific syntactic labels, that is, it is the dependency parser that carries out LVC detection. In this paper, we experiment with both ways and present and evaluate our results. 4 Related Work There have been a considerable number of studies on LVC detection for several languages. They have been automatically identified in several languages such as English (Cook et al., 2007; Tu and Roth, 2011), Dutch (Van de Cruys and Moir´on, 2007), Basque (Gurrutxaga and Alegria, 2011) and German (Evert and Kermes, 2003) just to mention a few. We are aware of one machine learning system that identifies Hungarian LVCs in texts: the system described in Vincze et al. (2013) selects LVC 209 • OBL-LVC – relation between a light verb and its nominal argument (which is not the subject or object or dative): LVCs in Hungarian texts as a “side effect” of parsing sentences. Our dependency parser based method for identifying Hungarian LVCs is novel since to the best of our knowledge, depe"
I13-1024,R11-1089,1,0.901524,"Missing"
I13-1024,W09-2903,0,0.0628033,"ntifiable with syntax-based methods, only with morphological methods, thus we omit them from our investigations. 2 In our view, take advantage of is a light verb construction rather than an idiom. 208 candidates from texts on the basis of syntactic information, then in a second step it classifies them as genuine LVCs or not, using morphological, lexical, syntactic and semantic features. Regarding the methods they use, Fazly and Stevenson (2007), Van de Cruys and Moir´on (2007) and Gurrutxaga and Alegria (2011) used statistical features for identifying LVCs. Others employed rule-based systems (Diab and Bhutada, 2009; Nagy T. et al., 2011), which usually make use of (shallow) linguistic information. Some hybrid systems integrated both statistical and linguistic information as well (Tan et al., 2006; Tu and Roth, 2011). As we aim at identifying LVCs by applying a dependency parser, next we concentrate on studies that are based on syntactic information and are related to MWE extraction. Seretan (2011) developed a method for collocation extraction based on syntactic constraints. Wehrli et al. (2010) argued that collocations can highly contribute to the performance of the parser since many parsing ambiguities"
I13-1024,E03-1080,0,0.0363378,"tic labels are overwritten. Second, we execute parsing in a way that the training dataset already contains LVC-specific syntactic labels, that is, it is the dependency parser that carries out LVC detection. In this paper, we experiment with both ways and present and evaluate our results. 4 Related Work There have been a considerable number of studies on LVC detection for several languages. They have been automatically identified in several languages such as English (Cook et al., 2007; Tu and Roth, 2011), Dutch (Van de Cruys and Moir´on, 2007), Basque (Gurrutxaga and Alegria, 2011) and German (Evert and Kermes, 2003) just to mention a few. We are aware of one machine learning system that identifies Hungarian LVCs in texts: the system described in Vincze et al. (2013) selects LVC 209 • OBL-LVC – relation between a light verb and its nominal argument (which is not the subject or object or dative): LVCs in Hungarian texts as a “side effect” of parsing sentences. Our dependency parser based method for identifying Hungarian LVCs is novel since to the best of our knowledge, dependency parsers have not been directly applied to identify LVCs. Moreover, it requires only a syntactic treebank enhanced with LVC annot"
I13-1024,W06-2407,0,0.134981,"Missing"
I13-1024,E12-1007,1,0.924702,"se (OBJ) and as a light verb object (OBJ-LVC) in the second case. Moreover, it is also seen that the two components of the LVC are not adjacent hence there are crossing branches in the dependency graph. Although the entire Szeged Corpus contains manual LVC and dependency annotation, for the purpose of our study, we just selected texts from the law domain since they contain the biggest number of LVCs. Sentences in the law subcorpus were further filtered due to the fact that state-ofthe-art dependency parsers cannot adequately treat verbless sentences, hence verbless sentences were ignored (see Farkas et al. (2012) for a detailed discussion of the problem). After this filtering step, we experimented with 6173 sentences, which consist of 156,744 tokens and contain 1101 LVCs. We present statistical data on the frequency of the LVC-specific relations in Table 1. As Hungarian is a free word order language, the two components of LVCs, namely, the noun and the light verb, may not be adjacent in all cases, The Corpus The Szeged Constituency Treebank has been manually annotated for light verb constructions (Vincze and Csirik, 2010). This treebank exists in another manually annotated version, namely, with depend"
I13-1024,W11-0807,0,0.125554,"es from texts on the basis of syntactic information, then in a second step it classifies them as genuine LVCs or not, using morphological, lexical, syntactic and semantic features. Regarding the methods they use, Fazly and Stevenson (2007), Van de Cruys and Moir´on (2007) and Gurrutxaga and Alegria (2011) used statistical features for identifying LVCs. Others employed rule-based systems (Diab and Bhutada, 2009; Nagy T. et al., 2011), which usually make use of (shallow) linguistic information. Some hybrid systems integrated both statistical and linguistic information as well (Tan et al., 2006; Tu and Roth, 2011). As we aim at identifying LVCs by applying a dependency parser, next we concentrate on studies that are based on syntactic information and are related to MWE extraction. Seretan (2011) developed a method for collocation extraction based on syntactic constraints. Wehrli et al. (2010) argued that collocations can highly contribute to the performance of the parser since many parsing ambiguities can be excluded if collocations are known and treated as one syntactic unit. Nivre and Nilsson (2004) analyzed the influence of (previous) MWE recognition on dependency parsing and showed that known MWEs"
I13-1024,W07-1102,0,0.0315916,"rules, certain nominal or participial occurrences of LVCs should be spelt as one word in Hungarian (such as tan´acsad´o advice.giver “consultant”). These latter cases are not identifiable with syntax-based methods, only with morphological methods, thus we omit them from our investigations. 2 In our view, take advantage of is a light verb construction rather than an idiom. 208 candidates from texts on the basis of syntactic information, then in a second step it classifies them as genuine LVCs or not, using morphological, lexical, syntactic and semantic features. Regarding the methods they use, Fazly and Stevenson (2007), Van de Cruys and Moir´on (2007) and Gurrutxaga and Alegria (2011) used statistical features for identifying LVCs. Others employed rule-based systems (Diab and Bhutada, 2009; Nagy T. et al., 2011), which usually make use of (shallow) linguistic information. Some hybrid systems integrated both statistical and linguistic information as well (Tan et al., 2006; Tu and Roth, 2011). As we aim at identifying LVCs by applying a dependency parser, next we concentrate on studies that are based on syntactic information and are related to MWE extraction. Seretan (2011) developed a method for collocation"
I13-1024,W07-1104,0,0.0172475,"Missing"
I13-1024,C10-1125,1,0.917628,"d multiword expressions has been investigated in the literature (see Section 4), and many MWE detection systems rely on syntactic information, we are not aware of any approach that aimed at applying a dependency parser for the dedicated task of identifying LVCs. Our approach requires a treebank annotated for syntactic and LVC information at the same time. Due to the availability of annotated resources, we focus on light verb constructions in Hungarian, a morphologically rich language. Thus, we present our experiments on the legal subcorpus of the Szeged Dependency Treebank annotated for LVCs (Vincze and Csirik, 2010) as well as dependency relations (Vincze et al., 2010). We will pay special attention to noncontiguous LVCs in our investigations as there are quite a few non-contiguous LVCs in Hungarian due to the free word order. Our results empirically prove that LVCs can be detected as a “side effect” of dependency parsing. Light verb constructions (LVCs) are verb and noun combinations in which the verb has lost its meaning to some degree and the noun is used in one of its original senses. They often share their syntactic pattern with other constructions (e.g. verbobject pairs) thus LVC detection can be v"
I13-1024,J13-1009,0,0.145725,"Missing"
I13-1024,W11-0802,0,0.0863274,"be spelt as one word in Hungarian (such as tan´acsad´o advice.giver “consultant”). These latter cases are not identifiable with syntax-based methods, only with morphological methods, thus we omit them from our investigations. 2 In our view, take advantage of is a light verb construction rather than an idiom. 208 candidates from texts on the basis of syntactic information, then in a second step it classifies them as genuine LVCs or not, using morphological, lexical, syntactic and semantic features. Regarding the methods they use, Fazly and Stevenson (2007), Van de Cruys and Moir´on (2007) and Gurrutxaga and Alegria (2011) used statistical features for identifying LVCs. Others employed rule-based systems (Diab and Bhutada, 2009; Nagy T. et al., 2011), which usually make use of (shallow) linguistic information. Some hybrid systems integrated both statistical and linguistic information as well (Tan et al., 2006; Tu and Roth, 2011). As we aim at identifying LVCs by applying a dependency parser, next we concentrate on studies that are based on syntactic information and are related to MWE extraction. Seretan (2011) developed a method for collocation extraction based on syntactic constraints. Wehrli et al. (2010) arg"
I13-1024,P13-2046,1,0.917127,"dependency parser that carries out LVC detection. In this paper, we experiment with both ways and present and evaluate our results. 4 Related Work There have been a considerable number of studies on LVC detection for several languages. They have been automatically identified in several languages such as English (Cook et al., 2007; Tu and Roth, 2011), Dutch (Van de Cruys and Moir´on, 2007), Basque (Gurrutxaga and Alegria, 2011) and German (Evert and Kermes, 2003) just to mention a few. We are aware of one machine learning system that identifies Hungarian LVCs in texts: the system described in Vincze et al. (2013) selects LVC 209 • OBL-LVC – relation between a light verb and its nominal argument (which is not the subject or object or dative): LVCs in Hungarian texts as a “side effect” of parsing sentences. Our dependency parser based method for identifying Hungarian LVCs is novel since to the best of our knowledge, dependency parsers have not been directly applied to identify LVCs. Moreover, it requires only a syntactic treebank enhanced with LVC annotation, in other words, there is no need to implement a separate LVC detector from scratch. In the following, we present our experiments and discuss our r"
I13-1024,vincze-2012-light,1,0.850036,"-score 0.2125 0.7445 0.7563 Table 2: Results on LVC detection. Precision Recall F-score 0.8746 0.9008 0.7854 0.7357 0.8276 0.8099 0.7103 0.7940 0.5188 0.5362 0.6000 0.6401 Table 3: Results on detecting contiguous and noncontiguous LVCs. preted on the LVC-specific relations to evaluate the performance of detecting LVCs in the corpus and we evaluated our system on contiguous and non-contiguous LVCs as well. As baselines, we made use of the methods described in Vincze et al. (2013). They first employed dictionary matching, where LVCs collected from a parallel corpus annotated for Hungarian LVCs (Vincze, 2012) were mapped to the lemmatized texts. We also applied dictionary matching as one of our baselines. The main method of Vincze et al. (2013) first parsed each sentence and extracted potential LVCs on the basis of the dependency relations found between verb-object, verb-subject, verbprepositional object, verb-other argument and noun-modifier pairs. The dependency labels were provided by magyarlanc (Zsibrita et al., 2013). Later, C4.5 decision trees were applied to classify candidate LVCs, which exploits a rich feature set. For instance, morphological features exploited the fact that the nominal c"
I13-1024,N10-1089,0,0.265712,"Missing"
I13-1024,W10-3705,0,0.140862,"ria (2011) used statistical features for identifying LVCs. Others employed rule-based systems (Diab and Bhutada, 2009; Nagy T. et al., 2011), which usually make use of (shallow) linguistic information. Some hybrid systems integrated both statistical and linguistic information as well (Tan et al., 2006; Tu and Roth, 2011). As we aim at identifying LVCs by applying a dependency parser, next we concentrate on studies that are based on syntactic information and are related to MWE extraction. Seretan (2011) developed a method for collocation extraction based on syntactic constraints. Wehrli et al. (2010) argued that collocations can highly contribute to the performance of the parser since many parsing ambiguities can be excluded if collocations are known and treated as one syntactic unit. Nivre and Nilsson (2004) analyzed the influence of (previous) MWE recognition on dependency parsing and showed that known MWEs have a beneficial effect on parsing results. Korkontzelos and Manandhar (2010) investigated whether known MWEs improve the performance of statistical shallow parsers and found that they can significantly contribute to the efficiency of parsing. Eryi˘git et al. (2011) analysed the imp"
I13-1024,R13-1099,1,0.861777,"baselines, we made use of the methods described in Vincze et al. (2013). They first employed dictionary matching, where LVCs collected from a parallel corpus annotated for Hungarian LVCs (Vincze, 2012) were mapped to the lemmatized texts. We also applied dictionary matching as one of our baselines. The main method of Vincze et al. (2013) first parsed each sentence and extracted potential LVCs on the basis of the dependency relations found between verb-object, verb-subject, verbprepositional object, verb-other argument and noun-modifier pairs. The dependency labels were provided by magyarlanc (Zsibrita et al., 2013). Later, C4.5 decision trees were applied to classify candidate LVCs, which exploits a rich feature set. For instance, morphological features exploited the fact that the nominal component of LVCs is typically derived from a verbal stem or coincides with a verb, on the other hand, the POS tags of the words and surrounding words were also used as features. As for semantic features, the activity or event semantic senses were looked for among the upper level hyperonyms of the head of the noun phrase in the Hungarian WordNet3 . As lexical features, fifteen typical light verbs were selected from the"
I13-1024,W04-2705,0,0.04354,"s that can only be expressed through a light verb construction (e.g. h´azkutat´ast tart (search.of.premises-ACC hold) ‘to conduct search of premises’ in Hungarian). Third, there are languages that abound in verb + noun constructions or multiword verbs (such as Estonian (Muischnek and Kaalep, 2010) or Persian (Mansoory and Bijankhan, 2008)): verbal concepts are mostly expressed by combining a noun with a light verb (Mansoory and Bijankhan, 2008). On the other hand, there are views that the relationship between the verbal and the nominal component is not that of a normal argument. For instance, Meyers et al. (2004) assume that support verbs (a term related to light verbs) share their arguments with a noun. Chomsky (1981, p.37) calls advantage a quasi-argument of take in the idiom take advantage of.2 Alonso Ramos (1998) proposes the role of quasi-object: this relationship holds between parts of idiomatic constructions, which is in accordance with Chomsky’s usage of the term idiom. In this spirit, the term quasiargument might be extended to signal the relationship between the verbal and the nominal components of light verb constructions as well since they behave as a semantic unit, forming one complex pre"
I13-1024,W11-3806,0,\N,Missing
I13-1024,vincze-etal-2010-hungarian,1,\N,Missing
I13-1038,W07-1101,0,0.0243724,"ind a noun that is a conversive of a verb (i.e. it can be used as a verb without any morphological change), while in vague action verbs such as to make an agreement there is a noun derived from a verb (i.e. there is morphological change). 330 verbs do, get, give, have, make, take were marked. Statistical data on the three corpora are listed in Table 1. Some of the earlier studies aimed at identifying or extracting only a restricted set of LVCs. Most of them focus on verb-object pairs when identifying LVCs (Stevenson et al., 2004; Tan et al., 2006; Fazly and Stevenson, 2007; Cook et al., 2007; Bannard, 2007; Tu and Roth, 2011), thus they concentrate on structures like give a decision or take control. With languages other than English, authors often select verb + prepositional object pairs (instead of verb-object pairs) and categorise them as LVCs or not. See, e.g. Van de Cruys and Moir´on (2007) for Dutch LVC detection or Krenn (2008) for German LVC detection. In other cases, only true LVCs were considered (Stevenson et al., 2004; Tu and Roth, 2011). In some other studies (Cook et al., 2007; Diab and Bhutada, 2009) the authors just distinguished between the literal and idiomatic uses of verb + n"
I13-1038,R11-1089,1,0.869779,"Missing"
I13-1038,C10-1011,0,0.0873412,"Missing"
I13-1038,calzolari-etal-2002-towards,0,0.020926,"e investigate the performance of different candidate extraction methods on two English full-coverage LVC annotated corpora, where we found that less severe candidate extraction methods should be applied. Then we follow a machine learning approach that makes use of an extended and rich feature set to select LVCs among extracted candidates. 1 Introduction A multiword expression (MWE) is a lexical unit that consists of more than one orthographical word, i.e. a lexical unit that contains spaces and displays lexical, syntactic, semantic, pragmatic and/or statistical idiosyncrasy (Sag et al., 2002; Calzolari et al., 2002). Light verb constructions (LVCs) (e.g. to take a decision, to take sg into consideration) form a subtype of MWEs, namely, they consist of a nominal and a verbal component where the verb functions as the syntactic head (the whole construction fulfills the role of a verb in the clause), but the semantic head is the noun (i.e. the noun is used in one of its original senses). The verbal component (also called a light verb) usually loses its original sense to some extent.1 The meaning of LVCs can only partially be computed on the basis of the meanings of their parts and the way they are related to"
I13-1038,W07-1106,0,0.0196449,"ve a laugh we can find a noun that is a conversive of a verb (i.e. it can be used as a verb without any morphological change), while in vague action verbs such as to make an agreement there is a noun derived from a verb (i.e. there is morphological change). 330 verbs do, get, give, have, make, take were marked. Statistical data on the three corpora are listed in Table 1. Some of the earlier studies aimed at identifying or extracting only a restricted set of LVCs. Most of them focus on verb-object pairs when identifying LVCs (Stevenson et al., 2004; Tan et al., 2006; Fazly and Stevenson, 2007; Cook et al., 2007; Bannard, 2007; Tu and Roth, 2011), thus they concentrate on structures like give a decision or take control. With languages other than English, authors often select verb + prepositional object pairs (instead of verb-object pairs) and categorise them as LVCs or not. See, e.g. Van de Cruys and Moir´on (2007) for Dutch LVC detection or Krenn (2008) for German LVC detection. In other cases, only true LVCs were considered (Stevenson et al., 2004; Tu and Roth, 2011). In some other studies (Cook et al., 2007; Diab and Bhutada, 2009) the authors just distinguished between the literal and idiomatic u"
I13-1038,W10-2108,0,0.0300711,"Missing"
I13-1038,W09-2903,0,0.0738282,"out contextual information – are then classified as LVCs or not (Stevenson et al., 2004; Tan et al., 2006; Fazly and Stevenson, 2007; Van de Cruys and Moir´on, 2007; Gurrutxaga and Alegria, 2011). As a gold standard, lists collected from dictionaries or other annotated corpora are used: if the extracted candidate is classified as an LVC and can be found on the list, it is a true positive, regardless of the fact whether it was a genuine LVC in its context. In the second approach, the goal is to detect individual LVC token instances in a running text, taking contextual information into account (Diab and Bhutada, 2009; Tu and Roth, 2011; Nagy T. et al., 2011). While the first approach assumes that a specific candidate in all of its occurrences constitutes an LVC or not (i.e. there are no ambiguous cases), the second one may account for the fact that there are contexts where a given candidate functions as an LVC whereas in other contexts it does not, recall the example of give a ring in Section 1. The authors of Stevenson et al. (2004), Fazly and Stevenson (2007), Van de Cruys and Moir´on 2 In theoretical linguistics, two types of LVCs are distinguished (Kearns, 2002). In true LVCs such as to have a laugh w"
I13-1038,W07-1102,0,0.191858,"t candidate extraction methods (earlier published methods and new solutions implemented by us). • We defined and evaluated several new feature templates like semantic or morphological features to select LVCs in context from extracted candidates. 2 Related Work Two approaches have been introduced for LVC detection. In the first approach, LVC candidates (usually verb-object pairs including one verb from a well-defined set of 3-10 verbs) are extracted from the corpora and these tokens – without contextual information – are then classified as LVCs or not (Stevenson et al., 2004; Tan et al., 2006; Fazly and Stevenson, 2007; Van de Cruys and Moir´on, 2007; Gurrutxaga and Alegria, 2011). As a gold standard, lists collected from dictionaries or other annotated corpora are used: if the extracted candidate is classified as an LVC and can be found on the list, it is a true positive, regardless of the fact whether it was a genuine LVC in its context. In the second approach, the goal is to detect individual LVC token instances in a running text, taking contextual information into account (Diab and Bhutada, 2009; Tu and Roth, 2011; Nagy T. et al., 2011). While the first approach assumes that a specific candidate in all"
I13-1038,W04-0401,0,0.400891,"stematically compare and evaluate different candidate extraction methods (earlier published methods and new solutions implemented by us). • We defined and evaluated several new feature templates like semantic or morphological features to select LVCs in context from extracted candidates. 2 Related Work Two approaches have been introduced for LVC detection. In the first approach, LVC candidates (usually verb-object pairs including one verb from a well-defined set of 3-10 verbs) are extracted from the corpora and these tokens – without contextual information – are then classified as LVCs or not (Stevenson et al., 2004; Tan et al., 2006; Fazly and Stevenson, 2007; Van de Cruys and Moir´on, 2007; Gurrutxaga and Alegria, 2011). As a gold standard, lists collected from dictionaries or other annotated corpora are used: if the extracted candidate is classified as an LVC and can be found on the list, it is a true positive, regardless of the fact whether it was a genuine LVC in its context. In the second approach, the goal is to detect individual LVC token instances in a running text, taking contextual information into account (Diab and Bhutada, 2009; Tu and Roth, 2011; Nagy T. et al., 2011). While the first appro"
I13-1038,W06-2407,0,0.113416,"Missing"
I13-1038,W11-0802,0,0.0236786,"nd new solutions implemented by us). • We defined and evaluated several new feature templates like semantic or morphological features to select LVCs in context from extracted candidates. 2 Related Work Two approaches have been introduced for LVC detection. In the first approach, LVC candidates (usually verb-object pairs including one verb from a well-defined set of 3-10 verbs) are extracted from the corpora and these tokens – without contextual information – are then classified as LVCs or not (Stevenson et al., 2004; Tan et al., 2006; Fazly and Stevenson, 2007; Van de Cruys and Moir´on, 2007; Gurrutxaga and Alegria, 2011). As a gold standard, lists collected from dictionaries or other annotated corpora are used: if the extracted candidate is classified as an LVC and can be found on the list, it is a true positive, regardless of the fact whether it was a genuine LVC in its context. In the second approach, the goal is to detect individual LVC token instances in a running text, taking contextual information into account (Diab and Bhutada, 2009; Tu and Roth, 2011; Nagy T. et al., 2011). While the first approach assumes that a specific candidate in all of its occurrences constitutes an LVC or not (i.e. there are no"
I13-1038,W11-0807,0,0.165256,"-noun combinations in Dutch. Their method relies on selectional preferences for both the noun and the verb. Idiomatic and light verb noun + verb combinations were extracted from Basque texts by employing statistical methods (Gurrutxaga and Alegria, 2011). Diab and Bhutada (2009) and Nagy T. et al. (2011) employed ruled-based methods to detect LVCs, which are usually based on (shallow) linguistic information, while the domain specificity of the problem was highlighted in Nagy T. et al. (2011). Both statistical and linguistic information were applied by the hybrid LVC systems (Tan et al., 2006; Tu and Roth, 2011; Samardˇzi´c and Merlo, 2010), which resulted in better recall scores. English and German LVCs were analysed in parallel corpora: the authors of Samardˇzi´c and Merlo (2010) focus on their manual and automatic alignment. They found that linguistic features (e.g. the degree of compositionality) and the frequency of the construction both have an impact on the alignment of the constructions. Tan et al. (2006) applied machine learning techniques to extract LVCs. They combined statistical and linguistic features, and trained a random forest classifier to separate LVC candidates. Tu and Roth (2011)"
I13-1038,W07-1104,0,0.0770712,"Missing"
I13-1038,P03-1054,0,0.0105105,"Missing"
I13-1038,vincze-2012-light,1,0.837926,"there has been a considerable amount of previous work on LVC detection, but some authors seek to capture just verb–object pairs, while others just verbs with prepositional complements. Actually, many of them exploited only constructions formed with a limited set of light verbs and identified or extracted just a specific type of LVCs. However, we cannot see any benefit that any NLP application could get from these limitations and here, we focus on the full-coverage identification of LVCs. We train and evaluate statistical models on the Wiki50 (Vincze et al., 2011) and SzegedParalellFX (SZPFX) (Vincze, 2012) corpora that have recently been published with full-coverage LVC annotation. We employ a two-stage procedure. First, The identification of light verb constructions (LVC) is an important task for several applications. Previous studies focused on some limited set of light verb constructions. Here, we address the full coverage of LVCs. We investigate the performance of different candidate extraction methods on two English full-coverage LVC annotated corpora, where we found that less severe candidate extraction methods should be applied. Then we follow a machine learning approach that makes use o"
I13-1038,W04-2705,0,\N,Missing
I13-1038,R11-1040,1,\N,Missing
I13-1044,baccianella-etal-2010-sentiwordnet,0,0.0121684,"/quantity is determined. The phenomenon of hedging in scientific articles is analyzed and categorized according to the functions it can fulfill in Hyland (1996). Subjectivity is also related to uncertainty. There is a great diversity among individual views and opinions: a feature of a product may be appreciated by some customers but it might be considered intolerable for others. Thus, what should be considered positive or negative seems subjective. Many approaches to subjectivity or sentiment analysis rely on lexicons and databases of subjective terms. For instance, the database SentiWordNet (Baccianella et al., 2010) contains a subset of the synsets of the Princeton Wordnet with positivity, negativity and neutrality scores assigned to each concept, depending on the use of its sentiment orientation, thus it is a lexicon where subjective terms are listed and ranked. Wilson (2008) defines subjectivity clues as words and phrases that express private states, that is, individual opinions. She distinguishes lexical cues and syntactic cues that are responsible for subjectivity. She lists several modifiers among her syntactic clues of subjectivity like quite and really. However, in contrast with other subjective e"
I13-1044,W10-3112,0,0.471842,"ton much more heavily in his campaign. The sentence does not reveal who has suggested the involvement of Clinton in the campaign. Hence, the source of the information is unclear and the source is missing from the sentence. The basic idea behind weasel phenomena is the lack of a reference: it is not known who the source of the opinion is. This view is supported by the fact that a weasel candidate ceases to be uncertain if it is enhanced by citations: Weasels The notion of source is important for deciding the reliability of information conveyed (Saur´ı and Pustejovsky, 2012; Wiebe et al., 2005; Nawaz et al., 2010). It is not a matter of indifference to whom the information / opinion belongs to, espeMost authors now prefer to place it within the genus Pezoporus, e.g. Leeton et al. (1998). 1 http://en.wikipedia.org/wiki/ Wikipedia:Manual_of_Style/Words_to_watch 384 group. Their effect is to add uncertainty to some elements in the proposition: they shift the value of some quality / quantity and the truth value of the proposition can only be decided if it is known what the reference point in the discourse is as the following example shows: The phrase most authors would indicate a weasel (it is not clear wh"
I13-1044,P13-1162,0,0.0381877,"ines subjectivity clues as words and phrases that express private states, that is, individual opinions. She distinguishes lexical cues and syntactic cues that are responsible for subjectivity. She lists several modifiers among her syntactic clues of subjectivity like quite and really. However, in contrast with other subjective elements, we do not regard them as peacock cues since – as Wilson (2008) herself states – they “work to intensify”, so in our system they are classified as hedge cues. On the other hand, some instances of biased language can also be classified as peacocks in our system (Recasens et al., 2013). Human communication and discourse is incremental in nature (Cristea and Webber, 1997). Information may be added at a later point of the discourse that clarifies a previously missing piece of information. Applying this to discourse-level uncertainty, it may be the case that an apparent weasel phrase is elaborated on later in the discourse, or the exact value of an apparent hedge expression is later provided. In such cases, the phrases should not be marked as uncertain, which indicates the essential role of co-text – i.e. surrounding words in the text (Brown and Yule, 1983) – in detecting disc"
I13-1044,P97-1012,0,0.133612,"ndividual opinions. She distinguishes lexical cues and syntactic cues that are responsible for subjectivity. She lists several modifiers among her syntactic clues of subjectivity like quite and really. However, in contrast with other subjective elements, we do not regard them as peacock cues since – as Wilson (2008) herself states – they “work to intensify”, so in our system they are classified as hedge cues. On the other hand, some instances of biased language can also be classified as peacocks in our system (Recasens et al., 2013). Human communication and discourse is incremental in nature (Cristea and Webber, 1997). Information may be added at a later point of the discourse that clarifies a previously missing piece of information. Applying this to discourse-level uncertainty, it may be the case that an apparent weasel phrase is elaborated on later in the discourse, or the exact value of an apparent hedge expression is later provided. In such cases, the phrases should not be marked as uncertain, which indicates the essential role of co-text – i.e. surrounding words in the text (Brown and Yule, 1983) – in detecting discourse-level uncertainty. 4 The Annotated Corpus In order to test the practical applicab"
I13-1044,J12-2003,0,0.0241626,"Missing"
I13-1044,W09-3012,0,0.0279685,"notated resources are at hand. Previous studies on uncertainty detection concentrated mostly on the semantic dimensions. Indeed, in many cases it is the lexical content (meaning) of the uncertainty marker (cue) that is responsible for uncertainty, i.e. it can be identified 2 Discourse-level Uncertainty Different concepts and terms that are related to uncertainty phenomena are employed. Modality is usually associated with uncertainty (Palmer, 1986), but the terms factuality (Saur´ı and Pustejovsky, 2012), veridicality (de Marneffe et al., 2012), evidentiality (Aikhenvald, 2004) and commitment (Diab et al., 2009) are also used. They all represent related but slightly different linguistic phenomena, which lie mostly in the category of semantic uncertainty. Propositions can be uncertain at the semantic level, that is, their truth value cannot be determined just given the speaker’s mental state. Szarvas et al. (2012) offer a classi383 International Joint Conference on Natural Language Processing, pages 383–391, Nagoya, Japan, 14-18 October 2013. cially in news media: people are more likely to believe a statement if it is communicated by a reliable source as opposed to a piece of sourceless information. I"
I13-1044,J12-2002,0,0.0280341,"Missing"
I13-1044,W10-3001,1,0.937787,"Missing"
I13-1044,J12-2004,1,0.835083,"Missing"
I13-1044,P09-2044,0,0.524773,", Japan, 14-18 October 2013. cially in news media: people are more likely to believe a statement if it is communicated by a reliable source as opposed to a piece of sourceless information. In the public mind, experts, scientists, ministers, etc. are viewed as credible sources (cf. Bell (1991)) while unnamed or unidentifiable sources are considered less reliable. If some pieces of information are backed by a credible source, they are more likely to be treated as trustworthy, however, sourceless information is given less credence. Events with no obvious sources are called weasels in Wikipedia1 (Ganter and Strube, 2009): their source is missing or is specified only vaguely or too generally, hence, it cannot be exactly determined who the holder of the opinion is (undetermined source) as it is either not expressed or expressed by an indefinite noun phrase. Weasel sentences usually invoke questions like Who said that? and Who thinks that? The following sentence illustrates this: fication of semantic uncertainty phenomena. Here, we use the term uncertainty similar to Szarvas et al. (2012), who aimed at giving a unified framework for the above-mentioned phenomena: “uncertain propositions are those [...] whose tru"
I13-1044,W08-0606,1,0.90979,"Missing"
I13-1044,W12-3702,0,0.0200613,"who the author/source is. Thus, information extraction applied for the news media may certainly profit from finding weasels, i.e. missing or undeterminable sources. Pieces of information without an identifiable (and reliable) source require special treatment: they will be excluded from the news or they will be communicated to the public in a special form, using phrases such as according to unnamed sources etc. In sentiment analysis and opinion mining, the identification of subjective terms is essential. These terms are often ambiguous hence a subjectivity word sense disambiguation is needed (Wiebe, 2012). In our corpus, peacock terms and intensifiers – a subtype of hedges – are manually annotated, thus it can be used in the development and evaluation of tools that seek to disambiguate elements of a subjectivity lexicon in running texts. Information retrieval may also be enhanced by detecting discourse-level uncertainty. In order to find relevant documents for queries that contain numbers, more specifically, to improve recall in such cases, it is important to handle numeric hedges. For instance, if someone looks for websites describing games appropriate for ten year old children, he also may b"
I13-1044,P07-1125,0,0.680172,"no more details are specified. Again, the weasel type of uncertainty is expressed here by the adjectives some and other. Note that there is another occurrence of the word some in the sentence, but it does not denote any uncertainty in this case since the relevant Vietnamera films are then listed. 2.2 Hedges Another type of discourse-level uncertainty that will be discussed later on is called a hedge. Although a lot of studies used the term hedge, it may denote different linguistic phenomena for different authors. For instance, hedge means mostly speculation in the biomedical domain (see e.g. Medlock and Briscoe (2007), Vincze et al. (2008), and Farkas et al. (2010)). When contrasting epistemic modality and hedging, Rizomilioti (2006) categorizes approximators, passive voice and attribution to unnamed sources, among others, as instances of hedging and Hyland (1996) also cites them among common hedging devices. Here, we understand hedge in the sense introduced by Lakoff (1973). For him, hedges are “words whose job is to make things fuzzier or less fuzzy”, that is, the exact meaning of some qualities or quantities is blurred by them. Intensifiers (very, much), deintensifiers (a bit, less) and circumscribers ("
J12-2004,W07-1011,0,0.381457,"Missing"
J12-2004,W10-3017,0,0.0948903,"d “pseudo-triggers”. A pseudo-trigger is a superstring of a cue and it is basically used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ an"
J12-2004,W09-1318,0,0.0333556,"Missing"
J12-2004,P07-2009,0,0.0431689,"Missing"
J12-2004,P07-1033,0,0.0872072,"Missing"
J12-2004,W10-3001,1,0.879192,"Missing"
J12-2004,P09-2044,0,0.188168,"Missing"
J12-2004,W10-3004,0,0.129684,"Missing"
J12-2004,W08-0607,0,0.0742456,"Missing"
J12-2004,W09-1418,0,0.0271284,"Missing"
J12-2004,W09-1401,0,0.038846,"Missing"
J12-2004,W10-3011,0,0.0641227,"re a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ and Radev (2009) and Velldal (2010) match cues from a lexicon then approaches. Ozg apply a binary classiﬁer based on feat"
J12-2004,W04-3103,0,0.0604055,"Missing"
J12-2004,W09-1410,0,0.0488634,"Missing"
J12-2004,P07-1125,0,0.422088,"g corpora and uncertainty recognition tools and our chief goal here is to provide a computational linguistics-oriented classiﬁcation. With this in mind, our subclasses are intended to be well-deﬁned and easily identiﬁable by automatic tools. Moreover, this classiﬁcation allows different applications to choose the subset of phenomena to be recognized in accordance with their main task (i.e., we tried to avoid an overly coarse or ﬁne-grained categorization). 2.1 Classiﬁcation of Uncertainty Types Several corpora annotated for uncertainty have been published in different domains such as biology (Medlock and Briscoe 2007; Kim, Ohta, and Tsujii 2008; Settles, Craven, and Friedland 2008; Shatkay et al. 2008; Vincze et al. 2008; Nawaz, Thompson, and Ananiadou 2010), medicine (Uzuner, Zhang, and Sibanda 2009), news media (Rubin, Liddy, and Kando 2005; Wilson 2008; Saur´ı and Pustejovsky 2009; Rubin 2010), and encyclopedia (Farkas et al. 2010). As can be seen from publicly available annotation guidelines, there are many overlaps but differences as well in the understanding of uncertainty, which is sometimes connected to domain- and genre-speciﬁc features of the texts. Here we introduce a domain- and genre-independ"
J12-2004,W09-1304,0,0.0179162,"expressions to deﬁne cues and “pseudo-triggers”. A pseudo-trigger is a superstring of a cue and it is basically used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁca"
J12-2004,W10-3006,0,0.060851,"Missing"
J12-2004,W10-3112,0,0.388287,"Missing"
J12-2004,D09-1145,0,0.0310857,"Missing"
J12-2004,W10-3008,0,0.0728038,"t imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ and Radev (2009) and Velldal (2010) match cues from a lexicon then approaches. Ozg apply a binary classiﬁer based on features describing the co"
J12-2004,W10-3018,0,0.132999,"Missing"
J12-2004,P08-1033,1,0.873695,"ing applications), and a recent pilot task sought to exploit negation and hedge cue detectors in machine reading (Morante and Daelemans 2011). As the focus of our paper is cue recognition, however, we omit their detailed description here. 4.1 In-Domain Cue Detection In-domain uncertainty detectors have been developed since the mid 1990s. Most of these systems use hand-crafted lexicons for cue recognition and they treat each occurrence of the lexicon items as a cue—that is, they do not address the problem of disambiguating cues (Friedman et al. 1994; Light, Qiu, and Srinivasan 2004; Farkas and Szarvas 2008; Saur´ı 2008; Conway, Doan, and Collier 2009; Van Landeghem et al. 2009). ConText (Chapman, Chu, and Dowling 2007) uses regular expressions to deﬁne cues and “pseudo-triggers”. A pseudo-trigger is a superstring of a cue and it is basically used for recognizing contexts where a cue does not imply uncertainty (i.e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have al"
J12-2004,W10-3012,0,0.0255504,"Missing"
J12-2004,W10-3002,0,0.214012,".e., it can be regarded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ and Radev (2009) and Velldal (2010) match cues from a lexicon then approaches. Ozg apply a binary classiﬁer based on features describing the context of the cue c"
J12-2004,W10-3022,0,0.044217,"Missing"
J12-2004,W10-2605,0,0.0210482,"Missing"
J12-2004,W09-1419,0,0.0223659,"Missing"
J12-2004,W10-3007,0,0.0436532,"Missing"
J12-2004,W08-0606,1,0.908141,"Missing"
J12-2004,W10-3013,0,0.0602854,"arded as a hand-crafted cue disambiguation module). MacKinlay, Martinez, and Baldwin (2009) introduced a system which also used non-consecutive tokens as cues (like not+as+yet). Utilizing manually labeled corpora, machine learning–based uncertainty cue detectors have also been developed (to the best of our knowledge each of them uses an in-domain training data set). They use token classiﬁcation (Morante and Daelemans 2009; Clausen 2010; Fernandes, Crestana, and Milidiu´ 2010; S´anchez, Li, and Vogel 2010) or sequence labeling approaches (Li et al. 2010; Rei and Briscoe 2010; Tang et al. 2010; Zhang et al. 2010). In both cases the tokens are labeled according to whether they are part of a cue. The latter assigns a label sequence to a sentence (a sequence of 344 Szarvas et al. Cross-Genre and Cross-Domain Detection of Semantic Uncertainty tokens) thus it naturally deals with the context of a particular word. On the other hand, context information for a token is built into the feature space of the token classiﬁcation ¨ ur ¨ and Radev (2009) and Velldal (2010) match cues from a lexicon then approaches. Ozg apply a binary classiﬁer based on features describing the context of the cue candidate. Each of th"
J12-2004,W10-3009,0,\N,Missing
L16-1459,D09-1062,0,0.0314565,"but it is negative combined with distinct product features in the sentence like It takes a long time to focus (Ding et al., 2008; Ahn et al., 2012). It is worth noting that both sentences could occur in the same domain. Since a great amount of sentiment words are ambiguous as their polarity is concerned, it is essential to adequately identify the actual sentiment value of the given polarity expression in different contexts. Another challenging and often discussed problem is the rule of the so called sentiment shifters in sentiment analysis (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Choi and Cardie, 2009; Ding et al., 2008; Feldman et al., 2010; Loughran and McDonald, 2011; Ruppenhofer, 2013). Sentiment shifters are types of expressions that are used to change the sentiment orientations, for instance, from positive to negative or vice versa (Liu, 2012). One of the classes of sentiment shifters is made up of negation words. For example, even though the sentence I don’t like this camera contains a positive polarity word (like), the sentiment value of the whole sentence is negative due to the negation of the positive polarity word. At the same time, it is essential to handle the sentiment shifte"
L16-1459,P14-2009,0,0.0133599,"owing. These opinions become important resources for those who need information about products, as well as manufacturers who wish to improve their productivity (Ahn et al., 2012). Therefore, the demand for efficient automatic extraction of opinions from the web is growing day by day. However, identifying polar phrases that express opinions towards a certain target seems to be an unsolved and challenging problem so far. With the high increase in the number of projects aiming at an effective sentiment analysis, target-dependent opinion mining is becoming a widely studied task (Hu and Liu, 2004; Dong et al., 2014; Liu, 2012; Jiang et al., 2011). Basically, two features of opinionated texts render the targetdependent analysis more difficult. On the one hand, the document- and the sentence-level sentiment classifications are based on the assumption that each document or each sentence expresses only one definite opinion on a single target (Turney, 2002; Liu, 2012). As a consequence, these methods of analysis are not applicable to documents or sentences which evaluate more than one entity (see Section 3.1.). In addition, classifying opinion texts at the entity2873 level is still insufficient for applicati"
L16-1459,P11-1016,0,0.0322738,"mportant resources for those who need information about products, as well as manufacturers who wish to improve their productivity (Ahn et al., 2012). Therefore, the demand for efficient automatic extraction of opinions from the web is growing day by day. However, identifying polar phrases that express opinions towards a certain target seems to be an unsolved and challenging problem so far. With the high increase in the number of projects aiming at an effective sentiment analysis, target-dependent opinion mining is becoming a widely studied task (Hu and Liu, 2004; Dong et al., 2014; Liu, 2012; Jiang et al., 2011). Basically, two features of opinionated texts render the targetdependent analysis more difficult. On the one hand, the document- and the sentence-level sentiment classifications are based on the assumption that each document or each sentence expresses only one definite opinion on a single target (Turney, 2002; Liu, 2012). As a consequence, these methods of analysis are not applicable to documents or sentences which evaluate more than one entity (see Section 3.1.). In addition, classifying opinion texts at the entity2873 level is still insufficient for applications because they do not reveal t"
L16-1459,W02-1011,0,0.0217102,"larity word. At the same time, it is essential to handle the sentiment shifters with care because not all occurrences of such words can change polarity. Consider the expression not only... but also, where the word not does not change sentiment orientation of the sentence (Liu, 2012). All the problems mentioned here emphasize the essential importance of manually annotated corpora in sentiment analysis tasks. Corpora annotated manually at the sentiment level play a key role not just in training and testing algorithms but in theoretical and applied research concerning sentiment analysis as well (Pang et al., 2002; Dave et al., 2003; Finn and Kushmerick, 2003; Salvetti et al., 2004; Aue and Gamon, 2005; Bai et al., 2005; Wang and Wang, 2007; Boiy and Moens, 2009). 3. 3.1. Annotation Principles of annotation Liu (2012) distinguishes three levels of granularities of the existing methods of sentiment analysis. In general, sentiment analysis can be carried out at the level of the whole document. The task at this level is to determine whether a document expresses a positive or a negative sentiment (Liu, 2012; Turney, 2002). This task is commonly known as document-level sentiment classification (Liu, 2012)."
L16-1459,P02-1053,0,0.026079,"s a certain target seems to be an unsolved and challenging problem so far. With the high increase in the number of projects aiming at an effective sentiment analysis, target-dependent opinion mining is becoming a widely studied task (Hu and Liu, 2004; Dong et al., 2014; Liu, 2012; Jiang et al., 2011). Basically, two features of opinionated texts render the targetdependent analysis more difficult. On the one hand, the document- and the sentence-level sentiment classifications are based on the assumption that each document or each sentence expresses only one definite opinion on a single target (Turney, 2002; Liu, 2012). As a consequence, these methods of analysis are not applicable to documents or sentences which evaluate more than one entity (see Section 3.1.). In addition, classifying opinion texts at the entity2873 level is still insufficient for applications because they do not reveal the relations between entities and their different aspects in the texts, so they are unable to handle entities and aspects adequately. For instance, a positive opinion about an entity does not mean that the author has positive opinions about all aspects of the entity. Likewise, a negative opinion on one or some"
L16-1459,C14-1174,1,0.898372,"Missing"
L18-1061,L16-1021,0,0.0222883,"d at automatic coreference resolution. There is coreference annotation in the DIRNDL and ANCOR Centre corpora, containing German and French spoken language data (Muzerelle et al., 2014; Bj¨orkelund et al., 2014). As for Japanese, the corpus NAIST Text contains coreference annotation, together with predicate-argument structure (Iida et al., 2007). A large coreference corpus is also available for Polish (Ogrodniczuk et al., 2014; Ogrodniczuk et al., 2013b), moreover, there are annotated coreference corpora for Dutch (Hendrickx et al., 2008) and Czech (Nedoluzhko et al., 2009) as well. Recently, Ghaddar and Langlais (2016) reported on WikiCoref, a coreference corpus of English Wikipedia articles. A small dataset with manual coreference annotation was earlier published for Hungarian (Mih´altz, 2012). In contrast, here we present our large corpus, SzegedKoref, which has been manually annotated for coreference data. Due to its size, the corpus can be used for training and evaluating machine learning-based systems, which is nowadays the most popular approach used for coreference resolution (Pradhan et al., 2012). In morphologically rich languages like Hungarian, some issues might occur concerning the annotation pro"
L18-1061,hendrickx-etal-2008-coreference,0,0.309591,"an et al., 2011) and CoNLL-2012 (Pradhan et al., 2012) shared tasks, which aimed at automatic coreference resolution. There is coreference annotation in the DIRNDL and ANCOR Centre corpora, containing German and French spoken language data (Muzerelle et al., 2014; Bj¨orkelund et al., 2014). As for Japanese, the corpus NAIST Text contains coreference annotation, together with predicate-argument structure (Iida et al., 2007). A large coreference corpus is also available for Polish (Ogrodniczuk et al., 2014; Ogrodniczuk et al., 2013b), moreover, there are annotated coreference corpora for Dutch (Hendrickx et al., 2008) and Czech (Nedoluzhko et al., 2009) as well. Recently, Ghaddar and Langlais (2016) reported on WikiCoref, a coreference corpus of English Wikipedia articles. A small dataset with manual coreference annotation was earlier published for Hungarian (Mih´altz, 2012). In contrast, here we present our large corpus, SzegedKoref, which has been manually annotated for coreference data. Due to its size, the corpus can be used for training and evaluating machine learning-based systems, which is nowadays the most popular approach used for coreference resolution (Pradhan et al., 2012). In morphologically r"
L18-1061,W07-1522,0,0.0255214,"oNotes contains coreference annotation for English, Chinese and Arabic (Weischedel et al., 2011; Pradhan et al., 2007). This database formed the training and test sets of the CoNLL-2011 (Pradhan et al., 2011) and CoNLL-2012 (Pradhan et al., 2012) shared tasks, which aimed at automatic coreference resolution. There is coreference annotation in the DIRNDL and ANCOR Centre corpora, containing German and French spoken language data (Muzerelle et al., 2014; Bj¨orkelund et al., 2014). As for Japanese, the corpus NAIST Text contains coreference annotation, together with predicate-argument structure (Iida et al., 2007). A large coreference corpus is also available for Polish (Ogrodniczuk et al., 2014; Ogrodniczuk et al., 2013b), moreover, there are annotated coreference corpora for Dutch (Hendrickx et al., 2008) and Czech (Nedoluzhko et al., 2009) as well. Recently, Ghaddar and Langlais (2016) reported on WikiCoref, a coreference corpus of English Wikipedia articles. A small dataset with manual coreference annotation was earlier published for Hungarian (Mih´altz, 2012). In contrast, here we present our large corpus, SzegedKoref, which has been manually annotated for coreference data. Due to its size, the co"
L18-1061,muzerelle-etal-2014-ancor,0,0.0187535,"e special attention from the viewpoint of coreference resolution. 2. Related Work There are several coreference corpora available for many languages, for instance, OntoNotes contains coreference annotation for English, Chinese and Arabic (Weischedel et al., 2011; Pradhan et al., 2007). This database formed the training and test sets of the CoNLL-2011 (Pradhan et al., 2011) and CoNLL-2012 (Pradhan et al., 2012) shared tasks, which aimed at automatic coreference resolution. There is coreference annotation in the DIRNDL and ANCOR Centre corpora, containing German and French spoken language data (Muzerelle et al., 2014; Bj¨orkelund et al., 2014). As for Japanese, the corpus NAIST Text contains coreference annotation, together with predicate-argument structure (Iida et al., 2007). A large coreference corpus is also available for Polish (Ogrodniczuk et al., 2014; Ogrodniczuk et al., 2013b), moreover, there are annotated coreference corpora for Dutch (Hendrickx et al., 2008) and Czech (Nedoluzhko et al., 2009) as well. Recently, Ghaddar and Langlais (2016) reported on WikiCoref, a coreference corpus of English Wikipedia articles. A small dataset with manual coreference annotation was earlier published for Hung"
L18-1061,ogrodniczuk-etal-2014-polish,0,0.0378524,"Missing"
L18-1061,W11-1901,0,0.0199418,"nal coreference (hypernyms, synonyms etc.), and we offer several examples of each annotated category. We also mark zero anaphors and pronouns coreferential with subordinate clauses since these are two linguistic phenomena of Hungarian that deserve special attention from the viewpoint of coreference resolution. 2. Related Work There are several coreference corpora available for many languages, for instance, OntoNotes contains coreference annotation for English, Chinese and Arabic (Weischedel et al., 2011; Pradhan et al., 2007). This database formed the training and test sets of the CoNLL-2011 (Pradhan et al., 2011) and CoNLL-2012 (Pradhan et al., 2012) shared tasks, which aimed at automatic coreference resolution. There is coreference annotation in the DIRNDL and ANCOR Centre corpora, containing German and French spoken language data (Muzerelle et al., 2014; Bj¨orkelund et al., 2014). As for Japanese, the corpus NAIST Text contains coreference annotation, together with predicate-argument structure (Iida et al., 2007). A large coreference corpus is also available for Polish (Ogrodniczuk et al., 2014; Ogrodniczuk et al., 2013b), moreover, there are annotated coreference corpora for Dutch (Hendrickx et al."
L18-1061,W12-4501,0,0.0203421,"tc.), and we offer several examples of each annotated category. We also mark zero anaphors and pronouns coreferential with subordinate clauses since these are two linguistic phenomena of Hungarian that deserve special attention from the viewpoint of coreference resolution. 2. Related Work There are several coreference corpora available for many languages, for instance, OntoNotes contains coreference annotation for English, Chinese and Arabic (Weischedel et al., 2011; Pradhan et al., 2007). This database formed the training and test sets of the CoNLL-2011 (Pradhan et al., 2011) and CoNLL-2012 (Pradhan et al., 2012) shared tasks, which aimed at automatic coreference resolution. There is coreference annotation in the DIRNDL and ANCOR Centre corpora, containing German and French spoken language data (Muzerelle et al., 2014; Bj¨orkelund et al., 2014). As for Japanese, the corpus NAIST Text contains coreference annotation, together with predicate-argument structure (Iida et al., 2007). A large coreference corpus is also available for Polish (Ogrodniczuk et al., 2014; Ogrodniczuk et al., 2013b), moreover, there are annotated coreference corpora for Dutch (Hendrickx et al., 2008) and Czech (Nedoluzhko et al.,"
L18-1208,C10-1011,0,0.116171,"Missing"
L18-1208,A00-1031,0,0.489098,"Missing"
L18-1208,P07-2053,0,0.0731944,"Missing"
L18-1208,halacsy-etal-2004-creating,0,0.172087,"Missing"
L18-1208,U08-1016,0,0.0609112,"Missing"
L18-1208,novak-2014-new,1,0.883351,"Missing"
L18-1208,R13-1071,1,0.901269,"Missing"
L18-1208,P06-1055,0,0.0450326,"Missing"
L18-1208,P99-1034,0,0.195182,"Missing"
L18-1208,E14-1015,1,0.911134,"Missing"
L18-1208,szarvas-etal-2006-highly,1,0.794467,"Missing"
L18-1208,W02-2024,0,0.347412,"Missing"
L18-1208,tron-etal-2006-morphdb,1,0.758805,"Missing"
L18-1208,vincze-etal-2010-hungarian,1,0.778219,"Missing"
L18-1208,E17-1034,1,0.865126,"Missing"
L18-1208,R13-1099,1,0.88567,"Missing"
P13-2046,W10-3704,0,0.0486031,"Missing"
P13-2046,W07-1101,0,0.0195743,"described below. The candidate extraction method investigated the dependency relation among the verbs and nouns. Verb-object, verb-subject, verbprepositional object, verb-other argument (in the case of Hungarian) and noun-modifier pairs were collected from the texts. The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al., 2013) for Hungarian. Related Work Recently, LVCs have received special interest in the NLP research community. They have been automatically identified in several languages such as English (Cook et al., 2007; Bannard, 2007; Vincze et al., 2011a; Tu and Roth, 2011), Dutch (Van de Cruys and Moir´on, 2007), Basque (Gurrutxaga and Alegria, 2011) and German (Evert and Kermes, 2003). Parallel corpora are of high importance in the automatic identification of multiword expressions: it is usually one-to-many correspondence that is exploited when designing methods for detecting multiword expressions. Caseli et al. (2010) developed an alignment-based method for extracting multiword expressions from Portuguese–English parallel corpora. Samardˇzi´c and Merlo (2010) analyzed English and German light verb constructions in par"
P13-2046,C10-1011,0,0.0699556,"ased approach was applied. This method first parsed each sentence and extracted potential LVCs. Afterwards, a binary classification method was utilized, which can automatically classify potential LVCs as an LVC or not. This binary classifier was based on a rich feature set described below. The candidate extraction method investigated the dependency relation among the verbs and nouns. Verb-object, verb-subject, verbprepositional object, verb-other argument (in the case of Hungarian) and noun-modifier pairs were collected from the texts. The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al., 2013) for Hungarian. Related Work Recently, LVCs have received special interest in the NLP research community. They have been automatically identified in several languages such as English (Cook et al., 2007; Bannard, 2007; Vincze et al., 2011a; Tu and Roth, 2011), Dutch (Van de Cruys and Moir´on, 2007), Basque (Gurrutxaga and Alegria, 2011) and German (Evert and Kermes, 2003). Parallel corpora are of high importance in the automatic identification of multiword expressions: it is usually one-to-many correspondence that is exploited when desig"
P13-2046,W10-2108,0,0.0311527,"Missing"
P13-2046,W07-1106,0,0.110007,"a rich feature set described below. The candidate extraction method investigated the dependency relation among the verbs and nouns. Verb-object, verb-subject, verbprepositional object, verb-other argument (in the case of Hungarian) and noun-modifier pairs were collected from the texts. The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al., 2013) for Hungarian. Related Work Recently, LVCs have received special interest in the NLP research community. They have been automatically identified in several languages such as English (Cook et al., 2007; Bannard, 2007; Vincze et al., 2011a; Tu and Roth, 2011), Dutch (Van de Cruys and Moir´on, 2007), Basque (Gurrutxaga and Alegria, 2011) and German (Evert and Kermes, 2003). Parallel corpora are of high importance in the automatic identification of multiword expressions: it is usually one-to-many correspondence that is exploited when designing methods for detecting multiword expressions. Caseli et al. (2010) developed an alignment-based method for extracting multiword expressions from Portuguese–English parallel corpora. Samardˇzi´c and Merlo (2010) analyzed English and German light verb const"
P13-2046,E03-1080,0,0.248517,"positional object, verb-other argument (in the case of Hungarian) and noun-modifier pairs were collected from the texts. The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al., 2013) for Hungarian. Related Work Recently, LVCs have received special interest in the NLP research community. They have been automatically identified in several languages such as English (Cook et al., 2007; Bannard, 2007; Vincze et al., 2011a; Tu and Roth, 2011), Dutch (Van de Cruys and Moir´on, 2007), Basque (Gurrutxaga and Alegria, 2011) and German (Evert and Kermes, 2003). Parallel corpora are of high importance in the automatic identification of multiword expressions: it is usually one-to-many correspondence that is exploited when designing methods for detecting multiword expressions. Caseli et al. (2010) developed an alignment-based method for extracting multiword expressions from Portuguese–English parallel corpora. Samardˇzi´c and Merlo (2010) analyzed English and German light verb constructions in parallel corpora: they pay special attention to their manual and automatic alignment. Zarrieß 256 The features used by the binary classifier can be categorised"
P13-2046,W09-2906,0,0.0297624,"of integrating language specific features into the systems and we explore how the systems could be further improved. For this purpose, we make use of the English–Hungarian parallel corpus SzegedParalellFX (Vincze, 2012), where LVCs have been manually annotated. 255 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 255–261, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics and Kuhn (2009) argued that multiword expressions can be reliably detected in parallel corpora by using dependency-parsed, word-aligned sentences. Sinha (2009) detected Hindi complex predicates (i.e. a combination of a light verb and a noun, a verb or an adjective) in a Hindi–English parallel corpus by identifying a mismatch of the Hindi light verb meaning in the aligned English sentence. Many-to-one correspondences were also exploited by Attia et al. (2010) when identifying Arabic multiword expressions relying on asymmetries between paralell entry titles of Wikipedia. Tsvetkov and Wintner (2010) identified Hebrew multiword expressions by searching for misalignments in an English–Hebrew parallel corpus. To the best of our knowledge, parallel corpora"
P13-2046,W07-1102,0,0.0909469,"ly bleached (Apresjan, 2004; Alonso Ramos, 2004; Sanrom´an Vilas, 2009) since it also adds important aspects to the meaning of the construction (for instance, the beginning of an action, such as set on fire, see Mel’ˇcuk (2004)). The meaning of LVCs can be only partially computed on the basis of the meanings of their parts and the way they are related to each other, hence it is important to treat them in a special way in many NLP applications. LVCs are usually distinguished from productive or literal verb + noun constructions on the one hand and idiomatic verb + noun expressions on the other (Fazly and Stevenson, 2007). Variativity and omitting the verb play the most significant role in distinguishing LVCs from productive constructions and idioms (Vincze, 2011). Variativity reflects the fact that LVCs can be often substituted by a verb derived from the same root as the nominal component within the construction: productive constructions and idioms can be rarely substituted by a single verb (like make a decision – decide). Omitting the verb exploits the fact that it is the nominal component that mostly bears the semantic content of the LVC, hence the event denoted by the construction can be determined even wi"
P13-2046,W11-0802,0,0.0607681,"nouns. Verb-object, verb-subject, verbprepositional object, verb-other argument (in the case of Hungarian) and noun-modifier pairs were collected from the texts. The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al., 2013) for Hungarian. Related Work Recently, LVCs have received special interest in the NLP research community. They have been automatically identified in several languages such as English (Cook et al., 2007; Bannard, 2007; Vincze et al., 2011a; Tu and Roth, 2011), Dutch (Van de Cruys and Moir´on, 2007), Basque (Gurrutxaga and Alegria, 2011) and German (Evert and Kermes, 2003). Parallel corpora are of high importance in the automatic identification of multiword expressions: it is usually one-to-many correspondence that is exploited when designing methods for detecting multiword expressions. Caseli et al. (2010) developed an alignment-based method for extracting multiword expressions from Portuguese–English parallel corpora. Samardˇzi´c and Merlo (2010) analyzed English and German light verb constructions in parallel corpora: they pay special attention to their manual and automatic alignment. Zarrieß 256 The features used by the b"
P13-2046,W04-0401,0,0.0699871,"Missing"
P13-2046,C10-2144,0,0.0218564,"mputational Linguistics and Kuhn (2009) argued that multiword expressions can be reliably detected in parallel corpora by using dependency-parsed, word-aligned sentences. Sinha (2009) detected Hindi complex predicates (i.e. a combination of a light verb and a noun, a verb or an adjective) in a Hindi–English parallel corpus by identifying a mismatch of the Hindi light verb meaning in the aligned English sentence. Many-to-one correspondences were also exploited by Attia et al. (2010) when identifying Arabic multiword expressions relying on asymmetries between paralell entry titles of Wikipedia. Tsvetkov and Wintner (2010) identified Hebrew multiword expressions by searching for misalignments in an English–Hebrew parallel corpus. To the best of our knowledge, parallel corpora have not been used for testing the efficiency of an MWE-detecting method for two languages at the same time. Here, we investigate the performance of our base LVC-detector on English and Hungarian and pay special attention to the added value of language-specific features. ring made of gold (non-LVC) and He gave her a ring because he wanted to hear her voice (LVC), hence it is important to identify them in context. In theoretical linguistics"
P13-2046,W11-0807,0,0.646827,"it is important to identify them in context. In theoretical linguistics, Kearns (2002) distinguishes between two subtypes of light verb constructions. True light verb constructions such as to give a wipe or to have a laugh and vague action verbs such as to make an agreement or to do the ironing differ in some syntactic and semantic features and can be separated by various tests, e.g. passivization, WH-movement, pronominalization etc. This distinction also manifests in natural language processing as several authors pay attention to the identification of just true light verb constructions, e.g. Tu and Roth (2011). However, here we do not make such a distinction and aim to identify all types of light verb constructions both in English and in Hungarian, in accordance with the annotation principles of SZPFX. The canonical form of a Hungarian light verb construction is a bare noun + third person singular verb. However, they may occur in non-canonical versions as well: the verb may precede the noun, or the noun and the verb may be not adjacent due to the free word order. Moreover, as Hungarian is a morphologically rich language, the verb may occur in different surface forms inflected for tense, mood, perso"
P13-2046,W07-1104,0,0.209543,"Missing"
P13-2046,C10-1125,1,0.899485,"ede the noun, or the noun and the verb may be not adjacent due to the free word order. Moreover, as Hungarian is a morphologically rich language, the verb may occur in different surface forms inflected for tense, mood, person and number. These features will be paid attention to when implementing our system for detecting Hungarian LVCs. 3 4 Experiments In our investigations we made use of the SzegedParalellFX English-Hungarian parallel corpus, which consists of 14,000 sentences and contains about 1370 LVCs for each language. In addition, we are aware of two other corpora – the Szeged Treebank (Vincze and Csirik, 2010) and Wiki50 (Vincze et al., 2011b) –, which were manually annotated for LVCs on the basis of similar principles as SZPFX, so we exploited these corpora when defining our features. To automatically identify LVCs in running texts, a machine learning based approach was applied. This method first parsed each sentence and extracted potential LVCs. Afterwards, a binary classification method was utilized, which can automatically classify potential LVCs as an LVC or not. This binary classifier was based on a rich feature set described below. The candidate extraction method investigated the dependency"
P13-2046,W11-0817,1,0.898653,"b may be not adjacent due to the free word order. Moreover, as Hungarian is a morphologically rich language, the verb may occur in different surface forms inflected for tense, mood, person and number. These features will be paid attention to when implementing our system for detecting Hungarian LVCs. 3 4 Experiments In our investigations we made use of the SzegedParalellFX English-Hungarian parallel corpus, which consists of 14,000 sentences and contains about 1370 LVCs for each language. In addition, we are aware of two other corpora – the Szeged Treebank (Vincze and Csirik, 2010) and Wiki50 (Vincze et al., 2011b) –, which were manually annotated for LVCs on the basis of similar principles as SZPFX, so we exploited these corpora when defining our features. To automatically identify LVCs in running texts, a machine learning based approach was applied. This method first parsed each sentence and extracted potential LVCs. Afterwards, a binary classification method was utilized, which can automatically classify potential LVCs as an LVC or not. This binary classifier was based on a rich feature set described below. The candidate extraction method investigated the dependency relation among the verbs and nou"
P13-2046,R11-1040,1,0.931733,"b may be not adjacent due to the free word order. Moreover, as Hungarian is a morphologically rich language, the verb may occur in different surface forms inflected for tense, mood, person and number. These features will be paid attention to when implementing our system for detecting Hungarian LVCs. 3 4 Experiments In our investigations we made use of the SzegedParalellFX English-Hungarian parallel corpus, which consists of 14,000 sentences and contains about 1370 LVCs for each language. In addition, we are aware of two other corpora – the Szeged Treebank (Vincze and Csirik, 2010) and Wiki50 (Vincze et al., 2011b) –, which were manually annotated for LVCs on the basis of similar principles as SZPFX, so we exploited these corpora when defining our features. To automatically identify LVCs in running texts, a machine learning based approach was applied. This method first parsed each sentence and extracted potential LVCs. Afterwards, a binary classification method was utilized, which can automatically classify potential LVCs as an LVC or not. This binary classifier was based on a rich feature set described below. The candidate extraction method investigated the dependency relation among the verbs and nou"
P13-2046,vincze-2012-light,1,0.827914,"t differences among languages and the usefulness of techniques that are applied. In this paper, we focus on the task of identifying light verb constructions (LVCs) in English and Hungarian free texts. Thus, the same task will be carried out for English and a morphologically rich language. We compare whether the same set of features can be used for both languages, we investigate the benefits of integrating language specific features into the systems and we explore how the systems could be further improved. For this purpose, we make use of the English–Hungarian parallel corpus SzegedParalellFX (Vincze, 2012), where LVCs have been manually annotated. 255 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 255–261, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics and Kuhn (2009) argued that multiword expressions can be reliably detected in parallel corpora by using dependency-parsed, word-aligned sentences. Sinha (2009) detected Hindi complex predicates (i.e. a combination of a light verb and a noun, a verb or an adjective) in a Hindi–English parallel corpus by identifying a mismatch of the Hindi light verb meaning in the"
P13-2046,W09-2904,0,0.0702218,"Missing"
P16-2030,W14-3204,0,0.0541916,"ur system uses machine learning techniques and is based on several features like linguistic characteristics of spontaneous speech as well as features exploiting morphological and syntactic parsing. Recently, several studies have reported results on identifying different types of dementia with NLP and speech recognition techniques. For instance, automatic speech recognition tools were employed in detecting aphasia (Fraser et al., 2013b; Fraser et al., 2014; Fraser et al., 2013a) and mild cognitive impairment (Lehr et al., 2012), and Alzheimer’s Disease (Baldas et al., 2010; Satt et al., 2014). Jarrold et al. (2014) distinguished four types of dementia on the basis of spontaneous speech samples. Lexical analysis of spontaneous speech may also indicate different types of dementia (Bucks et al., 2000; Holmes and Singh, 1996) and may be exploited in the automatic detection of patients suffering from dementia (Thomas et al., Here we seek to automatically identify Hungarian patients suffering from mild cognitive impairment (MCI) based on linguistic features collected from their speech transcripts. Our system uses machine learning techniques and is based on several linguistic features like characteristics of s"
P16-2030,W13-3909,0,0.0249698,"atest extent possible (K´alm´an et al., 2013). Here we seek to automatically identify Hungarian patients suffering from mild cognitive impairment based on their speech transcripts. Our system uses machine learning techniques and is based on several features like linguistic characteristics of spontaneous speech as well as features exploiting morphological and syntactic parsing. Recently, several studies have reported results on identifying different types of dementia with NLP and speech recognition techniques. For instance, automatic speech recognition tools were employed in detecting aphasia (Fraser et al., 2013b; Fraser et al., 2014; Fraser et al., 2013a) and mild cognitive impairment (Lehr et al., 2012), and Alzheimer’s Disease (Baldas et al., 2010; Satt et al., 2014). Jarrold et al. (2014) distinguished four types of dementia on the basis of spontaneous speech samples. Lexical analysis of spontaneous speech may also indicate different types of dementia (Bucks et al., 2000; Holmes and Singh, 1996) and may be exploited in the automatic detection of patients suffering from dementia (Thomas et al., Here we seek to automatically identify Hungarian patients suffering from mild cognitive impairment (MCI)"
P16-2030,I13-1044,1,0.835158,"he task was regarded as binary classification, i.e. subjects were classified as either an MCI patient or a healthy control, on the basis of a feature set derived from their transcripts. At first, transcripts were morphologically and syntactically analysed with magyarlanc, a linguistic preprocessing toolkit developed for Hungarian 1 Our experiments conform to all operative rules and restrictions on data collection, anonymization and publication according to the requirements in the European Union. 2 This words seem to have a lot in common with weasel and hedge words, which refer to uncertainty (Vincze, 2013). 182 (Zsibrita et al., 2013). For classification, we exploited morphological, syntactic and semantic features extracted from the output of magyarlanc. Each person was asked to recall three different stories. As MCI is strongly related to memory deficit, we believe that the order of the tasks might also influence performance, hence we opted for processing each transcript separately. Thus, for each person, features to be discussed below were calculated separately for the three transcripts and all of them were exploited in the system. can’t remember”) as they directly signal problems with memory"
P16-2030,R13-1099,1,0.88611,"as binary classification, i.e. subjects were classified as either an MCI patient or a healthy control, on the basis of a feature set derived from their transcripts. At first, transcripts were morphologically and syntactically analysed with magyarlanc, a linguistic preprocessing toolkit developed for Hungarian 1 Our experiments conform to all operative rules and restrictions on data collection, anonymization and publication according to the requirements in the European Union. 2 This words seem to have a lot in common with weasel and hedge words, which refer to uncertainty (Vincze, 2013). 182 (Zsibrita et al., 2013). For classification, we exploited morphological, syntactic and semantic features extracted from the output of magyarlanc. Each person was asked to recall three different stories. As MCI is strongly related to memory deficit, we believe that the order of the tasks might also influence performance, hence we opted for processing each transcript separately. Thus, for each person, features to be discussed below were calculated separately for the three transcripts and all of them were exploited in the system. can’t remember”) as they directly signal problems with memory and recall; number of negati"
R11-1023,C10-3015,0,0.0267635,"hieve considerably better results than mwetoolkit. Again, in this type of evaluation, CRF models which used NEs as feature reached the best F-score. The mwetoolkit style evaluation is useful in e.g. collecting dictionary entries while the other type of evaluation is useful in e.g. information extraction or machine translation. based and dictionary-based approaches is bigger in the BNC dataset. Furthermore, in this corpus too, CRF approaches enhanced with the NE feature performed best. We found only one available other system to English noun compound recognition. This is the mwetoolkit system (Ramisch et al., 2010), a language-independent tool developed for collecting MWEs from texts (which is able to identify noun compounds). We evaluated it on these two corpora too. This system also relies heavily on POS tag features, therefore we completed the mwetoolkit POS tag rules with our POS rules. However, the mwetoolkit basically does not mark MWEs in the raw text, it just extracts noun compounds from the text, i.e. multiple occurrences of the same MWE are not taken into account. Therefore, in order to compare the results of our approaches to those of mwetoolkit, we assessed our methods similarly to the evalu"
R11-1023,M98-1001,0,0.0397991,"several NLP-related tasks. 1 2 Noun compounds and named entities in NLP applications A compound is a lexical unit that consists of two or more elements that exist on their own. Compounds can be classified as follows (Sag et al., 2002; Kim, 2008): nominal compounds (bass player), adjectival compounds (dark skinned), adverbial compounds (all in all), prepositional compounds (in front of ), and multiword conjunctions (in order that). Named entity recognition is another widely researched topic in NLP. There are several methods developed for many languages and domains (Grishman and Sundheim, 1995; Chinchor, 1998; Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Multiword named entities can be composed of any words or even characters and their meaning cannot be traced back to their parts. For instance, Ford Focus refers to a car and has nothing to do with the original meaning of ford or focus, thus, it is justifiable to treat the whole expression as one unit. Multiword expressions and named entities usually need special treatment in NLP systems due to their idiosyncratic features. Named entities often consist of more than one word, i.e. they can be seen as a specific type of multiword expre"
R11-1023,W02-2024,0,0.0518918,"1 2 Noun compounds and named entities in NLP applications A compound is a lexical unit that consists of two or more elements that exist on their own. Compounds can be classified as follows (Sag et al., 2002; Kim, 2008): nominal compounds (bass player), adjectival compounds (dark skinned), adverbial compounds (all in all), prepositional compounds (in front of ), and multiword conjunctions (in order that). Named entity recognition is another widely researched topic in NLP. There are several methods developed for many languages and domains (Grishman and Sundheim, 1995; Chinchor, 1998; Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Multiword named entities can be composed of any words or even characters and their meaning cannot be traced back to their parts. For instance, Ford Focus refers to a car and has nothing to do with the original meaning of ford or focus, thus, it is justifiable to treat the whole expression as one unit. Multiword expressions and named entities usually need special treatment in NLP systems due to their idiosyncratic features. Named entities often consist of more than one word, i.e. they can be seen as a specific type of multiword expressions / noun compound"
R11-1023,W00-1308,0,0.0350999,"cted n-grams which occurred as links in English Wikipedia articles. Later, non-English terms, named entities and non-nominal compounds were automatically deleted from the list. We combined three methods: first, a noun compound candidate was marked if it occurred in the list. The second method involved the merge of two possible noun compounds: if a b and b c both occurred in the list, a b c was also accepted as a noun compound. Third, a noun compound candidate was marked if its POS-tag sequence matched one of the previously defined patterns. POS tags were determined by the Stanford POS Tagger (Toutanova and Manning, 2000). Results achieved by the combination of these methods are shown in the DictCombined row of Table 2. 3.2 Machine Learning approaches In addition to the above-described approach, we defined another method for automatically identifying noun compounds. The Conditional Random Fields (CRF) classifier was used (MALLET implementations (McCallum, 2002)). The feature set includes the following categories (Szarvas et al., 2006): orthographical features: capitalisation, word length, bit information about the word form (contains a digit or not, has uppercase character inside the word, etc.), character lev"
R11-1023,M95-1001,0,0.108841,"res tend to be beneficial in several NLP-related tasks. 1 2 Noun compounds and named entities in NLP applications A compound is a lexical unit that consists of two or more elements that exist on their own. Compounds can be classified as follows (Sag et al., 2002; Kim, 2008): nominal compounds (bass player), adjectival compounds (dark skinned), adverbial compounds (all in all), prepositional compounds (in front of ), and multiword conjunctions (in order that). Named entity recognition is another widely researched topic in NLP. There are several methods developed for many languages and domains (Grishman and Sundheim, 1995; Chinchor, 1998; Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Multiword named entities can be composed of any words or even characters and their meaning cannot be traced back to their parts. For instance, Ford Focus refers to a car and has nothing to do with the original meaning of ford or focus, thus, it is justifiable to treat the whole expression as one unit. Multiword expressions and named entities usually need special treatment in NLP systems due to their idiosyncratic features. Named entities often consist of more than one word, i.e. they can be seen as a specific type of"
R11-1023,R11-1040,1,0.761672,"as one unit. It is this similarity that we would like to exploit when investigating the effect of already known NEs/noun compounds on the identification of the other type. On the other hand, our research focuses on the role of noun compounds and named entities in keyphrase extraction. In order to gain keyphrases from free texts, noun compounds might be of great help since once identified, they can be considered as one unit, i.e. like any other single word, which can be beneficial in e.g. frequency counts. Furthermore, the subject of texts is in many cases a named entity (in the Wiki50 corpus (Vincze et al., 2011), 39 articles are about a person, organization, location or another named entity), which fact underlines the importance of giving named entities a special treatment when identifying the topic of a text by keyphrases. 3 R 58.07 65.65 85.58 87.07 P 69.86 72.44 86.02 87.28 F 63.42 68.68 85.81 87.18 Table 1: Results of leave-one-out approaches in terms of precision (P), recall (R) and F-measure (F) in Wiki50. MWE: our CRF trained with basic feature set, which was extended with automatically collected MWE dictionary, MWE + NE: our CRF with MWE features extended with NEs as feature, NE: our CRF trai"
R11-1023,S10-1004,0,0.0344256,"filiations part of a scientific paper is not likely to be a valid choice for such a phrase that describes well the content of the document.) In order to examine the possible utility of the usage of multiword expressions in the task of keyphrase extraction, we conducted experiments in this field. In our experiments we regarded the extraction of keyphrases from scientific documents as a supervised learning task, similarly to others (Frank et al., 1999; Turney, 2003; Witten et al., 1999). As for the dataset of our experiments, we used that of the shared task on keyphrase extraction of SemEval-2 (Kim et al., 2010). The dataset is a subset of the ACM Digital 4.1 Methodology In our system we used the supervised learning approach for keyphrase extraction, in which the keyphrases of a document are determined by first identifying a set of potentially good phrases, then classifying its elements as either proper or nonproper keyphrases, based on the prediction of a machine learned model. We used the machine learning framework of MALLET (McCallum, 2002) for learning the proper keyphrases. Experiments using Maximum Entropy and Na¨ıve Bayes classifiers were both conducted. One key aspect in keyphrase extraction"
R11-1023,W03-0419,0,\N,Missing
R11-1040,calzolari-etal-2002-towards,0,0.452683,"typically focus on only one specific type of MWE. That is, there are hardly any corpora that contain manual annotation for several types of English MWEs at the same time. On the other hand, to the best of our knowledge, there exist no corpora where various types of MWEs are anIntroduction In natural language processing (NLP), a challenging task is the proper treatment of multiword expressions (MWEs). Multiword expressions are lexical items that can be decomposed into single words and display lexical, syntactic, semantic, pragmatic and/or statistical idiosyncrasy (Sag et al., 2002; Kim, 2008; Calzolari et al., 2002) thus, they often pose a problem to NLP systems. Named Entity Recognition (NER) is another widely researched topic in NLP. There are several methods developed for many languages and domains, which are tested on manually annotated databases, e.g. the MUC-6 and MUC-7 and the CoNLL-2002/2003 challenges aimed at identifying NEs in newswire texts (Grishman and Sundheim, 1995; Chinchor, 1998; Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Multiword named entities can be composed of any words or even characters and their meaning cannot be traced back to their parts. For instance, Ford Fo"
R11-1040,W02-2024,0,0.0271478,"a database of English compound nouns (BNC dataset in Table 1). As for multiword verbs, corpora and databases for English (Cook et al., 2008), German (Krenn, 2008), Estonian (Muischnek and Kaalep, 2010) and Hungarian (Vincze and Csirik, 2010) have been recently developed. The Prague Dependency Treebank is also annotated for multiword expressions (Bejcek and Stran´ak, 2010). As for named entities, several corpora have been constructed, for instance, within the framework of the ACE project (Doddington et al., 2004) and for international challenges such as the CoNLL-2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) or the MUC datasets (Grishman and Sundheim, 1995; Chinchor, 1998) – just to name a few. As can be seen, although there are a number of corpora and databases annotated for MWEs, they typically focus on only one specific type of MWE. That is, there are hardly any corpora that contain manual annotation for several types of English MWEs at the same time. On the other hand, to the best of our knowledge, there exist no corpora where various types of MWEs are anIntroduction In natural language processing (NLP), a challenging task is the proper treatment of multi"
R11-1040,M98-1001,0,0.126154,"databases for English (Cook et al., 2008), German (Krenn, 2008), Estonian (Muischnek and Kaalep, 2010) and Hungarian (Vincze and Csirik, 2010) have been recently developed. The Prague Dependency Treebank is also annotated for multiword expressions (Bejcek and Stran´ak, 2010). As for named entities, several corpora have been constructed, for instance, within the framework of the ACE project (Doddington et al., 2004) and for international challenges such as the CoNLL-2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) or the MUC datasets (Grishman and Sundheim, 1995; Chinchor, 1998) – just to name a few. As can be seen, although there are a number of corpora and databases annotated for MWEs, they typically focus on only one specific type of MWE. That is, there are hardly any corpora that contain manual annotation for several types of English MWEs at the same time. On the other hand, to the best of our knowledge, there exist no corpora where various types of MWEs are anIntroduction In natural language processing (NLP), a challenging task is the proper treatment of multiword expressions (MWEs). Multiword expressions are lexical items that can be decomposed into single word"
R11-1040,W00-1308,0,0.0169929,"noun compound and b c too, they can be merged, so a b c is also a noun compound. In this case, a b c was only accepted as a noun compound if a b and b c occurred in the list and the frequency of the whole phrase exceeded the current threshold (Merge). As for the the third approach, several part-of-speech based patterns such as JJ (NN|NNS) were created. A potential noun compound in the text was accepted if it appeared in the list, its POS code sequence matched one of the patterns and its frequency exceeded the current threshold (POS rules). POS codes were determined using Stanford POS Tagger (Toutanova and Manning, 2000). Finally, we combined these approaches: we accepted a potential noun compound if it appeared in the list, its POS code sequence matched our patterns, furthermore, merges were allowed too (Combined). Results in terms of F-measure are shown in Table 4, from which it can be seen that best results were obtained when all the above mentioned extensions were combined and no frequency threshold was considered. 5.2 Machine Learning approaches In addition to the above-described approach, we defined another method for automatically identifying noun compounds. The Conditional Random Fields (CRF) classifi"
R11-1040,C10-1125,1,0.610087,"esting MWE-detectors or NER systems, which we illustrate with experiments and it also makes it possible to investigate the co-occurrences of different types of MWEs and NEs within the same domain. 1 2 Related corpora and databases Several corpora and databases of MWEs have been constructed for a number of languages. For instance, Nicholson and Baldwin (2008) describe a corpus and a database of English compound nouns (BNC dataset in Table 1). As for multiword verbs, corpora and databases for English (Cook et al., 2008), German (Krenn, 2008), Estonian (Muischnek and Kaalep, 2010) and Hungarian (Vincze and Csirik, 2010) have been recently developed. The Prague Dependency Treebank is also annotated for multiword expressions (Bejcek and Stran´ak, 2010). As for named entities, several corpora have been constructed, for instance, within the framework of the ACE project (Doddington et al., 2004) and for international challenges such as the CoNLL-2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) or the MUC datasets (Grishman and Sundheim, 1995; Chinchor, 1998) – just to name a few. As can be seen, although there are a number of corpora and databases annotated for MWEs, they typically f"
R11-1040,W03-0419,0,\N,Missing
R11-1040,doddington-etal-2004-automatic,0,\N,Missing
R11-1040,M95-1001,0,\N,Missing
R11-1089,ramisch-etal-2010-mwetoolkit,0,0.0411474,"Missing"
R11-1089,C10-1002,0,0.0496997,"Missing"
R11-1089,C10-2120,0,0.0396304,"Missing"
R11-1089,W07-1101,0,0.378328,"Missing"
R11-1089,W10-3711,0,0.0651327,"Missing"
R11-1089,W06-2407,0,0.698027,"Missing"
R11-1089,P10-2020,0,0.0668157,"Missing"
R11-1089,W00-1308,0,0.0386564,"der to delete nonEnglish terms, named entities and non-nominal compounds etc. In the case of the method ‘Match’, a noun compound candidate was marked if it occurred in the list. In the case of ‘POS-rules’, a noun compound candidate was marked if it occurred in the list and its POS-tag sequence matched one of the previously defined patterns (e.g. JJ (NN|NNS)). For light verb constructions, the POS-rule method meant that each n-gram for which the pre-defined patterns (e.g. VB.? (NN|NNS)) could be applied was accepted as light verb constructions. For POS-tagging, we used the Stanford POS-tagger (Toutanova and Manning, 2000). Since the methods to follow rely on morphological information 3.2 Corpora used for evaluation For the evaluation of our models, we made use of three corpora. Data on the corpora are shown in Table 1. First, we used Wiki50 (Vincze et al., 2011b), in which several types of multiword expressions (including nominal compounds and light verb constructions) and named entities were marked. The corpus contains 2929 occurrences of nominal compounds and 368 occurrences of light verb con623 Corpus Wikipedia BNC dataset Parallel Sentence 4350 1000 14,262 Token 114,570 21,631 298,948 NC 2929 368 - LVC 368"
R11-1089,calzolari-etal-2002-towards,0,0.348658,"s, on the other hand, there might be domain-related differences in their frequency and typology. In this paper, we show how our methods developed for identifying noun compounds and light verb constructions can be adapted to different domains and different types of texts. Our results indicate that with little effort, existing solutions for detecting multiword expressions can be successfully applied to other domains as well. 1 Introduction Multiword expressions (MWEs) are lexical units that consist of more than one orthographical word, i.e. a lexical unit that contains spaces (Sag et al., 2002; Calzolari et al., 2002). There are several methods developed for identifying several types of MWEs, however, different kinds of multiword expressions require different solutions. Furthermore, there might be domain-related differences in the frequency of a specific MWE type. In this paper, we show how our methods developed for identifying noun compounds and light verb constructions can be adapted to different domains and different types of texts, namely, Wikipedia articles and texts from various topics. Our results suggest that with simple modifications, competitive results can be achieved on the target domains. 2 3"
R11-1089,D07-1110,0,0.0636952,"Missing"
R11-1089,W07-1106,0,0.387773,"Missing"
R11-1089,W11-0817,1,0.636061,"rred in the list and its POS-tag sequence matched one of the previously defined patterns (e.g. JJ (NN|NNS)). For light verb constructions, the POS-rule method meant that each n-gram for which the pre-defined patterns (e.g. VB.? (NN|NNS)) could be applied was accepted as light verb constructions. For POS-tagging, we used the Stanford POS-tagger (Toutanova and Manning, 2000). Since the methods to follow rely on morphological information 3.2 Corpora used for evaluation For the evaluation of our models, we made use of three corpora. Data on the corpora are shown in Table 1. First, we used Wiki50 (Vincze et al., 2011b), in which several types of multiword expressions (including nominal compounds and light verb constructions) and named entities were marked. The corpus contains 2929 occurrences of nominal compounds and 368 occurrences of light verb con623 Corpus Wikipedia BNC dataset Parallel Sentence 4350 1000 14,262 Token 114,570 21,631 298,948 NC 2929 368 - LVC 368 1100 The adaptation process involved the development of more fine-tuned and sophisticated methods considering the domain-specific features of the texts and characteristics of the annotations. Thus, in the case of noun compounds, POSrules were"
R11-1089,R11-1040,1,0.763573,"rred in the list and its POS-tag sequence matched one of the previously defined patterns (e.g. JJ (NN|NNS)). For light verb constructions, the POS-rule method meant that each n-gram for which the pre-defined patterns (e.g. VB.? (NN|NNS)) could be applied was accepted as light verb constructions. For POS-tagging, we used the Stanford POS-tagger (Toutanova and Manning, 2000). Since the methods to follow rely on morphological information 3.2 Corpora used for evaluation For the evaluation of our models, we made use of three corpora. Data on the corpora are shown in Table 1. First, we used Wiki50 (Vincze et al., 2011b), in which several types of multiword expressions (including nominal compounds and light verb constructions) and named entities were marked. The corpus contains 2929 occurrences of nominal compounds and 368 occurrences of light verb con623 Corpus Wikipedia BNC dataset Parallel Sentence 4350 1000 14,262 Token 114,570 21,631 298,948 NC 2929 368 - LVC 368 1100 The adaptation process involved the development of more fine-tuned and sophisticated methods considering the domain-specific features of the texts and characteristics of the annotations. Thus, in the case of noun compounds, POSrules were"
R11-1089,P03-1054,0,0.0129412,"Missing"
R11-1089,C10-3015,0,\N,Missing
R11-1089,W03-1807,0,\N,Missing
R11-2001,W10-3711,0,0.0480466,"Missing"
R11-2001,calzolari-etal-2002-towards,0,0.0437412,"Missing"
R11-2001,D10-1098,0,0.0266724,"uns in the Genia and Europarl corpora and in general texts (Ramisch et al., 2010b; Ramisch et al., 2010c). Some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (Bannard, 2007; Cook et al., 2007; Al-Haj and Wintner, 2010). This results in better coverage of multiword expressions. Rule-based domain adaptation techniques are employed in multi-domain named entity recognition as well, and their usability is demonstrated in news stories, broadcast news and informal texts (Chiticariu et al., 2010). They show that domainspecific rules on the classification of ambiguous named entities (e.g. city names as locations or sports clubs) have positive influence on the results. 4 Corpora Used for Evaluation • verb + noun combinations: give advice • participles: photos taken • nominal forms: service provider • split constructions (i.e. the verb and the noun are not adjacent): a decision has rarely been made The average agreement rate between annotators was 0.7603 (F-measure). The corpus is available under the Creative Commons license at http:// www.inf.u-szeged.hu/rgai/mwe. Data on the corpora ar"
R11-2001,W07-1106,0,0.0330591,"andidates are ranked according to their belonging to the general language or the sublanguage of the domain. The tool mwetoolkit (Ramisch et al., 2010a) is designed to identify several types of MWEs in different domains, which is illustrated through the example of identifying English compound nouns in the Genia and Europarl corpora and in general texts (Ramisch et al., 2010b; Ramisch et al., 2010c). Some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (Bannard, 2007; Cook et al., 2007; Al-Haj and Wintner, 2010). This results in better coverage of multiword expressions. Rule-based domain adaptation techniques are employed in multi-domain named entity recognition as well, and their usability is demonstrated in news stories, broadcast news and informal texts (Chiticariu et al., 2010). They show that domainspecific rules on the classification of ambiguous named entities (e.g. city names as locations or sports clubs) have positive influence on the results. 4 Corpora Used for Evaluation • verb + noun combinations: give advice • participles: photos taken • nominal forms: service"
R11-2001,P07-1033,0,0.135809,"Missing"
R11-2001,W07-1102,0,0.0249869,"0 occurrences of light verb constructions). The parallel corpus consists of texts from magazines, novels1 , language books and texts on the European Union are also included. In this corpus, different syntactic forms of light verb constructions are annotated: Related Work Light verb constructions have been paid special attention in NLP literature. Sag et al. (2002) classify them as a subtype of lexicalized phrases and flexible expressions. They are usually distinguished from productive or literal verb + noun constructions on the one hand and idiomatic verb + noun expressions on the other hand: Fazly and Stevenson (2007) use statistical measures in order to classify subtypes of verb + noun combinations. There are several solutions developed for identifying different types of MWEs in different domains. Bonin et al. (2010) use contrastive filtering in order to identify multiword terminology in scientific, Wikipedia and legal texts: the extracted term candidates are ranked according to their belonging to the general language or the sublanguage of the domain. The tool mwetoolkit (Ramisch et al., 2010a) is designed to identify several types of MWEs in different domains, which is illustrated through the example of"
R11-2001,P03-1054,0,0.00766654,"nent was among those of the most frequent ones were accepted. The ‘Stem’ method pays attention to the stem of the noun. The nominal component is typically derived from a verbal stem (make a decision) or coincides with a verb (have a walk). In this case, we accepted only candidates that had a nominal component whose stem was of verbal nature, i.e. coincided with a stem of a verb. Syntactic information can also be exploited in identifying LVCs. Typically, the syntactic relation between the verb and the nominal component in a light verb construction is dobj or partmod (using the Stanford parser (Klein and Manning, 2003)) – if it is a prepositional light verb construction, the relation between the verb and the preposition is prep. The ‘Syntax’ method accepts candidates among whose members the above syntactic relations hold. We combined the above methods to identify light verb constructions in our databases (the union of candidates yielded by the methods is de4.3 Machine Learning Approaches for Identifying Light Verb Constructions In addition to the above-described approach, we defined another method for automatically identifying LVCs. The Conditional Random Fields (CRF) classifier was used (MALLET implementat"
R11-2001,C10-3015,0,0.012047,"r literal verb + noun constructions on the one hand and idiomatic verb + noun expressions on the other hand: Fazly and Stevenson (2007) use statistical measures in order to classify subtypes of verb + noun combinations. There are several solutions developed for identifying different types of MWEs in different domains. Bonin et al. (2010) use contrastive filtering in order to identify multiword terminology in scientific, Wikipedia and legal texts: the extracted term candidates are ranked according to their belonging to the general language or the sublanguage of the domain. The tool mwetoolkit (Ramisch et al., 2010a) is designed to identify several types of MWEs in different domains, which is illustrated through the example of identifying English compound nouns in the Genia and Europarl corpora and in general texts (Ramisch et al., 2010b; Ramisch et al., 2010c). Some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (Bannard, 2007; Cook et al., 2007; Al-Haj and Wintner, 2010). This results in better coverage of multiword expressions. Rule-based domain adaptation techniques are e"
R11-2001,ramisch-etal-2010-mwetoolkit,0,0.0192911,"r literal verb + noun constructions on the one hand and idiomatic verb + noun expressions on the other hand: Fazly and Stevenson (2007) use statistical measures in order to classify subtypes of verb + noun combinations. There are several solutions developed for identifying different types of MWEs in different domains. Bonin et al. (2010) use contrastive filtering in order to identify multiword terminology in scientific, Wikipedia and legal texts: the extracted term candidates are ranked according to their belonging to the general language or the sublanguage of the domain. The tool mwetoolkit (Ramisch et al., 2010a) is designed to identify several types of MWEs in different domains, which is illustrated through the example of identifying English compound nouns in the Genia and Europarl corpora and in general texts (Ramisch et al., 2010b; Ramisch et al., 2010c). Some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (Bannard, 2007; Cook et al., 2007; Al-Haj and Wintner, 2010). This results in better coverage of multiword expressions. Rule-based domain adaptation techniques are e"
R11-2001,C10-2120,0,0.0229445,"r literal verb + noun constructions on the one hand and idiomatic verb + noun expressions on the other hand: Fazly and Stevenson (2007) use statistical measures in order to classify subtypes of verb + noun combinations. There are several solutions developed for identifying different types of MWEs in different domains. Bonin et al. (2010) use contrastive filtering in order to identify multiword terminology in scientific, Wikipedia and legal texts: the extracted term candidates are ranked according to their belonging to the general language or the sublanguage of the domain. The tool mwetoolkit (Ramisch et al., 2010a) is designed to identify several types of MWEs in different domains, which is illustrated through the example of identifying English compound nouns in the Genia and Europarl corpora and in general texts (Ramisch et al., 2010b; Ramisch et al., 2010c). Some hybrid systems make use of both statistical and linguistic information as well, that is, rules based on syntactic or semantic regularities are also incorporated into the system (Bannard, 2007; Cook et al., 2007; Al-Haj and Wintner, 2010). This results in better coverage of multiword expressions. Rule-based domain adaptation techniques are e"
R11-2001,W00-1308,0,0.0879779,"Missing"
R11-2001,C10-1125,1,0.900321,"Missing"
R11-2001,W11-0817,1,0.812373,"ce on the results. 4 Corpora Used for Evaluation • verb + noun combinations: give advice • participles: photos taken • nominal forms: service provider • split constructions (i.e. the verb and the noun are not adjacent): a decision has rarely been made The average agreement rate between annotators was 0.7603 (F-measure). The corpus is available under the Creative Commons license at http:// www.inf.u-szeged.hu/rgai/mwe. Data on the corpora are shown in Table 1. 4.2 Rule-Based Methods for Identifying Light Verb Constructions In our investigations, we applied similar methods to those described in Vincze et al. (2011). Experiments 1 Not all of the literary texts have been annotated for light verb constructions in the corpus, which made us possible to study the characteristics of the domain and the corpus without having access to the test dataset. For the automatic identification of light verb constructions in corpora, we implemented sev2 Corpus Wikipedia Parallel Sentence 4,350 14,262 Token 114,570 298,948 LVC 368 1,100 noted by ∪ while the intersection is denoted by ∩ in the respective tables). In order to use the same dataset for evaluating rule based and machine learning methods, we randomly separated t"
R11-2001,W07-1101,0,\N,Missing
R11-2001,C10-1002,0,\N,Missing
R11-2006,baccianella-etal-2010-sentiwordnet,0,0.014041,"eristics of opinion phrase extraction. Opinionated phrases often bear special orthographic characteristics, e.g. in the case of so slooow or CHEAP. Features that represent this phenomenon were also incorporated in the feature space: the first feature is responsible for character runs (i.e. more than 2 of the same consecutive characters), and another is responsible for strange capitalization (i.e. the presence of uppercase characters besides the initial one). One feature used external information on the individual tokens of a candidate phrase. It relied on the sentiment scores of SentiWordNet (Baccianella et al., 2010), a publicly available database that contains a subset of the synsets of the Princeton Wordnet with positivity, negativity and neutrality scores assigned to each one, depending on the use of its sentiment orientation (which can be regarded as the probability of a phrase belonging to a synset being mentioned in a positive, negative or neutral context). These scores were utilized for the calculation of the sentiment orientations of each token of a candidate phrase. Surface-based SentiWordNet-calculated feature values for a candidate phrase included the maximal positivity and negativity and subje"
R11-2006,I11-1130,1,0.646311,"ng sentences and assigned to the candidate phrase. The mean of the sentiment scores of the individual sentences yielded a general score on the sentiment orientation of the sentences containing a candidate phrase, while higher values for the deviation was intended to capture cases when a reviewer writes both factual (i.e. uses few opinionated words) and non-factual (i.e. uses more emotional phrases and opinions) sentences about a product. A more detailed description on the framework and evaluation results dealing with the intradomain setting (including human evaluation as well) can be found in Berend (2011). In addition to that system, here the feature augmentation technique of ) was applyed to improve inter-domain results. 4 Dataset Evaluation Evaluation was carried out on two fairly different domains, i.e. on reviews dealing with mobile phones and movies from the site epinions. com. Section 4.1 presents the dataset of product reviews and the way the set of proper opinion phrases (which served as positive training examples) were determined for its elements, and Section 4.2 describes experimental results achieved on that dataset. The evaluation procedure was strict in the sense that only perfect"
R11-2006,P07-1033,0,0.0241746,"Missing"
R11-2006,P08-1036,0,0.0211045,"necessarily successful in the extraction of opinion phrases from product reviews. On the one hand, although proper phrases have their decisive role in both types of genres, opinion phrases are the ones that form the sentiments of the opinion holder, whereas in the case of scientific keyphrases they should be such phrases that summarize well the content of a document. Note the difference between opinionforming phrases and those which summarize well the content of a document, i.e. one can frequently 2 Related Work There have been many studies on opinion mining (Turney, 2002; Pang et al., 2002; Titov and McDonald, 2008; Liu and Seneff, 2009). Our approach relates to previous work on the extraction of reasons for opinions. Most of these papers treat the task of mining reasons from product reviews as one of identifying sentences that express the author’s negative or positive feelings (Hu and Liu, 2004a; Popescu and Etzioni, 2005). This paper is clearly distinguishable from previous opinion mining systems as our goal is to find the reasons for opinions expressed and we aim the task of phrase extraction instead of sentence recognition. This work differs in important aspects even from the frequent pattern mining"
R11-2006,W00-1308,0,0.0114238,"Missing"
R11-2006,P06-2063,0,0.0159897,"emselves review by review, instead of multi-labeling some aspects. These approaches are intended for applications used by companies who would like to obtain a general overview about a product or would like to monitor the polarity relating to their products in a particular community. In contrast, we introduce here a keyphrase extraction-based approach which works at the document level as it extracts keyphrases from reviews which are handled independently of each other. This approach is more appropriate for the consumers, who would like to be informed before purchasing some product. The work of Kim and Hovy (2006) lies probably the closest to ours. They addressed the task of extracting con and pro sentences, i.e. the sentences on why the reviewers liked or disliked the product. They also note that such pro and con expressions can differ from positive and negative opinion expressions as factual sentences can also be reason sentences (e.g. Video drains battery.). Here the difference is that they extracted sentences, but we targeted phrase extraction. Most of the keyphrase extraction approaches (Witten et al., 1999; Turney, 2003; Medelyan et al., 2009; Kim et al., 2010) extract phrases from one document t"
R11-2006,P02-1053,0,0.00345798,"ific keyphrase extraction are not necessarily successful in the extraction of opinion phrases from product reviews. On the one hand, although proper phrases have their decisive role in both types of genres, opinion phrases are the ones that form the sentiments of the opinion holder, whereas in the case of scientific keyphrases they should be such phrases that summarize well the content of a document. Note the difference between opinionforming phrases and those which summarize well the content of a document, i.e. one can frequently 2 Related Work There have been many studies on opinion mining (Turney, 2002; Pang et al., 2002; Titov and McDonald, 2008; Liu and Seneff, 2009). Our approach relates to previous work on the extraction of reasons for opinions. Most of these papers treat the task of mining reasons from product reviews as one of identifying sentences that express the author’s negative or positive feelings (Hu and Liu, 2004a; Popescu and Etzioni, 2005). This paper is clearly distinguishable from previous opinion mining systems as our goal is to find the reasons for opinions expressed and we aim the task of phrase extraction instead of sentence recognition. This work differs in important"
R11-2006,S10-1004,0,0.110633,"ews is introduced. Since the necessary amount of training examples from any arbitrary product type (target domain) is not always available, the possible usage of domain adaptation in the task of opinion phrase extraction is also examined. Experimental results show that models relying on training examples mainly from a different domain can still yield results that are comparable to that of the intra-domain settings. 1 Introduction There has been a growing interest in the NLP treatment of subjectivity and sentiment analysis (see e.g. Balahur et al. (2011)) and that of keyphrase extraction, e.g. Kim et al. (2010). Product reviews serve as perfect objects for the combination of the above mentioned research areas as the opinion bearing phrases of a product review can be interpreted analogously to regular keyphrases of scientific documents, i.e. in both cases proper phrases have decisive role within the document where they were present. The fact that some review portals have the possibility to leave a set of pro and con phrases underlines this resemblance between opinion phrases and scientific keyphrases. However, despite the somewhat common nature of opinion phrases and keyphrases, methods that work on"
R11-2006,P03-1054,0,0.00920024,"Missing"
R11-2006,D09-1017,0,0.0146379,"the extraction of opinion phrases from product reviews. On the one hand, although proper phrases have their decisive role in both types of genres, opinion phrases are the ones that form the sentiments of the opinion holder, whereas in the case of scientific keyphrases they should be such phrases that summarize well the content of a document. Note the difference between opinionforming phrases and those which summarize well the content of a document, i.e. one can frequently 2 Related Work There have been many studies on opinion mining (Turney, 2002; Pang et al., 2002; Titov and McDonald, 2008; Liu and Seneff, 2009). Our approach relates to previous work on the extraction of reasons for opinions. Most of these papers treat the task of mining reasons from product reviews as one of identifying sentences that express the author’s negative or positive feelings (Hu and Liu, 2004a; Popescu and Etzioni, 2005). This paper is clearly distinguishable from previous opinion mining systems as our goal is to find the reasons for opinions expressed and we aim the task of phrase extraction instead of sentence recognition. This work differs in important aspects even from the frequent pattern mining-based approach of Hu a"
R11-2006,D09-1137,0,0.0154574,"be informed before purchasing some product. The work of Kim and Hovy (2006) lies probably the closest to ours. They addressed the task of extracting con and pro sentences, i.e. the sentences on why the reviewers liked or disliked the product. They also note that such pro and con expressions can differ from positive and negative opinion expressions as factual sentences can also be reason sentences (e.g. Video drains battery.). Here the difference is that they extracted sentences, but we targeted phrase extraction. Most of the keyphrase extraction approaches (Witten et al., 1999; Turney, 2003; Medelyan et al., 2009; Kim et al., 2010) extract phrases from one document that are the most characteristic of its content. In these supervised approaches keyphrase extraction is regarded as a classification task, in which certain n-grams of a specific document function as keyphrase candidates, and the task is to classify them as proper or improper keyphrases. Here, our task formalization of keyphrase extraction is adapted from this line of research for opinion mining and we focus on the extraction of argument phrases from product reviews that induce sentiments in its author. As community generated pros and cons c"
R11-2006,W02-1011,0,0.0144683,"extraction are not necessarily successful in the extraction of opinion phrases from product reviews. On the one hand, although proper phrases have their decisive role in both types of genres, opinion phrases are the ones that form the sentiments of the opinion holder, whereas in the case of scientific keyphrases they should be such phrases that summarize well the content of a document. Note the difference between opinionforming phrases and those which summarize well the content of a document, i.e. one can frequently 2 Related Work There have been many studies on opinion mining (Turney, 2002; Pang et al., 2002; Titov and McDonald, 2008; Liu and Seneff, 2009). Our approach relates to previous work on the extraction of reasons for opinions. Most of these papers treat the task of mining reasons from product reviews as one of identifying sentences that express the author’s negative or positive feelings (Hu and Liu, 2004a; Popescu and Etzioni, 2005). This paper is clearly distinguishable from previous opinion mining systems as our goal is to find the reasons for opinions expressed and we aim the task of phrase extraction instead of sentence recognition. This work differs in important aspects even from t"
R11-2006,H05-1043,0,0.0808475,"ch phrases that summarize well the content of a document. Note the difference between opinionforming phrases and those which summarize well the content of a document, i.e. one can frequently 2 Related Work There have been many studies on opinion mining (Turney, 2002; Pang et al., 2002; Titov and McDonald, 2008; Liu and Seneff, 2009). Our approach relates to previous work on the extraction of reasons for opinions. Most of these papers treat the task of mining reasons from product reviews as one of identifying sentences that express the author’s negative or positive feelings (Hu and Liu, 2004a; Popescu and Etzioni, 2005). This paper is clearly distinguishable from previous opinion mining systems as our goal is to find the reasons for opinions expressed and we aim the task of phrase extraction instead of sentence recognition. This work differs in important aspects even from the frequent pattern mining-based approach of Hu and Liu (2004b), since they regarded the main task of mining opinion features with respect to a group of products, not individually at reviewlevel as we did. Even if an opinion feature phrase 41 Proceedings of the Student Research Workshop associated with RANLP 2011, pages 41–47, Hissar, Bulg"
R11-2006,H05-2017,0,\N,Missing
R13-1099,C10-1011,0,0.0132619,"rsing There are two mainstream approaches to syntactic parsing: the one based on constituency grammar and the other one based on dependency grammar. Dependency parsers are believed to be especially useful for parsing languages with free word order such as Hungarian since these parsers are able to connect grammatically related words that are not adjacent. 767 Borpancsol´okra , zajong´okra e´ s a´ llatk´ınz´okra nagyon sz´am´ıtanak . Farkas et al. (2012) made the first experiments on applying state-of-the-art dependency parsers to Hungarian. Since their results indicated that the Bohnet parser (Bohnet, 2010) was the most efficient on Hungarian dependency parsing, we integrated this parser into magyarlanc. The applied model was trained on the Szeged Dependency Treebank, which consists of 82,000 sentences, is manually POS-tagged and contains manually annotated dependency parses for each sentence (Vincze et al., 2010). Multiword named entities (e.g. Coca Cola Ltd.) and multiword numbers (e.g. 42 million) are treated in a special way. We consider the last word as the head because the last word of multiword units gets inflected in Hungarian and all the previous elements are attached to the succeeding"
R13-1099,E12-1007,1,0.927764,"of the methodology for morphosyntactic analysis has been developed for English. However, the linguistic analysis of morphologically rich and free word order languages requires special techniques. Hence, it was not sufficient to simply employ available tools and retrain on Hungarian corpora, we had to modify/adapt them. We hope that our findings and experiences gained during this 2 Grammatical Features of Hungarian In this section, we provide a basic description of the Hungarian language with special emphasis on the phenomena that are important for morphological and syntactic parsing, based on Farkas et al. (2012). For a better understanding of the phenomena described, English will be used as a contrast language. 763 Proceedings of Recent Advances in Natural Language Processing, pages 763–771, Hissar, Bulgaria, 7-13 September 2013. Figure 1: Dependency graph of the sentence V´artalak tegnap este “I was waiting for you last night”. Hungarian is an agglutinative language, thus a word can have hundreds of word forms due to inflectional or derivational affixation. Grammatical information is usually encoded in morphology and Hungarian is a typical morphologically rich language. Word order is free in the sen"
R13-1099,C94-1097,0,0.218107,"Missing"
R13-1099,C12-2105,1,0.832546,"attached to the last word, the antepenultimate word to the penultimate one etc. with an NE relation for named entities and a NUM relation for numbers. In the verbless clauses the Szeged Dependency Treebank introduces virtual nodes. This solution means that a similar tree structure is ascribed to the same sentence in the present third person singular / plural and all the other tenses / persons (see Figure 2). A further argument for the use of a virtual node is that the virtual node is always present at the syntactic level since it is overt in all the other forms, tenses and moods of the verb. Seeker et al. (2012) experimented with several methods for inserting virtual nodes into the verbless clauses. Although their results indicate that this issue still requires further investigation, in magyarlanc, we follow their complex label approach, which means that children of a virtual node are assigned a complex dependency label (e.g. ROOT-VAN-SUBJ), referring to the fact that the specific node is the subject of a virtual node (here VAN) which is itself not present in the sentence but functions as the root. Figure 2 shows variations of a sentence in the past tense and in the present tense with a virtual node"
R13-1099,N03-1033,0,0.00502307,"essive form of the noun, e.g. Ajk´an Ajka-SUP (a town in Hungary) or lip-SUP “in Ajka” or “on his lip” and here the reduced codes also differ from each other. An inflected form of a third person singular possessive form of a noun with front vowels may coincide with the inflected possessed form of the same noun, e.g. e´ nek´et song-3 SGPOSS-ACC or song-POSS-ACC “his song” or “that of his song”, egyed@Nn-sn eszik@Vmmp2s—y egy@Mc-snd—-s2 where the lemma and the morphological code are separated by an @ sign. 5.3 POS-tagging POS-tagging is executed by a modified version of the Stanford POS-tagger (Toutanova et al., 2003), which is based on a Maximum Entropy classifier and makes use of the possible tags provided 2 http://morphadorner.northwestern.edu/ 766 Feature SubPOS Num Cas NumP PerP NumPd Mood Tense Per Def Deg Clitic Form Coord Type N • • • • • • V • • V • • • • • • n A • • • • • • • P • • • • • • T • R • • • R l • S • C • M • • • • • • • • • I I o X Y Z O • • • • • • O e/d/n • • • • • • • • • Table 1: Relevant features for each part of speech and subtypes of parts of speech: type – SubPOS, number – Num, case – Cas, number of possessor – NumP, person of possessor – PerP, number of possessed – NumPd, mood"
R13-1099,W05-1106,0,0.345567,"Missing"
R13-1099,tron-etal-2006-morphdb,0,0.838656,"Missing"
R13-1099,varadi-2002-hungarian,0,0.318574,"Missing"
R13-1099,E03-1012,0,\N,Missing
R13-1099,vincze-etal-2010-hungarian,1,\N,Missing
R13-1099,halacsy-etal-2004-creating,0,\N,Missing
racz-etal-2014-4fx,W11-0807,0,\N,Missing
racz-etal-2014-4fx,W04-2705,0,\N,Missing
racz-etal-2014-4fx,C10-1125,1,\N,Missing
racz-etal-2014-4fx,steinberger-etal-2006-jrc,0,\N,Missing
racz-etal-2014-4fx,calzolari-etal-2002-towards,0,\N,Missing
racz-etal-2014-4fx,R11-1040,1,\N,Missing
racz-etal-2014-4fx,vincze-2012-light,1,\N,Missing
vincze-2012-light,W11-0807,0,\N,Missing
vincze-2012-light,W04-2705,0,\N,Missing
vincze-2012-light,W10-4006,0,\N,Missing
vincze-2012-light,C10-1125,1,\N,Missing
vincze-2012-light,C10-3015,0,\N,Missing
vincze-2012-light,W09-2904,0,\N,Missing
vincze-2012-light,W09-1418,0,\N,Missing
vincze-2012-light,W07-1106,0,\N,Missing
vincze-2012-light,W07-1101,0,\N,Missing
vincze-2012-light,W09-2906,0,\N,Missing
vincze-2012-light,C10-2144,0,\N,Missing
vincze-2012-light,W10-3001,1,\N,Missing
vincze-2012-light,calzolari-etal-2002-towards,0,\N,Missing
vincze-2012-light,W10-3704,0,\N,Missing
vincze-2012-light,W07-1104,0,\N,Missing
vincze-2012-light,W11-0802,0,\N,Missing
vincze-2012-light,W10-3705,0,\N,Missing
vincze-2012-light,W11-0817,1,\N,Missing
vincze-2012-light,R11-1040,1,\N,Missing
vincze-2012-light,W10-3713,0,\N,Missing
vincze-2012-light,R11-1089,1,\N,Missing
vincze-etal-2010-hungarian,C00-2143,0,\N,Missing
vincze-etal-2010-hungarian,dzeroski-etal-2006-towards,0,\N,Missing
vincze-etal-2010-hungarian,D07-1096,0,\N,Missing
vincze-etal-2014-automatic,R13-1099,1,\N,Missing
vincze-etal-2014-szeged,R13-1099,1,\N,Missing
vincze-etal-2014-szeged,tron-etal-2006-morphdb,0,\N,Missing
W08-0606,E99-1043,0,0.0344432,"display different properties in the use of speculative or negated phrases. Take, for instance, the Conclusions section of scientific papers that tends to contain significantly more uncertain or negative findings than the description of Experimental settings and methods. Scientific abstracts are the main targets for various Text Mining applications like proteinprotein interaction mining due to their public accessibility (e.g. through PubMed). We therefore decided to include quite a lot of texts from the abstracts of scientific papers. This is why we included the abstracts of the Genia corpus (Collier et al., 1999). This decision was straightforward for two reasons. First, the Genia corpus contains syntax tree annotation, which allows a comparison between scope annotation and syntactic structure. Being syntactic in nature, scopes should align with the bracket structure of syntax trees, while scope resolution algorithms that exploit treebank data can be used as a theoretical upper bound for the evaluation of parsers for resolving negative/hedge scopes. The other reason was that scope annotation can mutually benefit from the rich annotations of the Genia corpus, such as term annotation (evaluation) and ev"
W08-0606,W04-3103,0,0.587853,"are also encoded within the system, thus it enables them to make a distinction between negated/uncertain concepts and factual information which is crucial in information retrieval. Elkin et al. (2005) use a list of negation words and a list of negation scope-ending words in order to identify negated statements and their scope. Although a fair amount of literature on uncertainty (or hedging) in scientific texts has been produced since the 1990s (e.g. Hyland, 1994), speculative language from a Natural Language Processing perspective has only been studied in the past few years. Previous studies (Light et al., 2004) showed that the detection of hedging can be solved effectively by looking for specific keywords which imply speculative content. Another possibility is to treat the problem as a classification task and train a statistical model to discriminate speculative and non-speculative 39 assertions. This approach requires the availability of labeled instances to train the models on. Medlock and Briscoe (2007) proposed a weakly supervised setting for hedge classification in scientific texts where the aim is to minimise human supervision needed to obtain an adequate amount of training data. Their system"
W08-0606,P07-1125,0,0.819191,"scientific texts has been produced since the 1990s (e.g. Hyland, 1994), speculative language from a Natural Language Processing perspective has only been studied in the past few years. Previous studies (Light et al., 2004) showed that the detection of hedging can be solved effectively by looking for specific keywords which imply speculative content. Another possibility is to treat the problem as a classification task and train a statistical model to discriminate speculative and non-speculative 39 assertions. This approach requires the availability of labeled instances to train the models on. Medlock and Briscoe (2007) proposed a weakly supervised setting for hedge classification in scientific texts where the aim is to minimise human supervision needed to obtain an adequate amount of training data. Their system focuses on locating hedge cues in text and thus they do not determine the scopes (in other words in a text they define the scope to be a whole sentence). 1.2 Related resources Even though the problems of negation (mainly in the medical domain) and hedging (mainly in the scientific domain) have received much interest in the past few years, open access annotated resources for training, testing and comp"
W08-0606,W07-1013,0,0.0264665,"apers and biological paper abstracts (texts from Genia). Table 1 summarises the chief characteristics of the three subcorpora. The 3rd and 5th rows of the table show the ratio of sentences which contain negated or uncertain statements. The 4rd and 6th rows show the number of negation and hedge cue occurrences in the given corpus. A major part of the corpus consists of clinical free-texts. We chose to add medical texts to the corpus in order to facilitate research on negation/hedge detection in the clinical domain. The 43 radiology report corpus that was used for the clinical coding challenge (Pestian et al., 2007) organised by the Computational Medicine Center in Cincinatti, Ohio in 2007 was annotated for negations and uncertainty along with the scopes of each phenomenon. This part contains 1954 documents, each having a clinical history and an impression part, the latter being denser in negated and speculative parts. Another part of the corpus consists of full scientific articles. 5 articles from FlyBase (the same data were used by Medlock and Briscoe (2007) for evaluating sentence-level hedge classifiers) and 4 articles from the open access BMC Bioinformatics website were downloaded and annotated for"
W10-3001,W07-1011,0,0.139993,"hose not backed up by facts (e.g. references) should be avoided (see 3.2 for details). Thus, the community-edited encyclopedia, Wikipedia became one of the subjects of the shared task as well. The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motiva"
W10-3001,W09-1318,0,0.0289259,") should be avoided (see 3.2 for details). Thus, the community-edited encyclopedia, Wikipedia became one of the subjects of the shared task as well. The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information"
W10-3001,W10-3011,0,0.0922122,"Missing"
W10-3001,P09-2044,0,0.611839,"on exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of clinical free-texts, biological texts from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a su¨ ur pervised sequence labeling approach while Ozg¨ and Radev (2009) developed a rule-based system that exploits syntactic"
W10-3001,W04-3103,0,0.868901,"es. Uncertainty detection is also important, e.g. in encyclopedias, where the goal is to collect reliable world knowledge about real-world concepts and topics. For example, Wikipedia explicitly declares that statements reflecting author opinions or those not backed up by facts (e.g. references) should be avoided (see 3.2 for details). Thus, the community-edited encyclopedia, Wikipedia became one of the subjects of the shared task as well. The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-aut"
W10-3001,W10-3005,0,0.0502528,"arge pool of in-domain texts without time-wasting pre-processing tasks (cleaning and sentence splitting). Tables 1, 2 and 3 contain the results of the submitted systems for Task1 and Task2. The last name of the first author of the system description paper (published in these proceedings) is used here as a system name3 . The last column contains the type of submission. The system of Kilicoglu and Bergler (2010) is the only open submission. They adapted their system introduced in Kilicoglu and Bergler (2008) to the datasets of the shared task. Regarding cross submissions, Zhao et al. (2010) and Ji et al. (2010) managed to achieve a noticeable improvement by exploiting cross-domain 5.4 Data Format Both training and evaluation data were released in a custom XML format. For each task, a separate XML file was made available containing the 3 6 ¨ ur did not publish a description of her system. Ozg¨ Name Georgescul Ji Chen Morante Zhang Zheng T¨ackstr¨om Mamani S´anchez Tang Kilicoglu Tjong Kim Sang Clausen ¨ ur Ozg¨ Zhou Li Prabhakaran Ji P/R/F 72.0 / 51.7 / 60.2 62.7 / 55.3 / 58.7 68.0 / 49.7 / 57.4 80.6 / 44.5 / 57.3 76.6 / 44.4 / 56.2 76.3 / 43.6 / 55.5 78.3 / 42.8 / 55.4 68.3 / 46.2 / 55.1 82.3 / 41.4"
W10-3001,P07-1125,0,0.698493,"ll. The term hedging was originally introduced by Lakoff (1972). However, hedge detection has received considerable interest just recently in the NLP community. Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing un"
W10-3001,W09-1304,0,0.813023,"d features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of clinical free-texts, biological texts from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a su¨ ur pervised sequence labeling approach while Ozg¨ and Radev (2009) developed a rule-based system that exploits syntactic patterns. Several related works have also been published within the framework of The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), where a separate subtask was dedicated to predicting whether the recognized biological events are under negation or speculation, based on the GENIA event corpus annotations (Kilicoglu and Bergler, 2009; Van Landeghem et al., 2009). 3 3.1 Hedges in Biological Scientific Articles In the bi"
W10-3001,W10-3006,0,0.538411,"Missing"
W10-3001,W08-0607,0,0.827806,"cted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of clinical free-texts, biological texts from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a"
W10-3001,D09-1145,0,0.737415,"Missing"
W10-3001,W09-1418,0,0.0379406,"rom full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a su¨ ur pervised sequence labeling approach while Ozg¨ and Radev (2009) developed a rule-based system that exploits syntactic patterns. Several related works have also been published within the framework of The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), where a separate subtask was dedicated to predicting whether the recognized biological events are under negation or speculation, based on the GENIA event corpus annotations (Kilicoglu and Bergler, 2009; Van Landeghem et al., 2009). 3 3.1 Hedges in Biological Scientific Articles In the biomedical domain, sentences were manually annotated for both hedge cues and their linguistic scope. Hedging is typically expressed by using specific linguistic devices (which we refer to as cues in this article) that modify the meaning or reflect the author’s attitude towards the content of the text. Typical hedge cues fall into the following categories: • auxiliaries: may, might, can, would, should, could, etc. • verbs of hedging or verbs with speculative content: suggest, question, presume, suspect, indicat"
W10-3001,P08-1033,1,0.708595,"o identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009). The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora. Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues. Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of c"
W10-3001,W10-3010,0,0.200503,"tral database) and 1 million paragraphs from Wikipedia were offered to the participants as well. These datasets did not contain any manual annotation for uncertainty, but their usage permitted data sampling from a large pool of in-domain texts without time-wasting pre-processing tasks (cleaning and sentence splitting). Tables 1, 2 and 3 contain the results of the submitted systems for Task1 and Task2. The last name of the first author of the system description paper (published in these proceedings) is used here as a system name3 . The last column contains the type of submission. The system of Kilicoglu and Bergler (2010) is the only open submission. They adapted their system introduced in Kilicoglu and Bergler (2008) to the datasets of the shared task. Regarding cross submissions, Zhao et al. (2010) and Ji et al. (2010) managed to achieve a noticeable improvement by exploiting cross-domain 5.4 Data Format Both training and evaluation data were released in a custom XML format. For each task, a separate XML file was made available containing the 3 6 ¨ ur did not publish a description of her system. Ozg¨ Name Georgescul Ji Chen Morante Zhang Zheng T¨ackstr¨om Mamani S´anchez Tang Kilicoglu Tjong Kim Sang Clausen"
W10-3001,W10-3002,0,0.549474,"Missing"
W10-3001,W09-1401,0,0.0810938,"c patterns. The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope. It consists of clinical free-texts, biological texts from full papers and scientific abstracts. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a su¨ ur pervised sequence labeling approach while Ozg¨ and Radev (2009) developed a rule-based system that exploits syntactic patterns. Several related works have also been published within the framework of The BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009), where a separate subtask was dedicated to predicting whether the recognized biological events are under negation or speculation, based on the GENIA event corpus annotations (Kilicoglu and Bergler, 2009; Van Landeghem et al., 2009). 3 3.1 Hedges in Biological Scientific Articles In the biomedical domain, sentences were manually annotated for both hedge cues and their linguistic scope. Hedging is typically expressed by using specific linguistic devices (which we refer to as cues in this article) that modify the meaning or reflect the author’s attitude towards the content of the text. Typical h"
W10-3001,W09-1419,0,0.0654244,"Missing"
W10-3001,W08-0606,1,0.625107,"Missing"
W10-3001,W10-3013,0,0.0292459,"at biological sentences have relatively simple patterns. Thus the context of the cue words (token classification-based approaches used features derived from a window of the token in question, thus, they exploited the relationship among the tokens and their contexts) can be utilized while Wikipedia weasels have a diverse nature. Another observation is that the top systems in both Task1B and Task1W are the ones which did not derive features from syntactic parsing. Each Task2 system was built upon a Task1 system, i.e. they attempted to recognize the scopes for the predicted cue phrases (however, Zhang et al. (2010) have argued that the objective functions of Task1 and Task2 cue detection problems are different because of sentences containing multiple hedge spans). Most systems regarded multiple cues in a sentence to be independent from each other and formed different classification instances from them. There were three systems which incorporated information about other hedge cues (e.g. their distance) of the sentence into the feature space and Zhang et al. (2010) constructed a cascade system which utilized directly the predicted scopes (it processes cue phrases from left to right) during predicting othe"
W10-3001,W10-3014,0,0.00980276,"data sampling from a large pool of in-domain texts without time-wasting pre-processing tasks (cleaning and sentence splitting). Tables 1, 2 and 3 contain the results of the submitted systems for Task1 and Task2. The last name of the first author of the system description paper (published in these proceedings) is used here as a system name3 . The last column contains the type of submission. The system of Kilicoglu and Bergler (2010) is the only open submission. They adapted their system introduced in Kilicoglu and Bergler (2008) to the datasets of the shared task. Regarding cross submissions, Zhao et al. (2010) and Ji et al. (2010) managed to achieve a noticeable improvement by exploiting cross-domain 5.4 Data Format Both training and evaluation data were released in a custom XML format. For each task, a separate XML file was made available containing the 3 6 ¨ ur did not publish a description of her system. Ozg¨ Name Georgescul Ji Chen Morante Zhang Zheng T¨ackstr¨om Mamani S´anchez Tang Kilicoglu Tjong Kim Sang Clausen ¨ ur Ozg¨ Zhou Li Prabhakaran Ji P/R/F 72.0 / 51.7 / 60.2 62.7 / 55.3 / 58.7 68.0 / 49.7 / 57.4 80.6 / 44.5 / 57.3 76.6 / 44.4 / 56.2 76.3 / 43.6 / 55.5 78.3 / 42.8 / 55.4 68.3 / 46"
W10-3001,W10-3015,0,0.0866111,"Missing"
W10-3001,W09-1324,0,\N,Missing
W10-3105,W10-3001,1,0.894242,"Missing"
W10-3105,P09-2044,0,0.0680988,"g uncertain or negated information differently from factual information that is why the development of hedge and negation detection systems has received much interest – e.g. the objective of the CoNLL2010 Shared Task was also to develop hedge detection systems (Farkas et al., 2010). For the training and evaluation of such systems, corpora annotated for negation and speculation are necessary. There are several linguistic phenomena that can be grouped under the term uncertainty. Besides hedge and speculation, doubtful events are also considered as a subtype of uncertainty (Kim et al., 2008) and Ganter and Strube (2009) argue that the notion of weasel words are similar to hedges. A word is considered to be a weasel word if it creates an impression that something important has been said, but what is really communicated is vague, misleading, evasive or ambiguous, thus, it is also related to uncertainty. All these phenomena might be of interest for IE applications, which yields that the creation of corpora with uncertainty annotation is indispensable. 2 Related work There exist some corpora that contain annotation for speculation and/or negation. The GENIA Event corpus (Kim et al., 2008) annotates biological ev"
W10-3105,P07-1125,0,0.0885699,"d, but what is really communicated is vague, misleading, evasive or ambiguous, thus, it is also related to uncertainty. All these phenomena might be of interest for IE applications, which yields that the creation of corpora with uncertainty annotation is indispensable. 2 Related work There exist some corpora that contain annotation for speculation and/or negation. The GENIA Event corpus (Kim et al., 2008) annotates biological events with negation and two types of uncertainty. In the BioInfer corpus (Pyysalo et al., 2007) biological relations are annotated for negation. The system developed by Medlock and Briscoe (2007) made use of a corpus consisting of six papers from genomics literature in which sentences were annotated for speculation. Settles et al. (2008) constructed a corpus where sen28 tences are classified as either speculative or definite, however, no keywords are marked in the corpus and Shatkay et al. (2008) describe a database where sentences are annotated for certainty among other features. As a corpus specifically annotated for weasel words, WikiWeasel should be mentioned, which was constructed for the CoNLL2010 Shared Task (Farkas et al., 2010) and contains Wikipedia paragraphs annotated for"
W10-3105,W08-0606,1,0.912003,"Missing"
W11-0817,C10-1002,0,0.166615,"Missing"
W11-0817,W07-1101,0,0.26871,"e. a combination of a light verb and a noun, a verb or an adjective) in a Hindi–English parallel corpus by identifying a mismatch of the Hindi light verb meaning in the aligned English sentence. Van de Cruys and Moir´on (2007) describe a semantic-based method for identifying verb-preposition-noun combinations in Dutch, which relies on selectional preferences for both the noun and the verb. Cook et al. (2007) differentiate between literal and idiomatic usages of 117 verb and noun constructions in English. They make use of syntactic fixedness of idioms when developing their unsupervised method. Bannard (2007) also seeks to identify verb and noun constructions in English on the basis of syntactic fixedness. Samardˇzi´c and Merlo (2010) analyze English and German light verb constructions in parallel corpora. They found that linguistic features (i.e. the degree of compositionality) and the frequency of the construction both have an effect on aligning the constructions. 3 Experiments In order to identify multiword expressions, simple methods are worth examining, which can serve as a basis for implementing more complex systems and can be used as features in machine learning settings. Our aim being to c"
W11-0817,P10-2020,0,0.0891068,"Missing"
W11-0817,calzolari-etal-2002-towards,0,0.653385,"Missing"
W11-0817,W09-2901,0,0.175829,"Missing"
W11-0817,W07-1106,0,0.239131,"in machine learning settings. Our aim being to compare the effect of different methods on the identification of noun compounds and light verb constructions, we considered it important to develop methods for both MWE types that make use of their characteristics and to adapt those methods to the other type of MWE – in this way, the efficacy and the MWE-(in)dependence of the methods can be empirically evaluated, which can later have impact on developing statistical MWEdetectors. Earlier studies on the detection of light verb constructions generally take syntactic information as a starting point (Cook et al., 2007; Bannard, 2007; Tan et al., 2006), that is, their goal is to classify verb + object constructions selected on the basis of syntactic pattern as literal or idiomatic. However, we do not aim at classifying LVC candidates filtered by syntactic patterns but at identifying them in running text without assuming that syntactic information is necessarily available. In our investigations, we will pay distinctive attention to the added value of syntactic features on the system’s performance. 3.1 Methods for MWE identification For identifying noun compounds, we made use of a list constructed from the En"
W11-0817,E03-1080,0,0.454283,"Missing"
W11-0817,P03-1054,0,0.0155667,"onstructions, the nominal component is typically one that is derived from a verbal stem (make a decision) or coincides with a verb (have a walk). In this case, we accepted 1 as listed at http://en.wikipedia.org/wiki/ Most_common_words_in_English 118 only candidates that had the nominal component / the last noun whose stem was of verbal nature, i.e. coincided with a stem of a verb. Syntactic information can also be exploited in identifying MWEs. Typically, the syntactic relation between the verb and the nominal component in a light verb construction is dobj or prep – using Stanford parser (Klein and Manning, 2003)). The relation between the members of a typical noun compound is nn or amod in attributive constructions. The ‘Syntax’ method accepts candidates among whose members these syntactic relations hold. We also combined the above methods to identify noun compounds and light verb constructions in our databases (the union of candidates yielded by the methods is denoted by ∪ while the intersection is denoted by ∩ in the respective tables). 3.2 Results For the evaluation of our models, we developed a corpus of 50 Wikipedia articles, in which several types of multiword expressions (including nominal com"
W11-0817,W03-1807,0,0.32469,"re syntactically flexible, that is, they can manifest in various forms: the verb can be inflected, the noun can occur in its plural form and the noun can be modified. The nominal and the verbal component may not even be adjacent in e.g. passive sentences. Our goal being to compare how different approaches perform in the case of the different types of multiword expressions, we have chosen these two types of MWEs that are dissimilar in several aspects. 2 Related work There are several applications developed for identifying MWEs, which can be classified according to the methods they make use of (Piao et al., 2003). First, statistical models rely on word frequencies, co-occurrence data and contextual information in deciding whether a bigram or trigram (or even an n-gram) of words can be labeled as a multiword expression or not. Such systems are used for several 116 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 116–121, c Portland, Oregon, USA, 23 June 2011. 2011 Association for Computational Linguistics languages and several types of multiword expressions, see e.g. Bouma (2010). The advantage of statistical systems is that they can"
W11-0817,C10-3015,0,0.264981,"Missing"
W11-0817,C10-2120,0,0.0892639,"Missing"
W11-0817,W10-2108,0,0.112477,"Missing"
W11-0817,W09-2906,0,0.221923,"Missing"
W11-0817,W06-2407,0,0.548578,"aim being to compare the effect of different methods on the identification of noun compounds and light verb constructions, we considered it important to develop methods for both MWE types that make use of their characteristics and to adapt those methods to the other type of MWE – in this way, the efficacy and the MWE-(in)dependence of the methods can be empirically evaluated, which can later have impact on developing statistical MWEdetectors. Earlier studies on the detection of light verb constructions generally take syntactic information as a starting point (Cook et al., 2007; Bannard, 2007; Tan et al., 2006), that is, their goal is to classify verb + object constructions selected on the basis of syntactic pattern as literal or idiomatic. However, we do not aim at classifying LVC candidates filtered by syntactic patterns but at identifying them in running text without assuming that syntactic information is necessarily available. In our investigations, we will pay distinctive attention to the added value of syntactic features on the system’s performance. 3.1 Methods for MWE identification For identifying noun compounds, we made use of a list constructed from the English Wikipedia. Lowercase n-grams"
W11-0817,W00-1308,0,0.600838,"Missing"
W11-0817,W07-1104,0,0.148026,"Missing"
W11-0817,W09-2904,0,0.232319,"Missing"
W12-3715,I11-1130,1,0.891904,"ods have traditionally been applied. In the case of exact match, the gold standard keywords must be in perfect overlap with the extracted keywords (Witten et al., 1999; Frank et al., 1999) – also followed in the SemEval-2010 task on keyphrase extraction (Kim et al., 2010), while in other cases, approximate matches or semantically similar keyphrases are also accepted (Zesch and Gurevych, 2009; Medelyan et al., 2009). In this work we applied the former approach for the evaluation of opinion phrases and made a thorough comparison with the human judgement. Here, we use the framework introduced in Berend (2011) and conducted further experiments based on it to point out the characteristics of the evaluation of opinionated keyphrase extraction. Here we pinpoint the severe differences in performance measures when the output is evaluated by humans compared to strict exact match principles and also examine the benefit of hand-annotated corpus as opposed 99 Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 99–103, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics to an automatically crawled one. In addition, the"
W12-3715,baccianella-etal-2010-sentiwordnet,0,0.0118246,"ne. In addition, the extent to which original author keyphrases resemble those of independent readers’ is also investigated in this paper. 3 Methodology In our experiments, we used the methodology described in Berend (2011) to extract opinionated keyphrase candidates from the reviews. The system treats it as a supervised classification task using Maximum Entropy classifier, in which certain n-grams of the product reviews are treated as classification instances and the task is to classify them as proper or improper ones. It incorporates a rich feature set, relying on the usage of SentiWordNet (Esuli et al., 2010) and further orthological, morphological and syntactic features. Next, we present the difficulties of opinionated keyphrase extraction and offer our solutions to the emerging problems. 3.1 Author keyphrases Auth. Ann1 Ann2 Ann3 100 Ann1 0.415 – 0.702 0.690 Ann2 0.324 0.679 – 0.688 Ann3 0.396 0.708 0.713 – Table 1: Inter-annotator agreement among the author’s and annotators’ sets of opinion phrases. Elements above and under the main diagonal refer to the agreement rates in Dice coefficient for pro and con phrases, respectively. lent, yet much simpler forms, e.g. instead of ‘even I found the pho"
W12-3715,C10-2057,0,0.0237927,"the evaluation of subjective information mining through the example of assigning opinionated keyphrases to product reviews and compare the results of human- and machine-based evaluation on finding opinionated keyphrases. Related Work As the task we aim at involves extracting keyphrases that are responsible for the author’s opinion toward the product, aspects of both keyphrase extraction and opinion mining determine our methodology and evaluation procedure. There are several sentiment analysis approaches that make use of manually annotated review datasets (Zhuang et al., 2006; Li et al., 2010; Jang and Shin, 2010) and Wei and Gulla (2010) constructed a sentiment ontology tree in which attributes of the product and sentiments were paired. For evaluating scientific keyphrase extraction, several methods have traditionally been applied. In the case of exact match, the gold standard keywords must be in perfect overlap with the extracted keywords (Witten et al., 1999; Frank et al., 1999) – also followed in the SemEval-2010 task on keyphrase extraction (Kim et al., 2010), while in other cases, approximate matches or semantically similar keyphrases are also accepted (Zesch and Gurevych, 2009; Medelyan et al.,"
W12-3715,S10-1004,0,0.151377,"rsing, the level of ambiguity and subjectivity is essentially lower than for higher-level tasks such as question answering or machine translation, it is still an open question to find a satisfactory solution for the (automatic) evaluation of certain tasks. Here we present the difficulties of finding an appropriate way of evaluating a highly semantics- and subjectivity-related task, namely opinionated keyphrase extraction. There has been a growing interest in the NLP treatment of subjectivity and sentiment analysis – see e.g. Balahur et al. (2011) – on the one hand and on keyphrase extraction (Kim et al., 2010) on the other hand. The tasks themselves are demanding for automatic systems due to the variety of the linguistic ways people can express the same linguistic content. Here we focus on the evaluation of subjective information mining through the example of assigning opinionated keyphrases to product reviews and compare the results of human- and machine-based evaluation on finding opinionated keyphrases. Related Work As the task we aim at involves extracting keyphrases that are responsible for the author’s opinion toward the product, aspects of both keyphrase extraction and opinion mining determi"
W12-3715,C10-1074,0,0.0221235,"n mining through the example of assigning opinionated keyphrases to product reviews and compare the results of human- and machine-based evaluation on finding opinionated keyphrases. Related Work As the task we aim at involves extracting keyphrases that are responsible for the author’s opinion toward the product, aspects of both keyphrase extraction and opinion mining determine our methodology and evaluation procedure. There are several sentiment analysis approaches that make use of manually annotated review datasets (Zhuang et al., 2006; Li et al., 2010; Jang and Shin, 2010) and Wei and Gulla (2010) constructed a sentiment ontology tree in which attributes of the product and sentiments were paired. For evaluating scientific keyphrase extraction, several methods have traditionally been applied. In the case of exact match, the gold standard keywords must be in perfect overlap with the extracted keywords (Witten et al., 1999; Frank et al., 1999) – also followed in the SemEval-2010 task on keyphrase extraction (Kim et al., 2010), while in other cases, approximate matches or semantically similar keyphrases are also accepted (Zesch and Gurevych, 2009; Medelyan et al., 2009). In this work we ap"
W12-3715,W04-1013,0,0.00643372,"bigger number of keyphrases is to be identified, however, with a bigger number of gold standard keywords it is more probable that the automatic keywords occur among them. At the same time having a larger set of gold standard tags might affect the recall negatively since there are more keyphrases to return. On the other hand, in the case of intersection it can be measured whether the most important features (i.e. those that every annotator felt relevant) can be extracted from the text. Note that our strategy is similar to the one applied in the case of BLEU/ROUGE score (Papineni et al., 2002; Lin, 2004) with respect to the fact that multiple good solutions are taken into account whereas the application of union and intersection is determined by the nature of the task: different annotators may attach several outputs (in other words, different numbers of keyphrases) to the same document in the case of keyphrase extraction, which is not realistic in the case of machine translation or summarization (only one output is offered for each sentence / text). 3.4 Results In our experiments, we used the opinion phrase extraction system based on the paper of Berend (2011). Results vary whether the manual"
W12-3715,D09-1137,0,0.0351534,"g and Shin, 2010) and Wei and Gulla (2010) constructed a sentiment ontology tree in which attributes of the product and sentiments were paired. For evaluating scientific keyphrase extraction, several methods have traditionally been applied. In the case of exact match, the gold standard keywords must be in perfect overlap with the extracted keywords (Witten et al., 1999; Frank et al., 1999) – also followed in the SemEval-2010 task on keyphrase extraction (Kim et al., 2010), while in other cases, approximate matches or semantically similar keyphrases are also accepted (Zesch and Gurevych, 2009; Medelyan et al., 2009). In this work we applied the former approach for the evaluation of opinion phrases and made a thorough comparison with the human judgement. Here, we use the framework introduced in Berend (2011) and conducted further experiments based on it to point out the characteristics of the evaluation of opinionated keyphrase extraction. Here we pinpoint the severe differences in performance measures when the output is evaluated by humans compared to strict exact match principles and also examine the benefit of hand-annotated corpus as opposed 99 Proceedings of the 3rd Workshop on Computational Approach"
W12-3715,P02-1040,0,0.0884331,"nnotations means that a bigger number of keyphrases is to be identified, however, with a bigger number of gold standard keywords it is more probable that the automatic keywords occur among them. At the same time having a larger set of gold standard tags might affect the recall negatively since there are more keyphrases to return. On the other hand, in the case of intersection it can be measured whether the most important features (i.e. those that every annotator felt relevant) can be extracted from the text. Note that our strategy is similar to the one applied in the case of BLEU/ROUGE score (Papineni et al., 2002; Lin, 2004) with respect to the fact that multiple good solutions are taken into account whereas the application of union and intersection is determined by the nature of the task: different annotators may attach several outputs (in other words, different numbers of keyphrases) to the same document in the case of keyphrase extraction, which is not realistic in the case of machine translation or summarization (only one output is offered for each sentence / text). 3.4 Results In our experiments, we used the opinion phrase extraction system based on the paper of Berend (2011). Results vary whethe"
W12-3715,P10-1042,0,0.0225346,"ive information mining through the example of assigning opinionated keyphrases to product reviews and compare the results of human- and machine-based evaluation on finding opinionated keyphrases. Related Work As the task we aim at involves extracting keyphrases that are responsible for the author’s opinion toward the product, aspects of both keyphrase extraction and opinion mining determine our methodology and evaluation procedure. There are several sentiment analysis approaches that make use of manually annotated review datasets (Zhuang et al., 2006; Li et al., 2010; Jang and Shin, 2010) and Wei and Gulla (2010) constructed a sentiment ontology tree in which attributes of the product and sentiments were paired. For evaluating scientific keyphrase extraction, several methods have traditionally been applied. In the case of exact match, the gold standard keywords must be in perfect overlap with the extracted keywords (Witten et al., 1999; Frank et al., 1999) – also followed in the SemEval-2010 task on keyphrase extraction (Kim et al., 2010), while in other cases, approximate matches or semantically similar keyphrases are also accepted (Zesch and Gurevych, 2009; Medelyan et al., 2009). In this work we ap"
W12-3715,R09-1086,0,0.0285638,"2006; Li et al., 2010; Jang and Shin, 2010) and Wei and Gulla (2010) constructed a sentiment ontology tree in which attributes of the product and sentiments were paired. For evaluating scientific keyphrase extraction, several methods have traditionally been applied. In the case of exact match, the gold standard keywords must be in perfect overlap with the extracted keywords (Witten et al., 1999; Frank et al., 1999) – also followed in the SemEval-2010 task on keyphrase extraction (Kim et al., 2010), while in other cases, approximate matches or semantically similar keyphrases are also accepted (Zesch and Gurevych, 2009; Medelyan et al., 2009). In this work we applied the former approach for the evaluation of opinion phrases and made a thorough comparison with the human judgement. Here, we use the framework introduced in Berend (2011) and conducted further experiments based on it to point out the characteristics of the evaluation of opinionated keyphrase extraction. Here we pinpoint the severe differences in performance measures when the output is evaluated by humans compared to strict exact match principles and also examine the benefit of hand-annotated corpus as opposed 99 Proceedings of the 3rd Workshop o"
W13-3608,W02-1503,0,0.0851341,"Missing"
W13-3608,W13-1703,0,0.0939194,"egories. We focused on the two “nominal” error categories: We introduce here a participating system of the CoNLL-2013 Shared Task “Grammatical Error Correction”. We focused on the noun number and article error categories and constructed a supervised learning system for solving these tasks. We carried out feature engineering and we found that (among others) the f-structure of an LFG parser can provide very informative features for the machine learning system. 1 Introduction The CoNLL-2013 Shared Task aimed at identifying and correcting grammatical errors in the NUCLE learner corpus of English (Dahlmeier et al., 2013). This task has become popular in the natural language processing (NLP) community in the last few years (Dale and Kilgariff, 2010), which manifested in the organization of shared tasks. In 2011, the task Helping Our Own (HOO 2011) was held (Dale and Kilgariff, 2011), which targeted the promotion of NLP tools and techniques in improving the textual quality of papers written by non-native speakers of English within the field of NLP. The next year, HOO 2012 (Dale et al., 2012) specifically focused on the correction of determiner and preposition errors in a collection of essays written by candidat"
W13-3608,W10-4236,0,0.0258517,"k “Grammatical Error Correction”. We focused on the noun number and article error categories and constructed a supervised learning system for solving these tasks. We carried out feature engineering and we found that (among others) the f-structure of an LFG parser can provide very informative features for the machine learning system. 1 Introduction The CoNLL-2013 Shared Task aimed at identifying and correcting grammatical errors in the NUCLE learner corpus of English (Dahlmeier et al., 2013). This task has become popular in the natural language processing (NLP) community in the last few years (Dale and Kilgariff, 2010), which manifested in the organization of shared tasks. In 2011, the task Helping Our Own (HOO 2011) was held (Dale and Kilgariff, 2011), which targeted the promotion of NLP tools and techniques in improving the textual quality of papers written by non-native speakers of English within the field of NLP. The next year, HOO 2012 (Dale et al., 2012) specifically focused on the correction of determiner and preposition errors in a collection of essays written by candidates sitting for the Cambridge ESOL First Certificate in English (FCE) examination. In 2013, the CoNLL-2013 Shared Task has continue"
W13-3608,W11-2838,0,0.0446482,"Missing"
W13-3608,W12-2006,0,0.0626592,"oNLL-2013 Shared Task aimed at identifying and correcting grammatical errors in the NUCLE learner corpus of English (Dahlmeier et al., 2013). This task has become popular in the natural language processing (NLP) community in the last few years (Dale and Kilgariff, 2010), which manifested in the organization of shared tasks. In 2011, the task Helping Our Own (HOO 2011) was held (Dale and Kilgariff, 2011), which targeted the promotion of NLP tools and techniques in improving the textual quality of papers written by non-native speakers of English within the field of NLP. The next year, HOO 2012 (Dale et al., 2012) specifically focused on the correction of determiner and preposition errors in a collection of essays written by candidates sitting for the Cambridge ESOL First Certificate in English (FCE) examination. In 2013, the CoNLL-2013 Shared Task has continued this direction of research. The CoNLL-2013 Shared Task is based on the NUCLE corpus, which consists of about 1,400 1.1 Article and Determiner Errors This error type involved all kinds of errors which were related to determiners and articles (ArtOrDet). It required multiple correction strategies. On the one hand, superfluous articles or determin"
W13-3608,C08-1022,0,0.0640972,"Missing"
W13-3608,P02-1035,0,0.0477098,"grammatical errors in the sentence result in unusual constituency subtree patterns that could manifest in minimal governing phrases having too long spans for instance. The relative position of the candidate position inside the smallest dominating noun and prepositional phrases was also incorporated as a feature since this information might carry some information for noun errors. (ii) an Optimality Theory-style constraint mechanism for filtering and ranking competing analyses (Frank et al., 2001), and (iii) a stochastic disambiguation component which is based on a log-linear probability model (Riezler et al., 2002) and works on the packed representations. Although we use a deep, hand-crafted LFG grammar for processing the data, our approach is substantially different from other grammar-based approaches to CALL. For instance, Fortmann and Forst (2004) supplement a German LFG developed for newspaper text with so-called malrules that accept marked or ungrammatical input of some predefined types. In our work, we apply an LFG parser developed for standard texts to get a rich feature representation that can be exploited by a classifier. While malrules would certainly be useful for finding other error types, s"
W13-3608,W08-1705,0,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W14-0803,W02-2001,0,0.2008,"h Workshop on Multiword Expressions (MWE 2014), pages 17–25, c Gothenburg, Sweden, 26-27 April 2014. 2014 Association for Computational Linguistics root det nsubj prt punct dobj det The hitman did in the president . Figure 1: A dependency parse of the sentence “The hitman did in the president”. method is based on a rich feature set with new features like semantic or contextual features. We compare the performance of the parsers with that of our approach and we discuss the reasons for any possible differences. 2 Related Work Recently, some studies have attempted to identify VPCs. For instance, Baldwin and Villavicencio (2002) detected verb-particle constructions in raw texts with the help of information based on POS-tagging and chunking, and they also made use of frequency and lexical information in their classifier. Kim and Baldwin (2006) built their system on semantic information when deciding whether verb-preposition pairs were verb-particle constructions or not. Nagy T. and Vincze (2011) implemented a rule-based system based on morphological features to detect VPCs in raw texts. The (non-)compositionality of verb-particle combinations has also raised interest among researchers. McCarthy et al. (2003) implement"
W14-0803,C10-1011,0,0.127136,"Missing"
W14-0803,J13-1009,0,0.171466,"Missing"
W14-0803,W06-2110,0,0.346274,"dependency parse of the sentence “The hitman did in the president”. method is based on a rich feature set with new features like semantic or contextual features. We compare the performance of the parsers with that of our approach and we discuss the reasons for any possible differences. 2 Related Work Recently, some studies have attempted to identify VPCs. For instance, Baldwin and Villavicencio (2002) detected verb-particle constructions in raw texts with the help of information based on POS-tagging and chunking, and they also made use of frequency and lexical information in their classifier. Kim and Baldwin (2006) built their system on semantic information when deciding whether verb-preposition pairs were verb-particle constructions or not. Nagy T. and Vincze (2011) implemented a rule-based system based on morphological features to detect VPCs in raw texts. The (non-)compositionality of verb-particle combinations has also raised interest among researchers. McCarthy et al. (2003) implemented a method to determine the compositionality of VPCs and Baldwin (2005) presented a dataset in which non-compositional VPCs could be found. Villavicencio (2003) proposed some methods to extend the coverage of availabl"
W14-0803,P03-1054,0,0.0182859,"Missing"
W14-0803,J93-2004,0,0.0459395,"the first sentence where it functions as a VPC and in the second case, it is a simple verbprepositional phrase combination. For these reasons, VPCs are of great interest for natural language processing applications like machine translation or information extraction, where it is necessary to grab the meaning of the text. The special relation of the verb and particle within a VPC is often distinctively marked at several annotation layers in treebanks. For instance, in the Penn Treebank, the particle is assigned a specific part of speech tag (RP) and it also has a specific syntactic label (PRT) (Marcus et al., 1993), see also Figure 1. This entails that if a datadriven morphological parser or a syntactic parser is trained on a dataset annotated with extra information for VPCs, it will be able to assign these kind of tags as well. In other words, the morphological/syntactic parser itself will be able to identify VPCs in texts. In this paper, we seek to identify VPCs on the basis of syntactic information. We first examine how syntactic parsers perform on Wiki50 (Vincze et al., 2011), a dataset manually annotated for different types of MWEs, including VPCs. We then present our syntax-based tool called VPCTa"
W14-0803,W03-1810,0,0.126451,"Baldwin and Villavicencio (2002) detected verb-particle constructions in raw texts with the help of information based on POS-tagging and chunking, and they also made use of frequency and lexical information in their classifier. Kim and Baldwin (2006) built their system on semantic information when deciding whether verb-preposition pairs were verb-particle constructions or not. Nagy T. and Vincze (2011) implemented a rule-based system based on morphological features to detect VPCs in raw texts. The (non-)compositionality of verb-particle combinations has also raised interest among researchers. McCarthy et al. (2003) implemented a method to determine the compositionality of VPCs and Baldwin (2005) presented a dataset in which non-compositional VPCs could be found. Villavicencio (2003) proposed some methods to extend the coverage of available VPC resources. Tu and Roth (2012) distinguished genuine VPCs and verb-preposition combinations in context. They built a crowdsourced corpus of VPC candidates in context, where each candidate was manually classified as a VPC or not. However, during corpus building, they applied lexical restrictions and concentrated only on VPCs formed with six verbs. Their SVM-based al"
W14-0803,S12-1010,0,0.280352,"em on semantic information when deciding whether verb-preposition pairs were verb-particle constructions or not. Nagy T. and Vincze (2011) implemented a rule-based system based on morphological features to detect VPCs in raw texts. The (non-)compositionality of verb-particle combinations has also raised interest among researchers. McCarthy et al. (2003) implemented a method to determine the compositionality of VPCs and Baldwin (2005) presented a dataset in which non-compositional VPCs could be found. Villavicencio (2003) proposed some methods to extend the coverage of available VPC resources. Tu and Roth (2012) distinguished genuine VPCs and verb-preposition combinations in context. They built a crowdsourced corpus of VPC candidates in context, where each candidate was manually classified as a VPC or not. However, during corpus building, they applied lexical restrictions and concentrated only on VPCs formed with six verbs. Their SVM-based algorithm used syntactic and lexical features to classify VPCs candidates and they concluded that their system achieved good results on idiomatic VPCs, but the classification of more compositional VPCs is more challenging. Since in this paper we focus on syntax-bas"
W14-0803,W03-1808,0,0.0785269,"Missing"
W14-0803,I13-1024,1,0.765586,"hey concluded that their system achieved good results on idiomatic VPCs, but the classification of more compositional VPCs is more challenging. Since in this paper we focus on syntax-based 18 VPC identification more precisely, we also identify VPCs with syntactic parsers, it seems necessary to mention studies that experimented with parsers for identifying different types of MWEs. For instance, constituency parsing models were employed in identifying contiguous MWEs in French and Arabic (Green et al., 2013). Their method relied on a syntactic treebank, an MWE list and a morphological analyzer. Vincze et al. (2013) employed a dependency parser for identifying light verb constructions in Hungarian texts as a “side effect” of parsing sentences and report state-of-the-art results for this task. Here, we make use of parsers trained on the Penn Treebank (which contains annotation for VPCs) and we evaluate their performance on the Wiki50 corpus, which was manually annotated for VPCs. Thus, we first examine how well these parsers identify VPCs (i.e. assigning VPC-specific syntactic labels) and then we present how VPCTagger can carry out this task. First, we select VPC candidates from raw text and then, we clas"
W14-0803,R11-1040,1,\N,Missing
W14-4909,I13-1111,0,0.0185569,"nd more important in people’s lives, the huge amount of data available from this domain is a valuable source of information for computation linguistics. However, processing texts from the web – especially social media texts from blogs, status updates, chat logs and comments – revealed that they are very challenging for applications trained on standard texts. Most studies in this area focus on English, for instance, sentiment analysis from tweets has been the focus of recent challenges (Wilson et al., 2013) and Facebook posts have been analysed from the perspective of computational psychology (Celli et al., 2013). A syntactically This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 64 LAW VIII - The 8th Linguistic Annotation Workshop, pages 64–69, Dublin, Ireland, August 23-24 2014. annotated treebank of webtext has been also created for English (Bies et al., 2012). However, methods developed for processing English webtext require serious alterations to be applicable to other languages, for example Hungarian, which is very different from English"
W14-4909,R13-2007,0,0.175837,"Missing"
W14-4909,W10-3001,1,0.941197,"Missing"
W14-4909,P09-2044,0,0.616938,"eder, 2012). Several manually annotated corpora have been created, which serve as training and test databases of state-of-the-art uncertainty detectors based on supervised machine learning techniques. Most of these corpora are constructed for English, however, their domains and genres are diverse: biological texts (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), clinical texts (Uzuner et al., 2009), pieces of news (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia texts (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012; Vincze, 2013), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and tweets (Wei et al., 2013) have been annotated for uncertainty, just to mention a few examples. The diversity of the resources also manifests in the fact that the annotation principles behind the corpora might slightly differ, which led Szarvas et al. (2012) to compare the annotation schemes of three corpora (BioScope, FactBank and WikiWeasel) and they offered a unified classification of semantic uncertainty phenomena, on the basis of which these corpora were reannotated, using"
W14-4909,konstantinova-etal-2012-review,0,0.197507,"Missing"
W14-4909,P07-1125,0,0.071838,"the guidelines to other languages or domains and later on, in the construction of automatic uncertainty detectors. 1 Background Detecting uncertainty in natural language texts has received a considerable amount of attention in the last decade (Farkas et al., 2010; Morante and Sporleder, 2012). Several manually annotated corpora have been created, which serve as training and test databases of state-of-the-art uncertainty detectors based on supervised machine learning techniques. Most of these corpora are constructed for English, however, their domains and genres are diverse: biological texts (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), clinical texts (Uzuner et al., 2009), pieces of news (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia texts (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012; Vincze, 2013), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and tweets (Wei et al., 2013) have been annotated for uncertainty, just to mention a few examples. The diversity of the resources also manifests in the fact that the annotation principles behind the corpo"
W14-4909,J12-2001,0,0.118409,"w how the annotation guidelines originally developed for English standard texts can be adapted to Hungarian webtext. We annotated a small corpus of Facebook posts for uncertainty phenomena and we illustrate the main characteristics of such texts, with special regard to uncertainty annotation. Our results may be exploited in adapting the guidelines to other languages or domains and later on, in the construction of automatic uncertainty detectors. 1 Background Detecting uncertainty in natural language texts has received a considerable amount of attention in the last decade (Farkas et al., 2010; Morante and Sporleder, 2012). Several manually annotated corpora have been created, which serve as training and test databases of state-of-the-art uncertainty detectors based on supervised machine learning techniques. Most of these corpora are constructed for English, however, their domains and genres are diverse: biological texts (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), clinical texts (Uzuner et al., 2009), pieces of news (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia texts (Ganter and"
W14-4909,W10-3112,0,0.361239,"tors. 1 Background Detecting uncertainty in natural language texts has received a considerable amount of attention in the last decade (Farkas et al., 2010; Morante and Sporleder, 2012). Several manually annotated corpora have been created, which serve as training and test databases of state-of-the-art uncertainty detectors based on supervised machine learning techniques. Most of these corpora are constructed for English, however, their domains and genres are diverse: biological texts (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), clinical texts (Uzuner et al., 2009), pieces of news (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia texts (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012; Vincze, 2013), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and tweets (Wei et al., 2013) have been annotated for uncertainty, just to mention a few examples. The diversity of the resources also manifests in the fact that the annotation principles behind the corpora might slightly differ, which led Szarvas et al. (2012) to compare the annotation schemes of three cor"
W14-4909,J12-2004,1,0.908848,"Missing"
W14-4909,W08-0606,1,0.917011,"Missing"
W14-4909,I13-1044,1,0.912792,"hich serve as training and test databases of state-of-the-art uncertainty detectors based on supervised machine learning techniques. Most of these corpora are constructed for English, however, their domains and genres are diverse: biological texts (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), clinical texts (Uzuner et al., 2009), pieces of news (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia texts (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012; Vincze, 2013), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and tweets (Wei et al., 2013) have been annotated for uncertainty, just to mention a few examples. The diversity of the resources also manifests in the fact that the annotation principles behind the corpora might slightly differ, which led Szarvas et al. (2012) to compare the annotation schemes of three corpora (BioScope, FactBank and WikiWeasel) and they offered a unified classification of semantic uncertainty phenomena, on the basis of which these corpora were reannotated, using uniform guidelines. Some other uncertainty-related lingui"
W14-4909,C14-1174,1,0.442629,"not annotated in these cases, as these sentences do not hold uncertain meanings semantically, even if certain facts in them are clearly not true or at least the writers obviously lack evidence to back them up. Example 4: Information without evidence in our corpus. ´ megfigyel´es, hogy az elektronok u´ gy viselkednek, mint az antioxid´ansok. Uj (new observation that the electrons that.way behave as the antioxidants) “It is a new observation that electrons behave as antioxidants.” The uncertainty annotation of this text differed greatly from our corpus of Hungarian Wikipedia articles and news (Vincze, 2014), which domains are much closer to standard language use. Table 1 shows the distribution of the different types of uncertainty cues in these domains. Comparing this new subcorpus with the other two shows certain domain specific characteristics. Unlike Facebook posts and comments, the other two domains should not contain subjective opinions according to the objective nature of news media and encyclopedias. This is consistent with the difference in the proportion of peacock cues in each subcorpus: Facebook posts abound in them but their number is low in the other types of texts. The relatively s"
W14-4909,P13-2011,0,0.23454,"s based on supervised machine learning techniques. Most of these corpora are constructed for English, however, their domains and genres are diverse: biological texts (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), clinical texts (Uzuner et al., 2009), pieces of news (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia texts (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012; Vincze, 2013), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and tweets (Wei et al., 2013) have been annotated for uncertainty, just to mention a few examples. The diversity of the resources also manifests in the fact that the annotation principles behind the corpora might slightly differ, which led Szarvas et al. (2012) to compare the annotation schemes of three corpora (BioScope, FactBank and WikiWeasel) and they offered a unified classification of semantic uncertainty phenomena, on the basis of which these corpora were reannotated, using uniform guidelines. Some other uncertainty-related linguistic phenomena are described as discourse-level uncertainty in Vincze (2013). As a fir"
W14-4909,S13-2052,0,0.098191,"Missing"
W14-4909,R13-1099,1,0.786014,"h the usual difficulties of working with this domain. We annotated Hungarian posts and comments from Facebook, which made the uncertainty annotation more challenging than on standard texts. Texts were randomly selected from the public posts available at the Facebook-sites of some well-known brands (like mobile companies, electronic devices, nutrition expert companies etc.) and from the comments that users made on these posts. For our pilot annotation, we used 1373 sentences and 18,327 tokens (as provided by magyarlanc, a linguistic preprocessing toolkit developed for standard Hungarian texts (Zsibrita et al., 2013)). One fundamental property of social media texts is their similarity to oral communication despite their written form. The communication is online and multimodal; its speed causing a number of possibilities for error. The quick typing makes typos, abbreviations and lack of capitalization, punctuation and accentuated letters more common in these texts. Accentuated and unaccentuated vowels represent different sounds in Hungarian that can change the meaning of words (kerek “round”, ker´ek “wheel” and k´erek “I want”). Other types of linguistic creativity are also common, such as the use of emoti"
W14-4909,bies-etal-2014-incorporating,0,\N,Missing
W16-2115,D07-1013,0,0.0119591,"n Section 6. 2 description ADJ ADP ADV AUX CONJ DET INTJ NOUN NUM PART PRON PROPN PUNCT SCONJ VERB X adjective adposition adverb auxiliary coordinating conjunction determiner interjection noun number particle nominal pronoun proper noun punctuation subordinating conjunction verb other Table 1: POS tags for Old Hungarian. Universal Dependencies and Morphology terlingua for different morphological tagsets and it enables the conversion of different tagsets to the same morphological representation (Zeman, 2008). Rambow et al. (2006) defined a multilingual tagset for POS tagging and parsing, while McDonald and Nivre (2007) identified eight POS tags based on data from the CoNLL-2007 Shared Task (Nivre et al., 2007). Petrov et al. (2012) offered a tagset of 12 POS tags and applied this tagset to 22 languages. Universal Dependencies is an international project that aims at developing a unified annotation scheme for dependency syntax and morphology in a language-independent framework (Nivre, 2015). Currently (as of June 2016), there are annotated datasets available for 45 languages, including modern languages such as English, German, French, Hungarian and Irish, and old languages such as Ancient Greek, Coptic, Lati"
W16-2115,W13-2706,0,0.0501319,"Missing"
W16-2115,petrov-etal-2012-universal,0,0.0324974,"n adverb auxiliary coordinating conjunction determiner interjection noun number particle nominal pronoun proper noun punctuation subordinating conjunction verb other Table 1: POS tags for Old Hungarian. Universal Dependencies and Morphology terlingua for different morphological tagsets and it enables the conversion of different tagsets to the same morphological representation (Zeman, 2008). Rambow et al. (2006) defined a multilingual tagset for POS tagging and parsing, while McDonald and Nivre (2007) identified eight POS tags based on data from the CoNLL-2007 Shared Task (Nivre et al., 2007). Petrov et al. (2012) offered a tagset of 12 POS tags and applied this tagset to 22 languages. Universal Dependencies is an international project that aims at developing a unified annotation scheme for dependency syntax and morphology in a language-independent framework (Nivre, 2015). Currently (as of June 2016), there are annotated datasets available for 45 languages, including modern languages such as English, German, French, Hungarian and Irish, and old languages such as Ancient Greek, Coptic, Latin and Old Church Slavic, among others1 . Datasets from all these languages apply the same tagsets at the morphologi"
W16-2115,rambow-etal-2006-parallel,0,0.0281422,"for Old and Modern Hungarian. Conclusions and the planned future work end the paper in Section 6. 2 description ADJ ADP ADV AUX CONJ DET INTJ NOUN NUM PART PRON PROPN PUNCT SCONJ VERB X adjective adposition adverb auxiliary coordinating conjunction determiner interjection noun number particle nominal pronoun proper noun punctuation subordinating conjunction verb other Table 1: POS tags for Old Hungarian. Universal Dependencies and Morphology terlingua for different morphological tagsets and it enables the conversion of different tagsets to the same morphological representation (Zeman, 2008). Rambow et al. (2006) defined a multilingual tagset for POS tagging and parsing, while McDonald and Nivre (2007) identified eight POS tags based on data from the CoNLL-2007 Shared Task (Nivre et al., 2007). Petrov et al. (2012) offered a tagset of 12 POS tags and applied this tagset to 22 languages. Universal Dependencies is an international project that aims at developing a unified annotation scheme for dependency syntax and morphology in a language-independent framework (Nivre, 2015). Currently (as of June 2016), there are annotated datasets available for 45 languages, including modern languages such as English,"
W16-2115,W05-1106,0,0.105267,"Missing"
W16-2115,zeman-2008-reusable,0,0.0268458,"emes developed for Old and Modern Hungarian. Conclusions and the planned future work end the paper in Section 6. 2 description ADJ ADP ADV AUX CONJ DET INTJ NOUN NUM PART PRON PROPN PUNCT SCONJ VERB X adjective adposition adverb auxiliary coordinating conjunction determiner interjection noun number particle nominal pronoun proper noun punctuation subordinating conjunction verb other Table 1: POS tags for Old Hungarian. Universal Dependencies and Morphology terlingua for different morphological tagsets and it enables the conversion of different tagsets to the same morphological representation (Zeman, 2008). Rambow et al. (2006) defined a multilingual tagset for POS tagging and parsing, while McDonald and Nivre (2007) identified eight POS tags based on data from the CoNLL-2007 Shared Task (Nivre et al., 2007). Petrov et al. (2012) offered a tagset of 12 POS tags and applied this tagset to 22 languages. Universal Dependencies is an international project that aims at developing a unified annotation scheme for dependency syntax and morphology in a language-independent framework (Nivre, 2015). Currently (as of June 2016), there are annotated datasets available for 45 languages, including modern lang"
W16-2115,D07-1096,0,\N,Missing
W16-5002,W07-1011,0,0.0388679,"ine learning can be improved by adding out-domain data to the training. 2 Related Work Uncertainty detection has recently gained popularity in the NLP literature. The CoNLL-2010 Shared Task aimed at detecting uncertainty cues in biological papers and Wikipedia articles written in English (Farkas et al., 2010). More recently, a special issue of the journal Computational Linguistics (Vol. 38, No. 2) was dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). Among the systems for uncertainty detection we can find rule-based ones (Light et al., 2004; Chapman et al., 2007) but also those based on machine learning methods, usually applying a supervised approach. Some of them used token classification (Morante and Daelemans, 2009; S´anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at"
W16-5002,W10-3017,0,0.0165067,"biological papers and Wikipedia articles written in English (Farkas et al., 2010). More recently, a special issue of the journal Computational Linguistics (Vol. 38, No. 2) was dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). Among the systems for uncertainty detection we can find rule-based ones (Light et al., 2004; Chapman et al., 2007) but also those based on machine learning methods, usually applying a supervised approach. Some of them used token classification (Morante and Daelemans, 2009; S´anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al."
W16-5002,R13-2007,0,0.0295201,"Missing"
W16-5002,W10-3001,1,0.918072,"Missing"
W16-5002,W10-3009,0,0.0247739,"Missing"
W16-5002,P09-2044,0,0.166816,"y classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuner et al., 2009), news media (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and social media (Wei et al., 2013). Although most of the earlier studies experimented with indomain data, there are a few approaches that investigated domain differences. For instance, Szarvas et al. (2012) carried out domain adaptation for biological texts, news media and encyclopedia texts and Vincze (2014) experimented with pieces of news and Wikipedia texts. Our system described in this paper is also based on supervised machine learning techniques, namely, sequence labeling. The system rel"
W16-5002,konstantinova-etal-2012-review,0,0.0437829,"Missing"
W16-5002,W10-3011,0,0.0146409,"et al., 2010). More recently, a special issue of the journal Computational Linguistics (Vol. 38, No. 2) was dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). Among the systems for uncertainty detection we can find rule-based ones (Light et al., 2004; Chapman et al., 2007) but also those based on machine learning methods, usually applying a supervised approach. Some of them used token classification (Morante and Daelemans, 2009; S´anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010"
W16-5002,W04-3103,0,0.0589094,"e efficiency of machine learning can be improved by adding out-domain data to the training. 2 Related Work Uncertainty detection has recently gained popularity in the NLP literature. The CoNLL-2010 Shared Task aimed at detecting uncertainty cues in biological papers and Wikipedia articles written in English (Farkas et al., 2010). More recently, a special issue of the journal Computational Linguistics (Vol. 38, No. 2) was dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). Among the systems for uncertainty detection we can find rule-based ones (Light et al., 2004; Chapman et al., 2007) but also those based on machine learning methods, usually applying a supervised approach. Some of them used token classification (Morante and Daelemans, 2009; S´anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are awa"
W16-5002,P07-1125,0,0.0187104,"anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuner et al., 2009), news media (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and social media (Wei et al., 2013). Although most of the earlier studies experimented with indomain data, there are a few approaches that investigated domain differences. For instance, Szarvas et al. (2012) carried out domain adaptat"
W16-5002,W09-1304,0,0.0249795,"iterature. The CoNLL-2010 Shared Task aimed at detecting uncertainty cues in biological papers and Wikipedia articles written in English (Farkas et al., 2010). More recently, a special issue of the journal Computational Linguistics (Vol. 38, No. 2) was dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). Among the systems for uncertainty detection we can find rule-based ones (Light et al., 2004; Chapman et al., 2007) but also those based on machine learning methods, usually applying a supervised approach. Some of them used token classification (Morante and Daelemans, 2009; S´anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biolo"
W16-5002,J12-2001,0,0.104458,"luding lexical, morphological, syntactic, semantic and discourse-based features, and we evaluate our system on a small set of manually annotated social media texts. We also carry out cross-domain and domain adaptation experiments using an annotated corpus of standard Hungarian texts and show that domain differences significantly affect machine learning. Furthermore, we argue that differences among uncertainty cue types may also affect the efficiency of uncertainty detection. 1 Introduction In several fields of natural language processing, the factuality of information plays an important role (Morante and Sporleder, 2012). Factual and non-factual information should be treated separately, more precisely, negated or speculative/uncertain information should not be mixed up with factual information. For instance, search engines should not retrieve documents where the information in question is negated or unreliable. Uncertainty detectors can help select reliable (certain) and unreliable (uncertain) parts of documents. Thus, developing uncertainty detectors is highly desirable for many fields of NLP (Morante and Sporleder, 2012; Farkas et al., 2010). With the advent of Web2.0, many social media platforms have becom"
W16-5002,W10-3112,0,0.0206554,"0; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuner et al., 2009), news media (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and social media (Wei et al., 2013). Although most of the earlier studies experimented with indomain data, there are a few approaches that investigated domain differences. For instance, Szarvas et al. (2012) carried out domain adaptation for biological texts, news media and encyclopedia texts and Vincze (2014) experimented with pieces o"
W16-5002,D09-1145,0,0.0224583,"cently, a special issue of the journal Computational Linguistics (Vol. 38, No. 2) was dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). Among the systems for uncertainty detection we can find rule-based ones (Light et al., 2004; Chapman et al., 2007) but also those based on machine learning methods, usually applying a supervised approach. Some of them used token classification (Morante and Daelemans, 2009; S´anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuner et al.,"
W16-5002,W10-3008,0,0.0364331,"Missing"
W16-5002,W10-3018,0,0.0328487,"Missing"
W16-5002,J12-2004,1,0.744245,"Missing"
W16-5002,W10-3002,0,0.0159373,"2) was dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). Among the systems for uncertainty detection we can find rule-based ones (Light et al., 2004; Chapman et al., 2007) but also those based on machine learning methods, usually applying a supervised approach. Some of them used token classification (Morante and Daelemans, 2009; S´anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuner et al., 2009), news media (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al.,"
W16-5002,W08-0606,1,0.92009,"Missing"
W16-5002,W14-4909,1,0.701345,"Missing"
W16-5002,I13-1044,1,0.846284,"Hungarian that can change the meaning of words (compare sz´el “wind” and szel “cut”), which may lead to ambiguities. Other types of linguistic creativity are also common, such as the use of smileys and English words and abbreviations in Hungarian texts. These characteristics should be considered when processing Hungarian social media texts. In the second phase of data preparation, sentences were manually annotated for uncertainty cues (Vincze et al., 2014). Here we just provide a brief summary of uncertainty categories, for a more elaborated version, please refer to Szarvas et al. (2012) and Vincze (2013). There are several different linguistic phenomena that are categorized as semantic uncertainty. A proposition is epistemically uncertain if its truth value cannot be determined on the basis of world knowledge or on the basis of the speaker’s current mental state, e.g. Steve may have failed at the exam. Conditionals (If it rains, we won’t go to the party) and investigations also belong to semantic uncertainty – the latter is especially frequent in research papers, where it is used to formulate research questions (Here we aim at investigating whether domain specificities affect our results). Do"
W16-5002,C14-1174,1,0.315324,"ted data has hardly been investigated, in other words, social media users can publish whatever they want to and the factuality and (un)certainty of these contents may be an issue for those in need of collecting information from the web. In this paper, we aim at identifying uncertainty cues in social media texts. We focus on Hungarian, a morphologically rich language. Later, we present our machine learning based uncertainty detector. We evaluate our system on a small set of manually annotated social media texts and we compare our results with those obtained by earlier experiments on Hungarian (Vincze, 2014). Finally, we also carry out some cross domain and domain adaptation experiments and we argue that data sparsity may be overcome by simple domain adaptation techniques. The main contributions of this paper are the following: • we report the first results on uncertainty detection in Hungarian social media texts; • we introduce new features in the machine learning setting developed for the linguistic characteristics of social media texts; This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 11 Proceedi"
W16-5002,P13-2011,0,0.0252536,"re aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; Nawaz et al., 2010), medicine (Uzuner et al., 2009), news media (Saur´ı and Pustejovsky, 2009; Wilson, 2008; Rubin et al., 2005; Rubin, 2010), encyclopedia (Ganter and Strube, 2009; Farkas et al., 2010; Szarvas et al., 2012), reviews (Konstantinova et al., 2012; Cruz D´ıaz, 2013) and social media (Wei et al., 2013). Although most of the earlier studies experimented with indomain data, there are a few approaches that investigated domain differences. For instance, Szarvas et al. (2012) carried out domain adaptation for biological texts, news media and encyclopedia texts and Vincze (2014) experimented with pieces of news and Wikipedia texts. Our system described in this paper is also based on supervised machine learning techniques, namely, sequence labeling. The system relies on a rich feature set of lexical, morphological, syntactic, semantic and discourse-based features and also exploits contextual featu"
W16-5002,W10-3013,0,0.0186911,"n in English (Farkas et al., 2010). More recently, a special issue of the journal Computational Linguistics (Vol. 38, No. 2) was dedicated to detecting modality and negation in natural language texts (Morante and Sporleder, 2012). Among the systems for uncertainty detection we can find rule-based ones (Light et al., 2004; Chapman et al., 2007) but also those based on machine learning methods, usually applying a supervised approach. Some of them used token classification (Morante and Daelemans, 2009; S´anchez et al., 2010; Fernandes et al., 2010; Clausen, 2010) or sequence labeling approaches (Zhang et al., 2010; Li et al., 2010; Rei ¨ ur and Radev (2009) and Velldal (2010) matched cues from a and Briscoe, 2010; Tang et al., 2010). Ozg¨ lexicon then applied a binary classifier based on features describing the context of the cue candidate. Most of these systems focus on the English language, however, we are aware of a study aiming at detecting uncertainty in Hungarian texts (Vincze, 2014). Supervised machine learning methods were carried out on corpora from different domains such as biology (Medlock and Briscoe, 2007; Kim et al., 2008; Settles et al., 2008; Shatkay et al., 2008; Vincze et al., 2008; N"
W16-5002,R13-1099,1,0.892658,"in social media than in the other datasets. On the other hand, news tend to contain several instances of doxastic cues and Wikipedia has many weasels. In our experiments, we will demonstrate that such differences may strongly affect the performance of uncertainty detectors. 14 3.2 Machine Learning Methods In order to automatically identify uncertainty cues, we developed a machine learning method to be discussed below. In our experiments, we used our social media corpus as well as the HunCertainty corpus and morphologically and syntactically parsed them with the help of the toolkit magyarlanc (Zsibrita et al., 2013). On the basis of results reported in earlier literature, sequence labeling proved to be one of the most successful methods on English uncertainty detection (see e.g. (Szarvas et al., 2012)), hence we also relied on a method based on conditional random fields (CRF) (Lafferty et al., 2001) in our experiments. We used the MALLET implementation (McCallum, 2002) of CRF. Our feature set is constructed on the basis of earlier uncertainty detectors for Hungarian (Vincze, 2014), however, we added several new features, namely, discourse related features and social media features, due to the specialties"
W17-0606,2016.gwc-1.20,1,0.729948,"t group of Munkácsi’s enormous Mansi–Hungarian dictionary [2] – by relying on its simplified transcipt by Béla K’almán – and also expand it with the Northern Mansi material of Balandin’s and Vakhrusheva’s Mansi–Russian dictionary [7], as well as with dozens of the most necessary neologisms describing different features of contemporary lifestyle (such as the urban environment, oil mining or judicial terms), created and used first and foremost by the journalists of the Mansi newspaper Luima Seripos (see below). 2.2 Mansi Wordnet Recently, another lexical resource has been constructed for Mansi: [8] report on the construction of a wordnet for Mansi. Special challenges were met during the building process, among which the most important ones are the low number of native speakers, the lack of thesauri and the bear language. The bear is a prominently sacred animal venerated by Mansi, thus triggering a detailed taboo language. Since the bear is thought to understand the human speech, it is required to use taboo words while speaking about the bear, the parts of its body, or any activity connected with the bear (especially bear hunting) so that the bear would not understand it. Thus vocabulary"
W17-0606,W11-4101,0,0.0704431,"Missing"
W17-0606,L16-1262,0,0.0692639,"Missing"
W17-1704,J13-1009,0,0.0664231,"Missing"
W17-1704,W06-2408,0,0.14106,"Missing"
W17-1704,C14-1177,0,0.0618437,"Missing"
W17-1704,H05-1004,0,0.013573,"0. Note that these measures operate both on a micro scale (the optimal bijections are looked for within a given sentence) and a macro scale (the results are summed up for all sentences in the corpus). Alternatively, micro-only measures, i.e. the average values of precision and recall for individual sentences, could be considered. Given that the density of VMWEs per sentence can vary greatly, and in many languages the majority of sentences do not contain any VMWE, we believe that the macro measures are more appropriate. Note also that the measures in (2) are comparable to the CEAF-M measures (Luo, 2005) used in the coreference resolution task.20 There, mentions are grouped into entities (clusters) and the best bijection between gold and system entities is searched for. The main difference with our approach resides in the fact that, while coreference • T P 1max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t2,t3} |= 2 R = T P 1max /||G ||= 2/3 P = T P 1max /||S1 ||= 2/3. • T P 2max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t3} |+ |∅ ∩ {t2} |= 2 R = T P 2max /||G ||= 2/3 P = T P 2max /||S2 ||= 2/3. • T P 3max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t3} |+ |∅ ∩ {t2} |+ |∅ ∩ {t1,t3} |= 2 R = T P 3max /||G ||= 2/3 P = T P 3max /||S3"
W17-1704,I11-1024,0,0.0544171,"Missing"
W17-1704,J15-3003,0,0.143842,"ation of this large project included the definition of roles – project leaders, technical experts, language group leaders (LGLs), language leaders (LLs) and annotators – and their tasks. Annotation Methodology In order to bring about substantial progress in the state of the art presented in the preceding section, the European PARSEME network5 , dedicated to parsing and MWEs, proposed a shared task on automatic identification of VMWEs. This initiative required the construction of a large multilingual VMWE-annotated corpus. Within the challenging features of linguistic annotation, as defined by Mathet et al. (2015), the VMWE annotation task is concerned by: 3.1 The biggest challenge in the initial phase of the project was the development of the annotation guidelines7 which would be as universal as possible but which would still allow for languagespecific categories and tests. To this end, a twophase pilot annotation in most of the participating languages was carried out. Some corpora were annotated at this stage not only by native but also by near-native speakers, so as to promote cross-language convergences. Each pilot annotation phase provided feedback from annotators and was followed by enhancements"
W17-1704,Q14-1016,0,0.0486965,"and/or verbal idioms. They also underline the heterogeneity of these MWE annotations. Nivre and Vincze (2015) show that this is also the case in the treebanks of Universal Dependencies (UD), despite the homogenizing objective of the UD project (McDonald et al., 2013). More recent efforts (Adalı et al., 2016), while addressing VMWEs in a comprehensive way, still suffer from missing annotation standards. 3 2 4 http://multiword.sourceforge.net/sharedtask2017 32 http://multiword.sf.net/ http://dimsum16.github.io (DE) auf|machen (lit. out|make) ’open’.6 discontinuous. They were annotated following Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al"
W17-1704,P15-1108,1,0.941692,"ultiword.sf.net/ http://dimsum16.github.io (DE) auf|machen (lit. out|make) ’open’.6 discontinuous. They were annotated following Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portug"
W17-1704,C16-1042,1,0.824829,"r et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portuguese (PT); and others: Farsi (FA), Greek (EL), Hebrew (HE), Hungarian (HU), Maltese (MT) and Turkish (TR). Note that the 4 last are non-Indo-E"
W17-1704,S16-1084,0,0.0544884,"Missing"
W17-1704,W10-3705,0,0.0808784,"Missing"
W17-1704,W11-0807,0,0.0214913,"Missing"
W17-1704,W14-0804,0,0.0310488,"owing Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portuguese (PT); and others: Farsi (FA), Greek (EL), Hebrew (HE), Hungarian (HU), Maltese (MT) and Turkish (TR). Note that t"
W17-1704,S12-1010,0,0.026905,"Missing"
W17-1704,W16-1812,0,0.0461931,"Missing"
W17-1704,C10-1125,1,0.793436,"Missing"
W17-1704,R11-1040,1,0.890663,"Missing"
W17-1704,N09-1037,0,\N,Missing
W17-1704,P14-1070,1,\N,Missing
W17-1704,P16-1016,0,\N,Missing
W17-1705,W10-3705,0,0.148062,"een languages, but could be extended to any language if provided with POS tagging and dependency analysis of the training database. In the paper, we first describe how the system works in detail, then show the results achieved in the shared task on the nine languages with both POS tagging and dependency analysis, last we give an error analysis of our output. 2 3 System description Our system works through the connection of MWEs and parsing, an approach described by many sources (Constant and Nivre, 2016; Nasr et al., 2015; Candito and Constant, 2014; Green et al., 2011; Waszczuk et al., 2016; Wehrli et al., 2010; Green et al., 2013) and is one the basic ideas behind the work done by the PARSEME group 2 . The idea for our system is directly based on the work described in Vincze et al. (2013) to use dependency parsing to find MWEs. As a high number of the languages of the shared task are morphologically rich and have free word order, therefore syntactically flexible MWEs might not be adjacent, this approach seems a better fit for the task than sequence labeling or similar strategies. The system of that paper uses dependency relations specific to syntactic relation and MWE type, for example light verb c"
W17-1705,C10-1011,0,0.0300091,"part of the 2017 MWE workshop. 1 http://multiword.sourceforge.net/ PHITE.php?sitesig=CONF&page=CONF_05_MWE_ 2017___lb__EACL__rb__&subpage=CONF_40_ Shared_Task 2 http://typo.uni-konstanz.de/parseme/ 48 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 48–53, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics the highest element of the idiom az els˝o követ veti (“casts the first stone”) is the verb, vetette (cast.Sg3.Past). All other elements’ dependency labels are changed to ID. The second step is training the parser: we used the Bohnet parser (Bohnet, 2010) for both POS tagging and dependency parsing. For the singletoken MWEs, we trained the Bohnet parser’s POS tagger module on the MWE-merged corpora and its dependency parser for the multi-token MWEs. The parser would treat the MWE POS tags and dependency labels as any other POS tag and dependency label. We did the same for each language and created POS tagging and dependency parsing models capable of identifying MWEs for them. In the case of some of the languages in the shared task, we had to omit sentences from the training data that were overly long (spanning over 500 tokens in some cases) an"
W17-1705,P14-1070,0,0.255662,"analyzed training data. We submitted results for nine out of the eighteen languages, but could be extended to any language if provided with POS tagging and dependency analysis of the training database. In the paper, we first describe how the system works in detail, then show the results achieved in the shared task on the nine languages with both POS tagging and dependency analysis, last we give an error analysis of our output. 2 3 System description Our system works through the connection of MWEs and parsing, an approach described by many sources (Constant and Nivre, 2016; Nasr et al., 2015; Candito and Constant, 2014; Green et al., 2011; Waszczuk et al., 2016; Wehrli et al., 2010; Green et al., 2013) and is one the basic ideas behind the work done by the PARSEME group 2 . The idea for our system is directly based on the work described in Vincze et al. (2013) to use dependency parsing to find MWEs. As a high number of the languages of the shared task are morphologically rich and have free word order, therefore syntactically flexible MWEs might not be adjacent, this approach seems a better fit for the task than sequence labeling or similar strategies. The system of that paper uses dependency relations speci"
W17-1705,P16-1016,0,0.115911,"pendent, but relies on POS tagged, dependency analyzed training data. We submitted results for nine out of the eighteen languages, but could be extended to any language if provided with POS tagging and dependency analysis of the training database. In the paper, we first describe how the system works in detail, then show the results achieved in the shared task on the nine languages with both POS tagging and dependency analysis, last we give an error analysis of our output. 2 3 System description Our system works through the connection of MWEs and parsing, an approach described by many sources (Constant and Nivre, 2016; Nasr et al., 2015; Candito and Constant, 2014; Green et al., 2011; Waszczuk et al., 2016; Wehrli et al., 2010; Green et al., 2013) and is one the basic ideas behind the work done by the PARSEME group 2 . The idea for our system is directly based on the work described in Vincze et al. (2013) to use dependency parsing to find MWEs. As a high number of the languages of the shared task are morphologically rich and have free word order, therefore syntactically flexible MWEs might not be adjacent, this approach seems a better fit for the task than sequence labeling or similar strategies. The syste"
W17-1705,D11-1067,0,0.201487,"Missing"
W17-1705,J13-1009,0,0.15095,"Missing"
W17-1705,P15-1108,0,0.354209,"tagged, dependency analyzed training data. We submitted results for nine out of the eighteen languages, but could be extended to any language if provided with POS tagging and dependency analysis of the training database. In the paper, we first describe how the system works in detail, then show the results achieved in the shared task on the nine languages with both POS tagging and dependency analysis, last we give an error analysis of our output. 2 3 System description Our system works through the connection of MWEs and parsing, an approach described by many sources (Constant and Nivre, 2016; Nasr et al., 2015; Candito and Constant, 2014; Green et al., 2011; Waszczuk et al., 2016; Wehrli et al., 2010; Green et al., 2013) and is one the basic ideas behind the work done by the PARSEME group 2 . The idea for our system is directly based on the work described in Vincze et al. (2013) to use dependency parsing to find MWEs. As a high number of the languages of the shared task are morphologically rich and have free word order, therefore syntactically flexible MWEs might not be adjacent, this approach seems a better fit for the task than sequence labeling or similar strategies. The system of that paper use"
W17-1705,I13-1024,1,0.897428,"rks in detail, then show the results achieved in the shared task on the nine languages with both POS tagging and dependency analysis, last we give an error analysis of our output. 2 3 System description Our system works through the connection of MWEs and parsing, an approach described by many sources (Constant and Nivre, 2016; Nasr et al., 2015; Candito and Constant, 2014; Green et al., 2011; Waszczuk et al., 2016; Wehrli et al., 2010; Green et al., 2013) and is one the basic ideas behind the work done by the PARSEME group 2 . The idea for our system is directly based on the work described in Vincze et al. (2013) to use dependency parsing to find MWEs. As a high number of the languages of the shared task are morphologically rich and have free word order, therefore syntactically flexible MWEs might not be adjacent, this approach seems a better fit for the task than sequence labeling or similar strategies. The system of that paper uses dependency relations specific to syntactic relation and MWE type, for example light verb constructions that are made up of a verb-object relation syntactically, get the Shared task Our system was built for the shared task on automatic identification of verbal multiword ex"
W17-1705,C16-1042,0,0.109419,"r nine out of the eighteen languages, but could be extended to any language if provided with POS tagging and dependency analysis of the training database. In the paper, we first describe how the system works in detail, then show the results achieved in the shared task on the nine languages with both POS tagging and dependency analysis, last we give an error analysis of our output. 2 3 System description Our system works through the connection of MWEs and parsing, an approach described by many sources (Constant and Nivre, 2016; Nasr et al., 2015; Candito and Constant, 2014; Green et al., 2011; Waszczuk et al., 2016; Wehrli et al., 2010; Green et al., 2013) and is one the basic ideas behind the work done by the PARSEME group 2 . The idea for our system is directly based on the work described in Vincze et al. (2013) to use dependency parsing to find MWEs. As a high number of the languages of the shared task are morphologically rich and have free word order, therefore syntactically flexible MWEs might not be adjacent, this approach seems a better fit for the task than sequence labeling or similar strategies. The system of that paper uses dependency relations specific to syntactic relation and MWE type, for"
W17-1721,W14-0801,0,0.0304606,"unking, statistical and lexical information. Kim and Baldwin (2006) relied on semantic information when detecting verb-particle constructions. Nagy T. and Vincze (2011) introduced a rule-based system using morphological features to detect VPCs in texts. Tu and Roth (2012) used syntactic and lexical features to classify VPCs candidates on a crowdsourced corpus. Nagy T. and Vincze (2014) implemented VPCTagger, a machine learning-based tool that selects VPC candidates on the basis of syntactic information and then classifies them as VPCs or not, based on lexical, syntactic and semantic features. Smith (2014) extracted VPCs from an English–Spanish parallel subtitles corpus. 155 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 155–160, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics Here, we differ from earlier approaches in that we focus on just questions and we examine how linguistic features of questions may help in identifying VPCs in texts. 3 be come get go grow look make set stand take turn other Data Collection For data collection, we used three English corpora. First, we made use of the Google Web Treebank (Bies et al., 2012), which cont"
W17-1721,W02-2001,0,0.0654349,"onal phrase. This fact makes it hard to identify them on the basis of surface patterns. However, there are some syntactic or semantic processes that can be used to distinguish MWEs from compositional phrases. For instance, question formation (WHmovement), passivization and pronominalization are often listed among the distinctive tests (see e.g. (Kearns, 2002)). Phrasal-prepositional verbs usually employ the WH-words what or who, leaving the preposition at the end of the sentence as 2 Related Work Verb-particle constructions have been paid considerable attention in natural language processing. Baldwin and Villavicencio (2002) detected verbparticle constructions in raw texts on the basis of POS-tagging, chunking, statistical and lexical information. Kim and Baldwin (2006) relied on semantic information when detecting verb-particle constructions. Nagy T. and Vincze (2011) introduced a rule-based system using morphological features to detect VPCs in texts. Tu and Roth (2012) used syntactic and lexical features to classify VPCs candidates on a crowdsourced corpus. Nagy T. and Vincze (2014) implemented VPCTagger, a machine learning-based tool that selects VPC candidates on the basis of syntactic information and then cl"
W17-1721,S12-1010,0,0.181496,"asal-prepositional verbs usually employ the WH-words what or who, leaving the preposition at the end of the sentence as 2 Related Work Verb-particle constructions have been paid considerable attention in natural language processing. Baldwin and Villavicencio (2002) detected verbparticle constructions in raw texts on the basis of POS-tagging, chunking, statistical and lexical information. Kim and Baldwin (2006) relied on semantic information when detecting verb-particle constructions. Nagy T. and Vincze (2011) introduced a rule-based system using morphological features to detect VPCs in texts. Tu and Roth (2012) used syntactic and lexical features to classify VPCs candidates on a crowdsourced corpus. Nagy T. and Vincze (2014) implemented VPCTagger, a machine learning-based tool that selects VPC candidates on the basis of syntactic information and then classifies them as VPCs or not, based on lexical, syntactic and semantic features. Smith (2014) extracted VPCs from an English–Spanish parallel subtitles corpus. 155 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 155–160, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics Here, we differ from earlier"
W17-1721,C10-1011,0,0.0391377,"rom all three sources of data, we automatically filtered the sentences and selected questions from them. Furthermore, we also selected sentences that ended in a preposition or a particle (based on morphological information) and we grouped them into two classes: positive examples (questions with VPC) and negative examples (questions where the last token was a preposition due to preposition stranding). After these filtering steps, we got 280 questions out of which 227 were negative examples and the remaining 53 were positive examples. We parsed these sentences with the Bohnet dependency parser (Bohnet, 2010) in order to get a unified syntactic representation of the data. We will analyze these data from a quantitative point of view and report some statistics on them. This dataset will also be exploited by a machine learning system that aims at classifying each VPC candidate as a positive or negative one, which will be described in Section 5. 4 negative 36 18 2 2 0 2 19 0 32 2 1 113 total 36 23 12 5 3 3 25 1 32 4 2 134 Table 1: Distribution of verbs. the results, which are significant (χ2 -test, p = 6.72297E-12). The data reveal that there are some interesting differences in the distribution of ver"
W17-1721,P06-1063,0,0.0244922,"Missing"
W17-1721,W06-2110,0,0.0403746,"ed to distinguish MWEs from compositional phrases. For instance, question formation (WHmovement), passivization and pronominalization are often listed among the distinctive tests (see e.g. (Kearns, 2002)). Phrasal-prepositional verbs usually employ the WH-words what or who, leaving the preposition at the end of the sentence as 2 Related Work Verb-particle constructions have been paid considerable attention in natural language processing. Baldwin and Villavicencio (2002) detected verbparticle constructions in raw texts on the basis of POS-tagging, chunking, statistical and lexical information. Kim and Baldwin (2006) relied on semantic information when detecting verb-particle constructions. Nagy T. and Vincze (2011) introduced a rule-based system using morphological features to detect VPCs in texts. Tu and Roth (2012) used syntactic and lexical features to classify VPCs candidates on a crowdsourced corpus. Nagy T. and Vincze (2014) implemented VPCTagger, a machine learning-based tool that selects VPC candidates on the basis of syntactic information and then classifies them as VPCs or not, based on lexical, syntactic and semantic features. Smith (2014) extracted VPCs from an English–Spanish parallel subtit"
W17-1721,W14-0803,1,0.830924,"Missing"
W17-6527,C10-1011,0,0.0401521,"the third column of Table (2); in Figure (1) a sentence with two coordinated clauses with zero copula to show how the labels can combine. Table 3 summarizes in which conditions the different approaches give syntactic structures different from regular content verbs analysis for copular sentences. The content head approach gives the most linguistically based distinction by drawing the line between copula and non-copula main verbs. 4 Experiments We evaluated the three approaches in two parsing experiments. We used the same corpus with three different dependency annotations and the Bohnet parser (Bohnet, 2010) for both. 4.1 The corpus We used a section of the Szeged Dependency Treebank that is available with all three analyses: the original annotation is function head based, there is an automatically converted complex label version, and the converted, manually corrected Universal Dependencies treebank for the content head version. The section contains about 1300 sentences, 27000 tokens in total. The data contains 300 instances of virtual V, 230 overt copulas and 150 existential vans. 4.2 Experiment 1: Function head, content head or complex label In the first experiment, the Bohnet parser was traine"
W17-6527,C12-2105,0,0.316973,"es, but we have an analysis that has no issues with the zero copula. A section of the Szeged Dependency Treebank has been converted to the Universal Dependencies annotation (Vincze et al., 2015; Vincze et al., 2017). In the experiments, this treebank is used as the content head analysis. The second column of Table (2) shows the sentences in Examples (1), (3) and (4) again, this time with the content head analysis in the Szeged Universal Dependencies Treebank. 3.3 Complex label approach The complex label approach is a computational linguistic variation of the function head analysis detailed in Seeker et al. (2012). They keep the function words as the heads, therefore keeping the copula as the head of the copular clause, but they deal with the zero copula in a different way. The analysis does not use virtual nodes, but instead “shows” the missing copula in the dependency labels originating from where it would be inserted. As in the zero copula example for complex label in Table 2. , the root node of the structure in the function head analysis would be a virtual VAN node, the subject, Peter would be a dependent of VAN. Therefore the Complex label dependency label of the subject is ROOT-VAN-SUBJ: the orig"
W17-6527,E17-1034,1,0.875699,"Missing"
W18-4925,W17-1717,1,0.829766,"on in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our effo"
W18-4925,W17-1716,1,0.892903,"Missing"
W18-4925,P14-1070,1,0.86208,"mon guidelines. They highlight the heterogeneity of MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The fi"
W18-4925,F12-2024,1,0.868442,"om the Croatian version of the SETimes corpora: mostly running text but also selected fragments, such as introductory blurbs and image descriptions characteristic of newswire text. The English corpus consists of 7,437 sentences taken from three of the UD: the Gold Standard Universal Dependencies Corpus for English, the LinES parallel corpus and the Parallel Universal Dependencies treebank. The Farsi corpus is built on top of the MULTEXT-East corpora (QasemiZadeh and Rahimi, 2006) and VMWE annotations are added to a portion of Orwell’s 1984 novel. The French corpus contains the Sequoia corpus (Candito and Seddah, 2012) converted to UD, the GDS French UD treebank, the French part of the Partut corpus, and part of the Parallel UD (PUD) corpus. The German corpus contains shuffled sentences crawled from online news, reviews and wikis, derived from the WMT16 shared task data (Bojar et al., 2016), and Universal Dependencies v2.0. The Greek corpus comprises Wikipedia articles and newswire texts from various on-line newspaper editions and news portals. The Hebrew corpus contains news and articles from Arutz 7 and HaAretz news websites, collected by the MILA Knowledge Center for Processing Hebrew. The Hindi corpus r"
W18-4925,P16-1016,0,0.0692628,"actices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generat"
W18-4925,J17-4005,1,0.881403,"Missing"
W18-4925,N09-1037,0,0.0413995,"anks for 15 languages, collaboratively documented according to common guidelines. They highlight the heterogeneity of MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural netwo"
W18-4925,D11-1067,0,0.0237076,"Missing"
W18-4925,J13-1009,0,0.0221261,"Missing"
W18-4925,W17-1707,0,0.0299396,"en et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWErelated phenomena, and towards a better synergy of terminologies across languages and linguistic traditions. The annotation guidelines were gradually enhanced, so as"
W18-4925,C14-1177,0,0.023678,"Missing"
W18-4925,W14-0406,0,0.112167,"Missing"
W18-4925,W17-1715,0,0.019845,"ore integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWErelated phenomena, and towards a better synergy of terminologies across languages and linguistic traditions. The annotation"
W18-4925,P15-1108,1,0.836029,"f MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary e"
W18-4925,W17-1706,0,0.0136581,"ing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better"
W18-4925,L16-1262,0,0.0499042,"Missing"
W18-4925,S16-1084,1,0.93644,"participating systems, their methods and obtained results are also presented and analysed. 1 Introduction Across languages, multiword expressions (MWEs) are widely recognized as a significant challenge for natural language processing (NLP) (Sag et al., 2002; Baldwin and Kim, 2010). An international and highly multilingual research community, forged via regular workshops and initiatives such as the PARSEME network (Savary et al., 2015), has rallied around the goals of characterizing MWEs in lexicons, grammars and corpora and enabling systems to process them. Recent shared tasks, namely DiMSUM (Schneider et al., 2016) and the first edition of the PARSEME Shared Task on automatic identification of verbal multiword expressions in 2017 (Savary et al., 2017), have helped drive MWE research forward, yielding new corpora and testbeds for MWEs identification systems. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 222 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 222–240 Santa Fe, New Mexico, USA, August 25-26, 2018. This paper describ"
W18-4925,W17-1705,1,0.741206,"ome popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWE"
W18-4925,I13-1024,1,0.84782,"ns, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on lan"
W18-4925,C16-1042,1,0.834941,"guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous ver"
W18-4925,W10-3705,0,0.0133537,"rd to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new"
W18-4925,W14-0804,0,0.0128345,"ovide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not cov"
