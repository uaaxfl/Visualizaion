2021.iwslt-1.14,{FST}: the {FAIR} Speech Translation System for the {IWSLT}21 Multilingual Shared Task,2021,-1,-1,6,0,5768,yun tang,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"In this paper, we describe our end-to-end multilingual speech translation system submitted to the IWSLT 2021 evaluation campaign on the Multilingual Speech Translation shared task. Our system is built by leveraging transfer learning across modalities, tasks and languages. First, we leverage general-purpose multilingual modules pretrained with large amounts of unlabelled and labelled data. We further enable knowledge transfer from the text task to the speech task by training two tasks jointly. Finally, our multilingual model is finetuned on speech translation task-specific data to achieve the best translation results. Experimental results show our system outperforms the reported systems, including both end-to-end and cascaded based approaches, by a large margin. In some translation directions, our speech translation results evaluated on the public Multilingual TEDx test set are even comparable with the ones from a strong text-to-text translation system, which uses the oracle speech transcripts as input."
2021.eacl-main.115,{W}iki{M}atrix: Mining 135{M} Parallel Sentences in 1620 Language Pairs from {W}ikipedia,2021,-1,-1,1,1,5770,holger schwenk,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We do not limit the extraction process to alignments with English, but we systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 16720 different language pairs, out of which only 34M are aligned with English. This corpus is freely available. To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English."
2021.acl-long.507,{CCM}atrix: Mining Billions of High-Quality Parallel Sentences on the Web,2021,-1,-1,1,1,5770,holger schwenk,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences. Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English. We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark. Further, we evaluate on competitive translation benchmarks such as WMT and WAT. Using only mined bitext, we set a new state of the art for a single system on the WMT{'}19 test set for English-German/Russian/Chinese. In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT{'}19 systems, which train on the WMT training data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available."
2020.acl-main.653,{MLQA}: Evaluating Cross-lingual Extractive Question Answering,2020,-1,-1,5,0,3865,patrick lewis,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance."
W19-5435,Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings,2019,18,0,4,0,4499,vishrav chaudhary,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"In this paper, we describe our submission to the WMT19 low-resource parallel corpus filtering shared task. Our main approach is based on the LASER toolkit (Language-Agnostic SEntence Representations), which uses an encoder-decoder architecture trained on a parallel corpus to obtain multilingual sentence representations. We then use the representations directly to score and filter the noisy parallel sentences without additionally training a scoring function. We contrast our approach to other promising methods and show that LASER yields strong results. Finally, we produce an ensemble of different scoring methods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali-English and Sinhala-English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios."
Q19-1038,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,2019,23,31,2,0,10622,mikel artetxe,Transactions of the Association for Computational Linguistics,0,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER."
P19-1309,Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings,2019,0,4,2,0,10622,mikel artetxe,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Machine translation is highly sensitive to the size and quality of the training data, which has led to an increasing interest in collecting and filtering large parallel corpora. In this paper, we propose a new method for this task based on multilingual sentence embeddings. In contrast to previous approaches, which rely on nearest neighbor retrieval with a hard threshold over cosine similarity, our proposed method accounts for the scale inconsistencies of this measure, considering the margin between a given sentence pair and its closest candidates instead. Our experiments show large improvements over existing methods. We outperform the best published results on the BUCC mining task and the UN reconstruction task by more than 10 F1 and 30 precision points, respectively. Filtering the English-German ParaCrawl corpus with our approach, we obtain 31.2 BLEU points on newstest2014, an improvement of more than one point over the best official filtered version."
P18-2037,Filtering and Mining Parallel Data in a Joint Multilingual Space,2018,0,14,1,1,5770,holger schwenk,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We learn a joint multilingual sentence embedding and use the distance between sentences in different languages to filter noisy parallel data and to mine for parallel data in large news collections. We are able to improve a competitive baseline on the WMT{'}14 English to German task by 0.3 BLEU by filtering out 25{\%} of the training data. The same approach is used to mine additional bitexts for the WMT{'}14 system and to obtain competitive results on the BUCC shared task to identify parallel sentences in comparable corpora. The approach is generic, it can be applied to many language pairs and it is independent of the architecture of the machine translation system."
L18-1560,A Corpus for Multilingual Document Classification in Eight Languages,2018,7,2,1,1,5770,holger schwenk,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Cross-lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources. Several approaches have been proposed in the literature and the current best practice is to evaluate them on a subset of the Reuters Corpus Volume 2. However, this subset covers only few languages (English, German, French and Spanish) and almost all published works focus on the the transfer between English and German. In addition, we have observed that the class prior distributions differ significantly between the languages. We argue that this complicates the evaluation of the multilinguality. In this paper, we propose a new subset of the Reuters corpus with balanced class priors for eight languages. By adding Italian, Russian, Japanese and Chinese, we cover languages which are very different with respect to syntax, morphology, etc. We provide strong baselines for all language transfer directions using multilingual word and sentence embeddings respectively. Our goal is to offer a freely available framework to evaluate cross-lingual document classification, and we hope to foster by these means, research in this important area."
D18-1269,{XNLI}: Evaluating Cross-lingual Sentence Representations,2018,0,77,6,0.857143,2456,alexis conneau,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."
W17-2619,Learning Joint Multilingual Sentence Representations with Neural Machine Translation,2017,29,12,1,1,5770,holger schwenk,Proceedings of the 2nd Workshop on Representation Learning for {NLP},0,"In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics. We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages."
E17-1104,Very Deep Convolutional Networks for Text Classification,2017,12,185,2,0.857143,2456,alexis conneau,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing."
D17-1070,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,2017,36,271,3,0.857143,2456,alexis conneau,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available."
W15-4006,Incremental Adaptation Strategies for Neural Network Language Models,2015,32,6,2,0,36730,alex tersarkisov,Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality,0,"It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without over-fitting the small adaptation data."
N15-1103,Continuous Adaptation to User Feedback for Statistical Machine Translation,2015,11,0,5,1,3257,frederic blain,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper gives a detailed experiment feedback of different approaches to adapt a statistical machine translation system towards a targeted translation project, using only small amounts of parallel in-domain data. The experiments were performed by professional translators under realistic conditions of work using a computer assisted translation tool. We analyze the influence of these adaptations on the translator productivity and on the overall post-editing effort. We show that significant improvements can be obtained by using the presented adaptation techniques."
2015.iwslt-papers.5,Improving continuous space language models auxiliary features,2015,27,2,2,1,31616,walid aransa,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,None
D14-1179,Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation,2014,31,2281,6,0,1032,kyunghyun cho,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"In this paper, we propose a novel neural network model called RNN Encoderxe2x80x90 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoderxe2x80x90Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."
C14-2028,The {M}ate{C}at Tool,2014,7,36,12,0,3526,marcello federico,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: System Demonstrations",0,"We present a new web-based CAT tool providing translators with a professional work environment, integrating translation memories, terminology bases, concordancers, and machine translation. The tool is completely developed as open source software and has been already successfully deployed for business, research and education. The MateCat Tool represents today probably the best available open source platform for investigating, integrating, and evaluating under realistic conditions the impact of new machine translation technology on human post-editing."
2014.iwslt-evaluation.14,{LIUM} {E}nglish-to-{F}rench spoken language translation system and the Vecsys/{LIUM} automatic speech recognition system for {I}talian language for {IWSLT} 2014,2014,11,1,5,0.804598,17019,anthony rousseau,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the Spoken Language Translation system developed by the LIUM for the IWSLT 2014 evaluation campaign. We participated in two of the proposed tasks: (i) the Automatic Speech Recognition task (ASR) in two languages, Italian with the Vecsys company, and English alone, (ii) the English to French Spoken Language Translation task (SLT). We present the approaches and specificities found in our systems, as well as the results from the evaluation campaign."
P13-1082,A Multi-Domain Translation Model Framework for Statistical Machine Translation,2013,19,28,2,0,2690,rico sennrich,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 BLEU over unadapted systems and single-domain adaptation."
I13-1033,Multimodal Comparable Corpora as Resources for Extracting Parallel Data: Parallel Phrases Extraction,2013,29,4,3,0.740741,5094,haithem afli,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Discovering parallel data in comparable corpora is a promising approach for overcoming the lack of parallel texts in statistical machine translation and other NLP applications. In this paper we propose an alternative to comparable corpora of texts as resources for extracting parallel data: a multimodal comparable corpus of audio and texts. We present a novel method to detect parallel phrases from such corpora based on splitting comparable sentences into fragments, called phrases. The audio is transcribed by an automatic speech recognition system, split into fragments and translated with a baseline statistical machine translation system. We then use information retrieval in a large text corpus in the target language, split also into fragments, and extract parallel phrases. We compared our method with parallel sentences extraction techniques. We evaluate the quality of the extracted data on an English to French translation task and show significant improvements over a state-ofthe-art baseline."
2013.mtsummit-wptp.13,Issues in incremental adaptation of statistical {MT} from human post-edits,2013,19,7,6,0.114055,10592,mauro cettolo,Proceedings of the 2nd Workshop on Post-editing Technology and Practice,0,"This work investigates a crucial aspect for the integration of MT technology into a CAT environment, that is the ability of MT systems to adapt from the user feedback. In particular, we consider the scenario of an MT system tuned for a specific translation project that after each day of work adapts from the post-edited translations created by the user. We apply and compare different state-of-the-art adaptation methods on post-edited translations generated by two professionals during two days of work with a CAT tool embedding MT suggestions. Both translators worked at the same legal document from English into Italian and German, respectively. Although exactly the same amount of translations was available each day for each language , the application of the same adaptation methods resulted in quite different outcomes. This suggests that adaptation strategies should not be applied blindly, but rather taking into account language specific issues, such as data sparsity."
W12-3147,{LIUM}{'}s {SMT} Machine Translation Systems for {WMT} 2012,2012,14,1,4,0.714286,5281,christophe servan,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes the development of French--English and English--French statistical machine translation systems for the 2012 WMT shared task evaluation. We developed phrase-based systems based on the Moses decoder, trained on the provided data only. Additionally, new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection."
W12-2702,"Large, Pruned or Continuous Space Language Models on a {GPU} for Statistical Machine Translation",2012,27,75,1,1,5770,holger schwenk,Proceedings of the {NAACL}-{HLT} 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for {HLT},0,"Language models play an important role in large vocabulary speech recognition and statistical machine translation systems. The dominant approach since several decades are back-off language models. Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. Lately, this tendency has changed and recent works concentrate on data selection. Continuous space methods are a very competitive approach, but they have a high computational complexity and are not yet in widespread use. This paper presents an experimental comparison of all these approaches on a large statistical machine translation task. We also describe an open-source implementation to train and use continuous space language models (CSLM) for such large tasks. We describe an efficient implementation of the CSLM using graphical processing units from Nvidia. By these means, we are able to train an CSLM on more than 500 million words in 20 hours. This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build."
lambert-etal-2012-automatic,Automatic Translation of Scientific Documents in the {HAL} Archive,2012,8,4,2,0.783317,23604,patrik lambert,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the development of a statistical machine translation system between French and English for scientific papers. This system will be closely integrated into the French HAL open archive, a collection of more than 100.000 scientific papers. We describe the creation of in-domain parallel and monolingual corpora, the development of a domain specific translation system with the created resources, and its adaptation using monolingual resources only. These techniques allowed us to improve a generic system by more than 10 BLEU points."
F12-2039,Traduction automatique {\\`a} partir de corpus comparables: extraction de phrases parall{\\`e}les {\\`a} partir de donn{\\'e}es comparables multimodales (Automatic Translation from Comparable corpora : extracting parallel sentences from multimodal comparable corpora) [in {F}rench],2012,0,0,3,0.740741,5094,haithem afli,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
E12-2003,Collaborative Machine Translation Service for Scientific texts,2012,5,4,4,0.783317,23604,patrik lambert,Proceedings of the Demonstrations at the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"French researchers are required to frequently translate into French the description of their work published in English. At the same time, the need for French people to access articles in English, or to international researchers to access theses or papers in French, is incorrectly resolved via the use of generic translation tools. We propose the demonstration of an end-to-end tool integrated in the HAL open archive for enabling efficient translation for scientific texts. This tool can give translation suggestions adapted to the scientific domain, improving by more than 10 points the BLEU score of a generic system. It also provides a post-edition service which captures user post-editing data that can be used to incrementally improve the translations engines. Thus it is helpful for users which need to translate or to access scientific texts."
C12-2104,Continuous Space Translation Models for Phrase-Based Statistical Machine Translation,2012,17,82,1,1,5770,holger schwenk,Proceedings of {COLING} 2012: Posters,0,"This paper presents a new approach to perform the estimation of the translation model probabilities of a phrase-based statistical machine translation system. We use neural networks to directly learn the translation probability of phrase pairs using continuous representations. The system can be easily trained on the same data used to build standard phrase-based systems. We provide experimental evidence that the approach seems to be able to infer meaningful translation probabilities for phrase pairs not seen in the training data, or even predict a list of the most likely translations given a source phrase. The approach can be used to rescore n-best lists, but we also discuss an integration into the Moses decoder. A preliminary evaluation on the English/French IWSLT task achieved improvements in the BLEU score and a human analysis showed that the new model often chooses semantically better translations. Several extensions of this work are discussed."
2012.iwslt-papers.6,Semi-supervised transliteration mining from parallel and comparable corpora,2012,16,3,2,1,31616,walid aransa,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,Transliteration is the process of writing a word (mainly proper noun) from one language in the alphabet of another language. This process requires mapping the pronunciation of the word from the source language to the closest possible pronunciation in the target language. In this paper we introduce a new semi-supervised transliteration mining method for parallel and comparable corpora. The method is mainly based on a new suggested Three Levels of Similarity (TLS) scores to extract the transliteration pairs. The first level calculates the similarity of of all vowel letters and consonants letters. The second level calculates the similarity of long vowels and vowel letters at beginning and end position of the words and consonants letters. The third level calculates the similarity consonants letters only. We applied our method on Arabic-English parallel and comparable corpora. We evaluated the extracted transliteration pairs using a statistical based transliteration system. This system is built using letters instead or words as tokens. The transliteration system achieves an accuracy of 0.50 and a mean F-score 0.8958 when trained on transliteration pairs extracted from a parallel corpus. The accuracy is 0.30 and the mean F-score 0.84 when we used instead a comparable corpus to automatically extract the transliteration pairs. This shows that the proposed semi-supervised transliteration mining algorithm is effective and can be applied to other language pairs. We also evaluated two segmentation techniques and reported the impact on the transliteration performance.
2012.iwslt-papers.12,Incremental adaptation using translation information and post-editing analysis,2012,8,15,2,1,3257,frederic blain,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"It is well known that statistical machine translation systems perform best when they are adapted to the task. In this paper we propose new methods to quickly perform incremental adaptation without the need to obtain word-by-word alignments from GIZA or similar tools. The main idea is to use an automatic translation as pivot to infer alignments between the source sentence and the reference translation, or user correction. We compared our approach to the standard method to perform incremental re-training. We achieve similar results in the BLEU score using less computational resources. Fast retraining is particularly interesting when we want to almost instantly integrate user feed-back, for instance in a post-editing context or machine translation assisted CAT tool. We also explore several methods to combine the translation models."
2012.amta-papers.21,A General Framework to Weight Heterogeneous Parallel Data for Model Adaptation in Statistical {MT},2012,23,6,3,1,695,kashif shah,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"The standard procedure to train the translation model of a phrase-based SMT system is to concatenate all available parallel data, to perform word alignment, to extract phrase pairs and to calculate translation probabilities by simple relative frequency. However, parallel data is quite inhomogeneous in many practical applications with respect to several factors like data source, alignment quality, appropriateness to the task, etc. We propose a general framework to take into account these factors during the calculation of the phrase-table, e.g. by better distributing the probability mass of the individual phrase pairs. No additional feature functions are needed. We report results on two well-known tasks: the IWSLT{'}11 and WMT{'}11 evaluations, in both conditions translating from English to French. We give detailed results for different functions to weight the bitexts. Our best systems improve a strong baseline by up to one BLEU point without any impact on the computational complexity during training or decoding."
W11-2132,Investigations on Translation Model Adaptation Using Monolingual Data,2011,18,35,2,0.783317,23604,patrik lambert,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"Most of the freely available parallel data to train the translation model of a statistical machine translation system comes from very specific sources (European parliament, United Nations, etc). Therefore, there is increasing interest in methods to perform an adaptation of the translation model. A popular approach is based on unsupervised training, also called self-enhancing. Both only use monolingual data to adapt the translation model. In this paper we extend the previous work and provide new insight in the existing methods. We report results on the translation between French and English. Improvements of up to 0.5 BLEU were observed with respect to a very competitive baseline trained on more than 280M words of human translated parallel data."
W11-2158,{LIUM}{'}s {SMT} Machine Translation Systems for {WMT} 2011,2011,16,19,1,1,5770,holger schwenk,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the development of French--English and English--French statistical machine translation systems for the 2011 WMT shared task evaluation. Our main systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only, but we also performed initial experiments with hierarchical systems. Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words."
I11-1148,Parametric Weighting of Parallel Data for Statistical Machine Translation,2011,8,3,3,1,695,kashif shah,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"During the last years there is increasing interest in methods that perform some kind of weighting of heterogeneous parallel training data when building a statistical machine translation system. It was for instance observed that training data that is close to the period of the test data is more valuable than older data (Hardt and Elming, 2010; Levenberg et al., 2010). In this paper we obtain such a weighting by resampling alignments using weights that decrease with the temporal distance of bitexts to the test set. By these means, we can use all the available bitexts and still put an emphasis on the most recent one. The main idea of our approach is to use a parametric form or meta-weights for the weighting of the different parts of the bitexts. This ensures that our approach has only few parameters to optimize. We report experimental results on the Europarl corpus, translating from French to English and further verified it on the official WMTxe2x80x9911 task, translating from English to French. Our method achieves improvements of about 0.6 points BLEU on the test set with respect to a system trained on data without any weighting."
2011.mtsummit-papers.17,Qualitative Analysis of Post-Editing for High Quality Machine Translation,2011,-1,-1,3,1,3257,frederic blain,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.iwslt-evaluation.10,{LIUM}{'}s systems for the {IWSLT} 2011 speech translation tasks,2011,18,33,4,0.804598,17019,anthony rousseau,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign. We participated in three of the proposed tasks, namely the Automatic Speech Recognition task (ASR), the ASR system combination task (ASR{\_}SC) and the Spoken Language Translation task (SLT), since these tasks are all related to speech translation. We present the approaches and specificities we developed on each task."
W10-1716,{LIUM} {SMT} Machine Translation System for {WMT} 2010,2010,15,7,3,0.742684,23604,patrik lambert,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the development of French--English and English--French machine translation systems for the 2010 WMT shared task evaluation. These systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only. Most of our efforts were devoted to the choice and extraction of bilingual data used for training. We filtered out some bilingual corpora and pruned the phrase table. We also investigated the impact of adding two types of additional bilingual texts, extracted automatically from the available monolingual data. We first collected bilingual data by performing automatic translations of monolingual texts. The second type of bilingual text was harvested from comparable corpora with Information Retrieval techniques."
W10-1759,Translation Model Adaptation by Resampling,2010,16,13,3,1,695,kashif shah,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"The translation model of statistical machine translation systems is trained on parallel data coming from various sources and domains. These corpora are usually concatenated, word alignments are calculated and phrases are extracted. This means that the corpora are not weighted according to their importance to the domain of the translation task. This is in contrast to the training of the language model for which well known techniques are used to weight the various sources of texts. On a smaller granularity, the automatic calculated word alignments differ in quality. This is usually not considered when extracting phrases either.n n In this paper we propose a method to automatically weight the different corpora and alignments. This is achieved with a resampling technique. We report experimental results for a small (IWSLT) and large (NIST) Arabic/English translation tasks. In both cases, significant improvements in the BLEU score were observed."
2010.jeptalnrecital-long.1,Adaptation d{'}un Syst{\\`e}me de Traduction Automatique Statistique avec des Ressources monolingues,2010,-1,-1,1,1,5770,holger schwenk,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,Les performances d{'}un syst{\`e}me de traduction statistique d{\'e}pendent beaucoup de la qualit{\'e} et de la quantit{\'e} des donn{\'e}es d{'}apprentissage disponibles. La plupart des textes parall{\`e}les librement disponibles proviennent d{'}organisations internationales. Le jargon observ{\'e} dans ces textes n{'}est pas tr{\`e}s adapt{\'e} pour construire un syst{\`e}me de traduction pour d{'}autres domaines. Nous pr{\'e}sentons dans cet article une technique pour adapter le mod{\`e}le de traduction {\`a} un domaine diff{\'e}rent en utilisant des textes dans la langue source uniquement. Nous obtenons des am{\'e}liorations significatives du score BLEU dans des syst{\`e}mes de traduction de l{'}arabe vers le fran{\c{c}}ais et vers l{'}anglais.
2010.iwslt-evaluation.4,N-gram-based machine translation enhanced with neural networks,2010,-1,-1,3,0,44220,francisco zamoramartinez,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
W09-3109,Exploiting Comparable Corpora with {TER} and {TER}p,2009,19,3,2,1,44214,sadaf abdulrauf,Proceedings of the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora ({BUCC}),0,"In this paper we present an extension of a successful simple and effective method for extracting parallel sentences from comparable corpora and we apply it to an Arabic/English NIST system. We experiment with a new TERp filter, along with WER and TER filters. We also report a comparison of our approach with that of (Munteanu and Marcu, 2005) using exactly the same corpora and show performance gain by using much lesser data. Our approach employs an SMT system built from small amounts of parallel texts to translate the source side of the non-parallel corpus. The target side texts are used, along with other corpora, in the language model of this SMT system. We then use information retrieval techniques and simple filters to create parallel data from a comparable news corpora. We evaluate the quality of the extracted data by showing that it significantly improves the performance of an SMT systems."
W09-0423,{SMT} and {SPE} Machine Translation Systems for {WMT}{`}09,2009,14,19,1,1,5770,holger schwenk,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,This paper describes the development of several machine translation systems for the 2009 WMT shared task evaluation. We only consider the translation between French and English. We describe a statistical system based on the Moses decoder and a statistical post-editing system using SYSTRAN's rule-based system. We also investigated techniques to automatically extract additional bilingual texts from comparable corpora.
E09-1003,On the Use of Comparable Corpora to Improve {SMT} performance,2009,17,89,2,1,44214,sadaf abdulrauf,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We present a simple and effective method for extracting parallel sentences from comparable corpora. We employ a statistical machine translation (SMT) system built from small amounts of parallel texts to translate the source side of the non-parallel corpus. The target side texts are used, along with other corpora, in the language model of this SMT system. We then use information retrieval techniques and simple filters to create French/English parallel data from a comparable news corpora. We evaluate the quality of the extracted data by showing that it significantly improves the performance of an SMT systems."
2009.mtsummit-posters.17,Translation Model Adaptation for an {A}rabic/{F}rench News Translation System by Lightly- Supervised Training,2009,18,32,1,1,5770,holger schwenk,Proceedings of Machine Translation Summit XII: Posters,0,"Published in MT Summit XII, August 2009 Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domainmonolingual texts to modify the probability distribution of the phrase-table and to learn new task-specific phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair."
2009.iwslt-evaluation.10,{LIUM}{'}s statistical machine translation system for {IWSLT} 2009,2009,12,1,1,1,5770,holger schwenk,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the systems developed by the LIUM laboratory for the 2009 IWSLT evaluation. We participated in the Arabic and Chinese to English BTEC tasks. We developed three different systems: a statistical phrase-based system using the Moses toolkit, an Statistical Post-Editing system and a hierarchical phrase-based system based on Joshua. A continuous space language model was deployed to improve the modeling of the target language. These systems are combined by a confusion network based approach."
W08-0313,First Steps towards a General Purpose {F}rench/{E}nglish Statistical Machine Translation System,2008,12,12,1,1,5770,holger schwenk,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper describes an initial version of a general purpose French/English statistical machine translation system. The main features of this system are the open-source Moses decoder, the integration of a bilingual dictionary and a continuous space target language model. We analyze the performance of this system on the test data of the WMT'08 evaluation."
I08-2089,Large and Diverse Language Models for Statistical Machine Translation,2008,14,36,1,1,5770,holger schwenk,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,This paper presents methods to combine large language models trained from diverse text sources and applies them to a state-ofart Frenchxe2x80x93English and Arabicxe2x80x93English machine translation system. We show gains of over 2 BLEU points over a strong baseline by using continuous space language models in re-ranking.
2008.iwslt-papers.6,Investigations on large-scale lightly-supervised training for statistical machine translation.,2008,17,55,1,1,5770,holger schwenk,Proceedings of the 5th International Workshop on Spoken Language Translation: Papers,0,"Sentence-aligned bilingual texts are a crucial resource to build statistical machine translation (SMT) systems. In this paper we propose to apply lightly-supervised training to produce additional parallel data. The idea is to translate large amounts of monolingual data (up to 275M words) with an SMT system, and to use those as additional training data. Results are reported for the translation from French into English. We consider two setups: first the intial SMT system is only trained with a very limited amount of human-produced translations, and then the case where we have more than 100 million words. In both conditions, lightly-supervised training achieves significant improvements of the BLEU score."
2008.iwslt-evaluation.9,The {LIUM} {A}rabic/{E}nglish statistical machine translation system for {IWSLT} 2008.,2008,19,3,1,1,5770,holger schwenk,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,This paper describes the system developed by the LIUM laboratory for the 2008 IWSLT evaluation. We only participated in the Arabic/English BTEC task. We developed a statistical phrase-based system using the Moses toolkit and SYSTRAN{'}s rule-based translation system to perform a morphological decomposition of the Arabic words. A continuous space language model was deployed to improve the modeling of the target language. Both approaches achieved significant improvements in the BLEU score. The system achieves a score of 49.4 on the test set of the 2008 IWSLT evaluation.
W07-0725,Building a Statistical Machine Translation System for {F}rench Using the {E}uroparl Corpus,2007,8,4,1,1,5770,holger schwenk,Proceedings of the Second Workshop on Statistical Machine Translation,0,"This paper describes the development of a statistical machine translation system based on the Moses decoder for the 2007 WMT shared tasks. Several different translation strategies were explored. We also use a statistical language model that is based on a continuous representation of the words in the vocabulary. By these means we expect to take better advantage of the limited amount of training data. Finally, we have investigated the usefulness of a second reference translation of the development data."
W07-0409,Combining Morphosyntactic Enriched Representation with n-best Reranking in Statistical Translation,2007,13,5,4,0,40028,helene bonneaumaynard,"Proceedings of {SSST}, {NAACL}-{HLT} 2007 / {AMTA} Workshop on Syntax and Structure in Statistical Translation",0,"The purpose of this work is to explore the integration of morphosyntactic information into the translation model itself, by enriching words with their morphosyntactic categories. We investigate word disambiguation using morphosyntactic categories, n-best hypotheses reranking, and the combination of both methods with word or morphosyntactic n-gram language model reranking. Experiments are carried out on the English-to-Spanish translation task. Using the morphosyntactic language model alone does not results in any improvement in performance. However, combining morphosyntactic word disambiguation with a word based 4-gram language model results in a relative improvement in the BLEU score of 2.3% on the development set and 1.9% on the test set."
D07-1045,Smooth Bilingual $N$-Gram Translation,2007,18,34,1,1,5770,holger schwenk,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We address the problem of smoothing translation probabilities in a bilingual N-grambased statistical machine translation system. It is proposed to project the bilingual tuples onto a continuous space and to estimate the translation probabilities in this representation. A neural network is used to perform the projection and the probability estimation. Smoothing probabilities is most important for tasks with a limited amount of training material. We consider here the BTEC task of the 2006 IWSLT evaluation. Improvements in all official automatic measures are reported when translating from Italian to English. Using a continuous space model for the translation model and the target language model, an improvement of 1.5 BLEU on the test data is observed."
2007.mtsummit-papers.18,A state-of-the-art statistical machine translation system based on {M}oses,2007,-1,-1,2,0,47820,daniel dechelotte,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.jeptalnrecital-poster.25,Mod{\\`e}les statistiques enrichis par la syntaxe pour la traduction automatique,2007,0,5,1,1,5770,holger schwenk,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,La traduction automatique statistique par s{\'e}quences de mots est une voie prometteuse. Nous pr{\'e}sentons dans cet article deux {\'e}volutions compl{\'e}mentaires. La premi{\`e}re permet une mod{\'e}lisation de la langue cible dans un espace continu. La seconde int{\`e}gre des cat{\'e}gories morpho-syntaxiques aux unit{\'e}s manipul{\'e}es par le mod{\`e}le de traduction. Ces deux approches sont {\'e}valu{\'e}es sur la t{\^a}che Tc-Star. Les r{\'e}sultats les plus int{\'e}ressants sont obtenus par la combinaison de ces deux m{\'e}thodes.
2007.iwslt-1.26,The {TALP} ngram-based {SMT} system for {IWSLT} 2007,2007,-1,-1,8,0.565179,23604,patrik lambert,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"This paper describes TALPtuples, the 2007 N-gram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the system of previous years. Mainly, these include optimizing alignment parameters in function of translation metric scores and rescoring with a neural network language model. Results on two translation directions are reported, namely from Arabic and Chinese into English, thoroughly explaining all language-related preprocessing and translation schemes."
P06-2093,Continuous Space Language Models for Statistical Machine Translation,2006,25,79,1,1,5770,holger schwenk,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Statistical machine translation systems are based on one or more translation models and a language model of the target language. While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems.n n In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. We consider the translation of European Parliament Speeches. This task is part of an international evaluation organized by the TC-STAR project in 2006. The proposed method achieves consistent improvements in the BLEU score on the development and test data.n n We also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks."
2006.iwslt-papers.2,Continuous space language models for the {IWSLT} 2006 task,2006,14,21,1,1,5770,holger schwenk,Proceedings of the Third International Workshop on Spoken Language Translation: Papers,0,"The language model of the target language plays an important role in statistical machine translation systems. In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary. A neural network is used to perform the projection and the probability estimation. This kind of approach is in particular promising for tasks where a very limited amount of resources are available, like the BTEC corpus of tourism related questions. This language model is used in two state-of-the-art statistical machine translation systems that were developed by UPC for the 2006 IWSLT evaluation campaign: a phraseand an n-gram-based approach. An experimental evaluation for four different language pairs is provided (translation of Mandarin, Japanese, Arabic and Italian to English). The proposed method achieved improvements in the BLEU score of up to 3 points on the development data and of almost 2 points on the official test data."
H05-1026,Training Neural Network Language Models on Very Large Corpora,2005,19,94,1,1,5770,holger schwenk,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-of-the-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time."
