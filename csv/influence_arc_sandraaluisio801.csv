2020.coling-main.512,N16-1120,0,0.017194,"ing all sentences as complex in a text classified as complex can impair the training of methods, especially when these sentences are used to assess the task of predicting sentence complexity. This was demonstrated by (Vajjala and Meurers, 2014b) when investigating the reasons for the low accuracy obtained from the Wikipedia-SimpleWikipedia corpus, used without any sentence alignment method. (Howcroft and Demberg, 2017) and (Singh et al., 2016) also explored the RPSL task using new metrics; the first study exclusively evaluated psycholinguistic metrics and the second one eye-tracking metrics. (Ambati et al., 2016) improved the results significantly by using a Combinatory Categorial Grammar (CCG) parser, and (Gonzalez-Gardu˜no and Søgaard, 2018) achieved state-of-the-art performance in readability prediction for English sentences, using multi-task learning and eye-tracking measures combined with linguistic and psycholinguistic features. In addition, (GonzalezGardu˜no and Søgaard, 2018) compared the performance of readability models that use eye-tracking data of native speakers with models using data from language learners. There was no significant drop in performance when replacing learners with natives"
2020.coling-main.512,D18-1289,0,0.062145,"Missing"
2020.coling-main.512,W14-1820,0,0.0277822,"Missing"
2020.coling-main.512,W11-2308,0,0.0372781,"Missing"
2020.coling-main.512,E17-1090,0,0.0879617,"tence level readability is relevant because approaches to classifying text readability do not bring great advantages to the subsequent application of automatic simplification methods. Furthermore, considering all sentences as complex in a text classified as complex can impair the training of methods, especially when these sentences are used to assess the task of predicting sentence complexity. This was demonstrated by (Vajjala and Meurers, 2014b) when investigating the reasons for the low accuracy obtained from the Wikipedia-SimpleWikipedia corpus, used without any sentence alignment method. (Howcroft and Demberg, 2017) and (Singh et al., 2016) also explored the RPSL task using new metrics; the first study exclusively evaluated psycholinguistic metrics and the second one eye-tracking metrics. (Ambati et al., 2016) improved the results significantly by using a Combinatory Categorial Grammar (CCG) parser, and (Gonzalez-Gardu˜no and Søgaard, 2018) achieved state-of-the-art performance in readability prediction for English sentences, using multi-task learning and eye-tracking measures combined with linguistic and psycholinguistic features. In addition, (GonzalezGardu˜no and Søgaard, 2018) compared the performanc"
2020.coling-main.512,C18-1034,1,0.884374,"Missing"
2020.coling-main.512,N19-5004,0,0.0236439,"rning and eye-tracking measures. An example of an application for the sentence level approach is the complexity checker tool, proposed by (Scarton et al., 2017) that analyses all sentences in a text, highlighting the complex ones to help with the simplification process. This paper presents a thorough evaluation of sentence readability prediction for Brazilian Portuguese (BP), starting by evaluating single-task methods, followed by a replication of the work developed by (Gonzalez-Gardu˜no and Søgaard, 2018). At the end, we propose a new model based on the sequential transfer learning approach (Ruder et al., 2019), which has achieved state-of-the-art performance in readability prediction of BP sentences. Section 2 presents a literature review of the main works in readability prediction at sentence level (RPSL). Section 3 describes the corpora and metrics used and Section 4 presents the models evaluated and experimental results. Section 5 presents an analysis of the main errors of our best model, followed by a revision of the evaluation dataset and the results of our final best model. Section 6 draws the conclusions and proposes future research. 2 Readability Prediction at Sentence Level The first studi"
2020.coling-main.512,P18-2113,0,0.0452627,"Missing"
2020.coling-main.512,N10-2011,1,0.826014,"Missing"
2020.coling-main.512,I17-3007,0,0.0279807,"nformation to be able to infer and analyse its complexity. Although the same approach can be used to assess the complexity of texts at the sentence level, (Dell’Orletta et al., 2014) demonstrated that a greater number of features are needed for readability prediction at the sentence level. A study conducted by (Gonzalez-Gardu˜no and Søgaard, 2018) has achieved state-of-the-art performance in readability prediction for English sentences, using multi-task learning and eye-tracking measures. An example of an application for the sentence level approach is the complexity checker tool, proposed by (Scarton et al., 2017) that analyses all sentences in a text, highlighting the complex ones to help with the simplification process. This paper presents a thorough evaluation of sentence readability prediction for Brazilian Portuguese (BP), starting by evaluating single-task methods, followed by a replication of the work developed by (Gonzalez-Gardu˜no and Søgaard, 2018). At the end, we propose a new model based on the sequential transfer learning approach (Ruder et al., 2019), which has achieved state-of-the-art performance in readability prediction of BP sentences. Section 2 presents a literature review of the ma"
2020.coling-main.512,L18-1553,0,0.0617986,"Missing"
2020.coling-main.512,W16-4123,0,0.0197427,"nt because approaches to classifying text readability do not bring great advantages to the subsequent application of automatic simplification methods. Furthermore, considering all sentences as complex in a text classified as complex can impair the training of methods, especially when these sentences are used to assess the task of predicting sentence complexity. This was demonstrated by (Vajjala and Meurers, 2014b) when investigating the reasons for the low accuracy obtained from the Wikipedia-SimpleWikipedia corpus, used without any sentence alignment method. (Howcroft and Demberg, 2017) and (Singh et al., 2016) also explored the RPSL task using new metrics; the first study exclusively evaluated psycholinguistic metrics and the second one eye-tracking metrics. (Ambati et al., 2016) improved the results significantly by using a Combinatory Categorial Grammar (CCG) parser, and (Gonzalez-Gardu˜no and Søgaard, 2018) achieved state-of-the-art performance in readability prediction for English sentences, using multi-task learning and eye-tracking measures combined with linguistic and psycholinguistic features. In addition, (GonzalezGardu˜no and Søgaard, 2018) compared the performance of readability models t"
2020.coling-main.512,E14-1031,0,0.153159,"eedings of the 28th International Conference on Computational Linguistics, pages 5821–5831 Barcelona, Spain (Online), December 8-13, 2020 for classes (Davison and Green, 1988) (Bohn, 1990). At that time, it was considered that complexity could be inferred by surface-level metrics of words and sentences, based on the frequency and size (number of letters) of the words and on the average number of words per sentence. Since then, readability analysis has become a large area of multidisciplinary research, which has an ever growing body of literature, related tasks (e.g., text simplification task (Vajjala and Meurers, 2014a) and text summarization task (Vodolazova and Lloret, 2019)), and has gained new computational approaches in this century using Natural Language Processing (NLP) and Machine Learning methods (Collins-Thompson, 2014). Traditionally, the readability assessment task has been applied to the text level, assigning a grade (or level of proficiency ranking) for an entire document. However, in a document classified as simple, complex sentences can occur, just as there are simple sentences in a complex document. A sentence is an important unit that provides, in most cases, enough information to be able"
2020.coling-main.512,R19-1145,0,0.0184832,"onal Linguistics, pages 5821–5831 Barcelona, Spain (Online), December 8-13, 2020 for classes (Davison and Green, 1988) (Bohn, 1990). At that time, it was considered that complexity could be inferred by surface-level metrics of words and sentences, based on the frequency and size (number of letters) of the words and on the average number of words per sentence. Since then, readability analysis has become a large area of multidisciplinary research, which has an ever growing body of literature, related tasks (e.g., text simplification task (Vajjala and Meurers, 2014a) and text summarization task (Vodolazova and Lloret, 2019)), and has gained new computational approaches in this century using Natural Language Processing (NLP) and Machine Learning methods (Collins-Thompson, 2014). Traditionally, the readability assessment task has been applied to the text level, assigning a grade (or level of proficiency ranking) for an entire document. However, in a document classified as simple, complex sentences can occur, just as there are simple sentences in a complex document. A sentence is an important unit that provides, in most cases, enough information to be able to infer and analyse its complexity. Although the same appr"
2020.lrec-1.317,L16-1103,0,0.0287289,"sentence boundary. For instance, the CRF method proposed by (Fraser et al., 2015) uses lexical, prosodic and Part-of-Speech (PoS) tags as features to segment speech from elder people with aphasia. They found that by using all these features together the model yields better results and the mistakes made by the model don’t affect much the syntactic structure of the segmented transcript. More recently, Recurrent and Convolutional Neural Networks were employed for both types of speech and achieved good results by using word embeddings as the lexical representation of words (Tilk and Alumäe, 2016; Che et al., 2016; Treviso et al., 2017a; González-Gallardo and Torres-Moreno, 2018), suggesting that deep neural networks can be successfully applied for this task. Prosodic features have been shown to be very effective to discriminate between different types of sentence boundaries and in general their usage reflects better results (Shriberg et al., 2009; Huang et al., 2014; Khomitsevich et al., 2015). However, to put prosodic features into practice we need alignments between the audio and its transcription, which is hard to obtain mainly due to the low quality of the recordings. This problem is even more cri"
2020.lrec-1.317,N19-1423,0,0.040869,"Missing"
2020.lrec-1.317,N15-1087,0,0.246114,"procedure is also used in the Arizona Battery for Communication Disorders of Dementia (ABCD) (Bayles and Tomoeda, 1993). The single-scene description task called “The Cookie Theft Picture” is part of the Boston Diagnostic Aphasia Examination (BDAE) (Goodglass et al., 1983). The Cookie-Theft picture has been shown to be clinically relevant in identifying linguistic deficits in Alzheimer’s disease patients given the importance of using visual stimuli when evaluating individuals of this group. The Cinderella story is also very widely used in the assessment of aphasia and some types of dementia. (Fraser et al., 2015) and (Aluisio et al., 2016) based their work on the story of Cinderella. Participants were given a sequenced picture book (without words) to remind them of the story; then they were asked to tell the story in their own words. The narrative samples were then transcribed by trained annotators. A challenge in choosing the type of neuropsychological assessment of individuals with AD and MCI is the use of a battery that can distinguish these individuals. ABCD appears as an option since it is capable of detecting mild stage AD, while the Bateria de Avaliação da Linguagem no Envelhecimento (BALE) (Ba"
2020.lrec-1.317,W16-0301,0,0.0141602,"h transcriptions, along with an ideal combination of datasets and specific groups of narratives to be used as the training set. Keywords: Sentence Segmentation, Impaired Speech, Neuropsychological Language Tests 1. Introduction Language assessment has been shown to be an efficient complementary tool for detecting cognitive and neuropsychological disorders, therefore present in most tests, tasks and batteries that evaluate cognitive processes. For example, neuropsychological language tests are an important tool for diagnosing individuals with significant depression in Alzheimer’s disease (AD) (Fraser et al., 2016), to differentiate between Mild Cognitive Impairment (MCI) and AD (Drummond et al., 2015), to differentiate between AD and other neurodegenerative dementias (Yancheva et al., 2015; Beltrami et al., 2018) and to differentiate variants of neurodegenerative dementias, such as in Primary Progressive Aphasia (PPA) (Fraser et al., 2014). Language assessment has been performed mainly by using discursive production in which narratives are largely used, since they are a natural form of communication and favor the observation of the patient’s functionality in everyday life (Tillas, 2015). The discourse"
2020.lrec-1.317,P18-1031,0,0.0483347,"Missing"
2020.lrec-1.317,D15-1166,0,0.00967877,"la Story dataset. The choice of evaluating only on the MCI class was due to the high demanding time of running all experiments in all four datasets and the fact that this class represents cognitive impairment between the characteristics of Controls and ADs narratives. As we can see in Table 3, the models explore the use of CNNs, RNNs, QRNNs, different variants of attention mechanisms, and CRF; mixed models with two or more combinations were also explored. The dot product attention is the scaled version proposed by (Vaswani et al., 2017); the general attention is also known as Luong attention (Luong et al., 2015); the additive attention is also known as Bahdanau attention (Bahdanau et al., 2015). Hyperparameters Values Conv. filters Kernel size Conv. dropout Recurrent hidden size Recurrent type Recurrent dropout Attention dropout Attention variant Attention hidden size Number of heads Multi-head hidden size 35, 50, 100, 200 1, 3, 5, 7 0.0, 0.25, 0.5, 0.75 35, 50, 100, 200 RNN, GRU, LSTM, QRNN 0.0, 0.5 0.0, 0.25, 0.5, 0.75 Dot Product, General, Additive 35, 50, 100, 200, 300 1, 2, 4 50, 100, 200 Table 3: Hyperparameters tried during greedy search. We trained our models for a maximum of 40 epochs using"
2020.lrec-1.317,N18-1202,0,0.0258302,"Missing"
2020.lrec-1.317,W17-6618,1,0.300403,"voltou para casa . chegando em casa toca o telefone . era uma garotinha avisando ela que que tinha achado a carteira . é isso . tem mais coisa . não cortei . eu resumi o que eu ouvi . Figure 1: (a) Narrative transcribed where there is no punctuation or capitalization, besides presenting several disfluencies, such as unlexicalized filled pauses, restarts and patient’s comments, shown in bold. (b) Narrative manually segmented of a retelling task using The Wallet Story. evaluated with the other three datasets analyzed in this paper, F1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1). The average of F1 in the three datasets, for all classes, is 0.59. In the original evaluation with training and cross-validation testing in the same dataset, the best F1 value for Controls was 0.76, for MCIs, 0.74, and for ADs, 0.66. However, Table 1 shows that, in general, for sentence segmentation, more data is beneficial, independently of task and topic of datasets. Given this motivation scenario, where the main goal was to develop a robust and generic segmentation system for narratives of neuropsychological language tests, the present study tries to answer three questions: 1."
2020.lrec-1.317,E17-1030,1,0.282529,"voltou para casa . chegando em casa toca o telefone . era uma garotinha avisando ela que que tinha achado a carteira . é isso . tem mais coisa . não cortei . eu resumi o que eu ouvi . Figure 1: (a) Narrative transcribed where there is no punctuation or capitalization, besides presenting several disfluencies, such as unlexicalized filled pauses, restarts and patient’s comments, shown in bold. (b) Narrative manually segmented of a retelling task using The Wallet Story. evaluated with the other three datasets analyzed in this paper, F1 values were much lower than the original work on Cinderella (Treviso et al., 2017a) (Table 1). The average of F1 in the three datasets, for all classes, is 0.59. In the original evaluation with training and cross-validation testing in the same dataset, the best F1 value for Controls was 0.76, for MCIs, 0.74, and for ADs, 0.66. However, Table 1 shows that, in general, for sentence segmentation, more data is beneficial, independently of task and topic of datasets. Given this motivation scenario, where the main goal was to develop a robust and generic segmentation system for narratives of neuropsychological language tests, the present study tries to answer three questions: 1."
2020.lrec-1.317,W15-5123,0,0.0209183,"Neuropsychological Language Tests 1. Introduction Language assessment has been shown to be an efficient complementary tool for detecting cognitive and neuropsychological disorders, therefore present in most tests, tasks and batteries that evaluate cognitive processes. For example, neuropsychological language tests are an important tool for diagnosing individuals with significant depression in Alzheimer’s disease (AD) (Fraser et al., 2016), to differentiate between Mild Cognitive Impairment (MCI) and AD (Drummond et al., 2015), to differentiate between AD and other neurodegenerative dementias (Yancheva et al., 2015; Beltrami et al., 2018) and to differentiate variants of neurodegenerative dementias, such as in Primary Progressive Aphasia (PPA) (Fraser et al., 2014). Language assessment has been performed mainly by using discursive production in which narratives are largely used, since they are a natural form of communication and favor the observation of the patient’s functionality in everyday life (Tillas, 2015). The discourse tasks used to assess the narrative productions of elder individuals are often based on: (i) an illustrated story book without a text (e.g. Cinderella), (ii) an immediate and delay"
C18-1034,W10-1607,1,0.851188,"cause as much impact as they potentially might. Identifying which sentences of a text are more complex may help writers of newsletters, manuals and instructions, for example, to adequate their texts to their audiences. Among the solutions to alleviate this problem is the simplification or adaptation of complex texts, a task that has been partially or fully automatized by Natural Language Processing (NLP) applications. For Brazilian Portuguese, various applications, methods and resources aiming to support simplification in several levels of readability were developed in the Project PorSimples (Aluísio and Gasperin, 2010). Among these resources there is a parallel and aligned corpus with two levels of simplification and annotated simplification operations (Caseli et al., 2009). PorSimples corpus has been used to train readability classifiers for texts (Scarton et al., 2010). Table 1 shows examples of an original sentence of PorSimples corpus (O), its natural simplification (N) and its strong simplification (S). The natural simplification had a substitution of “Uma parcela” by “Alguns” and the strong simplification, shorter than the natural, had a clause removed. (O) Uma parcela critica o uniforme, porque acred"
C18-1034,N16-1120,0,0.251843,"tence-level readability methods to predict which sentences of a text the students will struggle to read. Furthermore, Open Educational Resources repositories Wiley et 1 http://www.ibge.gov.br/home/estatistica/economia/agropecuaria/censoagro/ 402 al. (2014) may also take profit of such methods in order to return not merely relevant educational resources, but documents appropriate to the reading level of the user. Due to its several applications, sentential complexity has been a focus of interest in the NLP studies in recent years, such as Vajjala and Meurers (2014), Vajjala and Meurers (2016), Ambati et al. (2016), Singh et al. (2016), Howcroft and Demberg (2017), Gonzalez-Garduño and Søgaard (2017). The lack of a sentence-based corpus annotated with regards to readability is a major obstacle to research in this area for Portuguese. Even the English language suffers some drawbacks in what concerns the evaluation of sentential complexity. One of them is the use of benchmarks built from adapted corpora which are automatically aligned, such as Wikipedia and Simple Wikipedia (Zhu et al., 2010). This corpus has some problems to be used as benchmark for text simplification which also prevents its use for the"
C18-1034,C12-3044,0,0.0230105,"Missing"
C18-1034,W11-2308,0,0.23522,"Missing"
C18-1034,ganitkevitch-callison-burch-2014-multilingual,0,0.0194688,"ces that may be used to design new metrics to deal with similar words and paraphases. For Portuguese, there are different similar projects of wordnets, among which stand out the OpenWordNet-PT (de Paiva et al., 2012), as the most complete with manual revision, and the CONTO.PT (Gonçalo Oliveira, 2016), built semiautomatically in order to comprise a greater number of words, and which describes itself as a diffuse wordnet. There is also the PPDB (Paraphrase Database), a resource that contains paraphrases in several languages, including Portuguese, automatically extracted from bilingual corpora (Ganitkevitch and Callison-Burch, 2014). Paraphrase in the context of PPDB refers to expressions or equivalent words. As it was generated automatically, the PPDB also contains some false positives. The resource is available in six different sizes: the difference is that larger sets extracted paraphrase rules with less confidence. For features other than the lexical ones, a very promising research avenue is to test simplified sentences with human readers to confirm whether they are simpler than their original counterparts or not (using eye-trackers). This is relevant because many simplification operations we use are inspired in the"
C18-1034,W17-5050,0,0.0905598,"students will struggle to read. Furthermore, Open Educational Resources repositories Wiley et 1 http://www.ibge.gov.br/home/estatistica/economia/agropecuaria/censoagro/ 402 al. (2014) may also take profit of such methods in order to return not merely relevant educational resources, but documents appropriate to the reading level of the user. Due to its several applications, sentential complexity has been a focus of interest in the NLP studies in recent years, such as Vajjala and Meurers (2014), Vajjala and Meurers (2016), Ambati et al. (2016), Singh et al. (2016), Howcroft and Demberg (2017), Gonzalez-Garduño and Søgaard (2017). The lack of a sentence-based corpus annotated with regards to readability is a major obstacle to research in this area for Portuguese. Even the English language suffers some drawbacks in what concerns the evaluation of sentential complexity. One of them is the use of benchmarks built from adapted corpora which are automatically aligned, such as Wikipedia and Simple Wikipedia (Zhu et al., 2010). This corpus has some problems to be used as benchmark for text simplification which also prevents its use for the sentential complexity task, for example, automatic sentence alignment errors, inadequa"
C18-1034,E17-1090,0,0.615807,"which sentences of a text the students will struggle to read. Furthermore, Open Educational Resources repositories Wiley et 1 http://www.ibge.gov.br/home/estatistica/economia/agropecuaria/censoagro/ 402 al. (2014) may also take profit of such methods in order to return not merely relevant educational resources, but documents appropriate to the reading level of the user. Due to its several applications, sentential complexity has been a focus of interest in the NLP studies in recent years, such as Vajjala and Meurers (2014), Vajjala and Meurers (2016), Ambati et al. (2016), Singh et al. (2016), Howcroft and Demberg (2017), Gonzalez-Garduño and Søgaard (2017). The lack of a sentence-based corpus annotated with regards to readability is a major obstacle to research in this area for Portuguese. Even the English language suffers some drawbacks in what concerns the evaluation of sentential complexity. One of them is the use of benchmarks built from adapted corpora which are automatically aligned, such as Wikipedia and Simple Wikipedia (Zhu et al., 2010). This corpus has some problems to be used as benchmark for text simplification which also prevents its use for the sentential complexity task, for example, automati"
C18-1034,N15-1022,0,0.191314,"Missing"
C18-1034,L18-1685,0,0.024596,"ave several positive points — the use of news articles which generalize better for other genres, not having sentence length as high predictive feature, as well as being available by requisition — but also can suffer from errors generated by automatic alignment. Newsela parallel corpus (cf. (Xu et al., 2015)), composed of news articles rewritten by professional editors to be read for children at multiple grade levels, is very beneficial for studying text simplification and could serve as benchmark for sentential complexity if the resulting sentence corpus could be publicly available. Moreover, Scarton et al. (2018) made available the SimPA, an English sentence level corpus for the Public Administration domain with 1,100 original sentences simplified in the lexical (3,300 pairs) and syntactic levels (another 1,100 pairs), annotated by 176 volunteers. In this paper, we aim at obtaining nontrivial sentence pairs in Portuguese in order to create a gold standard corpus, publicly available. By nontrivial we mean that the pairs are not significantly different in length to avoid the easy judgment that the shorter sentences are the simpler ones. Although it is natural to expect that the simplified sentences are"
C18-1034,W16-4123,0,0.206759,"y methods to predict which sentences of a text the students will struggle to read. Furthermore, Open Educational Resources repositories Wiley et 1 http://www.ibge.gov.br/home/estatistica/economia/agropecuaria/censoagro/ 402 al. (2014) may also take profit of such methods in order to return not merely relevant educational resources, but documents appropriate to the reading level of the user. Due to its several applications, sentential complexity has been a focus of interest in the NLP studies in recent years, such as Vajjala and Meurers (2014), Vajjala and Meurers (2016), Ambati et al. (2016), Singh et al. (2016), Howcroft and Demberg (2017), Gonzalez-Garduño and Søgaard (2017). The lack of a sentence-based corpus annotated with regards to readability is a major obstacle to research in this area for Portuguese. Even the English language suffers some drawbacks in what concerns the evaluation of sentential complexity. One of them is the use of benchmarks built from adapted corpora which are automatically aligned, such as Wikipedia and Simple Wikipedia (Zhu et al., 2010). This corpus has some problems to be used as benchmark for text simplification which also prevents its use for the sentential complexit"
C18-1034,E14-1031,0,0.144877,"ed language learning (CALL) systems can benefit from sentence-level readability methods to predict which sentences of a text the students will struggle to read. Furthermore, Open Educational Resources repositories Wiley et 1 http://www.ibge.gov.br/home/estatistica/economia/agropecuaria/censoagro/ 402 al. (2014) may also take profit of such methods in order to return not merely relevant educational resources, but documents appropriate to the reading level of the user. Due to its several applications, sentential complexity has been a focus of interest in the NLP studies in recent years, such as Vajjala and Meurers (2014), Vajjala and Meurers (2016), Ambati et al. (2016), Singh et al. (2016), Howcroft and Demberg (2017), Gonzalez-Garduño and Søgaard (2017). The lack of a sentence-based corpus annotated with regards to readability is a major obstacle to research in this area for Portuguese. Even the English language suffers some drawbacks in what concerns the evaluation of sentential complexity. One of them is the use of benchmarks built from adapted corpora which are automatically aligned, such as Wikipedia and Simple Wikipedia (Zhu et al., 2010). This corpus has some problems to be used as benchmark for text"
C18-1034,Q15-1021,0,0.0237177,"for Portuguese. Even the English language suffers some drawbacks in what concerns the evaluation of sentential complexity. One of them is the use of benchmarks built from adapted corpora which are automatically aligned, such as Wikipedia and Simple Wikipedia (Zhu et al., 2010). This corpus has some problems to be used as benchmark for text simplification which also prevents its use for the sentential complexity task, for example, automatic sentence alignment errors, inadequate simplifications generating sentences which are not simple, and poor generalization for other genre than encyclopedia (Xu et al., 2015). Other benchmarks for sentential complexity, such as OneStopEnglish corpus (Vajjala and Meurers, 2016), have several positive points — the use of news articles which generalize better for other genres, not having sentence length as high predictive feature, as well as being available by requisition — but also can suffer from errors generated by automatic alignment. Newsela parallel corpus (cf. (Xu et al., 2015)), composed of news articles rewritten by professional editors to be read for children at multiple grade levels, is very beneficial for studying text simplification and could serve as be"
C18-1034,C10-1152,0,0.0976633,"of interest in the NLP studies in recent years, such as Vajjala and Meurers (2014), Vajjala and Meurers (2016), Ambati et al. (2016), Singh et al. (2016), Howcroft and Demberg (2017), Gonzalez-Garduño and Søgaard (2017). The lack of a sentence-based corpus annotated with regards to readability is a major obstacle to research in this area for Portuguese. Even the English language suffers some drawbacks in what concerns the evaluation of sentential complexity. One of them is the use of benchmarks built from adapted corpora which are automatically aligned, such as Wikipedia and Simple Wikipedia (Zhu et al., 2010). This corpus has some problems to be used as benchmark for text simplification which also prevents its use for the sentential complexity task, for example, automatic sentence alignment errors, inadequate simplifications generating sentences which are not simple, and poor generalization for other genre than encyclopedia (Xu et al., 2015). Other benchmarks for sentential complexity, such as OneStopEnglish corpus (Vajjala and Meurers, 2016), have several positive points — the use of news articles which generalize better for other genres, not having sentence length as high predictive feature, as"
dayrell-etal-2012-rhetorical,W06-3309,0,\N,Missing
dayrell-etal-2012-rhetorical,P06-4011,0,\N,Missing
dayrell-etal-2012-rhetorical,J96-2004,0,\N,Missing
dayrell-etal-2012-rhetorical,I08-1050,0,\N,Missing
dayrell-etal-2012-rhetorical,W08-0908,0,\N,Missing
duran-aluisio-2012-propbank,D07-1002,0,\N,Missing
duran-aluisio-2012-propbank,burchardt-etal-2006-salto,0,\N,Missing
duran-aluisio-2012-propbank,P98-1013,0,\N,Missing
duran-aluisio-2012-propbank,C98-1013,0,\N,Missing
duran-aluisio-2012-propbank,P02-1031,0,\N,Missing
duran-aluisio-2012-propbank,J05-1004,0,\N,Missing
duran-aluisio-2012-propbank,choi-etal-2010-propbank,0,\N,Missing
duran-aluisio-2012-propbank,palmer-etal-2008-pilot,0,\N,Missing
duran-aluisio-2012-propbank,aldezabal-etal-2010-building,0,\N,Missing
duran-aluisio-2012-propbank,choi-etal-2010-propbank-instance,0,\N,Missing
duran-aluisio-2012-propbank,duran-etal-2010-assigning,1,\N,Missing
duran-aluisio-2012-propbank,afonso-etal-2002-floresta,0,\N,Missing
duran-etal-2010-assigning,burchardt-etal-2006-salto,0,\N,Missing
duran-etal-2010-assigning,P98-1013,0,\N,Missing
duran-etal-2010-assigning,C98-1013,0,\N,Missing
E17-1030,L16-1103,0,0.359867,"F based methods to identify word boundaries in speech corpora datasets, more specifically on English broadcast news data and English conversational speech (lecture recordings), respectively. Khomitsevich et al. (2015), similar to our work, used a combination of two models, one based on Support Vector Machines to deal with prosodic information, and other based on CRF to deal with lexical information. They combine the two models using a logistic regression classifier. Xu et al. (2014) uses a combination of CRF and a Deep neural network (DNN) to detect sentence boundaries on broadcast news data. Che et al. (2016) uses two different convolutional neural network (CNN), one which moves in only one dimension and another which moves in two. They achieved good results on a TED talks dataset. Tilk and Alum¨ae (2015) use a recurrent neural network (RNN) with long short-term memory units to restore punctuation in speech transcripts from broadcast news and conversations. Although there are proposed methods for sentence segmentation of Portuguese datasets (Silla Jr. and Kaestner, 2004; Batista and Mamede, 2011; L´opez and Pardo, 2015), none of them are used for transcriptions produced in a clinical setting for t"
E17-1030,D14-1181,0,0.00317244,", and Wy are the connection weights, by and bh are bias vectors, and f and g are non-linear functions. Here, we use a special unit known as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), which is able to learn over long dependencies between words by a purpose-built memory cell. Figure 2 shows a single LSTM memory cell. 5.2.2 Convolutional and pooling layer Once we have a matrix formed by the features of the words in the text, the convolutional layer receives it, which, in turn, is responsible for the automatic extraction of nf new features depending on hc neighboring words (Kim, 2014). The convolutional layer produces a new feature cj by applying a filter W ∈ Rhc ·d to a window of hc words xj−hc +1:j in a sentence with length m: cj = f (W x(j−hc +1):j + b), (3) Figure 2: Diagram of a LSTM memory cell. hc ≤ j ≤ m (2) The LSTM updates for time steps t are done as described by Jozefowicz et al. (2015), which is a slight simplification of the one described by Graves and Jailty (2014), where the memory cell is implemented as follows: Where b ∈ R represents a bias term and f is a non-linear function. Our convolutional layer simply moves one dimension vertically, making one step"
E17-1030,N15-1087,0,0.161716,"anscripts in order to calculate the number of recalled elements. The evaluation of narrative discourse production from the standpoint of linguistic impairment is an attractive alternative as it allows for linguistic microstructure analysis, including phonetic-phonological, morphosyntactic and semantic-lexical components, as well as semanticpragmatic macrostructures. Automated discourse analysis tools based on Natural Language Processing (NLP) resources and tools aiming at the diagnosis of language-impairing dementias via machine learning methods are already available for the English language (Fraser et al., 2015b; Yancheva et al., 2015; Roark et al., 2011) and also for Brazilian Portuguese (BP) (Alu´ısio et al., 2016). The latter study used a publicly available tool, Coh-Metrix-Dementia1 , to extract 73 textual metrics of narrative transcripts, comprising several levels of linguistic analysis from word counts to semantics and discourse. However, the absence of sentence boundary segmentation in transcripts prevents the direct application of NLP methods that rely on these marks in order for the tools to function properly. To our knowledge, only one study evaluating automatic sentence segmentation in En"
E17-1030,W15-5123,0,0.0903028,"calculate the number of recalled elements. The evaluation of narrative discourse production from the standpoint of linguistic impairment is an attractive alternative as it allows for linguistic microstructure analysis, including phonetic-phonological, morphosyntactic and semantic-lexical components, as well as semanticpragmatic macrostructures. Automated discourse analysis tools based on Natural Language Processing (NLP) resources and tools aiming at the diagnosis of language-impairing dementias via machine learning methods are already available for the English language (Fraser et al., 2015b; Yancheva et al., 2015; Roark et al., 2011) and also for Brazilian Portuguese (BP) (Alu´ısio et al., 2016). The latter study used a publicly available tool, Coh-Metrix-Dementia1 , to extract 73 textual metrics of narrative transcripts, comprising several levels of linguistic analysis from word counts to semantics and discourse. However, the absence of sentence boundary segmentation in transcripts prevents the direct application of NLP methods that rely on these marks in order for the tools to function properly. To our knowledge, only one study evaluating automatic sentence segmentation in English transcripts of eld"
hartmann-etal-2014-large,C10-1030,0,\N,Missing
hartmann-etal-2014-large,C10-3015,0,\N,Missing
hartmann-etal-2014-large,W09-2112,0,\N,Missing
hartmann-etal-2014-large,P11-2013,0,\N,Missing
hartmann-etal-2014-large,J06-3001,0,\N,Missing
hartmann-etal-2014-large,P11-1038,0,\N,Missing
hartmann-etal-2014-large,J96-2004,0,\N,Missing
hartmann-etal-2014-large,W11-4523,0,\N,Missing
N10-2011,W09-2105,1,0.768735,"Missing"
P17-1118,W14-3204,0,0.275317,"of linguistic microstructures, including phonetic-phonological, morphosyntactic and semantic-lexical components, as well as semantic-pragmatic macrostructures. Automated discourse analysis based on Natural Language Processing (NLP) resources and tools to diagnose dementias via machine learning methods has been used for English language (Lehr et al., 1284 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1284–1296 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1118 2012; Jarrold et al., 2014; Orimaye et al., 2014; Fraser et al., 2015; Davy et al., 2016) and for Brazilian Portuguese (Alu´ısio et al., 2016). A variety of features are required for this analysis, including Part-of-Speech (PoS), syntactic complexity, lexical diversity and acoustic features. Producing robust tools to extract these features is extremely difficult because speech transcripts used in neuropsychological evaluations contain disfluencies (repetitions, revisions, paraphasias) and patient’s comments about the task being evaluated. Another problem in using linguistic knowledge is the high dependence on manually"
P17-1118,E17-1030,1,0.862404,"Missing"
P17-1118,P16-2030,0,0.0304518,"Missing"
S15-1026,afonso-etal-2002-floresta,0,0.161961,"Missing"
S15-1026,P98-1013,0,0.41852,"Missing"
S15-1026,burchardt-etal-2006-salto,0,0.0280688,"the frame file could be filled in with information from English Propbank, which ones could be filled in with information from Propbank-Br and which fields would require new information, not available in any one of the existing resources. The idea was to 4http://verbs.colorado.edu/~mpalmer/projects/ace/PBguideline s.pdf 5 http://verbs.colorado.edu/~mpalmer/projects/ace/ FramingGuidelines.pdf 218 add the extra information required to the corpus Propbank-Br. Aiming this, we created six “word tags” in corpus Propbank-Br, using the same annotation tool used to annotate the original corpus (SALTO – Burchardt et al. 2006), as may be seen in Fig.1. Figure 1. Extra annotation inserted in Propbank-Br. The word tags are: (1) PB-roleset: an equivalent roleset-id in Propbank, was used as field key to bring, from Propbank, the semantic roles, the semantic roles description, the related Verbnet classes and the Verbnet roles to the framefiles (Fig. 2); (2) t-glosa: field that was filled in only in the first occurrence of a verb sense; it contains a brief description or a synonym of this sense of the verb to distinguish it from the other possible senses. (3) Nota (note): field used for observations regarding a roleset c"
S15-1026,J96-2004,0,0.0222258,"mantic role labels. Cornerstone frame files editor is being used for this task. 3 Evaluation The two strategies we reported to automatically generate Portuguese frame files gave us 2598 framefiles. The 541 frame files already revised 220 correspond to the verbs with frequency above 1000 in the corpus PLN-Br, which include the most polysemous verbs in Portuguese. Such verbs were target of a double-blind annotation task of 8345 instances extracted from the same corpus. The annotation task has just been accomplished and will be fully reported in a later date; the Kappa inter annotator agreement (Carletta, 1996) for verb sense identification was 0.93. This annotation task gave us feedback to evaluate and improve the respective frame files. Among the actions taken during the annotation task we can cite: adding new senses identified in the corpus; merging or splitting senses for verbs that presented low inter-annotator agreement; including new examples to better illustrate a verb sense. 4 Concluding Remarks and Future work The approach we adopted to build a Propbank-like lexical resource to support SRL in Brazilian Portuguese may be of use for other researchers working on under-represented languages an"
S15-1026,choi-etal-2010-propbank,0,0.0340033,"Missing"
S15-1026,duran-aluisio-2012-propbank,1,0.836945,"Verb-Index3, the system that gives access to Propbank’s frame files, may observe that Propbank has been improved over the years, incorporating evidence provided by continuous annotation experience. In a project with limited budget and time, it is natural to think about reusing existing resources in order to maximize the results. In this paper, we report the strategies used to build a lexical resource to support SRL in Portuguese (hereafter referred as Verbo-Brasil), profiting from the English resource developed within the Propbank project and of annotated instances of the corpus Propbank-Br (Duran and Aluísio, 2012). The remainder of this paper is organized as follows. Section 2 explains the strategies used in minimizing the efforts towards the construction of frame files; Section 3 briefly addresses an extrinsic evaluation of Verbo-Brasil obtained from a particular SRL annotation task. Finally, in Section 4, we present our conclusions and future work. 2 Methodology Initially, we intended to construct Verbo-Brasil by manually creating frame files for the 1000 most frequent verbs in Portuguese, using the editor of frame files Cornerstone (Choi et. al. 2010), developed within the Propbank project. We envis"
S15-1026,J05-1004,0,0.310556,"d, for this reason, the first difficult decision in a project of SRL is to choose which set to adopt. No matter which set is used, it is not always easy to decide which label to assign to each argument during the annotation task. In order to facilitate such decision, some projects of SRL developed lexical resources that predict the set of semantic roles required by each predicate. Some of such resources define semantic roles for verb classes, as Verbnet (Kipper et al. 2006); others for semantic frames, as Framenet (Baker et al. 1998); others define semantic roles for verb senses, as Propbank (Palmer et al. 2005) or for predicate nouns, as Nombank (Meyers et al, 2004). The more detailed and clear is the lexical resource, the easier the decision about which role label to assign during a manual annotation task. This is very important, because when we ease SRL annotation, we increase the likelihood of obtaining a high inter-annotator agreement and, consequently, the likelihood of obtaining a good precision for machine learning classifiers for the task. 216 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 216–221, Denver, Colorado, June 4–5, 2015. Among"
S15-1026,W04-2705,0,\N,Missing
S15-1026,C98-1013,0,\N,Missing
sepulveda-torres-etal-2014-generating,C10-1030,0,\N,Missing
sepulveda-torres-etal-2014-generating,C04-1137,0,\N,Missing
sepulveda-torres-etal-2014-generating,W09-2112,0,\N,Missing
sepulveda-torres-etal-2014-generating,P11-1092,0,\N,Missing
sepulveda-torres-etal-2014-generating,W11-4508,1,\N,Missing
sepulveda-torres-etal-2014-generating,P00-1056,0,\N,Missing
W09-2105,W03-1602,0,0.0579795,"potential change of meaning. The writer then chooses their preferred simplification. This system ensures accurate output, but requires human intervention at every step. Our system, on the other hand, is autonomous, even though the user is able to undo any undesirable simplification or to choose alternative simplifications. These alternative simplifications may be produced in two cases: i) to compose a new subject in simplifications involving relatives and appositions and ii) to choose among one of the coordinate or subordinate simplifications when there is ambiguity regarding to conjunctions. Inui et al. (2003) proposes a rule-based system for text simplification aimed at deaf people. The authors create readability assessments based on questionnaires answered by teachers about the deaf. With approximately one thousand manually created rules, the authors generate several paraphrases for each sentence and train a classifier to select the simpler ones. Promising results are obtained, although different types of errors on the paraphrase generation are encountered, such as problems with verb conjugation and regency. In our work we produce alternative simplifications only in the two cases explained above."
W09-2105,2005.jeptalnrecital-court.15,0,0.0397125,"on. Siddharthan's system deals with nonfinite clauses which are not handled by our system at this stage. Lal and Ruger’s (2002) created a bayesian summarizer with a built-in lexical simplification module, based on WordNet and MRC psycholinguistic database3. The system focuses on schoolchildren and provides background information about people and locations in the text, which are retrieved from databases. Our rule-based simplification system only replaces discourse markers for more common ones using lexical resources built in our project, instead of inserting additional information in the text. Max (2005, 2006) applies text simplification in the writing process by embedding an interactive text simplification system into a word processor. At the user’s request, an automatic parser analyzes an individual sentence and the system applies handcrafted rewriting rules. The resulting suggested simplifications are ranked by a score of syntactic complexity and potential change of meaning. The writer then chooses their preferred simplification. This system ensures accurate output, but requires human intervention at every step. Our system, on the other hand, is autonomous, even though the user is able to"
W10-1001,W09-2105,1,0.77245,"Missing"
W10-1001,dias-da-silva-etal-2008-automatic,0,0.0263079,"Missing"
W10-1001,N07-1058,0,0.509808,"more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on models for literary and expository texts, given that traditional metrics like Flesch-Kincaid Level score tend to overpredict the difficulty of literary texts and underpredict the difficulty of expository texts. Heilman et al. (2008) investigate an appropriate scale of measurement for reading difficulty – nominal, ordinal, or interval – by comparing the effectiveness of statistical models for each type of data. Petersen and O"
W10-1001,W08-0909,0,0.823408,"e. With our readability assessment tool, the author is able to automatically check the complexity/readability level of the original text, as well as modified versions of such text produced as he/she applies simplification operations offered by SIMPLIFICA, until the text reaches the expected level, adequate for the target reader. In this paper we present such readability assessment tool, developed as part of the PorSimples project, and discuss its application within the authoring tool. Different from previous work, the tool does not model text difficulty according to linear grade levels (e.g., Heilman et al., 2008), but instead maps the text into the three levels of literacy defined by INAF: rudimentary, basic or advanced. Moreover, it uses a more comprehensive set of features, different learning techniques and targets a new language and application, as we discuss in Section 4. More specifically, we address the following research questions: 1. Given some training material, is it possible to detect the complexity level of Portuguese texts, which corresponds to the different literacy levels defined by INAF? 2. What is the best way to model this problem and which features are relevant? We experiment with n"
W10-1001,W08-0911,0,0.115784,"Missing"
W10-1001,D08-1020,0,0.197,"lity Assessment Recent work on readability assessment for the English language focus on: (i) the feature set used to capture the various aspects of readability, to evaluate the contribution of lexical, syntactic, semantic and discursive features; (ii) the audience of the texts the readability measurement is intended to; (iii) the genre effects on the calculation of text difficult; (iv) the type of learning technique which is more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on mode"
W10-1001,W07-1001,0,0.0352639,"Missing"
W10-1001,P05-1065,0,0.40269,"f learning technique which is more appropriate: those producing nominal, ordinal or interval scales of measurement, and (v) providing an application for the automatic assessment of reading difficulty. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al., 2003). The following studies address readability assessment for specific audiences: learners of English as second language (Schwarm and Ostendorf, 2005; Heilman et al., 2007), people with intellectual disabilities (Feng et al., 2009), and people with 4 cognitive impairment caused by Alzheimer (Roark at al, 2007). Sheehan et al. (2007) focus on models for literary and expository texts, given that traditional metrics like Flesch-Kincaid Level score tend to overpredict the difficulty of literary texts and underpredict the difficulty of expository texts. Heilman et al. (2008) investigate an appropriate scale of measurement for reading difficulty – nominal, ordinal, or interval – by comparing the effectiveness of statistical models for each type"
W10-1001,E09-1027,0,\N,Missing
W10-1607,W09-2105,1,0.485881,"Missing"
W10-1607,duran-etal-2010-assigning,1,0.825964,"Missing"
W10-1607,J05-1004,0,0.00226541,"rguments through whquestions is a process that requires text understanding. This is a skill that the target audience of this project is weak at. In Figure 1 we show the link between the verb and its arguments (which can be subject, direct object, indirect object, time or location adverbial phrases, and also named entities). as well as the automatic labeling tool, an “answerquestioning” system, will be made publicly available at PorSimples site. Besides helping poorliteracy readers, the assignment of wh-questions will be used in the near future to map adjunct semantic roles (ArgMs of Propbank (Palmer et al., 2005)) in a project to build the PropBank.Br for Portuguese language. One may also take profit of this automatic tool and its training corpus to improve its opposite, question-answering systems. 4 Applications The text simplification and elaboration technologies developed in the context of the project are available by means of three systems aimed to distinct users: An authoring system, called SIMPLIFICA11, to help authors to produce simplified texts targeting people with low literacy levels, An assistive technology system, called FACILITA12, which explores the tasks of summarization and simplificat"
W11-0812,2009.mtsummit-btm.1,0,0.276847,"scuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995"
W11-0812,1992.tmi-1.3,0,0.0203849,"ian Portuguese, focusing on the development of a lexical resource for NLP tasks, such as SRL. The remainder of this paper is organized as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs"
W11-0812,W10-1812,0,0.139354,"Missing"
W11-0812,W10-1810,0,0.110853,"e present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995; Hwang et al., 2010)"
W11-0812,C10-3015,1,0.709696,"es from active sentences, both afﬁrmative and negative. Cases which present intervening material between the verb and the other element of the CP are not captured, but this is not a serious problem considering the size of our corpus, although it inﬂuences the frequencies used in candidate selection. In order to facilitate human analysis of candidate lists, we used the mwetoolkit4 : a tool that has been developed speciﬁcally to extract MWEs from corpora, which encompasses candidate extraction through pattern matching, candidate ﬁltering (e.g. through association measures) and evaluation tools (Ramisch et al., 2010). After generating separate lists of candidates for each pattern, we ﬁltered out all those occurring less than 10 times in the corpus. The entries resulting of automatic identiﬁcation were classiﬁed by their frequency and their annotation is discussed in the following section. 4 Discussion Each pattern of POS tags returned a large number of candidates. Our expectation was to identify CPs among the most frequent candidates. First we annotated “interesting” candidates and then, in a deep analysis, we judged their idiomaticity. In the Table 1, we show the total number of candidates extracted befo"
W11-0812,C90-3043,0,0.541419,"er is organized as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominal"
W11-0812,W04-0401,0,0.0631246,"as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Gre"
W11-0812,E95-1014,0,0.786068,"n et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995; Hwang et al., 2010). These approaches are not adopted here because our goal is precisely to identify which are the verbs, the nouns and other lexical elements that take part in CPs. Similar motivation to study LVCs/SVCs (for SRL) is found within the scope of Framenet (Atkins et al., 2003) and Propbank (Hwang et al., 2010). These projects have taken different decisions on how to annotate such constructions. Framenet annotates the head of the construction (noun or adjective) as argument taker (or frame evoker) and the light verb separately; Propbank, on its turn, ﬁrst annotates sep"
W11-2315,2009.mtsummit-btm.1,0,\N,Missing
W11-2315,W10-1607,1,\N,Missing
W11-2315,W03-1602,0,\N,Missing
W11-2315,P08-1040,0,\N,Missing
W11-2315,W10-0406,0,\N,Missing
W11-2315,daelemans-etal-2004-automatic,0,\N,Missing
W11-2315,E99-1042,0,\N,Missing
W11-4506,W10-1001,1,0.843543,"Entretanto, essas fórmulas mostram-se insuficientes quando não conseguem, por exemplo, abranger a importância de marcadores discursivos [Williams, 2004]. A maioria dos trabalhos mais atuais sobre avaliação da inteligibilidade de textos usa métodos de aprendizado de máquina (AM) e avalia suas abordagens para textos em inglês. Em outra abordagem, um trabalho para a língua alemã [Glöckner et al., 2006] propõe avaliar a inteligibilidade via uso de várias features e de uma medida global de inteligibilidade similar ao índice Flesch. A língua portuguesa também foi contemplada em trabalhos recentes [Aluísio et al., 2010; Scarton e Aluísio, 2010; Scarton et al., 2010]. Esses trabalhos empregam a abordagem de avaliação da inteligibilidade para apoiar a simplificação de textos destinados a leitores com níveis de letramento baixos. Os trabalhos atuais, geralmente, tratam de cinco aspectos principais: a) Avaliam o conjunto de features usado para capturar os vários aspectos da inteligibilidade e a contribuição das features de vários níveis linguísticos. É o caso do trabalho de Pitler and Nenkova (2008) que, com base nos trabalhos de criação de rubricas para avaliação de redações de alunos (essay scoring) [Burstein"
W11-4506,E09-1027,0,0.0144981,"a vocabulário, sintaxe, elementos de coesão lexical e relações discursivas para medir a qualidade de um texto. Feng et al. (2010), seguindo os estudos de Pitler e Nenkova (2008), propõem o uso de várias features, que são comparadas e avaliadas em termos de seu impacto para prever uma série de livros de leitura adequados para estudantes do nível fundamental. b) Focam uma dada audiência para a qual a avaliação da inteligibilidade é destinada. É o caso de trabalhos focando em aprendizes do inglês como língua estrangeira [Schwarm and Ostendorf, 2005], pessoas com capacidade intelectual reduzida [Feng et al., 2009], pessoas com problemas cognitivos causados por Alzheimer [Roark at al., 2007], textos para adultos ou para crianças [Scarton e Aluísio, 2010] e textos para um determinado nível de letramento [Aluísio et al., 2010]. c) Tratam dos efeitos do gênero textual no cálculo do índice de inteligibilidade. É o caso dos trabalhos de Sheehan et al. (2007), que estudam modelos para textos expositivos e literários, considerando que o uso de índices simples, como FleschKincaid Level, tendem a subestimar a dificuldade dos primeiros e sobrestimar a dos últimos e Scarton et al. (2010) que apresentam os primeir"
W11-4506,C10-2032,0,0.0296253,"etramento baixos. Os trabalhos atuais, geralmente, tratam de cinco aspectos principais: a) Avaliam o conjunto de features usado para capturar os vários aspectos da inteligibilidade e a contribuição das features de vários níveis linguísticos. É o caso do trabalho de Pitler and Nenkova (2008) que, com base nos trabalhos de criação de rubricas para avaliação de redações de alunos (essay scoring) [Burstein et al., 2003], 51 propõem um framework unificado composto de features relacionadas a vocabulário, sintaxe, elementos de coesão lexical e relações discursivas para medir a qualidade de um texto. Feng et al. (2010), seguindo os estudos de Pitler e Nenkova (2008), propõem o uso de várias features, que são comparadas e avaliadas em termos de seu impacto para prever uma série de livros de leitura adequados para estudantes do nível fundamental. b) Focam uma dada audiência para a qual a avaliação da inteligibilidade é destinada. É o caso de trabalhos focando em aprendizes do inglês como língua estrangeira [Schwarm and Ostendorf, 2005], pessoas com capacidade intelectual reduzida [Feng et al., 2009], pessoas com problemas cognitivos causados por Alzheimer [Roark at al., 2007], textos para adultos ou para cria"
W11-4506,N07-1058,0,0.0120193,"edida para dificuldade de leitura – nominal, ordinal e intervalar – via comparação da efetividade de modelos estatísticos para estes tipos de dados, enquanto que Petersen and Ostendorf (2009) defenderam o uso de Support Vector Machines tanto como modelo de regressão quanto de classificação para predizer níveis de inteligibilidade. Para o português, Aluísio et al. (2010) avaliaram modelos nominal, ordinal e intervalar para textos em português (originais e simplificados) obtendo resultados similares. e) Focam-se em aplicações computacionais que utilizam métodos de avaliação de inteligibilidade. Heilman et al. (2007) usaram um avaliador de inteligibilidade em sistemas tutores inteligentes para indicar textos de leitura com o nível adequado de dificuldade para aprendizes do inglês como segunda língua. Miltsakali e Troutt (2007 e 2008) propuseram uma ferramenta automática para avaliar textos da Web que fossem adequados para adolescentes e adultos com níveis baixos de letramento, enquanto Aluísio et al. (2010) utilizaram um avaliador de inteligibilidade em um editor de simplificação para medir o nível de inteligibilidade com relação a três padrões de letramento (rudimentar, básico e pleno). Considerando os t"
W11-4506,W08-0909,0,0.0137229,"inteligibilidade. É o caso dos trabalhos de Sheehan et al. (2007), que estudam modelos para textos expositivos e literários, considerando que o uso de índices simples, como FleschKincaid Level, tendem a subestimar a dificuldade dos primeiros e sobrestimar a dos últimos e Scarton et al. (2010) que apresentam os primeiros resultados na proposta de um avaliador de inteligibilidade global, usando as features do Coh-Metrix-Port. d) Avaliam qual o modelo estatístico é mais apropriado para os índices (escalas nominais, ordinais ou intervalares) e para a precisão de métodos de aprendizado de máquina. Heilman et al. (2008) investigaram as escalas de medida para dificuldade de leitura – nominal, ordinal e intervalar – via comparação da efetividade de modelos estatísticos para estes tipos de dados, enquanto que Petersen and Ostendorf (2009) defenderam o uso de Support Vector Machines tanto como modelo de regressão quanto de classificação para predizer níveis de inteligibilidade. Para o português, Aluísio et al. (2010) avaliaram modelos nominal, ordinal e intervalar para textos em português (originais e simplificados) obtendo resultados similares. e) Focam-se em aplicações computacionais que utilizam métodos de av"
W11-4506,W08-0911,0,0.0306231,"Missing"
W11-4506,D08-1020,0,0.0518173,"Missing"
W11-4506,W07-1001,0,0.0277346,"Missing"
W11-4506,P05-1065,0,0.0123016,"2003], 51 propõem um framework unificado composto de features relacionadas a vocabulário, sintaxe, elementos de coesão lexical e relações discursivas para medir a qualidade de um texto. Feng et al. (2010), seguindo os estudos de Pitler e Nenkova (2008), propõem o uso de várias features, que são comparadas e avaliadas em termos de seu impacto para prever uma série de livros de leitura adequados para estudantes do nível fundamental. b) Focam uma dada audiência para a qual a avaliação da inteligibilidade é destinada. É o caso de trabalhos focando em aprendizes do inglês como língua estrangeira [Schwarm and Ostendorf, 2005], pessoas com capacidade intelectual reduzida [Feng et al., 2009], pessoas com problemas cognitivos causados por Alzheimer [Roark at al., 2007], textos para adultos ou para crianças [Scarton e Aluísio, 2010] e textos para um determinado nível de letramento [Aluísio et al., 2010]. c) Tratam dos efeitos do gênero textual no cálculo do índice de inteligibilidade. É o caso dos trabalhos de Sheehan et al. (2007), que estudam modelos para textos expositivos e literários, considerando que o uso de índices simples, como FleschKincaid Level, tendem a subestimar a dificuldade dos primeiros e sobrestima"
W11-4508,C04-1137,0,0.814131,"h to the identification of cognates and false friends for the language pair Portuguese-Spanish. In Section 4, the results of attribute selection and classification experiments are shown and Section 5 is the conclusion of our study and future work we envisage. 2. Related Work Different natural language processing applications concentrate on cognate identification. Some of these applications include: sentence alignment [Simard and Isabelle 1992, Melamed 1999], parallel text alignment [Gomes 2009], inducing bilingual lexicons [Mann and Yarowsky 2001], and identification of confusable drug names [Kondrak and Dorr 2004]. There are several approaches to identify cognates between language pairs [Simard and Isabelle 1992, Kondrak 2001, Kondrak 2005, Inkpen&apos;s and Frunza 2005, Frunza and Inkpen&apos;s 2009]. Usually, the methods to compute similarity among words are divided into orthographic and phonetic. Some of the methods in the first group are EDIT distance, LCSR (longest common subsequence ratio) and measures based on the number of n-grams that are 68 shared by words. Other frequently employed measure is the binary identity function. The most well-known phonetic approaches are Soundex and Editex, which try to ta"
W11-4508,N01-1014,0,0.0272646,"attribute selection and classification experiments are shown and Section 5 is the conclusion of our study and future work we envisage. 2. Related Work Different natural language processing applications concentrate on cognate identification. Some of these applications include: sentence alignment [Simard and Isabelle 1992, Melamed 1999], parallel text alignment [Gomes 2009], inducing bilingual lexicons [Mann and Yarowsky 2001], and identification of confusable drug names [Kondrak and Dorr 2004]. There are several approaches to identify cognates between language pairs [Simard and Isabelle 1992, Kondrak 2001, Kondrak 2005, Inkpen&apos;s and Frunza 2005, Frunza and Inkpen&apos;s 2009]. Usually, the methods to compute similarity among words are divided into orthographic and phonetic. Some of the methods in the first group are EDIT distance, LCSR (longest common subsequence ratio) and measures based on the number of n-grams that are 68 shared by words. Other frequently employed measure is the binary identity function. The most well-known phonetic approaches are Soundex and Editex, which try to take advantage of individual features to determine similarity between words [Wesley and Kondrak 2005]. Simard and Isa"
W11-4508,2005.mtsummit-papers.40,0,0.448881,"ection and classification experiments are shown and Section 5 is the conclusion of our study and future work we envisage. 2. Related Work Different natural language processing applications concentrate on cognate identification. Some of these applications include: sentence alignment [Simard and Isabelle 1992, Melamed 1999], parallel text alignment [Gomes 2009], inducing bilingual lexicons [Mann and Yarowsky 2001], and identification of confusable drug names [Kondrak and Dorr 2004]. There are several approaches to identify cognates between language pairs [Simard and Isabelle 1992, Kondrak 2001, Kondrak 2005, Inkpen&apos;s and Frunza 2005, Frunza and Inkpen&apos;s 2009]. Usually, the methods to compute similarity among words are divided into orthographic and phonetic. Some of the methods in the first group are EDIT distance, LCSR (longest common subsequence ratio) and measures based on the number of n-grams that are 68 shared by words. Other frequently employed measure is the binary identity function. The most well-known phonetic approaches are Soundex and Editex, which try to take advantage of individual features to determine similarity between words [Wesley and Kondrak 2005]. Simard and Isabelle (1992) u"
W11-4508,W05-0606,0,0.0419375,"Missing"
W11-4508,N01-1020,0,0.0938119,"Missing"
W11-4508,J99-1003,0,0.499065,"y. In Section 2, we present works related to the identification and disambiguation of cognates and false friends. In Section 3, we introduce our approach to the identification of cognates and false friends for the language pair Portuguese-Spanish. In Section 4, the results of attribute selection and classification experiments are shown and Section 5 is the conclusion of our study and future work we envisage. 2. Related Work Different natural language processing applications concentrate on cognate identification. Some of these applications include: sentence alignment [Simard and Isabelle 1992, Melamed 1999], parallel text alignment [Gomes 2009], inducing bilingual lexicons [Mann and Yarowsky 2001], and identification of confusable drug names [Kondrak and Dorr 2004]. There are several approaches to identify cognates between language pairs [Simard and Isabelle 1992, Kondrak 2001, Kondrak 2005, Inkpen&apos;s and Frunza 2005, Frunza and Inkpen&apos;s 2009]. Usually, the methods to compute similarity among words are divided into orthographic and phonetic. Some of the methods in the first group are EDIT distance, LCSR (longest common subsequence ratio) and measures based on the number of n-grams that are 68 sh"
W11-4508,1992.tmi-1.7,0,0.347223,"phic or phonetic similarity. In Section 2, we present works related to the identification and disambiguation of cognates and false friends. In Section 3, we introduce our approach to the identification of cognates and false friends for the language pair Portuguese-Spanish. In Section 4, the results of attribute selection and classification experiments are shown and Section 5 is the conclusion of our study and future work we envisage. 2. Related Work Different natural language processing applications concentrate on cognate identification. Some of these applications include: sentence alignment [Simard and Isabelle 1992, Melamed 1999], parallel text alignment [Gomes 2009], inducing bilingual lexicons [Mann and Yarowsky 2001], and identification of confusable drug names [Kondrak and Dorr 2004]. There are several approaches to identify cognates between language pairs [Simard and Isabelle 1992, Kondrak 2001, Kondrak 2005, Inkpen&apos;s and Frunza 2005, Frunza and Inkpen&apos;s 2009]. Usually, the methods to compute similarity among words are divided into orthographic and phonetic. Some of the methods in the first group are EDIT distance, LCSR (longest common subsequence ratio) and measures based on the number of n-grams"
W11-4519,afonso-etal-2002-floresta,0,0.0993748,"Missing"
W11-4519,aldezabal-etal-2010-building,0,0.0203474,"nd delimitation of arguments associated with the “argument taker”, and 3) assignment of a semantic role label to each of these arguments. Annotation over a syntactic tree eliminates the step of arguments delimitation, as the syntactic constituents delimitated by the parser are kept for arguments annotation. Hence, the quality of SRL annotation is dependant of syntactic parsing quality. Recently there are initiatives to make corpus annotation, following Propbank model, for other languages besides English: Korean (Palmer et al, 2006), Chinese (Xue, 2009), Arabic (Palmer et al, 2008) and Basque (Aldezabal et al. 2010). However, as far as we know, there is not until this date such a corpus of Brazilian Portuguese. To fulfill this gap, we report here the construction of a Brazilian Portuguese Propbank: Propbank-Br. This first step of the research aims to pave the way for a broader and distributed annotation task. Language specific challenges became evident during the annotation task and several decisions have been taken to deal with them. This experience enabled us to customize Propbank guidelines and build frames files for Portuguese verbs, essential resources to guide annotators and ensure inter-annotator"
W11-4519,P98-1013,0,0.728549,"Missing"
W11-4519,burchardt-etal-2006-salto,0,0.0432501,"Missing"
W11-4519,choi-etal-2010-propbank,0,0.0133198,"te the Brazilian portion of Bosque, the manually revised subcorpus of Floresta Sintá(c)tica2 (Affonso et al, 2002), parsed by Palavras (Bick, 2000). Bosque has 9368 sentences and 4213 of them correspond to the Brazilian portion (extracted from the journal Folha de São Paulo of 1994). The annotation tool we have chosen was SALTO (Burchardt et al, 2008) due to a previous successful experience we had on assigning wh-questions to verbal arguments, a 1 2 http://verbs.colorado.edu/propbank/framesets-english/ http://linguateca.pt 165 related task. After annotation started, we have notice of Jubilee (Choi et al. 2010), a dedicated annotation tool developed by the Propbank team. SALTO has been developed for annotation of German Framenet, but its resources were adequate for our annotation purposes not requiring tool customization (we customized only the use). A facility of SALTO that we have extensively used is the sentence flag. For example, we have flagged as Wrongsubcorpus all sentences that present some error, and for this we created three parameters: EP corresponds to parsing errors or inadequacies, EC corresponds to corpus errors, like spelling or punctuation errors, and EV corresponds to invocation er"
W11-4519,choi-etal-2010-propbank-instance,0,0.0154263,"te the Brazilian portion of Bosque, the manually revised subcorpus of Floresta Sintá(c)tica2 (Affonso et al, 2002), parsed by Palavras (Bick, 2000). Bosque has 9368 sentences and 4213 of them correspond to the Brazilian portion (extracted from the journal Folha de São Paulo of 1994). The annotation tool we have chosen was SALTO (Burchardt et al, 2008) due to a previous successful experience we had on assigning wh-questions to verbal arguments, a 1 2 http://verbs.colorado.edu/propbank/framesets-english/ http://linguateca.pt 165 related task. After annotation started, we have notice of Jubilee (Choi et al. 2010), a dedicated annotation tool developed by the Propbank team. SALTO has been developed for annotation of German Framenet, but its resources were adequate for our annotation purposes not requiring tool customization (we customized only the use). A facility of SALTO that we have extensively used is the sentence flag. For example, we have flagged as Wrongsubcorpus all sentences that present some error, and for this we created three parameters: EP corresponds to parsing errors or inadequacies, EC corresponds to corpus errors, like spelling or punctuation errors, and EV corresponds to invocation er"
W11-4519,W03-1008,0,0.0604995,"Missing"
W11-4519,P02-1031,0,0.0305727,"papéis semânticos. Este artigo relata a anotação de um corpus de português do Brasil seguindo as instruções do Propbank. Este é o primeiro passo de um esforço mais amplo de anotação e tem por objetivo abrir caminho para uma tarefa de anotação distribuída. As decisões de anotação são discutidas a fim de salientar os aspectos específicos de língua envolvidos no projeto. 1. Introduction Semantic role labeling (SRL), as an NLP task based on annotated corpus, was first addressed by Gildea e Jurafsky (2002), employing Framenet corpus (Baker et al. 1998). Since then several projects dealt with SRL (Gildea & Palmer, 2002, Surdeanu et. al, 2003, Gildea & Hockenmaier, 2003, Yi, Loper and Palmer, 2007, Palmer et al. 2010, among others). There are at least two ways for improving SRL classifiers based on annotated corpus: one of them is trying out different machine learning methods; the other way is providing a large and properly annotated training corpus. Framenet was not originally conceived to provide a training corpus for machine learning. Its set of semantic role labels, for example, is fine grained and poses a problem of data sparsity for statistical learning methods. Propbank initiative, in contrast, focus"
W11-4519,palmer-etal-2008-pilot,0,0.0489932,"or example); 2) identification and delimitation of arguments associated with the “argument taker”, and 3) assignment of a semantic role label to each of these arguments. Annotation over a syntactic tree eliminates the step of arguments delimitation, as the syntactic constituents delimitated by the parser are kept for arguments annotation. Hence, the quality of SRL annotation is dependant of syntactic parsing quality. Recently there are initiatives to make corpus annotation, following Propbank model, for other languages besides English: Korean (Palmer et al, 2006), Chinese (Xue, 2009), Arabic (Palmer et al, 2008) and Basque (Aldezabal et al. 2010). However, as far as we know, there is not until this date such a corpus of Brazilian Portuguese. To fulfill this gap, we report here the construction of a Brazilian Portuguese Propbank: Propbank-Br. This first step of the research aims to pave the way for a broader and distributed annotation task. Language specific challenges became evident during the annotation task and several decisions have been taken to deal with them. This experience enabled us to customize Propbank guidelines and build frames files for Portuguese verbs, essential resources to guide ann"
W11-4519,P03-1002,0,0.0969279,"Missing"
W11-4519,N07-1069,0,0.0514229,"Missing"
W13-1014,W06-1207,0,0.0310531,"the uses of the clitic pronoun se share the same realization at the surface form level, the use as a CONSTITUTIVE PARTICLE of pronominal verbs is the only one in which the verb and the clitic form a multiword lexical unit on its own. In the other uses, the clitic keeps a separate syntactic and/or semantic function, as presented in Table 1. The particle se is an integral part of pronominal verbs in the same way as the particles of English phrasal verbs. As future work, we would like to investigate possible semantic contributions of the se particle to the meaning of pronominal verbs, as done by Cook and Stevenson (2006), for example, who try to automatically classify the uses of the particle up in verb-particle constructions. Like in the present paper, they estimate a set of linguistic features which are in turn used to train a Support Vector Machine (SVM) classifier citecook:2006:mwe. 3 Methodology For the automatic identification of multiword verb+se occurrences, we performed corpus searches on the PLN-BR-FULL corpus (Muniz et al., 2007), which consists of news texts extracted from a major Brazilian newspaper, Folha de S˜ao Paulo, from 1994 to 2005, with 29,014,089 tokens. The corpus was first preprocessed"
W13-1014,C10-3015,1,0.771149,"Missing"
W13-1014,slavcheva-2006-semantic,0,0.0193784,"SIVE uses (Morais Nunes, 1990; Cyrino, 2007; Pereira-Santos, 2010); REFLEXIVE use (Godoy, 2012), and IN CHOATIVE use (Fonseca, 2010; Nunes-Ribeiro, 2010; Ros´ario Ribeiro, 2011). Despite none of these works concerning specifically pronominal verbs, they provided us an important theoretical basis for the analysis undertaken herein. The problem of the multifunctional use of clitic pronouns is not restricted to Portuguese. Romance languages, Hebrew, Russian, Bulgarian and others also have similar constructions. There are crosslinguistic studies regarding this matter reported in Siloni (2001) and Slavcheva (2006), showing that there are partial coincidence of verbs taking clitic pronouns to produce alternations and reflexive voice. From an NLP perspective, the problem of the ambiguity of the clitic pronoun se was studied by Martins et al. (1999) to solve a problem of categorization, that is, to decide which part-of-speech tag should be assigned to se. However, we have not found studies regarding pronominal verbs aiming at Portuguese automatic language processing. Even though in Portuguese all the uses of the clitic pronoun se share the same realization at the surface form level, the use as a CONSTITUT"
W13-1014,vincze-2012-light,0,0.0198436,"n the one hand, the annotation of uses can be semi-automatically projected on the sentences extracted from the corpus. On the other hand, the findings of this work in terms of syntactic and semantic characteristics can be used to propose features for the classifier, trying to reproduce those that can be automatically obtained (e.g., subcategorization frame) and to simulate those that cannot be easily automated (e.g., whether the subject is animate). For these future experiments, we intend to compare different learning models, based on SVM and on sequence models like conditional random fields (Vincze, 2012). As languages are different in what concerns allowed alternations, the use of clitic se in Portuguese becomes even more complex when approached from a bilingual point of view. Depending on how different the languages compared are, the classification of se adopted here may be of little use. For example, several verbs classified as reflexive in Portuguese, like vestir-se (to dress), barbear-se (to shave) and demitir-se (to resign) are not translated into a reflexive form in English (*to dress oneself, *to shave oneself and *to dismiss oneself ). Similarly, typical inchoative verb uses in Portug"
W13-4820,P98-1013,0,0.293215,"Missing"
W13-4820,burchardt-etal-2006-salto,0,0.0242996,"servados e, de outro lado, tarefas muito complexas, como definir e criar identificações para cada sentido dos verbos. Visando acelerar o processo, decidimos separar as tarefas automatizáveis das que exigiam trabalho linguístico. Percebemos que, se o corpus contivesse todas as informações necessárias, os arquivos do repositório poderiam ser construídos automaticamente. Por isso, complementamos a anotação do corpus com as informações exigidas pelos arquivos do repositório. Para isso, criamos seis campos de 169 anotação ou wordtags, um recurso da ferramenta de anotação de corpus adotada (SALTO – Burchardt et al. 2006): (1) PB-roleset: sentido no repositório do Propbank inglês, equivalente ao sentido anotado em português, que permitirá herdarmos as definições dos papéis semânticos e seus mapeamentos para os papéis semânticos e classes da Verbnet; (2) Nota: campo utilizado sempre que for necessário fazer alguma observação sobre o sentido do verbo para os anotadores; (3) Nota do exemplo: esse campo é utilizado para chamar a atenção dos anotadores para algum aspecto do exemplo; (4) Predicate lemma: campo obrigatório na primeira ocorrência de um sentido; é onde se coloca o nome do predicado, incluindo predicado"
W13-4820,choi-etal-2010-propbank,0,0.0134542,"o Propbank do inglês), e em parte porque ele não contém distinção de sentidos dos verbos. A fim de superar esses pontos fracos, é preciso empreender um projeto de anotação de SRL em larga escala. Uma das condições para isso é a existência de um repositório lexical para guiar um processo de anotação que envolva diversos anotadores. A construção semi-automática desse repositório, ainda em curso, é o tema deste artigo. 2. Metodologia O modelo de banco de dados adotado foi o do repositório do projeto Propbank, composto por um arquivo xml para cada verbo, que possui um editor dedicado Cornerstone (Choi et al. 2010). As telas do Cornerstone possuem campos para inserir verbos, sentidos de verbos, definição de cada sentido dos verbos, os papéis semânticos previstos pelos sentidos dos verbos e exemplos anotados de cada um desses sentidos. Tínhamos algumas alternativas para construir o repositório: 1) Traduzir o recurso inglês. Descartamos essa alternativa pois ela excluiria todos os verbos e sentidos de verbos que não tivessem equivalentes em inglês. Além disso, a tradução dos exemplos seria trabalhosa; 2) Construir a partir do zero, usando informações de dicionários. Descartamos essa alternativa porque os"
W13-4820,W07-1508,0,0.0368283,"Missing"
W13-4820,N06-2015,0,0.122164,"Missing"
W13-4820,J05-1004,0,0.0738574,"njuntos de rótulos de papéis semânticos e como nem sempre é simples decidir qual o rótulo mais adequado para anotar um argumento, grandes projetos de anotação de papéis semânticos (SRL: Semantic Role Labeling) desenvolveram repositórios lexicais onde estão definidos os papéis semânticos previstos pelos sentidos dos predicadores. Alguns desses repositórios definem papéis semânticos para as classes verbais, como é o caso da Verbnet (Kipper et al. 2006), outros para os frames semânticos, como é o caso da Framenet (Baker et al. 1998) e outros ainda para os sentidos de cada verbo, como o Propbank (Palmer et al. 2005). Quanto mais completo for o repositório lexical e quanto mais clara for a 168 Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology, pages 168–172, c Fortaleza, CE, Brazil, October 21–23, 2013. 2013 Sociedade Brasileira de Computa¸c˜ ao distinção que ele faz dos sentidos do predicador, mais simples a tarefa se tornará para os anotadores, aumentando a probabilidade de se atingir bons índices de concordância entre anotadores e de se obter boa precisão no aprendizado de máquina da tarefa (Duffield et al. 2007; Palmer et al. 2007; Hovy et al. 2006). No repositóri"
W13-4822,dayrell-etal-2012-rhetorical,1,0.88843,"Missing"
W13-4829,P11-1032,0,0.0211172,"d in this evaluation are posemo (12,878 entries), which stands for positive emotion, and negemo (15,115 entries), which stands for negative emotion. Other categories would also be useful (e.g., affect, anger, sad, etc), however, we decided it would be a fair comparison against the other lexicons if we used only these two. The English version for LIWC dictionary has been used for a number of relevant works in sentiment analysis. For example, SentiStrength [Thelwall et al. 2010] uses LIWC dictionary for building its internal word list, which is the core of this sentiment classifier; Ott et al. [Ott et al. 2011] uses it to identify fictitious opinions that have been deliberately written to sound authentic; Kim et al. [Kim et al. 2012] uses it to classify anonymous texts. 2.2. OpinionLexicon The OpinionLexicon [Souza et al. 2011] is a dictionary built for sentiment analysis task. To construct this resource, the authors applied three methods from the literature: a corpusbased, a thesaurus-based and an automatic translation system. The lexicon in the version 2.1 is composed of 30,678 entries (30,236 words and 442 phrases). Opinion Lexicon was used by Souza and Vieira [Souza and Vieira 2012] for twitter"
W13-4829,W11-4507,0,0.0114689,"however, we decided it would be a fair comparison against the other lexicons if we used only these two. The English version for LIWC dictionary has been used for a number of relevant works in sentiment analysis. For example, SentiStrength [Thelwall et al. 2010] uses LIWC dictionary for building its internal word list, which is the core of this sentiment classifier; Ott et al. [Ott et al. 2011] uses it to identify fictitious opinions that have been deliberately written to sound authentic; Kim et al. [Kim et al. 2012] uses it to classify anonymous texts. 2.2. OpinionLexicon The OpinionLexicon [Souza et al. 2011] is a dictionary built for sentiment analysis task. To construct this resource, the authors applied three methods from the literature: a corpusbased, a thesaurus-based and an automatic translation system. The lexicon in the version 2.1 is composed of 30,678 entries (30,236 words and 442 phrases). Opinion Lexicon was used by Souza and Vieira [Souza and Vieira 2012] for twitter sentiment analysis; and by Ribeiro Junior et al. [Ribeiro Junior et al. 2012] to assess vehicle features in blogs. 2.3. SentiLex SentiLex [Silva et al. 2012] is a lexicon constructed for social judgments domain. The lexi"
W13-4829,J11-2001,0,0.0602472,". The core of this program is a lexicon resource, best known as LIWC dictionary, which recently has been made available for Portuguese Language 1 . Sentiment analysis, or opinion mining, is a relatively new topic of research in natural language processing that has gained lots of attention due to the growth of the social web. A common task in sentiment analysis is text classification. In this task, a text, sentence or piece of opinion may be classified as positive, negative or neutral. Sentiment classification is commonly categorized in two basic approaches: machine learning and lexicon-based [Taboada et al. 2011]. Machine learning approach uses a set of features, usually the vocabulary, which are learned from annotated corpora or labelled examples. The lexicon-based approach uses a lexicon to provide the polarity, or semantic orientation, for each word or phrase in the text. This last approach does not require an annotated corpora, and it is known for its domain independence, while the machine learning approach tends to adapt to the domain the classifier was trained [Aue and Gamon 2005]. The main component for the lexicon-based sentiment classifier is the lexicon resource, which needs to be precise a"
W14-0404,P06-2005,0,0.105193,"pose some procedures to minimize them. 1. Introduction Corpus normalization has become a common challenge for everyone interested in processing a web corpus. Some normalization tasks are language and genre independent, like boilerplate removal and deduplication of texts. Others, like orthographic errors correction and internet slang handling, are not. Two approaches to web corpus normalization have been discussed in Web as a Corpus (WAC) literature. One of them is to tackle the task as a translation problem, being the web texts the source language and the normalized texts the target language (Aw et al., 2006; Contractor et al., 2010; Schlippe et al., 2013). Such approach requires a parallel corpus of original and normalized texts of reasonable size for training a system with acceptable accuracy. The other approach is to tackle the problem as a number of sub problems to be solved in sequence 22 Felix Bildhauer & Roland Schäfer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 22–28, c Gothenburg, Sweden, April 26 2014. 2014 Association for Computational Linguistics keeping the corrected form as an additional annotation layer, may be the best solution. 2. Related Work"
W14-0404,cardoso-2012-rembrandt,0,0.0133997,"Entities from spelling corrections because when nonrecognized lowercase words are checked by spellers, there is the risk of wrong correction. Indeed, the more extensive is the speller lexicon, the greater is the risk of miscorrection. The genre under inspection presents a widespread misuse of case. By one side, lower case is used in place of uppercase in the initial letter of proper names. On the other side, upper case is used to emphasize any kind of word. Our first tentative to tackle the problem of capitalization was to submit the samples to a Named Entity Recognizer. We chose Rembrandt2 (Cardoso, 2012), a Portuguese NER that enhances both lexical knowledge extracted from Wikipedia and statistical knowledge. The procedure was: 1) to submit the sample to Rembrandt; 2) to capitalize the recognized entities written in lower case; 3) to change all the words capitalized, except the named entities, to lower case. Then we tagged the sample with MXPOST to evaluate the effect on POS tagging accuracy. The number of errors of POS tagging increased (149) when compared to the one of the sample without preprocessing (138). The Table 3. Percentage of case use in newspaper and products reviews corpus genres"
W14-0404,J96-2004,0,0.20529,"Missing"
W14-0404,hartmann-etal-2014-large,1,0.841051,"Missing"
W14-0404,P03-1020,0,0.0427581,"needed. Furthermore, the processing of a new genre is an opportunity not only to make genre-adaptation, but also to improve general purpose features of NLP tools. used as feature by Named Entities Recognizers (NER), POS taggers and parsers. To evaluate whether the case use distribution is different from that of a corpus of well written texts, we compared the statistics of case use in our corpus with those of a newspaper corpus (http://www.linguateca.pt/CETENFolha/), as shown in Table 3. 6.1 Case normalization: truecasing In NLP the problem of case normalization is usually called “truecasing” (Lita et al, 2003, Manning et al., 2008). The challenge is to decide when uppercase should be changed into lower case and when lower case should be changed into upper case. In brief, truecasing is the process of correcting case use in badly-cased or non-cased text. The problem is particularly relevant in two scenarios; speech recognition and informal web texts. We prioritized the case normalization for two reasons: first, badly-cased text seems to be a generalized problem in the genre of products reviews and, second, it is important to make case normalization before using a spell checker. This is crucial to “p"
W14-0404,J06-3001,0,0.617775,"Missing"
W14-0404,C10-2022,0,\N,Missing
W15-1508,P14-2131,0,0.0503194,"Missing"
W15-1508,D07-1101,0,0.0608187,"Missing"
W15-1508,D14-1082,0,0.13807,"Missing"
W15-1508,W07-2416,0,0.0615004,"Missing"
W15-1508,P10-1001,0,0.198329,"Missing"
W15-1508,P14-2050,0,0.0729154,"Missing"
W15-1508,D08-1017,0,0.223987,"Missing"
W15-1508,E06-1011,0,0.254086,"Missing"
W15-1508,H05-1066,0,0.489091,"Missing"
W15-1508,P08-1108,0,0.0740109,"Missing"
W15-1508,J07-2002,0,0.102854,"Missing"
W15-1508,P13-1045,0,0.0849635,"Missing"
W15-1508,P10-1040,0,0.121364,"Missing"
W15-1508,D08-1059,0,0.291658,"Missing"
W15-1508,D14-1109,0,0.0374323,"Missing"
W15-1508,J13-1002,0,\N,Missing
W15-5610,J03-1002,0,0.00922977,"Missing"
W15-5624,C04-1051,0,0.164426,"Missing"
W15-5624,marelli-etal-2014-sick,0,0.0803873,"Missing"
W15-5624,N13-1090,0,0.0230536,"Missing"
W15-5624,D14-1162,0,0.079058,"Missing"
W15-5624,D13-1170,0,0.00556574,"Missing"
W15-5624,N03-1003,0,\N,Missing
W17-1215,W16-4831,0,0.147939,"ing (Severyn and Moschitti, 2015; Santos et al., 2015; Rao et al., 2016), we wanted to experiment with similar network architectures, particularly CNNs, in the task of discriminating between similar languages. In the last shared task (DSL 2016), four teams used some form of convolutional neural network. The team mitsls (Belinkov and Glass, 2016) developed a character-level CNN, meaning that each sentence character was embedded in vector space. Their system ranked 6th out of seven rank positions, with 0.830 of overall accuracy, while the 1st system scored 0.894 using SVMs and character ngrams. Cianflone and Kosseim (2016) used a characterlevel convolutional network with a bidirectional long short term memory (BiLSTM) layer. This approach achieved accuracy of 0.785. A similar approach was used by the team ResIdent (Bjerva, 2016). They developed a residual network (a CNN combined with recurrent units) and represented sentences at byte-level, arguing that UTF-8 encodes non-ascii symbols with more than one byte, which potentially allows for more disambiguating power. This system achieved accuracy of 0.849. The fourth team used a word3 Data Since we participated in the closed track, all models were trained and test"
W17-1215,W16-4816,0,0.0153641,"red task (DSL 2016), four teams used some form of convolutional neural network. The team mitsls (Belinkov and Glass, 2016) developed a character-level CNN, meaning that each sentence character was embedded in vector space. Their system ranked 6th out of seven rank positions, with 0.830 of overall accuracy, while the 1st system scored 0.894 using SVMs and character ngrams. Cianflone and Kosseim (2016) used a characterlevel convolutional network with a bidirectional long short term memory (BiLSTM) layer. This approach achieved accuracy of 0.785. A similar approach was used by the team ResIdent (Bjerva, 2016). They developed a residual network (a CNN combined with recurrent units) and represented sentences at byte-level, arguing that UTF-8 encodes non-ascii symbols with more than one byte, which potentially allows for more disambiguating power. This system achieved accuracy of 0.849. The fourth team used a word3 Data Since we participated in the closed track, all models were trained and tested in the DSL Corpus Collection (Tan et al., 2014), provided by the organizers. This corpus was composed by merging different corpora subsets, for the purpose of the DSL shared task, and comprises news data of"
W17-1215,W15-5403,0,0.085969,"Missing"
W17-1215,L16-1284,0,0.225376,"Missing"
W17-1215,vatanen-etal-2010-language,0,0.0853171,"Missing"
W17-1215,D14-1181,0,0.00527564,"A B C D E F Language/Variety Bosnian Croatian Serbian Indonesian Malay Persian Dari Canadian French Hexagonal French Brazilian Portuguese European Portuguese Argentine Spanish Peninsular Spanish Peruvian Spanish Code bs hr sr id my fa-IR fa-AF fr-CA fr-FR pt-BR pt-PT es-AR es-ES es-PE development set. Values greater than 5 also give good results, but training is much slower. In the first system configuration, language varieties are classified using convolutional neural networks. This is our main approach. 4.1 The model, shown in Figure 1, is similar to one of the architectures experimented by Kim (2014). It takes raw sentences as input and generates class probabilities as output. The highest probability is selected as the predicted class. Let s = {w1 , w2 , w3 , . . . , wL } be a sentence of fixed length L. Each word wj must be mapped to a row vector xj ∈ Rd embedded in matrix W|V |+1×d , where |V |is the number of distinct words in the language group. Rows in W follow the same order as words in the vocabulary, so that the i-th row in W represents the vector of the ith word in the vocabulary V . Words are mapped to vectors by looking up their corresponding indexes in W (embedding lookup). Wo"
W17-1215,C16-1127,0,0.057766,"Missing"
W17-1215,W16-4814,0,0.0394856,"Missing"
W17-1215,W15-5401,0,0.0610798,"Missing"
W17-1215,W16-4801,0,0.131698,"Missing"
W17-1215,W17-1201,0,0.0531802,"Missing"
W17-1215,P15-2114,0,0.0235573,"close to our best run. We also found that combinations of unigrams and bigrams produce higher scores than unigrams alone. This was observed in both convolutional networks and multi-layer perceptron networks. 2 Related Work Many approaches to discriminating between similar languages have been attempted in previous DSL shared tasks, and best results were achieved by simpler machine learning methods like SVMs and Logistic Regression (Malmasi et al., 2016). However, since deep neural networks have been successfully applied to many NLP tasks such as question answering (Severyn and Moschitti, 2015; Santos et al., 2015; Rao et al., 2016), we wanted to experiment with similar network architectures, particularly CNNs, in the task of discriminating between similar languages. In the last shared task (DSL 2016), four teams used some form of convolutional neural network. The team mitsls (Belinkov and Glass, 2016) developed a character-level CNN, meaning that each sentence character was embedded in vector space. Their system ranked 6th out of seven rank positions, with 0.830 of overall accuracy, while the 1st system scored 0.894 using SVMs and character ngrams. Cianflone and Kosseim (2016) used a characterlevel co"
W17-1215,W16-4819,0,\N,Missing
W17-6615,W17-6615,1,0.106104,"Missing"
W17-6615,P14-1023,0,0.0189036,"ingful way. In recent years, word embeddings have been found to efficiently provide such representations, and consequently, have become common in modern NLP systems. They are vectors of real valued numbers, which represent words in an n-dimensional space, learned from large nonannotated corpora and able to capture syntactic, semantic and morphological knowledge. Different algorithms have been developed to generate embeddings [Bengio et al. 2003, Collobert et al. 2011, Mikolov et al. 2013, Ling et al. 2015, Lai et al. 2015, inter alia]. They can be roughly divided into two families of methods [Baroni et al. 2014]: the first is composed of methods that work with a co-occurrence word matrix, such as Latent Semantic Analysis (LSA) [Dumais et al. 1988], Hyperspace Analogue to Language (HAL) [Lund and Burgess 1996] and Global Vectors (GloVe) [Pennington et al. 2014]. The second is composed of predictive methods, which try to predict neighboring words given one or more context words, such as Word2Vec [Mikolov et al. 2013]. Given this variety of word embedding models, methods for evaluating them becomes a topic of interest. [Mikolov et al. 2013] developed a benchmark for embedding evaluation based on a seri"
W17-6615,Q17-1010,0,0.0635464,"kolov et al. 2013]. Wang2Vec is a modification of Word2Vec made in order to take into account the lack of word order in the original architecture. Two simple modifications were proposed in Wang2Vec expecting embeddings to better capture syntactic behavior of words [Ling et al. 2015]. In the Continuous Window architecture, the input is the concatenation of the context word embeddings in the order they occur. In Structured Skip-Gram, a different set of parameters is used to predict each context word, depending on its position relative to the target word. FastText is a recently developed method [Bojanowski et al. 2017] in which embeddings are associated to character n-grams, and words are represented as the summation of these representations. In this method, a word representation is induced by summing character n-gram vectors with vectors of surrounding words. Therefore, this method attempts to capture morphological information to induce word embeddings. 4. Evaluation In order to evaluate the robustness of the word embedding models we trained, we performed intrinsic and extrinsic evaluations. For the intrinsic evaluation, we used the set of syntactic and semantic analogies from [Rodrigues et al. 2016]. For"
W17-6615,J81-4005,0,0.730621,"Missing"
W17-6615,W16-2506,0,0.053698,"pairs of words 122 Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks that share some syntactic or semantic relationship, e.g., the names of two countries and their respective capitals, or two verbs in their present and past tense forms. In order to evaluate an embedding model, applying some vectorial algebra operation to the vectors of three of the words should yield the vector of the fourth one. A version of this dataset translated and adapted to Portuguese was created by [Rodrigues et al. 2016]. However, in spite of being popular and computationally cheap, [Faruqui et al. 2016] suggests that word analogies are not appropriate for evaluating embeddings. Instead, they suggest using task-specific evaluations, i.e., to compare word embedding models on how well they perform on downstream NLP tasks. In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants (Section 2). We trained our models using four different algorithms with varying dimensions (Section 3). We evaluated them on the aforementioned analogies as well as on POS tagging and sentence similarity, to assess both syntactic and"
W17-6615,N15-1142,0,0.0323576,"ons usually take words as basic input units; therefore, it is important that they be represented in a meaningful way. In recent years, word embeddings have been found to efficiently provide such representations, and consequently, have become common in modern NLP systems. They are vectors of real valued numbers, which represent words in an n-dimensional space, learned from large nonannotated corpora and able to capture syntactic, semantic and morphological knowledge. Different algorithms have been developed to generate embeddings [Bengio et al. 2003, Collobert et al. 2011, Mikolov et al. 2013, Ling et al. 2015, Lai et al. 2015, inter alia]. They can be roughly divided into two families of methods [Baroni et al. 2014]: the first is composed of methods that work with a co-occurrence word matrix, such as Latent Semantic Analysis (LSA) [Dumais et al. 1988], Hyperspace Analogue to Language (HAL) [Lund and Burgess 1996] and Global Vectors (GloVe) [Pennington et al. 2014]. The second is composed of predictive methods, which try to predict neighboring words given one or more context words, such as Word2Vec [Mikolov et al. 2013]. Given this variety of word embedding models, methods for evaluating them becom"
W17-6615,W16-2504,0,0.035912,"Missing"
W17-6615,D14-1162,0,0.0923969,"space, learned from large nonannotated corpora and able to capture syntactic, semantic and morphological knowledge. Different algorithms have been developed to generate embeddings [Bengio et al. 2003, Collobert et al. 2011, Mikolov et al. 2013, Ling et al. 2015, Lai et al. 2015, inter alia]. They can be roughly divided into two families of methods [Baroni et al. 2014]: the first is composed of methods that work with a co-occurrence word matrix, such as Latent Semantic Analysis (LSA) [Dumais et al. 1988], Hyperspace Analogue to Language (HAL) [Lund and Burgess 1996] and Global Vectors (GloVe) [Pennington et al. 2014]. The second is composed of predictive methods, which try to predict neighboring words given one or more context words, such as Word2Vec [Mikolov et al. 2013]. Given this variety of word embedding models, methods for evaluating them becomes a topic of interest. [Mikolov et al. 2013] developed a benchmark for embedding evaluation based on a series of analogies. Each analogy is composed of two pairs of words 122 Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks that share some syntactic or semantic relationship, e.g., the names of two countries and their respec"
W17-6618,L16-1103,0,0.508755,"input; therefore, words can be considered the basic processing unit. In this case, it is important that they are represented in a way which carries the load of all relevant information. In the current approach used here, words are induced representations in a dense vector space. These representations are known as word embeddings; able to capture semantic, syntactic and morphological information from large unannotated corpora [Mikolov et al. 2013, Ling et al. 2015, Lai et al. 2015]. Various studies show that textual information is important for SBD [Gotoh and Renals 2000, Batista et al. 2012, Che et al. 2016]. Even though textual information is a strong indicator for sentence delimitation, boundaries are often associated with prosodic information [Shriberg et al. 2000,Batista et al. 2012], like pause duration, change in pitch and change in energy. However, the extraction of this type of information requires the use of high quality resources, and consequently, few resources with prosodic information are available. On the other hand, textual information can easily be extracted in large scale from the web. Textual information can also be represented in various ways for SBD, for example, n-gram based"
W17-6618,N15-1087,0,0.312902,"and textual information, reaching state-ofthe-art results for impaired speech. Also, this study showed that it is possible to achieve good results when comparing them with prepared speech, even when practically the same quantity of text is used. Another interesting evidence was that the use of word embeddings, without morpho-syntactic labels was able to present the same results as when they were used; this indicates that word embeddings contain sufficient morpho-syntactic information for SBD. It was also shown that the method gains the better results than the state-of-the-art method used by [Fraser et al. 2015] by a great margin for both impaired and prepared speech (an absolute difference of ∼0.20 and ∼0.30, respectively). Beyond these findings, the method showed that the performance remains the same when a different story is used. 3. Word Embeddings Models The generation of vector representations of words (or word embeddings) is linked to the induction method utilized. The work of [Turian et al. 2010] divides these representations 2 https://www.ted.com/talks 153 Evaluating Word Embeddings for Sentence Boundary Detection in Speech Transcripts into three categories: cluster-based, distributional an"
W17-6618,E17-1031,0,0.0514246,"Missing"
W17-6618,W10-1603,0,0.0115273,"eech transcripts is important as discourse analysis involves the application of Natural Language Processing tools, such as taggers and parsers, which depend on the sentence as a processing unit. Our aim in this paper is to verify which embedding induction method works best for the sentence boundary detection task, specifically whether it be those which were proposed to capture semantic, syntactic or morphological similarities. 1. Introduction The concept of a sentence in written or spoken texts is important in several Natural Language Processing (NLP) tasks, such as morpho-syntactic analysis [Kepler and Finger 2010, Fonseca and Alu´ısio 2016], sentiment analysis [Brum et al. 2016], and speech processing [Mendonc¸a et al. 2014], among others. However, punctuation marks that constitute a sentence boundary are ambiguous The Disambiguation of Punctuation Marks (DPM) task analyzes punctuation marks in texts and indicates whether they correspond to a sentence boundary. The purpose of the DPM task is to answer the question: Among the tokens of punctuation marks in a text, which of them correspond to sentence boundaries? The Sentence Boundary Detection (SBD) task is very similar to DPM, both of which attempt to"
W17-6618,D14-1181,0,0.00435975,"ta, since text is readily found on the web. 3 http://g1.globo.com/ 155 Evaluating Word Embeddings for Sentence Boundary Detection in Speech Transcripts The model’s input is a tridimensional word embedding matrix E ∈ Rm×ϕ×d , where m is equal to the vocabulary size used for training the embeddings. Once we have an input matrix composed by word embeddings, the convolutional layer extracts nf new features from a sliding window with the size hc , which corresponds to the size of the filter applied to the concatenated vectors [e1 , ..., ehc ] corresponding to a region of hc neighboring embeddings [Kim 2014]. The convolutional layer produces features for each t-th word as it applies the shared filter for a window of hc embeddings et−hc +1:t in a sentence with the size ϕ. Our convolutional layer moves in a single vertical dimension (CNN 1D), one step at a time, which results in a quantity of filters qf equal to ϕ − hc + 2 ∗ p + 1. And since we want to classify exactly ϕ elements, we added p = bhc /2c elements of padding to both sides of the sentence. In addition, we applied a max-pooling operation on the temporal axis focusing on a region of hm words, with the idea of feeding only the most import"
W17-6618,N15-1142,0,0.11135,"ike to automate the application of neuropsychological tests and the discourse analysis of the retellings. NLP applications generally receive text as input; therefore, words can be considered the basic processing unit. In this case, it is important that they are represented in a way which carries the load of all relevant information. In the current approach used here, words are induced representations in a dense vector space. These representations are known as word embeddings; able to capture semantic, syntactic and morphological information from large unannotated corpora [Mikolov et al. 2013, Ling et al. 2015, Lai et al. 2015]. Various studies show that textual information is important for SBD [Gotoh and Renals 2000, Batista et al. 2012, Che et al. 2016]. Even though textual information is a strong indicator for sentence delimitation, boundaries are often associated with prosodic information [Shriberg et al. 2000,Batista et al. 2012], like pause duration, change in pitch and change in energy. However, the extraction of this type of information requires the use of high quality resources, and consequently, few resources with prosodic information are available. On the other hand, textual information"
W17-6618,D14-1162,0,0.0802504,"Missing"
W17-6618,C12-2096,0,0.167333,"r, performing SBD in speech texts is more complicated due to the lack of information such as punctuation and capitalization; moreover text output is susceptible to recognition errors, in case of Automatic Speech Recognition (ASR) systems are used for automatic transcriptions [Gotoh and Renals 2000]. SBD from speech transcriptions is a task which has gained more attention in the last decades due to the increasing popularity of ASR software which automatically generate text from audio input. This task can also be applied to written texts, like online product reviews [Silla Jr and Kaestner 2004, Read et al. 2012, L´opez and Pardo 2015], in order to better their intelligibility and facilitate the posterior use of NLP tools. It is important to point out that the differences between spoken and written texts are notable, mainly when we take into consideration the size of the utterances and the number of disfluencies provided in speech. Disfluencies include filled pauses, repetitions, 151 Evaluating Word Embeddings for Sentence Boundary Detection in Speech Transcripts modifications, repairs, partial utterances, nonword vocalizations and false starts. These phenomena are very common in spontaneous speech."
W17-6618,E17-1030,1,0.883419,"Missing"
W17-6618,P10-1040,0,0.0634086,"used; this indicates that word embeddings contain sufficient morpho-syntactic information for SBD. It was also shown that the method gains the better results than the state-of-the-art method used by [Fraser et al. 2015] by a great margin for both impaired and prepared speech (an absolute difference of ∼0.20 and ∼0.30, respectively). Beyond these findings, the method showed that the performance remains the same when a different story is used. 3. Word Embeddings Models The generation of vector representations of words (or word embeddings) is linked to the induction method utilized. The work of [Turian et al. 2010] divides these representations 2 https://www.ted.com/talks 153 Evaluating Word Embeddings for Sentence Boundary Detection in Speech Transcripts into three categories: cluster-based, distributional and distributed methods. In this paper, we focus only on distributed representations, because generally they are computationally faster to be induced. These representations are based on real vectors distributed in a multidimensional space induced by unsupervised learning. In the following paragraphs, we describe the three induction methods for word embeddings utilized in our evaluations. A well-used"
