2020.acl-main.499,D19-1405,0,0.125699,"ndard machine reading comprehension (MRC) datasets (Rajpurkar 1 Our code and data is available at https://github. com/AkariAsai/logic_guided_qa. et al., 2016; Joshi et al., 2017). This poses new challenges to standard models, which are known to exploit statistical patterns or annotation artifacts in these datasets (Sugawara et al., 2018; Min et al., 2019a). Importantly, state-of-the-art models show inconsistent comparison predictions as shown in Figure 1. Improving the consistency of predictions has been previously studied in natural language inference (NLI) tasks (Minervini and Riedel, 2018; Li et al., 2019), but has not been addressed in QA. In this paper, we address the task of producing globally consistent and accurate predictions for comparison questions leveraging logical and symbolic knowledge for data augmentation and training regularization. Our data augmentation uses a set of logical and linguistic knowledge to develop additional consistent labeled training data. Subsequently, our method uses symbolic logic to incorporate consistency regularization for additional supervision signal beyond inductive bias given by data augmentation. Our method generalizes previous consistency-promoting met"
2020.acl-main.499,2021.ccl-1.108,0,0.102092,"Missing"
2020.acl-main.499,N19-1244,0,0.0484079,"Missing"
2020.acl-main.499,P19-1416,1,0.823042,"to understand context presented in multiple paragraphs or carefully ground a question to the given situation. This makes it challenging to annotate a large number of comparison questions. Most current datasets on comparison questions are much smaller than standard machine reading comprehension (MRC) datasets (Rajpurkar 1 Our code and data is available at https://github. com/AkariAsai/logic_guided_qa. et al., 2016; Joshi et al., 2017). This poses new challenges to standard models, which are known to exploit statistical patterns or annotation artifacts in these datasets (Sugawara et al., 2018; Min et al., 2019a). Importantly, state-of-the-art models show inconsistent comparison predictions as shown in Figure 1. Improving the consistency of predictions has been previously studied in natural language inference (NLI) tasks (Minervini and Riedel, 2018; Li et al., 2019), but has not been addressed in QA. In this paper, we address the task of producing globally consistent and accurate predictions for comparison questions leveraging logical and symbolic knowledge for data augmentation and training regularization. Our data augmentation uses a set of logical and linguistic knowledge to develop additional co"
2020.acl-main.499,D18-1277,0,0.0390523,"Missing"
2020.acl-main.499,P19-1613,1,0.85063,"to understand context presented in multiple paragraphs or carefully ground a question to the given situation. This makes it challenging to annotate a large number of comparison questions. Most current datasets on comparison questions are much smaller than standard machine reading comprehension (MRC) datasets (Rajpurkar 1 Our code and data is available at https://github. com/AkariAsai/logic_guided_qa. et al., 2016; Joshi et al., 2017). This poses new challenges to standard models, which are known to exploit statistical patterns or annotation artifacts in these datasets (Sugawara et al., 2018; Min et al., 2019a). Importantly, state-of-the-art models show inconsistent comparison predictions as shown in Figure 1. Improving the consistency of predictions has been previously studied in natural language inference (NLI) tasks (Minervini and Riedel, 2018; Li et al., 2019), but has not been addressed in QA. In this paper, we address the task of producing globally consistent and accurate predictions for comparison questions leveraging logical and symbolic knowledge for data augmentation and training regularization. Our data augmentation uses a set of logical and linguistic knowledge to develop additional co"
2020.acl-main.499,K18-1007,0,0.283813,"ns are much smaller than standard machine reading comprehension (MRC) datasets (Rajpurkar 1 Our code and data is available at https://github. com/AkariAsai/logic_guided_qa. et al., 2016; Joshi et al., 2017). This poses new challenges to standard models, which are known to exploit statistical patterns or annotation artifacts in these datasets (Sugawara et al., 2018; Min et al., 2019a). Importantly, state-of-the-art models show inconsistent comparison predictions as shown in Figure 1. Improving the consistency of predictions has been previously studied in natural language inference (NLI) tasks (Minervini and Riedel, 2018; Li et al., 2019), but has not been addressed in QA. In this paper, we address the task of producing globally consistent and accurate predictions for comparison questions leveraging logical and symbolic knowledge for data augmentation and training regularization. Our data augmentation uses a set of logical and linguistic knowledge to develop additional consistent labeled training data. Subsequently, our method uses symbolic logic to incorporate consistency regularization for additional supervision signal beyond inductive bias given by data augmentation. Our method generalizes previous consist"
2020.acl-main.499,D19-6115,0,0.0570286,"Missing"
2020.acl-main.499,P17-1147,0,0.0281449,"ities or events such as cause-effect, qualitative or quantitative reasoning. To create comparison questions that require inferential knowledge and reasoning ability, annotators need to understand context presented in multiple paragraphs or carefully ground a question to the given situation. This makes it challenging to annotate a large number of comparison questions. Most current datasets on comparison questions are much smaller than standard machine reading comprehension (MRC) datasets (Rajpurkar 1 Our code and data is available at https://github. com/AkariAsai/logic_guided_qa. et al., 2016; Joshi et al., 2017). This poses new challenges to standard models, which are known to exploit statistical patterns or annotation artifacts in these datasets (Sugawara et al., 2018; Min et al., 2019a). Importantly, state-of-the-art models show inconsistent comparison predictions as shown in Figure 1. Improving the consistency of predictions has been previously studied in natural language inference (NLI) tasks (Minervini and Riedel, 2018; Li et al., 2019), but has not been addressed in QA. In this paper, we address the task of producing globally consistent and accurate predictions for comparison questions leveragi"
2020.acl-main.499,P18-1225,0,0.0427241,"than (a) in WIQA or QuaRel, while especially (b) contributes to the performance improvements on HotpotQA as much as (a) does. (c1 , e1 ) and (c2 , e2 ), where e1 = c2 holds. When a∗1 is a positive causal relationship, we create a new example Xtrans = (q3 , p, a∗2 ) for q3 = (c1 , e2 ). Sampling augmented data Adding all consistent examples may change the data distribution from the original one, which may lead to a deterioration in performance (Xie et al., 2019). One can select the data based on a model’s prediction inconsistencies (Minervini and Riedel, 2018) or randomly sample at each epoch (Kang et al., 2018). In this work, we randomly sample augmented data at the beginning of training, and use the same examples for all epochs during training. Despite its simplicity, this yields competitive or even better performance than other sampling strategies.3 3.3 Logic-guided Consistency Regularization We regularize the learning objective (task loss, Ltask ) with a regularization term that promotes consistency of predictions (consistency loss, Lcons ). L = Ltask (X) + Lcons (X, Xaug ). (3) The first term Ltask penalizes making incorrect predictions. The second term Lcons 4 penalizes making predictions that"
2020.acl-main.499,N19-4009,0,0.0668435,"Missing"
2020.acl-main.499,D16-1264,0,0.128484,"Missing"
2020.acl-main.499,D18-1453,0,\N,Missing
2020.acl-main.499,D18-1259,0,\N,Missing
2020.acl-main.499,N19-1423,0,\N,Missing
2020.acl-main.670,S17-2091,0,0.0796435,"Missing"
2020.acl-main.670,D19-1371,1,0.911721,"Missing"
2020.acl-main.670,N19-1361,1,0.893785,"Missing"
2020.acl-main.670,L18-1245,0,0.0166652,"s it requires to perform entity extraction, coreference resolution, saliency detection in addition to the relation extraction.2 General IE Most work in general domain IE focus on sentence-level information extraction (Stanovsky et al., 2018; Qin et al., 2018; Jie and Lu, 2019). Recently, however, Yao et al. (2019) introduced DocRED, a dataset of cross-sentence relation extractions on Wikipedia paragraphs. The paragraphs are of a comparable length to that of S CI ERC, which is significantly shorter than documents in our dataset. Previous IE work on the TAC KBP competitions (Ellis et al., 2017; Getman et al., 2018) comprise multiple knowledge base population tasks. Our task can be considered a variant of the TAC KBP “cold start” task that discovers new entities and entity attributes (slot filling) from scratch. Two aspects of our task make it more interesting, 1) our model needs to be able to extract facts that 2 Scientific IE In recent years, there has been multiple attempts to automatically extract structured 1 Papers with Code: paperswithcode.com Another approach is to perform entity extraction then use the binary classification approach with a list of all possible combinations of relation tuples. Th"
2020.acl-main.670,P19-1513,0,0.447014,"detection of entities and their coreference information (Tsai et al., 2013). Most structured extraction tasks from among these have revolved around extraction from sentences or abstracts of the articles. A recent example is S CI ERC (Luan et al., 2018), a dataset of 500 richly annotated scientific abstracts containing mention spans and their types, coreference information between mentions, and binary relations annotations. We use S CI ERC to bootstrap our data annotation procedure (Section 3.2). There has been a lack of comprehensive IE datasets annotated at the document level. Recent work by Hou et al. (2019); Jia et al. (2019) tried to rectify this by using distant supervision annotations to build datasets for document-level relation extraction. In both datasets, the task of relation extraction is formulated as a binary classification to check if a triplet of ground-truth entities is expressed in the document or not. Instead, our work focuses on a comprehensive list of information extraction tasks “from scratch”, where the input is the raw document. This makes the IE model more interesting as it requires to perform entity extraction, coreference resolution, saliency detection in addition to the r"
2020.acl-main.670,N19-1370,0,0.0784815,"Missing"
2020.acl-main.670,D19-1399,0,0.0307918,"sets, the task of relation extraction is formulated as a binary classification to check if a triplet of ground-truth entities is expressed in the document or not. Instead, our work focuses on a comprehensive list of information extraction tasks “from scratch”, where the input is the raw document. This makes the IE model more interesting as it requires to perform entity extraction, coreference resolution, saliency detection in addition to the relation extraction.2 General IE Most work in general domain IE focus on sentence-level information extraction (Stanovsky et al., 2018; Qin et al., 2018; Jie and Lu, 2019). Recently, however, Yao et al. (2019) introduced DocRED, a dataset of cross-sentence relation extractions on Wikipedia paragraphs. The paragraphs are of a comparable length to that of S CI ERC, which is significantly shorter than documents in our dataset. Previous IE work on the TAC KBP competitions (Ellis et al., 2017; Getman et al., 2018) comprise multiple knowledge base population tasks. Our task can be considered a variant of the TAC KBP “cold start” task that discovers new entities and entity attributes (slot filling) from scratch. Two aspects of our task make it more interesting, 1) our"
2020.acl-main.670,Q18-1028,0,0.033552,"nsive list of IE tasks, including N -ary relations that span long documents. This is a unique setting compared to prior work that focuses on short paragraphs or a single IE task. 2. We develop a baseline model that, to the best of our knowledge, is the first attempt toward a neural full document IE. Our analysis emphasizes the need for better IE models that can overcome the new challenges posed by our dataset. We invite the research community to focus on this important, challenging task. 2 Related Work information from scientific articles. These types of extractions include citation analysis (Jurgens et al., 2018; Cohan et al., 2019), identifying entities and relations (Augenstein et al., 2017; Luan et al., 2019, 2017), and unsupervised detection of entities and their coreference information (Tsai et al., 2013). Most structured extraction tasks from among these have revolved around extraction from sentences or abstracts of the articles. A recent example is S CI ERC (Luan et al., 2018), a dataset of 500 richly annotated scientific abstracts containing mention spans and their types, coreference information between mentions, and binary relations annotations. We use S CI ERC to bootstrap our data annotati"
2020.acl-main.670,D18-1360,1,0.943387,"this end, not much work has been done on developing full IE datasets and model for long documents. //github.com/allenai/SciREX 1 Introduction Extracting information about entities and their relationships from unstructured text is an important problem in NLP. Conventional datasets and methods for information extraction (IE) focus on within-sentence relations from general Newswire text (Zhang et al., 2017). However, recent work started studying the development of full IE models and datasets for short paragraphs (e.g., information extraction from abstracts of scientific articles as in S CI ERC (Luan et al., 2018)), or only extracting ∗ Work done while at AI2 Creating datasets for information extraction at the document level is challenging because it requires domain expertise and considerable annotation effort to comprehensively annotate a full document for multiple IE tasks. In addition to local relationships between entities, it requires identifying document-level relationships that go beyond sentences and even sections. Figure 1 shows an example of such document level relation (Dataset: SQuAD, Metric: EM, Method: BiDAF, Task:machine comprehension). In this paper, we introduce S CI REX, a new compreh"
2020.acl-main.670,D17-1279,1,0.922067,"Missing"
2020.acl-main.670,N19-1308,1,0.848251,"mpared to prior work that focuses on short paragraphs or a single IE task. 2. We develop a baseline model that, to the best of our knowledge, is the first attempt toward a neural full document IE. Our analysis emphasizes the need for better IE models that can overcome the new challenges posed by our dataset. We invite the research community to focus on this important, challenging task. 2 Related Work information from scientific articles. These types of extractions include citation analysis (Jurgens et al., 2018; Cohan et al., 2019), identifying entities and relations (Augenstein et al., 2017; Luan et al., 2019, 2017), and unsupervised detection of entities and their coreference information (Tsai et al., 2013). Most structured extraction tasks from among these have revolved around extraction from sentences or abstracts of the articles. A recent example is S CI ERC (Luan et al., 2018), a dataset of 500 richly annotated scientific abstracts containing mention spans and their types, coreference information between mentions, and binary relations annotations. We use S CI ERC to bootstrap our data annotation procedure (Section 3.2). There has been a lack of comprehensive IE datasets annotated at the docum"
2020.acl-main.670,P16-1105,0,0.0390436,"ractable for long documents because of the large number of entities. 7507 are mentioned once or twice rather than rely on the redundancy of information in their documents (e.g Rahman et al. (2016)), 2) TAC KBP relations are usually sentence-level binary relations between a query entity and an attribute (e.g Angeli et al. (2015)), while our relations are 4-ary, span the whole document, and can’t be split into multiple binary relations as discussed in Section 3.1. End-to-End Neural IE models With neural networks, a few end-to-end models have been proposed that perform multiple IE tasks jointly (Miwa and Bansal, 2016; Luan et al., 2018; Wadden et al., 2019). The closest to our work is DY GIE++ (Wadden et al., 2019), which does named entity recognition, binary relation extraction, and event extraction in one model. DY GIE++ is a span-enumeration based model which works well for short paragraphs but does not scale well to long documents. Instead, we use a CRF sequence tagger, which scales well. Our model also extracts 4-ary relations between salient entity clusters, which requires a more global view of the document than that needed to extract binary relations between all pairs of entity mentions. 3 Document"
2020.acl-main.670,P18-1199,0,0.022306,"tion. In both datasets, the task of relation extraction is formulated as a binary classification to check if a triplet of ground-truth entities is expressed in the document or not. Instead, our work focuses on a comprehensive list of information extraction tasks “from scratch”, where the input is the raw document. This makes the IE model more interesting as it requires to perform entity extraction, coreference resolution, saliency detection in addition to the relation extraction.2 General IE Most work in general domain IE focus on sentence-level information extraction (Stanovsky et al., 2018; Qin et al., 2018; Jie and Lu, 2019). Recently, however, Yao et al. (2019) introduced DocRED, a dataset of cross-sentence relation extractions on Wikipedia paragraphs. The paragraphs are of a comparable length to that of S CI ERC, which is significantly shorter than documents in our dataset. Previous IE work on the TAC KBP competitions (Ellis et al., 2017; Getman et al., 2018) comprise multiple knowledge base population tasks. Our task can be considered a variant of the TAC KBP “cold start” task that discovers new entities and entity attributes (slot filling) from scratch. Two aspects of our task make it more"
2020.acl-main.670,N18-1081,0,0.0270987,"nt-level relation extraction. In both datasets, the task of relation extraction is formulated as a binary classification to check if a triplet of ground-truth entities is expressed in the document or not. Instead, our work focuses on a comprehensive list of information extraction tasks “from scratch”, where the input is the raw document. This makes the IE model more interesting as it requires to perform entity extraction, coreference resolution, saliency detection in addition to the relation extraction.2 General IE Most work in general domain IE focus on sentence-level information extraction (Stanovsky et al., 2018; Qin et al., 2018; Jie and Lu, 2019). Recently, however, Yao et al. (2019) introduced DocRED, a dataset of cross-sentence relation extractions on Wikipedia paragraphs. The paragraphs are of a comparable length to that of S CI ERC, which is significantly shorter than documents in our dataset. Previous IE work on the TAC KBP competitions (Ellis et al., 2017; Getman et al., 2018) comprise multiple knowledge base population tasks. Our task can be considered a variant of the TAC KBP “cold start” task that discovers new entities and entity attributes (slot filling) from scratch. Two aspects of our"
2020.acl-main.670,D19-1585,1,0.93879,"large number of entities. 7507 are mentioned once or twice rather than rely on the redundancy of information in their documents (e.g Rahman et al. (2016)), 2) TAC KBP relations are usually sentence-level binary relations between a query entity and an attribute (e.g Angeli et al. (2015)), while our relations are 4-ary, span the whole document, and can’t be split into multiple binary relations as discussed in Section 3.1. End-to-End Neural IE models With neural networks, a few end-to-end models have been proposed that perform multiple IE tasks jointly (Miwa and Bansal, 2016; Luan et al., 2018; Wadden et al., 2019). The closest to our work is DY GIE++ (Wadden et al., 2019), which does named entity recognition, binary relation extraction, and event extraction in one model. DY GIE++ is a span-enumeration based model which works well for short paragraphs but does not scale well to long documents. Instead, we use a CRF sequence tagger, which scales well. Our model also extracts 4-ary relations between salient entity clusters, which requires a more global view of the document than that needed to extract binary relations between all pairs of entity mentions. 3 Document-Level IE Our goal is to extend sentence-"
2020.acl-main.670,D17-1004,0,\N,Missing
2020.acl-main.670,W18-2501,0,\N,Missing
2020.acl-main.721,D18-1476,0,0.231324,"Missing"
2020.acl-main.721,N19-2005,0,0.160401,"thors. Other recent approaches that attempt to address the layout structure of documents are CharGrid (Katti et al., 2018), which represents a document as a two-dimensional grid of characters, RiSER, an extraction technique targeted at templated emails (Kocayusufoglu et al., 2019), and that by Liu et al. (2018), which presents an RNN method for learning DOM-tree rules. However, none of these address the OpenIE setting, which requires understanding the relationship between different text fields on the page. The approaches most similar to ours are GraphIE (Qian et al., 2019) and the approach by Liu et al. (2019). Both approaches involve constructing a graph of text fields with edges representing 8106 Figure 2: A depiction of the web page representation module (left) and relation classifiers (right). horizontal and vertical adjacency, followed by an application of a GCN. However, neither approach makes use of visual features beyond text field adjacency nor DOM features, and both only consider extraction from a single text field rather than OpenIE. In addition, they show only very limited results on the ability of their model to generalize beyond the templates present in the training set. 3 Problem and"
2020.acl-main.721,N19-1309,1,0.600924,"n extraction (IE) from websites has largely ignored most of these features, instead relying only on HTML features specific to an individual website (Ferrara et al., 2014). This requires training data for every website targeted for extraction, an approach that cannot scale up if training data must be manually created. To circumvent manual data annotation, previous work used a distant supervision process requiring a knowledge base aligned to the website targeted for extraction (Gentile et al., 2015; Lockard et al., 2018), including for OpenIE extraction (Banko et al., 2007; Bronzi et al., 2013; Lockard et al., 2019). These methods, however, can only learn a website-specific model based on seed knowledge for the site, but cannot be generalized to the majority of websites with knowledge from new verticals, by long-tail specialists, and in different languages. In this paper, we introduce the task of zero-shot relation extraction from semi-structured websites, in which a learned model is applied to extract from a website that was not represented in its training data (Figure 1). Moreover, we introduce Z E RO S HOT C ERES , a graph neural network model that encodes semantic textual and visual patterns common a"
2020.acl-main.721,P14-1037,0,0.0282458,"tomatically generate training data based on distant supervision, and DIADEM (Furche et al., 2014) identifies matching rules for specific entity types. DOM-based OpenIE: WEIR (Bronzi et al., 2013) and OpenCeres (Lockard et al., 2019) offer OpenIE approaches to DOM extraction. The latter method uses visual features in a semi-supervised learning setting to identify candidate pairs that are visually similar to known (relation, object) pairs; however, the ultimate extraction model learned is still sitespecific and based on DOM features rather than the more generalizable visual or textual features. Pasupat and Liang (2014) present a zero-shot method for extraction from semi-structured webpages, but limit their work to extraction of entities rather than relationships and do not consider visual elements of the page. Multi-modal extraction: The incorporation of visual information into IE was proposed by Aumann et al. (2006), who attempted to learn a fitness function to calculate the visual similarity of a document to one in its training set to extract elements like headlines and authors. Other recent approaches that attempt to address the layout structure of documents are CharGrid (Katti et al., 2018), which repre"
2020.acl-main.721,N19-1082,0,0.11274,"extract elements like headlines and authors. Other recent approaches that attempt to address the layout structure of documents are CharGrid (Katti et al., 2018), which represents a document as a two-dimensional grid of characters, RiSER, an extraction technique targeted at templated emails (Kocayusufoglu et al., 2019), and that by Liu et al. (2018), which presents an RNN method for learning DOM-tree rules. However, none of these address the OpenIE setting, which requires understanding the relationship between different text fields on the page. The approaches most similar to ours are GraphIE (Qian et al., 2019) and the approach by Liu et al. (2019). Both approaches involve constructing a graph of text fields with edges representing 8106 Figure 2: A depiction of the web page representation module (left) and relation classifiers (right). horizontal and vertical adjacency, followed by an application of a GCN. However, neither approach makes use of visual features beyond text field adjacency nor DOM features, and both only consider extraction from a single text field rather than OpenIE. In addition, they show only very limited results on the ability of their model to generalize beyond the templates pres"
2020.acl-main.721,N19-1423,0,\N,Missing
2020.acl-main.85,P17-1171,0,0.198664,"0 (0.17) Sparse Representations of 587,000 tf-idf: amazon rose (1.00), . . . , 1991 (0.23), 2000 (0.19) Ours: 2000 (1.00), amazon (0.53), . . . , 1991 (0.21) Figure 1: An example of sparse vectors given a context from SQuAD. While tf-idf has high weights on infrequent n-grams, our contextualized sparse representation (S PARC) focuses on sematically related n-grams. Introduction Open-domain question answering (QA) is the task of answering generic factoid questions by looking up a large knowledge source, typically unstructured text corpora such as Wikipedia, and finding the answer text segment (Chen et al., 2017). One widely adopted strategy to handle such large corpus is to use an efficient document (or paragraph) retrieval technique to obtain a few relevant documents, and then use an accurate (yet expensive) QA model to read the retrieved documents and find the answer (Chen et al., 2017; Wang et al., 2018; Das et al., 2019; Yang et al., 2019). More recently, an alternative approach formulates the task as an end-to-end phrase retrieval problem by encoding and indexing every possible text span in a dense vector offline (Seo et al., 2018). The approach promises a massive speed advantage with 1 Code ava"
2020.acl-main.85,N19-1423,0,0.0473206,"019) introduce an endto-end, real-time open-domain QA approach to directly encode all phrases in documents agnostic of the question, and then perform similarity search on the encoded phrases. This is feasible by decomposing the scoring function F into two functions, a ˆ = argmaxxk Hx (xki:j ) · Hq (q) i:j where Hx is the query-agnostic phrase encoding, and Hq is the question encoding, and · denotes a fast inner product operation. Seo et al. (2019) propose to encode each phrase (and question) with the concatenation of a dense vector obtained via a deep contextualized word representation model (Devlin et al., 2019) and a sparse vector obtained via computing the tf-idf of the document (paragraph) that the phrase belongs to. We argue that the inherent characteristics of tf-idf, which is not learned and identical across the same document, has limited representational power. 2 Our method is inspired by the kernel method in SVMs (Cortes and Vapnik, 1995). Our goal in this paper is to propose a better and learned sparse representation model that can further improve the QA accuracy in the phrase retrieval setup. 3 Sparse Encoding of Phrases Our sparse model, unlike pre-computed sparse embeddings such as tf-idf"
2020.acl-main.85,P15-1144,0,0.0213031,"iate this issue, Seo et al. (2019) concatenate a term-frequency-based sparse vector with the dense vector to capture lexical information. However, such sparse vector is identical across the document (or paragraph), which means every word’s importance is equally considered regardless of its context (Figure 1). In this paper, we introduce a method to learn a Contextualized Sparse Representation (S PARC) for each phrase and show its effectiveness in opendomain QA under phrase retrieval setup. Related previous work (for a different task) often directly maps dense vectors to a sparse vector space (Faruqui et al., 2015; Subramanian et al., 2018), which can be at most only a few thousand dimensions due to computational cost and small gradients. We instead leverage rectified self-attention weights on the neighboring n-grams to scale up its cardinality to n-gram vocabulary space (billions), 912 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 912–919 c July 5 - 10, 2020. 2020 Association for Computational Linguistics allowing us to encode rich lexical information in each sparse vector. We kernelize2 the inner product space during training to avoid explicit mapping"
2020.acl-main.85,D18-1053,1,0.884869,"raining. We pre-compute and store all encoded phrase representations of all documents in Wikipedia (more than 5 million documents). It takes 600 GPU hours to index all phrases in Wikipedia. We use the same storage reduction and search techniques by Seo et al. (2019). For search, we perform dense search first and then rerank with sparse scores (DFS) or perform sparse search first and rerank with dense scores (SFS), or a combination of both (Hybrid). Comparisons For models using dedicated search engines, we show performances of DrQA (Chen et al., 2017), R3 (Wang et al., 2018), Paragraph Ranker (Lee et al., 2018), Multi-StepReasoner (Das et al., 2019), BERTserini (Yang et al., 2019), and Multi-passage BERT (Wang et al., 2019). For end-to-end models that do not rely on search engine results, D EN SPI (Seo et al., 2019), ORQA (Lee et al., 2019), and D EN SPI + S PARC (Ours) are evaluated. For D EN SPI and ours, ‘Hybrid’ search strategy is used. 4.2 Results Open-Domain QA Experiments Table 1 shows experimental results on two open-domain question answering datasets, comparing our method with previous pipeline and end-to-end approaches. On both datasets, our model with contextualized Model EM F1 Original D"
2020.acl-main.85,P19-1612,0,0.360615,"techniques by Seo et al. (2019). For search, we perform dense search first and then rerank with sparse scores (DFS) or perform sparse search first and rerank with dense scores (SFS), or a combination of both (Hybrid). Comparisons For models using dedicated search engines, we show performances of DrQA (Chen et al., 2017), R3 (Wang et al., 2018), Paragraph Ranker (Lee et al., 2018), Multi-StepReasoner (Das et al., 2019), BERTserini (Yang et al., 2019), and Multi-passage BERT (Wang et al., 2019). For end-to-end models that do not rely on search engine results, D EN SPI (Seo et al., 2019), ORQA (Lee et al., 2019), and D EN SPI + S PARC (Ours) are evaluated. For D EN SPI and ours, ‘Hybrid’ search strategy is used. 4.2 Results Open-Domain QA Experiments Table 1 shows experimental results on two open-domain question answering datasets, comparing our method with previous pipeline and end-to-end approaches. On both datasets, our model with contextualized Model EM F1 Original DrQA (Chen et al., 2017) BERT (Devlin et al., 2019) 69.5 84.1 78.8 90.9 QueryAgnostic LSTM + SA + ELMo D EN SPI D EN SPI + S PARC 52.7 73.6 76.4 62.7 81.7 84.8 Table 3: Results on the SQuAD development set. LSTM+SA+ELMo is a query-agno"
2020.acl-main.85,P18-1161,0,0.0242609,"t achieves the new state of the art even when compared to previous retrieve & read approaches, with at least 45x faster speed. 2 Background We focus on open-domain QA on unstructured text where the answer is a text span in a textual corpus (e.g., Wikipedia). Formally, given a set of K documents x1 , . . . , xK and a question q, the task ˆ by is to design a model that obtains the answer a k ˆ = argmaxxk F (xi:j , q), where F is the score a i:j model to learn and xki:j is a phrase consisting of words from the i-th to the j-th word in the k-th document. Pipeline-based methods (Chen et al., 2017; Lin et al., 2018; Wang et al., 2019) typically leverage a document retriever to reduce the number of documents to read, but they suffer from error propagation when wrong documents are retrieved and can be still slow due to the heavy reader model. Phrase-Indexed Open-domain QA As an alternative, Seo et al. (2018, 2019) introduce an endto-end, real-time open-domain QA approach to directly encode all phrases in documents agnostic of the question, and then perform similarity search on the encoded phrases. This is feasible by decomposing the scoring function F into two functions, a ˆ = argmaxxk Hx (xki:j ) · Hq (q"
2020.acl-main.85,D16-1264,0,0.0784844,"tations. Negative Sampling To learn robust phrase representations, we concatenate negative paragraphs to the original SQuAD paragraphs. To each paragraph x, we concatenate the paragraph xneg which was paired with the question whose dense representation h0neg is most similar to the original dense question representation h0 , following Seo et al. (2019). We find that adding tf-idf matching scores on the word-level logits of the negative paragraphs further improves the quality of sparse representations. 4 4.1 Experiments Experimental Setup Datasets SQuAD-Open is the open-domain version of SQuAD (Rajpurkar et al., 2016). We use 87,599 examples with the golden evidence paragraph to train our encoders and use 10,570 examples from dev set to test our model, as suggested by Chen et al. (2017). C URATED T REC consists of question-answer pairs from TREC QA (Voorhees ˇ et al., 1999) curated by Baudiˇs and Sediv` y (2015). We use 694 test set QA pairs for testing our model. We only train on SQuAD and test on both SQuADOpen and CuratedTREC, relying on the generalization ability of our model (zero-shot) for CuratedTREC. 914 Model C.TREC EM SQuAD-Open EM F1 s/Q 25.4* 28.4* 35.4* - 29.8** 29.1 30.2 31.9 38.6 53.0 37.5 3"
2020.acl-main.85,D18-1052,1,0.786989,"corpora such as Wikipedia, and finding the answer text segment (Chen et al., 2017). One widely adopted strategy to handle such large corpus is to use an efficient document (or paragraph) retrieval technique to obtain a few relevant documents, and then use an accurate (yet expensive) QA model to read the retrieved documents and find the answer (Chen et al., 2017; Wang et al., 2018; Das et al., 2019; Yang et al., 2019). More recently, an alternative approach formulates the task as an end-to-end phrase retrieval problem by encoding and indexing every possible text span in a dense vector offline (Seo et al., 2018). The approach promises a massive speed advantage with 1 Code available at https://github.com/jhyuklee/sparc. several orders of magnitude lower time complexity, but it performs poorly on entity-centric questions, often unable to disambiguate similar but different entities such as “1991” and “2001” in dense vector space. To alleviate this issue, Seo et al. (2019) concatenate a term-frequency-based sparse vector with the dense vector to capture lexical information. However, such sparse vector is identical across the document (or paragraph), which means every word’s importance is equally consider"
2020.acl-main.85,P19-1436,1,0.749468,"Wang et al., 2018; Das et al., 2019; Yang et al., 2019). More recently, an alternative approach formulates the task as an end-to-end phrase retrieval problem by encoding and indexing every possible text span in a dense vector offline (Seo et al., 2018). The approach promises a massive speed advantage with 1 Code available at https://github.com/jhyuklee/sparc. several orders of magnitude lower time complexity, but it performs poorly on entity-centric questions, often unable to disambiguate similar but different entities such as “1991” and “2001” in dense vector space. To alleviate this issue, Seo et al. (2019) concatenate a term-frequency-based sparse vector with the dense vector to capture lexical information. However, such sparse vector is identical across the document (or paragraph), which means every word’s importance is equally considered regardless of its context (Figure 1). In this paper, we introduce a method to learn a Contextualized Sparse Representation (S PARC) for each phrase and show its effectiveness in opendomain QA under phrase retrieval setup. Related previous work (for a different task) often directly maps dense vectors to a sparse vector space (Faruqui et al., 2015; Subramanian"
2020.acl-main.85,D19-1599,0,0.453997,"state of the art even when compared to previous retrieve & read approaches, with at least 45x faster speed. 2 Background We focus on open-domain QA on unstructured text where the answer is a text span in a textual corpus (e.g., Wikipedia). Formally, given a set of K documents x1 , . . . , xK and a question q, the task ˆ by is to design a model that obtains the answer a k ˆ = argmaxxk F (xi:j , q), where F is the score a i:j model to learn and xki:j is a phrase consisting of words from the i-th to the j-th word in the k-th document. Pipeline-based methods (Chen et al., 2017; Lin et al., 2018; Wang et al., 2019) typically leverage a document retriever to reduce the number of documents to read, but they suffer from error propagation when wrong documents are retrieved and can be still slow due to the heavy reader model. Phrase-Indexed Open-domain QA As an alternative, Seo et al. (2018, 2019) introduce an endto-end, real-time open-domain QA approach to directly encode all phrases in documents agnostic of the question, and then perform similarity search on the encoded phrases. This is feasible by decomposing the scoring function F into two functions, a ˆ = argmaxxk Hx (xki:j ) · Hq (q) i:j where Hx is th"
2020.acl-main.85,N19-4013,0,\N,Missing
2020.acl-tutorials.6,P09-1113,0,0.0263818,"Missing"
2020.acl-tutorials.6,N19-1082,0,0.0961229,"e 1. (30 mins) Introduction and Applications • Knowledge Base Population – Intro to knowledge graphs – Applications – Industry examples – Importance of the long tail • Unstructured, Semi-structured, and Tabular text – Unstructured Text – HTML and DOM trees – Webtables – Template learning vs. generalization • Schema-aligned extraction vs. OpenIE 5. (30 mins) Multi-modal extraction • Benefits of multi-modal extraction – Connecting tables and text (Ibrahim et al., 2019) – Visual signals for keyphrase extraction (Xiong et al., 2019) – Documents as images (Katti et al., 2018) – GCN-based encoders (Qian et al., 2019; Liu et al., 2019) • Multi-modal signals for creating training data (Wu et al., 2018) 24 • Multi-modal OpenIE PC co-chair for VLDB 2021, ICDE Industry 2019, VLDB Tutorial 2019, Sigmod 2018 and WAIM 2015. She has given multiple tutorials on data integration, graph mining, and knowledge management. Email: lunadong@amazon.com Homepage: http://lunadong.com/. Hannaneh Hajishirzi is an Assistant Professor at the Paul G. Allen School of Computer Science & Engineering at the University of Washington. She works on NLP, AI, and machine learning, particularly designing algorithms for semantic understand"
2020.acl-tutorials.6,N18-6003,0,0.0139676,"tational Linguistics https://doi.org/10.18653/v1/P17 provide insights from industry experiences building a production knowledge graph leveraging both unstructured and semi-structured text. Section 3 contains a full outline of planned content. Tutorial slides are available at https://sites. google.com/view/acl-2020-multi-modal-ie Relevance to ACL: Information Extraction is a core task in natural language processing, with the web serving as a rich source of information for constructing knowledge bases (KBs). A 2018 NAACL tutorial, “Scalable Construction and Reasoning of Massive Knowlege Bases” (Ren et al., 2018), provided an overview of recent IE and KB research. However, like most NLP research, that tutorial focused on methods that treat text as a simple string of natural language sentences in a txt file, while many real-world documents convey information via visual and layout relationships. A separate line of information extraction work has focused on learning to extract from these templatebased documents. As interest in multi-modal NLP techniques has grown in recent years, we think the community will be interested in a tutorial that compares and contrasts these approaches and examines recent resea"
2020.acl-tutorials.6,D19-1521,0,0.0168333,"etection (Venetis et al., 2011) • Joint approaches (LimayeGirija et al., 2010) Outline 1. (30 mins) Introduction and Applications • Knowledge Base Population – Intro to knowledge graphs – Applications – Industry examples – Importance of the long tail • Unstructured, Semi-structured, and Tabular text – Unstructured Text – HTML and DOM trees – Webtables – Template learning vs. generalization • Schema-aligned extraction vs. OpenIE 5. (30 mins) Multi-modal extraction • Benefits of multi-modal extraction – Connecting tables and text (Ibrahim et al., 2019) – Visual signals for keyphrase extraction (Xiong et al., 2019) – Documents as images (Katti et al., 2018) – GCN-based encoders (Qian et al., 2019; Liu et al., 2019) • Multi-modal signals for creating training data (Wu et al., 2018) 24 • Multi-modal OpenIE PC co-chair for VLDB 2021, ICDE Industry 2019, VLDB Tutorial 2019, Sigmod 2018 and WAIM 2015. She has given multiple tutorials on data integration, graph mining, and knowledge management. Email: lunadong@amazon.com Homepage: http://lunadong.com/. Hannaneh Hajishirzi is an Assistant Professor at the Paul G. Allen School of Computer Science & Engineering at the University of Washington. She works on NLP,"
2020.acl-tutorials.6,N19-1370,0,0.0170206,"sites The tutorial should be accessible to anyone with a background in natural language processing. It would be helpful to have a basic understanding of classification algorithms, preferably with some knowledge of neural network approaches, as well as unsupervised clustering algorithms. 5 Reading list • “Web-Scale Information Extraction With Vertex”, Gulhane et al. (2011) • “Ten Years of WebTables”, Cafarella et al. (2018) • “Fonduer: Knowledge Base Construction from Richly Formatted Data”, Wu et al. (2018) • “Document-level N-ary Relation Extraction with Multi-Scale Representation Learning”, Jia et al. (2019) • “Extraction and Integration of Partially Overlapping Web Sources” Bronzi et al. (2013) • “Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion”, Dong et al. (2014) • “A General Framework for Information Extraction Using Dynamic Span Graphs”, Luan et al. (2019) • “OpenCeres: When Open Information Extraction Meets the Semi-Structured Web”, Lockard et al. (2019) • “GraphIE: A Graph-Based Framework for Information Extraction”, Qian et al. (2019) 6 Presenters In alphabetical order, Xin Luna Dong is a Principal Scientist at Amazon, leading the efforts of constructing Amazon Pro"
2020.acl-tutorials.6,D18-1476,0,0.0247789,"Missing"
2020.acl-tutorials.6,N19-2005,0,0.0169222,"oduction and Applications • Knowledge Base Population – Intro to knowledge graphs – Applications – Industry examples – Importance of the long tail • Unstructured, Semi-structured, and Tabular text – Unstructured Text – HTML and DOM trees – Webtables – Template learning vs. generalization • Schema-aligned extraction vs. OpenIE 5. (30 mins) Multi-modal extraction • Benefits of multi-modal extraction – Connecting tables and text (Ibrahim et al., 2019) – Visual signals for keyphrase extraction (Xiong et al., 2019) – Documents as images (Katti et al., 2018) – GCN-based encoders (Qian et al., 2019; Liu et al., 2019) • Multi-modal signals for creating training data (Wu et al., 2018) 24 • Multi-modal OpenIE PC co-chair for VLDB 2021, ICDE Industry 2019, VLDB Tutorial 2019, Sigmod 2018 and WAIM 2015. She has given multiple tutorials on data integration, graph mining, and knowledge management. Email: lunadong@amazon.com Homepage: http://lunadong.com/. Hannaneh Hajishirzi is an Assistant Professor at the Paul G. Allen School of Computer Science & Engineering at the University of Washington. She works on NLP, AI, and machine learning, particularly designing algorithms for semantic understanding, reasoning, que"
2020.acl-tutorials.6,N19-1309,1,0.909077,"Recognition – Co-reference Resolution – Relation Extraction – Event Extraction • Featurization and Modeling – OpenTag (Zheng et al., 2018) – DyGIE (Luan et al., 2019) • Limited Training Data – Distant Supervision (Mintz et al., 2009) – Data Programming (Ratner et al., 2017) • OpenIE 3. (45 mins) IE from semi-structured documents • Supervised Wrapper Induction – Vertex (Gulhane et al., 2011) • Distantly Supervised approaches – LODIE (Ciravegna et al., 2012) – DIADEM (Furche et al., 2012) – Ceres (Lockard et al., 2018) • OpenIE / Schema-less approaches – WEIR (Bronzi et al., 2013) – OpenCeres (Lockard et al., 2019) Type of the tutorial: The tutorial will cover cutting-edge work in both unstructured and semi-structured information extraction, including visual and GCN-based approaches. However, our coverage of semistructured and tabular IE will cover introductory material since it is likely new to much of the NLP community. 3 • Common challenges, opportunities, and key intuitions 4. (15 mins) IE from tables • WebTables (Cafarella et al., 2018) • Subject detection (Venetis et al., 2011) • Joint approaches (LimayeGirija et al., 2010) Outline 1. (30 mins) Introduction and Applications • Knowledge Base Popula"
2020.acl-tutorials.6,N19-1308,1,0.84317,"separate line of information extraction work has focused on learning to extract from these templatebased documents. As interest in multi-modal NLP techniques has grown in recent years, we think the community will be interested in a tutorial that compares and contrasts these approaches and examines recent research that brings together textual, visual, and layout features of documents. 2 2. (45 mins) IE from unstructured text: • Tasks – Named Entity Recognition – Co-reference Resolution – Relation Extraction – Event Extraction • Featurization and Modeling – OpenTag (Zheng et al., 2018) – DyGIE (Luan et al., 2019) • Limited Training Data – Distant Supervision (Mintz et al., 2009) – Data Programming (Ratner et al., 2017) • OpenIE 3. (45 mins) IE from semi-structured documents • Supervised Wrapper Induction – Vertex (Gulhane et al., 2011) • Distantly Supervised approaches – LODIE (Ciravegna et al., 2012) – DIADEM (Furche et al., 2012) – Ceres (Lockard et al., 2018) • OpenIE / Schema-less approaches – WEIR (Bronzi et al., 2013) – OpenCeres (Lockard et al., 2019) Type of the tutorial: The tutorial will cover cutting-edge work in both unstructured and semi-structured information extraction, including visual"
2020.emnlp-main.153,P04-1035,0,\N,Missing
2020.emnlp-main.153,N16-3020,0,\N,Missing
2020.emnlp-main.153,N18-1023,0,\N,Missing
2020.emnlp-main.153,P19-1487,0,\N,Missing
2020.emnlp-main.153,D17-1042,0,\N,Missing
2020.emnlp-main.153,N19-1423,0,\N,Missing
2020.emnlp-main.153,D19-1420,0,\N,Missing
2020.emnlp-main.153,D19-1276,0,\N,Missing
2020.emnlp-main.466,D13-1160,0,0.0755468,"a dataset with 14,042 annotations on NQ- OPEN questions containing diverse types of ambiguity. 3. We introduce the first baseline models that produce multiple answers to open-domain questions, with experiments showing their effectiveness in learning from our data while highlighting avenues for future work. 2 Related Work Open-domain Question Answering requires a system to answer any factoid question based on evidence provided by a large corpus such as Wikipedia (Voorhees et al., 1999; Chen et al., 2017). Existing benchmarks use questions of various types, from open-ended information-seeking (Berant et al., 2013; Kwiatkowski et al., 2019; Clark et al., 2019) to more specialized trivia/quiz (Joshi et al., 2017; Dunn et al., 2017). To the best of our knowledge, all existing formulations assume each question has a single clear answer. Our work is built upon an open-domain version of NATURAL Q UESTIONS (Kwiatkowski et al., 2019), denoted NQ- OPEN, composed of questions posed by real users of Google search, each with an answer drawn from Wikipedia. NQ- OPEN has promoted several recent advances in open5784 domain question answering (Lee et al., 2019; Asai et al., 2020; Min et al., 2019a,b; Guu et al., 2020"
2020.emnlp-main.466,P17-1171,0,0.0606172,"an open-domain question, along with disambiguated questions to differentiate them. 2. We construct A MBIG NQ, a dataset with 14,042 annotations on NQ- OPEN questions containing diverse types of ambiguity. 3. We introduce the first baseline models that produce multiple answers to open-domain questions, with experiments showing their effectiveness in learning from our data while highlighting avenues for future work. 2 Related Work Open-domain Question Answering requires a system to answer any factoid question based on evidence provided by a large corpus such as Wikipedia (Voorhees et al., 1999; Chen et al., 2017). Existing benchmarks use questions of various types, from open-ended information-seeking (Berant et al., 2013; Kwiatkowski et al., 2019; Clark et al., 2019) to more specialized trivia/quiz (Joshi et al., 2017; Dunn et al., 2017). To the best of our knowledge, all existing formulations assume each question has a single clear answer. Our work is built upon an open-domain version of NATURAL Q UESTIONS (Kwiatkowski et al., 2019), denoted NQ- OPEN, composed of questions posed by real users of Google search, each with an answer drawn from Wikipedia. NQ- OPEN has promoted several recent advances in"
2020.emnlp-main.466,P17-1147,1,0.891536,"Missing"
2020.emnlp-main.466,2020.emnlp-main.550,1,0.929349,"found every possible interpretation of a question. Nonetheless, we are able to collect high quality data covering high levels of ambiguity (2.1 distinct answers per question on average) with high estimated agreement (89.0 F1) on valid answers. The types of ambiguity are diverse and sometimes subtle (Table 1), including ambiguous entity or event references, or ambiguity over the answer type; many are only apparent after examining one or more Wikipedia pages. To establish initial performance levels on this data, we present a set of strong baseline methods. We extend a state-of-the-art QA model (Karpukhin et al., 2020) with three new components: (1) set-based question answering with a sequence-tosequence model, (2) a question disambiguation model, and (3) a modification to democratic cotraining (Zhou and Goldman, 2004) which leverages the partial supervision available in the full NQ- OPEN dataset. We also do an ablation study and qualitative analysis, which suggest there is significant room for future work on this task. To summarize, our contributions are threefold. 1. We introduce A MBIG QA, a new task which requires identifying all plausible answers to an open-domain question, along with disambiguated que"
2020.emnlp-main.466,Q19-1026,0,0.17048,"Missing"
2020.emnlp-main.466,P19-1612,0,0.407958,"Missing"
2020.emnlp-main.466,2020.acl-main.703,1,0.719676,"Missing"
2020.emnlp-main.466,N18-2089,1,0.866416,"Missing"
2020.emnlp-main.466,N19-4009,0,0.0508657,"Missing"
2020.emnlp-main.466,P02-1040,0,0.106889,"imilarity function f valued in [0, 1]. ci = max I[yi ∈ Y¯j ]f (xi , x ¯j ). 1≤j≤n Intuitively, ci considers (1) the correctness of the answer and (2) the similarity f (xi , x ¯j ) between the predicted and reference question. We calculate F1 treating the ci as measures of correctness: P P ci i ci precf = , recf = i , m n 2 × precf × recf F1f = . precf + recf We consider three choices of Ff . F1ans is the F1 score on answers only, where f always yields 1. This may be used without the question disambiguation step. F1BLEU accounts for string similarity between questions, calculating f with BLEU (Papineni et al., 2002). F1EDIT-F1 uses E DIT-F1 as f , where E DIT-F1 is a new measure that represents each disambiguated question by its added and 5785 deleted unigrams compared to the prompt question, and computes the F1 score between them. For example, consider the prompt question “Who made the play the crucible?”, the reference “Who wrote the play the crucible?” and the prediction “Who made the play the crucible in 2012?”. The gold edits3 here are * -made , +wrote + while the predicted edits are * +in , +2012 +. Their E DIT-F1 is thus zero, even though the questions are similar. Unlike BLEU which we use to dire"
2020.emnlp-main.466,P18-1255,0,0.029512,"Missing"
2020.emnlp-main.466,N19-1013,0,0.058877,"Missing"
2020.emnlp-main.466,D16-1158,0,0.0163108,"xamples with multiple question-answer pairs (multi) are lower, indicating that predicting all plausible answers is more challenging than predicting a single answer, as expected. S PAN S EQ G EN also obtains the best performance in F1BLEU and F1EDIT-F1 , although their absolute values are low in general; we discuss this in our question disambiguation ablations below. There is a substantial difference in performance between development and test overall, likely due to distributional differences in the original questions 5 This problem has also been reported in other conditional generation tasks (Sountsov and Sarawagi, 2016; Stahlberg and Byrne, 2019); we leave it for future work. in NQ- OPEN; detailed discussion is in Appendix B. Effect of co-training. The last two rows of Table 3 reports the effect of our co-training method. As co-training requires multiple trained models, we compare with a naive ensemble. While we see gains from ensembling alone, an ensemble trained with the co-training method achieves the best performance on all metrics. This result demonstrates the potential of jointly using A MBIG NQ and partial supervision from NQ- OPEN. Ablations on question disambiguation. Table 4 reports results of an"
2020.emnlp-main.466,D19-1331,0,0.0139938,"n-answer pairs (multi) are lower, indicating that predicting all plausible answers is more challenging than predicting a single answer, as expected. S PAN S EQ G EN also obtains the best performance in F1BLEU and F1EDIT-F1 , although their absolute values are low in general; we discuss this in our question disambiguation ablations below. There is a substantial difference in performance between development and test overall, likely due to distributional differences in the original questions 5 This problem has also been reported in other conditional generation tasks (Sountsov and Sarawagi, 2016; Stahlberg and Byrne, 2019); we leave it for future work. in NQ- OPEN; detailed discussion is in Appendix B. Effect of co-training. The last two rows of Table 3 reports the effect of our co-training method. As co-training requires multiple trained models, we compare with a naive ensemble. While we see gains from ensembling alone, an ensemble trained with the co-training method achieves the best performance on all metrics. This result demonstrates the potential of jointly using A MBIG NQ and partial supervision from NQ- OPEN. Ablations on question disambiguation. Table 4 reports results of an ablation experiment on quest"
2020.emnlp-main.466,D19-1284,1,0.86334,"Missing"
2020.emnlp-main.466,N19-1300,0,\N,Missing
2020.emnlp-main.466,N19-1423,0,\N,Missing
2020.emnlp-main.466,D19-1172,0,\N,Missing
2020.emnlp-main.609,N18-2004,0,0.0643055,"Missing"
2020.emnlp-main.609,D12-1091,0,0.0472401,"Missing"
2020.emnlp-main.609,N19-1053,0,0.0404579,"Missing"
2020.emnlp-main.609,N15-1110,1,0.893905,"Missing"
2020.emnlp-main.609,N19-1423,0,0.0215467,"ion sentences as a source of claims both speeds the claim generation process and guarantees that the topics discussed in S CI FACT are representative of the research literature. In addition, citation links indicate the exact documents likely to contain evidence necessary to verify a given claim. We establish performance baselines on S CI FACT with an approach similar to DeYoung et al. (2020a), which achieves strong performance on the F EVER claim verification dataset (Thorne et al., 2018). Our baseline is a pipeline system which retrieves abstracts related to an input claim, uses a BERTbased (Devlin et al., 2019) sentence selector to identify rationale sentences, and labels each abstract as S UPPORTS, R EFUTES, or N O I NFO with respect to the claim. We demonstrate that our baseline can benefit from training on claims from domains including Wikipedia articles and politics. We showcase the ability of our model to verify expert-written claims concerning the novel coronavirus COVID-19 against the newly-released CORD-19 corpus (Wang et al., 2020). Expert annotators judge retrieved evidence to be plausible for 23 of 36 claims.1 Our results and analyses demonstrate the importance of the new task and dataset"
2020.emnlp-main.609,2020.bionlp-1.13,0,0.267257,"decision. To create the dataset, we develop a novel annotation protocol in which annotators re-formulate naturally occurring claims in the scientific literature – citation sentences – into atomic scientific claims. Using citation sentences as a source of claims both speeds the claim generation process and guarantees that the topics discussed in S CI FACT are representative of the research literature. In addition, citation links indicate the exact documents likely to contain evidence necessary to verify a given claim. We establish performance baselines on S CI FACT with an approach similar to DeYoung et al. (2020a), which achieves strong performance on the F EVER claim verification dataset (Thorne et al., 2018). Our baseline is a pipeline system which retrieves abstracts related to an input claim, uses a BERTbased (Devlin et al., 2019) sentence selector to identify rationale sentences, and labels each abstract as S UPPORTS, R EFUTES, or N O I NFO with respect to the claim. We demonstrate that our baseline can benefit from training on claims from domains including Wikipedia articles and politics. We showcase the ability of our model to verify expert-written claims concerning the novel coronavirus COVID"
2020.emnlp-main.609,N18-2017,0,0.0663832,"Missing"
2020.emnlp-main.609,K19-1046,0,0.381192,"e reported and the risks associated with making decisions based on outdated or incomplete information. As a result, there is a need for automated tools to assist researchers and the public in evaluating the veracity of scientific claims. ∗ Work performed during internship with the Allen Institute for Artificial Intelligence. Fact-checking – a task in which the veracity of an input claim is verified against a corpus of documents that support or refute the claim – has been studied to combat the proliferation of misinformation in political news, social media, and on the web (Thorne et al., 2018; Hanselowski et al., 2019). However, verifying scientific claims poses new challenges to both dataset construction and effective modeling. While political claims are readily available on fact-checking websites and can be verified by crowd workers, annotators with extensive domain knowledge are required to generate and verify scientific claims. In addition, NLP systems for scientific claim verification must possess additional capabilities beyond those required to verify factoid claims. For instance, to verify the claim shown in Figure 1, a 7534 Proceedings of the 2020 Conference on Empirical Methods in Natural Language"
2020.emnlp-main.609,N16-1138,0,\N,Missing
2020.emnlp-main.609,P17-4002,0,\N,Missing
2020.emnlp-main.609,S17-2006,0,\N,Missing
2020.emnlp-main.609,P17-2067,0,\N,Missing
2020.emnlp-main.609,D19-1341,0,\N,Missing
2020.emnlp-main.609,2020.acl-main.740,1,\N,Missing
2020.emnlp-main.609,W14-2508,0,\N,Missing
2020.emnlp-main.609,2020.nlpcovid19-acl.1,1,\N,Missing
2020.emnlp-main.707,D19-1219,0,0.0288482,"bilities are not tied to L XMERT’s underlying architecture. We expect that the entire family of multimodal BERT models can be enhanced with image generative capabilities using our introduced strategy. 2 Related works Visual-Language transformer models Recent multi-modal pre-training models show significant improvements on a wide range of downstream tasks, including discriminiative (eg., visual question answering) and generation task (eg. image captioning (Zhou et al., 2020)). Some methods use a single transformer architecture to jointly encode text and image (Li et al., 2019; Su et al., 2019; Alberti et al., 2019; Rahman et al., 2020; Li et al., 2020; Chen et al., 2019; Qi et al., 2020; Huang et al., 2020), while others use two-stream architectures (Lu et al., 2019, 2020; Tan and Bansal, 2019). These models typically consume object detection features. We probe this family of models at the task of image generation and present extensions that enable them to reliably generate images. Sequence generation with undirectional transformer When generating sequences with conventional transformer language models, it is natural to sample tokens from left to right. However, since undirectional transformers (eg. BE"
2020.emnlp-main.707,P19-1644,0,0.0625658,"ages annotated per worker. A high H UMMUS score reveals that the generated images contain the corresponding semantics, well enough to be recognized. The masked word is chosen from one of 3 categories: 80 C OCO nouns, verbs and colors. 6.2 Evaluating Visual Question Answering We train and evaluate models for visual question answering using the VQA2.0 (Goyal et al., 2019) and GQA (Hudson and Manning, 2019) datasets, which provide an image and a question and require the model to generate an answer. 6.3 Evaluating Visual Reasoning We train and evaluate models for visual reasoning using the NLVR2 (Suhr et al., 2019) dataset and report numbers on the dev and test-P splits. The NLVR2 dataset requires models to look at two images and determine if an accompanying caption is True or False. This is a particularly challenging dataset for present day vision and language models. 7 Experimental Results We now present a comparison of X-L XMERTwith several baselines on the generative and discriminative tasks, along with ablation studies and qualitative results. We also show the generality of our techniques via extending U NITER to create X-U NITER. 7.1 Quantitative Results Table 1 provides detailed metrics for X-L X"
2020.emnlp-main.707,D19-1514,0,0.364506,"ly transformers (Wang and Cho, 2019; Dong et al., 2019; Liao et al., 2020) adapt these models towards this capability using sampling procedures. Such techniques have also been adapted successfully for image captioning - inputting an image and sampling the textual side of the model to generate a relevant caption (Zhou et al., 2020). This begs the question: Can we go the other way and sample images from input pieces of text? i.e. Do vision-and-language BERT models know how to paint? In this work, we probe the ability of a powerful and popular representative from this family of models - L XMERT (Tan and Bansal, 2019), to produce high fidelity and semantically meaningful images conditioned on captions. Interestingly, our analysis leads us to the conclusion that L XMERT in its current form does not possess the ability to paint it produces images that have little resemblance to natural images. This is a somewhat surprising finding given L XMERT’s masked training objectives for both modalities and its impressive performance on tasks that seemingly require a similar skill set. We find that this is largely due to the regression training objective used by this family of models to predict masked features on the v"
2020.emnlp-main.707,W19-2304,0,0.342024,"ive results as well as recent probing mechanisms (Ilharco et al., 2020) suggest that these models are able to capture a variety of semantics in images including objects, attributes and their relationships and ground these in natural language. While these models have been extensively evaluated over several discriminative tasks, relatively little attention has been paid to their generative capabilities. Bidirectional transformer models like BERT which exploit context preceding and following the current token are not explicitly designed for generation. Recent work for language-only transformers (Wang and Cho, 2019; Dong et al., 2019; Liao et al., 2020) adapt these models towards this capability using sampling procedures. Such techniques have also been adapted successfully for image captioning - inputting an image and sampling the textual side of the model to generate a relevant caption (Zhou et al., 2020). This begs the question: Can we go the other way and sample images from input pieces of text? i.e. Do vision-and-language BERT models know how to paint? In this work, we probe the ability of a powerful and popular representative from this family of models - L XMERT (Tan and Bansal, 2019), to produce h"
2020.emnlp-main.746,D15-1075,0,0.585049,"dence) corresponds to easy-to-learn examples, the bottomleft corner (low variability, low confidence) corresponds to hard-to-learn examples, and examples on the right (with high variability) are ambiguous; all definitions are with respect to the RO BERTA-large model. The modal group in the data is formed by the easy-to-learn regions. For clarity we only plot 25K random samples from the SNLI train set. Fig. 8b in App. §C shows the same map in greater relief. The creation of large labeled datasets has fueled the advance of AI (Russakovsky et al., 2015; Antol et al., 2015) and NLP in particular (Bowman et al., 2015; Rajpurkar et al., 2016). The common belief is that the more abundant the labeled data, the higher the likelihood of learning diverse phenomena, which in turn leads to models that generalize well. In practice, however, out-of-distribution Work done at the Allen Institute for AI. ambiguou 0.4 0.2 Introduction ∗ 0.6 correct. 0.0 0.2 0.3 0.5 0.7 0.8 1.0 (OOD) generalization remains a challenge (Yogatama et al., 2019; Linzen, 2020); and, while recent large pretrained language models help, they fail to close this gap (Hendrycks et al., 2020). This urges a closer look at datasets, where not all exa"
2020.emnlp-main.746,P17-1152,0,0.0393322,"scussion (with empirical justifications) on connections between training dynamics measures and dropout-based (Srivastava et al., 2014), first-principles uncertainty estimates. These relations are further supported by previous work, which showed that deep ensembles provide well-calibrated uncertainty estimates (Lakshminarayanan et al., 2017; Gustafsson et al., 2019; Snoek et al., 2019). Generally, such approaches ensemble models trained from scratch; while ensembles of training checkpoints lose some diversity (Fort et al., 2019), they offer a cheaper alternative capturing some of the benefits (Chen et al., 2017a). Future work will involve investigation of such alternatives for building data maps. 7 Related Work Our work builds data maps using training dynamics measures for scoring data instances. Loss landscapes (Xing et al., 2018) are similar to training dynamics, but also consider variables from the stochastic optimization algorithm. Toneva et al. (2018) also use training dynamics to find train examples which are frequently “forgotten”, i.e., mis9282 classified during a later epoch of training, despite being classified correctly earlier; our correctness metric provides similar discrete scores, and"
2020.emnlp-main.746,N19-1423,0,0.0734792,"Missing"
2020.emnlp-main.746,D19-1224,1,0.892059,"Missing"
2020.emnlp-main.746,N18-2017,1,0.861687,"Missing"
2020.emnlp-main.746,2020.acl-main.244,0,0.0262881,"Missing"
2020.emnlp-main.746,N13-1132,0,0.0221471,"e ability of simple linear classifiers to predict them correctly. While AFLite, among others (Li and Vasconcelos, 2019; Gururangan et al., 2018), advocate removing “easy” instances from the dataset, our work shows that easy-to-learn instances can be useful. Similar intuitions have guided other work such as curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010; Lee and Grauman, 2011) where all examples are prioritized based on their “difficulty”. Other approaches have used training loss (Han et al., 2018; Arazo et al., 2019; Shen and Sanghavi, 2019), confidence (Hovy et al., 2013), and meta-learning (Ren et al., 2018), to differentiate instances within datasets. Perhaps our measures are the closest to those from Chang et al. (2017); they propose prediction variance and threshold closeness—which correspond to variability and confidence, respectively.18 However, they use these measures to reweight all instances, similar to sampling effective batches in online learning (Loshchilov and Hutter, 2016). Our work, instead, does a hard selection for the purpose of studying different groups within data. Our methods are also reminiscent of active learning methods (Settles, 2009;"
2020.emnlp-main.746,D17-1215,0,0.127475,"Missing"
2020.emnlp-main.746,W02-2015,0,0.109712,"oring data instances. Loss landscapes (Xing et al., 2018) are similar to training dynamics, but also consider variables from the stochastic optimization algorithm. Toneva et al. (2018) also use training dynamics to find train examples which are frequently “forgotten”, i.e., mis9282 classified during a later epoch of training, despite being classified correctly earlier; our correctness metric provides similar discrete scores, and results in models with better performance. Variants of such approaches address catastrophic forgetting, and are useful for analyzing data instances (Pan et al., 2020; Krymolowski, 2002). Prior work has proposed other criteria to score instances. AFLite (LeBras et al., 2020) is an adversarial filtering algorithm which ranks instances based on their “predictability”, i.e. the ability of simple linear classifiers to predict them correctly. While AFLite, among others (Li and Vasconcelos, 2019; Gururangan et al., 2018), advocate removing “easy” instances from the dataset, our work shows that easy-to-learn instances can be useful. Similar intuitions have guided other work such as curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010; Lee and Grauman"
2020.emnlp-main.746,2020.acl-main.465,0,0.0320741,"map in greater relief. The creation of large labeled datasets has fueled the advance of AI (Russakovsky et al., 2015; Antol et al., 2015) and NLP in particular (Bowman et al., 2015; Rajpurkar et al., 2016). The common belief is that the more abundant the labeled data, the higher the likelihood of learning diverse phenomena, which in turn leads to models that generalize well. In practice, however, out-of-distribution Work done at the Allen Institute for AI. ambiguou 0.4 0.2 Introduction ∗ 0.6 correct. 0.0 0.2 0.3 0.5 0.7 0.8 1.0 (OOD) generalization remains a challenge (Yogatama et al., 2019; Linzen, 2020); and, while recent large pretrained language models help, they fail to close this gap (Hendrycks et al., 2020). This urges a closer look at datasets, where not all examples might contribute equally towards learning (Vodrahalli et al., 2018). However, the scale of data can make this assessment challenging. How can we automatically characterize data instances with respect to their role in achieving good performance in- and out-of- distribution? Answering this question may take us a step closer to bridging the gap between dataset collection and broader task 9275 Proceedings of the 2020 Conferenc"
2020.emnlp-main.746,2021.ccl-1.108,0,0.106443,"Missing"
2020.emnlp-main.746,N19-1262,0,0.0341628,"Missing"
2020.emnlp-main.746,N18-1101,0,0.32608,"constructed using the RO BERTA-large model (Liu et al., 2019). The map reveals three distinct regions in the dataset: a region with instances whose true class probabilities fluctuate frequently during training (high variability), and are hence ambiguous for the model; a region with easy-to-learn instances that the model predicts correctly and consistently (high confidence, low variability); and a region with hard-to-learn instances with low confidence, low variability, many of which we find are mislabeled during annotation .1 Similar regions are observed across three other datasets: MultiNLI (Williams et al., 2018), WinoGrande (Sakaguchi et al., 2020) and SQuAD (Rajpurkar et al., 2016), with respect to respective RO BERTA-large classifiers. We further investigate the above regions by training models exclusively on examples from each region (§3). Training on ambiguous instances promotes generalization to OOD test sets, with little or no effect on in-distribution (ID) performance.2 Our data maps also reveal that datasets contain a majority of easy-to-learn instances, which are not as critical for ID or OOD performance, but without any such instances, training could fail to converge (§4). In §5, we show th"
2020.emnlp-main.746,W18-5446,0,0.0616166,"Missing"
2020.emnlp-main.746,N13-1086,0,0.0146549,", 2016). Our work, instead, does a hard selection for the purpose of studying different groups within data. Our methods are also reminiscent of active learning methods (Settles, 2009; Peris and Casacuberta, 2018; P.V.S and Meyer, 2019), such as uncertainty sampling (Lewis and Gale, 1994) which selects (unlabeled) data points, which a model trained on a small labeled subset, has least confidence in, or predicts as farthest (in vector space, based on cosine similarity) (Sener and Savarese, 2018; Wolf, 2011). Our approach uses labeled data for selection, similar to core-set selection approaches (Wei et al., 2013). Active learning approaches could be used in conjunction with data maps to create better datasets, similar to approaches proposed in Mishra et al. (2020). For instance, creating datasets 18 They also consider confidence intervals; our preliminary experiments, with and without, yielded similar results. with more ambiguous examples (with respect to a given model) could make it beneficial for OOD generalization. Data error detection also involves instance scoring. Influence functions (Koh and Liang, 2017), forgetting events (Toneva et al., 2018), cross validation (Chen et al., 2019), Shapely val"
2020.emnlp-main.86,P17-1171,0,0.115694,"ref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answering tasks (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017) present a model with a ques1137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1137–1147, c November 16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-i"
2020.emnlp-main.86,N19-1405,0,0.0300682,"Missing"
2020.emnlp-main.86,D13-1184,0,0.027616,"Missing"
2020.emnlp-main.86,2020.tacl-1.30,0,0.287457,"question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second question in Figure 2. Separation of questions from answer contexts Many prior datasets (e.g.: WhoDidWhat (Onishi et al., 2016), NewsQA (Trischler et al., 2016), DuoRC (Saha et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020)) have tried to remove simple lexical heuristics from reading comprehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, and must retrieve the necessary context to answer the question (Chen et al.,"
2020.emnlp-main.86,D19-1606,1,0.825824,". The answer is the underlined span. Introduction Humans often read text with the goal of obtaining information. Given that a single document is unlikely to contain all the information a reader might need, the reading process frequently involves identifying the information present in the given document, and what is missing, followed by locating a different source that could potentially contain the missing information. Most recent read∗ Work done as an intern at the Allen Institute for AI. ing comprehension tasks, such as SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019b), or Quoref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answering tasks (Chen et al., 2017; Joshi"
2020.emnlp-main.86,N19-1423,0,0.0355161,"C Task Overview Formally, a system tackling IIRC is provided with the following inputs: a question Q; a passage P ; a set of links contained in the passage, L = {li }N i=1 ; and the set of articles those links lead to, A = {ai }N i=1 . The surface form of each link, li is a Baseline Model 1. Identify relevant links 2. Select passages from linked articles 3. Pass the concatenated passages to a QA model 3.2.1 Identifying Links To identify the set of relevant links, L0 , in a passage, P, for a question, Q, the model first encodes the concatenation of the question and original passage using BERT (Devlin et al., 2019). It then concatenates the encoded representations of the first and last tokens of each link as input to a scoring function, following the span classification procedure used by Joshi et al. (2013), selecting any links that score above a threshold g. P 0 = BERT([Q||P ]) Score(l) = f ([p0i kp0j ]), l = (pi ...pj , a) L0 = {l : Score(l) &gt; g} where l is a link covering tokens pi ...pj linking to article a. 3.2.2 Selecting Context Given the set, L0 from the previous step, the model then must select relevant context passages from the documents. For each document, it first splits the document into ov"
2020.emnlp-main.86,N19-1246,1,0.90994,"tion for answering the question. The answer is the underlined span. Introduction Humans often read text with the goal of obtaining information. Given that a single document is unlikely to contain all the information a reader might need, the reading process frequently involves identifying the information present in the given document, and what is missing, followed by locating a different source that could potentially contain the missing information. Most recent read∗ Work done as an intern at the Allen Institute for AI. ing comprehension tasks, such as SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019b), or Quoref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answeri"
2020.emnlp-main.86,P17-1147,0,0.10626,"2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answering tasks (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017) present a model with a ques1137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1137–1147, c November 16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-information QA datase"
2020.emnlp-main.86,2020.emnlp-main.550,0,0.0351698,"separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, and must retrieve the necessary context to answer the question (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017; Yang et al., 2018; Seo et al., 2019; Karpukhin et al., 2020; Min et al., 2019a). IIRC is similar in that it also requires the retrieval of missing information. However, the questions are grounded in a given paragraph, meaning that a system must examine more than just the question in order to know what to retrieve. Most questions in IIRC do not make sense in an open-domain setting, without their associated paragraphs. Unanswerable questions Unlike SQuAD 2.0 (Rajpurkar et al., 2018) where the unanswerable questions were written to be close to answerable questions, IIRC contains naturally unanswerable questions that were not written with the goal of bein"
2020.emnlp-main.86,D19-1281,1,0.845722,"able questions were written to be close to answerable questions, IIRC contains naturally unanswerable questions that were not written with the goal of being unanswerable, a property that our dataset shares with NewsQA (Trischler et al., 2016), Natural Questions (Kwiatkowski et al., 2019), and TyDi QA (Clark et al., 2020). Results shown in Section 4.3 indicate that these questions cannot be trivially distinguished from answerable questions. Incomplete Information QA A few prior datasets have explored question answering given incomplete information, such as science facts (Mihaylov et al., 2018; Khot et al., 2019). However, these datasets contain multiple choice questions, and the answer choices provide hints as to what information may be needed. Yuan et al. (2020) explore this as well using a POMDP in which the context in existing QA datasets is hidden from the model until it explicitly searches for it. 7 Conclusion We introduced IIRC, a new dataset of incompleteinformation reading comprehension questions. These questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex wa"
2020.emnlp-main.86,Q19-1026,0,0.234108,"n et al., 2019b). In contrast, each question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second question in Figure 2. Separation of questions from answer contexts Many prior datasets (e.g.: WhoDidWhat (Onishi et al., 2016), NewsQA (Trischler et al., 2016), DuoRC (Saha et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020)) have tried to remove simple lexical heuristics from reading comprehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, and must retrieve the necessary context to answe"
2020.emnlp-main.86,2021.ccl-1.108,0,0.0756007,"Missing"
2020.emnlp-main.86,D18-1260,1,0.883113,"018) where the unanswerable questions were written to be close to answerable questions, IIRC contains naturally unanswerable questions that were not written with the goal of being unanswerable, a property that our dataset shares with NewsQA (Trischler et al., 2016), Natural Questions (Kwiatkowski et al., 2019), and TyDi QA (Clark et al., 2020). Results shown in Section 4.3 indicate that these questions cannot be trivially distinguished from answerable questions. Incomplete Information QA A few prior datasets have explored question answering given incomplete information, such as science facts (Mihaylov et al., 2018; Khot et al., 2019). However, these datasets contain multiple choice questions, and the answer choices provide hints as to what information may be needed. Yuan et al. (2020) explore this as well using a POMDP in which the context in existing QA datasets is hidden from the model until it explicitly searches for it. 7 Conclusion We introduced IIRC, a new dataset of incompleteinformation reading comprehension questions. These questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved infor"
2020.emnlp-main.86,D19-1284,1,0.917274,"16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-information QA datasets such as HotpotQA (Yang et al., 2018). However, they do not capture the information-seeking questions that arise from reading a single document with partial information (Min et al., 2019b; Chen and Durrett, 2019). We present a new dataset of incomplete information reading comprehension questions, IIRC, to address both of these limitations. IIRC is a crowdsourced dataset of 13441 questions over 5698 paragraphs from English Wikipedia, with most of the questions requiring information from one or more documents hyperlinked to the associated paragraphs, in addition to the original paragraphs themselves. Our crowdsourcing process (Section 2) ensures the questions are naturally information-seeking by decoupling question and answer collection pipelines. Crowd workers are instructed t"
2020.emnlp-main.86,P19-1416,1,0.923311,"16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-information QA datasets such as HotpotQA (Yang et al., 2018). However, they do not capture the information-seeking questions that arise from reading a single document with partial information (Min et al., 2019b; Chen and Durrett, 2019). We present a new dataset of incomplete information reading comprehension questions, IIRC, to address both of these limitations. IIRC is a crowdsourced dataset of 13441 questions over 5698 paragraphs from English Wikipedia, with most of the questions requiring information from one or more documents hyperlinked to the associated paragraphs, in addition to the original paragraphs themselves. Our crowdsourcing process (Section 2) ensures the questions are naturally information-seeking by decoupling question and answer collection pipelines. Crowd workers are instructed t"
2020.emnlp-main.86,D16-1241,0,0.027219,"se questions can be answered by focusing on just one of the facts used for building the questions (Min et al., 2019b). In contrast, each question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second question in Figure 2. Separation of questions from answer contexts Many prior datasets (e.g.: WhoDidWhat (Onishi et al., 2016), NewsQA (Trischler et al., 2016), DuoRC (Saha et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020)) have tried to remove simple lexical heuristics from reading comprehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a s"
2020.emnlp-main.86,P18-2124,0,0.0867562,"hat provide the missing information for answering the question. The answer is the underlined span. Introduction Humans often read text with the goal of obtaining information. Given that a single document is unlikely to contain all the information a reader might need, the reading process frequently involves identifying the information present in the given document, and what is missing, followed by locating a different source that could potentially contain the missing information. Most recent read∗ Work done as an intern at the Allen Institute for AI. ing comprehension tasks, such as SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019b), or Quoref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open"
2020.emnlp-main.86,D16-1264,0,0.0476757,"d context for unanswerable questions. We do this because by definition, unanswerable questions do not have annotated answer context. 4 4.1 Experiments Evaluation Metrics We use two evaluation metrics to compare model performance: Exact-Match (EM), and a numeracyfocused (macro-averaged) F1 score, which measures overlap between a bag-of-words representation of the gold and predicted answers. Due to the number of numeric answers in the data, we follow the evaluation methods used by DROP (Dua et al., 2019b). Specifically, we employ the same implementation of Exact-Match accuracy as used by SQuAD (Rajpurkar et al., 2016), which removes articles and does other simple normalization, and our F1 score is based on that used by SQuAD. We define F1 to be 0 when there is a number mismatch between the gold and predicted answers, regardless of other word overlap. When an answer has multiple spans, we first perform a one-to-one alignment greedily based on bag-of-word overlap on the set of spans and then compute average F1 over each span. For numeric answers, we ignore the units. Binary and unanswerable questions are both treated as span questions. In the unanswerable case, the answer is a special NONE token, and in the"
2020.emnlp-main.86,D19-1251,0,0.0366514,"en document, and what is missing, followed by locating a different source that could potentially contain the missing information. Most recent read∗ Work done as an intern at the Allen Institute for AI. ing comprehension tasks, such as SQuAD 2.0 (Rajpurkar et al., 2018), DROP (Dua et al., 2019b), or Quoref (Dasigi et al., 2019), evaluate models using a relatively simpler setup where all the information required to answer the questions (including judging them as being unanswerable) is provided in the associated contexts. While this setup has led to significant advances in reading comprehension (Ran et al., 2019; Zhang et al., 2020), the tasks are still limited since they do not evaluate the capability of models at identifying precisely what information, if any, is missing to answer a question, and where that information might be found. On the other hand, open-domain question answering tasks (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017) present a model with a ques1137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1137–1147, c November 16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retri"
2020.emnlp-main.86,P18-1156,0,0.0465345,"cts used for building the questions (Min et al., 2019b). In contrast, each question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second question in Figure 2. Separation of questions from answer contexts Many prior datasets (e.g.: WhoDidWhat (Onishi et al., 2016), NewsQA (Trischler et al., 2016), DuoRC (Saha et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020)) have tried to remove simple lexical heuristics from reading comprehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, an"
2020.emnlp-main.86,P19-1436,1,0.842729,"ehension tasks by separating the contexts that questions are anchored in from those that are used to answer them. IIRC also separates the two contexts, but is unique given that the linked documents elaborate on the information present in the original contexts, naturally giving rise to follow-up questions, instead of openended ones. Open-domain question answering In the opendomain QA setting, a system is given a question without any associated context, and must retrieve the necessary context to answer the question (Chen et al., 2017; Joshi et al., 2017; Dhingra et al., 2017; Yang et al., 2018; Seo et al., 2019; Karpukhin et al., 2020; Min et al., 2019a). IIRC is similar in that it also requires the retrieval of missing information. However, the questions are grounded in a given paragraph, meaning that a system must examine more than just the question in order to know what to retrieve. Most questions in IIRC do not make sense in an open-domain setting, without their associated paragraphs. Unanswerable questions Unlike SQuAD 2.0 (Rajpurkar et al., 2018) where the unanswerable questions were written to be close to answerable questions, IIRC contains naturally unanswerable questions that were not writt"
2020.emnlp-main.86,P19-1485,0,0.0330224,"Missing"
2020.emnlp-main.86,Q18-1021,0,0.0414415,"of the SQuAD or DROP data requires external links, this evaluation could only negatively impact precision. We find that precision dropped by 8 points, compared to a drop of 28 points when the model trained only on IIRC was used, indicating that the model is able to learn to identify when no external information is required. 6 Related Work Questions requiring multiple contexts Prior multi-context reading comprehension datasets were built by starting from discontiguous contexts, and forming compositional questions by stringing multiple facts either by relying on knowledge graphs as in QAngaroo (Welbl et al., 2018), or by having crowdworkers do so, as in HotpotQA (Yang et al., 2018). It has been shown that many of these questions can be answered by focusing on just one of the facts used for building the questions (Min et al., 2019b). In contrast, each question in IIRC was written by a crowdworker who had access to just one paragraph, with the goal of obtaining information missing in it, thus minimizing lexical overlap between questions and the answer contexts. Additionally, IIRC provides a unique question type: questions requiring aggregating information from many related documents, such as the second q"
2020.emnlp-main.86,D18-1259,0,0.438203,"7) present a model with a ques1137 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1137–1147, c November 16–20, 2020. 2020 Association for Computational Linguistics tion by itself, requiring the model to retrieve relevant information from some corpus. However, this approach loses grounding in a particular passage of text, and it has so far been challenging to collect diverse, complex question in this setting. Alternatively, complex questions grounded in context can be converted to open-domain or incomplete-information QA datasets such as HotpotQA (Yang et al., 2018). However, they do not capture the information-seeking questions that arise from reading a single document with partial information (Min et al., 2019b; Chen and Durrett, 2019). We present a new dataset of incomplete information reading comprehension questions, IIRC, to address both of these limitations. IIRC is a crowdsourced dataset of 13441 questions over 5698 paragraphs from English Wikipedia, with most of the questions requiring information from one or more documents hyperlinked to the associated paragraphs, in addition to the original paragraphs themselves. Our crowdsourcing process (Sect"
2020.emnlp-main.86,2020.acl-main.211,0,0.0349318,"ng unanswerable, a property that our dataset shares with NewsQA (Trischler et al., 2016), Natural Questions (Kwiatkowski et al., 2019), and TyDi QA (Clark et al., 2020). Results shown in Section 4.3 indicate that these questions cannot be trivially distinguished from answerable questions. Incomplete Information QA A few prior datasets have explored question answering given incomplete information, such as science facts (Mihaylov et al., 2018; Khot et al., 2019). However, these datasets contain multiple choice questions, and the answer choices provide hints as to what information may be needed. Yuan et al. (2020) explore this as well using a POMDP in which the context in existing QA datasets is hidden from the model until it explicitly searches for it. 7 Conclusion We introduced IIRC, a new dataset of incompleteinformation reading comprehension questions. These questions require identifying what information is missing from a paragraph in order to answer a question, predicting where to find it, then synthesizing the retrieved information in complex ways. Our baseline model, built on top of state-ofthe-art models for the most closely related existing datasets, performs quite poorly in this setting, even"
2020.findings-emnlp.117,W07-2441,0,0.0137691,"1. UD Parsing Finally, we discuss dependency parsing in the universal dependencies (UD) formalism (Nivre et al., 2016). We look at dependency parsing to show that contrast sets apply not only to modern “high-level” NLP tasks but also to longstanding linguistic analysis tasks. We first chose a specific type of attachment ambiguity to target: the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), e.g. We ate spaghetti with forks versus We ate spaghetti with meatballs. We use a subset of the English UD treebanks: GUM (Zeldes, 2017), the English portion of LinES (Ahrenberg, 2007), the English portion of ParTUT (Sanguinetti and Bosco, 2015), and the dependency-annotated English Web Treebank (Silveira et al., 2014). We searched these treebanks for sentences that include a potentially structurally ambiguous attachment from the head of a PP to either a noun or a verb. We then perturbed these sentences by altering one of their noun phrases such that the semantics of the perturbed sentence required a different attachment for the PP. We then re-annotated these perturbed sentences to indicate the new attachment(s). Summary While the overall process we recommend for constructi"
2020.findings-emnlp.117,D15-1075,0,0.0445289,"ow frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distribut"
2020.findings-emnlp.117,W18-6433,0,0.0341655,"Missing"
2020.findings-emnlp.117,D19-1606,1,0.924571,"round a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates"
2020.findings-emnlp.117,N19-1423,0,0.0110664,"st sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set de"
2020.findings-emnlp.117,N19-1246,1,0.944307,"arts of the gaps around a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while pe"
2020.findings-emnlp.117,D18-1407,1,0.898157,"uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011)"
2020.findings-emnlp.117,D19-1107,1,0.925439,"l language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps"
2020.findings-emnlp.117,P18-2103,0,0.0419316,"ded minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed inst"
2020.findings-emnlp.117,N18-2017,1,0.922108,"uction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in th"
2020.findings-emnlp.117,D19-1170,0,0.0120239,"work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set design, etc. On IMDb and PERSPECTRUM, the model achieves a reasonably high consistency, suggesting that, while there is definitely still room fo"
2020.findings-emnlp.117,D17-1263,0,0.0339764,"h to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring lo"
2020.findings-emnlp.117,D17-1215,0,0.560991,"rks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Lev"
2020.findings-emnlp.117,D19-1423,0,0.0613359,"Missing"
2020.findings-emnlp.117,2020.emnlp-main.12,1,0.714022,"ple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits simil"
2020.findings-emnlp.117,W17-5401,0,0.118381,"Missing"
2020.findings-emnlp.117,P19-1554,1,0.81477,"ctive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot confirm that a model does align with it. This is because annotators cannot exhaustively label all inputs near a pivot and thus a contrast set will necessarily be incomplete. However, note that this problem is not unique to contrast sets—similar issues hold for the original test set as well as adversarial test sets (Jia and Liang, 2017), challenge sets (Naik et al., 2018), and input perturbations (Ribeiro et al., 2018a; Feng et al., 2018). See Feng et al. (2019) for a detailed discussion of how dataset analysis methods only have negative predictive power. Dataset-Specific Instantiations The process for creating contrast sets is dataset-specific: although we present general guidelines that hold across many tasks, experts must still characterize the type of phenomena each individual dataset is intended to capture. Fortunately, the original dataset authors should already have thought deeply about such phenomena. Hence, creating contrast sets should be well-defined and relatively straightforward. 3 How to Create Contrast Sets Here, we walk through our pr"
2020.findings-emnlp.117,D19-5808,1,0.8991,"n the contrast sets (not including the original example). We report percentage accuracy for NLVR2, IMDb, PERSPECTRUM, MATRES, and BoolQ; F1 scores for DROP and Q UOREF; Exact Match (EM) scores for ROPES and MC-TACO; and unlabeled attachment score on modified attachments for the UD English dataset. We also report contrast consistency: the percentage of the “# Sets” contrast sets for which a model’s predictions are correct for all examples in the set (including the original example). More details on datasets, models, and metrics can be found in §A and §B. • Quoref (Dasigi et al., 2019) • ROPES (Lin et al., 2019) • BoolQ (Clark et al., 2019) • MC-TACO (Zhou et al., 2019) We choose these datasets because they span a variety of tasks (e.g., reading comprehension, sentiment analysis, visual reasoning) and input-output formats (e.g., classification, span extraction, structured prediction). We include high-level tasks for which dataset artifacts are known to be prevalent, as well as longstanding formalism-based tasks, where data artifacts have been less of an issue (or at least have been less well-studied). 4.2 Contrast Set Construction The contrast sets were constructed by NLP researchers who were deeply"
2020.findings-emnlp.117,2021.ccl-1.108,0,0.144976,"Missing"
2020.findings-emnlp.117,P11-1015,0,0.07085,"ena they are most interested in studying and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ"
2020.findings-emnlp.117,J93-2004,0,0.068933,"ation: Two similarly-colored and similarly-posed chow dogs are face to face in one image. Figure 1: An example contrast set for NLVR2 (Suhr and Artzi, 2019). The label for the original example is T RUE and the label for all of the perturbed examples is FALSE. The contrast set allows probing of a model’s decision boundary local to examples in the test set, which better evaluates whether the model has captured the relevant phenomena than standard metrics on i.i.d. test data. Introduction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input"
2020.findings-emnlp.117,D18-1151,0,0.0148944,"1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in systematic gaps. Thus contrast sets often require less effort to create, and they remain grounded i"
2020.findings-emnlp.117,N19-1314,0,0.0179195,"the task definition than a random selection of input / output pairs. 2.3 Contrast sets in practice Given these definitions, we now turn to the actual construction of contrast sets in practical NLP settings. There were two things left unspecified in the definitions above: the distance function d to use in discrete input spaces, and the method for sampling from a local decision boundary. While there has been some work trying to formally characterize dis2 In this discussion we are talking about the true decision boundary, not a model’s decision boundary. tances for adversarial robustness in NLP (Michel et al., 2019; Jia et al., 2019), we find it more useful in our setting to simply rely on expert judgments to generate a similar but meaningfully different x0 given x, addressing both the distance function and the sampling method. Future work could try to give formal treatments of these issues, but we believe expert judgments are sufficient to make initial progress in improving our evaluation methodologies. And while expertcrafted contrast sets can only give us an upper bound on a model’s local alignment with the true decision boundary, an upper bound on local alignment is often more informative than a pot"
2020.findings-emnlp.117,P19-1416,1,0.832263,"dissolute alcoholic. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap"
2020.findings-emnlp.117,C18-1198,0,0.264897,"nt work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that dataset authors manually perturb instances fro"
2020.findings-emnlp.117,D19-1642,1,0.874527,"Missing"
2020.findings-emnlp.117,P18-1122,1,0.846317,"and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ 339 70 RoBERTa 86.1 71.1 (–15.0) 59.0 MC-"
2020.findings-emnlp.117,P19-1459,0,0.0145999,"uestion: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be e"
2020.findings-emnlp.117,N18-1202,1,0.523856,"es from the different contrast sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model a"
2020.findings-emnlp.117,S18-2023,0,0.0681783,"Missing"
2020.findings-emnlp.117,P19-1621,1,0.860914,"Missing"
2020.findings-emnlp.117,P18-1079,1,0.679043,"of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that data"
2020.findings-emnlp.117,N18-2002,0,0.0515743,"Missing"
2020.findings-emnlp.117,E17-2060,0,0.0150328,"ds have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in system"
2020.findings-emnlp.117,2020.acl-main.468,0,0.0236124,"ing set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distributional biases in the data that was chosen for annotation, as well as numerous other biases that are more subtle and harder to discern (Shah et al., 2020). Completely removing these gaps in the initial data collection process would be ideal, but is likely impossible—language has too much inherent variability in a very high-dimensional space. Instead, we use contrast sets to fill in gaps in the test data to give more thorough evaluations than what the original data provides. 1309 2.2 Definitions We begin by defining a decision boundary as a partition of some space into labels.2 This partition can be represented by the set of all points in the space with their associated labels: {(x, y)}. This definition differs somewhat from the canonical defini"
2020.findings-emnlp.117,silveira-etal-2014-gold,0,0.0413118,"Missing"
2020.findings-emnlp.117,P19-1644,1,0.918073,"hanging questions asking for counts to questions asking for sets (How many countries. . . to Which countries. . . ). Finally, we changed the ordering of events. A large number of questions about war paragraphs ask which of two events happened first. We changed (1) the order the events were asked about in the question, (2) the order that the events showed up in the passage, and (3) the dates associated with each event to swap their temporal order. NLVR2 We next consider NLVR2, a dataset where a model is given a sentence about two provided images and must determine whether the sentence is true (Suhr et al., 2019). The data collection process encouraged highly compositional language, which was intended to require understanding the relationships between objects, properties of objects, and counting. We constructed NLVR2 contrast sets by modifying the sentence or replacing one of the images with freely-licensed images from web searches. For example, we might change The left image contains twice the number of dogs as the right image to The left image contains three times the number of dogs as the right image. Similarly, given an image pair with four dogs in the left and two dogs in the right, we can replac"
2020.findings-emnlp.117,D19-1608,1,0.822267,"esources that we have created, is giving a simple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various pheno"
2020.findings-emnlp.117,D18-1009,0,0.0260643,"is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that"
2020.findings-emnlp.117,P19-1472,0,0.0134417,"annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that model. 2.5 Limitations of Contrast Sets Solely Negative Predictive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot"
2020.findings-emnlp.171,N19-1300,0,0.202745,"n answering is a common tool for assessing how well can computers understand language and reason with it. To this end, the NLP community has introduced several distinct datasets, with four popular QA formats illustrated in Fig. 1. For instance, some datasets expect the answer to be “yes” or “no”, or a unique answer span in the associated paragraph (as opposed to multiple or no spans). These differences have motivated their study in silos, often encoding QA format into the model architecture itself. Efforts to exploit multiple datasets remain largely restricted to a single format. For example, Clark et al. (2019c) limit consideration to 1 https://github.com/allenai/unifiedqa multiple-choice datasets, while Talmor and Berant (2019) focus their generalization study on extractive span prediction models. To the best of our knowledge, no single QA system targets, not to mention excels at, all of these formats. This raises the question: Can QA models learn linguistic reasoning abilities that generalize across formats? Our intuition is simple: while question format and relevant knowledge may vary across QA datasets, the underlying linguistic understanding and reasoning abilities are largely common. A multip"
2020.findings-emnlp.171,P19-1595,0,0.0581037,"Missing"
2020.findings-emnlp.171,D19-1606,0,0.0603237,"Missing"
2020.findings-emnlp.171,N19-1423,0,0.0418664,"an any single-format expert. For example, while the system is trained on multiple-choice questions with 4 candidate answers, it works quite well on datasets with more than 4 candidate answers (QASC and CommonsenseQA have has 8 and 5 candidate answers per question, respectively). (3) Single-format experts are better at generalization only when the source and target datasets are very similar (for instance SQuAD and Quoref). 6.3 State-of-the-Art via Simple Fine-tuning Fine-tuning of pre-trained language models has become the standard paradigm for building datasetspecific stat-of-the-art systems (Devlin et al., 2019; Liu et al., 2019). The question we address here is: when it comes to QA, is there a value in using U NIFIED QA as a starting point for fine-tuning, as opposed to a vanilla language model that has not seen other QA datasets before? To address this question, we fine-tune each of U NIFIED QA, T5, and BART on several datasets by selecting the best check point on the dev set, and evaluating on the test set. Table 5 summarizes the results of the experiments. The table shows two variants: U NIFIED QAT5 and U NIFIED QABART . All results are based on the 11B version of T5. The columns indicate the ev"
2020.findings-emnlp.171,D19-5820,0,0.0458318,"asets (§6.3), establishing it as a powerful starting point for QA research. Our findings demonstrate that crossing QA format boundaries is not only qualitatively desirable but also quantitatively beneficial. 2 Related Work Several QA efforts have studied generalization across datasets of a single format. For instance, in MultiQA, Talmor and Berant (2019) study generalization and transfer, but only across extractive span selection datasets. Further, while they show strong leave-one-out style results, they find a single system performs substantially worse than one tuned to each dataset. In ORB, Dua et al. (2019a) propose a multi-dataset evaluation benchmark spanning extractive and abstractive formats. However, that study is limited to an evaluation of systems, falling short of addressing how to build such generalized models. The MRQA shared task (Fisch et al., 2019) focuses on span-prediction datasets. Unlike all these efforts, our goal is to investigate transfer and generalization across different QA formats, as well as to build a single system that does this well. Exploiting commonality across machine learning tasks has a rich history studied under transfer learning (Caruana, 1997; Clark et al., 2"
2020.findings-emnlp.171,N19-1246,0,0.120445,"asets (§6.3), establishing it as a powerful starting point for QA research. Our findings demonstrate that crossing QA format boundaries is not only qualitatively desirable but also quantitatively beneficial. 2 Related Work Several QA efforts have studied generalization across datasets of a single format. For instance, in MultiQA, Talmor and Berant (2019) study generalization and transfer, but only across extractive span selection datasets. Further, while they show strong leave-one-out style results, they find a single system performs substantially worse than one tuned to each dataset. In ORB, Dua et al. (2019a) propose a multi-dataset evaluation benchmark spanning extractive and abstractive formats. However, that study is limited to an evaluation of systems, falling short of addressing how to build such generalized models. The MRQA shared task (Fisch et al., 2019) focuses on span-prediction datasets. Unlike all these efforts, our goal is to investigate transfer and generalization across different QA formats, as well as to build a single system that does this well. Exploiting commonality across machine learning tasks has a rich history studied under transfer learning (Caruana, 1997; Clark et al., 2"
2020.findings-emnlp.171,D19-5801,0,0.0264481,"neralization across datasets of a single format. For instance, in MultiQA, Talmor and Berant (2019) study generalization and transfer, but only across extractive span selection datasets. Further, while they show strong leave-one-out style results, they find a single system performs substantially worse than one tuned to each dataset. In ORB, Dua et al. (2019a) propose a multi-dataset evaluation benchmark spanning extractive and abstractive formats. However, that study is limited to an evaluation of systems, falling short of addressing how to build such generalized models. The MRQA shared task (Fisch et al., 2019) focuses on span-prediction datasets. Unlike all these efforts, our goal is to investigate transfer and generalization across different QA formats, as well as to build a single system that does this well. Exploiting commonality across machine learning tasks has a rich history studied under transfer learning (Caruana, 1997; Clark et al., 2019b). McCann et al. (2018) and Keskar et al. (2019) study transfer among various NLP tasks by casting them into a single QA format—an elegant transfer learning approach but orthogonal to the goal of this work. As noted earlier, Raffel et al. (2020) investigat"
2020.findings-emnlp.171,2020.emnlp-main.550,1,0.900352,"Missing"
2020.findings-emnlp.171,N18-1023,1,0.910292,"Missing"
2020.findings-emnlp.171,2020.emnlp-main.12,1,0.824253,"Missing"
2020.findings-emnlp.171,Q18-1023,0,0.0434413,"e system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questi"
2020.findings-emnlp.171,Q19-1026,0,0.0482833,"ARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other"
2020.findings-emnlp.171,D17-1082,0,0.0548349,"ed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig. 2), others have more. Lat"
2020.findings-emnlp.171,2020.acl-main.703,0,0.297564,"✓ Figure 2: Properties of various QA datasets included in this study: 5 extractive (EX), 3 abstractive (AB), 9 multiplechoice (MC), and 3 yes/no (YN). ‘idk’ denotes ‘I don’t know’ or unanswerable questions. BoolQ represents both the original dataset and its contrast-sets extension BoolQ-CS; similarly for ROPES, Quoref, and DROP. commonsense QA datasets listed in Fig. 2. In this work, we advocate for a unifying view of QA formats by building a format-agnostic QA system. Our work leverages recent progress in text-to-text pre-trained neural models, specifically T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), but with a strong focus on differing QA formats. This paradigm allows unifying many NLP models, which formerly had task-specific designs, into a single text-to-text framework. Previous work uses textual prefixes to explicitly define the task associated with each input instance (Raffel et al., 2020; Radford et al., 2019b); often such attempts to build a single model for multiple NLP tasks underperform the standard pre-training plus finetuning setup (a model per task) (Raffel et al., 2020). Our work narrows down the scope to tasks that stay within the boundaries of QA, demonstrating that a uni"
2020.findings-emnlp.171,N06-1059,0,0.0398904,", and the binary (yes/no) subset of MultiRC (Khashabi et al., 2018). Contrast-sets. Additionally, we use contrastsets (Gardner et al., 2020) for several of our datasets (denoted with “CS”): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset. 4.2 Evaluation Metrics for Textual Output We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it’s the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct ‘yes’ or ‘no’ label. In rare cases where the output is longer than one word (e.g., ‘yes it is’), we check if it contains the correct label but Pilot Study: Can Out-of-Format Training Help? We first answer the question: Is the broad idea of benefiting from"
2020.findings-emnlp.171,D19-5808,1,0.901882,"argest available T5 model (11B parameters) as the starting point for training our model and call the system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not com"
2020.findings-emnlp.171,2021.ccl-1.108,0,0.333636,"Missing"
2020.findings-emnlp.171,D18-1260,1,0.840127,"used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig. 2), others have more. Later (in §6.2) we will see that our approac"
2020.findings-emnlp.171,D19-1284,1,0.85112,"yes/no) subset of MultiRC (Khashabi et al., 2018). Contrast-sets. Additionally, we use contrastsets (Gardner et al., 2020) for several of our datasets (denoted with “CS”): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset. 4.2 Evaluation Metrics for Textual Output We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it’s the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct ‘yes’ or ‘no’ label. In rare cases where the output is longer than one word (e.g., ‘yes it is’), we check if it contains the correct label but Pilot Study: Can Out-of-Format Training Help? We first answer the question: Is the broad idea of benefiting from out-of-format tra"
2020.findings-emnlp.171,2020.emnlp-main.466,1,0.911295,"astsets (Gardner et al., 2020) for several of our datasets (denoted with “CS”): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset. 4.2 Evaluation Metrics for Textual Output We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it’s the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct ‘yes’ or ‘no’ label. In rare cases where the output is longer than one word (e.g., ‘yes it is’), we check if it contains the correct label but Pilot Study: Can Out-of-Format Training Help? We first answer the question: Is the broad idea of benefiting from out-of-format training even viable? For instance, is our intuition correct that an MC dataset can, in practice"
2020.findings-emnlp.171,P19-1225,0,0.0223133,"MultiRC (Khashabi et al., 2018). Contrast-sets. Additionally, we use contrastsets (Gardner et al., 2020) for several of our datasets (denoted with “CS”): BoolQ-CS, ROPESCS, Quoref-CS, DROP-CS. These evaluation sets are expert-generated perturbations that deviate from the patterns common in the original dataset. 4.2 Evaluation Metrics for Textual Output We evaluate each dataset using the metric used most often for it in prior work. For the EX format, it’s the F1 score of the extracted span relative to the gold label. For the AB format, we use ROUGE-L metric (Lin et al., 2006; Min et al., 2019; Nishida et al., 2019). For NatQA we use the exact-match metric, following Min et al. (2020). For the MC format, we match the generated text with the closest answer candidate based token overlap and compute the accuracy. For the YN format, we follow Clark et al. (2019a) to measure if the generated output matches the correct ‘yes’ or ‘no’ label. In rare cases where the output is longer than one word (e.g., ‘yes it is’), we check if it contains the correct label but Pilot Study: Can Out-of-Format Training Help? We first answer the question: Is the broad idea of benefiting from out-of-format training even viable? For"
2020.findings-emnlp.171,N15-1082,1,0.895224,"Missing"
2020.findings-emnlp.171,P18-2124,0,0.0304563,"e datasets become available or new formats are introduced. Unless otherwise noted, we use the largest available T5 model (11B parameters) as the starting point for training our model and call the system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et a"
2020.findings-emnlp.171,D16-1264,0,0.108989,"ED QA model, or extend it as future datasets become available or new formats are introduced. Unless otherwise noted, we use the largest available T5 model (11B parameters) as the starting point for training our model and call the system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA ("
2020.findings-emnlp.171,D13-1020,0,0.0572589,"A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig."
2020.findings-emnlp.171,2020.emnlp-main.437,0,0.0611834,"ethodology, U NIFIED QA achieves performance on par with 8 dataset-specific expert models (§6.1), while also generalizing well to many unseen datasets of seen formats (§6.2). At the same time, we demonstrated that U NIFIED QA is a strong starting point for building QA systems: it can achieve state-of-the-art performance by simply fine-tuning on target datasets (6.3). We hope this effort will inspire a future line of work in the QA and NLP communities, moving towards more general and broader system designs. We leave extensions of U NIFIED QA to other formats such as to direct-answer questions (Roberts et al., 2020) as a promising avenue for future work. Acknowledgments The authors would like to thank Collin Raffel, Adam Roberts, and Nicholas Lourie for their help with the T5 framework and for providing feedback on an earlier version of this work. The authors would like to acknowledge grants by ONR N00014-18-1-2826 and DARPA N66001-19-2-403, and gifts from the Sloan Foundation and the Allen Institute for AI. Moreover, the authors would like to thank members of the Allen Institute for AI, UW-NLP, and the H2Lab at the University of Wash1904 ington for their valuable feedback and comments. TPU machines for"
2020.findings-emnlp.171,P16-1043,0,0.0228867,"ta and confirms our intuition that there are strong ties across QA formats in terms of the underlying reasoning abilities.2 Our unified question-answering system is based on the recent text-to-text frameworks, particularly, T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). We first define a unifying encoding of the instances across various formats (§3.1). We then introduce U NIFIED QA (§3.2) that is a QA system trained on datasets in multiple formats, indicating new state-of-the-art results on 10 datasets and generalization to unseen datasets. 2 A more sophisticated teaching curriculum (Sachan and Xing, 2016) or approaches such as model distillation and teacher annealing (Clark et al., 2019b) are likely to further improve the performance of the resulting unified model, bolstering the strength of our advocacy for a unified view of all QA formats. We leave their exploration to future work. 3.1 Text-to-Text Encoding We convert each of our target datasets into a textin/text-out format (Raffel et al., 2020; Lewis et al., 2020; Radford et al., 2019b). The question always comes first, followed by some additional information (context paragraph or candidate answers, or both). We use “
” separators between"
2020.findings-emnlp.171,D19-1454,0,0.102815,"Missing"
2020.findings-emnlp.171,P19-1485,0,0.101486,"nd, the NLP community has introduced several distinct datasets, with four popular QA formats illustrated in Fig. 1. For instance, some datasets expect the answer to be “yes” or “no”, or a unique answer span in the associated paragraph (as opposed to multiple or no spans). These differences have motivated their study in silos, often encoding QA format into the model architecture itself. Efforts to exploit multiple datasets remain largely restricted to a single format. For example, Clark et al. (2019c) limit consideration to 1 https://github.com/allenai/unifiedqa multiple-choice datasets, while Talmor and Berant (2019) focus their generalization study on extractive span prediction models. To the best of our knowledge, no single QA system targets, not to mention excels at, all of these formats. This raises the question: Can QA models learn linguistic reasoning abilities that generalize across formats? Our intuition is simple: while question format and relevant knowledge may vary across QA datasets, the underlying linguistic understanding and reasoning abilities are largely common. A multiple-choice model may, therefore, benefit from training on an extractive answers dataset. Building upon this intuition, we"
2020.findings-emnlp.171,N19-1421,0,0.0331123,"1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakaguchi et al., 2020). Several of the MC datasets do not come with accompanying paragraphs (such as ARC, QASC, OBQA). For most of this the work, we keep the questions as is with no additional retrieval (unless otherwise mentioned). One other variability among these datasets is their number of candidate answers. While many datasets have four candidates (see Fig. 2), others have more. Later (in §6.2) we will see that our approach generalizes to datasets with different numbers of candidates, even if such questions have not bee"
2020.findings-emnlp.171,W17-2623,0,0.0547264,"w formats are introduced. Unless otherwise noted, we use the largest available T5 model (11B parameters) as the starting point for training our model and call the system U NIFIED QA. We also report results of training our system with BARTlarge , referred to as U NI FIED QABART (see §6.3). Details on the parameters of the models used are deferred to Appendix A.2. 3 Future references to ‘seed dataset’ point to the QA datasets used in this section. Extractive QA (EX). Among the datasets in this popular format, we adopt SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2 (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019). Abstractive QA (AB). The datasets used from this format are: NarrativeQA/NarQA (Kociský et al., 2018), the open-domain version of NaturalQuestions/NatQA (Kwiatkowski et al., 2019), and DROP (Dua et al., 2019b). Multiple-choice QA (MC). We use the following MC datasets: MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), OpenBookQA/OBQA (Mihaylov et al., 2018), ARC (Clark et al., 2018, 2016), QASC (Khot et al., 2019), CommonsenseQA/CQA (Talmor et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and Winogrande (Sakagu"
2021.emnlp-main.140,P17-1171,0,0.0264896,"directly optimizing for knowledge extraction helps resolve complex user queries (Feng et al., 2020) and increases user engagement in social chat (Dinan et al., 2019; Moghe et al., 2018). For long documents, explicit knowledge identification can also be useful for model interpretability and human-in-the-loop assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is relevant to the current user query given the conversation context (Figure 1). Knowledge identification is similar to open question answering (Chen et al., 2017; Min ∗ Equal contribution et al., 2019), the task of answering a factoid question given a large grounding, except that it is not an interactive setting like dialogues. With the assumption of a long grounding document, our task differs from prior work in conversational question answering (Choi et al., 2018; Reddy et al., 2019), which focuses on answering a sequence of factoid questions about a short text snippet. Additionally, real user information needs can involve conversations with diverse forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1."
2021.emnlp-main.140,2021.naacl-main.85,0,0.016939,"istory turns becomes: Lnext = Lpsg + Lbegin + Lend . Lhist = Lhpsg + Lhbegin + Lhend . Training Objectives Eq. (1-3) show objective functions of knowledge passage Lpsg , begin Lbegin and end Lend span predictions. q(.)t denotes the t-th index of the vector resulting from the softmax ˆ ˆb and eˆ correspond to function. The variables k, the gold passage, begin and end span indices. Lpsg = − log q(Wp Z)kˆ ˙ ˆ Lbegin = − log q(Wb S) (1) b (2) ˙ eˆ Lend = − log q(We S) (3) 1855 3.3 Training and Inference Posterior Regularization During training, we incorporate a posterior regularization mechanism (Cheng et al., 2021) to enhance the model’s robustness to domain shift. Specifically, we add an additional adversarial training loss as below. Div is some f-divergence.2 Let x be the encoded X (defined in Section 3.1.1) after the BERT word embedding layer, D IALKI outputs fpsg (x), fbegin (x) and fend (x) as the next turn passage, begin and end knowledge span logits (the results before the softmax function q(.) in Eq. (1-3)) respectively. X  Ladv = max Div f (x)||f (x+) kk≤a f ∈{fpsg ,fbegin ,fend } The above loss function essentially regularizes the g-based worst-case posterior difference between the clean an"
2021.emnlp-main.140,2020.acl-main.501,0,0.107902,"and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to address knowledge identification in conversational systems with long grounding documents. In contrast to previous work, D IALKI extends multi-passage reader models in open question answering (Karpukhin et al., 2020; Cheng et al., 2020) to obtain dense 1852 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1852–1863 c November 7–11, 2021. 2021 Association for Computational Linguistics encodings of different spans in multiple passages in the grounding document, and it contextualizes them with the dialogue history. Specifically, D I ALKI extracts knowledge given a long document by dividing it into paragraphs or sections and individually contextualizes them with dialogue context. It then extracts knowledge by first selecting the most relevant passage to the dialogue context and then s"
2021.emnlp-main.140,D18-1241,0,0.0217273,"op assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is relevant to the current user query given the conversation context (Figure 1). Knowledge identification is similar to open question answering (Chen et al., 2017; Min ∗ Equal contribution et al., 2019), the task of answering a factoid question given a large grounding, except that it is not an interactive setting like dialogues. With the assumption of a long grounding document, our task differs from prior work in conversational question answering (Choi et al., 2018; Reddy et al., 2019), which focuses on answering a sequence of factoid questions about a short text snippet. Additionally, real user information needs can involve conversations with diverse forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to addr"
2021.emnlp-main.140,N19-1423,0,0.281929,"ges D = {p1 , p2 , . . . , p|D |} based on paragraphs or sections. Each passage p consists of a sequence of semantic units p = (s1 , s2 , . . . , sl ), where each semantic unit (SU) can be either a token or a span or a sentence depending on how the document is segmented. For simplicity, we use “span” as the semantic unit in this section to describe our model. Method overview In this section, we introduce D IALKI, a multi-task learning model for knowledge identification as illustrated in Figure 2. We first introduce how we obtain dialogue utterance and knowledge span representations from BERT (Devlin et al., 2019) and a span-level knowledge contextualization mechanism (Section 3.1). These representations are then used for knowledge identification in our multi-task learning framework, which includes the main task of next-turn knowledge identification and an auxiliary task of history knowledge prediction applied during training only (Section 3.2). Finally, we describe our joint training objective and inference details (Section 3.3). 3.1 Encoding Dialog Context and Knowledge the concatenated sequence. More formally, the model input X for a passage p of length l becomes: X = [cls][usr] u1 [agt]u2 · · · [us"
2021.emnlp-main.140,2020.emnlp-main.652,0,0.061473,"Missing"
2021.emnlp-main.140,2020.emnlp-main.550,0,0.0708781,"se forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to address knowledge identification in conversational systems with long grounding documents. In contrast to previous work, D IALKI extends multi-passage reader models in open question answering (Karpukhin et al., 2020; Cheng et al., 2020) to obtain dense 1852 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1852–1863 c November 7–11, 2021. 2021 Association for Computational Linguistics encodings of different spans in multiple passages in the grounding document, and it contextualizes them with the dialogue history. Specifically, D I ALKI extracts knowledge given a long document by dividing it into paragraphs or sections and individually contextualizes them with dialogue context. It then extracts knowledge by first selecting the most relevant passage to the dialog"
2021.emnlp-main.140,2020.acl-main.703,0,0.0282365,"Missing"
2021.emnlp-main.140,P19-1002,0,0.0302065,"Missing"
2021.emnlp-main.140,2020.acl-main.6,0,0.0997061,"Missing"
2021.emnlp-main.140,2021.ccl-1.108,0,0.0434227,"Missing"
2021.emnlp-main.140,D18-1255,0,0.158255,"ded response generation, where knowledge is represented in written documents. Most prior work has explored architectures for knowledge grounding in an end-to-end framework, optimizing a loss function targeting response generation (Ghazvininejad et al., 2018; Zhou et al., 2018; Yavuz et al., 2019). However, the knowledge needed at any one turn in the dialogue is typically localized in the document, and some studies have shown that directly optimizing for knowledge extraction helps resolve complex user queries (Feng et al., 2020) and increases user engagement in social chat (Dinan et al., 2019; Moghe et al., 2018). For long documents, explicit knowledge identification can also be useful for model interpretability and human-in-the-loop assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is relevant to the current user query given the conversation context (Figure 1). Knowledge identification is similar to open question answering (Chen et al., 2017; Min ∗ Equal contribution et al., 2019), the task of answering a factoid question given a large grounding, except that it is not an interactive setting like dialogues."
2021.emnlp-main.140,2020.acl-main.396,0,0.0545627,"Missing"
2021.emnlp-main.140,W18-6319,0,0.0123671,"given the concatenated dialogue context and grounding knowledge (e.g., document or predicted knowledge string) as the input. BART is also used as the baseline for the agent response generation task on Doc2Dial (Feng et al., 2020),8 where the model is given the dialogue history concatenated with the full document to decode the next agent response. We conduct experiments on the same model architecture, with the knowledge input being the predicted knowledge string or passage. Without changing the model at all, using knowledge predicted by D IALKI leads to almost 3 points in the sacrebleu score (Post, 2018), as shown in Table 4. Examples of generated responses are shown in Table 5. Passage Identification Accuracy We map predicted knowledge strings back to the passages and calculate the passage-level accuracy. Table 6 shows that D IALKI outperforms baseline models in locating the passage containing the knowledge string. Notably, our models generalize well in passage prediction to unseen documents or dialogue topics. Similarity Between Global and History Turn Representations The dot product between z (the encoding of the whole input sequence) and each history utterance representation ui (sigmoid n"
2021.emnlp-main.140,Q19-1016,0,0.102423,"ios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is relevant to the current user query given the conversation context (Figure 1). Knowledge identification is similar to open question answering (Chen et al., 2017; Min ∗ Equal contribution et al., 2019), the task of answering a factoid question given a large grounding, except that it is not an interactive setting like dialogues. With the assumption of a long grounding document, our task differs from prior work in conversational question answering (Choi et al., 2018; Reddy et al., 2019), which focuses on answering a sequence of factoid questions about a short text snippet. Additionally, real user information needs can involve conversations with diverse forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to address knowledge identif"
2021.emnlp-main.140,D18-1233,0,0.0188621,"nd evaluate on the 2 Related Work knowledge identification task. Feng et al. (2020) Conversational Question Answering. Existing encodes each long document as a single string withconversational question answering tasks (Choi out leveraging document structures, while Dinan et al., 2018; Reddy et al., 2019) are generally de- et al. (2019); Lian et al. (2019); Kim et al. (2020); fined as the task of reading a short text passage and Zheng et al. (2020) separately encode sentences answering a series of interconnected questions in in documents, which may have strong contextual a conversation. ShARC (Saeidi et al., 2018) is a dependencies among each other. Our model leverconversational machine reading dataset to address ages document structures and divides each docuunder-specified questions by requiring agents to ment into multiple passages to process. Similar to 1 our model, Kim et al. (2020); Zheng et al. (2020) We release our source code for experiments at https: //github.com/ellenmellon/DIALKI. incorporate previously used knowledge, but they 1853 use a single vector to sequentially track the state of the used knowledge. Instead, we apply a multi-task learning framework to model relations between grounding"
2021.emnlp-main.140,W19-5917,0,0.0239019,"V in Albany, … • When the inspector visits your location, … Figure 1: In a document-grounded conversation, knowledge identification targets to locate a knowledge string within a long document to assist the agent in addressing the current user query. Introduction Many conversational agent scenarios require knowledge-grounded response generation, where knowledge is represented in written documents. Most prior work has explored architectures for knowledge grounding in an end-to-end framework, optimizing a loss function targeting response generation (Ghazvininejad et al., 2018; Zhou et al., 2018; Yavuz et al., 2019). However, the knowledge needed at any one turn in the dialogue is typically localized in the document, and some studies have shown that directly optimizing for knowledge extraction helps resolve complex user queries (Feng et al., 2020) and increases user engagement in social chat (Dinan et al., 2019; Moghe et al., 2018). For long documents, explicit knowledge identification can also be useful for model interpretability and human-in-the-loop assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long document that is releva"
2021.emnlp-main.140,P18-1205,0,0.0606293,"Missing"
2021.emnlp-main.140,2020.emnlp-main.272,0,0.0575001,"Missing"
2021.emnlp-main.140,2020.findings-emnlp.11,0,0.265178,"a long grounding document, our task differs from prior work in conversational question answering (Choi et al., 2018; Reddy et al., 2019), which focuses on answering a sequence of factoid questions about a short text snippet. Additionally, real user information needs can involve conversations with diverse forms of user queries and dialogue acts (e.g., asking for user preference, etc) as shown in Figure 1. Previous work in knowledge identification encode the grounding document as a single string (Feng et al., 2020), or splitting it into isolated sentences (Dinan et al., 2019; Kim et al., 2020; Zheng et al., 2020) which potentially loses important discourse context. In this paper, we introduce D IALKI to address knowledge identification in conversational systems with long grounding documents. In contrast to previous work, D IALKI extends multi-passage reader models in open question answering (Karpukhin et al., 2020; Cheng et al., 2020) to obtain dense 1852 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1852–1863 c November 7–11, 2021. 2021 Association for Computational Linguistics encodings of different spans in multiple passages in the grounding document,"
2021.emnlp-main.140,D18-1076,0,0.068441,"reviewed by the DMV in Albany, … • When the inspector visits your location, … Figure 1: In a document-grounded conversation, knowledge identification targets to locate a knowledge string within a long document to assist the agent in addressing the current user query. Introduction Many conversational agent scenarios require knowledge-grounded response generation, where knowledge is represented in written documents. Most prior work has explored architectures for knowledge grounding in an end-to-end framework, optimizing a loss function targeting response generation (Ghazvininejad et al., 2018; Zhou et al., 2018; Yavuz et al., 2019). However, the knowledge needed at any one turn in the dialogue is typically localized in the document, and some studies have shown that directly optimizing for knowledge extraction helps resolve complex user queries (Feng et al., 2020) and increases user engagement in social chat (Dinan et al., 2019; Moghe et al., 2018). For long documents, explicit knowledge identification can also be useful for model interpretability and human-in-the-loop assistant scenarios. Following (Feng et al., 2020), we define knowledge identification as the task of locating knowledge in a long do"
2021.emnlp-main.141,2021.emnlp-main.167,0,0.029922,"meteor <extra_id_1> observatory. Drawer Encoding meteor destroying* an* observatory. [ICON1058] [X10] [Y5] [SCALE5] [ROT0] [MIRROR1] [ICON65] [X13] [Y10]……. Figure 3: Game state encoding for our models. For each encoding method, the upper text is the input and the lower text is the target output. et al., 2020) language model by treating the task as a text-to-text conditional generation task. Interestingly, we find vision-and-language (V+L) models (Tan and Bansal, 2019; Chen et al., 2020) to be less effective, which might be because current V+L models have inferior language-related abilities (Iki and Aizawa, 2021), or because models trained on photographic images are not well-suited to understand the non-literal imagery found in Iconary. 3.1 Guesser experiments found encoding positional information more precisely, or encoding earlier drawings if they exist, did not improve performance when using T5. Next, we append the text ‘phrase:’ and, for each word in the target phrase, either an underscore or the correct word if it is known (see Figure 3, top). We experimented with encoding previous incorrect guesses but found it unnecessary as long as models are prevented from repeating those guesses during gener"
2021.emnlp-main.141,2020.emnlp-main.707,1,0.768191,"ding scheme similar to the pre-training one is effective for O OD generalization. Unsurprisingly, many of these optimizations reduce I ND performance because they increase the usage O OV words, which never appear in the I ND dev sets. Table 7 shows the Drawer ablations. Our initialization strategy proves to be critical, which suggests it is what allows TD RAWER to leverage the T5 parameter initialization even though it does not output natural language. We also get a modest boost by training with the formatting constraints. 6 Related Work produce images from text has been studied for captions (Cho et al., 2020), image specifications (Reed et al., 2016), and dialogue (Sharma et al., 2018). Unlike in these works, the drawings in Iconary are not photographic and constructed to communicate a phrase. As a result, they can be non-literal and deictic, which makes understanding them a significantly different challenge. Using a pre-trained language model to understand mixed language and visual input has been considered by Marasovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationale"
2021.emnlp-main.141,D18-1241,1,0.827931,"s (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skills like understanding arrows or using icons to represent concepts, diagrams are usually used to convey technical information and therefore are unlikely to use things like visual metaphor, scenes, or icon compositions to signal words. The back-and-forth of Iconary follows a dialogue structure where the Guesser is seeking information from the Drawer. A similar format can be found in dialogue QA datasets (Reddy et al., 2019; Choi et al., 2018; Aliannejadi et al., 2019), and task-oriented dialogue in general similarly requires understanding the intent of a human communicator (Young et al., 2013; Chen et al., 2017). Iconary, however, makes this a multimodal process. There is a long history of using games as a testbed for AI. Traditionally these have been adversarial strategy games like Chess (Silver et al., 2018), Go (Silver et al., 2016), and many others (Moravcík et al., 2017; Vinyals et al., 2017; Mnih et al., 2013) A few cooperative games have been studied, like Codenames (Kim et al., 2019) or Hanabi (WaltonRivers et al., 2019),"
2021.emnlp-main.141,W18-0907,0,0.0264587,"visual input has been considered by Marasovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationales. Scialom et al. (2020) also show BERT (Devlin et al., 2019) can be trained for Visual Question Generation (Mostafazadeh et al., 2016). We also find combining high-level visual features with a pre-trained language model is an effective way to generate visually relevant text, although again our focus is on drawings rather than photographs. Figurative text is well studied (Leong et al., 2018; Veale et al., 2016; Shutova et al., 2016), but non-literal imagery has mostly only been explored in the context of parsing charts or diagrams. This includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skills like understanding arrows or using icons to represent concepts, diagrams are usually used to convey technical information and therefore are unlikely to use things like visual metaphor, scenes, or icon compositions to signal words. The back-and-forth of Ico"
2021.emnlp-main.141,N19-1423,0,0.0114671,"), and dialogue (Sharma et al., 2018). Unlike in these works, the drawings in Iconary are not photographic and constructed to communicate a phrase. As a result, they can be non-literal and deictic, which makes understanding them a significantly different challenge. Using a pre-trained language model to understand mixed language and visual input has been considered by Marasovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationales. Scialom et al. (2020) also show BERT (Devlin et al., 2019) can be trained for Visual Question Generation (Mostafazadeh et al., 2016). We also find combining high-level visual features with a pre-trained language model is an effective way to generate visually relevant text, although again our focus is on drawings rather than photographs. Figurative text is well studied (Leong et al., 2018; Veale et al., 2016; Shutova et al., 2016), but non-literal imagery has mostly only been explored in the context of parsing charts or diagrams. This includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for"
2021.emnlp-main.141,2020.acl-main.703,0,0.0156602,"mer Win I ND Soft O OD Win Soft∗ 84.25 85.91 79.34 78.84 79.89 97.62 98.55 97.09 96.69 93.64 37.39 22.67 33.30 27.07 0.00 44.06 27.24 40.61 34.48 0.00 Table 4: Automatic evaluation metrics on the test sets for TG UESSER and our baselines. method of encoding the drawing, we compare the perplexity of each human drawing, averaged over all drawings per game, then averaged over all games in the corpus. 4.3 Baselines We use the following baselines: Identical TGuesser-Large/T5Drawer-Base: models but with smaller versions of T5. BART Guesser/Bart Drawer: Identical models with the BART language model (Lewis et al., 2020). For BART Guesser, we adapt the fill-in-the-blank encoding scheme to generate a copy of the input with the mask tokens replaced, instead of only generating the masked-out tokens, to match BART’s pre-training format. Transformer Guesser/Transformer Drawer: We train a transformer-based model (Vaswani et al., 2017) on this task that does not use a pre-trained language model. This model also encodes the drawings as a sequence of special tokens during both decoding and encoding, in which case we find it important to apply a data-augmentation strategy to help the model learn mappings between icons"
2021.emnlp-main.141,2020.emnlp-main.602,0,0.0202194,"t is the game phrase. During generation, we constrain models to ensure the output contains the right number of words, includes words that are known to be correct from previous guesses, and exclude words that are known to be incorrect. This is non-trivial for wordpiece models, but we leave details in the appendix. 3.2 Handling OOV Words We observe that naively trained models often generate words seen in the training data even when they do not match the drawing. To combat this, we propose several extensions to TG UESSER: Rare Word Boosting: Based on a method from controlled language generation (Ma et al., 2020; Ghosh et al., 2017), we boost the logit score of wordpieces not seen during training. In particular, we add a fixed value (chosen as a hyperparmeter), to the log-probabilities of those wordpieces and then re-apply the softmax operator to get updated word-piece probabilities during generation. Fill-in-the-Blank Encoding: Following the T5 pre-training format (Raffel et al., 2020), we encode the phrase using ‘extra_id’ tokens for sequences of unknown words instead of underscores and train the model to only predict the text that ought to replace those tokens. Figure 3 contains an example. We exp"
2021.emnlp-main.141,P16-1170,0,0.0268231,"awings in Iconary are not photographic and constructed to communicate a phrase. As a result, they can be non-literal and deictic, which makes understanding them a significantly different challenge. Using a pre-trained language model to understand mixed language and visual input has been considered by Marasovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationales. Scialom et al. (2020) also show BERT (Devlin et al., 2019) can be trained for Visual Question Generation (Mostafazadeh et al., 2016). We also find combining high-level visual features with a pre-trained language model is an effective way to generate visually relevant text, although again our focus is on drawings rather than photographs. Figurative text is well studied (Leong et al., 2018; Veale et al., 2016; Shutova et al., 2016), but non-literal imagery has mostly only been explored in the context of parsing charts or diagrams. This includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skill"
2021.emnlp-main.141,D14-1162,0,0.0839227,"Missing"
2021.emnlp-main.141,Q19-1016,0,0.0242719,"is includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skills like understanding arrows or using icons to represent concepts, diagrams are usually used to convey technical information and therefore are unlikely to use things like visual metaphor, scenes, or icon compositions to signal words. The back-and-forth of Iconary follows a dialogue structure where the Guesser is seeking information from the Drawer. A similar format can be found in dialogue QA datasets (Reddy et al., 2019; Choi et al., 2018; Aliannejadi et al., 2019), and task-oriented dialogue in general similarly requires understanding the intent of a human communicator (Young et al., 2013; Chen et al., 2017). Iconary, however, makes this a multimodal process. There is a long history of using games as a testbed for AI. Traditionally these have been adversarial strategy games like Chess (Silver et al., 2018), Go (Silver et al., 2016), and many others (Moravcík et al., 2017; Vinyals et al., 2017; Mnih et al., 2013) A few cooperative games have been studied, like Codenames (Kim et al., 2019) or Hanabi (WaltonRi"
2021.emnlp-main.141,N16-1020,0,0.0288099,"sovi´c et al. (2020), who use features produced by object detectors or other visual understanding systems as input to GPT-2 (Radford et al., 2019) to generate natural language rationales. Scialom et al. (2020) also show BERT (Devlin et al., 2019) can be trained for Visual Question Generation (Mostafazadeh et al., 2016). We also find combining high-level visual features with a pre-trained language model is an effective way to generate visually relevant text, although again our focus is on drawings rather than photographs. Figurative text is well studied (Leong et al., 2018; Veale et al., 2016; Shutova et al., 2016), but non-literal imagery has mostly only been explored in the context of parsing charts or diagrams. This includes food webs (Mitra et al., 2018), science diagrams (Kembhavi et al., 2016), charts (Kafle et al., 2018) or for geometry problems (Seo et al., 2014). While this can involve related skills like understanding arrows or using icons to represent concepts, diagrams are usually used to convey technical information and therefore are unlikely to use things like visual metaphor, scenes, or icon compositions to signal words. The back-and-forth of Iconary follows a dialogue structure where the"
2021.emnlp-main.141,P19-1644,0,0.0537855,"Missing"
2021.emnlp-main.141,D19-1514,0,0.0141506,"rvatory. Fill-in-the-Blank Encoding drawing: huge comet, 2 smoke, telescope, huge dome. phrase: <extra_id_0> destroying a <extra_id_1> <extra_id_0> meteor <extra_id_1> observatory. Drawer Encoding meteor destroying* an* observatory. [ICON1058] [X10] [Y5] [SCALE5] [ROT0] [MIRROR1] [ICON65] [X13] [Y10]……. Figure 3: Game state encoding for our models. For each encoding method, the upper text is the input and the lower text is the target output. et al., 2020) language model by treating the task as a text-to-text conditional generation task. Interestingly, we find vision-and-language (V+L) models (Tan and Bansal, 2019; Chen et al., 2020) to be less effective, which might be because current V+L models have inferior language-related abilities (Iki and Aizawa, 2021), or because models trained on photographic images are not well-suited to understand the non-literal imagery found in Iconary. 3.1 Guesser experiments found encoding positional information more precisely, or encoding earlier drawings if they exist, did not improve performance when using T5. Next, we append the text ‘phrase:’ and, for each word in the target phrase, either an underscore or the correct word if it is known (see Figure 3, top). We expe"
2021.emnlp-main.141,Q14-1006,0,0.164456,"Missing"
2021.emnlp-main.141,D17-1099,0,0.0358424,"Missing"
2021.emnlp-main.560,D13-1160,0,0.0653188,"tive is the sum of −logP (p|q, B) of the passages including any valid answer to q. At inference, I NDEP PR outputs the top k passages based the logit values of the passage indices. We compare mainly to I N DEP PR because it is the strict non-autoregressive version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA tracks, along with as the retrieval corpus C, where each article is regular expressi"
2021.emnlp-main.560,P17-1171,0,0.0188345,"ropose JPR, a joint passage retrieval model that integrates dependencies among selected passages, along with new training and decoding algorithms. 3. On three multi-answer QA datasets, JPR significantly outperforms a range of baselines with independent scoring of passages, both in retrieval recall and answer accuracy. 2 2.1 Background Review: Single-Answer Retrieval In a typical single-answer retrieval problem, a model is given a natural language question q and retrieves k passages {p1 ...pk } from a large text corpus C (Voorhees et al., 1999; Ramos et al., 2003; Robertson and Zaragoza, 2009; Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020; Luan et al., 2020). The goal is to retrieve at least one passage that contains the answer to q. During training, question-answer pairs (q, a) are given to the model. Task Single-answer Retrieval Multi-answer Retrieval Train Data (q, a) (q, {a1 ...an }) Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-ans"
2021.emnlp-main.560,2021.eacl-main.74,0,0.444028,"Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time inputs and outputs are the same. We propose JPR as an instance of P (p1 ...pk |q). trieval successful if the answer a is included in {p1 ...pk }. Extrinsic evaluation uses the retrieved passages as input to an answer generation model such as the model in Izacard and Grave (2021) and evaluates final question answering performance. Reranking Much prior work (Liu, 2011; Asadi and Lin, 2013; Nogueira et al., 2020) found an effective strategy in using a two-step approach of (1) retrieving a set of candidate passages B from the corpus C (k &lt; |B | |C|) and (2) using another model to rerank the passages, obtaining a final top k. A reranker could be more expressive than the first-stage model (e.g. by using cross-attention), as it needs to process much fewer candidates. Most prior work in reranking, including the current stateof-the-art (Nogueira et al., 2020), scores each pa"
2021.emnlp-main.560,2020.emnlp-main.550,1,0.91278,"model that integrates dependencies among selected passages, along with new training and decoding algorithms. 3. On three multi-answer QA datasets, JPR significantly outperforms a range of baselines with independent scoring of passages, both in retrieval recall and answer accuracy. 2 2.1 Background Review: Single-Answer Retrieval In a typical single-answer retrieval problem, a model is given a natural language question q and retrieves k passages {p1 ...pk } from a large text corpus C (Voorhees et al., 1999; Ramos et al., 2003; Robertson and Zaragoza, 2009; Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020; Luan et al., 2020). The goal is to retrieve at least one passage that contains the answer to q. During training, question-answer pairs (q, a) are given to the model. Task Single-answer Retrieval Multi-answer Retrieval Train Data (q, a) (q, {a1 ...an }) Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time i"
2021.emnlp-main.560,Q19-1026,1,0.820022,"ve version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA tracks, along with as the retrieval corpus C, where each article is regular expressions as answers. Prior work uses split into passages with up to 288 wordpieces, All this data as a task of finding a single answer (where rerankers are based on T5 (Raffel et al., 2020), retrieving any of the correct answers is sufficient), a pretrained encode"
2021.emnlp-main.560,P19-1612,1,0.924355,"e reof passages relevant to a natural language question call, and keeping passages relevant to the question. from a large text corpus. Most prior work focuses In this work, we introduce Joint Passage Reon single-answer retrieval, which scores passages trieval (JPR), a new model that addresses these independently from each other according to their challenges. To jointly score passages, JPR emrelevance to the given question, assuming there is ploys an encoder-decoder reranker and autoregresa single answer (Voorhees et al., 1999; Chen et al., sively generates passage references by modeling 2017; Lee et al., 2019). However, questions posed the probability of each passage as a function of by humans are often open-ended and ambiguous, previously retrieved passages. Since there is no leading to multiple valid answers (Min et al., 2020). ground truth ordering of passages, we employ a For example, for the question in Figure 1, “What new training method that dynamically forms suwas Eli Whitney’s job?”, an ideal retrieval system pervision to drive the model to prefer passages should provide passages covering all professions of with answers not already covered by previously seEli Whitney. This introduces the p"
2021.emnlp-main.560,2020.emnlp-main.466,1,0.870763,"ieval, which scores passages trieval (JPR), a new model that addresses these independently from each other according to their challenges. To jointly score passages, JPR emrelevance to the given question, assuming there is ploys an encoder-decoder reranker and autoregresa single answer (Voorhees et al., 1999; Chen et al., sively generates passage references by modeling 2017; Lee et al., 2019). However, questions posed the probability of each passage as a function of by humans are often open-ended and ambiguous, previously retrieved passages. Since there is no leading to multiple valid answers (Min et al., 2020). ground truth ordering of passages, we employ a For example, for the question in Figure 1, “What new training method that dynamically forms suwas Eli Whitney’s job?”, an ideal retrieval system pervision to drive the model to prefer passages should provide passages covering all professions of with answers not already covered by previously seEli Whitney. This introduces the problem of multilected passages. Furthermore, we introduce a new answer retrieval—retrieval of multiple passages tree-decoding algorithm to allow flexibility in the ∗ Work done while interning at Google. degree of diversity."
2021.emnlp-main.560,P19-1416,1,0.828576,"ncreasing the distances between retrieved passages does not help.7 Second, multi-answer retrieval uses a clear notion of “answers”; “sub-topics” in diverse IR are more subjective and hard to enumerate fully. Multi-hop passage retrieval Recent work studies multi-hop passage retrieval, where a passage containing the answer is the destination of a chain of multiple hops (Asai et al., 2020; Xiong et al., 2021; Khattab et al., 2021). This is a difficult problem as passages in a chain are dissimilar to each other, but existing datasets often suffer from annotation artifacts (Chen and Durrett, 2019; Min et al., 2019), resulting in strong lexical cues for each hop. We study an orthogonal problem of finding multiple answers, where the challenge is in controlling the trade-off between relevance and diversity. 7 Conclusion We introduce JPR, an autoregressive passage reranker designed to address the multi-answer retrieval problem. On three multi-answer datasets, JPR significantly outperforms a range of baselines 7 Diverse retrieval Studies on diverse retrieval in In our preliminary experiment, we tried increasing diversity based on Maximal Marginal Relevance (Carbonell and the context of information retrieval"
2021.emnlp-main.560,2020.findings-emnlp.63,0,0.293556,"pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time inputs and outputs are the same. We propose JPR as an instance of P (p1 ...pk |q). trieval successful if the answer a is included in {p1 ...pk }. Extrinsic evaluation uses the retrieved passages as input to an answer generation model such as the model in Izacard and Grave (2021) and evaluates final question answering performance. Reranking Much prior work (Liu, 2011; Asadi and Lin, 2013; Nogueira et al., 2020) found an effective strategy in using a two-step approach of (1) retrieving a set of candidate passages B from the corpus C (k &lt; |B | |C|) and (2) using another model to rerank the passages, obtaining a final top k. A reranker could be more expressive than the first-stage model (e.g. by using cross-attention), as it needs to process much fewer candidates. Most prior work in reranking, including the current stateof-the-art (Nogueira et al., 2020), scores each passage independently, modeling P (p|q). 2.2 Multi-Answer Retrieval We now formally define the task of multi-answer retrieval. A model i"
2021.emnlp-main.560,P16-2033,1,0.83115,"rained to output a single token i (1 ≤ i ≤ |B|) rather than a sequence. The objective is the sum of −logP (p|q, B) of the passages including any valid answer to q. At inference, I NDEP PR outputs the top k passages based the logit values of the passage indices. We compare mainly to I N DEP PR because it is the strict non-autoregressive version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA"
2021.findings-acl.366,2020.acl-main.703,1,0.930521,"the task, and facilitate a novel method to evaluate explanation faithfulness. 1 ii) The geese prefer to nest in the fields rather than the forests because in the predators are more hidden. Contrastive Expl. - Forests are denser than fields Table 1: Examples of Winograd Schema Instances where the correct and incorrect answer choices are highlighted in blue and red respectively. Choices are contrasted along attributes like taste (for i) and density of vegetation (for ii) by humans to explain why they prefer some answer choice. Introduction Pretrained Language Models (PLMs) (Raffel et al., 2020; Lewis et al., 2020; Radford et al., 2019; Brown et al., 2020) have been shown to encode substantial amounts of knowledge in their parameters (Petroni et al., 2019; Talmor et al., 2020; Roberts et al., 2020) and have achieved impressive performance on commonsense reasoning (CSR) tasks without the use of external knowledge (Trinh and Le, 2018; Yang et al., 2020). However, these models provide little human-interpretable evidence of the intermediate commonsense knowledge or reasoning they use, and have been observed to overly rely on superficial dataset artifacts (Poliak et al., 2018; Geva et al., 2019). To overcom"
2021.findings-acl.366,2020.emnlp-main.153,1,0.893158,"Missing"
2021.findings-acl.366,D19-1250,0,0.0567481,"Missing"
2021.findings-acl.366,S18-2023,0,0.0649282,"Missing"
2021.findings-acl.366,P19-1487,0,0.382699,"l., 2020; Roberts et al., 2020) and have achieved impressive performance on commonsense reasoning (CSR) tasks without the use of external knowledge (Trinh and Le, 2018; Yang et al., 2020). However, these models provide little human-interpretable evidence of the intermediate commonsense knowledge or reasoning they use, and have been observed to overly rely on superficial dataset artifacts (Poliak et al., 2018; Geva et al., 2019). To overcome this limitation, recent work has shown that PLMs can explain themselves by generating free-form natural language explanations of their reasoning patterns (Rajani et al., 2019a; Camburu et al., 2018; Narang et al., 2020). However, the space of possible free-form explanations is incredibly large, inherently ambiguous, and difficult to annotate or evaluate (Wiegreffe et al., 2020; Latcinnik and Berant, 2020). Furthermore, quantifying the model’s dependence on free-form explanations is also challenging (Camburu et al., 2020). We address these challenges by proposing an unsupervised method that uses contrastive prompts, which require the model to explicitly contrast different possible answers in its explanation (Table 1). Our approach is based on a key observation: Man"
2021.findings-acl.366,2020.emnlp-main.437,0,0.0379399,"Missing"
2021.findings-acl.366,2021.findings-acl.336,0,0.0392813,"Missing"
2021.findings-acl.366,N18-2002,0,0.0642609,"Missing"
2021.findings-acl.366,2020.emnlp-main.346,0,0.0324025,"., 2020). Most recently, Latcinnik and Berant (2020) use an unsupervised approach to generate free-form explanations as sequences of tokens that are not well-formed sentences. In contrast, our method uses specialized prompts to generate well-formed human-interpretable explanations without any additional supervision. Specialized prompts have been shown useful for extracting knowledge from PLMs in a targeted manner (Petroni et al., 2020; Richardson and Sabharwal, 2020; Talmor et al., 2020; Donahue et al., 2020; Lin et al., 2019) and improving performance on downstream tasks (Brown et al., 2020; Shin et al., 2020). Most relevant to our work is the self-talk model of Shwartz et al. (2020), an unsupervised approach using a fixed set of clarification questions as prompts to elicit knowledge from PLMs for commonsense reasoning tasks. Our work differs by focusing specifically on contrastive PLM prompts, which we find further improve performance by eliciting explanations which are highly relevant to the classification decision (Section 6). Our approach to contrastive reasoning is also closely related to counterfactuals, which can be used to give contrastive explanations, i.e., answers to “Why P rather than Q"
2021.findings-acl.366,2020.emnlp-main.373,0,0.0483818,"Missing"
2021.findings-acl.366,2020.tacl-1.48,0,0.184828,"edators are more hidden. Contrastive Expl. - Forests are denser than fields Table 1: Examples of Winograd Schema Instances where the correct and incorrect answer choices are highlighted in blue and red respectively. Choices are contrasted along attributes like taste (for i) and density of vegetation (for ii) by humans to explain why they prefer some answer choice. Introduction Pretrained Language Models (PLMs) (Raffel et al., 2020; Lewis et al., 2020; Radford et al., 2019; Brown et al., 2020) have been shown to encode substantial amounts of knowledge in their parameters (Petroni et al., 2019; Talmor et al., 2020; Roberts et al., 2020) and have achieved impressive performance on commonsense reasoning (CSR) tasks without the use of external knowledge (Trinh and Le, 2018; Yang et al., 2020). However, these models provide little human-interpretable evidence of the intermediate commonsense knowledge or reasoning they use, and have been observed to overly rely on superficial dataset artifacts (Poliak et al., 2018; Geva et al., 2019). To overcome this limitation, recent work has shown that PLMs can explain themselves by generating free-form natural language explanations of their reasoning patterns (Rajani e"
2021.findings-acl.366,N19-1421,0,0.0228381,"rmation required for commonsense tasks. Even if the full model does not always use the explanations, these evaluations show that our contrastive explanations contain rich task-relevant knowledge, and suggest that future work might focus on how to better make use of this signal. 6.4 Generalizability of Prompts The set of contrastive prompts used in our framework are curated from an in-house analysis of training instances from Winogrande and PIQA datasets. To determine the generalizability of these prompts for other commonsense reasoning tasks, we also experiment with the CommonsenseQA dataset (Talmor et al., 2019), which consists of multiple-choice questions created over ConceptNet – “Where on a river can you hold a cup upright to catch water on a sunny day? a) waterfall, b) bridge, c) valley, d) pebble, e) mountain”. Since there are more than two answer choices to contrast, we convert each instance into 10 pairwise (binary) classification instances. Contrastive explanations are generated for each pairwise decision in the zero-shot setting, similar to Winograd and PIQA datasets. To choose the final answer, we consider two inference procedures: (a) Vote: The answer that receives the maximum number of vo"
2021.findings-acl.366,2020.findings-emnlp.90,0,0.051371,"Missing"
2021.findings-acl.366,2020.acl-main.508,0,0.112732,"ess.1 2 Related Work Models that rationalize their decisions by extracting a contiguous subsequence of the input as an explanation (Lei et al., 2016; DeYoung et al., 2020; Paranjape et al., 2020) are inadequate in explaining 1 Code is available at https://github.com/ bhargaviparanjape/RAG-X commonsense reasoning tasks that require knowledge that is implicit in the input. Such tasks necessitate PLMs to rely on embedded parametric knowledge. Recent work use free-form textual explanations to generate explanations for commonsense reasoning tasks like SNLI (Camburu et al., 2018), Winograd Schemas (Zhang et al., 2020) and CommonsenseQA (Rajani et al., 2019b) through explicit human supervision, which are inherently ambiguous, incomplete and consequently, expensive to collect and evaluate on (Camburu et al., 2019b,a; DeYoung et al., 2020). Most recently, Latcinnik and Berant (2020) use an unsupervised approach to generate free-form explanations as sequences of tokens that are not well-formed sentences. In contrast, our method uses specialized prompts to generate well-formed human-interpretable explanations without any additional supervision. Specialized prompts have been shown useful for extracting knowledge"
2021.findings-emnlp.38,2020.emnlp-main.437,0,0.0555104,"rmance on the recent ELI5 long-answers dataset. We release G OOAQ to facilitate further research on improving QA with diverse response types.1 1 Introduction Research in “open” question answering (also referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge 1 The dataset is available at https://github.com/ allenai/gooaq under an appropriate license. bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021). In contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., ‘what is’ or ‘can you’ questions), a collection of items such as ingredients (‘what are kinds of’, ‘things to’) or of steps towards a goal such"
2021.findings-emnlp.38,P17-1147,0,0.0314502,"uestion answering (also referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge 1 The dataset is available at https://github.com/ allenai/gooaq under an appropriate license. bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021). In contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., ‘what is’ or ‘can you’ questions), a collection of items such as ingredients (‘what are kinds of’, ‘things to’) or of steps towards a goal such as unlocking a phone (‘how to’), etc. Even when the answer is short, it can have rich types, e.g., unit conversion, time zone conversion, or a variety of knowledge look-up (‘h"
2021.findings-emnlp.38,2021.naacl-main.393,0,0.0234617,"cles. While our questions (extracted via autocomplete) were also likely frequently asked by Google users, our dataset represents a different and wider distribution of questions (§3.3), likely because it encompasses different classes of answers, particularly snippet and collection responses. Specifically, while NQ is dominated by ‘who’, ‘when’, and ‘how many’ questions (cf. Fig. 3(d)), G OOAQ has notably few ‘who’ questions and a substantial portion of questions starting with ‘how to’, ‘what is’, ‘what does’, ‘can you’. One notable QA dataset with long-form responses is ELI5 (Fan et al., 2019; Krishna et al., 2021), containing questions/answers mined from Reddit forums. In contrast, G OOAQ is collected differently and is several orders of magnitude larger than ELI5. Empirically, we show that models trained on G OOAQ transfer surprisingly well to ELI5 (§5.3), indicating G OOAQ’s broad coverage. It is worth highlighting that there is precedent for using search engines to create resources for the analysis of AI systems. Search engines harness colossal amounts of click information to help them effectively map input queries to a massive collection of information available in their index (Brin and Page, 1998;"
2021.findings-emnlp.38,Q19-1026,0,0.0606875,"Missing"
2021.findings-emnlp.38,P19-1612,0,0.0906374,"lso referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge 1 The dataset is available at https://github.com/ allenai/gooaq under an appropriate license. bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021). In contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., ‘what is’ or ‘can you’ questions), a collection of items such as ingredients (‘what are kinds of’, ‘things to’) or of steps towards a goal such as unlocking a phone (‘how to’), etc. Even when the answer is short, it can have rich types, e.g., unit conversion, time zone conversion, or a variety of knowledge look-up (‘how much’, ‘when is"
2021.findings-emnlp.38,2020.acl-main.703,0,0.0448029,"Missing"
2021.findings-emnlp.38,2021.eacl-main.86,0,0.0827255,"LI5 long-answers dataset. We release G OOAQ to facilitate further research on improving QA with diverse response types.1 1 Introduction Research in “open” question answering (also referred to as open-response, open-domain, or direct answer QA) has resulted in numerous datasets and powerful models for answering questions without a specified context. This task requires the use of background knowledge either stored in the QA model or retrieved from large corpora or knowledge 1 The dataset is available at https://github.com/ allenai/gooaq under an appropriate license. bases (Roberts et al., 2020; Lewis et al., 2021). Existing effort, however, involves isolated studies on niche answer types, mainly short responses and, in a few cases, long responses (Joshi et al., 2017; Lee et al., 2019; Bhakthavatsalam et al., 2021). In contrast, many of the everyday questions that humans deal with and pose to search engines have a more diverse set of response types, as illustrated in Fig. 1. Their answer can be a multi-sentence description (a snippet) (e.g., ‘what is’ or ‘can you’ questions), a collection of items such as ingredients (‘what are kinds of’, ‘things to’) or of steps towards a goal such as unlocking a phone"
2021.findings-emnlp.71,2020.acl-tutorials.1,0,0.153859,"trained on very large corpora (Peters et al., Finally, we compare probes across time with re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we systematically con- shown by linguistic probes. Some other tasks that duct probing across the pretraining iterations, to are designed to test more complex knowledge (e.g., understand not just what is learned (as explored in ReCoRD, Zhang et al., 2018 and WSC, Levesque numerous past analyses of fixed, already-trained et al., 2012) benefit from longer pretraining time, ∗ Equal contribution. aligning well with our findings for the correspond820 Findings of the Association for Computational Linguistics: EM"
2021.findings-emnlp.71,2020.emnlp-main.553,0,0.0442426,"re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we systematically con- shown by linguistic probes. Some other tasks that duct probing across the pretraining iterations, to are designed to test more complex knowledge (e.g., understand not just what is learned (as explored in ReCoRD, Zhang et al., 2018 and WSC, Levesque numerous past analyses of fixed, already-trained et al., 2012) benefit from longer pretraining time, ∗ Equal contribution. aligning well with our findings for the correspond820 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 820–842 November 7–11, 2021. ©2021 Association for Computational Linguis"
2021.findings-emnlp.71,W19-4828,0,0.0190918,"iverse Current NLP approaches lean heavily on language domains is more important than the quantity alone. models trained on very large corpora (Peters et al., Finally, we compare probes across time with re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we systematically con- shown by linguistic probes. Some other tasks that duct probing across the pretraining iterations, to are designed to test more complex knowledge (e.g., understand not just what is learned (as explored in ReCoRD, Zhang et al., 2018 and WSC, Levesque numerous past analyses of fixed, already-trained et al., 2012) benefit from longer pretraining time, ∗ Equal contribution"
2021.findings-emnlp.71,N19-1423,0,0.195512,"led learning that these models undergo and patterns of different types of knowledge generally guide us toward more efficient approaches that hold regardless of the data variation. However, difaccomplish necessary learning faster. ferent data choices do have an impact on the learning speed and the final performance. Our findings 1 Introduction suggest that the inclusion of data in more diverse Current NLP approaches lean heavily on language domains is more important than the quantity alone. models trained on very large corpora (Peters et al., Finally, we compare probes across time with re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we sy"
2021.findings-emnlp.71,I05-5002,0,0.0104199,"Missing"
2021.findings-emnlp.71,D19-1445,0,0.0347624,"Missing"
2021.findings-emnlp.71,N19-1112,1,0.691418,"m future work on more efficient pretraining (e.g., Models of language trained on very large corfewer iterations are needed to acquire some kinds pora have been demonstrated useful for natof knowledge) and on understanding dependencies ural language processing. As fixed artifacts, among different kinds of knowledge. they have become the object of intense study, Specifically, we apply a probing across time with many researchers “probing” the extent to which they acquire and readily demonstrate framework to the widely used RoBERTa masked linguistic abstractions, factual and commonlanguage model (Liu et al., 2019b). We reproduce sense knowledge, and reasoning abilities. Rethe pretraining of RoBERTa and apply a suite of cent work applied several probes to intermeprobes at many checkpoint iterations across prediate training stages to observe the developtraining (§3). Our rich probe suite covers a dimental process of a large-scale model (Chiverse range of desirable knowledge types: linguisang et al., 2020). Following this effort, we tic properties (Liu et al., 2019a), factual knowledge systematically answer a question: for vari(Petroni et al., 2019), and commonsense (Zhou ous types of knowledge a languag"
2021.findings-emnlp.71,N16-1098,0,0.0628918,"Missing"
2021.findings-emnlp.71,2020.acl-main.240,0,0.013765,"transferability (LKT) of contextual representations. For all tasks in LKT, we train a linear classifier model to predict the linguistic annotation of each word in a sentence. Through the performance of the classifier, we measure how closely the encoded information in the word representations conforms to linguistic annotations from human experts. Following Liu et al. (2019a), we use learnable coefficients to weigh a sum of representations from all the transformer layers, and compute the input vector to the classifier. We measure the probe performance by accuracy or F1 on the test sets. BL I MP Salazar et al. (2020) introduce a behavioral linguistic probe suite on the benchmark of linguistic minimal pairs (BL I MP, Warstadt et al., 2020). This benchmark isolates specific phenomena in syntax, morphology, or semantics such as island effects and subject-verb agreement. As seen in Table 1, input sentence pairs differ only by a word or a short phrase, but contrast in grammatical 2.1 Probe Suite Construction acceptability. We test whether RoBERTa scores the grammatical sentence higher than the ungramBelinkov et al. (2020) categorize existing probes matical one. The score for a sentence is calcuinto two familie"
2021.findings-emnlp.71,N19-1329,0,0.0483699,"Missing"
2021.findings-emnlp.71,D13-1170,0,0.0127317,"do have an impact on the learning speed and the final performance. Our findings 1 Introduction suggest that the inclusion of data in more diverse Current NLP approaches lean heavily on language domains is more important than the quantity alone. models trained on very large corpora (Peters et al., Finally, we compare probes across time with re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we systematically con- shown by linguistic probes. Some other tasks that duct probing across the pretraining iterations, to are designed to test more complex knowledge (e.g., understand not just what is learned (as explored in ReCoRD, Zhang et al., 2018 a"
2021.findings-emnlp.71,2020.tacl-1.25,0,0.0156644,"e linguistic annotation of each word in a sentence. Through the performance of the classifier, we measure how closely the encoded information in the word representations conforms to linguistic annotations from human experts. Following Liu et al. (2019a), we use learnable coefficients to weigh a sum of representations from all the transformer layers, and compute the input vector to the classifier. We measure the probe performance by accuracy or F1 on the test sets. BL I MP Salazar et al. (2020) introduce a behavioral linguistic probe suite on the benchmark of linguistic minimal pairs (BL I MP, Warstadt et al., 2020). This benchmark isolates specific phenomena in syntax, morphology, or semantics such as island effects and subject-verb agreement. As seen in Table 1, input sentence pairs differ only by a word or a short phrase, but contrast in grammatical 2.1 Probe Suite Construction acceptability. We test whether RoBERTa scores the grammatical sentence higher than the ungramBelinkov et al. (2020) categorize existing probes matical one. The score for a sentence is calcuinto two families. Structural probes train a lated by sequentially masking one word at a time lightweight classifier that predicts a label o"
2021.findings-emnlp.71,D14-1162,0,0.0850785,"er targeted knowledge can be easily extracted with few or zero additional parameters (i.e., ease of extraction, as suggested by Pimentel et al., 2020). We treat the probing scores as relative performance; and we address their concerns by comparing RoBERTa with the the following baselines: • Random Guess randomly selects one class label or token from the candidate pool. • Random Vector + Linear Classifier uses a random vector to represent each type, and trains a linear classifier on the top to predict the label with the token vector being frozen. • GloVe + Linear Classifier uses GloVe vectors (Pennington et al., 2014), and trains a linear classifier on top to predict the label. • Original RoBERTa probes the officially released checkpoint2 of RoBERTa base to see if our checkpoints are pretrained properly and can achieve reasonable performance. Moreover, our probing results on different checkpoints can illustrate the relative performance change during pretraining. 2.3 Pretraining Setups We choose base-size RoBERTa as a case study. In order to conduct probing over time, we replicate the RoBERTa pretraining procedure and periodically save checkpoints for later probing. To ensure that probe-task relevant text i"
2021.findings-emnlp.71,Q19-1040,0,0.044648,"Missing"
2021.findings-emnlp.71,N18-1202,0,0.176215,"Missing"
2021.findings-emnlp.71,N18-1101,0,0.0477928,"Missing"
2021.findings-emnlp.71,D19-1250,0,0.113859,"d linguistic abstractions, factual and commonlanguage model (Liu et al., 2019b). We reproduce sense knowledge, and reasoning abilities. Rethe pretraining of RoBERTa and apply a suite of cent work applied several probes to intermeprobes at many checkpoint iterations across prediate training stages to observe the developtraining (§3). Our rich probe suite covers a dimental process of a large-scale model (Chiverse range of desirable knowledge types: linguisang et al., 2020). Following this effort, we tic properties (Liu et al., 2019a), factual knowledge systematically answer a question: for vari(Petroni et al., 2019), and commonsense (Zhou ous types of knowledge a language model learns, when during (pre)training are they acet al., 2020) and basic reasoning capabilities (Talquired? Using RoBERTa as a case study, we mor et al., 2019). Our main finding is that linguistic find: linguistic knowledge is acquired fast, stainformation tends to be acquired fast, factual and bly, and robustly across domains. Facts and commonsense knowledge slower, and reasoning commonsense are slower and more domainabilities are largely unlearned. sensitive. Reasoning abilities are, in general, We next apply probing across time to"
2021.findings-emnlp.71,2020.acl-main.420,0,0.0436311,"into pretraining RoBERTa. 2.2 Baselines for Relative Performance Probes are not a perfect, absolute measure of encoded knowledge. In particular, Hewitt and Liang (2019) find that probing classifiers can memorize labeling decisions independently of the linguistic knowledge of the representations. Pimentel et al. Talmor et al. (2019) introduce a be- (2020) argue that a tighter estimate of the encoded 822 knowledge can be obtained by complex probing models. We ask whether targeted knowledge can be easily extracted with few or zero additional parameters (i.e., ease of extraction, as suggested by Pimentel et al., 2020). We treat the probing scores as relative performance; and we address their concerns by comparing RoBERTa with the the following baselines: • Random Guess randomly selects one class label or token from the candidate pool. • Random Vector + Linear Classifier uses a random vector to represent each type, and trains a linear classifier on the top to predict the label with the token vector being frozen. • GloVe + Linear Classifier uses GloVe vectors (Pennington et al., 2014), and trains a linear classifier on top to predict the label. • Original RoBERTa probes the officially released checkpoint2 of"
2021.findings-emnlp.71,2020.emnlp-main.58,0,0.0334738,"Missing"
2021.naacl-main.422,P18-1198,0,0.0234565,"(Tan and Bansal, 2019; Lu et al., 2019, 2020)— that see visual data in training, finding them to slightly outperform text-only models. Contrasting the learned mappings with human judgment, the examined visually grounded language models significantly underperform human subjects, exposing much room for future improvement. 2 Related Work What is encoded in language representations? Understanding what information NLP models encode has attracted great interest in recent years 5368 (Rogers et al., 2020). From factual (Petroni et al., 2019; Jawahar et al., 2019; Roberts et al., 2020) to linguistic (Conneau et al., 2018; Liu et al., 2019a; Talmor et al., 2019) and commonsense (Forbes et al., 2019) knowledge, a wide set of properties have been previously analysed. We refer to Belinkov and Glass (2019) and Rogers et al. (2020) for a more comprehensive literature review. A common approach, often used for inspecting contextual models, is probing (Shi et al., 2016; Adi et al., 2016; Conneau et al., 2018; Hewitt and Liang, 2019). In short, it consists of using supervised models to predict properties not directly inferred by the models. Probing is typically used in settings were discrete, linguistic annotations suc"
2021.naacl-main.422,N19-1423,0,0.18548,"xt-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models. 1 Figure 1: We introduce a probing mechanism that learns a mapping from contextual language representations to visual features. For a number of contextual language models, we evaluate how useful their representations are for retrieving matching image patches. Introduction Contextual language models trained on text-only corpora are prevalent in recent natural language processing (NLP) literature (Devlin et al., 2019; Liu et al., 2019b; Lan et al., 2019; Raffel et al., 2019). Understanding what their representations encode has been the goal of a number of recent studies (Belinkov and Glass, 2019; Rogers et al., 2020). Yet, much is left to be understood about whether—or to what extent—these models can encode visual information. We study this problem in the context of language grounding (Searle et al., 1984; Harnad, 1990; McClelland et al., 2019; Bisk et al., 2020; Bender and Koller, 2020), empirically investigating whether text-only representations can naturally be connected to the visual domain, without e"
2021.naacl-main.422,W18-2501,0,0.021854,"Missing"
2021.naacl-main.422,2020.acl-main.463,0,0.0121709,"ontextual language models trained on text-only corpora are prevalent in recent natural language processing (NLP) literature (Devlin et al., 2019; Liu et al., 2019b; Lan et al., 2019; Raffel et al., 2019). Understanding what their representations encode has been the goal of a number of recent studies (Belinkov and Glass, 2019; Rogers et al., 2020). Yet, much is left to be understood about whether—or to what extent—these models can encode visual information. We study this problem in the context of language grounding (Searle et al., 1984; Harnad, 1990; McClelland et al., 2019; Bisk et al., 2020; Bender and Koller, 2020), empirically investigating whether text-only representations can naturally be connected to the visual domain, without explicit visual supervision in pre-training. We argue that context plays a significant role in this investigation. In language, the ability to form context-dependent representations has shown to be crucial in designing pre-trained language models (Peters et al., 2018; Devlin et al., 2019). This is even more important for studying grounding since many visual properties depend strongly on context (Sadeghi and Farhadi, 2011). For instance, a “flying bat” shares very few visual si"
2021.naacl-main.422,2020.emnlp-main.703,0,0.0372567,"Missing"
2021.naacl-main.422,D19-1275,0,0.0670812,"ation NLP models encode has attracted great interest in recent years 5368 (Rogers et al., 2020). From factual (Petroni et al., 2019; Jawahar et al., 2019; Roberts et al., 2020) to linguistic (Conneau et al., 2018; Liu et al., 2019a; Talmor et al., 2019) and commonsense (Forbes et al., 2019) knowledge, a wide set of properties have been previously analysed. We refer to Belinkov and Glass (2019) and Rogers et al. (2020) for a more comprehensive literature review. A common approach, often used for inspecting contextual models, is probing (Shi et al., 2016; Adi et al., 2016; Conneau et al., 2018; Hewitt and Liang, 2019). In short, it consists of using supervised models to predict properties not directly inferred by the models. Probing is typically used in settings were discrete, linguistic annotations such as parts of speech are available. Our approach differs from previous work in both scope and methodology, using a probe to measure similarities with continuous, visual representations. Closer to our goal of better understanding grounding is the work of Cao et al. (2020), that design probes for examining multi-modal models. In contrast, our work examines text-only models and does not rely on their ability to"
2021.naacl-main.422,P19-1356,0,0.0162842,"mber of grounded language models—such as LXMERT and VILBERT (Tan and Bansal, 2019; Lu et al., 2019, 2020)— that see visual data in training, finding them to slightly outperform text-only models. Contrasting the learned mappings with human judgment, the examined visually grounded language models significantly underperform human subjects, exposing much room for future improvement. 2 Related Work What is encoded in language representations? Understanding what information NLP models encode has attracted great interest in recent years 5368 (Rogers et al., 2020). From factual (Petroni et al., 2019; Jawahar et al., 2019; Roberts et al., 2020) to linguistic (Conneau et al., 2018; Liu et al., 2019a; Talmor et al., 2019) and commonsense (Forbes et al., 2019) knowledge, a wide set of properties have been previously analysed. We refer to Belinkov and Glass (2019) and Rogers et al. (2020) for a more comprehensive literature review. A common approach, often used for inspecting contextual models, is probing (Shi et al., 2016; Adi et al., 2016; Conneau et al., 2018; Hewitt and Liang, 2019). In short, it consists of using supervised models to predict properties not directly inferred by the models. Probing is typically"
2021.naacl-main.422,P14-1132,0,0.467126,"ation. In language, the ability to form context-dependent representations has shown to be crucial in designing pre-trained language models (Peters et al., 2018; Devlin et al., 2019). This is even more important for studying grounding since many visual properties depend strongly on context (Sadeghi and Farhadi, 2011). For instance, a “flying bat” shares very few visual similarities with a “baseball bat”; likewise, a “dog sleeping” looks different from a “dog running”. While alignments between language representations and visual attributes have attracted past interest (Leong and Mihalcea, 2011; Lazaridou et al., 2014, 2015; Lucy and Gauthier, 2017; Collell Talleda et al., 2017), the role of context has been previously overlooked, leaving many open questions about what visual information contextual language representations encode. In this work, we introduce a method for empirically probing contextual language representations and their relation to the visual domain. In general, probing examines properties for which the models are not designed to predict, but can be encoded in their representations (Shi et al., 2016; Rogers et al., 5367 Proceedings of the 2021 Conference of the North American Chapter of the"
2021.naacl-main.422,N15-1016,0,0.0251123,"19). A number of previous work have investigated mappings between language and visual representations or mappings from both to a shared space. Leong and Mihalcea (2011) investigate semantic similarities between words and images through a joint latent space, finding a positive correlation with human rated similarities. Similarly, Silberer and Lapata (2014) builds multi-modal representations by using stacked autoencoders. Socher et al. (2013) and Lazaridou et al. (2014) show that a shared latent space allows for zero-shot learning, demonstrating some generalization to previously unseen objects. Lazaridou et al. (2015) construct grounded word representations by exposing them to aligned visual features at training time. Lucy and Gauthier (2017) investigate how well word representations can predict perceptual and conceptual features, showing that a number of such features are not adequately predicted. Collell Talleda et al. (2017) uses word embeddings to create a mapping from language to visual features, using its outputs to build multimodal representations. While our conclusions are generally aligned, our work differs from these in two important ways. Firstly, previous work studies context-independent word r"
2021.naacl-main.422,W11-0120,0,0.228385,"cant role in this investigation. In language, the ability to form context-dependent representations has shown to be crucial in designing pre-trained language models (Peters et al., 2018; Devlin et al., 2019). This is even more important for studying grounding since many visual properties depend strongly on context (Sadeghi and Farhadi, 2011). For instance, a “flying bat” shares very few visual similarities with a “baseball bat”; likewise, a “dog sleeping” looks different from a “dog running”. While alignments between language representations and visual attributes have attracted past interest (Leong and Mihalcea, 2011; Lazaridou et al., 2014, 2015; Lucy and Gauthier, 2017; Collell Talleda et al., 2017), the role of context has been previously overlooked, leaving many open questions about what visual information contextual language representations encode. In this work, we introduce a method for empirically probing contextual language representations and their relation to the visual domain. In general, probing examines properties for which the models are not designed to predict, but can be encoded in their representations (Shi et al., 2016; Rogers et al., 5367 Proceedings of the 2021 Conference of the North"
2021.naacl-main.422,N19-1112,0,0.247235,"ls in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models. 1 Figure 1: We introduce a probing mechanism that learns a mapping from contextual language representations to visual features. For a number of contextual language models, we evaluate how useful their representations are for retrieving matching image patches. Introduction Contextual language models trained on text-only corpora are prevalent in recent natural language processing (NLP) literature (Devlin et al., 2019; Liu et al., 2019b; Lan et al., 2019; Raffel et al., 2019). Understanding what their representations encode has been the goal of a number of recent studies (Belinkov and Glass, 2019; Rogers et al., 2020). Yet, much is left to be understood about whether—or to what extent—these models can encode visual information. We study this problem in the context of language grounding (Searle et al., 1984; Harnad, 1990; McClelland et al., 2019; Bisk et al., 2020; Bender and Koller, 2020), empirically investigating whether text-only representations can naturally be connected to the visual domain, without explicit visual sup"
2021.naacl-main.422,2021.ccl-1.108,0,0.0335089,"Missing"
2021.naacl-main.422,W17-2810,0,0.105685,"y to form context-dependent representations has shown to be crucial in designing pre-trained language models (Peters et al., 2018; Devlin et al., 2019). This is even more important for studying grounding since many visual properties depend strongly on context (Sadeghi and Farhadi, 2011). For instance, a “flying bat” shares very few visual similarities with a “baseball bat”; likewise, a “dog sleeping” looks different from a “dog running”. While alignments between language representations and visual attributes have attracted past interest (Leong and Mihalcea, 2011; Lazaridou et al., 2014, 2015; Lucy and Gauthier, 2017; Collell Talleda et al., 2017), the role of context has been previously overlooked, leaving many open questions about what visual information contextual language representations encode. In this work, we introduce a method for empirically probing contextual language representations and their relation to the visual domain. In general, probing examines properties for which the models are not designed to predict, but can be encoded in their representations (Shi et al., 2016; Rogers et al., 5367 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational L"
2021.naacl-main.422,D14-1162,0,0.0886585,"Missing"
2021.naacl-main.422,N18-1202,0,0.0686356,"whether—or to what extent—these models can encode visual information. We study this problem in the context of language grounding (Searle et al., 1984; Harnad, 1990; McClelland et al., 2019; Bisk et al., 2020; Bender and Koller, 2020), empirically investigating whether text-only representations can naturally be connected to the visual domain, without explicit visual supervision in pre-training. We argue that context plays a significant role in this investigation. In language, the ability to form context-dependent representations has shown to be crucial in designing pre-trained language models (Peters et al., 2018; Devlin et al., 2019). This is even more important for studying grounding since many visual properties depend strongly on context (Sadeghi and Farhadi, 2011). For instance, a “flying bat” shares very few visual similarities with a “baseball bat”; likewise, a “dog sleeping” looks different from a “dog running”. While alignments between language representations and visual attributes have attracted past interest (Leong and Mihalcea, 2011; Lazaridou et al., 2014, 2015; Lucy and Gauthier, 2017; Collell Talleda et al., 2017), the role of context has been previously overlooked, leaving many open que"
2021.naacl-main.422,D19-1250,0,0.0590458,"Missing"
2021.naacl-main.422,2020.emnlp-main.437,0,0.155175,"Missing"
2021.naacl-main.422,2020.tacl-1.54,0,0.0335355,"Missing"
2021.naacl-main.422,D16-1159,0,0.0364226,"entations and visual attributes have attracted past interest (Leong and Mihalcea, 2011; Lazaridou et al., 2014, 2015; Lucy and Gauthier, 2017; Collell Talleda et al., 2017), the role of context has been previously overlooked, leaving many open questions about what visual information contextual language representations encode. In this work, we introduce a method for empirically probing contextual language representations and their relation to the visual domain. In general, probing examines properties for which the models are not designed to predict, but can be encoded in their representations (Shi et al., 2016; Rogers et al., 5367 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5367–5377 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 2: Examples of retrieved image patches from text-only representations using our probe. All shown images are retrieved from MS-COCO (Lin et al., 2014), using representations from BERT base. Importantly, these object categories (e.g. kite) are previously unseen by our probe. On the bottom rows, we show examples of the influence of context in retr"
2021.naacl-main.422,Q14-1006,0,0.0289368,"embeddings trained on 840 billion tokens of web data, with dL = 300 and a vocabulary size of 2.2 million.3 4.2 Vision models As is common practice in natural language grounding literature (Anderson et al., 2018; Tan and Bansal, 2019; Su et al., 2020; Lu et al., 2020), we use a Faster R-CNN model (Ren et al., 2015) trained on Visual Genome (Krishna et al., 2017) to extract visual features with dV = 2048. We use the trained network provided by Anderson et al. (2018)4 , and do not fine-tune during probe training. 4.3 Data We collect representations from two image captioning datasets, Flickr30k (Young et al., 2014), with over 150 thousand captions and 30 thousand images, and MS-COCO (Lin et al., 2014), with 600 thousand captions and 120 thousand images Higher IR and CR scores indicate better performance and, by definition, CR@k cannot be smaller than IR@k. These metrics form the basis of our 5371 2 https://github.com/huggingface/transformers https://nlp.stanford.edu/projects/glove/ 4 https://github.com/peteanderson80/bottom-up-attention 3 in English. The larger MS-COCO is the focus of the majority of our experiments. We build disjoint training, validation and test sets from the aggregated training and v"
2021.naacl-main.422,P14-1068,0,0.0183889,"2019, 2020; Chen et al., 2020; Li et al., 2020; Tan and Bansal, 2020). This is typically done through training and evaluating models in tasks and datasets where both images and text are used, such as visual question answering (Antol et al., 2015; Hudson and Manning, 2019). A number of previous work have investigated mappings between language and visual representations or mappings from both to a shared space. Leong and Mihalcea (2011) investigate semantic similarities between words and images through a joint latent space, finding a positive correlation with human rated similarities. Similarly, Silberer and Lapata (2014) builds multi-modal representations by using stacked autoencoders. Socher et al. (2013) and Lazaridou et al. (2014) show that a shared latent space allows for zero-shot learning, demonstrating some generalization to previously unseen objects. Lazaridou et al. (2015) construct grounded word representations by exposing them to aligned visual features at training time. Lucy and Gauthier (2017) investigate how well word representations can predict perceptual and conceptual features, showing that a number of such features are not adequately predicted. Collell Talleda et al. (2017) uses word embeddi"
2021.naacl-main.422,2020.tacl-1.48,0,0.070603,"Missing"
2021.naacl-main.422,D19-1514,0,0.278649,"eir textual counterparts. Retrieval performance drops substantially in these settings, attesting the selectivity of our probe. Moreover, we measure the impact of context on retrieval at the instance level. Contextual models substantially outperform non-contextual embeddings, but this difference disappears as context is gradually hidden from contextual models. When the context includes adjectives directly associated with the noun being inspected, we find significantly better instance retrieval performance. Finally, we investigate a number of grounded language models—such as LXMERT and VILBERT (Tan and Bansal, 2019; Lu et al., 2019, 2020)— that see visual data in training, finding them to slightly outperform text-only models. Contrasting the learned mappings with human judgment, the examined visually grounded language models significantly underperform human subjects, exposing much room for future improvement. 2 Related Work What is encoded in language representations? Understanding what information NLP models encode has attracted great interest in recent years 5368 (Rogers et al., 2020). From factual (Petroni et al., 2019; Jawahar et al., 2019; Roberts et al., 2020) to linguistic (Conneau et al., 2018;"
2021.naacl-main.422,2020.emnlp-main.162,0,0.0223286,"work in both scope and methodology, using a probe to measure similarities with continuous, visual representations. Closer to our goal of better understanding grounding is the work of Cao et al. (2020), that design probes for examining multi-modal models. In contrast, our work examines text-only models and does not rely on their ability to process images. Language grounding. A widely investigated research direction aims to connect natural language to the physical world (Bisk et al., 2020; McClelland et al., 2019; Tan and Bansal, 2019; Lu et al., 2019, 2020; Chen et al., 2020; Li et al., 2020; Tan and Bansal, 2020). This is typically done through training and evaluating models in tasks and datasets where both images and text are used, such as visual question answering (Antol et al., 2015; Hudson and Manning, 2019). A number of previous work have investigated mappings between language and visual representations or mappings from both to a shared space. Leong and Mihalcea (2011) investigate semantic similarities between words and images through a joint latent space, finding a positive correlation with human rated similarities. Similarly, Silberer and Lapata (2014) builds multi-modal representations by usin"
2021.naacl-main.46,P17-1171,0,0.0182428,"paragraph ranking (Path Retriever, Asai et al. 2020), and end-to-end neural retrieval (DPR, Karpukhin et al. 2020). Multilingual baselines. Alternatively, we can directly apply a multilingual pretrained model to retrieve paragraphs. We initialize and train a DPR encoder with multilingual BERT to enable multilingual document retrieval (Devlin et al., 2019). 3.2 X OR -E NGLISH S PAN: L-to-English Open-Retrieval QA Task. Given a question in Li and English Wikipedia Weng , a system retrieves paragraphs from Weng and extracts an answer. This task is equivalent to existing open-retrieval QA tasks (Chen et al., 2017), except that the query is not in English. This task involves challenging cross-lingual retrieval and question answering on the Li query and English evidence paragraphs. Evaluation. We use Exact Match (EM) and F1 over the annotated answer’s token set following prior work (Rajpurkar et al., 2016). Baselines. Our pipeline uses a machine reading model to find a minimal span that answers the question given paragraphs selected from the previous XOR-R ETRIEVE step. In particular, for the translate baselines, we use the same approach as state-ofthe-art models (Asai et al., 2020; Karpukhin et al., 202"
2021.naacl-main.46,2020.tacl-1.30,1,0.927876,"ing a real-world open-retrieval QA system for diverse languages. We expect that our dataset opens up new challenges to make progress in multilingual representation learning. In this paper, we introduce the task of crosslingual open-retrieval question answering (X OR QA) which aims at answering multilingual questions from non-English native speakers given multilingual resources. To support research in this area, we construct a dataset (called X OR-T Y D I QA) of 40k annotated questions and answers across 7 typologically diverse languages. Questions in our dataset are inherited from T Y D I QA (Clark et al., 2020), which are written by native speakers and are originally unanswerable due to the information scarcity or asymmetry issues. X OR-T Y D I QA is the first large-scale cross-lingual open-retrieval QA dataset that consists of information-seeking questions from native speakers and multilingual reference documents. 2 X OR-T Y D I QA is constructed with an annotation pipeline that allows for cross-lingual retrieval from large-scale Wikipedia corpora (§2). Unanswerable questions in T Y D I QA are first translated into English by professional translators. Then, annotators find answers to translated que"
2021.naacl-main.46,2020.acl-main.747,0,0.0393973,"evidence paragraphs can be found both in the target language and English, and a system has to output final answers based on the most plausible paragraphs. In this work, we introduce a simple multi7 We use the Moses tokenizer (Koehn et al., 2007) for all languages except we apply MeCab (Kudo, 2006) to Japanese. 552 lingual baseline that first looks for answers in the target language and then English if no answers are found in the target language. Specifically, we apply monolingual retrieval (i.e., BM25, Google Custom Search) for Wi and a multilingual machine reading model based on XLM-RoBERTa (Conneau et al., 2020) to find in-language answers in the target language (monolingual model; the bottom half of Fig. 3). If no answers are found by the monolingual model, we apply an X OR -E NGLISH S PAN baseline and translate English answers into the target language (the top half of Fig. 3). 4 68.3 85.6 73.1 68.9 70.9 65.2 72.2 70.0 82.0 70.2 63.0 63.6 63.7 64.1 41.6 57.0 43.7 38.8 43.8 35.2 44.6 67.5 83.2 68.1 60.1 66.3 60.4 65.0 63.3 78.9 64.1 52.3 54.0 56.5 62.5 52.5 63.2 65.9 52.1 46.5 47.3 22.7 51.6 64.8 59.5 41.7 37.6 38.1 18.1 50.4 57.7 58.9 37.3 42.8 44.0 44.9 Av. 72.1 68.1 43.5 67.2 61.7 50.0 44.5 48.0 A"
2021.naacl-main.46,N19-1423,1,0.0916543,"or do we know what model and training data they use. We encourage the community to use open MT systems where system details are available. For retrieval, we explore term-based retrieval (BM25, Robertson and Zaragoza 2009), term-based retrieval followed by neural paragraph ranking (Path Retriever, Asai et al. 2020), and end-to-end neural retrieval (DPR, Karpukhin et al. 2020). Multilingual baselines. Alternatively, we can directly apply a multilingual pretrained model to retrieve paragraphs. We initialize and train a DPR encoder with multilingual BERT to enable multilingual document retrieval (Devlin et al., 2019). 3.2 X OR -E NGLISH S PAN: L-to-English Open-Retrieval QA Task. Given a question in Li and English Wikipedia Weng , a system retrieves paragraphs from Weng and extracts an answer. This task is equivalent to existing open-retrieval QA tasks (Chen et al., 2017), except that the query is not in English. This task involves challenging cross-lingual retrieval and question answering on the Li query and English evidence paragraphs. Evaluation. We use Exact Match (EM) and F1 over the annotated answer’s token set following prior work (Rajpurkar et al., 2016). Baselines. Our pipeline uses a machine rea"
2021.naacl-main.46,2020.findings-emnlp.107,0,0.0444385,"Missing"
2021.naacl-main.46,forner-etal-2010-evaluating,0,0.0246994,"i et al., 2018; Zhang et al., 2019). Our X OR-T Y D I QA is also closely related to QA@CLEF 2003-2008 (Magnini et al., 2003, 2004; Vallin et al., 2005; Magnini et al., 2006; Giampiccolo et al., 2007; Forner et al., 2008); both QA@CLEF and X OR-T Y D I QA attempt to develop and evaluate multilingual QA systems. Nevertheless, there are three crucial differences. First, our X OR-T Y D I QA has a large number of questions that are required for training current state-of-the-art QA models like DPR, while QA@CLEF only has 200 evaluation questions 6 Conclusion for each language without training data (Forner et al., 2010). Secondly, the languages tested in We presented the task of X OR QA, in which a QA@CLEF are all European languages, with the system retrieves and reads documents across lanone exception of Indonesian; X OR-T Y D I QA in- guages to answer non-English information-seeking cludes typologically diverse languages. Lastly, questions. We introduced a new large-scale X OR the task setup of QA@CLEF 2003-2008 is either QA dataset, X OR-T Y D I QA, with 40k newly anmonolingual—questions and documents are writ- notated open-retrieval questions that cover seven ten in the same non-English language—or cross"
2021.naacl-main.46,P19-1227,0,0.0189436,"25.2 21.3 X OR -E NGLISH S PAN F1 Fi Ja Ko Ru 31.8 31.9 30.6 27.2 19.6 19.0 32.5 25.3 25.3 34.7 16.1 29.6 Avg 34.0 23.2 25.7 Table 8: F1 scores on X OR -E NGLISH S PAN and the BLEU scores in query translation on the dev set. All configurations use DPR. Telugu is excluded since Helsinki does not support it as of October, 2020. come the data scarcity in non-English languages. In addition to the datasets we already discussed in §2.2, several other non-English reading comprehension datasets have been created (Asai et al., 2018; Lim et al., 2019; Mozannar et al., 2019; d’Hoffschmidt et al., 2020). Liu et al. (2019) developed a template-based cloze task, leading to different data distributions from realistic questions with a great degree of lexical overlap between questions and reference paragraphs (Lee et al., 2019). More recently, Hardalov et al. (2020) introduced EXAMS, a multilingual multiple-choice reading comprehension dataset from school exams. does not know in which language it can find an answer in a non-parallel Wikipedia collection. Those differences from QA@CLEF tasks better simulate real-world scenarios and introduce new challenges that have yet to be extensively studied. Cross-lingual Infor"
2021.naacl-main.46,2020.emnlp-main.438,0,0.0318983,"ions use DPR. Telugu is excluded since Helsinki does not support it as of October, 2020. come the data scarcity in non-English languages. In addition to the datasets we already discussed in §2.2, several other non-English reading comprehension datasets have been created (Asai et al., 2018; Lim et al., 2019; Mozannar et al., 2019; d’Hoffschmidt et al., 2020). Liu et al. (2019) developed a template-based cloze task, leading to different data distributions from realistic questions with a great degree of lexical overlap between questions and reference paragraphs (Lee et al., 2019). More recently, Hardalov et al. (2020) introduced EXAMS, a multilingual multiple-choice reading comprehension dataset from school exams. does not know in which language it can find an answer in a non-parallel Wikipedia collection. Those differences from QA@CLEF tasks better simulate real-world scenarios and introduce new challenges that have yet to be extensively studied. Cross-lingual Information Retrieval Crosslingual Information Retrieval (CLIR) is the task of retrieving relevant documents when the document collection is in a different language from the query language (Hull and Grefenstette, 1996). The retrieval component in X"
2021.naacl-main.46,2020.emnlp-main.550,0,0.0338453,"Missing"
2021.naacl-main.46,P07-2045,0,0.00633655,"L are translated from English so the same spans may not exist in the target language’s Wikipedia. For this reason, we use token-level BLEU scores (Papineni et al., 2002) over a ground-truth token set in addition to F1 and EM. The same tokenizer is applied to ground-truth and predicted answers to 7 compute token-level F1 and BLEU. Baselines. Unlike the previous two tasks, evidence paragraphs can be found both in the target language and English, and a system has to output final answers based on the most plausible paragraphs. In this work, we introduce a simple multi7 We use the Moses tokenizer (Koehn et al., 2007) for all languages except we apply MeCab (Kudo, 2006) to Japanese. 552 lingual baseline that first looks for answers in the target language and then English if no answers are found in the target language. Specifically, we apply monolingual retrieval (i.e., BM25, Google Custom Search) for Wi and a multilingual machine reading model based on XLM-RoBERTa (Conneau et al., 2020) to find in-language answers in the target language (monolingual model; the bottom half of Fig. 3). If no answers are found by the monolingual model, we apply an X OR -E NGLISH S PAN baseline and translate English answers in"
2021.naacl-main.46,W19-4612,0,0.0429851,"Missing"
2021.naacl-main.46,D16-1264,0,0.124361,"Missing"
2021.naacl-main.46,P19-1612,1,0.919657,"we train base-sized (large for Russian) autoregressive transformers (Vaswani et al., 2017) on parallel corpora from OPUS (Tiedemann and Nygaard, 2004), MultiUN (Ziemski et al., 2016), or WMT19 (Barrault et al., 2019). All data are encoded into subwords by BPE (Sennrich et al., 2016) or SentencePiece (Kudo and Richardson, 2018). We use the fairseq library (Ott et al., 2019). Additional experimental details and full lists of hyperparameteres are available in Appendix §C. We only evaluate questions having answers and do not give credit to predicting “no answers” as in prior open-retrieval work (Lee et al., 2019). For XOR-R ETRIEVE and X OR -E NGLISH S PAN, we use cross-lingual data only and both cross-lingual and in-language data for X OR -F ULL. English translations of the questions used during the dataset collection as an upper bound of translate baselines. The best R@5kt macro-averaged over the 7 languages comes from running DPR on human translations: 72.1. Machine translation systems achieve averages of 67.2 (GMT) and 50.0 (our MT) again with DPR. The discrepancy between human and machine translation suggests that even state-of-the-art translation systems struggle to translate questions precisely"
2021.naacl-main.46,2020.emnlp-main.477,0,0.0193099,"4: Comparison with recent multilingual QA datasets. MKQA’s answers are aligned to WikiData. native speakers, and better reflect native speakers’ interests and their own linguistic phenomena. This distinguishes X OR-T Y D I QA from translationbased datasets such as MLQA (Lewis et al., 2020) and MKQA (Longpre et al., 2020). Second, our dataset requires cross-lingual retrieval unlike other multilingual datasets such as T Y D I QA or XQuAD (Artetxe et al., 2020), which focus on samelanguage QA. Lastly, questions in X OR-T Y D I QA require open retrieval from Wikipedia, whereas MLQA-R and XQuAD-R (Roy et al., 2020) limit the search space to matching each question with the predetermined 21k/31k sentences. 3 X OR QA Tasks and Baselines lingual and in-language questions from X OR-T Y D I QA. To diagnose where models fail and to allow researchers to use the data with less coding effort or computational resource, we also introduce the first two intermediate tasks that only use the crosslingual data (Table 2). We denote the target language by Li . We also denote the English Wikipedia collection by Weng and the Wikipedia collection in each target language Li by Wi . We experiment with baselines using black-box"
2021.naacl-main.46,2020.acl-main.653,0,0.0641186,"Missing"
2021.naacl-main.46,P16-1162,0,0.0109272,"machine reading models with the Natural Questions data (Kwiatkowski et al., 2019) and then further finetune on our X OR-T Y D I QA data. For the BM25 8 retrieval baseline, we use ElasticSearch to store and search documents using BM25 similarities. For both Path Retriever and DPR, we run the official open-source code. For our MT systems, we train base-sized (large for Russian) autoregressive transformers (Vaswani et al., 2017) on parallel corpora from OPUS (Tiedemann and Nygaard, 2004), MultiUN (Ziemski et al., 2016), or WMT19 (Barrault et al., 2019). All data are encoded into subwords by BPE (Sennrich et al., 2016) or SentencePiece (Kudo and Richardson, 2018). We use the fairseq library (Ott et al., 2019). Additional experimental details and full lists of hyperparameteres are available in Appendix §C. We only evaluate questions having answers and do not give credit to predicting “no answers” as in prior open-retrieval work (Lee et al., 2019). For XOR-R ETRIEVE and X OR -E NGLISH S PAN, we use cross-lingual data only and both cross-lingual and in-language data for X OR -F ULL. English translations of the questions used during the dataset collection as an upper bound of translate baselines. The best R@5kt"
2021.naacl-main.46,tiedemann-nygaard-2004-opus,0,0.151982,"Human Translation GMT Our MT Multi. DPR PATH BM DPR PATH DPR PATH DPR Experimental Setup For training, we first finetune the retrieval and machine reading models with the Natural Questions data (Kwiatkowski et al., 2019) and then further finetune on our X OR-T Y D I QA data. For the BM25 8 retrieval baseline, we use ElasticSearch to store and search documents using BM25 similarities. For both Path Retriever and DPR, we run the official open-source code. For our MT systems, we train base-sized (large for Russian) autoregressive transformers (Vaswani et al., 2017) on parallel corpora from OPUS (Tiedemann and Nygaard, 2004), MultiUN (Ziemski et al., 2016), or WMT19 (Barrault et al., 2019). All data are encoded into subwords by BPE (Sennrich et al., 2016) or SentencePiece (Kudo and Richardson, 2018). We use the fairseq library (Ott et al., 2019). Additional experimental details and full lists of hyperparameteres are available in Appendix §C. We only evaluate questions having answers and do not give credit to predicting “no answers” as in prior open-retrieval work (Lee et al., 2019). For XOR-R ETRIEVE and X OR -E NGLISH S PAN, we use cross-lingual data only and both cross-lingual and in-language data for X OR -F U"
2021.naacl-main.46,2020.eamt-1.61,0,0.0149872,"nslation-based counterpart (15.7 vs. 18.7 F1 points on average). Similar baselines perform considerably better in prior open-retrieval QA datasets, such as MKQA (30 EM points, Longpre et al., 2020) and NQ questions (40 F1, Karpukhin et al., 2020). This gap illustrates the multidimensional challenge of X OR-T Y D I QA. 4.5 Further Analysis Effects of translation performance on overall QA results. Table 8 compares the query translation BLEU scores and the final QA F1 performance of the translation-based baseline with three different MT systems in X OR -E NGLISH S PAN: GMT, Our MT, and Helsinki (Tiedemann and Thottingal, 2020). GMT significantly outperforms the other two baselines, demonstrating that its training setup may yield large improvements in these languages; similarly, in cases where additional parallel training data is not available, multilingual models may remain strong modeling tools. On the other hand, it is noteworthy that high BLEU scores do not always lead to better QA performance. In Bengali and Finnish, while Helsinki achieves a considerably better BLEU score than our MT (33.0 vs. 30.8 in Bengali and 29.8 vs. 27.4 in Finnish), our MT is 3.9 and 1.3 F1 points better in downstream X OR E NGLISH S PA"
2021.naacl-main.46,P19-1306,0,0.0130818,"ce new challenges that have yet to be extensively studied. Cross-lingual Information Retrieval Crosslingual Information Retrieval (CLIR) is the task of retrieving relevant documents when the document collection is in a different language from the query language (Hull and Grefenstette, 1996). The retrieval component in X OR QA is closely related to CLIR, but differs in several critical ways. First, since the end goal of X OR QA is QA, X OR QA queries always take question forms rather than search key words. Further, while CLIR typically retrieves documents from a single (low-resource) language (Zhang et al., 2019), X OR QA considers documents from both English and the query language. In many applications, we do not know a priori in which language we can find target information. Lastly, our document collection is orders of magnitude bigger than typical CLIR benchmarks (Sasaki et al., 2018; Zhang et al., 2019). Our X OR-T Y D I QA is also closely related to QA@CLEF 2003-2008 (Magnini et al., 2003, 2004; Vallin et al., 2005; Magnini et al., 2006; Giampiccolo et al., 2007; Forner et al., 2008); both QA@CLEF and X OR-T Y D I QA attempt to develop and evaluate multilingual QA systems. Nevertheless, there are"
2021.naacl-tutorials.5,2020.lrec-1.6,0,0.0151121,"ication to NLP tasks. We will start by explaining why processing long sequences is difficult. Many popular models scale poorly with the sequence length, either in computational or memory requirements, making them too expensive or impossible to run on current hardware. Another reason is that we want models that can capture long-distance information while ignoring large amounts of irrelevant text. The introduction also covers the tasks that we will use throughout the tutorial, namely information extraction (relation extraction (Jia et al., 2019) and coreference resolution (Pradhan et al., 2012; Bamman et al., 2020)), question answering (especially the multi-hop setting as in HotpotQA (Yang et al., 2018) and Wikihop (Welbl et al., 2018)), and document classification, and summarization. The next section will review well-established methods for dealing with long sequences, namely chunking and graph based methods. Chunking refers to splitting the sequence into smaller chunks, processing each one independently, then aggregating them in a task-specific way (Joshi et al., 2019). Hierarchical models are a special case of chunking where the chunks are linguistic constructs (usually sentences) that are aggregated"
2021.naacl-tutorials.5,2020.acl-tutorials.8,0,0.0285992,"chunking where the chunks are linguistic constructs (usually sentences) that are aggregated following the document hierarchy (Yang et al., 2016). Finally, retrieval-based methods use a recall-optimized simple model to retrieve short text snippets relevant for the task, then follow up with a stronger, more 1 Slides and Code https://github.com/allenai/ naacl2021-longdoc-tutorial 20 Proceedings of NAACL-HLT 2021: Tutorials, pages 20–24 June 6–11, 2021. ©2021 Association for Computational Linguistics expensive model. Retrieval methods have been discussed in detail in the Open Domain QA tutorial (Chen and Yih, 2020) so we will cover it here very briefly. Graph-based methods will also be discussed, with a focus on question answering. These methods usually use local context to identify potentially relevant information across the document, heuristically connect the identified information in a graph, then apply a graph neural network (Kipf and Welling, 2017) to propagate information across the document between the snippets. This is particularly effective for the multi-hop reasoning setting (Fang et al., 2019). model on current hardware, including memory optimization techniques such as gradient checkpointing"
2021.naacl-tutorials.5,N19-1370,0,0.0551966,"Missing"
2021.naacl-tutorials.5,N18-3011,1,0.797584,"on extraction as motivating tasks. In the end, we will have a hands-on coding exercise focused on summarization.1 A significant subset of natural language data includes documents that span thousands of tokens. The ability to process such long sequences is critical for many NLP tasks including document classification, summarization, multi-hop, and opendomain question answering, and document-level or multi-document relationship extraction and coreference resolution. These tasks have important practical applications in domains such as scientific document understanding and the digital humanities (Ammar et al., 2018; Cohan et al., 2018; Kocisk´y et al., 2018; Lo et al., 2020; Wang et al., 2020a). Yet, scaling state-of-the-art models to long sequences is challenging as many models are designed and tested for shorter sequences. One notable example is transformer models (Vaswani et al., 2017) that have O(N 2 ) computational cost in the sequence length N , making them prohibitively expensive to run for many long sequence tasks. This is reflected in many widely-used models such as RoBERTa and BERT where the sequence length is limited to only 512 tokens. In this tutorial, we aim at bringing interested NLP rese"
2021.naacl-tutorials.5,Q18-1023,0,0.0463441,"Missing"
2021.naacl-tutorials.5,2020.acl-main.703,0,0.0227461,"long sequence is important and why it is difficult. It will also introduce the NLP end-tasks that we will use throughout the tutorial. 2. Chunking, hierarchical, and graph based methods (35 minutes long): This section discusses graph-based methods and their application to information extraction and question answering, especially in the multi-hop reasoning setting. It also covers chunking and hierarchical methods as applied to coreference resolution, classification, and question answering. The following section is a practical use case on summarization. We will show how to start from the BART (Lewis et al., 2020) checkpoint, convert it into a model that can work with a long input that’s tens of thousands of tokens long, then finetune it on a long-input summarization task. It will also discuss practical techniques necessary to run the 21 2 • Survey of long sequence transformers (Tay et al., 2020) 3. Transformer-based methods (45 minutes long): This section reviews recently introduced long-sequence transformer models, compares the pros and cons of their designs, and discuss their applicability to NLP applications. • Extractive/Abstractive summarization (Subramanian et al., 2019) 8 4. Pretraining and fin"
2021.naacl-tutorials.5,2020.acl-main.447,0,0.014945,"ands-on coding exercise focused on summarization.1 A significant subset of natural language data includes documents that span thousands of tokens. The ability to process such long sequences is critical for many NLP tasks including document classification, summarization, multi-hop, and opendomain question answering, and document-level or multi-document relationship extraction and coreference resolution. These tasks have important practical applications in domains such as scientific document understanding and the digital humanities (Ammar et al., 2018; Cohan et al., 2018; Kocisk´y et al., 2018; Lo et al., 2020; Wang et al., 2020a). Yet, scaling state-of-the-art models to long sequences is challenging as many models are designed and tested for shorter sequences. One notable example is transformer models (Vaswani et al., 2017) that have O(N 2 ) computational cost in the sequence length N , making them prohibitively expensive to run for many long sequence tasks. This is reflected in many widely-used models such as RoBERTa and BERT where the sequence length is limited to only 512 tokens. In this tutorial, we aim at bringing interested NLP researchers up to speed about the recent and ongoing techniques"
2021.naacl-tutorials.5,W12-4501,0,0.0126724,"cessing and their application to NLP tasks. We will start by explaining why processing long sequences is difficult. Many popular models scale poorly with the sequence length, either in computational or memory requirements, making them too expensive or impossible to run on current hardware. Another reason is that we want models that can capture long-distance information while ignoring large amounts of irrelevant text. The introduction also covers the tasks that we will use throughout the tutorial, namely information extraction (relation extraction (Jia et al., 2019) and coreference resolution (Pradhan et al., 2012; Bamman et al., 2020)), question answering (especially the multi-hop setting as in HotpotQA (Yang et al., 2018) and Wikihop (Welbl et al., 2018)), and document classification, and summarization. The next section will review well-established methods for dealing with long sequences, namely chunking and graph based methods. Chunking refers to splitting the sequence into smaller chunks, processing each one independently, then aggregating them in a task-specific way (Joshi et al., 2019). Hierarchical models are a special case of chunking where the chunks are linguistic constructs (usually sentence"
2021.naacl-tutorials.5,Q18-1021,0,0.0162236,"ly with the sequence length, either in computational or memory requirements, making them too expensive or impossible to run on current hardware. Another reason is that we want models that can capture long-distance information while ignoring large amounts of irrelevant text. The introduction also covers the tasks that we will use throughout the tutorial, namely information extraction (relation extraction (Jia et al., 2019) and coreference resolution (Pradhan et al., 2012; Bamman et al., 2020)), question answering (especially the multi-hop setting as in HotpotQA (Yang et al., 2018) and Wikihop (Welbl et al., 2018)), and document classification, and summarization. The next section will review well-established methods for dealing with long sequences, namely chunking and graph based methods. Chunking refers to splitting the sequence into smaller chunks, processing each one independently, then aggregating them in a task-specific way (Joshi et al., 2019). Hierarchical models are a special case of chunking where the chunks are linguistic constructs (usually sentences) that are aggregated following the document hierarchy (Yang et al., 2016). Finally, retrieval-based methods use a recall-optimized simple model"
2021.naacl-tutorials.5,D18-1259,0,0.0126031,". Many popular models scale poorly with the sequence length, either in computational or memory requirements, making them too expensive or impossible to run on current hardware. Another reason is that we want models that can capture long-distance information while ignoring large amounts of irrelevant text. The introduction also covers the tasks that we will use throughout the tutorial, namely information extraction (relation extraction (Jia et al., 2019) and coreference resolution (Pradhan et al., 2012; Bamman et al., 2020)), question answering (especially the multi-hop setting as in HotpotQA (Yang et al., 2018) and Wikihop (Welbl et al., 2018)), and document classification, and summarization. The next section will review well-established methods for dealing with long sequences, namely chunking and graph based methods. Chunking refers to splitting the sequence into smaller chunks, processing each one independently, then aggregating them in a task-specific way (Joshi et al., 2019). Hierarchical models are a special case of chunking where the chunks are linguistic constructs (usually sentences) that are aggregated following the document hierarchy (Yang et al., 2016). Finally, retrieval-based methods us"
2021.naacl-tutorials.5,N16-1174,0,0.0227032,"he multi-hop setting as in HotpotQA (Yang et al., 2018) and Wikihop (Welbl et al., 2018)), and document classification, and summarization. The next section will review well-established methods for dealing with long sequences, namely chunking and graph based methods. Chunking refers to splitting the sequence into smaller chunks, processing each one independently, then aggregating them in a task-specific way (Joshi et al., 2019). Hierarchical models are a special case of chunking where the chunks are linguistic constructs (usually sentences) that are aggregated following the document hierarchy (Yang et al., 2016). Finally, retrieval-based methods use a recall-optimized simple model to retrieve short text snippets relevant for the task, then follow up with a stronger, more 1 Slides and Code https://github.com/allenai/ naacl2021-longdoc-tutorial 20 Proceedings of NAACL-HLT 2021: Tutorials, pages 20–24 June 6–11, 2021. ©2021 Association for Computational Linguistics expensive model. Retrieval methods have been discussed in detail in the Open Domain QA tutorial (Chen and Yih, 2020) so we will cover it here very briefly. Graph-based methods will also be discussed, with a focus on question answering. These"
D13-1029,P12-1041,0,0.0150219,"ow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from"
D13-1029,H05-1013,0,0.0623872,"Missing"
D13-1029,D13-1203,0,0.0561717,"Missing"
D13-1029,P08-2012,0,0.0307614,"C O + Gold NEL NEC O Stanford Sieves 85.8 84.6 84.5 NEC O + Gold NEL NEC O Stanford Sieves 56.4 51.3 43.9 F1 P Gold Mentions 75.5 80.3 91.4 81.2 74.0 78.9 90.5 80.4 72.2 77.8 89.9 77.7 Predicted Mentions 58.8 57.5 78.2 78.3 53.5 52.4 76.5 76.4 46.4 45.1 74.4 74.2 Pairwise R F1 F1 P 86.0 85.2 83.4 89.1 83.9 89.9 68.0 66.0 57.3 77.1 73.9 68.1 78.3 76.5 74.3 68.0 61.2 51.3 54.3 45.6 36.1 60.4 52.2 42.4 Table 4: Coreference results on ACE- NWIRE - NEL with gold and predicted mentions and gold or automatic linking. Method NEC O Stanford Sieves Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) P 85.0 84.6 77.0 71.3 78.7 MUC R 76.6 75.1 75.9 70.5 58.5 F1 80.6 79.6 76.5 70.9 67.1 P 87.6 87.3 79.4 86.8 B3 R 76.4 74.1 74.5 65.2 F1 81.6 80.2 76.9 74.5 P 79.3 79.4 66.9 62.6 76.1 Pairwise R F1 56.1 65.8 50.1 61.4 49.2 56.7 38.9 48.0 44.2 55.9 Table 5: Coreference results on ACE- NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia entity. As another example, two mentions of “Rutgers”"
D13-1029,D09-1120,0,0.2063,"es precision for two main reaMethod B3 R MUC R P NEC O + Gold NEL NEC O Stanford Sieves 85.8 84.6 84.5 NEC O + Gold NEL NEC O Stanford Sieves 56.4 51.3 43.9 F1 P Gold Mentions 75.5 80.3 91.4 81.2 74.0 78.9 90.5 80.4 72.2 77.8 89.9 77.7 Predicted Mentions 58.8 57.5 78.2 78.3 53.5 52.4 76.5 76.4 46.4 45.1 74.4 74.2 Pairwise R F1 F1 P 86.0 85.2 83.4 89.1 83.9 89.9 68.0 66.0 57.3 77.1 73.9 68.1 78.3 76.5 74.3 68.0 61.2 51.3 54.3 45.6 36.1 60.4 52.2 42.4 Table 4: Coreference results on ACE- NWIRE - NEL with gold and predicted mentions and gold or automatic linking. Method NEC O Stanford Sieves Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) P 85.0 84.6 77.0 71.3 78.7 MUC R 76.6 75.1 75.9 70.5 58.5 F1 80.6 79.6 76.5 70.9 67.1 P 87.6 87.3 79.4 86.8 B3 R 76.4 74.1 74.5 65.2 F1 81.6 80.2 76.9 74.5 P 79.3 79.4 66.9 62.6 76.1 Pairwise R F1 56.1 65.8 50.1 61.4 49.2 56.7 38.9 48.0 44.2 55.9 Table 5: Coreference results on ACE- NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia ent"
D13-1029,N10-1061,0,0.0609941,"the Stanford multi-pass sieve algorithm, which is the foundation for NEC O. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our m"
D13-1029,P03-1054,0,0.0243169,"l be assigned the same link as “president” but “The governor of Alaska Sarah Palin” would not be assigned an exact link to Sarah Palin. For mentions m0 that do not receive an exact link, we assign a head link h(m0 ) if the head word2 m has been linked, by setting h(m0 ) = l(m). For instance, the head link for the mention “President Clinton” (with “Clinton” as head word) will be the Wikipedia title of Bill Clinton. We use head links for the Relaxed NEL sieve (Sec. 3.6). Next, we define L(m) to be the set con2 A head word is assigned to every mention with the Stanford parser head finding rules (Klein and Manning, 2003). country company place body manager site senator attraction origin capital nominee president state agency market organization prosecutor stadium government plant operation candidate city region power park owner attorney network department airport author film area location unit province trial county building person kingdom period venue Figure 3: The most commonly used fine-grained attributes from Freebase and Wikipedia (out of over 500 total attributes). taining l(m) and l(m0 ) for all sub-phrases m0 of m. We add the sub-phrase links only if their confidence is higher than the confidence for l"
D13-1029,W11-1902,0,0.086838,"Missing"
D13-1029,J13-4004,0,0.472275,"new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NEC O is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NEC O extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent. We use NEL to increase recall during the mention detection phase and introduce two new cluster-merging sieves, which compare the Freebase attributes of entities. NEC O also improves NEL by initially favoring high precision linking results and then propagating links and attributes as clusters are formed. In summary we make the following contributions: 2.1 Coreference Resolution Coreference resolution is the the task of identifying all text spans (called mentions) that refer to"
D13-1029,P10-1142,0,0.0377113,"rained attributes caused precision errors in cases where many proper noun mentions were potential antecedents for a common noun. Although attributes such as country are useful for resolving a generic “country” mention, this information is insufficient when two distinct mentions such as “China” and “Russia” both have the country attribute. However, many recall errors are also caused by the lack of fine-grained attributes. Finding the ideal set of fine-grained attributes remains an open problem. 6 Related Work Coreference resolution has a fifty year history which defies brief summarization; see Ng (2010) for a recent survey. Section 2.1 described the Stanford multi-pass sieve algorithm, which is the foundation for NEC O. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., apposit"
D13-1029,N06-1025,0,0.132958,"hen two distinct mentions such as “China” and “Russia” both have the country attribute. However, many recall errors are also caused by the lack of fine-grained attributes. Finding the ideal set of fine-grained attributes remains an open problem. 6 Related Work Coreference resolution has a fifty year history which defies brief summarization; see Ng (2010) for a recent survey. Section 2.1 described the Stanford multi-pass sieve algorithm, which is the foundation for NEC O. Earlier coreference resolution systems used shallow semantics and pioneered knowledge extraction from online encyclopedias (Ponzetto and Strube, 2006; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations."
D13-1029,D08-1068,0,0.0651385,"reaMethod B3 R MUC R P NEC O + Gold NEL NEC O Stanford Sieves 85.8 84.6 84.5 NEC O + Gold NEL NEC O Stanford Sieves 56.4 51.3 43.9 F1 P Gold Mentions 75.5 80.3 91.4 81.2 74.0 78.9 90.5 80.4 72.2 77.8 89.9 77.7 Predicted Mentions 58.8 57.5 78.2 78.3 53.5 52.4 76.5 76.4 46.4 45.1 74.4 74.2 Pairwise R F1 F1 P 86.0 85.2 83.4 89.1 83.9 89.9 68.0 66.0 57.3 77.1 73.9 68.1 78.3 76.5 74.3 68.0 61.2 51.3 54.3 45.6 36.1 60.4 52.2 42.4 Table 4: Coreference results on ACE- NWIRE - NEL with gold and predicted mentions and gold or automatic linking. Method NEC O Stanford Sieves Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) P 85.0 84.6 77.0 71.3 78.7 MUC R 76.6 75.1 75.9 70.5 58.5 F1 80.6 79.6 76.5 70.9 67.1 P 87.6 87.3 79.4 86.8 B3 R 76.4 74.1 74.5 65.2 F1 81.6 80.2 76.9 74.5 P 79.3 79.4 66.9 62.6 76.1 Pairwise R F1 56.1 65.8 50.1 61.4 49.2 56.7 38.9 48.0 44.2 55.9 Table 5: Coreference results on ACE- NWIRE with gold mentions and automatic linking. sons. First, it reduces the coreference errors caused by incorrect NEL links. For instance, gold linking replaces the erroneous link generated by our NEL systems for “Nasser al-Kidwa” to the correct Wikipedia entity. As another example,"
D13-1029,W11-1901,0,0.156903,"to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the following three datasets: ACE- NWIRE, CONLL, and ACE- NWIRE - NEL. ACE- NWIRE, the newswire subset of the ACE 2004 corpus (NIST, 2004), includes 128 documents. The CONLL coreference dataset includes text from five different domains: broadcast conversation (BC), broadcast news (BN), magazine (MZ), newswire (NW), and web data (WB) (Pradhan et al., 2011). The broadcast conversation and broadcast news domains consist of transcripts, whereas magazine and newswire contain more standard written text. The development data includes 303 documents and the test data includes 322 documents. We created ACE- NWIRE - NEL by taking a subset of ACE- NWIRE and annotating with gold-standard entity links. We segment and link all the expressions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 lin"
D13-1029,D10-1048,0,0.119626,"stics We present NEC O, a new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NEC O is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NEC O extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent. We use NEL to increase recall during the mention detection phase and introduce two new cluster-merging sieves, which compare the Freebase attributes of entities. NEC O also improves NEL by initially favoring high precision linking results and then propagating links and attributes as clusters are formed. In summary we make the following contributions: 2.1 Coreference Resolution Coreference resolution is the the task of identifying all text spans (called ment"
D13-1029,P11-1082,0,0.142131,"; Daum´e III and Marcu, 2005; Ng, 2007). Some recent work shows improvement in coreference resolution by incorporating semantic information from Web-scale structured knowledge bases. Haghighi and Klein (2009) use a rule-based system to extract fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from Wikipedia categories and Freebase notable types. Ratinov and Roth (2012) investigated using NEL"
D13-1029,D12-1113,0,0.750669,"tures. For example, “Michael Eisner” is relatively unambiguous but the isolated mention “Eisner” is more challenging. However, these mentions could be clustered with a coreference model, allowing for improved NEL through link propagation from the easier mentions. 289 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 289–299, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics We present NEC O, a new algorithm for jointly solving named entity linking and coreference resolution. Our work is related to that of Ratinov and Roth (2012), which also uses knowledge derived from an NEL system to improve coreference. However, NEC O is the first joint model we know of, is purely deterministic with no learning phase, does automatic mention detection, and improves performance on both tasks. NEC O extends the Stanford’s sieve-based model, in which a high recall mention detection phase is followed by a sequence of cluster merging operations ordered by decreasing precision (Raghunathan et al., 2010; Lee et al., 2013). At each step, it merges two clusters only if all available information about their respective entities is consistent."
D13-1029,P11-1138,0,0.649874,") mj is a linked proper noun, (2) if mi or the title of its linked Wikipedia page is in the list of fine-grained attributes of mj , or (3) if h(mj ) is related to the head link h(mi ) according to Freebase as defined above. Because this sieve has low precision, we only allow merges between mentions that have a maximum distance of three sentences between one another. We add the Relaxed NEL sieve near the end of the pipeline, just before pronoun resolution. 293 4 Experimental Setup Core Components and Baselines The Stanford sieve-based coreference system (Lee et al., 2013), the GLOW NEL system (Ratinov et al., 2011), and WikipediaMiner (Milne and Witten, 2008) provide core functionality for our joint model, and are also the state-of-the-art baselines against which we measure performance. Parameter Settings Based on performance on the development set, we set the GLOW’s confidence parameter to 1.0 and WikipediaMiner’s to 0.4 to assure high-precision NEL. We also optimized for the set of fine-grained attributes to import from Wikipedia and Freebase, and the best way to incorporate the NEL constraints into the sieve architecture. Datasets We report results on the following three datasets: ACE- NWIRE, CON"
D13-1029,P10-1144,0,0.0134412,"ions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 linked entities. Metrics We evaluate our system using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and pairwise scores. MUC is a link-based metric which measures how many clusters need to be merged to cover the gold clusters and favors larger clusters; B 3 computes the proportion of intersection between predicted and gold clusters for every mention and favors singletons (Recasens and Hovy, 2010). We computed the scores using the Stanford Method Stanford Sieves NEC O No NEL Mentions No Mention Pruning No Attributes No Constraints P 39.9 46.8 46.1 43.6 45.9 42.3 MUC R 46.2 52.5 48.3 45.6 47.4 49.3 F1 42.8 49.5 47.2 44.6 46.6 45.5 P 67.9 70.4 71.4 70.5 71.8 68.3 B3 R 71.8 72.6 70.0 69.9 69.7 72.3 F1 69.8 71.5 70.9 70.2 70.7 70.2 P 44.2 51.5 49.7 46.2 48.6 44.2 Pairwise R F1 29.7 35.6 34.6 41.4 30.9 38.1 29.4 35.9 27.0 34.7 28.6 34.7 Table 1: Coreference results on ACE- NWIRE with predicted mentions and automatic linking. coreference software for ACE2004 and using the CoNLL scorer fo"
D13-1029,M95-1005,0,0.548965,"consist of transcripts, whereas magazine and newswire contain more standard written text. The development data includes 303 documents and the test data includes 322 documents. We created ACE- NWIRE - NEL by taking a subset of ACE- NWIRE and annotating with gold-standard entity links. We segment and link all the expressions in text that refer to Wikipedia pages, allowing for nested linking. For instance, both the phrase “Hong Kong Disneyland,” and the sub-phrase “Hong Kong” are linked. This dataset includes 12 documents and 350 linked entities. Metrics We evaluate our system using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and pairwise scores. MUC is a link-based metric which measures how many clusters need to be merged to cover the gold clusters and favors larger clusters; B 3 computes the proportion of intersection between predicted and gold clusters for every mention and favors singletons (Recasens and Hovy, 2010). We computed the scores using the Stanford Method Stanford Sieves NEC O No NEL Mentions No Mention Pruning No Attributes No Constraints P 39.9 46.8 46.1 43.6 45.9 42.3 MUC R 46.2 52.5 48.3 45.6 47.4 49.3 F1 42.8 49.5 47.2 44.6 46.6 45.5 P 67.9 70.4 71.4 70.5 71.8 68."
D13-1029,W13-3517,0,0.406478,"act fine-grained attributes for mentions by analyzing 297 precise constructs (e.g., appositives) in Wikipedia articles. Subsequently, Haghighi and Klein (2010) used a generative approach to learn entity types from an initial list of unambiguous mention types. Bansal and Klein (2012) use statistical analysis of Web ngram features including lexical relations. Rahman and Ng (2011) use YAGO to extract type relations for all mentions. These methods incorporate knowledge about all possible meanings of a mention. If a mention has multiple meanings, extraneous information might be associated with it. Zheng et al. (2013) use a ranked list of candidate entities for each mention and maintain the ranked list when mentions are merged. Unlike previous work, our method relies on NEL systems to disambiguate possible meanings of a mention and capture highprecision semantic knowledge from Wikipedia categories and Freebase notable types. Ratinov and Roth (2012) investigated using NEL to improve coreference resolution, but did not consider a joint approach. They extracted attributes from Wikipedia categories and used them as features in a learned mention-pair model, but did not do mention detection. Unfortunately, it is"
D13-1029,H05-1033,0,\N,Missing
D13-1029,W11-1900,0,\N,Missing
D13-1029,W11-0300,0,\N,Missing
D14-1043,Q13-1005,0,0.03636,"l” also has an affinity for the pass events:head pass attribute due to the fact that many events with this attribute are attempts on goal. This correlates with domain knowledge about soccer, because, although there may be other uses of their head by a player in the game, shots on goal are events which will nearly always be commented upon by an announcer. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for langu"
D14-1043,P09-1010,0,0.0466613,"r model by encoding the dynamics of the environment. We did not attempt to learn this information in our process, but it is likely that modeling the event transition probabilities could provide better results. A larger future work would extend the method outlined herein to produce templates for automated commentary generation. ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. Impressive advances have been made grounding language in instructions. Branavan et al. (2009) and Vogel and Jurafsky (2010) work in the domain of computer technical support instructions, mapping language to actions using reinforcement learning. Matuszek et al. (2012b) parses simple language to robot control instructions. Our work focuses on dealing with a richer space, both in terms of the language used and the worldrepresentation into which it is grounded, and leveraging the multiple resolutions of reference. An exciting direction of research, closer to our own, aims to ground natural language in visual perception systems. Matuszek et al. (2012a) attempts to learn a joint model of la"
D14-1043,D09-1001,0,0.0601136,"to the fact that many events with this attribute are attempts on goal. This correlates with domain knowledge about soccer, because, although there may be other uses of their head by a player in the game, shots on goal are events which will nearly always be commented upon by an announcer. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 200"
D14-1043,P13-1092,0,0.0689355,"ents with this attribute are attempts on goal. This correlates with domain knowledge about soccer, because, although there may be other uses of their head by a player in the game, shots on goal are events which will nearly always be commented upon by an announcer. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011) where simple language describes each event, and events are in a one-to-one correspondence with utterances. Another dataset used for language grounding is the Weather Report Dataset (Liang et al., 2009). Here, aga"
D14-1043,P06-2034,0,0.346297,"t, 3 of these are aligned with at least 1 pass events:head pass event, making this strong association a correct one. The word “goal” also has an affinity for the pass events:head pass attribute due to the fact that many events with this attribute are attempts on goal. This correlates with domain knowledge about soccer, because, although there may be other uses of their head by a player in the game, shots on goal are events which will nearly always be commented upon by an announcer. 6 Related Work Early semantic parsing work made use of fully supervised training (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work has focused on reducing the amount of supervision required (Artzi and Zettlemoyer, 2013). A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013), but these are specific to translating language into queries in highly structured database and cannot be applied to our more flexible domain. There are few datasets as detailed as the Professional Soccer Commentary Dataset. Early work in understanding soccer commentaries focused on RoboCup soccer (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011"
D14-1043,P10-1083,0,0.0544924,"amics of the environment. We did not attempt to learn this information in our process, but it is likely that modeling the event transition probabilities could provide better results. A larger future work would extend the method outlined herein to produce templates for automated commentary generation. ical cues. The NFL Recap dataset (Snyder and Barzilay, 2007) is also laden with numerical fact matching, and does not include the fragment-level segmentation annotation that the PSC dataset provides. Impressive advances have been made grounding language in instructions. Branavan et al. (2009) and Vogel and Jurafsky (2010) work in the domain of computer technical support instructions, mapping language to actions using reinforcement learning. Matuszek et al. (2012b) parses simple language to robot control instructions. Our work focuses on dealing with a richer space, both in terms of the language used and the worldrepresentation into which it is grounded, and leveraging the multiple resolutions of reference. An exciting direction of research, closer to our own, aims to ground natural language in visual perception systems. Matuszek et al. (2012a) attempts to learn a joint model of language and object characterist"
D14-1043,P09-1011,0,0.446269,"in an utterance with a game event and achieve the desired segmentation. 2 Though it is tempting to discritize meaning in text, Chafe (1988) shows that readers imbue text with meaningful intonational patterns drawn from the potentially continuous space of auditory signals. 3 Our updated dataset is available at http://ssli. ee.washington.edu/tial/projects/multires/ 387 Supervision: For language grounding generally, and multi-resolution grounding specifically, supervised training data is expensive to produce. Also, the various grounding domains of interest are highly independent of one another (Liang et al., 2009). In the face of these issues, the ideal correspondence between language and world representation would be learned with as little supervision as possible. Unfortunately, there is a preponderance of examples such as (II) in Figure 2, where 4 verbs are used to describe a single “miss” event. (II) illustrates just one of the many difficulties of using syntactic information – elsewhere, events are referenced without an explicit verb whatsoever (such as the use of the phrase “into the books” to refer to a foul event). What is needed instead is a language model that is powerful enough to proscribe s"
D14-1043,P13-1006,0,0.0498117,"technical support instructions, mapping language to actions using reinforcement learning. Matuszek et al. (2012b) parses simple language to robot control instructions. Our work focuses on dealing with a richer space, both in terms of the language used and the worldrepresentation into which it is grounded, and leveraging the multiple resolutions of reference. An exciting direction of research, closer to our own, aims to ground natural language in visual perception systems. Matuszek et al. (2012a) attempts to learn a joint model of language and object characteristics of a workplace environment. Yu and Siskind (2013) grounds moderately rich language in automatically annotated video clips. Again, the contribution of our work versus the above is in the complexity of the language with which we deal and our multi-resolution model. 7 Acknowledgments This research was supported in part by a grant from the NSF (IIS-1352249), and the Royalty Research Fund (RRF) at the University of Washington. The authors also wish to thank Gina-Anne Levow, Yoav Artzi, Ben Hixon, and the anonymous reviewers for their valuable feedback on this work. Conclusion The problem of grounding complex natural human language such as soccer"
D14-1043,W08-1301,0,\N,Missing
D14-1043,C08-2022,0,\N,Missing
D14-1058,P09-1010,0,0.036039,"at https://www.cs. washington.edu/nlp/arithmetic. 523 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent w"
D14-1058,P12-1014,1,0.71329,"hington.edu/nlp/arithmetic. 523 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equati"
D14-1058,P09-1068,0,0.0400928,"tural Language Processing (EMNLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the othe"
D14-1058,de-marneffe-etal-2006-generating,0,0.0379725,"Missing"
D14-1058,D09-1100,0,0.00681362,"ms, and report on a series of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states in which the quantities of entities are updated or observed (Figure 2). We refer to these fragments as sentences. We represent the world state as a tuple hE, C, Ri consis"
D14-1058,P05-1045,0,0.0111166,"S automatically identifies entities, attributes, containers, and quantities corresponding to every sentence fragment (details in Figure 3 step 1). For every problem, this module returns a sequence of sentence fragments hw1 , . . . , wT , wx i where every wt consists of a verb vt , an entity et , its quantity numt , its attributes at , and up to two containers ct1 , ct2 . wx corresponds to the question sentence inquiring about an unknown entity. A RIS applies the Stanford dependency parser, named entity recognizer and coreference resolution system to the problem text (de Marneffe et al., 2006; Finkel et al., 2005; Raghunathan et al., 2010). It uses the predicted coreference relationships to replace pronouns (including possessive pronouns) with their Attributes: A RIS selects attributes A as modifiers for every entity from the dependency parser (details in Figure 3 step 1a). For example black is an attribute of the entity kitten and is an adjective modifier in the parser. These attributes are 526 1. Grounding into entities and containers: for every problem p in dataset (Section 4.1) (a) he1 , . . . , eT , ex ip ← extract all entities and the question entity i. Extract all numbers and noun phrases (NP)."
D14-1058,D09-1001,0,0.00705295,"addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states in which the quantities of entities are updated or observed (Figure 2). We refer to these fragments as sentences. We represent the world state as a tuple hE, C, Ri consisting of entities E, containers C, and relations R among entities, container"
D14-1058,P06-2034,0,0.075461,"sy-to-obtain training data; our results refine verb senses in WordNet (Miller, 1995) for arithmetic word problems; (c) We introduce a corpus of arithmetic word problems, and report on a series of experiments showing high efficacy in solving addition and subtraction problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states i"
D14-1058,D10-1048,0,0.00825614,"ifies entities, attributes, containers, and quantities corresponding to every sentence fragment (details in Figure 3 step 1). For every problem, this module returns a sequence of sentence fragments hw1 , . . . , wT , wx i where every wt consists of a verb vt , an entity et , its quantity numt , its attributes at , and up to two containers ct1 , ct2 . wx corresponds to the question sentence inquiring about an unknown entity. A RIS applies the Stanford dependency parser, named entity recognizer and coreference resolution system to the problem text (de Marneffe et al., 2006; Finkel et al., 2005; Raghunathan et al., 2010). It uses the predicted coreference relationships to replace pronouns (including possessive pronouns) with their Attributes: A RIS selects attributes A as modifiers for every entity from the dependency parser (details in Figure 3 step 1a). For example black is an attribute of the entity kitten and is an adjective modifier in the parser. These attributes are 526 1. Grounding into entities and containers: for every problem p in dataset (Section 4.1) (a) he1 , . . . , eT , ex ip ← extract all entities and the question entity i. Extract all numbers and noun phrases (NP). ii. h ← all noun types whi"
D14-1058,P11-1149,0,0.0379957,"problems based on verb categorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states in which the quantities of entities are updated or observed (Figure 2). We refer to these fragments as sentences. We represent the world state as a tuple hE, C, Ri consisting of entities E, containers C, and relations R among entities, containers, attributes, and quanti"
D14-1058,P10-1083,0,0.00746666,"tic. 523 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast w"
D14-1058,N13-1103,1,0.644952,"tegorization. 2 Related Work Understanding semantics of a natural language text has been the focus of many researchers in natural language processing (NLP). Recent work focus on learning to align text with meaning representations in specific, controlled domains. A few methods (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006) use an expensive supervision in the form of manually annotated formal representations for every sentence in the training data. More recent work (Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Goldwasser et al., 2011; Kushman and Barzilay, 2013) reduce the amount of required supervision in mapping sentences to 3 Arithmetic Problem Representation We address solving arithmetic word problems that include addition and subtraction. A problem text is split into fragments where each fragment is represented as a transition between two world states in which the quantities of entities are updated or observed (Figure 2). We refer to these fragments as sentences. We represent the world state as a tuple hE, C, Ri consisting of entities E, containers C, and relations R among entities, containers, attributes, and quantities. Entities: An entity is"
D14-1058,P14-1026,1,0.700373,"in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previous work on studying math word and logic problems uses manually aligned meaning representations or domain knowledge where the semantics for all the words is provided (Lev, 2007; Lev et al., 2004). Most recently, Kushman et al. (2014) introduced an algorithm that learns to align algebra problems to equations through the use of templates. This method applies to broad range of math problems, including multiplication, division, and simultaneous equations, while A RIS only handles arithmetic problems (addition and subtraction). However, our empirical results show that for the problems it handles, A RIS is much more robust to diversity in the problem types between the training and test data. a set of entities, their containers, attributes, quantities, and relations. A problem text is split into fragments where each fragment cor"
D14-1058,W04-0902,0,0.154997,"from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previous work on studying math word and logic problems uses manually aligned meaning representations or domain knowledge where the semantics for all the words is provided (Lev, 2007; Lev et al., 2004). Most recently, Kushman et al. (2014) introduced an algorithm that learns to align algebra problems to equations through the use of templates. This method applies to broad range of math problems, including multiplication, division, and simultaneous equations, while A RIS only handles arithmetic problems (addition and subtraction). However, our empirical results show that for the problems it handles, A RIS is much more robust to diversity in the problem types between the training and test data. a set of entities, their containers, attributes, quantities, and relations. A problem text is split"
D14-1058,P09-1011,0,0.0251498,"NLP), pages 523–533, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics meaning representations while taking advantage of special properties of the domains. Our method, on the other hand, requires small, easy-to-obtain training data in the form of verb categories that are shared among many different problem types. Our work is also closely related to the grounded language acquisition research (Snyder and Barzilay, 2007; Branavan et al., 2009; Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Chambers and Jurafsky, 2009; Liang et al., 2009; Bordes et al., 2010) where the goal is to align a text into underlying entities and events of an environment. These methods interact with an environment to obtain supervision from the real events and entities in the environment. Our method, on the other hand, grounds the problem into world state transitions by learning to predict verb categories in sentences. In addition, our method combines the representations of individual sentences into a coherent whole to form the equations. This is in contrast with the previous work that study each sentence in isolation from the other sentences. Previou"
D14-1058,D13-1029,1,\N,Missing
D15-1171,D14-1059,0,0.0195865,"tions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instructions. Coupling images and the"
D15-1171,Q13-1005,0,0.0108256,"allenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instructions. Coupling images and the corresponding text has attracted attention in both vision and NLP (Farhadi et al."
D15-1171,P14-1133,0,0.0123141,". Our contributions include: (1) designing and implementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work. 2 Related Work Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition"
D15-1171,D13-1160,0,0.11577,"Missing"
D15-1171,P12-1014,0,0.0126973,"ski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Castin"
D15-1171,P14-1134,0,0.0153331,"5 “A tangent line is drawn to circle O with radius of 5” Figure 4: Hypergraph representation of the sentence “A tangent line is drawn to circle O with radius of 5”. als L and the question text t. This score is the sum of the affinity scores P of individual literals lj ∈ L i.e., Atext (L, t) = j Atext (lj , t) where Atext (lj , t) 7→ [−∞, 0].3 G EO S learns a discriminative model Atext (lj , t; θ) that scores the affinity of every literal lj ∈ L and the question text t through supervised learning from training data. We represent literals using a hypergraph (Figure 4) (Klein and Manning, 2005; Flanigan et al., 2014). Each node in the graph corresponds to a concept in the geometry language (i.e. constants, variables, functions, or predicates). The edges capture the relations between concepts; concept nodes are connected if one concept is the argument of the other in the geometry language. In order to interpret the question text (Figure 3 step 1), G EO S first identifies concepts evoked by the words or phrases in the input text. Then, it learns the affinity scores which are the weights of edges in the hypergraph. It finally completes relations so that type matches are satistfied in the formal language. 4.1"
D15-1171,P06-2034,0,0.165077,"Missing"
D15-1171,D13-1029,1,0.84185,"Missing"
D15-1171,D14-1082,0,0.00336714,"etup Logical Language Ω: Ω consists of 13 types of entities and 94 function and predicates observed in our development set of geometry questions. Implementation details: Sentences in geometry questions often contain in-line mathematical expressions, such as “If AB=x+5, what is x?”. These mathematical expressions cause general purpose parsers to fail. G EO S uses an equation analyzer and pre-processes question text by replacing “=” with “equals”, and replacing mathematical terms (e.g., “x+5”) with a dummy noun so that the dependency parser does not fail. G EO S uses Stanford dependency parser (Chen and Manning, 2014) to obtain syntactic information, which is used to compute features for relation identification (Table 1). For diagram parsing, similar to Seo et al. (2014), we assume that G EO S has access to ground truth optical character recognition for labels in the diagrams. For optimization, we tune the parameters λ to 0.5, based on the training examples.4 Dataset: We built a dataset of SAT plane geometry questions where every question has a tex4 In our dataset, the number of all possible literals for each sentence is at most 1000. 1472 Questions Sentences Words Literals Binary relations Unary relations"
D15-1171,N15-1086,1,0.11084,"red to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instructions. Coupling images and the corresponding text"
D15-1171,P04-1054,0,0.0104618,"oney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., c"
D15-1171,D14-1058,1,0.463238,"correct a) 15 b) 30 c) 45 d) 60 e) 75 Figure 1: Questions (left column) and interpretations (right column) derived by G EO S. Introduction This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram (Figure 1). The geometry domain has a long history in AI, but previous work has focused on geometric theorem proving (Feigenbaum and Feldman, 1963) or geometric analogies (Evans, 1964). Arithmetic and algebraic word problems have attracted several NLP researchers (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015), but geometric word problems were first explored only last year by Seo et al. (2014). Still, this system merely aligned diagram elements with their textual mentions (e.g., “Circle O”)—it did not attempt to fully represent geometry problems or solve them. Answering geometry questions requires a method that interpert question text and diagrams in concert. 1 The source code, the dataset and the annotations are publicly available at geometry.allenai.org. The geometry genre has several distinctive characteristics. First, diagrams provide essential information absent from questio"
D15-1171,D09-1100,0,0.0429332,"Missing"
D15-1171,P13-1022,0,0.0100165,"vailable geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instruction"
D15-1171,D14-1043,1,0.835908,"geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsi"
D15-1171,P14-1026,0,0.380456,"what, MeasureOf(BAC)) correct a) 15 b) 30 c) 45 d) 60 e) 75 Figure 1: Questions (left column) and interpretations (right column) derived by G EO S. Introduction This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram (Figure 1). The geometry domain has a long history in AI, but previous work has focused on geometric theorem proving (Feigenbaum and Feldman, 1963) or geometric analogies (Evans, 1964). Arithmetic and algebraic word problems have attracted several NLP researchers (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015), but geometric word problems were first explored only last year by Seo et al. (2014). Still, this system merely aligned diagram elements with their textual mentions (e.g., “Circle O”)—it did not attempt to fully represent geometry problems or solve them. Answering geometry questions requires a method that interpert question text and diagrams in concert. 1 The source code, the dataset and the annotations are publicly available at geometry.allenai.org. The geometry genre has several distinctive characteristics. First, diagrams provide essential informat"
D15-1171,D13-1161,0,0.042558,"ude: (1) designing and implementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work. 2 Related Work Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al.,"
D15-1171,P09-1011,0,0.00771506,"tical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation problem as selecting the most likely subset of literals can be genera"
D15-1171,D09-1001,0,0.00968221,"irst results of this kind. Our contributions include: (1) designing and implementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work. 2 Related Work Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of ground"
D15-1171,Q14-1030,0,0.0116589,"lementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work. 2 Related Work Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jura"
D15-1171,Q15-1001,0,0.0621838,"45 d) 60 e) 75 Figure 1: Questions (left column) and interpretations (right column) derived by G EO S. Introduction This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram (Figure 1). The geometry domain has a long history in AI, but previous work has focused on geometric theorem proving (Feigenbaum and Feldman, 1963) or geometric analogies (Evans, 1964). Arithmetic and algebraic word problems have attracted several NLP researchers (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015), but geometric word problems were first explored only last year by Seo et al. (2014). Still, this system merely aligned diagram elements with their textual mentions (e.g., “Circle O”)—it did not attempt to fully represent geometry problems or solve them. Answering geometry questions requires a method that interpert question text and diagrams in concert. 1 The source code, the dataset and the annotations are publicly available at geometry.allenai.org. The geometry genre has several distinctive characteristics. First, diagrams provide essential information absent from question text. In Figure 1"
D15-1171,P10-1083,0,0.00953773,"et al., 2014). However, semantic parsers do not tackle diagrams—a critical element of the geometry genre. In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it challenging to learn semantic parsers directly from geometry questions. Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004). Again, both diagrams and small corpora are problematic for this body of work. Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation). In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information. Casting the interpretation probl"
D15-1239,P12-1094,0,0.162244,"n this paper, we look at whether (and how) language use affects the reaction, compared to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest. Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion. The primary contributions of th"
D15-1239,P14-1017,0,0.490505,"ed to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest. Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion. The primary contributions of this work include findings about the role of author reputation and"
D16-1168,P09-1068,0,0.0697666,"essing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the rewrite algorithm constructs new texts by substituting thematically a"
D16-1168,P11-1020,0,0.0392888,"l requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automati"
D16-1168,D14-1082,0,0.0399284,"Missing"
D16-1168,W14-3348,0,0.0586977,"Missing"
D16-1168,W08-1105,0,0.0318061,"s method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external parap"
D16-1168,P05-1045,0,0.0718412,"Missing"
D16-1168,D11-1108,0,0.0684546,"Missing"
D16-1168,N13-1092,0,0.0381253,"Missing"
D16-1168,S13-1035,0,0.0126664,"structed from fan-authored scripts of the first 10 episodes of the show (Springfield, 2016) totaling 1370 words. Since our thematic options are taken from arbitrary text, we use the lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our"
D16-1168,N15-1113,0,0.043938,"Missing"
D16-1168,W11-2123,0,0.00909718,"pringfield, 2016) totaling 1370 words. Since our thematic options are taken from arbitrary text, we use the lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our full model (F ULL): -S YN that only uses semantic and thematici"
D16-1168,D14-1058,1,0.830597,"iven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad defi"
D16-1168,P16-1084,0,0.0294871,"), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the script of a single movie, or a sampling of fan fiction from the Internet. This flexibility adds to the utility of our work, a"
D16-1168,N15-1022,1,0.807902,"7) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Si"
D16-1168,P13-1151,0,0.0128537,"o stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven"
D16-1168,Q15-1042,1,0.885046,"Missing"
D16-1168,N16-1136,1,0.804369,"2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the script of a single movie, or a sampling of fan fiction from the Internet. This flexibility adds to the u"
D16-1168,P14-1026,1,0.822938,"ckrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as them"
D16-1168,A97-1039,0,0.0925939,"tyre and Lapata (2009; 2010) address story generation through the automatic deduction and reassembly of scripts (Schank and Abelson, 1977), or structured representations of events and their participants, and causal relationships involved. Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events involving the entity and an ordering between these events, with the use of a genetic algorithm. Then plots are compiled into stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hw"
D16-1168,P14-2050,0,0.0415559,"lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our full model (F ULL): -S YN that only uses semantic and thematicity components and does not incorporate the syntactic compatibility score, -S EM replaces the semantic coher5 We set α"
D16-1168,P14-5010,0,0.00577863,"Missing"
D16-1168,P09-1025,0,0.190599,"the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the re"
D16-1168,P10-1158,0,0.294347,"rical Methods in Natural Language Processing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the rewrite algorithm constructs n"
D16-1168,D15-1118,0,0.0160036,"ch as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic"
D16-1168,N16-1098,0,0.0219292,", discourse relations, and solvability is essential. Previous work mainly focuses on rewriting single sentences. Second, we build a theme from a text corpus and show how the stories can be adapted to new themes. Third, our method leverages the human-authored story to capture the semantic skeleton and the plot of the current story, rather than synthesizing the story plot. To our knowledge, we are the first to introduce a text rewriting formulation for story generation. Story generation has been of long interest to AI researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016). Recent methods in story generation first synthesize candidate plots for a story and then compile those plots into text. Li et al. (2013) use crowdsourcing to build plot graphs. McIntyre and Lapata (2009; 2010) address story generation through the automatic deduction and reassembly of scripts (Schank and Abelson, 1977), or structured representations of events and their participants, and causal relationships involved. Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events invol"
D16-1168,D15-1202,0,0.0216471,"roduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a"
D16-1168,D16-1117,0,0.0127235,"es take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the"
D16-1168,D15-1044,0,0.029105,"highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (X"
D16-1168,D15-1171,1,0.815083,"ques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the scri"
D16-1168,D15-1135,0,0.0661954,"under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is def"
D16-1168,P08-1040,0,0.0150168,"apata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Z"
D16-1168,D11-1038,0,0.0274259,"izer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and"
D16-1168,P12-1107,0,0.0327745,"Missing"
D16-1168,Q16-1029,0,0.0113683,"), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3"
D16-1168,N10-1056,0,0.0213029,"3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Konc"
D16-1168,D15-1096,0,0.013681,"), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, th"
D16-1168,C10-1152,0,0.0280014,"gh the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are int"
D17-1279,P11-1051,0,0.0115548,"ropagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by intro"
D17-1279,U14-1012,0,0.0131253,"on of ULM. Dashed box is the uncertain token which is going to be marginalized over. Arrows and grey nodes are paths to be summed over during training. When all tokens are confident, the score of only one path is calculated. CRF prediction if the node is not connected to a labeled vertex, ensuring the algorithm performs at least as well as standard self-training. Posterior Estimates We develop two strategies to estimate the new posteriors pˆ(yt |x; θ), which can then be used in the CRF training. The first strategy (called G RAPH I NTERP) is the commonly used approach (Subramanya et al., 2010; Aliannejadi et al., 2014) that interpolates the smoothed posterior {q} with CRF marginals p: pˆ(yt |x; θ) = αp(yt |x; θ) + (1 − α)q(y) (3) where α is a mixing coefficient. A second strategy introduced here (called G RAPH F EAT) uses the smoothed posterior {q} as features and learns it with other parameters in the neural network. Given a sentence {x1 , . . . , xn }, let Q = {q1 , . . . , qn } be the predicted label distribution from the graph. We then use Q as a feature input to neural network as P˜ = P + M Q where P is the n × m matrix output by the bidirectional LSTM network as in Eq. 1, and M is m × m matrix and is"
D17-1279,W12-3202,0,0.0383559,"ng advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping fr"
D17-1279,N12-1073,0,0.0231903,"EVAL Task 10 by extending recent advances in neural tagging models; ii) introducing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain,"
D17-1279,W12-4303,0,0.0438615,"EVAL Task 10 by extending recent advances in neural tagging models; ii) introducing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain,"
D17-1279,S17-2091,0,0.250403,"ing (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and Søgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nich"
D17-1279,P17-2054,0,0.232683,"limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and Søgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nich"
D17-1279,D15-1041,0,0.0151072,"aset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes,"
D17-1279,bird-etal-2008-acl,0,0.109449,"sed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and Søgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural ap"
D17-1279,Q16-1026,0,0.011056,"øgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph"
D17-1279,W99-0613,0,0.258013,"(Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999) to the bootstrapping framework. QasemiZadeh and Schumann (2012) compile a dataset of scientific terms into 7 fine-grained categories for 171 abstracts of ACL anothology. Similar to our work, very recently Augenstein and Søgaard (2017) also evaluated on ScienceIE dataset, but use multi-task learning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a seque"
D17-1279,L16-1586,0,0.0916796,"g models; ii) introducing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrap"
D17-1279,I11-1001,0,0.0743763,"between an [atomic force micorscopy]Material and [metal surface]Material . Figure 1: Annotated ScienceIE examples. Introduction As a research community grows, more and more papers are published each year. As a result there is increasing demand for improved methods for finding relevant papers and automatically understanding the key ideas in those papers. However, due to the large variety of domains and extremely limited annotated resources, there has been relatively little work on scientific information extraction. Previous research has focused on unsupervised approaches such as bootstrapping (Gupta and Manning, 2011; Tsai et al., 2013), where hand-designed templates are used to extract scientific keyphrases, and more templates are added through bootstrapping. Very recently a new challenge on Scientific Information Extraction (ScienceIE) (Augenstein et al., 2017)1 provides a dataset consisting of 500 1 SemEval (Task 10)https://scienceie.github. io/index.html scientific paragraphs with keyphrase annotations for three categories: TASK, P ROCESS, M ATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in Figure 1. This dataset enables the use of more adv"
D17-1279,P14-2050,0,0.0133013,"h LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone. 3 Problem Definition and Data The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What P ROCESS or method has the paper"
D17-1279,P16-2020,1,0.773433,"mance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone. 3 Problem Definition and Data The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What P ROCESS or method has the paper used or compared t"
D17-1279,N15-1009,0,0.0782864,"ed embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone. 3 Problem Definition and Data The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What P ROCESS or method has the paper used or compared to? What M ATERIALS has the paper utilized in experiments? While these fundamental concepts are important in a wide variety of scientific disciplines, the terms that are used in specific disciplines can be substantially differ2642 ent. For example, M ATERIALS in"
D17-1279,N16-1030,0,0.56146,"n et al., 2017)1 provides a dataset consisting of 500 1 SemEval (Task 10)https://scienceie.github. io/index.html scientific paragraphs with keyphrase annotations for three categories: TASK, P ROCESS, M ATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in Figure 1. This dataset enables the use of more advanced approaches such as neural network (NN) models. To that end, we cast the keyphrase extraction task as a sequence tagging problem, and build on recent progress in another information extraction task: Named Entity Recognition (NER) (Lample et al., 2016; Peng and Dredze, 2015). Like named entities, keyphrases can be identified by their linguistic context, e.g. researchers ”use” methods. In addition, keyphrases can be associated with different categories in different contexts. For example, ‘semantic parsing’ can be labeled as a TASK in one article and as a PROCESS in another. Scientific keyphrases differ in that they can include both noun phrases and verb phrases and in that non-standard “words” (equations, chemical compounds, references) can provide important cues. Since the scale of the data is still small for supervised training of neural"
D17-1279,P16-1101,0,0.0122727,"earning to improve the performance of a supervised neural approach. Instead, we introduce a semi-supervised neural tagging approach that leverages unlabeled data. Neural tagging models have been recently introduced to tagging problems such as NER. For example, Collobert et al. (2011) use a CNN over a sequence of word embeddings and apply a CRF layer on top. Huang et al. (2015) use hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchh"
D17-1279,P14-5010,0,0.0251778,"each domain. Two special tokens BOS and EOS are added when pre-training, indicating the begin and end of a sentence. The number of the graph vertices is 2M in tranductive setting and 1.4M in inductive setting. The ULM parameter η in Eq. 4 is tuned from 0.1 to 0.9, the best η is 0.4. The best parameters of label propagation are µ = 10−6 and ν = 10−5 . The interpolation parameter α in Eq. 3 is tuned from 0.1 to 0.9, the best α is 0.3. We do iteration of semi-supervised learning until we obtain the best result on the dev set, which is mostly achieved in the second round. We use Stanford CoreNLP (Manning et al., 2014) tokenizer to tokenize words. The tokenizer is augmented with a few hand-designed rules to handle equations (e.g. “fs(B,t)=Spel(t)S” is a single token) and other non-standard word phenomena (Cu40Zn, 20MW/m2) in scientific literature. We use Approximate Nearest Neighbor Searching (ANN)4 to calculate the k-nearest neighbors. For all experiments in this paper, k = 10. Setup We evaluate our system in both inductive and transductive settings. The systems with a ∗ superscript in the table are transductive. The inductive setting uses 400 full articles in ScienceIE training and dev sets, while the tra"
D17-1279,D15-1064,0,0.0316089,"ides a dataset consisting of 500 1 SemEval (Task 10)https://scienceie.github. io/index.html scientific paragraphs with keyphrase annotations for three categories: TASK, P ROCESS, M ATERIAL across three scientific domains, Computer Science (CS), Material Science (MS), and Physics (Phy), as in Figure 1. This dataset enables the use of more advanced approaches such as neural network (NN) models. To that end, we cast the keyphrase extraction task as a sequence tagging problem, and build on recent progress in another information extraction task: Named Entity Recognition (NER) (Lample et al., 2016; Peng and Dredze, 2015). Like named entities, keyphrases can be identified by their linguistic context, e.g. researchers ”use” methods. In addition, keyphrases can be associated with different categories in different contexts. For example, ‘semantic parsing’ can be labeled as a TASK in one article and as a PROCESS in another. Scientific keyphrases differ in that they can include both noun phrases and verb phrases and in that non-standard “words” (equations, chemical compounds, references) can provide important cues. Since the scale of the data is still small for supervised training of neural systems, we introduce se"
D17-1279,D14-1162,0,0.0809487,"hand-crafted features with LSTMs to improve performance. There is currently great interest in using characterbased embeddings in neural models. (Chiu and Nichols, 2016; Lample et al., 2016; Ballesteros et al., 2015; Ma and Hovy, 2016). Our approach also takes advantage of neural tagging models and character-based embeddings for IE in scientific articles. Previous work on semi-supervised learning for neural models has mainly focused on transfer learning (Dai and Le, 2015; Luan et al., 2014a; Harsham et al., 2015) or initializing the model with pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Luan et al., 2016b, 2015, 2016a). In our work, we use pre-training but also use more powerful methods including graph-based semisupervision (Subramanya and Bilmes, 2011; Liu and Kirchhoff, 2013, 2015, 2016a,b) and a method for leveraging partially labeled data (Kim et al., 2015). We show that the combination of these techniques gives better results than any one alone. 3 Problem Definition and Data The purpose of this work is to extract phrases that can answer questions that researchers usually face when reading a paper: What TASK has the paper addressed? What P ROCES"
D17-1279,W12-3203,0,0.159301,"ucing a semi-supervised learning algorithm that uses graph-based label propagation and confidence-aware data selection, iii) exploring different alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the"
D17-1279,D10-1017,0,0.0224238,"Missing"
D17-1279,W12-3204,0,0.0712801,"rent alternatives for taking advantage of large, multi-domain unannotated data including both unsupervised embedding initialization and semi-supervised model training. 2 Related Work There has been growing interest in research on automatic methods to help researchers search and extract useful information from scientific literature. Past research has addressed citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012; Luan et al., 2012, 2014b; Levow et al., 2014). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Gupta and Manning (2011) first proposed a task that defines scientific terms for 474 abstracts from the ACL anthologhy (Bird et al., 2008) into three aspects: domain, technique and focus and apply templatebased bootstrapping to tackle the problem. Based on this study, Tsai et al. (2013) improve the performance by introducing hand-designed features from NER (Collins and Singer, 1999)"
D17-1279,L16-1294,0,\N,Missing
D18-1052,P17-1171,0,0.0574574,"tribution of the answer given the question and the document. In existing literature (Section 2), the distribution is mainly featurized by Pr(a|Q, D) ∝ exp(Fθ (Q, D, a)) where Fθ could be any realvalued scoring function parameterized by θ. Once θ is learned, the prediction a ˆ is obtained by Related Work Reading comprehension. Massive reading comprehension question answering datasets (Hermann et al., 2015; Hill et al., 2016; Dhingra et al., 2017; Dunn et al., 2017) have driven a large number of successful neural approaches (Kadlec et al., 2016; Hu et al., 2017, inter alia). Choi et al. (2017); Chen et al. (2017); Clark and Gardner (2017); Min et al. (2018) tackled large-scale QA by using a fast, coarse model (e.g. TF-IDF) to retrieve few documents or sentences and then using a slower, accurate model to obtain the answer. Salant and Berant (2018) proposed to minimize (but not prohibit) the influence of question when modeling the document. Similarly to ours, Lee et al. (2016) proposed to explicitly learn the representation for each answer candidate (phrase) in the document, but it was conditioned (dependent) on the question. Sentence retrieval. A closely related task to ours is that of retrieving a sen"
D18-1052,P17-1020,0,0.0180082,"he probabilistic distribution of the answer given the question and the document. In existing literature (Section 2), the distribution is mainly featurized by Pr(a|Q, D) ∝ exp(Fθ (Q, D, a)) where Fθ could be any realvalued scoring function parameterized by θ. Once θ is learned, the prediction a ˆ is obtained by Related Work Reading comprehension. Massive reading comprehension question answering datasets (Hermann et al., 2015; Hill et al., 2016; Dhingra et al., 2017; Dunn et al., 2017) have driven a large number of successful neural approaches (Kadlec et al., 2016; Hu et al., 2017, inter alia). Choi et al. (2017); Chen et al. (2017); Clark and Gardner (2017); Min et al. (2018) tackled large-scale QA by using a fast, coarse model (e.g. TF-IDF) to retrieve few documents or sentences and then using a slower, accurate model to obtain the answer. Salant and Berant (2018) proposed to minimize (but not prohibit) the influence of question when modeling the document. Similarly to ours, Lee et al. (2016) proposed to explicitly learn the representation for each answer candidate (phrase) in the document, but it was conditioned (dependent) on the question. Sentence retrieval. A closely related task to ours is that"
D18-1052,D17-1070,0,0.0209116,"uestion (Tay et al., 2017). A comprehensive survey for neural approaches in information retrieval literature is discussed in Mitra and Craswell (2017). We note that our problem is focused on phrasal answer extraction, which presents a unique challenge over sentence retrieval—the need for context-based representation as opposed to the content-based representation in the sentence-retrieval literature. Language representation. Recently there has been a growing interest in developing natural language representations that can be transferred across tasks (Vendrov et al., 2016; Wieting et al., 2016; Conneau et al., 2017, inter alia). In particular, SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017) encourage architectures that first encode the hypothesis and the premise independently before a comparator neu1 Phrase-Indexed Question Answering a ˆ = argmax Fθ (Q, D, a). (1) a So far, most competitive designs of Fθ (Q, D, a) make use of attention connections between the words in Q and D. As a result, these models cannot yield a query independent representation of the document D. It is subsequently not possible to independently assess the document understanding capability of the model. Furthermore,"
D18-1052,P82-1020,0,0.74185,"Missing"
D18-1052,P17-1147,0,0.060812,"requires one to index each phrase in documents by its context. We formally define the PIQA problem and provide baseline models for the new task. Our experiments show that the constraint introduced Introduction Extractive question answering (QA) is the task of selecting an answer phrase (span) to a question given an evidence document. Due to the easiness of evaluation (compared to generative QA) and the fine-grainess of the answer (compared to sentence-level QA), it has become one of the most popular QA tasks, driven by massive new datasets such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017). Current QA models heavily rely on explicitly learning the interaction between the evidence document and the question using neural attention mechanisms (Wang and Jiang, 2017; Xiong et al., 2017; Seo et al., 2017; Lee et al., 2016, inter alia), in which the model is fully aware of the question before or as it reads the document. As a result, despite significant advances, they have not led to the standalone representation of document discourse which is never∗ Most work done during internship with Google AI. 559 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processi"
D18-1052,P16-1086,0,0.0298567,"ocument. The task is often formulated as learning the probabilistic distribution of the answer given the question and the document. In existing literature (Section 2), the distribution is mainly featurized by Pr(a|Q, D) ∝ exp(Fθ (Q, D, a)) where Fθ could be any realvalued scoring function parameterized by θ. Once θ is learned, the prediction a ˆ is obtained by Related Work Reading comprehension. Massive reading comprehension question answering datasets (Hermann et al., 2015; Hill et al., 2016; Dhingra et al., 2017; Dunn et al., 2017) have driven a large number of successful neural approaches (Kadlec et al., 2016; Hu et al., 2017, inter alia). Choi et al. (2017); Chen et al. (2017); Clark and Gardner (2017); Min et al. (2018) tackled large-scale QA by using a fast, coarse model (e.g. TF-IDF) to retrieve few documents or sentences and then using a slower, accurate model to obtain the answer. Salant and Berant (2018) proposed to minimize (but not prohibit) the influence of question when modeling the document. Similarly to ours, Lee et al. (2016) proposed to explicitly learn the representation for each answer candidate (phrase) in the document, but it was conditioned (dependent) on the question. Sentence"
D18-1052,D14-1181,0,0.00311248,"ctor for the answer’s end. Thus, Equation 2’s SA Gθ (Q) = [qSA s , qe ] where the subscripts s (start) and e (end) imply that different sets of parameters were used. Now we define several baselines. LSTM baseline. An answer candidate a = (s, e) is represented using the LSTM outputs at its endpoints: from Equation 2, Hθ (D, (s, e)) = SA 4k [ds , de ] ∈ R4k and Gθ (Q) = [qSA s , qe ] ∈ R . Baseline Models We introduce several baselines for PIQA that are motivated by related literature. For all (neural) baselines, we represent the words in D and Q with one of three embedding mechanisms: CharCNN (Kim, 2014) + GloVe (Pennington et al., 2014), and ELMo (Peters et al., 2018). We follow the majority of the related literature and apply bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) to these embeddings to build the context-aware representations of the document D = {d1 . . . dm } and question Q = {q1 . . . qn }, where the forward & backward LSTM outputs are concatenated to get a single word representation, i.e. di , qi ∈ R2k LSTM+SA baseline. The LSTM outputs are augmented with the endpoint representations that come out of the document’s self-attention (SA): SA 8k and Hθ (D, (s, e)) = [ds , dSA"
D18-1052,P18-1160,0,0.0253816,"the document. In existing literature (Section 2), the distribution is mainly featurized by Pr(a|Q, D) ∝ exp(Fθ (Q, D, a)) where Fθ could be any realvalued scoring function parameterized by θ. Once θ is learned, the prediction a ˆ is obtained by Related Work Reading comprehension. Massive reading comprehension question answering datasets (Hermann et al., 2015; Hill et al., 2016; Dhingra et al., 2017; Dunn et al., 2017) have driven a large number of successful neural approaches (Kadlec et al., 2016; Hu et al., 2017, inter alia). Choi et al. (2017); Chen et al. (2017); Clark and Gardner (2017); Min et al. (2018) tackled large-scale QA by using a fast, coarse model (e.g. TF-IDF) to retrieve few documents or sentences and then using a slower, accurate model to obtain the answer. Salant and Berant (2018) proposed to minimize (but not prohibit) the influence of question when modeling the document. Similarly to ours, Lee et al. (2016) proposed to explicitly learn the representation for each answer candidate (phrase) in the document, but it was conditioned (dependent) on the question. Sentence retrieval. A closely related task to ours is that of retrieving a sentence/paragraph in a corpus that answers the"
D18-1052,D14-1162,0,0.0851472,"s end. Thus, Equation 2’s SA Gθ (Q) = [qSA s , qe ] where the subscripts s (start) and e (end) imply that different sets of parameters were used. Now we define several baselines. LSTM baseline. An answer candidate a = (s, e) is represented using the LSTM outputs at its endpoints: from Equation 2, Hθ (D, (s, e)) = SA 4k [ds , de ] ∈ R4k and Gθ (Q) = [qSA s , qe ] ∈ R . Baseline Models We introduce several baselines for PIQA that are motivated by related literature. For all (neural) baselines, we represent the words in D and Q with one of three embedding mechanisms: CharCNN (Kim, 2014) + GloVe (Pennington et al., 2014), and ELMo (Peters et al., 2018). We follow the majority of the related literature and apply bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) to these embeddings to build the context-aware representations of the document D = {d1 . . . dm } and question Q = {q1 . . . qn }, where the forward & backward LSTM outputs are concatenated to get a single word representation, i.e. di , qi ∈ R2k LSTM+SA baseline. The LSTM outputs are augmented with the endpoint representations that come out of the document’s self-attention (SA): SA 8k and Hθ (D, (s, e)) = [ds , dSA s , de , de ] ∈ R SA SA SA SA 8k"
D18-1052,N18-1202,0,0.0190435,"= [qSA s , qe ] where the subscripts s (start) and e (end) imply that different sets of parameters were used. Now we define several baselines. LSTM baseline. An answer candidate a = (s, e) is represented using the LSTM outputs at its endpoints: from Equation 2, Hθ (D, (s, e)) = SA 4k [ds , de ] ∈ R4k and Gθ (Q) = [qSA s , qe ] ∈ R . Baseline Models We introduce several baselines for PIQA that are motivated by related literature. For all (neural) baselines, we represent the words in D and Q with one of three embedding mechanisms: CharCNN (Kim, 2014) + GloVe (Pennington et al., 2014), and ELMo (Peters et al., 2018). We follow the majority of the related literature and apply bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) to these embeddings to build the context-aware representations of the document D = {d1 . . . dm } and question Q = {q1 . . . qn }, where the forward & backward LSTM outputs are concatenated to get a single word representation, i.e. di , qi ∈ R2k LSTM+SA baseline. The LSTM outputs are augmented with the endpoint representations that come out of the document’s self-attention (SA): SA 8k and Hθ (D, (s, e)) = [ds , dSA s , de , de ] ∈ R SA SA SA SA 8k Gθ (Q) = [qs1 , qs2 , qe1 , qe2"
D18-1052,N18-2088,0,0.0116634,"by θ. Once θ is learned, the prediction a ˆ is obtained by Related Work Reading comprehension. Massive reading comprehension question answering datasets (Hermann et al., 2015; Hill et al., 2016; Dhingra et al., 2017; Dunn et al., 2017) have driven a large number of successful neural approaches (Kadlec et al., 2016; Hu et al., 2017, inter alia). Choi et al. (2017); Chen et al. (2017); Clark and Gardner (2017); Min et al. (2018) tackled large-scale QA by using a fast, coarse model (e.g. TF-IDF) to retrieve few documents or sentences and then using a slower, accurate model to obtain the answer. Salant and Berant (2018) proposed to minimize (but not prohibit) the influence of question when modeling the document. Similarly to ours, Lee et al. (2016) proposed to explicitly learn the representation for each answer candidate (phrase) in the document, but it was conditioned (dependent) on the question. Sentence retrieval. A closely related task to ours is that of retrieving a sentence/paragraph in a corpus that answers the question (Tay et al., 2017). A comprehensive survey for neural approaches in information retrieval literature is discussed in Mitra and Craswell (2017). We note that our problem is focused on p"
D18-1052,D16-1264,0,\N,Missing
D18-1360,P11-1051,0,0.034294,"Missing"
D18-1360,D17-1181,0,0.0731328,"Missing"
D18-1360,N18-3011,0,0.139142,"Missing"
D18-1360,S17-2097,0,0.0608893,"2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline pr"
D18-1360,W12-3202,0,0.0187928,"oreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature. 2 Related Work There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2016). More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synony"
D18-1360,S17-2091,0,0.38619,"ny pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016,"
D18-1360,P17-2054,0,0.0351087,"litate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Si"
D18-1360,P16-1061,0,0.0307772,"ing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic rol"
D18-1360,L16-1586,0,0.0363768,"Extending a previous end-to-end coreference resolution system, we develop a multi-task learning framework that can detect scientific entities, relations, and coreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature. 2 Related Work There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2016). More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science,"
D18-1360,I11-1001,0,0.608513,"that refer to the same scientific concept, including generic terms (such as the pronoun it, or phrases like our method) that are not informative by themselves. With co-reference, context-free grammar can be connected to MORPA through the intermediate co-referred pronoun it. Applying existing IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base). In this paper, we develop a unified learning model for extracting scientific entities, relations, and coreference resolution. This is different from previous work (Luan et al., 2017b; Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and coreference resolution (Lee"
D18-1360,P18-2058,1,0.910122,"2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and coreference resolution (Lee et al., 2017; He et al., 2018). Different from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans. It avoids cascading errors between tasks by jointly modeling all spans and span-span relations. To explore this problem, we create a dataset S CI ERC for scientific information extraction, which includes annotations of scientific terms, relation categories and co-reference links. Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relatio"
D18-1360,P17-1085,0,0.0391968,"et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and core"
D18-1360,N16-1179,0,0.0276611,"anning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016, 2017a; Rei, 2017), while Peng et al. (2017) uses high-order cross-task factors. Our model instead propagates 3220 cross-task information via span representations, which is related to Swayamdipta et al. (2017). 3 Statistics #Entities #Relations #Relations/Doc #Coref links #Coref clusters Dataset Our dataset (called S CI ERC) includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts. These abstracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Semantic Scholar Corpus2 . S CI ER"
D18-1360,N16-1030,0,0.246994,"systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-t"
D18-1360,D17-1018,1,0.224528,"011; Tsai et al., 2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and coreference resolution (Lee et al., 2017; He et al., 2018). Different from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans. It avoids cascading errors between tasks by jointly modeling all spans and span-span relations. To explore this problem, we create a dataset S CI ERC for scientific information extraction, which includes annotations of scientific terms, relation categories and co-reference links. Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on"
D18-1360,N18-2108,1,0.89945,"Missing"
D18-1360,I17-1061,1,0.790259,"onnected by entities that refer to the same scientific concept, including generic terms (such as the pronoun it, or phrases like our method) that are not informative by themselves. With co-reference, context-free grammar can be connected to MORPA through the intermediate co-referred pronoun it. Applying existing IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base). In this paper, we develop a unified learning model for extracting scientific entities, relations, and coreference resolution. This is different from previous work (Luan et al., 2017b; Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and c"
D18-1360,P16-2020,1,0.809336,"nd Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016, 2017a; Rei, 2017), while Peng et al. (2017) uses high-order cross-task factors. Our model instead propagates 3220 cross-task information via span representations, which is related to Swayamdipta et al. (2017). 3 Statistics #Entities #Relations #Relations/Doc #Coref links #Coref clusters Dataset Our dataset (called S CI ERC) includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts. These abstracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Semantic Scholar Corpus2 . S CI ERC extends previous"
D18-1360,D17-1279,1,0.790305,"onnected by entities that refer to the same scientific concept, including generic terms (such as the pronoun it, or phrases like our method) that are not informative by themselves. With co-reference, context-free grammar can be connected to MORPA through the intermediate co-referred pronoun it. Applying existing IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base). In this paper, we develop a unified learning model for extracting scientific entities, relations, and coreference resolution. This is different from previous work (Luan et al., 2017b; Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2018) which often addresses these tasks as independent 3219 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics components of a pipeline. Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links. Specifically, we extend prior work for learning span representations and c"
D18-1360,S18-1125,1,0.840133,"r et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Z"
D18-1360,P16-1105,0,0.311677,"nd Materials and two relation types: hyponym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extract"
D18-1360,Q17-1008,0,0.197674,"onym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang"
D18-1360,P17-1161,0,0.031299,"on Triples Figure 4: A part of an automatically constructed Experimental Setup We evaluate our unified framework S CI IE on S CI ERC and SemEval 17. The knowledge graph for 3224 • LSTM+CRF The state-of-the-art NER system (Lample et al., 2016), which applies CRF on top of LSTM for named entity tagging, the approach has also been used in scientific term extraction (Luan et al., 2017b). • LSTM+CRF+ELMo LSTM+CRF ELM O as an additional input feature. with • E2E Rel State-of-the-art joint entity and relation extraction system (Miwa and Bansal, 2016) that has also been used in scientific literature (Peters et al., 2017; Augenstein et al., 2017). This system uses syntactic features such as part-of-speech tagging and dependency parsing. • E2E Rel(Pipeline) Pipeline setting of E2E Rel. Extract entities first and use entity results as input to relation extraction task. Dev Model LSTM+CRF LSTM+CRF+ELMo E2E Rel(Pipeline) E2E Rel E2E Rel+ELM O S CI IE • E2E Rel+ELMo E2E Rel with ELM O as an additional input feature. • E2E Coref State-of-the-art coreference system Lee et al. (2017) combined with ELM O. Our system S CI IE extends E2E Coref with multi-task learning. Test P R F1 P R F1 67.2 68.1 66.7 64.3 67.5 70.0 65"
D18-1360,N18-1202,0,0.0333363,"es to compute the different Φ: ΦE (e, si ) = φe (si ) (4) ΦR (r, si , sj ) = φmr (si ) + φmr (sj ) + φr (si , sj ) ΦC (si , sj ) = φmc (si ) + φmc (sj ) + φc (si , sj ) The scores in Equation (4) are defined for entity types, relations, and antecedents that are not the null-type . Scores involving the null label are set to a constant 0: ΦE (, si ) = ΦR (, si , sj ) = ΦC (si , ) = 0. We use the same span representations g from (Lee et al., 2017) and share them across the three tasks. We start by building bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) from word, character and ELMo (Peters et al., 2018) embeddings. For a span si , its vector representation gi is constructed by concatenating si ’s left and right end points from the BiLSTM outputs, an attentionbased soft “headword,” and embedded span width features. Hyperparameters and other implementation details will be described in Section 6. 4.4 Inference and Pruning Following previous work, we use beam pruning to reduce the number of pairwise span factors from O(n4 ) to O(n2 ) at both training and test time, where n is the number of words in the document. We define two separate beams: BC to prune spans for the coreference resolution task,"
D18-1360,L16-1294,0,0.0979017,"Missing"
D18-1360,E17-1110,0,0.0299337,"-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel an"
D18-1360,J86-2003,0,0.048649,"Missing"
D18-1360,N16-1114,0,0.0782222,"Missing"
D18-1360,C16-1138,0,0.0255081,"lation types: hyponym-of and synonym-of. SemEval 18 (G´abor et al., 2018) is focused on predicting relations between entities within a sentence. It consists of six relation types. Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information. We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints. Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and"
D18-1360,Q15-1009,0,0.0489779,"Missing"
D18-1360,D17-1182,0,0.0830998,"Missing"
D18-1360,P17-1113,0,0.0653047,"8; Adel and Sch¨utze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans. While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Sch¨utze, 2017; Zheng et al., 2017). Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Sch¨utze (2017) and Peng et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of"
D18-1360,P17-1194,0,0.0213066,"g et al. (2017). Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans. Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference. Neural multi-task learning has been applied to a range of NLP tasks. Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016, 2017a; Rei, 2017), while Peng et al. (2017) uses high-order cross-task factors. Our model instead propagates 3220 cross-task information via span representations, which is related to Swayamdipta et al. (2017). 3 Statistics #Entities #Relations #Relations/Doc #Coref links #Coref clusters Dataset Our dataset (called S CI ERC) includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts. These abstracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Semantic Scholar Corpus2 . S CI ERC extends previous datasets in scienti"
D18-1360,W12-3203,0,0.0547651,"end-to-end coreference resolution system, we develop a multi-task learning framework that can detect scientific entities, relations, and coreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature. 2 Related Work There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2016). More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and mater"
D18-1360,W16-1300,0,0.129169,"Missing"
D18-1360,E12-2021,0,0.0929946,"Missing"
D18-1360,W12-3204,0,0.0604475,"entities, relations, and coreference clusters without hand-engineered features. We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature. 2 Related Work There has been growing interest in research on automatic methods for information extraction from scientific articles. Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; G´abor et al., 2016). More recently, two datasets in SemEval 2017 and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction. SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science. It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation type"
D18-1360,W12-4303,0,\N,Missing
D18-1491,W17-5221,0,0.0491476,"Missing"
D18-1491,N16-1082,0,0.0310723,"ount for coarse-grained group similarities, allowing for finer individual word distinctions in the embedding layer. This hypothesis is strengthened by the entropy results described above: a model which can make finer distinctions between individual words can more confidently assign probability mass. A model that cannot make these distinctions, such as the LSTM, must spread its probability mass across a larger class of similar words. Gradient-based analysis: Saliency analysis using gradients help identify relevant words in a test sequence that contribute to the prediction (Gevrey et al., 2003; Li et al., 2016; Arras et al., 2017). These approaches compute the relevance as the squared norm of the gradients obtained through back-propagation. Table 2a visualizes the heatmaps for different sequences. PRUs, in general, give more relevance to contextual words than LSTMs, such as southeast (sample 1), cost (sample 2), face (sample 4), and introduced (sample 5), which help in making more confident decisions. Furthermore, when gradients during backpropagation are visualized (Selvaraju et al., 2017) (Table 2b), we find that PRUs have better gradient coverage than LSTMs, suggesting PRUs use more features tha"
D18-1491,J93-2004,0,0.0610373,"where ⊗ represents the element-wise multiplication operation, and σ and tanh are the sigmoid and hyperbolic tangent activation functions. We note that LSTM is a special case of PRU when g=K=1. 4 Experiments To showcase the effectiveness of the PRU, we evaluate the performance on two standard datasets for word-level language modeling and compare with state-of-the-art methods. Additionally, we provide a detailed examination of the PRU and its behavior on the language modeling tasks. 4623 4.1 Set-up Dataset: Following recent works, we compare on two widely used datasets, the Penn Treebank (PTB) (Marcus et al., 1993) as prepared by Mikolov et al. (2010) and WikiText2 (WT-2) (Merity et al., 2017). For both datasets, we follow the same training, validation, and test splits as in Merity et al. (2018). Language Model: We extend the language model, AWD-LSTM (Merity et al., 2018), by replacing LSTM layers with PRU. Our model uses 3-layers of PRU with an embedding size of 400. The number of parameters learned by state-of-theart methods vary from 18M to 66M with majority of the methods learning about 22M to 24M parameters on the PTB dataset. For a fair comparison with state-of-the-art methods, we fix the model si"
D18-1491,P17-1172,0,0.0959835,"s (Melis et al., 2018). We inherit ideas from convolution-based approaches, such as sub-sampling, to learn richer representations (Krizhevsky et al., 2012; Han et al., 2017). 4621 Regularization: Methods such as dropout (Srivastava et al., 2014), variational dropout (Kingma et al., 2015), and weight dropout (Merity et al., 2018) have been proposed to regularize RNNs. These methods can be easily applied to PRUs. Other efficient RNN networks: Recently, there has been an effort to improve the efficiency of RNNs. These approaches include quantization (Xu et al., 2018), skimming (Seo et al., 2018; Yu et al., 2017), skipping (Campos et al., 2018), and query reduction (Seo et al., 2017). These approaches extend standard RNNs and therefore, these approaches are complementary to our work. Language modeling: Language modeling is a fundamental task for NLP and has garnered significant attention in recent years (see Table 1 for comparison with state-of-the-art methods). Merity et al. (2018) introduce regularization techniques such as weight dropping which, coupled with a non-monotonically triggered ASGD optimization, achieves strong performance improvements. Yang et al. (2018) extend Merity et al. (2018) with"
D19-1284,P17-1147,1,0.890082,"bers in the reference text. Such weak supervision is attractive because it is relatively easy to gather, allowing for large datasets, but complicates learning because there are many different spurious ways to derive the correct answer. It is natural to 1 Our code is publicly available at https://github. com/shmsw25/qa-hard-em. model such ambiguities with a latent variable during learning, but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision (e.g. by selecting the first answer span in T RIVIAQA (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019)). Some models are trained with maximum marginal likelihood (MML) (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019), but it is unclear if it gives a meaningful improvement over the heuristics. In this paper, we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent-variable learning problems. First, we define a solution to be a particular derivation of a model to predict the answer (e.g. a span in 2851 Proceedings of the 2019 Conference on Empirical Methods in Natural La"
D19-1284,P16-1086,0,0.176959,"ets, but complicates learning because there are many different spurious ways to derive the correct answer. It is natural to 1 Our code is publicly available at https://github. com/shmsw25/qa-hard-em. model such ambiguities with a latent variable during learning, but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision (e.g. by selecting the first answer span in T RIVIAQA (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019)). Some models are trained with maximum marginal likelihood (MML) (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019), but it is unclear if it gives a meaningful improvement over the heuristics. In this paper, we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent-variable learning problems. First, we define a solution to be a particular derivation of a model to predict the answer (e.g. a span in 2851 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2851–2864, c Hong Kong, China,"
D19-1284,Q18-1023,0,0.115292,"Missing"
D19-1284,D17-1160,0,0.0174131,"Parsing. Latent-variable learning has been extensively studied in the literature of semantic parsing (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; Liang et al., 2017, 2018). Since it is computationally expensive to obtain a precomputed set in semantic parsing, these methods typically recompute the set of logical forms with respect to the beam at every parameter update. In contrast, our learning method targets tasks that a set of solutions can be precomputed, which include many recent QA tasks such as reading comprehension, open-domain QA and a recent SQL-based semantic parsing task (Zhong et al."
D19-1284,P17-1171,1,0.854607,"lin et al., 2019) for multiparagraph reading comprehension (Min et al., 2019). Training details. We use uncased version of BERTbase . For all datasets, we split documents into a set of segments up to 300 tokens because BERT limits the size of the input. We use batch 5 size of 20 for two reading comprehension tasks and 192 for two open-domain QA tasks. Following Clark and Gardner (2018), we filter a subset of segments in T RIVIAQA through TFIDF similarity between a segment and a question to maintain a reasonable length. For opendomain QA tasks, we retrieve 50 Wikipedia articles through TF-IDF (Chen et al., 2017) and further run BM25 (Robertson et al., 2009) to retrieve 20 (for train) or 80 (for development and test) paragraphs. We try 10, 20, 40 and 80 paragraphs on the development set to choose the number of paragraphs to use on the test set. To avoid local optima, we perform annealing: at training step t, the model optimizes on MML objective with a probability of min(t/τ , 1) and otherwise use our objective, where τ is a hyperparameter. We observe that the performance is improved by annealing while not being overly sensitive to the hyperparameter τ . We include full hyperparameters and detailed abl"
D19-1284,Q19-1026,0,0.43565,"Missing"
D19-1284,P18-1078,0,0.532296,"many different spurious ways to derive the correct answer. It is natural to 1 Our code is publicly available at https://github. com/shmsw25/qa-hard-em. model such ambiguities with a latent variable during learning, but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision (e.g. by selecting the first answer span in T RIVIAQA (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019)). Some models are trained with maximum marginal likelihood (MML) (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019), but it is unclear if it gives a meaningful improvement over the heuristics. In this paper, we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent-variable learning problems. First, we define a solution to be a particular derivation of a model to predict the answer (e.g. a span in 2851 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2851–2864, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computatio"
D19-1284,P19-1612,0,0.739327,"ways to derive the correct answer. It is natural to 1 Our code is publicly available at https://github. com/shmsw25/qa-hard-em. model such ambiguities with a latent variable during learning, but most prior work on reading comprehension has rather focused on the model architecture and used heuristics to map the weak signal to full supervision (e.g. by selecting the first answer span in T RIVIAQA (Joshi et al., 2017; Tay et al., 2018; Talmor and Berant, 2019)). Some models are trained with maximum marginal likelihood (MML) (Kadlec et al., 2016; Swayamdipta et al., 2018; Clark and Gardner, 2018; Lee et al., 2019), but it is unclear if it gives a meaningful improvement over the heuristics. In this paper, we show it is possible to formulate a wide range of weakly supervised QA tasks as discrete latent-variable learning problems. First, we define a solution to be a particular derivation of a model to predict the answer (e.g. a span in 2851 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2851–2864, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Tas"
D19-1284,W10-2903,0,0.0201778,"highlight the learning challenge and show that our learning method, independent of the model architecture, can give a significant gain. Specifically, we assume that one of 2852 mentions are related to the question and others are false positives because (i) this happens for most cases, as the first example in Table 2, and (ii) even in the case where multiple mentions contribute to the answer, there is often a single span which fits the question the best. Semantic Parsing. Latent-variable learning has been extensively studied in the literature of semantic parsing (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (I"
D19-1284,P17-1003,0,0.0312862,"3; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; Liang et al., 2017, 2018). Since it is computationally expensive to obtain a precomputed set in semantic parsing, these methods typically recompute the set of logical forms with respect to the beam at every parameter update. In contrast, our learning method targets tasks that a set of solutions can be precomputed, which include many recent QA tasks such as reading comprehension, open-domain QA and a recent SQL-based semantic parsing task (Zhong et al., 2017). 3 Method In this section, we first formally define our general setup, which we will instantiate for specific tasks in Section 4 and then we describe our l"
D19-1284,D13-1160,0,0.213043,"that our learning method, independent of the model architecture, can give a significant gain. Specifically, we assume that one of 2852 mentions are related to the question and others are false positives because (i) this happens for most cases, as the first example in Table 2, and (ii) even in the case where multiple mentions contribute to the answer, there is often a single span which fits the question the best. Semantic Parsing. Latent-variable learning has been extensively studied in the literature of semantic parsing (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; Liang et al., 2017, 20"
D19-1284,N19-1423,0,0.0764522,"hat answers are likely to appear earlier in the paragraph. Second, while MML achieves comparable result to the First-Only baseline, our learning method outperforms others by 2+ F1/ROUGE-L/EM consistently on all datasets. Lastly, our method achieves the new state-ofthe-art on NARRATIVE QA, T RIVIAQA- OPEN and NATURAL Q UESTIONS - OPEN, and is comparable to the state-of-the-art on T RIVIAQA, despite our aggressive truncation of documents. 5.2 P(zi |Q, D) can be obtained by any model which outputs the start and end positions of the input document. In this work, we use a modified version of BERT (Devlin et al., 2019) for multiparagraph reading comprehension (Min et al., 2019). Training details. We use uncased version of BERTbase . For all datasets, we split documents into a set of segments up to 300 tokens because BERT limits the size of the input. We use batch 5 size of 20 for two reading comprehension tasks and 192 for two open-domain QA tasks. Following Clark and Gardner (2018), we filter a subset of segments in T RIVIAQA through TFIDF similarity between a segment and a question to maintain a reasonable length. For opendomain QA tasks, we retrieve 50 Wikipedia articles through TF-IDF (Chen et al., 2017"
D19-1284,P18-1068,0,0.021492,"Missing"
D19-1284,N19-1246,0,0.16032,"on Natural Language Processing, pages 2851–2864, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Task & Dataset Train # Examples Dev Test Avg |Z| Median 1. Multi-mention reading comprehension T RIVIAQA (Joshi et al., 2017) NARRATIVE QA (Koˇcisk`y et al., 2018) T RIVIAQA- OPEN (Joshi et al., 2017) NATURAL Q UESTIONS - OPEN (Kwiatkowski et al., 2019) 61,888 32,747 78,785 79,168 7,993 3,461 8,837 8,757 7,701 10,557 11,313 3,610 2.7 4.3 6.7 1.8 2 5 4 1 46,973 5,850 - 8.2 3 56,355 8,421 15,878 346.1 5 2. Reading comprehension with discrete reasoning DROPnum (Dua et al., 2019) 3. Semantic Parsing W IKI SQL (Zhong et al., 2017) Table 1: Six QA datasets in three different categories used in this paper (detailed in Section 5) along with the size of each dataset. An average and median of the size of precomputed solution sets (denoted by Z) are also reported. Details on how to obtain Z are given in Section 4. the document or an equation to compute the answer). We demonstrate that for many recently introduced tasks, which we group into three categories as given in Table 1, it is relatively easy to precompute a discrete, task-specific set of possible solutions that contai"
D19-1284,J13-2005,0,0.0226519,"g challenge and show that our learning method, independent of the model architecture, can give a significant gain. Specifically, we assume that one of 2852 mentions are related to the question and others are false positives because (i) this happens for most cases, as the first example in Table 2, and (ii) even in the case where multiple mentions contribute to the answer, there is often a single span which fits the question the best. Semantic Parsing. Latent-variable learning has been extensively studied in the literature of semantic parsing (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; L"
D19-1284,P19-1416,1,0.817092,"nd, while MML achieves comparable result to the First-Only baseline, our learning method outperforms others by 2+ F1/ROUGE-L/EM consistently on all datasets. Lastly, our method achieves the new state-ofthe-art on NARRATIVE QA, T RIVIAQA- OPEN and NATURAL Q UESTIONS - OPEN, and is comparable to the state-of-the-art on T RIVIAQA, despite our aggressive truncation of documents. 5.2 P(zi |Q, D) can be obtained by any model which outputs the start and end positions of the input document. In this work, we use a modified version of BERT (Devlin et al., 2019) for multiparagraph reading comprehension (Min et al., 2019). Training details. We use uncased version of BERTbase . For all datasets, we split documents into a set of segments up to 300 tokens because BERT limits the size of the input. We use batch 5 size of 20 for two reading comprehension tasks and 192 for two open-domain QA tasks. Following Clark and Gardner (2018), we filter a subset of segments in T RIVIAQA through TFIDF similarity between a segment and a question to maintain a reasonable length. For opendomain QA tasks, we retrieve 50 Wikipedia articles through TF-IDF (Chen et al., 2017) and further run BM25 (Robertson et al., 2009) to retrieve"
D19-1284,P19-1220,0,0.0556534,"Missing"
D19-1284,P17-1167,0,0.0191902,"0; Liang et al., 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013). For example, a question and an answer pair (x, y) is given but the logical form that is used to compute the answer is not. Two common learning paradigms are maximum marginal likelihood (MML) and rewardbased the objective maximizes P methods. In MML, ˆ is an approximation of P(z|x), where Z ˆ z∈Z a set of logical forms executing y (Liang et al., 2013; Berant et al., 2013; Krishnamurthy et al., 2017). In reward-based methods, a reward function is defined as a prior, and the model parameters are updated with respect to it (Iyyer et al., 2017; Liang et al., 2017, 2018). Since it is computationally expensive to obtain a precomputed set in semantic parsing, these methods typically recompute the set of logical forms with respect to the beam at every parameter update. In contrast, our learning method targets tasks that a set of solutions can be precomputed, which include many recent QA tasks such as reading comprehension, open-domain QA and a recent SQL-based semantic parsing task (Zhong et al., 2017). 3 Method In this section, we first formally define our general setup, which we will instantiate for specific tasks in Section 4 and th"
D19-1284,D16-1264,0,0.333329,"Missing"
D19-1308,K16-1002,0,0.0459265,"dictor is assigned to each data point, which is also known as multiple choice learning (Guzman-Rivera et al., 2012; Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make S ELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly. Variational Autoencoders Variational Autoencoders (VAE) (Kingma and Welling, 2013) are used for diverse generation in several tasks, such as language modeling (Bowman et al., 2016), machine translation (Zhang et al., 2016; Su et al., 2018; Deng et al., 2018; Shankar and Sarawagi, 2019), and conversation modeling (Serban et al., 2017; Wen et al., 2017; Zhao et al., 2017; Park et al., 2018; Wen and Luong, 2018; Gu et al., 2019). These methods sample diverse latent variables from an approximate posterior distribution, but often suffer from a posterior collapse where the sampled latent variables are ignored (Bowman et al., 2016; Park et al., 2018; Kim et al., 2018b; 3122 (a) Ours 1) Diverse Content Selection 2) Focused Generation Selector 1 Enc Dec Selector 2 Enc Dec Select"
D19-1308,N18-1150,0,0.0456855,"Missing"
D19-1308,P18-1082,0,0.46739,"sults in diverse target sequences with different semantics. Fig. 1 shows different questions that can be generated from a given passage. Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018; Vijayakumar et al., 2018) or mixture of decoders (He et al., 2018; Shen et al., 2019). These methods promote diversity at the decoding 3121 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3121–3131, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences. In this paper, we present a method for diverse generation"
D19-1308,D18-1443,0,0.269771,"rom adding these regularization terms to our objective function. Content Selection in NLP Selecting important parts of the context has been an important step in NLP applications (Reiter and Dale, 2000). Most recently, Ke et al. (2018); Min et al. (2018) conduct soft-/ hard-selection of key parts from source passages for question answering. Zhou et al. (2017b) use soft gating on the source document encoder for abstractive summarization. Li et al. (2018) guide abstractive summarization models with off-the-shelf keyword extractors. The most relevant work to ours are Subramanian et al. (2018) and Gehrmann et al. (2018). Subramanian et al. (2018) use a pointer network (Vinyals et al., 2015) for extracting key phrases for question generation and Gehrmann et al. (2018) use content selector to limit copying probability for abstractive summarization. The main purpose of these approaches is to enhance accuracy, while our method uses diverse content selection to enhance both accuracy and diversity (refer to our empirical results). Additionally, our method allows models to learn how to utilize information from the selected content, whereas Gehrmann et al. (2018) manually limit the copying mechanism on non-selected"
D19-1308,P16-1014,0,0.0251665,"question from a passage-answer pair. Answers are given as a span in the passage (See Fig.1). Dataset We conduct experiments on SQuAD (Rajpurkar et al., 2016) and use the same dataset split of Zhou et al. (2017a), resulting in 86,635, 8,965, and 8,964 source-target pairs for training, validation, and test, respectively. Both source passages and target questions are single sentences. The average length of source passage and target question are 32 and 11 tokens. Generator We use NQG++ (Zhou et al., 2017a) as the generator, which is an RNN-based encoderdecoder architecture with copying mechanism (Gulcehre et al., 2016). 4.2 Abstractive Summarization Abstractive summarization is the task of generating a summary sentence from a source document that consists of multiple sentences. 3125 Dataset We conduct experiments on the nonanonymized version of CNN-DM dataset (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017), whose training, validation, test splits have size of 287,113, 13,368, and 11,490 source-target pairs, respectively. The average length of the source documents and target summaries are 386 and 55 tokens. Following See et al. (2017), we truncate source and target sentences to 400 and 100 t"
D19-1308,K18-1056,0,0.185313,"hows different questions that can be generated from a given passage. Encoder-decoder models (Cho et al., 2014) are widely used for sequence generation, most notably in machine translation where neural models are now often almost as good as human translators in some language pairs. However, a standard encoder-decoder often shows a poor performance when it attempts to produce multiple, diverse outputs. Most recent methods for diverse sequence generation leverage diversifying decoding steps through alternative search algorithms (Fan et al., 2018; Vijayakumar et al., 2018) or mixture of decoders (He et al., 2018; Shen et al., 2019). These methods promote diversity at the decoding 3121 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3121–3131, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics step, while a more focused selection of the source sequence can lead to diversifying the semantics of the generated target sequences. In this paper, we present a method for diverse generation that separates diversification and generation stages. The diversifi"
D19-1308,N18-2009,0,0.0198164,"lyan et al. (2018) introduce terms enforcing knowledge transfer among similar annotations. Our work is orthogonal to these methods and can potentially benefit from adding these regularization terms to our objective function. Content Selection in NLP Selecting important parts of the context has been an important step in NLP applications (Reiter and Dale, 2000). Most recently, Ke et al. (2018); Min et al. (2018) conduct soft-/ hard-selection of key parts from source passages for question answering. Zhou et al. (2017b) use soft gating on the source document encoder for abstractive summarization. Li et al. (2018) guide abstractive summarization models with off-the-shelf keyword extractors. The most relevant work to ours are Subramanian et al. (2018) and Gehrmann et al. (2018). Subramanian et al. (2018) use a pointer network (Vinyals et al., 2015) for extracting key phrases for question generation and Gehrmann et al. (2018) use content selector to limit copying probability for abstractive summarization. The main purpose of these approaches is to enhance accuracy, while our method uses diverse content selection to enhance both accuracy and diversity (refer to our empirical results). Additionally, our me"
D19-1308,N16-1014,0,0.341319,"2019) that also aims to diversify outputs by creating multiple decoders, our modular method not only demonstrates better accuracy and diversity, but also trains 3.7 times faster. 2 Related Work Diverse Search Algorithms Beam search, the most commonly used search algorithm for decoding, is known to produce samples that are short, contain repetitive phrases, and share majority of their tokens. Hence several methods are introduced to diversify search algorithms for decoding. Graves (2013); Chorowski and Jaitly (2017) tune temperature hyperparameter in softmax function. Vijayakumar et al. (2018); Li et al. (2016b) penalize similar samples during beam search in order to obtain diverse set of samples. Cho (2016) adds random noise to RNN decoder hidden states. Fan et al. (2018) sample tokens from top-k tokens at each decoding step. Our method is orthogonal to these search-based strategies, in that they diversify decoding while our method diversifies which content to be focused during encoding. Moreover, our empirical results show diversification with stochastic sampling hurts accuracy significantly. Deep Mixture of Experts Several methods adopt a deep mixture of experts (MoE) (Jacobs et al., 1991; Eigen"
D19-1308,W04-1013,0,0.0416157,"l greedy decoding. All decoders share all parameters but use different embeddings for start-of-sequence token. Mixture Selector (Ours) We construct a hardMoE of K S ELECTORs with uniform mixing coefficient that infers K different focus from source sequence. Guided by K focus, generator conducts parallel greedy decoding. 4.4 Metrics: Accuracy and Diversity We use metrics introduced by previous works (Ott et al., 2018; Vijayakumar et al., 2018; Zhu et al., 2018) to evaluate the diversity promoting approaches. These metrics are extensions over BLEU-4 (Papineni et al., 2002) and ROUGE-2 F1 score (Lin, 2004) and aim to evaluate the trade-off between accuracy and diversity. Top-1 metric (⇑) This measures the Top-1 accuracy among the generated K-best hypotheses. The accuracy is measured using a corpus-level metric, i.e., BLEU-4 or ROUGE-2. Oracle metric (⇑) This measures the quality of the target distribution coverage among the Top-K generated target sequences (Ott et al., 2018; Vijayakumar et al., 2018). Given an optimal ranking method (oracle), this metric measures the upper bound of Top-1 accuracy by comparing the best hypothesis with the target. Concretely, we genˆ K } from each source erate hy"
D19-1308,P18-1160,0,0.0148452,"regularization to objective functions is used to diversify generation. Li et al. (2016a) introduce a term maximizing mutual information between source and target sentences. Chorowski and Jaitly (2017); Kalyan et al. (2018) introduce terms enforcing knowledge transfer among similar annotations. Our work is orthogonal to these methods and can potentially benefit from adding these regularization terms to our objective function. Content Selection in NLP Selecting important parts of the context has been an important step in NLP applications (Reiter and Dale, 2000). Most recently, Ke et al. (2018); Min et al. (2018) conduct soft-/ hard-selection of key parts from source passages for question answering. Zhou et al. (2017b) use soft gating on the source document encoder for abstractive summarization. Li et al. (2018) guide abstractive summarization models with off-the-shelf keyword extractors. The most relevant work to ours are Subramanian et al. (2018) and Gehrmann et al. (2018). Subramanian et al. (2018) use a pointer network (Vinyals et al., 2015) for extracting key phrases for question generation and Gehrmann et al. (2018) use content selector to limit copying probability for abstractive summarization."
D19-1308,P02-1040,0,0.103711,"p in Shen et al. (2019)) and conducts parallel greedy decoding. All decoders share all parameters but use different embeddings for start-of-sequence token. Mixture Selector (Ours) We construct a hardMoE of K S ELECTORs with uniform mixing coefficient that infers K different focus from source sequence. Guided by K focus, generator conducts parallel greedy decoding. 4.4 Metrics: Accuracy and Diversity We use metrics introduced by previous works (Ott et al., 2018; Vijayakumar et al., 2018; Zhu et al., 2018) to evaluate the diversity promoting approaches. These metrics are extensions over BLEU-4 (Papineni et al., 2002) and ROUGE-2 F1 score (Lin, 2004) and aim to evaluate the trade-off between accuracy and diversity. Top-1 metric (⇑) This measures the Top-1 accuracy among the generated K-best hypotheses. The accuracy is measured using a corpus-level metric, i.e., BLEU-4 or ROUGE-2. Oracle metric (⇑) This measures the quality of the target distribution coverage among the Top-K generated target sequences (Ott et al., 2018; Vijayakumar et al., 2018). Given an optimal ranking method (oracle), this metric measures the upper bound of Top-1 accuracy by comparing the best hypothesis with the target. Concretely, we g"
D19-1308,N18-1162,1,0.851792,"E to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly. Variational Autoencoders Variational Autoencoders (VAE) (Kingma and Welling, 2013) are used for diverse generation in several tasks, such as language modeling (Bowman et al., 2016), machine translation (Zhang et al., 2016; Su et al., 2018; Deng et al., 2018; Shankar and Sarawagi, 2019), and conversation modeling (Serban et al., 2017; Wen et al., 2017; Zhao et al., 2017; Park et al., 2018; Wen and Luong, 2018; Gu et al., 2019). These methods sample diverse latent variables from an approximate posterior distribution, but often suffer from a posterior collapse where the sampled latent variables are ignored (Bowman et al., 2016; Park et al., 2018; Kim et al., 2018b; 3122 (a) Ours 1) Diverse Content Selection 2) Focused Generation Selector 1 Enc Dec Selector 2 Enc Dec Selector 3 Enc Dec (c) Mixture Decoder (b) Diverse Search Dec 1 Enc Enc Dec Dec 2 Dec 3 Figure 2: Overview of diverse sequence-to-sequence generation methods. (a) refers to our two-stage approach described throughout"
D19-1308,D14-1162,0,0.0820287,"the best oracle metric. We use Adam (Kingma and Ba, 2015) optimizer with learning rate 0.001 and momentum parmeters β1 = 0.9 and β2 = 0.999. Minibatch size is 64 and 32 for question generation and abstractive summarization. All models are implemented in PyTorch (Paszke et al., 2017) and trained on single Tesla P40 GPU, based on NAVER Smart Machine Learning (NSML) platform (Kim et al., 2018a). Question Generation Following Zhou et al. (2017a), we use 256-dim hidden states for each direction of Bi-GRU encoder, 512-dim hidden states for GRU decoder, 300-dim word embedding initialized from GloVe (Pennington et al., 2014), vocabulary of 20,000 most frequent words, 16dim embeddings for three linguistic features (POS, NER and word case) respectively. Abstractive Summarization Following See et al. (2017), we use 256-dim hidden states for each direction of Bi-LSTM encoder and LSTM decoder, 128-dim word embedding trained from scratch, and vocabulary of 50,000 most frequent words. Following See et al. (2017), we train our model to generate concatenation of target summaries and split it with periods. S ELECTOR The GRU has the same size as the generator encoder, and the dimension of expert embedding ez is 300 for NQG+"
D19-1308,E17-2025,0,0.0142588,") 15.672 22.451 59.815 Focus Guide during Test Time 5-Beam + Focus Guide 24.580 - out of 3 questions / summaries with highest logprobability from each method. They are instructed to select a question / summary that is more coherent with the source passage / document. Each annotator is asked to choose either a better method (resulting in “win” or “lose”) or “tie” if their quality is indistinguishable. Diversity and accuracy evaluations are conducted separately, and every pair of methods are presented to 10 annotators1 . 4.6 Method Implementation details For all experiments, we tie the weights (Press and Wolf, 2017) of the encoder embedding, the decoder embedding, and the decoder output layers. This significantly reduces the number of parameters and training time until convergence. We train up to 20 epochs and select the checkpoint with the best oracle metric. We use Adam (Kingma and Ba, 2015) optimizer with learning rate 0.001 and momentum parmeters β1 = 0.9 and β2 = 0.999. Minibatch size is 64 and 32 for question generation and abstractive summarization. All models are implemented in PyTorch (Paszke et al., 2017) and trained on single Tesla P40 GPU, based on NAVER Smart Machine Learning (NSML) platform"
D19-1308,D16-1264,0,0.248124,"-decoder model generates multiple target sequences given these binary masks along with the original source tokens. Due to the non-differentiable nature of discrete sampling, we adopt stochastic hard-EM for training S ELECTOR. To mitigate the lack of ground truth annotation for the mask (content selection), we use the overlap between the source and target sequences as a simple proxy for the ground-truth mask. We experiment on question generation and abstractive summarization tasks and show that our method achieves the best trade-off between accuracy and diversity over previous models on SQuAD (Rajpurkar et al., 2016) and CNN-DM (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017) datasets. In particular, compared to the recently-introduced mixture decoder (Shen et al., 2019) that also aims to diversify outputs by creating multiple decoders, our modular method not only demonstrates better accuracy and diversity, but also trains 3.7 times faster. 2 Related Work Diverse Search Algorithms Beam search, the most commonly used search algorithm for decoding, is known to produce samples that are short, contain repetitive phrases, and share majority of their tokens. Hence several methods are introduced"
D19-1308,D18-1480,0,0.0203295,"les are ignored (Bowman et al., 2016; Park et al., 2018; Kim et al., 2018b; 3122 (a) Ours 1) Diverse Content Selection 2) Focused Generation Selector 1 Enc Dec Selector 2 Enc Dec Selector 3 Enc Dec (c) Mixture Decoder (b) Diverse Search Dec 1 Enc Enc Dec Dec 2 Dec 3 Figure 2: Overview of diverse sequence-to-sequence generation methods. (a) refers to our two-stage approach described throughout Section. 3, (b) refers to search-based methods (Vijayakumar et al., 2018; Li et al., 2016b; Fan et al., 2018), and (c) refers to mixture decoders (Shen et al., 2019; He et al., 2018). Dieng et al., 2018; Xu and Durrett, 2018; He et al., 2019; Razavi et al., 2019). This is also observed in our initial experiments and Shen et al. (2019), where MoE-based methods significantly outperforms VAE-based methods because of the posterior collapse. Moreover, we observe that sampling mixtures makes training more stable compared to stochastic sampling of latent variables. Furthermore, our latent structure as a sequence of binary variables is different from most VAEs which use a fixed-size continuous latent variable. This gives a finer-grained control and interpretability on where to focus, especially when source sequence is lo"
D19-1308,D16-1050,0,0.0256304,"h is also known as multiple choice learning (Guzman-Rivera et al., 2012; Lee et al., 2016). While Shen et al. (2019) makes RNN decoder as a MoE, we make S ELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly. Variational Autoencoders Variational Autoencoders (VAE) (Kingma and Welling, 2013) are used for diverse generation in several tasks, such as language modeling (Bowman et al., 2016), machine translation (Zhang et al., 2016; Su et al., 2018; Deng et al., 2018; Shankar and Sarawagi, 2019), and conversation modeling (Serban et al., 2017; Wen et al., 2017; Zhao et al., 2017; Park et al., 2018; Wen and Luong, 2018; Gu et al., 2019). These methods sample diverse latent variables from an approximate posterior distribution, but often suffer from a posterior collapse where the sampled latent variables are ignored (Bowman et al., 2016; Park et al., 2018; Kim et al., 2018b; 3122 (a) Ours 1) Diverse Content Selection 2) Focused Generation Selector 1 Enc Dec Selector 2 Enc Dec Selector 3 Enc Dec (c) Mixture Decoder (b) Dive"
D19-1308,P17-1099,0,0.710727,"with the original source tokens. Due to the non-differentiable nature of discrete sampling, we adopt stochastic hard-EM for training S ELECTOR. To mitigate the lack of ground truth annotation for the mask (content selection), we use the overlap between the source and target sequences as a simple proxy for the ground-truth mask. We experiment on question generation and abstractive summarization tasks and show that our method achieves the best trade-off between accuracy and diversity over previous models on SQuAD (Rajpurkar et al., 2016) and CNN-DM (Hermann et al., 2015; Nallapati et al., 2016; See et al., 2017) datasets. In particular, compared to the recently-introduced mixture decoder (Shen et al., 2019) that also aims to diversify outputs by creating multiple decoders, our modular method not only demonstrates better accuracy and diversity, but also trains 3.7 times faster. 2 Related Work Diverse Search Algorithms Beam search, the most commonly used search algorithm for decoding, is known to produce samples that are short, contain repetitive phrases, and share majority of their tokens. Hence several methods are introduced to diversify search algorithms for decoding. Graves (2013); Chorowski and Ja"
D19-1308,P17-1061,0,0.0351786,"e S ELECTOR as a MoE to diversify content selection and let the encoderdecoder models one-to-one generation. As shown in our empirical results, our method achieves a better accuracy-diversity trade-off while reducing training time significantly. Variational Autoencoders Variational Autoencoders (VAE) (Kingma and Welling, 2013) are used for diverse generation in several tasks, such as language modeling (Bowman et al., 2016), machine translation (Zhang et al., 2016; Su et al., 2018; Deng et al., 2018; Shankar and Sarawagi, 2019), and conversation modeling (Serban et al., 2017; Wen et al., 2017; Zhao et al., 2017; Park et al., 2018; Wen and Luong, 2018; Gu et al., 2019). These methods sample diverse latent variables from an approximate posterior distribution, but often suffer from a posterior collapse where the sampled latent variables are ignored (Bowman et al., 2016; Park et al., 2018; Kim et al., 2018b; 3122 (a) Ours 1) Diverse Content Selection 2) Focused Generation Selector 1 Enc Dec Selector 2 Enc Dec Selector 3 Enc Dec (c) Mixture Decoder (b) Diverse Search Dec 1 Enc Enc Dec Dec 2 Dec 3 Figure 2: Overview of diverse sequence-to-sequence generation methods. (a) refers to our two-stage approach d"
D19-1308,P17-1101,0,0.348812,"maximizing mutual information between source and target sentences. Chorowski and Jaitly (2017); Kalyan et al. (2018) introduce terms enforcing knowledge transfer among similar annotations. Our work is orthogonal to these methods and can potentially benefit from adding these regularization terms to our objective function. Content Selection in NLP Selecting important parts of the context has been an important step in NLP applications (Reiter and Dale, 2000). Most recently, Ke et al. (2018); Min et al. (2018) conduct soft-/ hard-selection of key parts from source passages for question answering. Zhou et al. (2017b) use soft gating on the source document encoder for abstractive summarization. Li et al. (2018) guide abstractive summarization models with off-the-shelf keyword extractors. The most relevant work to ours are Subramanian et al. (2018) and Gehrmann et al. (2018). Subramanian et al. (2018) use a pointer network (Vinyals et al., 2015) for extracting key phrases for question generation and Gehrmann et al. (2018) use content selector to limit copying probability for abstractive summarization. The main purpose of these approaches is to enhance accuracy, while our method uses diverse content select"
D19-1308,W18-2609,0,0.0961872,"and can potentially benefit from adding these regularization terms to our objective function. Content Selection in NLP Selecting important parts of the context has been an important step in NLP applications (Reiter and Dale, 2000). Most recently, Ke et al. (2018); Min et al. (2018) conduct soft-/ hard-selection of key parts from source passages for question answering. Zhou et al. (2017b) use soft gating on the source document encoder for abstractive summarization. Li et al. (2018) guide abstractive summarization models with off-the-shelf keyword extractors. The most relevant work to ours are Subramanian et al. (2018) and Gehrmann et al. (2018). Subramanian et al. (2018) use a pointer network (Vinyals et al., 2015) for extracting key phrases for question generation and Gehrmann et al. (2018) use content selector to limit copying probability for abstractive summarization. The main purpose of these approaches is to enhance accuracy, while our method uses diverse content selection to enhance both accuracy and diversity (refer to our empirical results). Additionally, our method allows models to learn how to utilize information from the selected content, whereas Gehrmann et al. (2018) manually limit the copying"
D19-1585,D15-1075,0,0.0536042,"provide information to help infer the type of a difficult-to-classify entity mention. In event extraction, knowledge of the entities present in a sentence can provide information that is useful for predicting event triggers. To model global context, previous works have used pipelines to extract syntactic, discourse, and other hand-engineered features as inputs to structured prediction models (Li et al., 2013; Yang and Meanwhile, contextual language models (Dai and Le, 2015; Peters et al., 2017, 2018; Devlin et al., 2018) have proven successful on a range of natural language processing tasks (Bowman et al., 2015; Sang and De Meulder, 2003; Rajpurkar et al., 2016). Some of these models are also capable of modeling context beyond the sentence boundary. For instance, the attention mechanism in BERT’s transformer architecture can capture relationships between tokens in nearby sentences. In this paper, we study different methods to incorporate global context in a general multi-task IE framework, building upon a previous span-based IE method (Luan et al., 2019). Our DY GIE++ framework, shown in Figure 1, enumerates candidate text 5784 Proceedings of the 2019 Conference on Empirical Methods in Natural Langu"
D19-1585,P18-2014,0,0.0284624,"for new tasks or datasets. Graph propagation Coreference Relation propagation Auxiliary Span enumeration BERT … Sentence Sentence Event propagation Coreference propagation Sentence … Figure 1: Overview of our framework: DY GIE++. Shared span representations are constructed by refining contextualized word embeddings via span graph updates, then passed to scoring functions for three IE tasks. Mitchell, 2016; Li and Ji, 2014) and neural scoring functions (Nguyen and Nguyen, 2019), or as a guide for the construction of neural architectures (Peng et al., 2017; Zhang et al., 2018; Sha et al., 2018; Christopoulou et al., 2018). Recent end-toend systems have achieved strong performance by dynmically constructing graphs of spans whose edges correspond to task-specific relations (Luan et al., 2019; Lee et al., 2018; Qian et al., 2018). Introduction Many information extraction tasks – including named entity recognition, relation extraction, event extraction, and coreference resolution – can benefit from incorporating global context across sentences or from non-local dependencies among phrases. For example, knowledge of a coreference relationship can provide information to help infer the type of a difficult-to-classify"
D19-1585,N18-2016,0,0.13582,"Missing"
D19-1585,N18-2108,0,0.0266564,"w of our framework: DY GIE++. Shared span representations are constructed by refining contextualized word embeddings via span graph updates, then passed to scoring functions for three IE tasks. Mitchell, 2016; Li and Ji, 2014) and neural scoring functions (Nguyen and Nguyen, 2019), or as a guide for the construction of neural architectures (Peng et al., 2017; Zhang et al., 2018; Sha et al., 2018; Christopoulou et al., 2018). Recent end-toend systems have achieved strong performance by dynmically constructing graphs of spans whose edges correspond to task-specific relations (Luan et al., 2019; Lee et al., 2018; Qian et al., 2018). Introduction Many information extraction tasks – including named entity recognition, relation extraction, event extraction, and coreference resolution – can benefit from incorporating global context across sentences or from non-local dependencies among phrases. For example, knowledge of a coreference relationship can provide information to help infer the type of a difficult-to-classify entity mention. In event extraction, knowledge of the entities present in a sentence can provide information that is useful for predicting event triggers. To model global context, previous"
D19-1585,P14-1038,0,0.0837228,"predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github. com/dwadden/dygiepp and can be easily adapted for new tasks or datasets. Graph propagation Coreference Relation propagation Auxiliary Span enumeration BERT … Sentence Sentence Event propagation Coreference propagation Sentence … Figure 1: Overview of our framework: DY GIE++. Shared span representations are constructed by refining contextualized word embeddings via span graph updates, then passed to scoring functions for three IE tasks. Mitchell, 2016; Li and Ji, 2014) and neural scoring functions (Nguyen and Nguyen, 2019), or as a guide for the construction of neural architectures (Peng et al., 2017; Zhang et al., 2018; Sha et al., 2018; Christopoulou et al., 2018). Recent end-toend systems have achieved strong performance by dynmically constructing graphs of spans whose edges correspond to task-specific relations (Luan et al., 2019; Lee et al., 2018; Qian et al., 2018). Introduction Many information extraction tasks – including named entity recognition, relation extraction, event extraction, and coreference resolution – can benefit from incorporating glob"
D19-1585,N19-1308,1,0.869742,"… Figure 1: Overview of our framework: DY GIE++. Shared span representations are constructed by refining contextualized word embeddings via span graph updates, then passed to scoring functions for three IE tasks. Mitchell, 2016; Li and Ji, 2014) and neural scoring functions (Nguyen and Nguyen, 2019), or as a guide for the construction of neural architectures (Peng et al., 2017; Zhang et al., 2018; Sha et al., 2018; Christopoulou et al., 2018). Recent end-toend systems have achieved strong performance by dynmically constructing graphs of spans whose edges correspond to task-specific relations (Luan et al., 2019; Lee et al., 2018; Qian et al., 2018). Introduction Many information extraction tasks – including named entity recognition, relation extraction, event extraction, and coreference resolution – can benefit from incorporating global context across sentences or from non-local dependencies among phrases. For example, knowledge of a coreference relationship can provide information to help infer the type of a difficult-to-classify entity mention. In event extraction, knowledge of the entities present in a sentence can provide information that is useful for predicting event triggers. To model global"
D19-1585,P16-1105,0,0.146629,"Missing"
D19-1585,Q17-1008,0,0.0233132,"://github. com/dwadden/dygiepp and can be easily adapted for new tasks or datasets. Graph propagation Coreference Relation propagation Auxiliary Span enumeration BERT … Sentence Sentence Event propagation Coreference propagation Sentence … Figure 1: Overview of our framework: DY GIE++. Shared span representations are constructed by refining contextualized word embeddings via span graph updates, then passed to scoring functions for three IE tasks. Mitchell, 2016; Li and Ji, 2014) and neural scoring functions (Nguyen and Nguyen, 2019), or as a guide for the construction of neural architectures (Peng et al., 2017; Zhang et al., 2018; Sha et al., 2018; Christopoulou et al., 2018). Recent end-toend systems have achieved strong performance by dynmically constructing graphs of spans whose edges correspond to task-specific relations (Luan et al., 2019; Lee et al., 2018; Qian et al., 2018). Introduction Many information extraction tasks – including named entity recognition, relation extraction, event extraction, and coreference resolution – can benefit from incorporating global context across sentences or from non-local dependencies among phrases. For example, knowledge of a coreference relationship can pro"
D19-1585,P17-1161,0,0.0323169,"across sentences or from non-local dependencies among phrases. For example, knowledge of a coreference relationship can provide information to help infer the type of a difficult-to-classify entity mention. In event extraction, knowledge of the entities present in a sentence can provide information that is useful for predicting event triggers. To model global context, previous works have used pipelines to extract syntactic, discourse, and other hand-engineered features as inputs to structured prediction models (Li et al., 2013; Yang and Meanwhile, contextual language models (Dai and Le, 2015; Peters et al., 2017, 2018; Devlin et al., 2018) have proven successful on a range of natural language processing tasks (Bowman et al., 2015; Sang and De Meulder, 2003; Rajpurkar et al., 2016). Some of these models are also capable of modeling context beyond the sentence boundary. For instance, the attention mechanism in BERT’s transformer architecture can capture relationships between tokens in nearby sentences. In this paper, we study different methods to incorporate global context in a general multi-task IE framework, building upon a previous span-based IE method (Luan et al., 2019). Our DY GIE++ framework, sh"
D19-1585,N18-1202,0,0.0598601,"roles are also correct, respectively. Model Variations We perform experiments with the following variants of our model architecture. BERT + LSTM feeds pretrained BERT embeddings to a bi-directional LSTM layer, and the LSTM parameters are trained together with task specific layers. BERT Finetune uses supervised fine-tuning of BERT on the end-task. For each variation, we study the effect of integrating different task-specific message propagation approaches. Comparisons For entity and relation extraction, we compare DY GIE++ against the DY GIE system it extends. DY GIE is a system based on ELMo (Peters et al., 2018) that uses dynamic span graphs to propagate global context. For event extraction, we compare against the method of Zhang et al. (2019), which is also an ELMo-based approach that relies on inverse reinforcement learning to focus the model on more difficult-to-detect events. Implementation Details Our model is implemented using AllenNLP (Gardner et al., 2017). We use BERTBASE for entity and relation extraction tasks and use BERTLARGE for event extraction. For BERT finetuning, we use BertAdam with the learning rates of 1 × 10−3 for the task specific layers, and 5.0 × 10−5 for BERT. We use a longe"
D19-1585,W12-4501,0,0.131367,"Missing"
D19-1585,W03-0419,0,0.473206,"Missing"
D19-1585,N16-1033,0,0.102998,"Missing"
D19-1585,P13-1008,0,0.179753,"raction, and coreference resolution – can benefit from incorporating global context across sentences or from non-local dependencies among phrases. For example, knowledge of a coreference relationship can provide information to help infer the type of a difficult-to-classify entity mention. In event extraction, knowledge of the entities present in a sentence can provide information that is useful for predicting event triggers. To model global context, previous works have used pipelines to extract syntactic, discourse, and other hand-engineered features as inputs to structured prediction models (Li et al., 2013; Yang and Meanwhile, contextual language models (Dai and Le, 2015; Peters et al., 2017, 2018; Devlin et al., 2018) have proven successful on a range of natural language processing tasks (Bowman et al., 2015; Sang and De Meulder, 2003; Rajpurkar et al., 2016). Some of these models are also capable of modeling context beyond the sentence boundary. For instance, the attention mechanism in BERT’s transformer architecture can capture relationships between tokens in nearby sentences. In this paper, we study different methods to incorporate global context in a general multi-task IE framework, buildi"
D19-1585,D18-1360,1,0.824491,"Missing"
D19-1585,D18-1244,0,0.0328085,"den/dygiepp and can be easily adapted for new tasks or datasets. Graph propagation Coreference Relation propagation Auxiliary Span enumeration BERT … Sentence Sentence Event propagation Coreference propagation Sentence … Figure 1: Overview of our framework: DY GIE++. Shared span representations are constructed by refining contextualized word embeddings via span graph updates, then passed to scoring functions for three IE tasks. Mitchell, 2016; Li and Ji, 2014) and neural scoring functions (Nguyen and Nguyen, 2019), or as a guide for the construction of neural architectures (Peng et al., 2017; Zhang et al., 2018; Sha et al., 2018; Christopoulou et al., 2018). Recent end-toend systems have achieved strong performance by dynmically constructing graphs of spans whose edges correspond to task-specific relations (Luan et al., 2019; Lee et al., 2018; Qian et al., 2018). Introduction Many information extraction tasks – including named entity recognition, relation extraction, event extraction, and coreference resolution – can benefit from incorporating global context across sentences or from non-local dependencies among phrases. For example, knowledge of a coreference relationship can provide information to"
D19-1585,W18-2501,0,\N,Missing
D19-1585,N19-1082,0,\N,Missing
D19-1585,N19-1423,0,\N,Missing
D19-5815,P17-1147,0,0.0660527,"Missing"
D19-5815,W13-2322,0,0.0634753,"Missing"
D19-5815,P16-1223,0,0.0995681,"Missing"
D19-5815,P18-1078,1,0.890843,"Missing"
D19-5815,Q18-1023,0,0.0674018,"Missing"
D19-5815,N19-1300,0,0.0236782,"al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these sorts of phenomena could be incredibly interesting, and very challenging. 4 Ways"
D19-5815,Q19-1026,0,0.011731,"swering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these sorts of phenomena could be incredibly interest"
D19-5815,P19-1612,0,0.0261817,"a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these"
D19-5815,D19-1606,1,0.90781,"ets that we construct. Paragraph-level structure While the input to a reading comprehension dataset is a paragraph of text, most datasets do not explicitly target questions that require understanding the entire paragraph, or how the sentences fit together into a coherent whole. Some post-hoc analyses attempt to reveal the percentage of questions that require more than one sentence, but it is better to design the datasets from the beginning to obtain questions that look at paragraph- or discourse-level phenomena, such as entity tracking, discourse relations, or pragmatics. For example, Quoref (Dasigi et al., 2019) is a dataset that targets entity tracking and coreference resolution. There are few linguistic formalisms targeting structures larger than a paragraph, but those that do exist, such as rhetorical structure theory (Mann and Thompson, 1988), could form the basis of an interesting and useful reading comprehension dataset. Sentence-level linguistic structure Most existing reading comprehension datasets implicitly target local predicate-argument structures. The incentives involved in the creation of SQuAD encouraged workers to create questions that were close paraphrases of some part of a paragrap"
D19-5815,D19-5808,1,0.90061,"Mary. Mary was just diagnosed with cancer. means also understanding that Bill will be sad. In some sense this can be seen as “grounding” the predicates in the text to some prior knowledge that includes the implications of that predicate, but it also includes the more general notion of reconstructing a model of the world being described by the text. There are two datasets that just scratch 107 the surface of this kind of reading: ShARC (Saeidi et al., 2018) requires reading rules and applying them to questions asked by users, though its format is not standard reading comprehension; and ROPES (Lin et al., 2019), which requires reading descriptions of causes and effects and applying them to situated questions. a similar meaning. Examples include NARRA TIVE QA (Koˇcisk` y et al., 2018), where question authors were shown a summary of a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which"
D19-5815,N19-1246,1,0.946639,"et local predicate-argument structures. The incentives involved in the creation of SQuAD encouraged workers to create questions that were close paraphrases of some part of a paragraph, replacing a noun phrase with a question word. This, and other cloze-style question construction, encourages very local reasoning that amounts to finding and then understanding the argument structure of a single sentence. This is an important aspect of meaning, but one could construct much harder datasets than this. One direction to push on linguistic structure is to move beyond locating a single sentence. DROP (Dua et al., 2019) largely involves the same level of linguistic structural analysis as SQuAD, but the questions require combining pieces from several parts of the passage, forcing a more comprehensive analysis of the passage contents. A separate direction one could push on sentence-level linguistic structure in reading comprehension is to target other phenomena than predicate argument structure. There are many rich problems in semantic analysis, such as negation scope, distributive vs. non-distributive coordination, factuality, deixis, briding and empty elements, preposition senses, noun compounds, and many mo"
D19-5815,D10-1123,0,0.0140052,"and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For example, in the question What is the capital of the largest economy in Europe? we would like the largest economy in Europe to be one answer we can use to modify the question to what is the capital of Germany. 4.5 Adversarial construction 4.7 Minimal question pairs ROPES (Lin et al., 2019) borrowed the idea of “minimal pairs” from linguistic analys"
D19-5815,P19-1416,1,0.897168,"easoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For e"
D19-5815,P18-2124,0,0.0366525,"a et al., 2018). Accordingly, more recent reading comprehension datasets are constructed with several different approaches to prevent such shortcuts in order to foster natural language understanding. 4.1 4.2 “No answer” option Most of the reasoning shortcuts in existing datasets arise due to the fact that the system can assume that the answer is guaranteed to exist in the given passage. Removing this assumption and requiring the system to identify whether the question is even answerable from the passage can prevent such shortcuts. One example of this kind of dataset construction is SQ UAD 2.0(Rajpurkar et al., 2018), which asked annotators to read the given passage and write a question which the passage does not contain the answer to but contains a plausible negative answer. A drawback of this approach is that annotators see the passage when asking the question, which can introduce its own biases and shortcuts. An alternative is to combine a “no answer” option with the approach the previous section, where an annotator writes questions without knowing the answer, and another annotator verifies whether they are answerable by the paired passage. Example datasets include N EWS QA (Trischler et al., 2016)4 ,"
D19-5815,D19-1243,0,0.0520439,"Missing"
D19-5815,Q19-1016,0,0.0223257,"ntial means of avoiding reasoning shortcuts. A person is not able to answer a simple question such as How many? without the additional context of a prior question describing what is being counted. Care needs to be taken with this method, however, as some datasets are amenable to input reduction (Feng et al., 2018), where there is only one plausible answer to such a short question. If done well, however, this method provides additional challenges such as clarification, coreference resolution, and aggregation of pieces scattered across conversation history. Q UAC (Choi et al., 2018) and C O QA (Reddy et al., 2019) are two datasets that focus on such setting. 4.4 4.6 One promising means of removing reasoning shortcuts is to encode those shortcuts into a learned system, and use that system to filter out questions that are too easy during dataset construction. DROP (Dua et al., 2019) and Quoref (Dasigi et al., 2019) used a model trained on SQuAD 1.1 (Rajpurkar et al., 2016) as an “adversarial” baseline when having crowd workers write questions. Because the people could see when the system answered their questions correctly, they learned to ask harder questions. This kind of adversarial construction can in"
D19-5815,D17-1215,0,0.0795702,"Missing"
D19-5815,D13-1020,0,0.0492515,"it is not even clear what it means to understand text, or how to judge whether a machine has achieved success at this task. Much recent research in the natural language processing community has converged on an approach to this problem called machine reading comprehension, where a system is given a passage of text and a natural language question that presumably requires some level of “understanding” of the passage in order to answer. While there have been many papers in the last few years studying this basic problem, as far as we are aware, there is no paper formally justifying this approach 1 Richardson et al. (2013) give a good overview of the early history of this approach, but provide only very little justification. 2 Though SQuAD was not nearly the first reading comprehension dataset, its introduction of the span extraction format was innovative and useful, and most new datasets follow its design. 105 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 105–112 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics so this is the definition we choose, while admitting that it is not perfect. Using natural language questions to test comprehens"
D19-5815,P19-1262,0,0.0173168,"ch require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For example, in the question W"
D19-5815,D18-1233,0,0.0558381,"Missing"
D19-5815,P18-1156,0,0.0337327,"the world being described by the text. There are two datasets that just scratch 107 the surface of this kind of reading: ShARC (Saeidi et al., 2018) requires reading rules and applying them to questions asked by users, though its format is not standard reading comprehension; and ROPES (Lin et al., 2019), which requires reading descriptions of causes and effects and applying them to situated questions. a similar meaning. Examples include NARRA TIVE QA (Koˇcisk` y et al., 2018), where question authors were shown a summary of a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al"
D19-5815,N18-1059,1,0.835931,"eems unsatisfying. Overall, however, we believe this is a good method that could be used more widely when collecting reading comprehension datasets. Complex reasoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functi"
D19-5815,P19-1485,1,0.878847,"Missing"
D19-5815,D18-1259,0,0.133395,"l, however, we believe this is a good method that could be used more widely when collecting reading comprehension datasets. Complex reasoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and no"
D19-5815,D18-1009,0,0.0295379,"ose shortcuts into a learned system, and use that system to filter out questions that are too easy during dataset construction. DROP (Dua et al., 2019) and Quoref (Dasigi et al., 2019) used a model trained on SQuAD 1.1 (Rajpurkar et al., 2016) as an “adversarial” baseline when having crowd workers write questions. Because the people could see when the system answered their questions correctly, they learned to ask harder questions. This kind of adversarial construction can introduce its own biases, however, especially if the questions being filtered are generated by machines instead of humans (Zellers et al., 2018). This also makes a dataset dependent on another dataset and model in complex ways, which has both positive and negative aspects to it. In some sense, it is a good thing to get a diverse set of reading comprehension questions, and encoding one dataset’s biases into a model to enforce a different distribution for new datasets helps in collecting diverse datasets. If crowd workers end up simply wordsmithing their questions in order to pass the adversary, however, this seems unsatisfying. Overall, however, we believe this is a good method that could be used more widely when collecting reading com"
N15-1022,W03-1004,0,0.0768064,"Missing"
N15-1022,W03-0310,0,0.100888,"Missing"
N15-1022,P11-2117,0,0.57538,"Missing"
N15-1022,de-marneffe-etal-2006-generating,0,0.0513912,"Missing"
N15-1022,W04-3208,0,0.0543309,"Missing"
N15-1022,N13-1092,0,0.148358,"Missing"
N15-1022,D13-1029,1,0.821758,"Missing"
N15-1022,D14-1058,1,0.494388,"Missing"
N15-1022,P13-1151,0,0.0751078,"Missing"
N15-1022,D14-1043,1,0.798061,"Missing"
N15-1022,P03-1054,0,0.0453331,"Missing"
N15-1022,P14-5010,0,0.0053723,"Missing"
N15-1022,E09-1065,0,0.0331283,"Missing"
N15-1022,J05-4003,0,0.154675,"Missing"
N15-1022,E06-1021,0,0.127307,"Missing"
N15-1022,N10-1063,0,0.0263911,"Missing"
N15-1022,P12-1107,0,0.559583,"Missing"
N15-1022,N10-1056,0,\N,Missing
N15-1022,C10-1152,0,\N,Missing
N15-1022,P12-1091,0,\N,Missing
N15-1022,P94-1019,0,\N,Missing
N15-1022,W10-0406,0,\N,Missing
N15-1086,D11-1039,0,0.026743,"However, for our domain, each distinct question warrants its own actions, slots, and values. Such a complex model would require abundant training data or laboriously handcrafted interpretation rules. In contrast, an open dialog system can usefully interpret, learn from, and respond to user utterances without a comprehensive dialog model. Domainindependent dialog systems with the flexibility to accept novel user utterances are a longstanding goal in dialog research (Polifroni et al., 2003). Recent work to address more open dialog includes bootstrapping a semantic parser from unlabeled dialogs (Artzi and Zettlemoyer, 2011), extracting potential user goals and system responses from backend databases (Hixon and Passonneau, 2013), and inducing slots and slot-fillers from a corpus of humanhuman dialogs with the use of FrameNet (Chen et al., 2014). These works focus on systems that learn about their domain prior to any human-system dialog. Our system learns about its domain during the dialog. While we rely on a limited number of templates to generate system responses, unscripted user utterances can usefully progress the dialog. This allows relation extraction from complex natural language utterances without a closed"
N15-1086,D14-1159,1,0.496981,"Missing"
N15-1086,N13-1092,0,0.0122334,"Missing"
N15-1086,N13-1128,1,0.794506,"model would require abundant training data or laboriously handcrafted interpretation rules. In contrast, an open dialog system can usefully interpret, learn from, and respond to user utterances without a comprehensive dialog model. Domainindependent dialog systems with the flexibility to accept novel user utterances are a longstanding goal in dialog research (Polifroni et al., 2003). Recent work to address more open dialog includes bootstrapping a semantic parser from unlabeled dialogs (Artzi and Zettlemoyer, 2011), extracting potential user goals and system responses from backend databases (Hixon and Passonneau, 2013), and inducing slots and slot-fillers from a corpus of humanhuman dialogs with the use of FrameNet (Chen et al., 2014). These works focus on systems that learn about their domain prior to any human-system dialog. Our system learns about its domain during the dialog. While we rely on a limited number of templates to generate system responses, unscripted user utterances can usefully progress the dialog. This allows relation extraction from complex natural language utterances without a closed set of recognized actions and known slot-value decompositions. 7 Discussion and Future Work K NOWBOT acqu"
N15-1086,D14-1058,1,0.100391,"ing is both flexible and powerful (Liu and Singh, 2004). However, in a small number of cases, relations that align known facts with question-answer statements are unlikely to lead to the correct answer. For example, our question set contains a single math problem, How long does it take for Earth to rotate on its axis seven times? (A) one day (B) one week (C) one month (D) one year. The multiplication operation necessary to infer the answer from the S CITEXT fact “The Earth rotates, or spins, on its axis once every 24 hours” is not easily represented by our model and requires other techniques (Hosseini et al., 2014). We observed only slight transfer of knowledge between questions. A larger question set with multiple questions per topic will allow us to better evaluate knowledge transfer. Our long-term goal is learning through any conversational interaction in a com859 pletely open domain, but because the fundamental trick that enables model-free NLU is computing progress towards an explicit dialog goal as a function of possible extractions, our current method is limited to tasks with explicit goals. The simple redundancy filter we use effectively distinguishes salient from noisy relations, but could be i"
N15-1086,C10-2065,0,0.221053,"Missing"
N15-1086,D14-1108,0,0.0180464,"alignment score is a Jaccard overlap modified to use relations, which makes it fast and practical, but results in many ties which we score as incorrect, and also ignores word order. For example, the bag-of-keywords is identical for contradicting answers “changing from liquid to solid” and “changing from solid to liquid.” To make this distinction, we could use an alignment score that is sensitive to word order such as an edit distance. We could expand our simple pruning constraints to take more advantage of syntax, for example by using dependency parsers optimized for conversational language (Kong et al., 2014). The relational model for reasoning is both flexible and powerful (Liu and Singh, 2004). However, in a small number of cases, relations that align known facts with question-answer statements are unlikely to lead to the correct answer. For example, our question set contains a single math problem, How long does it take for Earth to rotate on its axis seven times? (A) one day (B) one week (C) one month (D) one year. The multiplication operation necessary to infer the answer from the S CITEXT fact “The Earth rotates, or spins, on its axis once every 24 hours” is not easily represented by our mode"
N15-1086,W14-4326,0,0.105717,"ional agents. Williams et al (2015) combine active learning and dialog to efficiently label training data for dialog act classifiers. Relatively little work integrates relation extraction and dialog systems. Attribute-value pairs from restaurant reviews can generate system prompts (Reschke et al., 2013), and single-turn exchanges with search engines can populate a knowledge graph (Hakkani-Tur et al., 2014). Dependency relations extracted from individual dialog utterances by a parser also make effective features for dialog act classification (Kl¨uwer et al., 2010). The work closest to our own, Pappu and Rudnicky (2014a; 2014b), investigates knowledge acquisition strategies for academic events. Their system asks its users open-ended questions in order to elicit information about academic events of interest. They compare strategies by how many new vocabulary words are acquired, so that the best strategy prompts the user to mention the most OOV words. In their most recent work (2014b), they group the acquired researcher names by their interests to form a bipartite graph, and use acquired keywords for query ex858 pansion in a simple information retrieval task. Our present contribution builds on this general id"
N15-1086,P13-2089,0,0.0164603,"87) and its prototypes (Leal and Pearl, 1977) learn decision structures through stylized but conversational dialogs. An interactive interface for CYC (Witbrock et al., 2003) learns from experts but don’t use natural language. Fern´andez et al (2011) argue the importance of interactive language learning for conversational agents. Williams et al (2015) combine active learning and dialog to efficiently label training data for dialog act classifiers. Relatively little work integrates relation extraction and dialog systems. Attribute-value pairs from restaurant reviews can generate system prompts (Reschke et al., 2013), and single-turn exchanges with search engines can populate a knowledge graph (Hakkani-Tur et al., 2014). Dependency relations extracted from individual dialog utterances by a parser also make effective features for dialog act classification (Kl¨uwer et al., 2010). The work closest to our own, Pappu and Rudnicky (2014a; 2014b), investigates knowledge acquisition strategies for academic events. Their system asks its users open-ended questions in order to elicit information about academic events of interest. They compare strategies by how many new vocabulary words are acquired, so that the best"
N15-1086,P06-1101,0,0.0336763,"s the best supporting sentence in a text corpus. Equation (1) scores each questionanswer statement by using domain relations to align question concepts to support concepts. The next section describes sources of domain relations. 5.1 Sources of domain knowledge We compare relations from five sources: I DENTITY: An edgeless knowledge graph. The only relations are between identical concepts, equivalent to Jaccard overlap of concept keyword roots. W ORDNET: Paraphrase relations from Wordnet. Wordnet (Fellbaum, 1998) is a lexical database of synonyms and hypernyms common in NLP tasks. For example, Snow et al (2006) use Wordnet as training data for ontology induction. To build W ORDNET, we draw an edge between every pair of Wordnet concepts (ws , wq ) for which the WuPalmer Similarity (WUP) (Wu and Palmer, 1994) of the first sense in each concept’s synset exceeds 0.9, the best-performing WUP threshold we found. Concepts in the Wordnet hierarchy have a higher WUP when they have a closer common ancestor. If a known fact is Heat energy causes snow to melt, but a question asks if ice melts, then Wordnet should provide the missing knowledge that ice acts like snow. PPDB: Paraphrase relations from PPDB (Ganitk"
N15-1086,H89-1033,0,0.528703,"Missing"
N15-1086,P94-1019,0,\N,Missing
N15-1161,N01-1016,0,0.185611,"Missing"
N15-1161,W10-4343,0,0.0559791,"Missing"
N15-1161,N09-2028,0,0.0683417,"Missing"
N15-1161,P06-1021,0,0.0478851,"Missing"
N15-1161,Q14-1011,0,0.0242004,"Missing"
N15-1161,P04-1005,0,0.0984326,"Missing"
N15-1161,H05-1030,1,0.817171,"Missing"
N15-1161,P09-2070,0,0.0325579,"Missing"
N15-1161,N09-1074,0,0.0352402,"Missing"
N15-1161,N13-1102,0,0.0263867,"Missing"
N15-1161,D13-1013,0,0.0240239,"Missing"
N15-1161,D10-1017,0,0.0265576,"Missing"
N15-1161,C14-1138,0,0.0305281,"Missing"
N15-1161,C10-1154,0,0.0561419,"Missing"
N16-1136,D14-1058,1,0.467126,"testbed for algorithms that build a precise understanding of what is being asserted in text. Consider the following: Rachel bought two coloring books. One had 23 pictures and the other had 32. After one week she had colored 44 of the pictures. How many pictures does she still have to color? To solve such a problem, an algorithm must model implicit and explicit quantities in the text and their relationships through mathematical operations. Many researchers have taken on this challenge to design data-driven approaches to solve different * denotes equal contribution types of math word problems (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Zhou et al., 2015; Koncel-Kedziorski et al., 2015). Treatments of this task range from template-matching to narrative-building, and methods including integer linear programming, factorization, and more appear in the literature. As a result of the variety of approaches, several datasets have emerged. The variety of math word problems in these datasets is such that researchers have already begun specializing in the types of problems that their systems are able to successfully solve. Additionally, in some datasets one may find significant repetition of"
N16-1136,P14-1026,1,0.684656,"that build a precise understanding of what is being asserted in text. Consider the following: Rachel bought two coloring books. One had 23 pictures and the other had 32. After one week she had colored 44 of the pictures. How many pictures does she still have to color? To solve such a problem, an algorithm must model implicit and explicit quantities in the text and their relationships through mathematical operations. Many researchers have taken on this challenge to design data-driven approaches to solve different * denotes equal contribution types of math word problems (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Zhou et al., 2015; Koncel-Kedziorski et al., 2015). Treatments of this task range from template-matching to narrative-building, and methods including integer linear programming, factorization, and more appear in the literature. As a result of the variety of approaches, several datasets have emerged. The variety of math word problems in these datasets is such that researchers have already begun specializing in the types of problems that their systems are able to successfully solve. Additionally, in some datasets one may find significant repetition of common words, a small"
N16-1136,D15-1118,0,0.0320259,"2015) and elsewhere, the appearance of a similar template in the training data is a requirement for some systems at test time. More general math word problem solvers have been introduced that reduce or eliminate this reliance. M AWPS provides for the automatic construction of a dataset with minimized template overlap. Grammaticality. Many math word problems for use in AI research are drawn from user-submitted Related Work Recently, automatically solving math word problems has attracted several researchers. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), and geometry word problems (Seo et al., 2015). A few recent works, Kushman et al. (2014) and Zhou et al. (2015) focus on solving math word problems by matching quantities and variables (nouns) extracted from the problem text to templates appearing in the training data. These methods have a broad scope, but they rely heavily on overlap between templates in the training and test data. As shown in our experiments, when that overla"
N16-1136,D13-1020,0,0.0158064,"c word problem templates as equation trees and introduce a method for learning the least governing node for two text quantities. KoncelKedziorski et al. (2015) focus on single equation problems and use typed semantically-rich equation trees where nodes correspond to numbers and an associated type derived from the problem text, and efficiently enumerate the space of these trees using integer linear programming. Our work is also inspired by the recent work in introducing datasets to evaluate question answering and reading comprehension tasks that require reasoning and entailment. In contrast to Richardson et al. (2013), our work is focused on making a dataset for math word problems to evaluate robustness, scalability, and scope of algorithms in quantitative reasoning. In contrast to Weston et al. (2015), our work 1153 3 Data Online Interfaces for Contributing Problems/Datasets MAWPS Reduce Lexical Overlap Repository Word Problem Dataset pi ,pj ∈D i&lt;j Minimize Template Overlap Grammaticality Checker Automatic Optimization Tools Figure 1: M AWPS System overview: Online interfaces allow for extending repository. Optimization Tools allow for real-time delivery of usable datasets with particular characteristics."
N16-1136,D15-1202,1,0.572949,"understanding of what is being asserted in text. Consider the following: Rachel bought two coloring books. One had 23 pictures and the other had 32. After one week she had colored 44 of the pictures. How many pictures does she still have to color? To solve such a problem, an algorithm must model implicit and explicit quantities in the text and their relationships through mathematical operations. Many researchers have taken on this challenge to design data-driven approaches to solve different * denotes equal contribution types of math word problems (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Zhou et al., 2015; Koncel-Kedziorski et al., 2015). Treatments of this task range from template-matching to narrative-building, and methods including integer linear programming, factorization, and more appear in the literature. As a result of the variety of approaches, several datasets have emerged. The variety of math word problems in these datasets is such that researchers have already begun specializing in the types of problems that their systems are able to successfully solve. Additionally, in some datasets one may find significant repetition of common words, a small set of equation temp"
N16-1136,D15-1171,1,0.839534,"liance. M AWPS provides for the automatic construction of a dataset with minimized template overlap. Grammaticality. Many math word problems for use in AI research are drawn from user-submitted Related Work Recently, automatically solving math word problems has attracted several researchers. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), and geometry word problems (Seo et al., 2015). A few recent works, Kushman et al. (2014) and Zhou et al. (2015) focus on solving math word problems by matching quantities and variables (nouns) extracted from the problem text to templates appearing in the training data. These methods have a broad scope, but they rely heavily on overlap between templates in the training and test data. As shown in our experiments, when that overlap is reduced, the performance of the systems drops significantly. Other work has focused on more limited domains, but aims to reduce the reliance on data overlap. Hosseini et al. (2014) solve addition and subtracti"
N16-1136,D15-1135,0,0.164514,"a ∗ b) + c = x. As shown in Roy and Roth (2015) and elsewhere, the appearance of a similar template in the training data is a requirement for some systems at test time. More general math word problem solvers have been introduced that reduce or eliminate this reliance. M AWPS provides for the automatic construction of a dataset with minimized template overlap. Grammaticality. Many math word problems for use in AI research are drawn from user-submitted Related Work Recently, automatically solving math word problems has attracted several researchers. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), and geometry word problems (Seo et al., 2015). A few recent works, Kushman et al. (2014) and Zhou et al. (2015) focus on solving math word problems by matching quantities and variables (nouns) extracted from the problem text to templates appearing in the training data. These methods have a broad scope, but they rely heavily on overlap between templates in the training and test data"
N16-1136,D15-1096,0,0.373441,"t is being asserted in text. Consider the following: Rachel bought two coloring books. One had 23 pictures and the other had 32. After one week she had colored 44 of the pictures. How many pictures does she still have to color? To solve such a problem, an algorithm must model implicit and explicit quantities in the text and their relationships through mathematical operations. Many researchers have taken on this challenge to design data-driven approaches to solve different * denotes equal contribution types of math word problems (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015; Zhou et al., 2015; Koncel-Kedziorski et al., 2015). Treatments of this task range from template-matching to narrative-building, and methods including integer linear programming, factorization, and more appear in the literature. As a result of the variety of approaches, several datasets have emerged. The variety of math word problems in these datasets is such that researchers have already begun specializing in the types of problems that their systems are able to successfully solve. Additionally, in some datasets one may find significant repetition of common words, a small set of equation templates used again an"
N16-1136,Q15-1001,1,\N,Missing
N16-1136,Q15-1042,1,\N,Missing
N18-2058,P15-1017,0,0.31184,"Tevent where Tevent is the set of triggers for this event in the gold training data. If the maximum similarity is greater than some threshold, θsim , the sentence and the corresponding trigger are added to the training data. Event Trigger Identification Systems Event extraction tasks such as ACE and TAC-KBP have frequently been approached with supervised machine learning systems based on hand-crafted features, such as the system adapted from Li et al. (2013) which we make use of here. Recently, state-of-the-art results have been obtained with neural-network-based systems (Nguyen et al., 2016; Chen et al., 2015; Feng et al., 2016). Here, we make use of two systems whose implementations are publicly available and show that adding additional data would improve their performance. The first system is the joint recurrent neural net (JRNN) introduced by Nguyen et al. (2016). This model uses a bi-directional GRU layer to encode the input sentence. It then concatenates that with the vectors of words in a window around the current word, and passes the concatenated vectors into a feed-forward network to predict trigger types for each token. Because we are only classifying triggers, and not arguments, we don’t"
N18-2058,P16-2011,0,0.35536,"is the set of triggers for this event in the gold training data. If the maximum similarity is greater than some threshold, θsim , the sentence and the corresponding trigger are added to the training data. Event Trigger Identification Systems Event extraction tasks such as ACE and TAC-KBP have frequently been approached with supervised machine learning systems based on hand-crafted features, such as the system adapted from Li et al. (2013) which we make use of here. Recently, state-of-the-art results have been obtained with neural-network-based systems (Nguyen et al., 2016; Chen et al., 2015; Feng et al., 2016). Here, we make use of two systems whose implementations are publicly available and show that adding additional data would improve their performance. The first system is the joint recurrent neural net (JRNN) introduced by Nguyen et al. (2016). This model uses a bi-directional GRU layer to encode the input sentence. It then concatenates that with the vectors of words in a window around the current word, and passes the concatenated vectors into a feed-forward network to predict trigger types for each token. Because we are only classifying triggers, and not arguments, we don’t include the memory"
N18-2058,N10-1112,0,0.0358718,"size of the gold training data. Using a modest amount of semi-supervised data improves extractor performance on both ACE & TAC-KBP events. * indicates that the difference in F1 relative to training with just the gold data is statistically significant (p &lt; 0.05). prediction with hidden layer size of 300. Finally, for training, We apply the stochastic gradient descent algorithm with mini-batches of size 50 and the AdaDelta update rule (Zeiler, 2012) with L2 regularization. For the CRF model, we maximize the conditional log likelihood of the training data with a loss function via softmax-margin (Gimpel and Smith, 2010). We optimize using AdaGrad (Duchi et al., 2011) with L2 regularization. 4 Experiments Varying Amounts of Additional Data In this section we show that the addition of automatically-generated training examples improves the performance of both systems we tested it on. We sample examples from the automatically-generated data, limiting the total number of positive examples to a specific number. In order to avoid biasing the system in favor of a specific event type, we ensure that the additional data has a uniform distribution of event types. We run 10 trials at each point, and report average resul"
N18-2058,P11-1055,1,0.737275,"nt extraction is the relatively small number of labeled training examples available. Researchers have dealt with this by framing event extraction in a way that allows them to rely heavily on systems built for dependency parsing (McClosky et al., 2011) and semantic role labeling (Peng et al., 2016). Unlike these researchers, we join a line of work that attempts to directly harvest additional training examples for use in traditional event extraction systems. Distant supervision is one source of additional data that has been successfully applied to relation extraction tasks (Riedel et al., 2010; Hoffmann et al., 2011; Mintz et al., 2009), which align a background knowledge base to an accompanying corpus of natural language documents. For event extraction, such data sources are not as easily available since there are no pre-existing stores of tuples of attacks, movements or meetings. Other work has generated additional data by using a pattern-based model of event mentions and bootstrapping on top of a small set of seed examples. Huang and Riloff (2012) begin with a set of nouns that are specific to certain event roles and extract patterns based on the contexts in which those words appear. Li et al. (2014)"
N18-2058,E12-1029,0,0.104728,"extraction systems. Distant supervision is one source of additional data that has been successfully applied to relation extraction tasks (Riedel et al., 2010; Hoffmann et al., 2011; Mintz et al., 2009), which align a background knowledge base to an accompanying corpus of natural language documents. For event extraction, such data sources are not as easily available since there are no pre-existing stores of tuples of attacks, movements or meetings. Other work has generated additional data by using a pattern-based model of event mentions and bootstrapping on top of a small set of seed examples. Huang and Riloff (2012) begin with a set of nouns that are specific to certain event roles and extract patterns based on the contexts in which those words appear. Li et al. (2014) extracted additional patterns using various event inference mechanisms. The work most similar to ours is that of Liao and Grishman (2010, 2011) to identify articles from a corpus which described the same event instances found in training examples. These articles are then used in self-training an ACE-trained system after being filtered to select passages with consistent roles and triggers. Their method provides a 2.7 point boost to F1, but"
N18-2058,C14-1204,0,0.0203355,"mann et al., 2011; Mintz et al., 2009), which align a background knowledge base to an accompanying corpus of natural language documents. For event extraction, such data sources are not as easily available since there are no pre-existing stores of tuples of attacks, movements or meetings. Other work has generated additional data by using a pattern-based model of event mentions and bootstrapping on top of a small set of seed examples. Huang and Riloff (2012) begin with a set of nouns that are specific to certain event roles and extract patterns based on the contexts in which those words appear. Li et al. (2014) extracted additional patterns using various event inference mechanisms. The work most similar to ours is that of Liao and Grishman (2010, 2011) to identify articles from a corpus which described the same event instances found in training examples. These articles are then used in self-training an ACE-trained system after being filtered to select passages with consistent roles and triggers. Their method provides a 2.7 point boost to F1, but their baseline system results are much lower than ours (54.1 vs 69.1) and it is unclear what improvement their method would have on a state-of-the-art extra"
N18-2058,Q15-1009,1,0.931828,"ration process has three steps. The first is to identify clusters of news articles all describing the same event. The second step is to run a baseline system over the sentences in these clusters to identify events found in each cluster. Finally, once we have identified an event in one article in a cluster, our system scans through the other articles in that cluster choosing the most likely trigger in each article for the given event type. Cluster Articles In order to identify groups of articles describing the same event instance, we use an approach inspired by the NewsSpike idea introduced in Zhang et al. (2015). The main intuition is that rare entities that are mentioned a lot on a single date are more indicative that two articles are covering the same event. We assign a score, S, to each pair of articles, (ai , aj ) appearing on the same day, for whether or not they cover the same event, as follows: S(ai , aj ) = X e∈Eai ∩Eaj t∈Tevent where Tevent is the set of triggers for this event in the gold training data. If the maximum similarity is greater than some threshold, θsim , the sentence and the corresponding trigger are added to the training data. Event Trigger Identification Systems Event extract"
N18-2058,P13-1008,0,0.356439,"S, to each pair of articles, (ai , aj ) appearing on the same day, for whether or not they cover the same event, as follows: S(ai , aj ) = X e∈Eai ∩Eaj t∈Tevent where Tevent is the set of triggers for this event in the gold training data. If the maximum similarity is greater than some threshold, θsim , the sentence and the corresponding trigger are added to the training data. Event Trigger Identification Systems Event extraction tasks such as ACE and TAC-KBP have frequently been approached with supervised machine learning systems based on hand-crafted features, such as the system adapted from Li et al. (2013) which we make use of here. Recently, state-of-the-art results have been obtained with neural-network-based systems (Nguyen et al., 2016; Chen et al., 2015; Feng et al., 2016). Here, we make use of two systems whose implementations are publicly available and show that adding additional data would improve their performance. The first system is the joint recurrent neural net (JRNN) introduced by Nguyen et al. (2016). This model uses a bi-directional GRU layer to encode the input sentence. It then concatenates that with the vectors of words in a window around the current word, and passes the conc"
N18-2058,D13-1183,1,0.840511,"same train/development/test split as has been previously used in (Li et al., 2013), consisting of 529 training documents, 30 development documents, and a test set consisting of 40 newswire articles containing 672 sentences. For the TAC-KBP 2015 dataset, we use the official train/test split as previously used in Peng et al. (2016) consisting of 158 training documents and 202 test documents. ACE contains 33 event types, and TAC-KBP contains 38 event types. For our approach, we use a collection of news articles scraped from the web. These articles were scraped following the approach described in Zhang and Weld (2013). The process involves collecting article titles from RSS news seeds, and then querying the Bing news search with these titles to collect additional articles. This process was repeated on a daily basis between January 2013 and February 2015, resulting in approximately 70 million sentences from 8 million articles. Although the seed titles were collected during that two year period, the search results include articles from prior years with similar titles, so the articles range from 1970 to 2015. P 62.9 64.5 65.1 65.1 65.7 67.4 67.6 67.5 84.6 - ACE R 70.0 69.8 70.2 69.9 72.9 72.7 73.5 73.3 64.9 -"
N18-2058,C10-1077,0,0.182704,"ments. For event extraction, such data sources are not as easily available since there are no pre-existing stores of tuples of attacks, movements or meetings. Other work has generated additional data by using a pattern-based model of event mentions and bootstrapping on top of a small set of seed examples. Huang and Riloff (2012) begin with a set of nouns that are specific to certain event roles and extract patterns based on the contexts in which those words appear. Li et al. (2014) extracted additional patterns using various event inference mechanisms. The work most similar to ours is that of Liao and Grishman (2010, 2011) to identify articles from a corpus which described the same event instances found in training examples. These articles are then used in self-training an ACE-trained system after being filtered to select passages with consistent roles and triggers. Their method provides a 2.7 point boost to F1, but their baseline system results are much lower than ours (54.1 vs 69.1) and it is unclear what improvement their method would have on a state-of-the-art extractor. In addition, their system attempts to identify relevant articles that describe event instances already present in their training da"
N18-2058,P11-2045,0,0.350976,"Missing"
N18-2058,P14-5010,0,0.00360884,"in both datasets. This is likely due to the domain mismatch between the gold and additional data. For reference purposes, we also include the result of using the HNN model from (Feng et al., 2016) and the SSED system from Evaluation We report the micro-averaged F1 scores over all events. A trigger is considered correctly labeled if both its offsets and event type match those of a reference trigger. Implementation details For creating the automatically-generated data, we set thresholds θevent and θsim to 2 and 0.4 respectively, which were selected according to validation data. We use CoreNLP (Manning et al., 2014) for named entity recognition, and we use a pre-trained Word2Vec model (Mikolov et al., 2013) for the vector representations. For the JRNN model, we follow the parameter settings of (Nguyen et al., 2016) and use a context window of 2 for context words, and a feed-forward neural network with one hidden layer for trigger 361 Incorrect Correct clustering event identification trigger assignment 72 5 13 10 Table 2: The results of manually labeling 100 examples that were automatically-generated using JRNN as the supervised system. Incorrect clustering refers to cases in which a sentence does not cov"
N18-2058,P11-1163,0,0.420507,"Missing"
N18-2058,P09-1113,0,0.15508,"latively small number of labeled training examples available. Researchers have dealt with this by framing event extraction in a way that allows them to rely heavily on systems built for dependency parsing (McClosky et al., 2011) and semantic role labeling (Peng et al., 2016). Unlike these researchers, we join a line of work that attempts to directly harvest additional training examples for use in traditional event extraction systems. Distant supervision is one source of additional data that has been successfully applied to relation extraction tasks (Riedel et al., 2010; Hoffmann et al., 2011; Mintz et al., 2009), which align a background knowledge base to an accompanying corpus of natural language documents. For event extraction, such data sources are not as easily available since there are no pre-existing stores of tuples of attacks, movements or meetings. Other work has generated additional data by using a pattern-based model of event mentions and bootstrapping on top of a small set of seed examples. Huang and Riloff (2012) begin with a set of nouns that are specific to certain event roles and extract patterns based on the contexts in which those words appear. Li et al. (2014) extracted additional"
N18-2058,W15-0809,0,0.0226158,"fully-supervised training data and the number of additional heuristicallylabeled examples. Introduction Event extraction is a challenging task, which aims to discover event triggers in a sentence and classify them by type. Training an event extraction system requires a large dataset of annotated event triggers and their types in a sentence. Unfortunately, because of the large amount of different event types, each with its own set of annotation rules, such manual annotation is both time-consuming and expensive. As a result, popular event datasets, such as ACE (Walker et al., 2006) and TAC-KBP (Mitamura et al., 2015), are small (e.g., the median number of positive examples per subtype is only 65 and 86, respectively) and biased towards frequent event types, such as Attack. When an event occurs, there are often multiple parallel descriptions of that event (Figure 1) available somewhere on the Web due to the large number of different news sources. Some descriptions are simple, explaining in basic language the event that occurred. These are often easier for existing extraction systems to identify. Meanwhile, 1 The generated data and our code can be found at https://github.com/jferguson144/NewsCluster 359 Pro"
N18-2058,N16-1034,0,0.484256,"j ) = X e∈Eai ∩Eaj t∈Tevent where Tevent is the set of triggers for this event in the gold training data. If the maximum similarity is greater than some threshold, θsim , the sentence and the corresponding trigger are added to the training data. Event Trigger Identification Systems Event extraction tasks such as ACE and TAC-KBP have frequently been approached with supervised machine learning systems based on hand-crafted features, such as the system adapted from Li et al. (2013) which we make use of here. Recently, state-of-the-art results have been obtained with neural-network-based systems (Nguyen et al., 2016; Chen et al., 2015; Feng et al., 2016). Here, we make use of two systems whose implementations are publicly available and show that adding additional data would improve their performance. The first system is the joint recurrent neural net (JRNN) introduced by Nguyen et al. (2016). This model uses a bi-directional GRU layer to encode the input sentence. It then concatenates that with the vectors of words in a window around the current word, and passes the concatenated vectors into a feed-forward network to predict trigger types for each token. Because we are only classifying triggers, and not"
N18-2058,D16-1038,0,0.0321234,"data. Furthermore, we use an extractor trained on fullysupervised examples to filter clusters, in contrast to Zhang et al., whose method is completely unsupervised, which allows us to relax some of the assumptions made by Zhang et al. and consider “spikes” of individual entities as opposed to pairs. A challenge in event extraction is the relatively small number of labeled training examples available. Researchers have dealt with this by framing event extraction in a way that allows them to rely heavily on systems built for dependency parsing (McClosky et al., 2011) and semantic role labeling (Peng et al., 2016). Unlike these researchers, we join a line of work that attempts to directly harvest additional training examples for use in traditional event extraction systems. Distant supervision is one source of additional data that has been successfully applied to relation extraction tasks (Riedel et al., 2010; Hoffmann et al., 2011; Mintz et al., 2009), which align a background knowledge base to an accompanying corpus of natural language documents. For event extraction, such data sources are not as easily available since there are no pre-existing stores of tuples of attacks, movements or meetings. Other"
N19-1238,N18-3011,0,0.18965,"ins, and often provide rich annotations of relationships that extend beyond the scope of 2284 Proceedings of NAACL-HLT 2019, pages 2284–2293 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics a single sentence. But due to their automatic nature, they also introduce challenges for generation such as erroneous annotations, structural variety, and significant abstraction of surface textual features (such as grammatical relations or predicateargument structure). To effect our study, we use a collection of abstracts from a corpus of scientific articles (Ammar et al., 2018). We extract entity, coreference, and relation annotations for each abstract with a stateof-the-art information extraction system (Luan et al., 2018), and represent the annotations as a knowledge graph which collapses co-referential entities. An example of a text and graph are shown in Figure 1. We use these graph/text pairs to train a novel attention-based encoder-decoder model for knowledge-graph-to-text generation. Our model, GraphWriter, extends the successful Transformer for text encoding (Vaswani et al., 2017) to graphstructured inputs, building on the recent Graph Attention Network arch"
N19-1238,H05-1042,1,0.689082,"nowledgegraphs paired with scientific texts for further study. Through detailed automatic and human evaluations, we demonstrate that automatically extracted knowledge can be used for multi-sentence text generation. We further show that structuring and encoding this knowledge as a graph leads to improved generation performance compared to other encoder-decoder setups. Finally, we show that GraphWriter’s transformer-style encoder is more effective than Graph Attention Networks on the knowledge-graph-to-text task. 2 Related Work Our work falls under the larger scope of conceptto-text generation. Barzilay and Lapata (2005) introduced a collective content selection model for generating summaries of football games from tables of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach. These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but r"
N19-1238,P18-1026,0,0.280878,"Veliˇckovi´c et al. (2018), a direct descendant of the convolutional approach which offers more modeling power and has been 2285 Vocab Tokens Entities Avg Length Avg #Vertices Avg #Edges Title 29K 413K 9.9 - Abstract 77K 5.8M 141.2 - KG 54K 1.2M 518K 12.42 4.43 Table 1: Data statistics of our AGENDA dataset. Averages are computed per instance. shown to improve performance. Song et al. (2018) uses a graph LSTM model to effect information propagation. At each timestep, a vertex is represented by a gated combination of the vertices to which it is connected and the labeled edges connecting them. Beck et al. (2018) use a similar gated graph neural network. Both of these gated models make heavy use of label information, which is much sparser in our knowledge graphs than in AMR. Generally, AMR graphs are denser, rooted, and connected, whereas the knowledge our model works with lacks these characteristics. For this reason, we focus on attention-based models such as Veliˇckovi´c et al. (2018), which impose fewer constraints on their input. Finally, our work is related to Wang et al. (2018) who offer a method for generating scientific abstracts from titles. Their model uses a gated rewriter network to write"
N19-1238,W14-3348,0,0.0162431,"e provided examples of good and bad abstracts and explain how they succeed or fail to meet the defined criteria. Because our dataset is scientific in nature, evaluations must be done by experts and we can only collect a limited number of these high quality datapoints.2 The study was conducted by 15 experts (i.e. computer science students) who were familiar with the abstract writing task and the content of the abstracts they judged. To supplement this, we also provide automatic metrics. We use BLEU (Papineni et al., 2002), an n-gram overlap measure popular in text generation tasks, and METEOR (Denkowski and Lavie, 2014), a machine translation with paraphrase and language-specific considerations. Comparisons We compare our GraphWriter against several strong baselines. In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of (Veliˇckovi´c et al., 2018). This encoder consists of PReLU activations stacked between 6 self-attention layers. To determine the usefulness of including graph relations, we compare to a model which uses only entities and title (EntityWriter). Finally, we compare with the gated rewriter model of Wang et al. (2018) (Rewriter). This model uses only the document titl"
N19-1238,C10-2062,0,0.019601,"ledge as a graph leads to improved generation performance compared to other encoder-decoder setups. Finally, we show that GraphWriter’s transformer-style encoder is more effective than Graph Attention Networks on the knowledge-graph-to-text task. 2 Related Work Our work falls under the larger scope of conceptto-text generation. Barzilay and Lapata (2005) introduced a collective content selection model for generating summaries of football games from tables of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach. These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but recently Mei et al. (2016) showed how neural models can achieve strong results on these standards, prompting researchers to investigate more challenging domains such as ours. Lebret et al. (2016) tackles the task of generating the first sentence of a Wikipedia entry"
N19-1238,N16-1095,0,0.014676,"1−p probability is given to αvocab , which is calculated by scaling [ht kct ] to the vocabulary size and taking a softmax. 5 Experiments Evaluation Metrics We evaluate using a combination of human and automatic evaluations. For human evaluation, participants were asked to compare abstracts generated by various models and those written by the authors of the scientific articles. We used Best-Worst Scaling (BWS; (Louviere and Woodworth, 1991; Louviere et al., 2015)), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2016). Participants were presented with two or three abstracts and asked to decide which one was better and which one was worse in order of grammar and fluency (is the abstract written in well-formed English?), coherence (does the abstract have an introduction, state the problem or task, describe a solution, and discuss evaluations or results?), and informativeness (does the abstract relate to the provided title and make use of appropriate scientific terms?). We provided examples of good and bad abstracts and explain how they succeed or fail to meet the defined criteria. Because our dataset is scie"
N19-1238,P17-1014,0,0.0733805,"al models to the data-to-text task. They introduce a large dataset where a text summary of a basketball game is paired with two tables of relevant statistics and show that neural models struggle to compete with template based methods over this data. We propose generating from graphs rather than tables, and show that graphs can be effectively encoded to capture both local and global structure in the input. We show that modeling knowledge as a graph improves generation results, connecting our work to other graph-to-text tasks such as generating from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural model for this task, and show that pretraining on a large dataset of noisy automatic parses can improve results. However, they do not directly model the graph structure, relying on linearization and sequence encoding instead. Current works improve this through more sophisticated graph encoding techniques. Marcheggiani and Perez-Beltrachini (2018) encode input graphs directly using a graph convolution encoder (Kipf and Welling, 2017). Our model extends the graph attention networks of Veliˇckovi´c et al. (2018), a direct descendant of the convolutional approach which of"
N19-1238,D13-1157,1,0.957938,"h strings of natural language text. However, generating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge. The difficulties are compounded in domains of interest such as scientific writing. Here the variety of possible topics is great (e.g. topics as diverse as driving, writing poetry, and picking stocks are all referenced in one subfield of 1 Data and code available at https://github.com/ rikdz/GraphWriter Many researchers have sought to address these issues by working with structured inputs. Data-totext generation models (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Puduppully et al., 2019) condition text generation on table-structured inputs. Tabular input representations provide more guidance for producing longer texts, but are only available for limited domains as they are assembled at great expense by manual annotation processes. The current work explores the possibility of using information extraction (IE) systems to automatically provide context for generating longer texts (Figure 1). Robust IE systems are available and have support over a large variety of textual domains, and often provide rich annotatio"
N19-1238,D16-1128,0,0.438369,"age text. However, generating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge. The difficulties are compounded in domains of interest such as scientific writing. Here the variety of possible topics is great (e.g. topics as diverse as driving, writing poetry, and picking stocks are all referenced in one subfield of 1 Data and code available at https://github.com/ rikdz/GraphWriter Many researchers have sought to address these issues by working with structured inputs. Data-totext generation models (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Puduppully et al., 2019) condition text generation on table-structured inputs. Tabular input representations provide more guidance for producing longer texts, but are only available for limited domains as they are assembled at great expense by manual annotation processes. The current work explores the possibility of using information extraction (IE) systems to automatically provide context for generating longer texts (Figure 1). Robust IE systems are available and have support over a large variety of textual domains, and often provide rich annotations of relationships t"
N19-1238,P18-1150,0,0.0951155,"e sophisticated graph encoding techniques. Marcheggiani and Perez-Beltrachini (2018) encode input graphs directly using a graph convolution encoder (Kipf and Welling, 2017). Our model extends the graph attention networks of Veliˇckovi´c et al. (2018), a direct descendant of the convolutional approach which offers more modeling power and has been 2285 Vocab Tokens Entities Avg Length Avg #Vertices Avg #Edges Title 29K 413K 9.9 - Abstract 77K 5.8M 141.2 - KG 54K 1.2M 518K 12.42 4.43 Table 1: Data statistics of our AGENDA dataset. Averages are computed per instance. shown to improve performance. Song et al. (2018) uses a graph LSTM model to effect information propagation. At each timestep, a vertex is represented by a gated combination of the vertices to which it is connected and the labeled edges connecting them. Beck et al. (2018) use a similar gated graph neural network. Both of these gated models make heavy use of label information, which is much sparser in our knowledge graphs than in AMR. Generally, AMR graphs are denser, rooted, and connected, whereas the knowledge our model works with lacks these characteristics. For this reason, we focus on attention-based models such as Veliˇckovi´c et al. (2"
N19-1238,P09-1011,0,0.0929414,"racted knowledge can be used for multi-sentence text generation. We further show that structuring and encoding this knowledge as a graph leads to improved generation performance compared to other encoder-decoder setups. Finally, we show that GraphWriter’s transformer-style encoder is more effective than Graph Attention Networks on the knowledge-graph-to-text task. 2 Related Work Our work falls under the larger scope of conceptto-text generation. Barzilay and Lapata (2005) introduced a collective content selection model for generating summaries of football games from tables of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach. These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but recently Mei et al. (2016) showed how neural models can achieve strong results on these standards, prompting researchers to investigate more cha"
N19-1238,P16-1008,0,0.0321171,"Missing"
N19-1238,D18-1360,1,0.917823,"olis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics a single sentence. But due to their automatic nature, they also introduce challenges for generation such as erroneous annotations, structural variety, and significant abstraction of surface textual features (such as grammatical relations or predicateargument structure). To effect our study, we use a collection of abstracts from a corpus of scientific articles (Ammar et al., 2018). We extract entity, coreference, and relation annotations for each abstract with a stateof-the-art information extraction system (Luan et al., 2018), and represent the annotations as a knowledge graph which collapses co-referential entities. An example of a text and graph are shown in Figure 1. We use these graph/text pairs to train a novel attention-based encoder-decoder model for knowledge-graph-to-text generation. Our model, GraphWriter, extends the successful Transformer for text encoding (Vaswani et al., 2017) to graphstructured inputs, building on the recent Graph Attention Network architecture (Veliˇckovi´c et al., 2018). The result is a powerful, general model for graph encoding which can incorporate global structural information"
N19-1238,D15-1166,0,0.0334909,"r copying input from the knowledge graph and title. At each decoding timestep t we use decoder hidden state ht to compute context vectors cg and cs for the graph and 2288 title sequence respectively. cg is computed using multi-headed attention contextualized by ht : c g = ht + N n X n L αjn WG v j (6) n=1 j∈V αj = a(ht , vL j ) (7) for a as described in Equation (1) by attending over the graph contextualized encodings VL . cs is computed similarly, attending over the title encoding T. We then construct the final context vector by concatenation, ct = [cg kcs ]. We use an input-feeding decoder (Luong et al., 2015) where both ht and ct are passed as input to the next RNN timestep. We compute a probability p of copying from the input using ht and ct in a fashion similar to See et al. (2017), that is: p = σ(Wcopy [ht kct ] + bcopy ) (8) The final next-token probability distribution is: p ∗ αcopy + (1 − p) ∗ αvocab , (9) Where the probability distribution αcopy over entities and input tokens is computed as αjcopy = a([ht kct ], xj ) for xj ∈ VkT. The remaining 1−p probability is given to αvocab , which is calculated by scaling [ht kct ] to the vocabulary size and taking a softmax. 5 Experiments Evaluation"
N19-1238,W18-6501,0,0.171392,"capture both local and global structure in the input. We show that modeling knowledge as a graph improves generation results, connecting our work to other graph-to-text tasks such as generating from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural model for this task, and show that pretraining on a large dataset of noisy automatic parses can improve results. However, they do not directly model the graph structure, relying on linearization and sequence encoding instead. Current works improve this through more sophisticated graph encoding techniques. Marcheggiani and Perez-Beltrachini (2018) encode input graphs directly using a graph convolution encoder (Kipf and Welling, 2017). Our model extends the graph attention networks of Veliˇckovi´c et al. (2018), a direct descendant of the convolutional approach which offers more modeling power and has been 2285 Vocab Tokens Entities Avg Length Avg #Vertices Avg #Edges Title 29K 413K 9.9 - Abstract 77K 5.8M 141.2 - KG 54K 1.2M 518K 12.42 4.43 Table 1: Data statistics of our AGENDA dataset. Averages are computed per instance. shown to improve performance. Song et al. (2018) uses a graph LSTM model to effect information propagation. At eac"
N19-1238,P18-2042,0,0.248995,"x is represented by a gated combination of the vertices to which it is connected and the labeled edges connecting them. Beck et al. (2018) use a similar gated graph neural network. Both of these gated models make heavy use of label information, which is much sparser in our knowledge graphs than in AMR. Generally, AMR graphs are denser, rooted, and connected, whereas the knowledge our model works with lacks these characteristics. For this reason, we focus on attention-based models such as Veliˇckovi´c et al. (2018), which impose fewer constraints on their input. Finally, our work is related to Wang et al. (2018) who offer a method for generating scientific abstracts from titles. Their model uses a gated rewriter network to write and revise several draft outputs in several sequence-to-sequence steps. While we operate in the same general domain as this work, our task setup is ultimately different due to the use of extracted information as input. We argue that our setup improves the task defined in Wang et al. (2018), and our more general model can be applied across tasks and domains. 3 The AGENDA Dataset We consider the problem of generating a text from automatically extracted information (knowledge)."
N19-1238,N16-1086,0,0.0278393,"ced a collective content selection model for generating summaries of football games from tables of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach. These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but recently Mei et al. (2016) showed how neural models can achieve strong results on these standards, prompting researchers to investigate more challenging domains such as ours. Lebret et al. (2016) tackles the task of generating the first sentence of a Wikipedia entry from the associated infobox. They provide a large dataset of such entries and a language model conditioned on tables. Our work focuses on a multi-sentence task where relations can extend beyond sentence boundaries. Wiseman et al. (2017) study the difficulty of applying neural models to the data-to-text task. They introduce a large dataset where a text summa"
N19-1238,D17-1239,0,0.221143,"nerating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge. The difficulties are compounded in domains of interest such as scientific writing. Here the variety of possible topics is great (e.g. topics as diverse as driving, writing poetry, and picking stocks are all referenced in one subfield of 1 Data and code available at https://github.com/ rikdz/GraphWriter Many researchers have sought to address these issues by working with structured inputs. Data-totext generation models (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Puduppully et al., 2019) condition text generation on table-structured inputs. Tabular input representations provide more guidance for producing longer texts, but are only available for limited domains as they are assembled at great expense by manual annotation processes. The current work explores the possibility of using information extraction (IE) systems to automatically provide context for generating longer texts (Figure 1). Robust IE systems are available and have support over a large variety of textual domains, and often provide rich annotations of relationships that extend beyond the"
N19-1238,P02-1040,0,0.104479,"does the abstract relate to the provided title and make use of appropriate scientific terms?). We provided examples of good and bad abstracts and explain how they succeed or fail to meet the defined criteria. Because our dataset is scientific in nature, evaluations must be done by experts and we can only collect a limited number of these high quality datapoints.2 The study was conducted by 15 experts (i.e. computer science students) who were familiar with the abstract writing task and the content of the abstracts they judged. To supplement this, we also provide automatic metrics. We use BLEU (Papineni et al., 2002), an n-gram overlap measure popular in text generation tasks, and METEOR (Denkowski and Lavie, 2014), a machine translation with paraphrase and language-specific considerations. Comparisons We compare our GraphWriter against several strong baselines. In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of (Veliˇckovi´c et al., 2018). This encoder consists of PReLU activations stacked between 6 self-attention layers. To determine the usefulness of including graph relations, we compare to a model which uses only entities and title (EntityWriter). Finally, we compare wi"
N19-1238,P17-1099,0,0.0956651,"uence respectively. cg is computed using multi-headed attention contextualized by ht : c g = ht + N n X n L αjn WG v j (6) n=1 j∈V αj = a(ht , vL j ) (7) for a as described in Equation (1) by attending over the graph contextualized encodings VL . cs is computed similarly, attending over the title encoding T. We then construct the final context vector by concatenation, ct = [cg kcs ]. We use an input-feeding decoder (Luong et al., 2015) where both ht and ct are passed as input to the next RNN timestep. We compute a probability p of copying from the input using ht and ct in a fashion similar to See et al. (2017), that is: p = σ(Wcopy [ht kct ] + bcopy ) (8) The final next-token probability distribution is: p ∗ αcopy + (1 − p) ∗ αvocab , (9) Where the probability distribution αcopy over entities and input tokens is computed as αjcopy = a([ht kct ], xj ) for xj ∈ VkT. The remaining 1−p probability is given to αvocab , which is calculated by scaling [ht kct ] to the vocabulary size and taking a softmax. 5 Experiments Evaluation Metrics We evaluate using a combination of human and automatic evaluations. For human evaluation, participants were asked to compare abstracts generated by various models and tho"
N19-1245,N16-1136,1,0.890692,"nsolvable without brute-force enumeration of solutions, and rationales that contain few or none of the steps required to solve the corresponding problem. The motivation for our dataset comes from the fact we want to maintain the challenging nature of the problems included in the AQuA dataset, while removing noise that hinders the ability of neuralized models to learn the types of signal neccessary for problem-solving by logical reasoning. Additional Datasets Several smaller datasets have been compiled in recent years. Most of these works have focused on algebra word problems, including MaWPS (Koncel-Kedziorski et al., 2016), Alg514 (Kushman et al., 2014), and DRAW-1K (Upadhyay and Chang, 2017). Many of these datasets have sought to align underlying equations or systems of equations with word problem text. While recent works like (Liang et al., 2018; Locascio et al., 2016) have explored representing math word problems with logical formalisms and regular expressions, our work is the first to provide well-defined formalisms for representing intermediate problem-solving steps that are shown to be generalizable beyond algebra problems. Solving with Handcrafted Features Due to sparsity of suitable data, early work on"
N19-1245,P14-1026,0,0.419077,"a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017). We introduce a neural model for mapping problems to operation programs with domain categorization. 1 The dataset is available at: https://math-qa. github.io/math-QA/ 2357 Proceedings of NAACL-HLT 2019, pages 2357–2367 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Most current datasets in this domain are small in scale (Kushman et al., 2014) or do not offer precise operational annotations over diverse problem types (Ling et al., 2017). This is mainly due to the fact that annotating math word problems precisely across diverse problem categories is challenging even for humans, requiring background math knowledge for annotators. Our representation language facilitates the annotation task for crowd-sourcing and increases the interpretability of the proposed model. Our sequence-to-program model with categorization trained on our MathQA dataset outperforms previous state-of-the-art on the AQuA test set in spite of the smaller training"
N19-1245,P17-1005,0,0.0268874,"ould make use of both math knowledge and 2 Here the arguments 174, 254 and 349 are the outputs of operations 1, 2 and 3 respectively. 2359 Category Geometry Physics Probability Gain-Loss General Other All domain knowledge associated with subfields like geometry and probability to determine which operations and arguments to use. • Human interpretability → Each operation and argument used to obtain the correct solution should relate to part of the input word problem context or a previous step in the operation program. Learning logical forms has led to success in other areas of semantic parsing (Cheng et al., 2017; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007, 2005) and is a natural representation for math word problem-solving steps. By augmenting our dataset with these formalisms, we are able to cover most types of math word problems3 . In contrast to other representations like simultaneous equations, our formalisms ensure that every problem-solving step is aligned to a previous one. There are three advantages to this approach. First, we use this representation language to provide human annotators with clear steps for how a particular problem should be solved with math and domain knowledge. S"
N19-1245,W14-4012,0,0.0498136,"Missing"
N19-1245,N18-1060,0,0.0583799,"ature of the problems included in the AQuA dataset, while removing noise that hinders the ability of neuralized models to learn the types of signal neccessary for problem-solving by logical reasoning. Additional Datasets Several smaller datasets have been compiled in recent years. Most of these works have focused on algebra word problems, including MaWPS (Koncel-Kedziorski et al., 2016), Alg514 (Kushman et al., 2014), and DRAW-1K (Upadhyay and Chang, 2017). Many of these datasets have sought to align underlying equations or systems of equations with word problem text. While recent works like (Liang et al., 2018; Locascio et al., 2016) have explored representing math word problems with logical formalisms and regular expressions, our work is the first to provide well-defined formalisms for representing intermediate problem-solving steps that are shown to be generalizable beyond algebra problems. Solving with Handcrafted Features Due to sparsity of suitable data, early work on math word problem solving used pattern-matching to map word problems to mathematical expressions (Bobrow, 1964; Charniak, 1968, 1969), as well as non-neural statistical modeling and semantic parsing approaches (Liguda and Pfeiffe"
N19-1245,P17-1015,0,0.0617703,"ation program underlying the problem in Figure 1 highlights the complexity of the problem-solving task. Here, we need the ability to deduce implied constants (pi) and knowledge of domain-specific formulas (area of the square). In this paper, we introduce a new operationbased representation language for solving math word problems. We use this representation language to construct MathQA1 , a new large-scale, diverse dataset of 37k English multiple-choice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset (Ling et al., 2017). We introduce a neural model for mapping problems to operation programs with domain categorization. 1 The dataset is available at: https://math-qa. github.io/math-QA/ 2357 Proceedings of NAACL-HLT 2019, pages 2357–2367 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Most current datasets in this domain are small in scale (Kushman et al., 2014) or do not offer precise operational annotations over diverse problem types (Ling et al., 2017). This is mainly due to the fact that annotating math word problems precisely across diverse problem categories"
N19-1245,D16-1197,0,0.0206349,"s included in the AQuA dataset, while removing noise that hinders the ability of neuralized models to learn the types of signal neccessary for problem-solving by logical reasoning. Additional Datasets Several smaller datasets have been compiled in recent years. Most of these works have focused on algebra word problems, including MaWPS (Koncel-Kedziorski et al., 2016), Alg514 (Kushman et al., 2014), and DRAW-1K (Upadhyay and Chang, 2017). Many of these datasets have sought to align underlying equations or systems of equations with word problem text. While recent works like (Liang et al., 2018; Locascio et al., 2016) have explored representing math word problems with logical formalisms and regular expressions, our work is the first to provide well-defined formalisms for representing intermediate problem-solving steps that are shown to be generalizable beyond algebra problems. Solving with Handcrafted Features Due to sparsity of suitable data, early work on math word problem solving used pattern-matching to map word problems to mathematical expressions (Bobrow, 1964; Charniak, 1968, 1969), as well as non-neural statistical modeling and semantic parsing approaches (Liguda and Pfeiffer, 2012). Some effort ha"
N19-1245,Q18-1012,0,0.0143044,"lient entities (Hosseini et al., 2017). This approach views entities as containers, which can be composed into an equation tree representation. The equation tree representation is changed over time by operations implied by the problem text. Many early works focused on solving addition and subtraction problems (Briars and Larkin, 1984; Dellarosa, 1986; Bakman, 2007). As word problems become more diverse and complex, we require models capable of solving simultaneous equation systems. This has led to an increasing focus on finding semantic alignment of math word problems and mentions of numbers (Roy and Roth, 2018). The main idea behind those work is to find all possible patterns of equations and rank 2358 them based on the problem. Context and Question Neural Word Problem Solvers Following the increasing availability of large-scale datasets like AQuA, several recent works have explored deep neural approaches to math word problem solving (Wang et al., 2017). Our representation language is motivated by exploration of using intermediate formalisms in the training of deep neural problem-solving networks, as is done in the work of (Huang et al., 2018b) to solve problems with sequence to sequence models. Whi"
N19-1245,C18-1018,0,0.0597624,"alignment of math word problems and mentions of numbers (Roy and Roth, 2018). The main idea behind those work is to find all possible patterns of equations and rank 2358 them based on the problem. Context and Question Neural Word Problem Solvers Following the increasing availability of large-scale datasets like AQuA, several recent works have explored deep neural approaches to math word problem solving (Wang et al., 2017). Our representation language is motivated by exploration of using intermediate formalisms in the training of deep neural problem-solving networks, as is done in the work of (Huang et al., 2018b) to solve problems with sequence to sequence models. While this work focused on single-variable arithmetic problems, our work introduces a formal language of operations for covering more complex multivariate problems and systems of equations. Interpretability of Solvers While the statistical models with handcrafted features introduced by prior work are arguably “interpretable” due to the relative sparsity of features as well as the clear alignments between inputs and outputs, new neuralized approaches present new challenges to model interpretability of math word problem solvers (Huang et al."
N19-1245,E17-1047,0,0.0179992,"ontain few or none of the steps required to solve the corresponding problem. The motivation for our dataset comes from the fact we want to maintain the challenging nature of the problems included in the AQuA dataset, while removing noise that hinders the ability of neuralized models to learn the types of signal neccessary for problem-solving by logical reasoning. Additional Datasets Several smaller datasets have been compiled in recent years. Most of these works have focused on algebra word problems, including MaWPS (Koncel-Kedziorski et al., 2016), Alg514 (Kushman et al., 2014), and DRAW-1K (Upadhyay and Chang, 2017). Many of these datasets have sought to align underlying equations or systems of equations with word problem text. While recent works like (Liang et al., 2018; Locascio et al., 2016) have explored representing math word problems with logical formalisms and regular expressions, our work is the first to provide well-defined formalisms for representing intermediate problem-solving steps that are shown to be generalizable beyond algebra problems. Solving with Handcrafted Features Due to sparsity of suitable data, early work on math word problem solving used pattern-matching to map word problems to"
N19-1245,P16-1084,0,0.0448286,"ems that are densely annotated with operation programs • We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. • We introduce a neural architecture leveraging a sequence-to-program model with automatic problem categorization, achieving competitive results on our dataset as well as the AQuA dataset 2 Background and Related Work Large-Scale Datasets Several large-scale math word problem datasets have been released in recent years. These include Dolphin18K (Huang et al., 2016), Math23K (Wang et al., 2017) and AQuA. We choose the 2017 AQUA-RAT dataset to demonstrate use of our representation language on an existing large-scale math word problem solving dataset. The AQuA provides over 100K GRE- and GMAT-level math word problems. The problems are multiple choice and come from a wide range of domains. The scale and diversity of this dataset makes it particularly suited for use in training deeplearning models to solve word problems. However there is a significant amount of unwanted noise in the dataset, including problems with incorrect solutions, problems that are unso"
N19-1245,D17-1088,0,0.181866,"Missing"
N19-1245,P18-1039,0,0.014109,"alignment of math word problems and mentions of numbers (Roy and Roth, 2018). The main idea behind those work is to find all possible patterns of equations and rank 2358 them based on the problem. Context and Question Neural Word Problem Solvers Following the increasing availability of large-scale datasets like AQuA, several recent works have explored deep neural approaches to math word problem solving (Wang et al., 2017). Our representation language is motivated by exploration of using intermediate formalisms in the training of deep neural problem-solving networks, as is done in the work of (Huang et al., 2018b) to solve problems with sequence to sequence models. While this work focused on single-variable arithmetic problems, our work introduces a formal language of operations for covering more complex multivariate problems and systems of equations. Interpretability of Solvers While the statistical models with handcrafted features introduced by prior work are arguably “interpretable” due to the relative sparsity of features as well as the clear alignments between inputs and outputs, new neuralized approaches present new challenges to model interpretability of math word problem solvers (Huang et al."
N19-1245,D07-1071,0,0.0165843,"ere the arguments 174, 254 and 349 are the outputs of operations 1, 2 and 3 respectively. 2359 Category Geometry Physics Probability Gain-Loss General Other All domain knowledge associated with subfields like geometry and probability to determine which operations and arguments to use. • Human interpretability → Each operation and argument used to obtain the correct solution should relate to part of the input word problem context or a previous step in the operation program. Learning logical forms has led to success in other areas of semantic parsing (Cheng et al., 2017; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007, 2005) and is a natural representation for math word problem-solving steps. By augmenting our dataset with these formalisms, we are able to cover most types of math word problems3 . In contrast to other representations like simultaneous equations, our formalisms ensure that every problem-solving step is aligned to a previous one. There are three advantages to this approach. First, we use this representation language to provide human annotators with clear steps for how a particular problem should be solved with math and domain knowledge. Second, our formalisms provide neural models with a cont"
N19-1245,D14-1058,1,\N,Missing
N19-1308,D18-1307,0,0.102224,"Missing"
N19-1308,P18-2014,0,0.0234032,"g systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations (Peng et al., 2017; Song et al., 2018; Zhang et al., 2018). Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the mode"
N19-1308,Q14-1037,0,0.0405465,"ght benefit another. Most previous work in IE (e.g., (Nadeau and Sekine, 2007; Chan and Roth, 2011)) employs a pipeline approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating a benefit from incorporat"
N19-1308,D13-1029,1,0.80646,"learned from one task might benefit another. Most previous work in IE (e.g., (Nadeau and Sekine, 2007; Chan and Roth, 2011)) employs a pipeline approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating"
N19-1308,P18-2058,1,0.867573,"hristopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018)"
N19-1308,N18-1079,0,0.0616091,"ns that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs"
N19-1308,N18-2016,0,0.25756,"Missing"
N19-1308,N16-1030,0,0.0666976,"r to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over"
N19-1308,D17-1018,1,0.941791,"al., 2017; Song et al., 2018; Zhang et al., 2018). Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning"
N19-1308,N18-2108,1,0.938635,"end points of si , an attention-based soft “headword,” and an embedded span width feature, following Lee et al. (2017). Coreference Propagation Layer The propagation process starts from the span representations gi0 . At each iteration t, we first compute an update vector utC for each span si . Then we use utC to update the current representation git , producing the next span representation git+1 . By repeating this process N times, the final span representations giN share contextual information across spans that are likely to be antecedents in the coreference graph, similar to the process in (Lee et al., 2018). Relation Propagation Layer The outputs giN from the coreference propagation layer are passed as inputs to the relation propagation layer. Similar to the coreference propagation process, at each iteration t, we first compute the update vectors utR for each span si , then use it to compute git+1 . Information can be integrated from multiple relation paths by repeating this process M times. Final Prediction Layer We use the outputs of the relation graph layer giN +M to predict the entity labels E and relation labels R. For entities, we pass giN +M to a feed-forward network (FFNN) to 3038 Final"
N19-1308,P14-1038,0,0.10148,"fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the interaction between tasks through dynamic graph propagation. While the performance of DyGIE benefits from ELMo, it advances over some systems (Luan et al., 2018a; Sanh et al., 2019) that also inc"
N19-1308,I17-1061,1,0.845416,"18a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on"
N19-1308,D18-1360,1,0.148181,"ne approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating a benefit from incorporating broader context learned from relation and coreference annotations. Figure 1 shows an example illustrating"
N19-1308,D17-1279,1,0.868329,"18a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on"
N19-1308,S18-1125,1,0.146415,"ne approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and coreferences. Our framework is effective in several domains, demonstrating a benefit from incorporating broader context learned from relation and coreference annotations. Figure 1 shows an example illustrating"
N19-1308,P16-1101,0,0.073951,"in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional feature"
N19-1308,P16-1105,0,0.500436,"the same entity into one cluster. Thus, we might expect that knowledge learned from one task might benefit another. Most previous work in IE (e.g., (Nadeau and Sekine, 2007; Chan and Roth, 2011)) employs a pipeline approach, first detecting entities and then using the detected entity spans for relation extraction and coreference resolution. To avoid cascading 1 Code and pre-trained models are publicly available at https://github.com/luanyi/DyGIE. errors introduced by pipeline-style systems, recent work has focused on coupling different IE tasks as in joint modeling of entities and relations (Miwa and Bansal, 2016; Zhang et al., 2017), entities and coreferences (Hajishirzi et al., 2013; Durrett and Klein, 2014), joint inference (Singh et al., 2013) or multi-task (entity/relation/coreference) learning (Luan et al., 2018a). These models mostly rely on the first layer LSTM to share span representations between different tasks and are usually designed for specific domains. In this paper, we introduce a general framework Dynamic Graph IE (DY GIE) for coupling multiple information extraction tasks through shared span representations which are refined leveraging contextualized information from relations and c"
N19-1308,D15-1064,0,0.0263659,"demonstrate that our framework significantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Protocol Corpus. 3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA. 2 Related Work Previous studies have explored joint modeling (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et al., 2013; Yang and Mitchell, 2016)) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a, 2017a) as methods to share representational strength across related information extraction tasks. The most similar to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recogni"
N19-1308,Q17-1008,0,0.180357,"amework significantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Protocol Corpus. 3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA. 2 Related Work Previous studies have explored joint modeling (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et al., 2013; Yang and Mitchell, 2016)) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a, 2017a) as methods to share representational strength across related information extraction tasks. The most similar to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly b"
N19-1308,D14-1162,0,0.0860569,"from neighboring relation types and co-referred entities. These refined span representations are used in a multi-task framework to predict entity types, relation types, and coreference links. 3.1 Model Architecture In this section, we give an overview of the main components and layers of the DY GIE framework, as illustrated in Figure 2. Details of the graph construction and refinement process will be presented in the next section. Token Representation Layer We apply a bidirectional LSTM over the input tokens. The input for each token is a concatenation of the character reprensetation, GLoVe (Pennington et al., 2014) word embeddings, and ELMo embeddings (Peters et al., 2018). The output token representations are obtained by stacking the forward and backward LSTM hidden states. Span Representation Layer For each span si , its initial vector representation gi0 is obtained by concatenating BiLSTM outputs at the left and right end points of si , an attention-based soft “headword,” and an embedded span width feature, following Lee et al. (2017). Coreference Propagation Layer The propagation process starts from the span representations gi0 . At each iteration t, we first compute an update vector utC for each sp"
N19-1308,N18-1202,0,0.400911,"ng (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the interaction between tasks through dynamic graph propagation. While the performance of DyGIE benefits from ELMo, it advances over some systems (Luan et al., 2018a; Sanh et al., 2019) that also incorporate ELMo. The analyses presented here give insights into the benefits of joint modeling. 3 Model Problem Definition The input is a document represented as a sequence of words D, from which we derive S = {s1 , . . . , sT }, the set of all possible within-sentence word sequence spans (u"
N19-1308,W12-4501,0,0.182999,"Missing"
N19-1308,W16-1300,0,0.149812,"Missing"
N19-1308,D18-1246,0,0.018417,"presentations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations (Peng et al., 2017; Song et al., 2018; Zhang et al., 2018). Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and"
N19-1308,D18-1019,0,0.141103,"er context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. Entity recognition has commonly been cast as a sequence labeling problem, and has benefited substantially from the use of neural architectures (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Luan et al., 2017b, 2018b). However, most systems based on sequence labeling suffer from an inability to extract entities with overlapping spans. Recently Katiyar and Cardie (2018) and Wang and Lu (2018) have presented methods enabling neural models to extract overlapping entities, applying hypergraph-based representations on top of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs have been used primaril"
N19-1308,D15-1062,0,0.0312111,"the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018), the model uses a beam pruning strategy to dynamically select high-quality spans, and constructs a graph using the selected spans as nodes. 3037 Many state-of-the-art RE models rely upon domain-specific external syntactic tools to construct dependency paths between the entities in a sentence (Li and Ji, 2014; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017). These systems suffer from cascading errors from these tools and are hard to generalize to different domains. To make the model more general, we combine the multitask learning framework with ELMo embeddings (Peters et al., 2018) without relying on external syntactic tools and risking the cascading errors that accompany them, and improve the interaction between tasks through dynamic graph propagation. While the performance of DyGIE benefits from ELMo, it advances over some systems (Luan et al., 2018a; Sanh et al., 2019) that also incorporate ELMo. Th"
N19-1308,N16-1033,0,0.0664368,"rmation, making the code publicly available. 2) We demonstrate that our framework significantly outperforms the state-of-the-art on joint entity and relation detection tasks across four datasets: ACE 2004, ACE 2005, SciERC and the Wet Lab Protocol Corpus. 3) We further show that our approach excels at detecting entities with overlapping spans, achieving an improvement of up to 8 F1 points on three benchmarks annotated with overlapped spans: ACE 2004, ACE 2005 and GENIA. 2 Related Work Previous studies have explored joint modeling (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et al., 2013; Yang and Mitchell, 2016)) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a, 2017a) as methods to share representational strength across related information extraction tasks. The most similar to ours is the work in Luan et al. (2018a) that takes a multi-task learning approach to entity, relation, and coreference extraction. In this model, the different tasks share span representations that only incorporate broader context indirectly via the gradients passed back to the LSTM layer. In contrast, DY GIE uses dynamic graph propagation to explicitly incorporate rich contextual informati"
N19-1308,D17-1182,0,0.0957664,"Missing"
N19-1308,D18-1244,0,0.0315507,"p of sequence labeling systems. Our framework offers an alternative approach, forgoing sequence labeling entirely and simply considering all possible spans as candidate entities. Neural graph-based models have achieved significant improvements over traditional featurebased approaches on several graph modeling tasks. Knowledge graph completion (Yang et al., 2015; Bordes et al., 2013) is one prominent example. For relation extraction tasks, graphs have been used primarily as a means to incorporate pipelined features such as syntactic or discourse relations (Peng et al., 2017; Song et al., 2018; Zhang et al., 2018). Christopoulou et al. (2018) models all possible paths between entities as a graph, and refines pair-wise embeddings by performing a walk on the graph structure. All these previous works assume that the nodes of the graph (i.e. the entity candidates to be considered during relation extraction) are predefined and fixed throughout the learning process. On the other hand, our framework does not require a fixed set of entity boundaries as an input for graph construction. Motivated by state-ofthe-art span-based approaches to coreference resolution (Lee et al., 2017, 2018) and semantic role labelin"
P16-1167,N04-1015,0,0.0552637,"STM captions having little caption variation because the model learns frequency statistics without any knowledge of latent events. Almost all LSTM captions mention the words bride, wedding, or groom, yielding a very high scenario score for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw captions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario. 7 Related Work Previous studies have explored unsupervised induction of salient content structure in newswire texts (Barzilay and Lee, 2004), temporal graph representations (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Moon"
P16-1167,W06-1623,0,0.0346578,"model learns frequency statistics without any knowledge of latent events. Almost all LSTM captions mention the words bride, wedding, or groom, yielding a very high scenario score for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw captions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario. 7 Related Work Previous studies have explored unsupervised induction of salient content structure in newswire texts (Barzilay and Lee, 2004), temporal graph representations (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 200"
P16-1167,P14-2082,0,0.0171906,"raction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, resulting in a new dataset of scenario types that are not naturally accessible from newswire or literature. While recent studies have explored videos and photo streams as a source of discovering complex events and learning their sequential patterns (Kim and Xing, 2014; Kim and Xing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimod"
P16-1167,P08-1090,0,0.187199,"early artificial intelligence research. Scripts (Schank and Abelson, 1975), an early formalism, were developed to encode the necessary background knowledge to support an inference engine for common sense reasoning in limited domains. However, early approaches based on hand-coded symbolic representations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2008; Chambers, 2013), recent work begins exploring the use of images as well (Bagherinezhad et al., 2016; Vedantam et al., 2015). In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see Table 1 for more details). We cast unsupervised learning of event structure as a sequential multimodal cl"
P16-1167,P09-1068,0,0.053877,"ons (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, resulting in a new dataset of scenario types that are not naturally accessible from newswire or literature. While recent studies have explored videos and photo streams as a source of discovering complex events and learning their sequential patterns (Kim and Xing, 2014; Kim and Xing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality"
P16-1167,D13-1185,0,0.138681,"research. Scripts (Schank and Abelson, 1975), an early formalism, were developed to encode the necessary background knowledge to support an inference engine for common sense reasoning in limited domains. However, early approaches based on hand-coded symbolic representations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2008; Chambers, 2013), recent work begins exploring the use of images as well (Bagherinezhad et al., 2016; Vedantam et al., 2015). In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see Table 1 for more details). We cast unsupervised learning of event structure as a sequential multimodal clustering prob1769"
P16-1167,N15-1053,1,0.835041,"ing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimodal information extraction focusing specifically on identifying video clips that referred to the same event in television news. This contrasts to the goal of our study that aims to learn the temporal structure by which common scenarios unfold. Integrating language and vision has attracted increasing attention in recent years across diverse tasks such as image captioning (Karpathy and FeiFei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015; Chen et al., 2015), cross modal semantic modeling (Lazaridou et al., 2015), information extraction (Morency et al., 2011; Rosas et al., 2013; Zhang et al., 2015; Izadinia et al., 2015), common-sense knowledge (Vedantam et al., 2015; Bagherinezhad et al., 2016), and visual storytelling (Huang et al., 2016). Our work is similar to both common sense knowledge learning and visual story completion. Our model learns commonsense knowledge on the hierarchical and temporal event structure from scenario-specific multimodal photo albums, which can be viewed as visual stories about common life events. Recent work focused o"
P16-1167,D10-1008,0,0.0625052,"Missing"
P16-1167,N16-1147,0,0.0392715,"the goal of our study that aims to learn the temporal structure by which common scenarios unfold. Integrating language and vision has attracted increasing attention in recent years across diverse tasks such as image captioning (Karpathy and FeiFei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015; Chen et al., 2015), cross modal semantic modeling (Lazaridou et al., 2015), information extraction (Morency et al., 2011; Rosas et al., 2013; Zhang et al., 2015; Izadinia et al., 2015), common-sense knowledge (Vedantam et al., 2015; Bagherinezhad et al., 2016), and visual storytelling (Huang et al., 2016). Our work is similar to both common sense knowledge learning and visual story completion. Our model learns commonsense knowledge on the hierarchical and temporal event structure from scenario-specific multimodal photo albums, which can be viewed as visual stories about common life events. Recent work focused on photo album summarization using visual (Sadeghi et al., 2015) and multimodal representations (Sinha et al., 2011). Our work identifies the nature of common events in scenarios and learns their timelines and characteristic forms. 8 Conclusion We introduce a novel exploration to learn sc"
P16-1167,E12-1034,0,0.513523,"arios goes back to early artificial intelligence research. Scripts (Schank and Abelson, 1975), an early formalism, were developed to encode the necessary background knowledge to support an inference engine for common sense reasoning in limited domains. However, early approaches based on hand-coded symbolic representations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2008; Chambers, 2013), recent work begins exploring the use of images as well (Bagherinezhad et al., 2016; Vedantam et al., 2015). In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see Table 1 for more details). We cast unsupervised learning of event structure"
P16-1167,N15-1016,0,0.0294687,"2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimodal information extraction focusing specifically on identifying video clips that referred to the same event in television news. This contrasts to the goal of our study that aims to learn the temporal structure by which common scenarios unfold. Integrating language and vision has attracted increasing attention in recent years across diverse tasks such as image captioning (Karpathy and FeiFei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015; Chen et al., 2015), cross modal semantic modeling (Lazaridou et al., 2015), information extraction (Morency et al., 2011; Rosas et al., 2013; Zhang et al., 2015; Izadinia et al., 2015), common-sense knowledge (Vedantam et al., 2015; Bagherinezhad et al., 2016), and visual storytelling (Huang et al., 2016). Our work is similar to both common sense knowledge learning and visual story completion. Our model learns commonsense knowledge on the hierarchical and temporal event structure from scenario-specific multimodal photo albums, which can be viewed as visual stories about common life events. Recent work focused on photo album summarization using visual (Sadeghi et al."
P16-1167,P13-2109,0,0.0331585,"t tend to be relatively uninformative about specific events. This cluster is excluded when computing temporal knowledge probabilities (Section 4.2). The visual and textual representations of an event are computed using the average of the visual and textual features, respectively, of photos assigned to that event. We compute each textual affinity Aci,k in the event assignment scores (Equation 3) as the cosine similarity between the textual features of the caption for photo pi and the textual representation of event ek . For textual features, we extract noun and verb unigrams using TurboTagger (Martins et al., 2013) and weigh them by their discriminativeness relative to their scenario, P (S|w). Given scenario S and word w, P (S|w) is defined as the number of albums for the scenario the word occurs in divided by the total number of albums in that scenario. The visual affinity Avi,k is the similarity between the visual features of photo pi and the visual representation of event ek . For visual features, we use the convolutional features from the final layer activations of the 16-layer VGGNet model (Simonyan and Zisserman, 2015). 4.2 Learned Event Representation Notre Dame Temporal Knowledge Local transitio"
P16-1167,P09-1025,0,0.0899443,"core for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw captions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario. 7 Related Work Previous studies have explored unsupervised induction of salient content structure in newswire texts (Barzilay and Lee, 2004), temporal graph representations (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded e"
P16-1167,E14-1024,0,0.329966,"l patterns in everyday scenarios goes back to early artificial intelligence research. Scripts (Schank and Abelson, 1975), an early formalism, were developed to encode the necessary background knowledge to support an inference engine for common sense reasoning in limited domains. However, early approaches based on hand-coded symbolic representations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learning script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2008; Chambers, 2013), recent work begins exploring the use of images as well (Bagherinezhad et al., 2016; Vedantam et al., 2015). In this paper, we present the first study for learning knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million images and captions (see Table 1 for more details). We cast unsupervised learning"
P16-1167,P10-1100,0,0.108494,"arch finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, resulting in a new dataset of scenario types that are not naturally accessible from newswire or literature. While recent studies have explored videos and photo streams as a source of discovering complex events and learning their sequential patterns (Kim and Xing, 2014; Kim and Xing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimodal information extraction focusing specifically on identifying video cl"
P16-1167,W04-2401,0,0.0743044,"ions by extracting captions whose lemmatized forms are frequently observed throughout multiple albums in the scenario. Sample events and their prototypical captions from three scenarios are displayed in Table 2. 5 Experimental Setup Data split. For scenarios with more than 1000 albums, we use 100 albums for each of the development and test sets and use the rest for training. For scenarios with less than 1000 albums, we use 50 albums for each of the development and test sets, and the rest for training. Implementation details. We optimize our objective function using integer linear programming (Roth and Yih, 2004) with the Gurobi solver (Inc., 2015). For computational efficiency, temporally close sets of consecutive photos are treated as one unit during the optimization. We use these units to reduce the number of variables and constraints in the model from a function of the number of photos to a function of the number of units. We form these units heuristically by merging images agglomeratively when their timestamps are within a certain range of the closest image in a unit. When merging photos, the textual affinity of each unit for a particular event is the maximum affinity for that event among photos"
P16-1167,D15-1020,0,0.0305678,"Chambers, 2013; Cassidy et al., 2014), or compile script knowledge from crowdsourcing (Regneri et al., 2010), our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, resulting in a new dataset of scenario types that are not naturally accessible from newswire or literature. While recent studies have explored videos and photo streams as a source of discovering complex events and learning their sequential patterns (Kim and Xing, 2014; Kim and Xing, 2013; Tang et al., 2012; Tschiatschek et al., 2014), their focus was mostly on the visual modality. Zhang et al. (2015) explored multimodal information extraction focusing specifically on identifying video clips that referred to the same event in television news. This contrasts to the goal of our study that aims to learn the temporal structure by which common scenarios unfold. Integrating language and vision has attracted increasing attention in recent years across diverse tasks such as image captioning (Karpathy and FeiFei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015; Chen et al., 2015), cross modal semantic modeling (Lazaridou et al., 2015), information extraction (Morency et al., 2011; Ro"
P16-1167,D13-1127,0,0.0294331,"Almost all LSTM captions mention the words bride, wedding, or groom, yielding a very high scenario score for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw captions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario. 7 Related Work Previous studies have explored unsupervised induction of salient content structure in newswire texts (Barzilay and Lee, 2004), temporal graph representations (Bramsen et al., 2006), and storyline extraction and event summarization (Xu et al., 2013). Another line of research finds the common event structure from children’s stories (McIntyre and Lapata, 2009), where the learned plot structure is used to stochastically generate new stories (Goyal et al., 2010; Goyal et al., 2013). Our work similarly aims to learn the typical temporal patterns and compositional elements that define common scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language (Pichotta and Mooney, 2014; Jans et al., 2012; Chambers and Jurafsky, 2009; Chambers, 2013; Cassidy et al., 2014), or compile script knowledg"
P16-2020,W09-1206,0,0.0474037,"Missing"
P16-2020,N10-1138,0,0.0279495,"s demonstrate our model can effectively bias argument embeddings based on their dependency role. 1 Introduction Semantic role labeling (SRL) aims to identify predicate-argument structures of a sentence. The following example shows the arguments labeled with the roles A0 (typically the agent of an action) and A1 (typically the patient of an action), as well as the predicate in bold. [Little Willy A0 ] broke [a window A1 ]. As manual annotations are expensive and timeconsuming, supervised approaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceedings of the 54th Annual Meeting of the Association for Computational"
P16-2020,D15-1112,0,0.0265338,"Missing"
P16-2020,J12-1005,0,0.0526853,"Missing"
P16-2020,P12-2029,0,0.0388577,"Missing"
P16-2020,J02-3001,0,0.0636144,"e induction on the CoNLL 2008 dataset and the SimLex999 word similarity task. Qualitative results demonstrate our model can effectively bias argument embeddings based on their dependency role. 1 Introduction Semantic role labeling (SRL) aims to identify predicate-argument structures of a sentence. The following example shows the arguments labeled with the roles A0 (typically the agent of an action) and A1 (typically the patient of an action), as well as the predicate in bold. [Little Willy A0 ] broke [a window A1 ]. As manual annotations are expensive and timeconsuming, supervised approaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu"
P16-2020,W05-0634,0,0.0421425,"Lex999 word similarity task. Qualitative results demonstrate our model can effectively bias argument embeddings based on their dependency role. 1 Introduction Semantic role labeling (SRL) aims to identify predicate-argument structures of a sentence. The following example shows the arguments labeled with the roles A0 (typically the agent of an action) and A1 (typically the patient of an action), as well as the predicate in bold. [Little Willy A0 ] broke [a window A1 ]. As manual annotations are expensive and timeconsuming, supervised approaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceedings of the 54th Annual M"
P16-2020,P14-1136,0,0.0130532,"oaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 118–123, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ut-k … × Et-k … ut+k × Et+k ut-k up … × Et-k … vt ut+k up vt × Et+k (a) SYMDEP Look-up Look-up NMOD SBJ VC LGS PMOD NMOD Dt × × Dt D2/E2 D1/E1 A car is hit by another car . Look-up u1/v1 (b) ASYMDEP Look-up up (C) Look-up u2/v2 Figure 1: (a): The S YM D EP model. (b): The A SYM D EP model. (c): An example of how embeddings relate to the parse tree. In S YM D EP, the biasing of depend"
P16-2020,J15-4004,0,0.0332481,"YM1D EP, where we force all dependencies in S YM D EP to use the same matrix. The network has the same structure as S YM D EP, but the dependency information is removed. Its performance on SRL is shown at the bottom of Table 1. S YM1D EP performs slightly worse than Arg2vec. This suggests that the performance gain in S YM D EP can be attributed to the use of dependency information instead of the way of constructing context. 4.3 Word Similarity Results As a further evaluation of the learned embeddings, we test if similarities between word embeddings agree with human annotation from SimLex999 (Hill et al., 2015). Table 3 shows that S YM D EP outperforms Arg2vec on both nouns and verbs, suggesting multiplicative dependency relations are indeed effective. However, A SYM D EP performs better than S YM D EP on noun similarity but much worse on verb similarity. We explore this further in an ablation study. 3 Except that Arg2vec is reimplemented since there is no public code online. 4 The numbers reported for Arg2vec with gold parsing (80.7) is different from Woodsend and Lapata (2015) (80.9) since we use a different clustering method and different training data. 121 Argument S YM D EP (SBJ) S YM D EP (OBJ"
P16-2020,J08-2006,0,0.0226789,"ample shows the arguments labeled with the roles A0 (typically the agent of an action) and A1 (typically the patient of an action), as well as the predicate in bold. [Little Willy A0 ] broke [a window A1 ]. As manual annotations are expensive and timeconsuming, supervised approaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 118–123, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ut-k … × Et-k … ut+k × Et+k ut-k up … × Et-k … vt ut+k up vt × Et+k (a) SYMDEP Look-up Look-up NMOD SBJ VC LGS"
P16-2020,J08-2005,0,0.0271993,"task. Qualitative results demonstrate our model can effectively bias argument embeddings based on their dependency role. 1 Introduction Semantic role labeling (SRL) aims to identify predicate-argument structures of a sentence. The following example shows the arguments labeled with the roles A0 (typically the agent of an action) and A1 (typically the patient of an action), as well as the predicate in bold. [Little Willy A0 ] broke [a window A1 ]. As manual annotations are expensive and timeconsuming, supervised approaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceedings of the 54th Annual Meeting of the Association"
P16-2020,kingsbury-palmer-2002-treebank,0,0.0330639,". . . , ut+k } are the vectors surrounding the tth argument with a window of size k.1 The prediction of the tth argument is: p(vt |up , uc ) ∝ exp(f(vt ) |g(up , uc )) Approach Most unsupervised approaches to SRL perform the following two steps: (1) identifying the arguments of the predicate and (2) assigning arguments to unlabeled roles, such as argument clusters. Step (1) can be usually tackled with heuristic rules (Lang and Lapata, 2014). In this paper, we focus on tackling step (2) by creating clusters of arguments that belongs to the same semantic role. As we assume PropBank-style roles (Kingsbury and Palmer, 2002), our models allocate a separate set of role clusters for each predicate and assign its arguments to the clusters. We evaluate the results by the overlapping between the induced clusters and PropBank-style gold labels. The example below suggests that SRL requires more than just lexical embeddings. vt ⊗ Dt , tanh (Dt vt ) ut ⊗ Et , tanh (Et ut ) , (1) (2) where tanh(·) is the element-wise tanh function. Eq. 2 composes an argument and its dependency with a multiplicative nonlinear operation. The multiplicative formulation encourages the decoupling of dependencies and arguments, which is [A car A"
P16-2020,W08-2121,0,0.106885,"Missing"
P16-2020,P15-2036,0,0.0349679,"Missing"
P16-2020,N15-1001,0,0.137566,"includes the predicate and other arguments in the same sentence. Driven by the importance of syntactic dependency relations in SRL, we explicitly model dependencies as multiplicative factors in neural networks, yielding more succinct models than existing representation learning methods employing dependencies (Levy and Goldberg, 2014; Woodsend and Lapata, 2015). The learned argument embeddings are then clustered and are evaluated by the clusters’ agreement with ground truth labels. On unsupervised SRL, our models outperform the state of the art by Woodsend and Lapata (2015) on gold parses and Titov and Khoddam (2015) on automatic parses. Qualitative results suggest our model is effective in biasing argument embeddings toward a specific dependency relation. In unsupervised semantic role labeling, identifying the role of an argument is usually informed by its dependency relation with the predicate. In this work, we propose a neural model to learn argument embeddings from the context by explicitly incorporating dependency relations as multiplicative factors, which bias argument embeddings according to their dependency roles. Our model outperforms existing state-of-the-art embeddings in unsupervised semantic"
P16-2020,P11-1112,0,0.0714711,"t of an action) and A1 (typically the patient of an action), as well as the predicate in bold. [Little Willy A0 ] broke [a window A1 ]. As manual annotations are expensive and timeconsuming, supervised approaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 118–123, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ut-k … × Et-k … ut+k × Et+k ut-k up … × Et-k … vt ut+k up vt × Et+k (a) SYMDEP Look-up Look-up NMOD SBJ VC LGS PMOD NMOD Dt × × Dt D2/E2 D1/E1 A car is hit by another car . Look-up"
P16-2020,E12-1003,0,0.105529,"(C, C 0 ) be the set of arguments ai ∈ C such that ai appears in the 2 Resulted embeddings can be downloaded from https: //bitbucket.org/luanyi/unsupervised-srl. 120 able code3 , and then apply the same clustering algorithm. Introduced by Lang and Lapata (2014), S YNT F is a strong baseline that clusters arguments based on purely syntactic cues: voice of the verb, relative position to the predicate, syntactic relations, and realizing prepositions. The window size for Arg2vec and our models are set to 1, while all other embeddings are set to 2. We also employ two state-of-the-art methods from Titov and Klementiev (2012) (T&K12) and Titov and Khoddam (2015) (T&K15). 4.2 SRL Results Gold parses CO F1 Automatic parses PU CO F1 Model PU S YNT F 81.6 78.1 79.8 77.0 71.5 74.1 Skip-Gram CBOW GloVe L&G Arg2vec S YM D EP A SYM D EP 86.6 84.6 84.9 87.0 84.0 85.3 85.6 74.7 74.9 74.1 75.6 77.7 77.9 78.3 80.2 79.4 79.2 80.9 80.7 81.4 81.8 84.3 84.0 83.0 86.6 86.9 81.9 82.9 72.4 71.5 70.8 71.3 71.4 76.6 75.2 77.9 77.2 76.5 78.2 78.4 79.2 78.9 T&K12 T&K15 88.7 79.7 78.1 86.2 83.0 82.8 86.2 - 72.7 - 78.8 - S YM1D EP 83.8 77.4 80.5 82.3 74.8 78.4 Table 1: Purity, collocation and F1 measures for the CoNLL-2008 data set. Follo"
P16-2020,J14-3006,0,0.211697,"the tth argument in a sentence, and ut the embedding of the argument when it is part of the context. Let up be the embedding of the predicate. uc = {ut−k , . . . , ut−1 , ut+1 , . . . , ut+k } are the vectors surrounding the tth argument with a window of size k.1 The prediction of the tth argument is: p(vt |up , uc ) ∝ exp(f(vt ) |g(up , uc )) Approach Most unsupervised approaches to SRL perform the following two steps: (1) identifying the arguments of the predicate and (2) assigning arguments to unlabeled roles, such as argument clusters. Step (1) can be usually tackled with heuristic rules (Lang and Lapata, 2014). In this paper, we focus on tackling step (2) by creating clusters of arguments that belongs to the same semantic role. As we assume PropBank-style roles (Kingsbury and Palmer, 2002), our models allocate a separate set of role clusters for each predicate and assign its arguments to the clusters. We evaluate the results by the overlapping between the induced clusters and PropBank-style gold labels. The example below suggests that SRL requires more than just lexical embeddings. vt ⊗ Dt , tanh (Dt vt ) ut ⊗ Et , tanh (Et ut ) , (1) (2) where tanh(·) is the element-wise tanh function. Eq. 2 compo"
P16-2020,D15-1295,0,0.373666,"Levy and Goldberg, 2014; Pennington et al., 2014), we introduce two unsupervised models that learn embeddings of arguments, predicates, and syntactic dependency relations between them. The embeddings are learned by predicting each argument from its context, which includes the predicate and other arguments in the same sentence. Driven by the importance of syntactic dependency relations in SRL, we explicitly model dependencies as multiplicative factors in neural networks, yielding more succinct models than existing representation learning methods employing dependencies (Levy and Goldberg, 2014; Woodsend and Lapata, 2015). The learned argument embeddings are then clustered and are evaluated by the clusters’ agreement with ground truth labels. On unsupervised SRL, our models outperform the state of the art by Woodsend and Lapata (2015) on gold parses and Titov and Khoddam (2015) on automatic parses. Qualitative results suggest our model is effective in biasing argument embeddings toward a specific dependency relation. In unsupervised semantic role labeling, identifying the role of an argument is usually informed by its dependency relation with the predicate. In this work, we propose a neural model to learn argu"
P16-2020,P14-2050,0,0.564332,"♣ Disney Research {luanyi,hannaneh}@uw.edu, jiyfeng@gatech.edu, boyang.li@disney.com Abstract 2012; Garg and Henderson, 2012; Lang and Lapata, 2014; Titov and Khoddam, 2015). It is commonly formalized as a clustering problem, where each cluster represents an induced semantic role. Such clustering is usually performed through manually defined semantic and syntactic features defined over argument instances. However, the representation based on these features are usually sparse and difficult to generalize. Inspired by the recent success of distributed word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014), we introduce two unsupervised models that learn embeddings of arguments, predicates, and syntactic dependency relations between them. The embeddings are learned by predicting each argument from its context, which includes the predicate and other arguments in the same sentence. Driven by the importance of syntactic dependency relations in SRL, we explicitly model dependencies as multiplicative factors in neural networks, yielding more succinct models than existing representation learning methods employing dependencies (Levy and Goldberg, 2014; Woodsend and Lapata, 20"
P16-2020,W04-3212,0,0.0606206,"08 dataset and the SimLex999 word similarity task. Qualitative results demonstrate our model can effectively bias argument embeddings based on their dependency role. 1 Introduction Semantic role labeling (SRL) aims to identify predicate-argument structures of a sentence. The following example shows the arguments labeled with the roles A0 (typically the agent of an action) and A1 (typically the patient of an action), as well as the predicate in bold. [Little Willy A0 ] broke [a window A1 ]. As manual annotations are expensive and timeconsuming, supervised approaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceeding"
P16-2020,P15-1109,0,0.0197361,"afsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 118–123, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ut-k … × Et-k … ut+k × Et+k ut-k up … × Et-k … vt ut+k up vt × Et+k (a) SYMDEP Look-up Look-up NMOD SBJ VC LGS PMOD NMOD Dt × × Dt D2/E2 D1/E1 A car is hit by another car . Look-up u1/v1 (b) ASYMDEP Look-up up (C) Look-up u2/v2 Figure 1: (a): The S YM D EP model. (b): The A SYM D EP model. (c): An example of how embeddings relate to the parse tree. In S YM D EP, the biasing of dependency is uniformly a"
P16-2020,C10-2107,0,0.0284085,"tic role labeling (SRL) aims to identify predicate-argument structures of a sentence. The following example shows the arguments labeled with the roles A0 (typically the agent of an action) and A1 (typically the patient of an action), as well as the predicate in bold. [Little Willy A0 ] broke [a window A1 ]. As manual annotations are expensive and timeconsuming, supervised approaches (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2005; Punyakanok et al., 2008; Das et al., 2010; Das et al., 2014) to this problem are held back by limited coverage of available gold annotations (Palmer and Sporleder, 2010). SRL performance decreases remarkably when applied to out-of-domain data (Pradhan et al., 2008). Unsupervised SRL offer a promising alternative (Lang and Lapata, 2011; Titov and Klementiev, 2 Related Work There has been growing interest in using neural networks and representation learning for supervised and unsupervised SRL (Collobert et al., 2011; Hermann et al., 2014; Zhou and Xu, 2015; 118 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 118–123, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ut-k … × Et-k"
P16-2020,D14-1162,0,0.117741,"i,hannaneh}@uw.edu, jiyfeng@gatech.edu, boyang.li@disney.com Abstract 2012; Garg and Henderson, 2012; Lang and Lapata, 2014; Titov and Khoddam, 2015). It is commonly formalized as a clustering problem, where each cluster represents an induced semantic role. Such clustering is usually performed through manually defined semantic and syntactic features defined over argument instances. However, the representation based on these features are usually sparse and difficult to generalize. Inspired by the recent success of distributed word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014), we introduce two unsupervised models that learn embeddings of arguments, predicates, and syntactic dependency relations between them. The embeddings are learned by predicting each argument from its context, which includes the predicate and other arguments in the same sentence. Driven by the importance of syntactic dependency relations in SRL, we explicitly model dependencies as multiplicative factors in neural networks, yielding more succinct models than existing representation learning methods employing dependencies (Levy and Goldberg, 2014; Woodsend and Lapata, 2015). The learned argument"
P16-2020,N07-1070,0,\N,Missing
P17-2081,D10-1098,0,0.0110636,"P H A RTE SICK Four kids are doing backbends in the park. Four girls are doing backbends and playing outdoors. Entailment Table 1: Examples of question-context pairs from QA datasets and premise-hypothesis pair from RTE dataset. Q indicates question, C indicates context, A indicates answer, P indicates premise and H indicates hypothesis. graph from Wikipedia and a question created by a human, and the answer is a span in the context. ural language processing, domain adaptation has traditionally been an important topic for syntactic parsing (McClosky et al., 2010) and named entity recognition (Chiticariu et al., 2010), among others. With the popularity of distributed representation, pre-trained word embedding models such as word2vec (Mikolov et al., 2013b,a) and glove (Pennington et al., 2014) are also widely used for natural language tasks (Karpathy and Fei-Fei, 2015; Kumar et al., 2016). Instead of these, we initialize our models from a QA dataset and show how standard transfer learning can achieve stateof-the-art in target QA datasets. There have been several QA paradigms in NLP, which can be categorized by the context and supervision used to answer questions. This context can range from structured and"
P17-2081,S16-1172,0,0.0271855,"66.10 71.13 76.30 82.51 86.64 78.37 85.58 87.68 80.20 86.44 89.14 79.19 86.42 88.82 77.66 84.93 88.05 77.58 85.21 88.14 Table 2: Results on WikiQA and SemEval-2016 (Task 3A). The first row is a result from non-pretrained model, and * indicates ensemble method. Metrics used are Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), Precision at rank 1 (P@1), and Average Recall (AvgR). Rank 1,2,3 indicate the results by previous works, ordered by MAP. For WikiQA, they are from Wang and Jiang (2017a); Tymoshenko et al. (2016); Miller et al. (2016), respectively. For SemEval2016, they are from Filice et al. (2016); Joty et al. (2016); Mihaylov and Nakov (2016). SQuAD*&Yes sets the new state of the art on both datasets. Question Answering Results. Table 2 reports the state-of-the-art results of our transfer learning on WikiQA and SemEval-2016 and the performance of previous models as well as several ablations that use no pretraining or no finetuning. There are multiple interesting observations from Table 2 as follows: (a) If we only train the BiDAF-T model on the target datasets with no pretraining (first row of Table 2), the results are poor. This shows the importance of both pretraining and finetuning"
P17-2081,S14-2131,0,0.0257402,"Missing"
P17-2081,D16-1264,0,0.176846,"can achieve stateof-the-art in target QA datasets. There have been several QA paradigms in NLP, which can be categorized by the context and supervision used to answer questions. This context can range from structured and confined knowledge bases (Berant et al., 2013) to unstructured and unbounded natural language form (e.g., documents on the web (Voorhees and Tice, 2000)) and unstructured, but restricted in size (e.g., a paragraph or multiple sentences (Hermann et al., 2015)). The recent advances in neural question answering lead to numerous datasets and successful models in these paradigms (Rajpurkar et al., 2016; Yang et al., 2015; Nguyen et al., 2016; Trischler et al., 2016). The answer types in these datasets are largely divided into three categories: sentencelevel, in-context span, and generation. In this paper, we specifically focus on the former two and show that span-supervised models can better learn syntactic and lexical features. Among these datasets, we briefly describe three QA datasets to be used for the experiments in this paper. We also give the description of an RTE dataset for an example of a non-QA task. Refer to Table 1 to see the examples of the datasets. SQUAD-T is our modificatio"
P17-2081,N16-1152,0,0.0123548,"4.61 70.37 75.31 64.61 - SemEval-2016 MAP MRR AvgR 76.40 82.20 86.51 47.23 49.31 60.01 57.80 66.10 71.13 76.30 82.51 86.64 78.37 85.58 87.68 80.20 86.44 89.14 79.19 86.42 88.82 77.66 84.93 88.05 77.58 85.21 88.14 Table 2: Results on WikiQA and SemEval-2016 (Task 3A). The first row is a result from non-pretrained model, and * indicates ensemble method. Metrics used are Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), Precision at rank 1 (P@1), and Average Recall (AvgR). Rank 1,2,3 indicate the results by previous works, ordered by MAP. For WikiQA, they are from Wang and Jiang (2017a); Tymoshenko et al. (2016); Miller et al. (2016), respectively. For SemEval2016, they are from Filice et al. (2016); Joty et al. (2016); Mihaylov and Nakov (2016). SQuAD*&Yes sets the new state of the art on both datasets. Question Answering Results. Table 2 reports the state-of-the-art results of our transfer learning on WikiQA and SemEval-2016 and the performance of previous models as well as several ablations that use no pretraining or no finetuning. There are multiple interesting observations from Table 2 as follows: (a) If we only train the BiDAF-T model on the target datasets with no pretraining (first row of Tab"
P17-2081,D14-1162,0,\N,Missing
P17-2081,marelli-etal-2014-sick,0,\N,Missing
P17-2081,D15-1237,0,\N,Missing
P17-2081,D16-1046,0,\N,Missing
P17-2081,N10-1004,0,\N,Missing
P19-1416,P17-1147,1,0.809277,"r most of the original single-hop accuracy, indicating that these distractors are still insufficient. Another method is to consider very large distractor sets such as all of Wikipedia or the entire Web, as done in open-domain H OTPOT QA and ComplexWebQuestions (Talmor and Berant, 2018). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multihop reasoning remains an open challenge that is worthy of follow up work. 2 Related Work Large-scale RC datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) have enabled rapid advances in neural QA models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2018). To foster research on reasoning across multiple pieces of text, multi-hop QA has been introduced (Koˇcisk`y et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP (Welbl et al., 2017) and C OMPLEX W EB Q UESTIONS (Talmor and Berant, 2018), or us"
P19-1416,Q18-1023,0,0.0788822,"Missing"
P19-1416,D16-1264,0,0.0696953,"e distractors can recover most of the original single-hop accuracy, indicating that these distractors are still insufficient. Another method is to consider very large distractor sets such as all of Wikipedia or the entire Web, as done in open-domain H OTPOT QA and ComplexWebQuestions (Talmor and Berant, 2018). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multihop reasoning remains an open challenge that is worthy of follow up work. 2 Related Work Large-scale RC datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) have enabled rapid advances in neural QA models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2018). To foster research on reasoning across multiple pieces of text, multi-hop QA has been introduced (Koˇcisk`y et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP (Welbl et al., 2017) and C OMPLEX W EB Q UESTIONS (Talmor and"
P19-1416,N18-1059,0,0.20532,"difficulty. However, since only one of the ten paragraphs is about an animal, one can immediately locate the answer in Paragraph 1 using one hop. The full example is provided in Appendix A. established to protect?”, and then answer “What is the former name of that animal?”. However, when considering the evidence paragraphs, the question is solvable in a single hop by finding the only paragraph that describes an animal. Introduction Multi-hop reading comprehension (RC) requires reading and aggregating information over multiple pieces of textual evidence (Welbl et al., 2017; Yang et al., 2018; Talmor and Berant, 2018). In this work, we argue that it can be difficult to construct large multi-hop RC datasets. This is because multi-hop reasoning is a characteristic of both the question and the provided evidence; even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. For example, the question in Figure 1 is compositional: a plausible solution is to find “What animal’s habitat was the R´eserve Naturelle Lomako Yokokala ∗ Equal Contribution. Our analysis is centered on H OTPOT QA (Yang et al., 2018), a dataset"
P19-1416,D13-1160,0,\N,Missing
P19-1416,D18-1453,0,\N,Missing
P19-1416,D18-1259,0,\N,Missing
P19-1416,P18-1078,1,\N,Missing
P19-1416,N19-1423,0,\N,Missing
P19-1416,Q18-1021,0,\N,Missing
P19-1436,D13-1160,0,0.139773,"ty under an academic setting, we discuss optimization strategies for reducing time and memory usage during each stage in Section 5. This enables us to start from scratch and fully deploy the model with a 4-GPU, 128GB memory, 2 TB PCIe1 SSD server in a week. 2 Related Work Open-domain question answering Creating a system that can answer an open-domain factoid question has been a significant interest to both academic and industrial communities. The problem is largely approached from two subfields: knowledge base (KB) and text (document) retrieval. Earlier work in large-scale question answering (Berant et al., 2013) has focused on answering questions from a structured KB such as Freebase (Bollacker et al., 2008). These approaches usually achieve a high precision, but their scope is limited to the ontology of the knowledge graph. While KB QA is undoubtedly an important part of opendomain QA, we mainly discuss literature in textbased QA, which is most relevant to our work. Sentence-level QA has been studied since early 2000s, some of the most notable datasets being TrecQA (Voorhees and Tice, 2000) and WikiQA (Yang et al., 2015). See Prager et al. (2007) 1 Disk random read access is a major bottleneck that"
P19-1436,K17-1034,1,0.858087,"the summation in Equation 6 is computed over T 2 terms which is quite large and causes small gradient. To aid training, we define an auxilary loss L1 corresponding to the start logits, L1 = −li1∗ + log X i exp( 1X li,j ) T (7) j and L2 for the end logits in a similar way. By early summation (taking the mean), we reduce the number of exponential terms and allow larger gradients. We average between the true and aux loss 1 2 . for the final loss: L2 + L +L 4 No-Answer Bias During training SQuAD (v1.1), we never observe negative examples (i.e. an unanswerable question in the paragraph). Following Levy et al. (2017), we introduce a trainable 4434 no-answer bias when computing softmax. For each paragraph, we create two negative examples by bringing one question from another article and one question from the same article but different paragraphs. Instead of randomly sampling, we bring the question with the highest inner product (i.e. most similar) with a randomly-picked positive question in the current paragraph, using a question embedding model trained on SQuAD v1.1. We jointly train the positive examples with the negative examples. 5.2 Indexing Wikipedia consists of approximately 3 billion tokens, so enu"
P19-1436,P18-1161,0,0.0700817,"as SQuAD (Rajpurkar et al., 2016), open-domain phrase-level question answering has gained a great popularity (Shen et al., 2017; Raiman and Miller, 2017; Min et al., 2018; Das et al., 2019), where a few (5-10) documents relevant to the question are retrieved and then a deep neural model finds the answer in the document. Most previous work on open-domain QA has focused on mitigating error propagation of retriever models in a pipelined setting (Chu-Carroll et al., 2012). For instance, retrieved documents could be re-ranked using reinforcement learning (Wang et al., 2018a), distant supervision (Lin et al., 2018), or multi-task learning (Nishida et al., 2018). Several studies have also shown that answer aggregation modules could improve performance of the pipelined models (Wang et al., 2018b; Lee et al., 2018). Our work is motivated by Seo et al. (2018) and adopts the concept and the advantage of using phrase index for large-scale question answering, though they only experiment in a close-domain (vanilla SQuAD) setup. Approximate similarity search Sublinear-time search for the nearest neighbor from a large collection of vectors is a significant interest to the information retrieval community (Deerwest"
P19-1436,P17-1171,0,0.340472,"on indexable representations allows fast and direct retrieval in a web-scale environment. Experiments on SQuAD-Open (Chen et al., 4430 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4430–4441 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Document Index Query vector for document When was Barack Obama born? Dense-Sparse Phrase Index … Dense start When was Barack Obama born? Reader Model … Dense end Coherency Sparse 1961 1961 Figure 1: An illustrative comparison between a pipelined QA system, e.g. DrQA (Chen et al., 2017) (left) and our proposed Dense-Sparse Phrase Index (right) for open-domain QA, best viewed in color. Dark blue vectors indicate the retrieved items from the index by the query. 2017) show that D EN SPI is on par with or better than most state-of-the-art open-domain QA systems on Wikipedia with 6000x reduced computational cost on RAM. In our end-to-end benchmark, this translates into at least 68x faster query inference including disk access time. At the web scale, every detail of the training, indexing, and inference needs to be carefully designed. For reproducibility under an academic setting,"
P19-1436,D16-1147,0,0.0217987,"e query. Generative question answering Mapping the phrases in a document to a common vector space to that of the questions can be viewed as an exhaustive enumeration of all possible questions that can be asked on the document in the vector space, but without a surface-form decoder. It is worth noting that generative question answering (Lewis and Fan, 2019) has the opposite property; while it has a surface-form decoder by definition, it cannot easily enumerate a compact list of all possible semantically-unique questions. Memory networks One can view the phrase index as a fixed external memory (Miller et al., 2016) where the key is the phrase vector and the value is the corresponding answer phrase span. 3 Overview 3.2 Encoding and Indexing Phrases Motivated by Seo et al. (2018), our model encodes query-agnostic representations of text spans in Wikipedia offline and obtains the answer in realtime by performing nearest neighbor search at inference time. We represent each phrase span in the corpus (Wikipedia) with a dense vector and a sparse vector. The dense vector is effective for encoding syntactic and semantic cues, while the sparse vector is good at encoding precise lexical information. That is, the e"
P19-1436,P18-1160,0,0.0396641,"d QA, which is most relevant to our work. Sentence-level QA has been studied since early 2000s, some of the most notable datasets being TrecQA (Voorhees and Tice, 2000) and WikiQA (Yang et al., 2015). See Prager et al. (2007) 1 Disk random read access is a major bottleneck that PCIe over SATA is preferred. for a comprehensive overview of early work. With the advancement of deep neural networks and the availability of massive QA datasets such as SQuAD (Rajpurkar et al., 2016), open-domain phrase-level question answering has gained a great popularity (Shen et al., 2017; Raiman and Miller, 2017; Min et al., 2018; Das et al., 2019), where a few (5-10) documents relevant to the question are retrieved and then a deep neural model finds the answer in the document. Most previous work on open-domain QA has focused on mitigating error propagation of retriever models in a pipelined setting (Chu-Carroll et al., 2012). For instance, retrieved documents could be re-ranked using reinforcement learning (Wang et al., 2018a), distant supervision (Lin et al., 2018), or multi-task learning (Nishida et al., 2018). Several studies have also shown that answer aggregation modules could improve performance of the pipeline"
P19-1436,N18-1202,0,0.0630883,"Missing"
P19-1436,D17-1111,0,0.0219709,"ss literature in textbased QA, which is most relevant to our work. Sentence-level QA has been studied since early 2000s, some of the most notable datasets being TrecQA (Voorhees and Tice, 2000) and WikiQA (Yang et al., 2015). See Prager et al. (2007) 1 Disk random read access is a major bottleneck that PCIe over SATA is preferred. for a comprehensive overview of early work. With the advancement of deep neural networks and the availability of massive QA datasets such as SQuAD (Rajpurkar et al., 2016), open-domain phrase-level question answering has gained a great popularity (Shen et al., 2017; Raiman and Miller, 2017; Min et al., 2018; Das et al., 2019), where a few (5-10) documents relevant to the question are retrieved and then a deep neural model finds the answer in the document. Most previous work on open-domain QA has focused on mitigating error propagation of retriever models in a pipelined setting (Chu-Carroll et al., 2012). For instance, retrieved documents could be re-ranked using reinforcement learning (Wang et al., 2018a), distant supervision (Lin et al., 2018), or multi-task learning (Nishida et al., 2018). Several studies have also shown that answer aggregation modules could improve performan"
P19-1436,N19-1423,0,0.58735,"le evidence to a few documents, in which the answer span can be extracted using a reading comprehension model (Chen et al., ∗ Equal contribution. 2017). However, the accuracy of the final QA system is bounded by the performance of the search engine due to the pipeline nature of the search process. What is more, running a neural reading comprehension model (Seo et al., 2017) on a few documents is still computationally costly, since it needs to process the evidence document for every new question at inference time. This often requires multi-GPU-seconds or tens to hundreds of CPU-seconds – BERT (Devlin et al., 2019) can process only a few thousand words per second on an Nvidia V100 GPU. In this paper, we introduce Dense-Sparse Phrase Index (D EN SPI), an indexable query-agnostic phrase representation model for real-time opendomain QA. The phrase representations are indexed offline using efficient training and memoryefficient strategies for storage. During inference time, the input question is mapped to the same representation space, and the phrase with maximum inner product search is retrieved. Our phrase encoding model combines both dense and sparse vectors, eliminating the pipeline filtering of the con"
P19-1436,D16-1264,0,0.569318,"0 billion phrases) using under 2TB. Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on CPUs. Code and demo are available at nlp. cs.washington.edu/denspi 1 Introduction Extractive open-domain question answering (QA) is usually referred to the task of answering an arbitrary factoid question (such as “Where was Barack Obama born?”) from a general web text (such as Wikipedia). This is an extension of the reading comprehension task (Rajpurkar et al., 2016) of selecting an answer phrase to a question given an evidence document. To make a scalable opendomain QA system, One can leverage a search engine to filter the web-scale evidence to a few documents, in which the answer span can be extracted using a reading comprehension model (Chen et al., ∗ Equal contribution. 2017). However, the accuracy of the final QA system is bounded by the performance of the search engine due to the pipeline nature of the search process. What is more, running a neural reading comprehension model (Seo et al., 2017) on a few documents is still computationally costly, sin"
P19-1436,D18-1052,1,0.860194,"are retrieved and then a deep neural model finds the answer in the document. Most previous work on open-domain QA has focused on mitigating error propagation of retriever models in a pipelined setting (Chu-Carroll et al., 2012). For instance, retrieved documents could be re-ranked using reinforcement learning (Wang et al., 2018a), distant supervision (Lin et al., 2018), or multi-task learning (Nishida et al., 2018). Several studies have also shown that answer aggregation modules could improve performance of the pipelined models (Wang et al., 2018b; Lee et al., 2018). Our work is motivated by Seo et al. (2018) and adopts the concept and the advantage of using phrase index for large-scale question answering, though they only experiment in a close-domain (vanilla SQuAD) setup. Approximate similarity search Sublinear-time search for the nearest neighbor from a large collection of vectors is a significant interest to the information retrieval community (Deerwester et al., 1990; Blei et al., 2003). In metric space (L1 or L2), one of the most classic search algorithms is Locality-Sensitive Hashing (LSH) (Gionis et al., 1999), which uses a data-independent hashing function to map nearby vectors to the sam"
P19-1436,D18-1053,1,0.864236,"ew (5-10) documents relevant to the question are retrieved and then a deep neural model finds the answer in the document. Most previous work on open-domain QA has focused on mitigating error propagation of retriever models in a pipelined setting (Chu-Carroll et al., 2012). For instance, retrieved documents could be re-ranked using reinforcement learning (Wang et al., 2018a), distant supervision (Lin et al., 2018), or multi-task learning (Nishida et al., 2018). Several studies have also shown that answer aggregation modules could improve performance of the pipelined models (Wang et al., 2018b; Lee et al., 2018). Our work is motivated by Seo et al. (2018) and adopts the concept and the advantage of using phrase index for large-scale question answering, though they only experiment in a close-domain (vanilla SQuAD) setup. Approximate similarity search Sublinear-time search for the nearest neighbor from a large collection of vectors is a significant interest to the information retrieval community (Deerwester et al., 1990; Blei et al., 2003). In metric space (L1 or L2), one of the most classic search algorithms is Locality-Sensitive Hashing (LSH) (Gionis et al., 1999), which uses a data-independent hashi"
P19-1436,N19-4013,0,0.619426,"Missing"
P19-1436,D15-1237,0,0.0433352,"nd text (document) retrieval. Earlier work in large-scale question answering (Berant et al., 2013) has focused on answering questions from a structured KB such as Freebase (Bollacker et al., 2008). These approaches usually achieve a high precision, but their scope is limited to the ontology of the knowledge graph. While KB QA is undoubtedly an important part of opendomain QA, we mainly discuss literature in textbased QA, which is most relevant to our work. Sentence-level QA has been studied since early 2000s, some of the most notable datasets being TrecQA (Voorhees and Tice, 2000) and WikiQA (Yang et al., 2015). See Prager et al. (2007) 1 Disk random read access is a major bottleneck that PCIe over SATA is preferred. for a comprehensive overview of early work. With the advancement of deep neural networks and the availability of massive QA datasets such as SQuAD (Rajpurkar et al., 2016), open-domain phrase-level question answering has gained a great popularity (Shen et al., 2017; Raiman and Miller, 2017; Min et al., 2018; Das et al., 2019), where a few (5-10) documents relevant to the question are retrieved and then a deep neural model finds the answer in the document. Most previous work on open-doma"
P19-1613,D13-1160,0,0.209872,"Missing"
P19-1613,N19-1240,0,0.07606,"Missing"
P19-1613,P17-1171,0,0.0428235,"tor setting contains the question and a collection of 10 paragraphs: 2 paragraphs are provided to crowd workers to write a multi-hop question, and 8 distractor paragraphs are collected separately via TF-IDF between the question and the paragraph. The train set contains easy, medium and hard examples, where easy examples are single-hop, and medium and hard examples are multi-hop. The dev and test sets are made up of only hard examples. Full wiki setting is an open-domain setting which contains the same questions as distractor setting but does not provide the collection of paragraphs. Following Chen et al. (2017), we retrieve 30 Wikipedia paragraphs based on TF-IDF similarity between the paragraph and the question (or subquestion). 6101 All D ECOMP RC 1hop train BERT 1hop train BiDAF 70.57 61.73 67.08 56.27 58.28 Distractor setting Bridge Comp Single Multi All 72.53 61.57 69.41 62.77 59.09 58.74 46.53 53.38 29.64 - 43.26 39.17 38.40 29.97 34.36 62.78 62.36 57.81 30.40 55.05 84.31 79.38 82.98 87.21 - Full wiki setting Bridge Comp Single Multi 40.30 35.30 34.77 32.15 30.42 35.64 29.83 31.74 15.18 - 55.04 54.57 52.85 21.29 50.70 52.11 50.03 46.14 47.14 - Table 3: F1 scores on the dev set of H OTPOT QA in"
P19-1613,D13-1020,0,0.0488979,"2, 2019. 2019 Association for Computational Linguistics bust than an end-to-end BERT baseline (Devlin et al., 2019). Finally, our ablation studies show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance. Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC. 2 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document (Richardson et al., 2013). Recently, the availability of large-scale reading comprehensiondatasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) has led to the development of advanced RC models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2019). Most of the questions on these datasets can be answered in a single sentence (Min et al., 2018), which is a key difference from multi-hop reading comprehension. Multi-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop dataset"
P19-1613,N18-1059,0,0.356645,"layer who won MVP from one paragraph, and then finding the team that player plays for from another paragraph. In this paper, we propose D ECOMP RC, a system for multi-hop RC, that learns to break compositional multi-hop questions into simpler, singlehop sub-questions using spans from the original question. For example, for the question in Table 1, we can create the sub-questions “Which player named 2015 Diamond Head Classics MVP?” and “Which team does ANS play for?”, Recent work on question decomposition relies on distant supervision data created on top of underlying relational logical forms (Talmor and Berant, 2018), making it difficult to generalize to diverse natural language questions such as those on H OTPOT QA (Yang et al., 2018). In contrast, our method presents a new approach which simplifies the process as a span prediction, thus requiring only 400 decomposition examples to train a competitive decomposition neural model. Furthermore, we propose a rescoring approach which obtains answers from different possible decompositions and rescores each decomposition with the answer to decide on the final answer, rather than deciding on the decomposition in the beginning. Our experiments show that D ECOMP R"
P19-1613,P18-1078,0,0.037198,") q1c , q2c ← form subq(Q, ent1 , ent2 , op) q3c ← op (ent1 , ANS) (ent2 , ANS) Single-hop Reading Comprehension Given a decomposition, we use a single-hop RC model to answer each sub-question. Specifically, the goal is to obtain the answer and the evidence, given the sub-question and N paragraphs. Here, the answer is a span from one of paragraphs, yes or no. The evidence is one of N paragraphs on which the answer is based. Any off-the-shelf RC model can be used. In this work, we use the BERT reading comprehension model (Devlin et al., 2019) combined with the paragraph selection approach from Clark and Gardner (2018) to handle multiple paragraphs. Given N paragraphs S1 , . . . , SN , this approach independently computes answeri and yinone from each paragraph Si , where answeri and yinone denote the answer candidate from ith paragraph and the score indicating ith paragraph does not contain the answer. The final answer is selected from the paragraph with the lowest yinone . Although this approach takes a set of multiple paragraphs as an input, it is not capable of jointly reasoning across different paragraphs. For each paragraph Si , let Ui ∈ Rn×h be the BERT encoding of the sub-question concatenated with a"
P19-1613,N19-1423,0,0.603521,"n deciding on the decomposition in the beginning. Our experiments show that D ECOMP RC outperforms other published methods on H OTPOT QA (Yang et al., 2018), while providing explainable evidence in the form of sub-questions. In addition, we evaluate with alternative distrator paragraphs and questions and show that our decomposition-based approach is more ro6097 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097–6109 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics bust than an end-to-end BERT baseline (Devlin et al., 2019). Finally, our ablation studies show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance. Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC. 2 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document (Richardson et al., 2013). Recently, the availability of large-scale reading comprehensiondatasets (Hermann et al.,"
P19-1613,N18-2007,0,0.0429541,"-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop datasets contain questions that are, or are based on relational queries (Welbl et al., 2017; Talmor and Berant, 2018). In contrast, H OTPOT QA (Yang et al., 2018), on which we evaluate our method, contains more natural, hand-written questions that are not based on relational queries. Prior methods on multi-hop reading comprehension focus on answering relational queries, and emphasize attention models that reason over coreference chains (Dhingra et al., 2018; Zhong et al., 2019; Cao et al., 2019). In contrast, our method focuses on answering natural language questions via question decomposition. By providing decomposed single-hop sub-questions, our method allows the model’s decisions to be explainable. Our work is most related to Talmor and Berant (2018), which answers questions over web snippets via decomposition. There are three key differences between our method and theirs. First, they decompose questions that are correspond to relational queries, whereas we focus on natural language questions. Next, they rely on an underlying relational query"
P19-1613,D18-1259,0,0.635327,"propose D ECOMP RC, a system for multi-hop RC, that learns to break compositional multi-hop questions into simpler, singlehop sub-questions using spans from the original question. For example, for the question in Table 1, we can create the sub-questions “Which player named 2015 Diamond Head Classics MVP?” and “Which team does ANS play for?”, Recent work on question decomposition relies on distant supervision data created on top of underlying relational logical forms (Talmor and Berant, 2018), making it difficult to generalize to diverse natural language questions such as those on H OTPOT QA (Yang et al., 2018). In contrast, our method presents a new approach which simplifies the process as a span prediction, thus requiring only 400 decomposition examples to train a competitive decomposition neural model. Furthermore, we propose a rescoring approach which obtains answers from different possible decompositions and rescores each decomposition with the answer to decide on the final answer, rather than deciding on the decomposition in the beginning. Our experiments show that D ECOMP RC outperforms other published methods on H OTPOT QA (Yang et al., 2018), while providing explainable evidence in the form"
P19-1613,P17-1147,1,0.883857,"s show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance. Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC. 2 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document (Richardson et al., 2013). Recently, the availability of large-scale reading comprehensiondatasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) has led to the development of advanced RC models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2019). Most of the questions on these datasets can be answered in a single sentence (Min et al., 2018), which is a key difference from multi-hop reading comprehension. Multi-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop datasets contain questions that are, or are based on relational queries (Welbl et al., 2017; Talmor and Berant, 2018). In contrast, H OTPOT QA (Yan"
P19-1613,P11-1060,0,0.0305598,"their model, while our method requires only 400 decomposition examples. Finally, they decide on a decomposition operation exclusively based on the question. In contrast, we decompose the question in multiple ways, obtain answers, and determine the best decomposition based on all given context, which we show is crucial to improving performance. Semantic Parsing. Semantic parsing is a larger area of work that involves producing logical forms from natural language utterances, which are then usually executed over structured knowledge graphs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Our work is inspired by the idea of compositionality from semantic parsing, however, we focus on answering natural language questions over unstructured text documents. 3 Model 3.1 Overview In multi-hop reading comprehension, a system answers a question over a collection of paragraphs by combining evidence from multiple paragraphs. In contrast to single-hop reading comprehension, in which a system can obtain good performance using a single sentence (Min et al., 2018), multi-hop reading comprehension typically requires more complex reasoning over how two pieces of evidence relate to each other"
P19-1613,P18-1160,1,0.923302,"teractive demo are publicly available at https://github.com/ shmsw25/DecompRC. 2 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document (Richardson et al., 2013). Recently, the availability of large-scale reading comprehensiondatasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) has led to the development of advanced RC models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2019). Most of the questions on these datasets can be answered in a single sentence (Min et al., 2018), which is a key difference from multi-hop reading comprehension. Multi-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop datasets contain questions that are, or are based on relational queries (Welbl et al., 2017; Talmor and Berant, 2018). In contrast, H OTPOT QA (Yang et al., 2018), on which we evaluate our method, contains more natural, hand-written questions that are not based on relational queries. Prior methods on multi-hop reading comprehension focus on answering relational querie"
P19-1613,D17-1238,0,0.0134653,"of three types. In addition, these multi-hop reasoning types correspond to the types of compositional questions identified by Berant et al. (2013) and Talmor and Berant (2018). 3.2 Decomposition The goal of question decomposition is to convert a multi-hop question into simpler, single-hop subquestions. A key challenge of decomposition is that it is difficult to obtain annotations for how to decompose questions. Moreover, generating the question word-by-word is known to be a difficult task that requires substantial training data and is not straight-forward to evaluate (Gatt and Krahmer, 2018; Novikova et al., 2017). Instead, we propose a method to create subquestions using span prediction over the question. 6099 The key idea is that, in practice, each sub-question can be formed by copying and lightly editing a key span from the original question, with different span extraction and editing required for each reasoning type. For instance, the bridging question in Table 2 requires finding “the player named 2015 Diamond Head Classic MVP” which is easily extracted as a span. Similarly, the intersection question in Table 2 specifies the type of entity to find (“which actor and comedian”), with two conditions ("
P19-1613,Q18-1021,0,\N,Missing
Q15-1042,Q13-1005,0,0.00664686,"Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). W"
Q15-1042,D14-1159,0,0.0163322,"aking advantage of a richer representation of quantified nouns and their properties, as well as the recursive nature of equation trees. These allow A LGES to use a bottom-up approach to learn the correspondence between spans of texts and arithmetic operators (corresponding to intermediate nodes in the tree). A LGES then scores equations using global structure of the problem to produce the final result. Our work is also related to research in using ILP to enforce global constraints in NLP applications (Roth and Yih, 2004). Most previous work (Srikumar and Roth, 2011; Goldwasser and Roth, 2011; Berant et al., 2014; Liu et al., 2015) utilizes ILP as an inference procedure to find the best global prediction over initially trained local classifiers. Similarly, we use ILP to enforce global and domain specific constraints. We, however, use ILP to form candidate equations which are then used to generate training data for our classifiers. Our work is also related to parser re-ranking (Collins, 2005; Ge and Mooney, 2005), where a re-ranker model attempts to improve the output of an existing probabilistic parser. Similarly, the global equation model designed in A LGES attempts to re-rank equations based on glob"
Q15-1042,P09-1010,0,0.0315966,"t any manual annotation; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015)"
Q15-1042,J05-1003,0,0.0221186,"generating and ranking equation trees; (2) We show how to score the likelihood of equation trees by learning discriminative models trained from a small number of word problems and their solutions – without any manual annotation; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often r"
Q15-1042,de-marneffe-etal-2006-generating,0,0.0321535,"Missing"
Q15-1042,P10-1126,0,0.00889558,"scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra wor"
Q15-1042,W05-0602,0,0.0364487,"Our work is also related to research in using ILP to enforce global constraints in NLP applications (Roth and Yih, 2004). Most previous work (Srikumar and Roth, 2011; Goldwasser and Roth, 2011; Berant et al., 2014; Liu et al., 2015) utilizes ILP as an inference procedure to find the best global prediction over initially trained local classifiers. Similarly, we use ILP to enforce global and domain specific constraints. We, however, use ILP to form candidate equations which are then used to generate training data for our classifiers. Our work is also related to parser re-ranking (Collins, 2005; Ge and Mooney, 2005), where a re-ranker model attempts to improve the output of an existing probabilistic parser. Similarly, the global equation model designed in A LGES attempts to re-rank equations based on global problem structure. 3 On Monday, 375 students went on a trip to the zoo. All 7 buses were ﬁlled and 4 students had to travel in cars. How many students were in each bus ? 1. Ground text w into base Qsets (sec(on 5) Qnt: 375 Ent: Student Qnt: x Ent: Student Ctr: Bus Tl (w): subset of T(w) yielding correct solu(on = = -s 375s 2 Problems involving simultaneous equations require combining multiple equation"
Q15-1042,P06-2034,0,0.0261535,"ranking equation trees; (2) We show how to score the likelihood of equation trees by learning discriminative models trained from a small number of word problems and their solutions – without any manual annotation; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning"
Q15-1042,N15-1086,1,0.0320073,"ch aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et"
Q15-1042,D14-1058,1,0.84117,"t al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond"
Q15-1042,P14-1026,0,0.572616,"et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond verbs. For instance, “Tina has 2 cats. John has 3 more cats than T"
Q15-1042,D10-1119,0,0.0108677,"s; (2) We show how to score the likelihood of equation trees by learning discriminative models trained from a small number of word problems and their solutions – without any manual annotation; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence"
Q15-1042,P09-1011,0,0.00650665,"; and (3) We demonstrate empirically that A LGES has broader scope than the system of Hosseini et al. (2014), and overcomes the brittleness of the method of Kushman et al. (2014). 2 Previous Work Our work is related to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word pr"
Q15-1042,N15-1114,0,0.00562616,"richer representation of quantified nouns and their properties, as well as the recursive nature of equation trees. These allow A LGES to use a bottom-up approach to learn the correspondence between spans of texts and arithmetic operators (corresponding to intermediate nodes in the tree). A LGES then scores equations using global structure of the problem to produce the final result. Our work is also related to research in using ILP to enforce global constraints in NLP applications (Roth and Yih, 2004). Most previous work (Srikumar and Roth, 2011; Goldwasser and Roth, 2011; Berant et al., 2014; Liu et al., 2015) utilizes ILP as an inference procedure to find the best global prediction over initially trained local classifiers. Similarly, we use ILP to enforce global and domain specific constraints. We, however, use ILP to form candidate equations which are then used to generate training data for our classifiers. Our work is also related to parser re-ranking (Collins, 2005; Ge and Mooney, 2005), where a re-ranker model attempts to improve the output of an existing probabilistic parser. Similarly, the global equation model designed in A LGES attempts to re-rank equations based on global problem structur"
Q15-1042,D15-1118,0,0.112838,"(Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well fo"
Q15-1042,W04-2401,0,0.0641659,"ve been observed during training. Instead, our method maps word problems to equation trees, taking advantage of a richer representation of quantified nouns and their properties, as well as the recursive nature of equation trees. These allow A LGES to use a bottom-up approach to learn the correspondence between spans of texts and arithmetic operators (corresponding to intermediate nodes in the tree). A LGES then scores equations using global structure of the problem to produce the final result. Our work is also related to research in using ILP to enforce global constraints in NLP applications (Roth and Yih, 2004). Most previous work (Srikumar and Roth, 2011; Goldwasser and Roth, 2011; Berant et al., 2014; Liu et al., 2015) utilizes ILP as an inference procedure to find the best global prediction over initially trained local classifiers. Similarly, we use ILP to enforce global and domain specific constraints. We, however, use ILP to form candidate equations which are then used to generate training data for our classifiers. Our work is also related to parser re-ranking (Collins, 2005; Ge and Mooney, 2005), where a re-ranker model attempts to improve the output of an existing probabilistic parser. Simila"
Q15-1042,D15-1202,0,0.536055,"al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond verbs. For instance,"
Q15-1042,D15-1171,1,0.671041,"nd Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond verbs. For instance, “Tina has 2 cats. John has 3 more cats than Tina. How many cats do they have together?” and “Tina has 2 cats. Jo"
Q15-1042,D15-1135,0,0.519108,"in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. Whil"
Q15-1042,S14-1015,0,0.013061,"to situated semantic interpretation, which aims to map natural language sentences to formal meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Kwiatkowski et al., 2010). More closely related is work on language grounding, whose goal is the interpretation of a sentence in the context of a world representation (Branavan et al., 2009; Liang et al., 2009; Chen et al., 2010; Bordes et al., 2010; Feng and Lapata, 2010; Hajishirzi et al., 2011; Matuszek et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works c"
Q15-1042,D15-1096,0,0.509188,"k et al., 2012; Hajishirzi et al., 2012; Artzi and Zettlemoyer, 2013; KoncelKedziorski et al., 2014; Yatskar et al., 2014; Seo et al., 2014; Hixon et al., 2015). However, while most previous work considered individual sentences in isolation, solving word problems often requires 586 reasoning across the multi-sentence discourse of the problem text. Recent efforts in the math domain have studied number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015), and geometry word problems (Seo et al., 2015). We discuss in more detail below two pioneering works closely related to our own. Hosseini et al. (2014) solve elementary addition and subtraction problems by learning verb categories. They ground the problem text to a semantics of entities and containers, and decide if quantities are increasing or decreasing in a container based upon the learned verb categories. While relying only on verb categories works well for + and −, modeling ∗ or / requires going beyond verbs. For instance, “Tina has 2 cats. John has 3 more cats than Tina. How many cats d"
Q15-1042,Q15-1001,0,\N,Missing
Q15-1042,D11-1012,0,\N,Missing
Q15-1042,D14-1043,1,\N,Missing
S18-1125,P11-1051,0,0.192322,"Missing"
S18-1125,W12-3202,0,0.0355986,"r and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Model T1.1 T2-E T2-C Our system (Micro) Our system (Macro) 78.9 78.4 50.0 49.3 39.1 37.0 Team-1 Team-2 81.7 76.7 48.8 37.4 49.3 33.6 Table 2: Competition result for the top 3 teams. The official evaluation metric is macro F1 score. T1.1 means Subtask 1.1, T2-E means Subtask 2 senerio 1 (extraction task), T2-C means Subtask 2 senerio 2 (classification task). Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Most previous work focuses on unsupervised methods for extracting scientific terms such as bootstrapping Gupta and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although"
S18-1125,N12-1073,0,0.0386558,"Missing"
S18-1125,W12-4303,0,0.181219,"Missing"
S18-1125,P17-2054,0,0.0362696,"score. T1.1 means Subtask 1.1, T2-E means Subtask 2 senerio 1 (extraction task), T2-C means Subtask 2 senerio 2 (classification task). Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Most previous work focuses on unsupervised methods for extracting scientific terms such as bootstrapping Gupta and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significa"
S18-1125,bird-etal-2008-acl,0,0.0822868,"enate the distance embedding with all the other features. The concatenated features are then projected down to a lower dimension through tanh function and make the final prediction through a sof tmax function. 3 Experimental Setup External Data We use two external resources for pretraining word embeddings: i) the Semantic Scholar Corpus,2 a collection of over 20 million research papers from which we extract a subset of 110k abstracts of publications in the artificial intelligence area; and ii) the ACL Anothology Reference Corpus, which contains 22k full papers published in the ACL Anothology (Bird et al., 2008). Baseline We compare our model with a baseline that removes the Concept Selection Layer and replaces it with a weighted sum (using attention) of hidden states (from the Sequential LSTM Layer) for all words in a concept. For all experiments, we explore tuning with two different evaluation metrics: macro-F1 score and micro-F1 score.3 We keep the pre-trained concept embedding fixed as additional input feature. The word embedding dimension is 250; the LSTM hidden dimension is 100 (for both sequential and dependency layer); the character-level hidden dimension is 25; and the optimization algorithm"
S18-1125,L16-1586,0,0.166587,"Missing"
S18-1125,D17-1279,1,0.943033,"unt when relevant for the classification task (5 out of the 6 semantic relations are asymmetrical). We will use this example throughout the paper to illustrate various parts of our system. The SemEval 2018 Task 7 dataset contains 350 abstracts from the ACL Anthology for training and validation, and 150 abstracts for testing each subtask. Since the scale of the data is small for supervised training of neural systems, we introduce several strategies to leverage a large quantity of unlabeled scientific articles. In addition to initializing a neural system with pre-trained word embeddings, as in (Luan et al., 2017), we also try to incorporate embeddings of concepts that span multiple words. In neural models such as (Miwa and Bansal, 2016), phrases are often represented by an average (or weighted average) of the token’s sequential LSTM representation. The intuition behind explicit modeling of multi-word concept embeddings is that the concept use may be different from that of its individual words. Due to the size of the dataset and the nature of scientific literature, a large number of the scientific terms in the test set have never appeared in the training set, so supervised learning of the phrase embedd"
S18-1125,P16-1105,0,0.397933,"le throughout the paper to illustrate various parts of our system. The SemEval 2018 Task 7 dataset contains 350 abstracts from the ACL Anthology for training and validation, and 150 abstracts for testing each subtask. Since the scale of the data is small for supervised training of neural systems, we introduce several strategies to leverage a large quantity of unlabeled scientific articles. In addition to initializing a neural system with pre-trained word embeddings, as in (Luan et al., 2017), we also try to incorporate embeddings of concepts that span multiple words. In neural models such as (Miwa and Bansal, 2016), phrases are often represented by an average (or weighted average) of the token’s sequential LSTM representation. The intuition behind explicit modeling of multi-word concept embeddings is that the concept use may be different from that of its individual words. Due to the size of the dataset and the nature of scientific literature, a large number of the scientific terms in the test set have never appeared in the training set, so supervised learning of the phrase embeddings is not feasible. Therefore, we pre-trained scientific term embeddings on a large scientific corpus and provide a strategy"
S18-1125,Q17-1008,0,0.0577441,"or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significantly improve performance. 6 Conclusion This paper describes the system of the UWNLP team submitted to SemEval 2018 Task 7. We extend state-of-the-art neural models for information extraction by proposing a Concept Selection module which can leverage the semantic information of concepts pre-trained from a large scholarly dataset. Our system ranked second in the relation classification task (subtask 1.1 and subtask 2 senerio 2), and first in the relation extraction task (subtask 2 scenario 1). Acknowledgments This re"
S18-1125,E17-1110,0,0.0318402,"tions (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significantly improve performance. 6 Conclusion This paper describes the system of the UWNLP team submitted to SemEval 2018 Task 7. We extend state-of-the-art neural models for information extraction by proposing a Concept Selection module which can leverage the semantic information of concepts pre-trained from a large scholarly dataset. Our system ranked second in the relation classification task (subtask 1.1 and subtask 2 senerio 2), and first in the relation extraction task (subtask 2 scenario 1). Acknowledgments This research was supported by"
S18-1125,P15-1061,0,0.0642462,"ting scientific terms such as bootstrapping Gupta and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significantly improve performance. 6 Conclusion This paper describes the system of the UWNLP team submitted to SemEval 2018 Task 7. We extend state-of-the-art neural models for information extraction by proposing a Concept Selection module which can leverage the semantic information of concepts pre-trained from a large scholarly dataset. Our system ranked second in the relation classification task (subtask 1.1 and subtask 2 senerio 2), and"
S18-1125,W12-3203,0,0.136778,"Missing"
S18-1125,W12-3204,0,0.0699626,"d citation sentiment (Athar and Teufel, 2012b,a), citation networks (Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014), summarization (Abu-Jbara and Model T1.1 T2-E T2-C Our system (Micro) Our system (Macro) 78.9 78.4 50.0 49.3 39.1 37.0 Team-1 Team-2 81.7 76.7 48.8 37.4 49.3 33.6 Table 2: Competition result for the top 3 teams. The official evaluation metric is macro F1 score. T1.1 means Subtask 1.1, T2-E means Subtask 2 senerio 1 (extraction task), T2-C means Subtask 2 senerio 2 (classification task). Radev, 2011) and some analysis of research community (Vogel and Jurafsky, 2012; Anderson et al., 2012). However, due to scarce hand-annotated data resources, previous work on information extraction (IE) for scientific literature is very limited. Most previous work focuses on unsupervised methods for extracting scientific terms such as bootstrapping Gupta and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and S"
S18-1125,C16-1138,0,0.0265432,"and Manning (2011); Tsai et al. (2013), or extracting relations (G´abor et al., 2016). Luan et al. (2017); Augenstein and Søgaard (2017) applied semi-supervised learning and multi-task learning to neural based models to leverage large unannotated scholarly datasets for a scientific term extraction task (Augenstein and Søgaard, 2017). Although not much supervised relation extraction work has been done on scientific literature, neural network techniqueshave obtained the state of the art for general domain relation extraction. Both convolutional (Santos et al., 2015) and RNNbased architectures (Xu et al., 2016; Miwa and Bansal, 2016; Peng et al., 2017; Quirk and Poon, 2017) have been successfully applied to the task and significantly improve performance. 6 Conclusion This paper describes the system of the UWNLP team submitted to SemEval 2018 Task 7. We extend state-of-the-art neural models for information extraction by proposing a Concept Selection module which can leverage the semantic information of concepts pre-trained from a large scholarly dataset. Our system ranked second in the relation classification task (subtask 1.1 and subtask 2 senerio 2), and first in the relation extraction task (subt"
S19-2153,P16-1202,0,0.0154978,"sible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibit interesting semantic phenomena like cross-senten"
S19-2153,D15-1202,0,0.585751,"y: The lengths of two sides of a triangle are (x − 2) and (x + 2), where x > 2. Which of the following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metric"
S19-2153,D17-1081,0,0.323397,"ew Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibit interesting semantic phenomena like cross-sentence coreference and indirect coreference (e.g. in the geometry question from Figure"
S19-2153,D15-1171,1,0.884394,"Missing"
S19-2153,D17-1083,1,0.849523,"Missing"
S19-2153,D15-1135,0,0.137105,"are (x − 2) and (x + 2), where x > 2. Which of the following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of"
S19-2153,D14-1058,1,0.915493,"ketball tournament involving 8 teams, each team played 4 games with each of the other teams. How many games were played at this tournament? Geometry: The lengths of two sides of a triangle are (x − 2) and (x + 2), where x > 2. Which of the following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (1"
S19-2153,D17-1088,0,0.0895999,"ere has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibit interesting semantic phenomena like cross-sentence coreference and indirect coreference (e.g. in the geometry question from Figure 1, “the third side” refers t"
S19-2153,D15-1096,0,0.0142085,"following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibi"
S19-2153,N16-1136,1,0.815245,"Missing"
S19-2153,P14-1026,0,0.219149,"ing baseline. 1 Open-vocabulary algebra: At a basketball tournament involving 8 teams, each team played 4 games with each of the other teams. How many games were played at this tournament? Geometry: The lengths of two sides of a triangle are (x − 2) and (x + 2), where x > 2. Which of the following ranges includes all and only the possible values of the third side y? (A) 0 < y < x (B) 0 < y < 2x (C) 4 < y < 2x Overview Over the past four years, there has been a surge of interest in math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math q"
S19-2153,P17-1015,0,0.0625938,"n math question answering. Research groups from around the globe have published papers on the topic, including MIT (Kushman et al., 2014), University of Washington (Hosseini et al., 2014; Koncel-Kedziorski et al., 2015), National Institute of Informatics, Japan (Matsuzaki et al., 2014), University of Illinois (Roy and Roth, 2015), Microsoft Research (Shi et al., 2015; Upadhyay and Chang, 2016), Baidu (Zhou et al., 2015), Arizona State University (Mitra and Baral, 2016), KU Leuven (Dries et al., 2017), Carnegie Mellon University (Sachan et al., 2017), Tencent (Wang et al., 2017), and DeepMind (Ling et al., 2017). Math question answering has several attractive properties that have rekindled this interest: Figure 1: Example math questions from different genres. correct answer (11) has a subtle relationship to the other quantities mentioned in the text (3 and 15). There is no obvious shortcut (like word association metrics on a bag-of-words representation of the question) to guessing 11. 3. Math questions exhibit interesting semantic phenomena like cross-sentence coreference and indirect coreference (e.g. in the geometry question from Figure 1, “the third side” refers to a triangle introduced in a previ"
S19-2153,S19-2227,0,0.0273382,"Missing"
W12-1629,E06-1022,0,\N,Missing
W12-1629,W11-2013,0,\N,Missing
W12-1629,W09-3935,0,\N,Missing
W12-1629,W09-3914,0,\N,Missing
W12-1629,W09-3936,0,\N,Missing
W12-1629,W11-2012,0,\N,Missing
