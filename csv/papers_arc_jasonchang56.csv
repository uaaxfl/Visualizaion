2021.rocling-1.32,Automatic Extraction of {E}nglish Grammar Pattern Correction Rules,2021,-1,-1,3,0,2387,kuanyu shen,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"We introduce a method for generating error-correction rules for grammar pattern errors in a given annotated learner corpus. In our approach, annotated edits in the learner corpus are converted into edit rules for correcting common writing errors. The method involves automatic extraction of grammar patterns, and automatic alignment of the erroneous patterns and correct patterns. At run-time, grammar patterns are extracted from the grammatically correct sentences, and correction rules are retrieved by aligning the extracted grammar patterns with the erroneous patterns. Using the proposed method, we generate 1,499 high-quality correction rules related to 232 headwords. The method can be used to assist ESL students in avoiding grammatical errors, and aid teachers in correcting students{'} essays. Additionally, the method can be used in the compilation of collocation error dictionaries and the construction of grammar error correction systems."
2021.rocling-1.39,Learning to Find Translation of Grammar Patterns in Parallel Corpus,2021,-1,-1,6,0,2413,kaiwen tuan,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"We introduce a method for assisting English as Second Language (ESL) learners by providing translations of Collins COBUILD grammar patterns(GP) for a given word. In our approach, bilingual parallel corpus is transformed into bilingual GP pairs aimed at providing native language support for learning word usage through GPs. The method involves automatically parsing sentences to extract GPs, automatically generating translation GP pairs from bilingual sentences, and automatically extracting common bilingual GPs. At run-time, the target word is used for lookup GPs and translations, and the retrieved common GPs and their example sentences are shown to the user. We present a prototype phrase search engine, Linggle GPTrans, that implements the methods to assist ESL learners. Preliminary evaluation on a set of more than 300 GP-translation pairs shows that the methods achieve 91{\%} accuracy."
2021.rocling-1.43,Identify Bilingual Patterns and Phrases from a Bilingual Sentence Pair,2021,-1,-1,3,1,2414,yijyun chen,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This paper presents a method for automatically identifying bilingual grammar patterns and extracting bilingual phrase instances from a given English-Chinese sentence pair. In our approach, the English-Chinese sentence pair is parsed to identify English grammar patterns and Chinese counterparts. The method involves generating translations of each English grammar pattern and calculating translation probability of words from a word-aligned parallel corpora. The results allow us to extract the most probable English-Chinese phrase pairs in the sentence pair. We present a prototype system that applies the method to extract grammar patterns and phrases in parallel sentences. An evaluation on randomly selected examples from a dictionary shows that our approach has reasonably good performance. We use human judge to assess the bilingual phrases generated by our approach. The results have potential to assist language learning and machine translation research."
2021.rocling-1.44,Extracting Academic Senses: Towards An Academic Writer{'}s Dictionary,2021,-1,-1,3,0,2425,hsinyun chung,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"We present a method for determining intended sense definitions of a given academic word in an academic keyword list. In our approach, the keyword list are converted into unigram of all possible Mandarin translations, intended or not.The method involve converting words in the keyword list into all translations using a bilingual dictionary, computing the unigram word counts of translations, and computing character counts from the word counts. At run-time, each definition (with associated translation) of the given word is scored with word and character counts, and the definition with the highest count is returned. We present a prototype system for the Academic Keyword List to generate definitions and translation for pedagogy purposes. We also experimented with clustering definition embeddings of all words and definitions, and identifying intended sense in favor of embedding in larger clusters.Preliminary evaluation shows promising performance. This endeavor is a step towards creating a full-fledged dictionary from an academic word list."
2020.rocling-1.3,Improving Phrase Translation Based on Sentence Alignment of {C}hinese-{E}nglish Parallel Corpus,2020,-1,-1,3,1,2414,yijyun chen,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.rocling-1.34,Email Writing Assistant System,2020,-1,-1,1,1,2389,jason chang,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.ijclclp-2.3,æ¹åè©å½å°é½ä»¥æ·åçèªç¿»è­¯ä¹æ¹æ³ (Improving Word Alignment for Extraction Phrasal Translation),2020,-1,-1,3,1,2414,yijyun chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 25, Number 2, December 2020",0,None
2020.ijclclp-1.1,{C}hinese Spelling Check based on Neural Machine Translation,2020,-1,-1,5,1,19033,jhihjie chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 25, Number 1, June 2020",0,None
2020.acl-demos.17,{L}inggle{W}rite: a Coaching System for Essay Writing,2020,-1,-1,4,0,23147,chungting tsai,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper presents LinggleWrite, a writing coach that provides writing suggestions, assesses writing proficiency levels, detects grammatical errors, and offers corrective feedback in response to user{'}s essay. The method involves extracting grammar patterns, training models for automated essay scoring (AES) and grammatical error detection (GED), and finally retrieving plausible corrections from a n-gram search engine. Experiments on public test sets indicate that both AES and GED models achieve state-of-the-art performance. These results show that LinggleWrite is potentially useful in helping learners improve their writing skills."
P19-3033,Level-Up: Learning to Improve Proficiency Level of Essays,2019,0,0,4,0,25483,wenbin han,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We introduce a method for generating suggestions on a given sentence for improving the proficiency level. In our approach, the sentence is transformed into a sequence of grammatical elements aimed at providing suggestions of more advanced grammar elements based on originals. The method involves parsing the sentence, identifying grammatical elements, and ranking related elements to recommend a higher level of grammatical element. We present a prototype tutoring system, Level-Up, that applies the method to English learners{'} essays in order to assist them in writing and reading. Evaluation on a set of essays shows that our method does assist user in writing."
P19-3034,Learning to Link Grammar and Encyclopedic Information of Assist {ESL} Learners,2019,0,0,9,1,19033,jhihjie chen,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We introduce a system aimed at improving and expanding second language learners{'} English vocabulary. In addition to word definitions, we provide rich lexical information such as collocations and grammar patterns for target words. We present Linggle Booster that takes an article, identifies target vocabulary, provides lexical information, and generates a quiz on target words. Linggle Booster also links named-entity to corresponding Wikipedia pages. Evaluation on a set of target words shows that the method have reasonably good performance in terms of generating useful and information for learning vocabulary."
N19-4005,Learning to Respond to Mixed-code Queries using Bilingual Word Embeddings,2019,0,0,2,0,25486,chiafang ho,Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations),0,"We present a method for learning bilingual word embeddings in order to support second language (L2) learners in finding recurring phrases and example sentences that match mixed-code queries (e.g., {``}æ¥ å sentence{''}) composed of words in both target language and native language (L1). In our approach, mixed-code queries are transformed into target language queries aimed at maximizing the probability of retrieving relevant target language phrases and sentences. The method involves converting a given parallel corpus into mixed-code data, generating word embeddings from mixed-code data, and expanding queries in target languages based on bilingual word embeddings. We present a prototype search engine, x.Linggle, that applies the method to a linguistic search engine for a parallel corpus. Preliminary evaluation on a list of common word-translation shows that the method performs reasonablly well."
D19-3040,{T}ell{M}e{W}hy: Learning to Explain Corrective Feedback for Second Language Learners,2019,0,0,2,0,26752,yihuei lai,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"We present a writing prototype feedback system, TellMeWhy, to provide explanations of errors in submitted essays. In our approach, the sentence with corrections is analyzed to identify error types and problem words, aimed at customizing explanations based on the context of the error. The method involves learning the relation of errors and problem words, generating common feedback patterns, and extracting grammar patterns, collocations and example sentences. At run-time, a sentence with corrections is classified, and the problem word and template are identified to provide detailed explanations. Preliminary evaluation shows that the method has potential to improve existing commercial writing services."
2019.rocling-1.38,æ¼¢èªåç©åçå¤§æ¸æç ç©¶(A Data Scientific Study of Transitivization in {C}hinese),2019,-1,-1,5,0,27263,weitien tsai,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.40,æ¨è¨»è±ä¸­åæ­¥æ¨£å¼ææ³ä¹ç ç©¶(Annotating Synchronous Grammar Patterns across {E}nglish and {C}hinese),2019,-1,-1,3,1,15570,chingyu yang,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
Y18-1042,{C}hinese Spelling Check based on Neural Machine Translation,2018,0,0,3,0,19034,chiaowen li,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1062,{S}mart{W}rite: Extracting {C}hinese Lexical Grammar Patterns Using Dependency Parsing,2018,0,0,4,0,27539,chengcyuan peng,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
O18-1027,ç¯è½ç¥è­åç­æ©å¨äºº (Energy Saving Knowledge Chatbot) [In {C}hinese],2018,0,0,5,1,19033,jhihjie chen,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),0,None
C18-2018,Cool {E}nglish: a Grammatical Error Correction System Based on Large Learner Corpora,2018,0,1,4,0,30722,yuchun lo,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"This paper presents a grammatical error correction (GEC) system that provides corrective feedback for essays. We apply the sequence-to-sequence model, which is frequently used in machine translation and text summarization, to this GEC task. The model is trained by EF-Cambridge Open Language Database (EFCAMDAT), a large learner corpus annotated with grammatical errors and corrections. Evaluation shows that our system achieves competitive performance on a number of publicly available testsets."
C18-2022,{L}anguage{N}et: Learning to Find Sense Relevant Example Sentences,2018,0,0,4,0,30724,shangchien cheng,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"In this paper, we present a system, LanguageNet, which can help second language learners to search for different meanings and usages of a word. We disambiguate word senses based on the pairs of an English word and its corresponding Chinese translations in a parallel corpus, UM-Corpus. The process involved performing word alignment, learning vector space representations of words and training a classifier to distinguish words into groups of senses. LanguageNet directly shows the definition of a sense, bilingual synonyms and sense relevant examples."
I17-3013,Verb Replacer: An {E}nglish Verb Error Correction System,2017,7,0,3,0,32844,yuhsuan wu,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"According to the analysis of Cambridge Learner Corpus, using a wrong verb is the most common type of grammatical errors. This paper describes Verb Replacer, a system for detecting and correcting potential verb errors in a given sentence. In our approach, alternative verbs are considered to replace the verb based on an error-annotated corpus and verb-object collocations. The method involves applying regression on channel models, parsing the sentence, identifying the verbs, retrieving a small set of alternative verbs, and evaluating each alternative. Our method combines and improves channel and language models, resulting in high recall of detecting and correcting verb misuse."
I17-3014,Learning Synchronous Grammar Patterns for Assisted Writing for Second Language Learners,2017,2,0,4,0,32845,chien wu,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"In this paper, we present a method for extracting Synchronous Grammar Patterns (SGPs) from a given parallel corpus in order to assisted second language learners in writing. A grammar pattern consists of a head word (verb, noun, or adjective) and its syntactic environment. A synchronous grammar pattern describes a grammar pattern in the target language (e.g., English) and its counterpart in an other language (e.g., Mandarin), serving the purpose of native language support. Our method involves identifying the grammar patterns in the target language, aligning these patterns with the target language patterns, and finally filtering valid SGPs. The extracted SGPs with examples are then used to develop a prototype writing assistant system, called WriteAhead/bilingual. Evaluation on a set of randomly selected SGPs shows that our system provides satisfactory writing suggestions for English as a Second Language (ESL) learners."
C16-2035,Linggle Knows: A Search Engine Tells How People Write,2016,6,0,5,1,19033,jhihjie chen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"This paper shows the great potential of incorporating different approaches to help writing. Not only did they solve different kinds of writing problems, but also they complement and reinforce each other to be a complete and effective solution. Despite the extensive and multifaceted feedback and suggestion, writing is not all about syntactically or lexically well-written. It involves contents, structure, the certain understanding of the background, and many other factors to compose a rich, organized and sophisticated text. (e.g., conventional structure and idioms in academic writing). There is still a long way to go to accomplish the ultimate goal. We envision the future of writing to be a joyful experience with the help of instantaneous suggestion and constructive feedback."
Y15-2005,Learning Sentential Patterns of Various Rhetoric Moves for Assisted Academic Writing,2015,19,0,6,1,32846,jim chang,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"We introduce a new method for extracting representative sentential patterns from a corpus for the purpose of assisting ESL learners in academic writing. In our approach, sentences are transformed into patterns for statistical analysis and filtering, and then are annotated with relevant rhetoric moves. The method involves annotating every sentence in a given corpus with part of speech and base phrase information, converting the sentence into formulaic patterns, and filtering salient patterns for key content words (verbs and nouns). We display the patterns in the interactive writing environment, WriteAhead, to prompt the user as they type away."
P15-4024,{W}rite{A}head: Mining Grammar Patterns in Corpora for Assisted Writing,2015,21,2,5,0,37406,tzuhsi yen,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"This paper describes WriteAhead, a resource-rich, Interactive Writing Environment that provides L2 learners with writing prompts, as well as xe2x80x9dget it rightxe2x80x9d advice, to helps them write fluently and accurately. The method involves automatically analyzing reference and learner corpora, extracting grammar patterns with example phrases, and computing dubious, overused patterns. At run-time, as the user types (or mouses over) a word, the system automatically retrieves and displays grammar patterns and examples, most relevant to the word. The user can opt for patterns from a general corpus, academic corpus, learner corpus, or commonly overused dubious patterns found in a learner corpus. WriteAhead proactively engages the user with steady, timely, and spot-on information for effective assisted writing. Preliminary experiments show that WriteAhead fulfills the design goal of fostering learner independence and encouraging self-editing, and is likely to induce better writing, and improve writing skills in the long run."
O15-1009,åºæ¼è²æ°å®çèªååæèªæåº«èæ¨å®ææ­¥ (A {B}ayesian approach to determine move tags in corpus) [In {C}hinese],2015,9,0,3,0,37567,jialien hsu,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
N15-3022,{W}rite{A}head2: Mining Lexical Grammar Patterns for Assisted Writing,2015,7,2,2,1,32846,jim chang,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"This paper describes WriteAhead2, an interactive writing environment that provides lexical and grammatical suggestions for second language learners, and helps them write fluently and avoid common writing errors. The method involves learning phrase templates from dictionary examples, and extracting grammar patterns with example phrases from an academic corpus. At run-time, as the user types word after word, the actions trigger a list after list of suggestions. Each successive list contains grammar patterns and examples, most relevant to the half-baked sentence. WriteAhead2 facilitates steady, timely, and spot-on interactions between learner writers and relevant information for effective assisted writing. Preliminary experiments show that WriteAhead2 has the potential to induce better writing and improve writing skills."
Y14-1034,{T}ake{T}wo: A Word Aligner based on Self Learning,2014,27,0,3,1,32846,jim chang,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"State of the art statistical machine translation systems are typically trained by symmetrizing word alignments in two translation directions. We introduce a new method that improves word alignment results, based on self learn- ing using the initial symmetrized word align- ments results. The method involves align- ing words and symmetrizing alignments, gen- erating labeled training data, and construct a classifier for predicting word-translation rela- tion in another alignment round. In the first alignment round, we use the original grow- diag-final-and procedure, while in the second round, we use the classifier and a modified GDFA procedure to validate and fill in align- ment links. We present a prototype system, TakeTwo, which applies the method to im- prove on GDFA. Preliminary experiments and evaluation on a hand-annotated dataset show that the method significantly increases the pre- cision rate by a wide margin (16%) with comparable recall rate (-3%)."
W14-6832,{C}hinese Spell Checking Based on Noisy Channel Model,2014,4,2,3,1,38140,hsunwen chiu,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Chinese spell checking is an important component of many Chinese NLP applications, including word processors, search engines, and automatic essay rating. Compared to English, Chinese has no word boundaries, and there are various Chinese input methods that cause different kinds of typos. Therefore, it is more difficult to develop a spell checker for Chinese. In this paper, we introduce a novel method for correcting Chinese errors based on sound or shape similarity. In our approach, potential typos in a given sentence are then corrected using a channel model and a character-based language model in the noisy channel model. In the training phase, we estimate the channel probabilities for each character based on ngrams in Web corpus. At run-time, the system generates correction candidates for each character in the given sentence and selects the appropriate correction using the channel model and the language model. The experimental results show that the proposed method achieves significantly better accuracy and recall than more complicated methods in the previous work."
W14-1712,{NTHU} at the {C}o{NLL}-2014 Shared Task,2014,7,3,8,1,34550,jiancheng wu,Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task,0,"In this paper, we describe a system for correcting grammatical errors in texts written by non-native learners. In our approach, a given sentence with syntactic features are sent to a number of modules, each focuses on a specific error type. A main program integrates corrections from these modules and outputs the corrected sentence. We evaluated our system on the official test data of the CoNLL-2014 shared task and obtained 0.30 in F-measure."
O14-5003,å­¸è¡è«æç°¡ä»çèªåææ­¥åæèå¯«ä½æç¤º (Automatic Move Analysis of Research Articles for Assisting Writing) [In {C}hinese],2014,0,0,5,0,38748,guancheng huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 19, Number 4, {D}ecember 2014 - Special Issue on Selected Papers from {ROCLING} {XXVI}",0,None
O14-1016,å­¸è¡è«æç°¡ä»çèªåææ­¥åæèå¯«ä½æç¤º (Automatic Move Analysis of Research Articles for Assisting Writing)[In {C}hinese],2014,0,0,5,0,38748,guancheng huang,Proceedings of the 26th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2014),0,None
D14-1100,Ambiguity Resolution for Vt-N Structures in {C}hinese,2014,14,2,2,0.66851,34554,yuming hsieh,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"The syntactic ambiguity of a transitive verb (Vt) followed by a noun (N) has long been a problem in Chinese parsing. In this paper, we propose a classifier to resolve the ambiguity of Vt-N structures. The design of the classifier is based on three important guidelines, namely, adopting linguistically motivated features, using all available resources, and easy integration into a parsing model. The linguistically motivated features include semantic relations, context, and morphological structures; and the available resources are treebank, thesaurus, affix database, and large corpora. We also propose two learning approaches that resolve the problem of data sparseness by autoparsing and extracting relative knowledge from large-scale unlabeled data. Our experiment results show that the Vt-N classifier outperforms the current PCFG parser. Furthermore, it can be easily and effectively integrated into the PCFG parser and general statistical parsing models. Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambiguation through data selection and error correction."
W13-4408,{C}hinese Spelling Checker Based on Statistical Machine Translation,2013,11,16,3,1,38140,hsunwen chiu,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"Chinese spelling check is an important component for many NLP applications, including word processor and search engines. However, compared to checkers for alphabetical languages (e.g., English or French), Chinese spelling checkers are more difficult to develop, because there are no word boundaries in Chinese writing system, and errors may be caused by various Chinese input methods. In this paper, we proposed a novel method to Chinese spelling checking. Our approach involves error detection and correction based on the phrasal statistical machine translation framework. The results show that the proposed system achieves significantly better accuracy in error detecting and more satisfactory performance in error correcting."
W13-4411,Automatic {C}hinese Confusion Words Extraction Using Conditional Random Fields and the Web,2013,7,3,2,0,40678,chunhung wang,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"A ready set of commonly confused words plays an important role in spelling error detection and correction in texts. In this paper, we present a system named ACE (Automatic Confusion words Extraction), which takes a Chinese word as input (e.g., xe2x80x9cxe4xb8x8dxe8x84x9bxe8x80x8cxe8xb5xb0 xe2x80x9d) and automatically outputs its easily confused words (e.g., xe2x80x9cxe4xb8x8dxe5xbex91 xe5xbex91 xe2x80x9d, xe2x80x9cxe4xb8x8dxe9x80x95 xe9x80x95 xe9x80x95 xe9x80x95xe8x80x8cxe8xb5xb0 xe2x80x9d). The purpose of ACE is similar to web-based set expansion xe2x80x90 the problem of finding all instances (e.g. xe2x80x9cHalloweenxe2x80x9d, xe2x80x9cThanksgiving Dayxe2x80x9d, xe2x80x9cIndependence Dayxe2x80x9d, etc.) of a set given a small number of class names (e.g. xe2x80x9cholidaysxe2x80x9d). Unlike set expansion , our system is used to produce commonly confused words of a given Chinese word. In brief, we use some handcoded patterns to find a set of sentence fragments from search engine, and then assign an array of tags to each character in each sentence fragment. Finally, these tagged fragments are served as inputs to a pre-learned conditional random fields (CRFs) model. We present experiment results on 3,211 test cases, showing that our system can achieve 95.2% precision rate while maintaining 91.2% recall rate."
W13-3603,{C}o{NLL}-2013 Shared Task: Grammatical Error Correction {NTHU} System Description,2013,10,17,7,0,38604,tinghui kao,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,"Grammatical error correction has been an active research area in the field of Natural Language Processing. This paper describes the grammatical error correction system developed at NTHU in participation of the CoNLL-2013 Shared Task. The system consists of four modules in a pipeline to correct errors related to determiners, prepositions, verb forms and noun number. Although more types of errors are involved that than last yearxe2x80x99s Shared Task, leading to more complicated problem this year, our system still obtain higher F-score as compared to last year. We received an overall F-measure score of 0.325, which put our system in second place among 17 systems evaluated."
P13-4024,{L}inggle: a Web-scale Linguistic Search Engine for Words in Context,2013,17,10,5,0,36261,joanne boisson,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"In this paper, we introduce a Web-scale linguistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and additional regular expression (RE) operators. In our approach, we incorporate inverted file indexing, PoS information from BNC, and semantic indexing based on Latent Dirichlet Allocation with Google Web 1T. The method involves parsing the query to transforming it into several keyword retrieval commands. Word chunks are retrieved with counts, further filtering the chunks with the query as a RE, and finally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are also provided. In addition, Linggle provides example sentences from The New York Times on demand. The current implementation of Linggle is the most functionally comprehensive, and is in principle language and dataset independent. We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major languages in the world."
O13-5002,Integrating Dictionary and Web N-grams for {C}hinese Spell Checking,2013,12,4,3,1,34550,jiancheng wu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 18, Number 4, {D}ecember 2013-Special Issue on Selected Papers from {ROCLING} {XXV}",0,"Chinese spell checking is an important component of many NLP applications, including word processors, search engines, and automatic essay rating. Nevertheless, compared to spell checkers for alphabetical languages (e.g., English or French), Chinese spell checkers are more difficult to develop because there are no word boundaries in the Chinese writing system and errors may be caused by various Chinese input methods. In this paper, we propose a novel method for detecting and correcting Chinese typographical errors. Our approach involves word segmentation, detection rules, and phrase-based machine translation. The error detection module detects errors by segmenting words and checking word and phrase frequency based on compiled and Web corpora. The phonological or morphological typographical errors found then are corrected by running a decoder based on the statistical machine translation model (SMT). The results show that the proposed system achieves significantly better accuracy in error detection and more satisfactory performance in error correction than the state-of-the-art systems."
O13-5003,Correcting Serial Grammatical Errors based on N-grams and Syntax,2013,16,2,3,1,34550,jiancheng wu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 18, Number 4, {D}ecember 2013-Special Issue on Selected Papers from {ROCLING} {XXV}",0,"In this paper, we present a new method based on machine translation for correcting serial grammatical errors in a given sentence in learners' writing. In our approach, translation models are generated to translate the input into a grammatical sentence. The method involves automatically learning two translation models that are based on Web-scale n-grams. The first model translates trigrams containing serial preposition-verb errors into correct ones. The second model is a back-off model, used in the case where the trigram is not found in the training data. At run-time, the phrases in the input are matched and translated, and ranking is performed on all possible translations to produce a corrected sentence as output. Evaluation on a set of sentences in a learner corpus shows that the method corrects serial errors reasonably well. Our methodology exploits the state-of-the art in machine translation, resulting in an effective system that can deal with many error types at the same time."
O13-2002,Learning to Find Translations and Transliterations on the Web based on Conditional Random Fields,2013,25,1,2,1,41539,joseph chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 18, Number 1, March 2013",0,"In recent years, state-of-the-art cross-linguistic systems have been based on parallel corpora. Nevertheless, it is difficult at times to find translations of a certain technical term or named entity even with a very large parallel corpora. In this paper, we present a new method for learning to find translations on the Web for a given term. In our approach, we use a small set of terms and translations to obtain mixed-code snippets returned by a search engine. We then automatically annotate the data with translation tags, automatically generate features to augment the tagged data, and automatically train a conditional random fields model for identifying translations. At runtime, we obtain mixed-code webpages containing the given term and run the model to extract translations as output. Preliminary experiments and evaluation results show our method cleanly combines various features, resulting in a system that outperforms previous works."
O13-1005,æ©å¨ç¿»è­¯çºæ¬çä¸­ææ¼å­æ¹é¯ç³»çµ± ({C}hinese Spelling Checker Based on Statistical Machine Translation),2013,11,16,3,1,38140,hsunwen chiu,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,"Chinese spelling check is an important component for many NLP applications, including word processor and search engines. However, compared to checkers for alphabetical languages (e.g., English or French), Chinese spelling checkers are more difficult to develop, because there are no word boundaries in Chinese writing system, and errors may be caused by various Chinese input methods. In this paper, we proposed a novel method to Chinese spelling checking. Our approach involves error detection and correction based on the phrasal statistical machine translation framework. The results show that the proposed system achieves significantly better accuracy in error detecting and more satisfactory performance in error correcting."
O13-1006,Detecting {E}nglish Grammatical Errors based on Machine Translation,2013,5,0,3,1,32846,jim chang,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,"Many people are learning English as a second or foreign language, and there are estimated 375 million English as a Second Language (ESL) and 750 million English as a Foreign Language (EFL) learners around the world according to Graddol (2006). Evidently, automatic grammar checkers are much needed to help learners improve their writing. However, typical English proofreading tools do not target specifically the most common errors made by second language learners. The grammar checkers available in popular word processors have been developed with a focus on native speaker errors such as subject-verb agreement and pronoun reference. Therefore, these word processors (e.g., Microsoft Word) often offer little or no help with common errors causing problems for English learners. Grammatical Error Detection (GED) for language learners has been an area of active research. GED involves pinpointing some words in a given sentence as grammatically erroneous and possibly offering correction. Common errors in learnersxe2x80x99 writing include missing, unnecessary, and misuse of articles, prepositions, noun number, and verb form. Recently, the state-of-the-art research on GED has been surveyed by Leacock et al. (2010). In our work, we address serial errors in English learnersxe2x80x99 writing related to the proposition and verb form, an aspect that has not been dealt with in most GED research. We also consider the issues of broadening the training data for better coverage, and coping with data sparseness when unseen events happen."
I13-1083,Augmentable Paraphrase Extraction Framework,2013,12,0,4,1,20350,meihua chen,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Paraphrase extraction relying on a single factor such as distribution similarity or translation similarity might lead to the loss of some linguistic properties. In this paper, we propose a paraphrase extraction framework, which accommodates various linguistically motivated factors to optimize the quality of paraphrase extraction. The major contributions of this study lie in the augmentable paraphrasing framework and the three kinds of factors conducive to both semantic and syntactic correctness. A manual evaluation showed that our model achieves more successful results than the state-of-the-art methods."
I13-1103,Translating {C}hinese Unknown Words by Automatically Acquired Templates,2013,14,0,4,1,34549,minghong bai,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper, we present a translation template model to translate Chinese unknown words. The model exploits translation templates, which are extracted automatically from a word-aligned parallel corpus, to translate unknown words. The translation templates are designed in accordance with the structure of unknown words. When an unknown word is detected during translation, the model applies translation templates to the word to get a set of matched templates, and then translates the word into a set of suggested translations. Our experiment results demonstrate that the translations suggested by the unknown word translation template model significantly improve the performance of the Moses machine translation system."
W12-6338,Improving {PCFG} {C}hinese Parsing with Context-Dependent Probability Re-estimation,2012,16,3,3,0.71692,34554,yuming hsieh,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Selecting the best structure from several ambiguous structures produced by a syntactic parser is a challenging issue. The quality of the solution depends on the precision of the structure evaluation methods. In this paper, we propose a general model (context-dependent probability re-estimation model, CDM) to enhance the structure probabilities estimation. Compared with using rule probabilities only, the CDM has the advantage of an effective, flexible, and broader range of contexturefeature selection. We conduct experiments on the CDM parsing model by using Sinica Chinese Treebank. The results show that our proposed model significantly outperforms the baseline parser and the open source Berkeley statistical parser. More importantly, we demonstrate that the basic framework of the parsing model does not need to be changed, and the proposed re-estimation functions will adjust the probability estimation for every particular structure, and obtaining the better parsing results."
W12-2009,{PREFER}: Using a Graph-Based Approach to Generate Paraphrases for Language Learning,2012,14,4,5,1,20350,meihua chen,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"Paraphrasing is an important aspect of language competence; however, EFL learners have long had difficulty paraphrasing in their writing owing to their limited language proficiency. Therefore, automatic paraphrase suggestion systems can be useful for writers. In this paper, we present PREFER, a paraphrase reference tool for helping language learners improve their writing skills. In this paper, we attempt to transform the paraphrase generation problem into a graphical problem in which the phrases are treated as nodes and translation similarities as edges. We adopt the PageRank algorithm to rank and filter the paraphrases generated by the pivot-based paraphrase generation method. We manually evaluate the performance of our method and assess the effectiveness of PREFER in language learning. The results show that our method successfully preserves both the semantic meaning and syntactic structure of the query phrase. Moreover, the students' writing performance improve most with the assistance of PREFER."
W12-2035,Helping Our Own: {NTHU} {NLPLAB} System Description,2012,9,1,6,1,34550,jiancheng wu,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"Grammatical error correction has been an active research area in the field of Natural Language Processing. In this paper, we integrated four distinct learning-based modules to correct determiner and preposition errors in leaners' writing. Each module focuses on a particular type of error. Our modules were tested in well-formed data and learners' writing. The results show that our system achieves high recall while preserves satisfactory precision."
P12-3010,{DOMCAT}: A Bilingual Concordancer for Domain-Specific Computer Assisted Translation,2012,20,0,4,1,34549,minghong bai,Proceedings of the {ACL} 2012 System Demonstrations,0,"In this paper, we propose a web-based bilingual concordancer, DOMCAT, for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the Dice coefficient for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively."
P12-3027,{FLOW}: A First-Language-Oriented Writing Assistant System,2012,12,6,5,1,20350,meihua chen,Proceedings of the {ACL} 2012 System Demonstrations,0,"Writing in English might be one of the most difficult tasks for EFL (English as a Foreign Language) learners. This paper presents FLOW, a writing assistance system. It is built based on first-language-oriented input function and context sensitive approach, aiming at providing immediate and appropriate suggestions including translations, paraphrases, and n-grams during composing and revising processes. FLOW is expected to help EFL writers achieve their writing flow without being interrupted by their insufficient lexical knowledge."
P12-2026,Learning to Find Translations and Transliterations on the Web,2012,15,3,2,1,41539,joseph chang,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we present a new method for learning to finding translations and transliterations on the Web for a given term. The approach involves using a small set of terms and translations to obtain mixed-code snippets from a search engine, and automatically annotating the snippets with tags and features for training a conditional random field model. At run-time, the model is used to extracting translation candidates for a given term. Preliminary experiments and evaluation show our method cleanly combining various features, resulting in a system that outperforms previous work."
O12-1006,Associating Collocations with {W}ord{N}et Senses Using Hybrid Models,2012,12,1,3,1,41680,yichun chen,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,"In this paper, we introduce a hybrid method to associate English collocations with sense class members chosen from WordNet. Our combinational approach includes a learning-based method, a paraphrase-based method and a sense frequency ranking method. At training time, a set of collocations with their tagged senses is prepared. We use the sentence information extracted from a large corpus and cross-lingual information to train a learning-based model. At run time, the corresponding senses of an input collocation will be decided via majority voting. The three outcomes participated in voting are as follows: 1. the result from a learning-based model; 2. the result from a paraphrase-based model; 3. the result from sense frequency ranking method. The sense with most votes will be associated with the input collocation. Evaluation shows that the hybrid model achieves significant improvement when comparing with the other method described in evaluation time. Our method provides more reliable result on associating collocations with senses that can help lexicographers in Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012)"
O12-1024,Translating Collocation using Monolingual and Parallel Corpus,2012,16,0,5,0,42776,mingzhuan jiang,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,"In this paper, we propose a method for translating a given verb-noun collocation based on a parallel corpus and an additional monolingual corpus. Our approach involves two models to generate collocation translations. The combination translation model generates combined translations of the collocate and the base word, and filters translations by a target language model from a monolingual corpus, and the bidirectional alignment translation model generates translations using bidirectional alignment information. At run time, each model generates a list of possible translation candidates, and translations in two candidate lists are re-ranked and returned as our system output. We describe the implementation of using method using Hong Kong Parallel Text. The experiment results show that our method improves the quality of top-ranked collocation translations, which could be used to assist ESL learners and bilingual dictionaries editors. Keyword: collocation, statistical machine translation, computer-assisted translation"
O12-1027,Context-Aware In-Page Search,2012,21,1,4,0,8102,yuhao lin,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,"In this paper we introduce a method for searching appropriate articles from knowledge bases (e.g. Wikipedia) for a given query and its context. In our approach, this problem is transformed into a multi-class classification of candidate articles. The method involves automatically augmenting smaller knowledge bases using larger ones and learning to choose adequate articles based on hyperlink similarity between article and context. At run-time, keyphrases in given context are extracted and the sense ambiguity of query term is resolved by computing similarity of keyphrases between context and candidate articles. Evaluation shows that the method significantly outperforms the strong baseline of assigning most frequent articles to the query terms. Our method effectively determines adequate articles for given query-context pairs, suggesting the possibility of using our methods in context-aware search engines."
N12-1036,{T}rans{A}head: A Computer-Assisted Translation and Writing Tool,2012,13,1,4,1,33416,chungchi huang,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce a method for learning to predict text completion given a source text and partial translation. In our approach, predictions are offered aimed at alleviating users' burden on lexical and grammar choices, and improving productivity. The method involves learning syntax-based phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate and rank completion candidates, which are then displayed to users. We present a prototype writing assistant, TransAhead, that applies the method to computer-assisted translation and language learning. The preliminary results show that the method has great potentials in CAT and CALL with significant improvement in translation quality across users."
E12-2004,{T}rans{A}head: A Writing Assistant for {CAT} and {CALL},2012,14,2,6,1,33416,chungchi huang,Proceedings of the Demonstrations at the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We introduce a method for learning to predict the following grammar and text of the ongoing translation given a source text. In our approach, predictions are offered aimed at reducing users' burden on lexical and grammar choices, and improving productivity. The method involves learning syntactic phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate subsequent grammar and translation predictions. We present a prototype writing assistant, TransAhead, that applies the method to where computer-assisted translation and language learning meet. The preliminary results show that the method has great potentials in CAT and CALL (significant boost in translation quality is observed)."
C12-3007,Word Root Finder: a Morphological Segmentor Based on {CRF},2012,11,2,2,1,41539,joseph chang,Proceedings of {COLING} 2012: Demonstration Papers,0,"Morphological segmentation of words is a subproblem of many natural language tasks, including handling out-of-vocabulary (OOV) words in machine translation, more effective information retrieval, and computer assisted vocabulary learning. Previous work typically relies on extensive statistical and semantic analyses to induce legitimate stems and affixes. We introduce a new learning based method and a prototype implementation of a knowledge light system for learning to segment a given word into word parts, including prefixes, suffixes, stems, and even roots. The method is based on the Conditional Random Fields (CRF) model. Evaluation results show that our method with a small set of seed training data and readily available resources can produce fine-grained morphological segmentation results that rival previous work and systems."
W11-1412,{GRASP}: Grammar- and Syntax-based Pattern-Finder in {CALL},2011,25,5,5,1,33416,chungchi huang,Proceedings of the Sixth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We introduce a method for learning to describe the attendant contexts of a given query for language learning. In our approach, we display phraseological information in the form of a summary of general patterns as well as lexical bundles anchored at the query. The method involves syntactical analyses and inverted file construction. At run-time, grammatical constructions and their lexical instantiations characterizing the usage of the given query are generated and displayed, aimed at improving learners' deep vocabulary knowledge. We present a prototype system, GRASP, that applies the proposed method for enhanced collocation learning. Preliminary experiments show that language learners benefit more from GRASP than conventional dictionary lookup. In addition, the information produced by GRASP is potentially useful information for automatic or manual editing process."
P11-4005,{E}d{I}t: A Broad-Coverage Grammar Checker Using Pattern Grammar,2011,14,4,4,1,33416,chungchi huang,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"We introduce a new method for learning to detect grammatical errors in learner's writing and provide suggestions. The method involves parsing a reference corpus and inferring grammar patterns in the form of a sequence of content words, function words, and parts-of-speech (e.g., play ~ role in Ving and look forward to Ving). At runtime, the given passage submitted by the learner is matched using an extended Levenshtein algorithm against the set of pattern rules in order to detect errors and provide suggestions. We present a prototype implementation of the proposed method, EdIt, that can handle a broad range of errors. Promising results are illustrated with three common types of errors in non-native writing."
Y10-1040,{GRASP}: Grammar- and Syntax-based Pattern-Finder for Collocation and Phrase Learning,2010,15,3,4,1,20350,meihua chen,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"We introduce a method for learning to find the representative syntax-based context of a given collocation/phrase. In our approach, grammatical patterns are extracted for query terms aimed at accelerating lexicographersxe2x80x99 and language learnersxe2x80x99 navigation through the word usage and learning process. The method involves automatically lemmatizing, part-of-speech tagging and shallowly parsing the sentences of a large-sized general corpus, and automatically constructing inverted files for quick search. At run-time, contextual grammar patterns are retrieved and presented to users with their corresponding statistical analyses. We present a prototype system, GRASP (grammarand syntax-based pattern-finder), that applies the method to computer-assisted language learning. Preliminary results show that the extracted patterns not only resemble phrases in grammar books (e.g., make up onexe2x80x99s mind) but help to assist the process of language learning and sentence composition/translation."
P10-2021,Automatic Collocation Suggestion in Academic Writing,2010,6,18,4,1,34550,jiancheng wu,Proceedings of the {ACL} 2010 Conference Short Papers,0,"In recent years, collocation has been widely acknowledged as an essential characteristic to distinguish native speakers from non-native speakers. Research on academic writing has also shown that collocations are not only common but serve a particularly important discourse function within the academic community. In our study, we propose a machine learning approach to implementing an online collocation writing assistant. We use a data-driven classifier to provide collocation suggestions to improve word choices, based on the result of classification. The system generates and ranks suggestions to assist learners' collocation usages in their academic writing with satisfactory results."
2010.amta-papers.13,Using Sublexical Translations to Handle the {OOV} Problem in {MT},2010,20,4,4,1,33416,chungchi huang,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We introduce a method for learning to translate out-of-vocabulary (OOV) words. The method focuses on combining sublexical/constituent translations of an OOV to generate its translation candidates. In our approach, wild-card searches are formulated based on our OOV analysis, aimed at maximizing the probability of retrieving OOVs{'} sublexical translations from existing resource of machine translation (MT) systems. At run-time, translation candidates of the unknown words are generated from their suitable sublexical translations and ranked based on monolingual and bilingual information. We have incorporated the OOV model into a state-of-the-art MT system and experimental results show that our model indeed helps to ease the negative impact of OOVs on translation quality, especially for sentences containing more OOVs (significant improvement)."
Y09-1009,{W}iki{S}ense: Supersense Tagging of {W}ikipedia Named Entities Based {W}ord{N}et,2009,16,7,3,1,41539,joseph chang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"In this paper, we introduce a minimally supervised method for learning to classify named-entity titles in a given encyclopedia into broad semantic categories in an existing ontology. Our main idea involves using overlapping entries in the encyclopedia and ontology and a small set of 30 handed tagged parenthetic explanations to automatically generate the training data. The proposed method involves automatically recognizing whether a title is a named entity, automatically generating two sets of training data, and automatically building a classification model for training a classification model based on textual and non-textual features. We present WikiSense, an implementation of the proposed method for extending the named entity coverage of WordNet by sense tagging Wikipedia titles. Experimental results show WikiSense achieves accuracy of over 95% and near 80% applicability for all NE titles in Wikipedia. WikiSense cleanly produces over 1.2 million of NEs tagged with broad categories, based on the lexicographersxe2x80x99 files of WordNet, effectively extending WordNet to form a very large scale semantic category, a potentially useful resource for many natural language related tasks."
Y09-1022,Review Classification Using Semantic Features and Run-Time Weighting,2009,11,0,4,1,33416,chungchi huang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"We introduce a method for learning to assign suitable sentiment ratings to review articles. In our approach, reviews are transformed into collections of n-gram and semantic word class features aimed at maximizing the probability of classifying them into accurate ratings. The method involves automatically segmenting review articles into sentences and automatically estimating associations between features and sentiment ratings via machine learning techniques. At run-time, a simple weighting strategy is performed to give extra weights to features in potential evaluative sentences (e.g., the first, the last sentences and sentences with adverbs) from others. Experiments show that word class information alleviates data sparseness problem facing higher-level n-grams (e.g., bigrams and trigrams) and that our model using both training-time n-gram and semantic features and run-time weighting mechanism outperforms a strong baseline with surface n-gram features by 2.5% relatively."
Y09-1040,Extending Bilingual {W}ord{N}et via Hierarchical Word Translation Classification,2009,12,0,5,0,46745,tzuyi nien,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"We introduce a method for learning to assign word senses to translation pairs. In our approach, this sense assignment or disambiguation problem is transformed into one on how to navigate through a sense network like WordNet aimed at distinguishing the more adequate senses from others. The method involves automatically constructing classification models for branching nodes in the network, and automatically learning to reject less probable senses, based on the translation characteristics of word senses and semanticallyxefxbfxbd related word groups (e.g., lexicographer files) respectively. At runtime, translation pairs are expanded with their synonyms and sense ambiguity is resolved using a greedy algorithm choosing the most likely branches based on the trained classification models. Evaluation shows that our method significantly outperforms the strong baseline of assigning most frequent sense to the translation pairs and effectively determines suitable word senses for given translation pairs, suggesting the possibility of employing our method as a computerxefxbfxbd assisted tool for speeding up the process of lexicography or of using our method to assist machine translation systems in word selection."
O09-5002,A Thesaurus-Based Semantic Classification of {E}nglish Collocations,2009,35,3,4,1,33416,chungchi huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 14, Number 3, September 2009",0,"Researchers have developed many computational tools aimed at extracting collocations for both second language learners and lexicographers. Unfortunately, the tremendously large number of collocates returned by these tools usually overwhelms language learners. In this paper, we introduce a thesaurus-based semantic classification model that automatically learns semantic relations for classifying adjective-noun (A-N) and verb-noun (V-N) collocations into different thesaurus categories. Our model is based on iterative random walking over a weighted graph derived from an integrated knowledge source of word senses in WordNet and semantic categories of a thesaurus for collocation classification. We conduct an experiment on a set of collocations whose collocates involve varying levels of abstractness in the collocation usage box of Macmillan English Dictionary. Experimental evaluation with a collection of 150 multiple-choice questions commonly used as a similarity benchmark in the TOEFL synonym test shows that a thesaurus structure is successfully imposed to help enhance collocation production for L2 learners. As a result, our methodology may improve the effectiveness of state-of-the-art collocation reference tools concerning the aspects of language understanding and learning, as well as lexicography."
O09-3001,Fertility-based Source-Language-biased Inversion Transduction Grammar for Word Alignment,2009,99,1,2,1,33416,chungchi huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 14, Number 1, March 2009",0,"We propose a version of Inversion Transduction Grammar (ITG) model with IBM-style notation of fertility to improve word-alignment performance. In our approach, binary context-free grammar rules of the source language, accompanied by orientation preferences of the target language and fertilities of words, are leveraged to construct a syntax-based statistical translation model. Our model, inherently possessing the characteristics of ITG restrictions and allowing for many consecutive words aligned to one and vice-versa, outperforms the Bracketing Transduction Grammar (BTG) model and GIZA, a state-of-the-art word aligner, not only in alignment error rate (23% and 14% error reduction) but also in consistent phrase error rate (13% and 9% error reduction). Better performance in these two evaluation metrics suggests that, based on our word alignment result, more accurate phrase pairs may be acquired, leading to better machine translation quality."
N09-1029,Learning Bilingual Linguistic Reordering Model for Statistical Machine Translation,2009,24,3,3,0,43741,hanbin chen,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this paper, we propose a method for learning reordering model for BTG-based statistical machine translation (SMT). The model focuses on linguistic features from bilingual phrases. Our method involves extracting reordering examples as well as features such as part-of-speech and word class from aligned parallel sentences. The features are classified with special considerations of phrase lengths. We then use these features to train the maximum entropy (ME) reordering model. With the model, we performed Chinese-to-English translation tasks. Experimental results show that our bilingual linguistic model outperforms the state-of-the-art phrase-based and BTG-based SMT systems by improvements of 2.41 and 1.31 BLEU points respectively."
D09-1050,Acquiring Translation Equivalences of Multiword Expressions by Normalized Correlation Frequencies,2009,18,14,4,1,34549,minghong bai,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we present an algorithm for extracting translations of any given multiword expression from parallel corpora. Given a multiword expression to be translated, the method involves extracting a short list of target candidate words from parallel corpora based on scores of normalized frequency, generating possible translations and filtering out common subsequences, and selecting the top-n possible translations using the Dice coefficient. Experiments show that our approach outperforms the word alignment-based and other naive association-based methods. We also demonstrate that adopting the extracted translations can significantly improve the performance of the Moses machine translation system."
O08-1003,A Thesaurus-Based Semantic Classification of {E}nglish Collocations,2008,0,1,4,1,33416,chungchi huang,Proceedings of the 20th Conference on Computational Linguistics and Speech Processing,0,None
I08-1033,Improving Word Alignment by Adjusting {C}hinese Word Segmentation,2008,22,18,3,1,34549,minghong bai,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Most of the current Chinese word alignment tasks often adopt word segmentation systems firstly to identify words. However, word-mismatching problems exist between languages and will degrade the performance of word alignment. In this paper, we propose two unsupervised methods to adjust word segmentation to make the tokens 1-to-1 mapping as many as possible between the corresponding sentences. The first method is learning affix rules from a bilingual terminology bank. The second method is using the concept of impurity measure motivated by the decision tree. Our experiments showed that both of the adjusting methods improve the performance of word alignment significantly."
2008.amta-papers.20,Mining the Web for Domain-Specific Translations,2008,0,0,4,1,34550,jiancheng wu,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We introduce a method for learning to find domain-specific translations for a given term on the Web. In our approach, the source term is transformed into an expanded query aimed at maximizing the probability of retrieving translations from a very large collection of mixed-code documents. The method involves automatically generating sets of target-language words from training data in specific domains, automatically selecting target words for effectiveness in retrieving documents containing the sought-after translations. At run time, the given term is transformed into an expanded query and submitted to a search engine, and ranked translations are extracted from the document snippets returned by the search engine. We present a prototype, TermMine, which applies the method to a Web search engine. Evaluations over a set of domains and terms show that TermMine outperforms state-of-the-art machine translation systems."
O07-1011,Word Translation Disambiguation via Dependency (å©ç¨ä¾å­éä¿ä¹è¾­å½ç¿»è­¯),2007,11,1,3,0,49267,mengchin hsiao,Proceedings of the 19th Conference on Computational Linguistics and Speech Processing,0,None
N07-4011,Learning to Find Transliteration on the Web,2007,6,2,2,1,49286,chiencheng wu,Proceedings of Human Language Technologies: The Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics ({NAACL}-{HLT}),0,"This prototype demonstrate a novel method for learning to find transliterations of proper nouns on the Web based on query expansion aimed at maximizing the probability of retrieving transliterations from existing search engines. Since the method we used involves learning the morphological relationships between names and their transliterations, we refer to this IR-based approach as morphological query expansion for machine transliteration. The morphological query expansion approach is general in scope and can be applied to translation and transliteration, but we focus on transliteration in this paper."
D07-1106,Learning to Find {E}nglish to {C}hinese Transliterations on the Web,2007,16,0,2,1,34550,jiancheng wu,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,None
P06-4001,{FAST} {--} An Automatic Generation System for Grammar Tests,2006,13,41,3,0,49886,chiayin chen,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"This paper introduces a method for the semi-automatic generation of grammar test items by applying Natural Language Processing (NLP) techniques. Based on manually-designed patterns, sentences gathered from the Web are transformed into tests on grammaticality. The method involves representing test writing knowledge as test patterns, acquiring authentic sentences on the Web, and applying generation strategies to transform sentences into items. At runtime, sentences are converted into two types of TOEFL-style question: multiple-choice and error detection. We also describe a prototype system FAST (Free Assessment of Structural Tests). Evaluation on a set of generated questions indicates that the proposed method performs satisfactory quality. Our methodology provides a promising approach and offers significant potential for computer assisted language learning and assessment."
P06-4011,Computational Analysis of Move Structures in Academic Abstracts,2006,5,13,4,0,49894,jienchen wu,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"This paper introduces a method for computational analysis of move structures in abstracts of research articles. In our approach, sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions. The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves. We also present a prototype concordancer, CARE, which exploits the move-tagged abstracts for digital learning. This system provides a promising approach to Web-based computer-assisted academic writing."
O06-4003,Sense Extraction and Disambiguation for {C}hinese Words from Bilingual Terminology Bank,2006,11,2,3,1,34549,minghong bai,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 3, September 2006: Special Issue on Selected Papers from {ROCLING} {XVII}",0,"Using lexical semantic knowledge to solve natural language processing problems has been getting popular in recent years. Because semantic processing relies heavily on lexical semantic knowledge, the construction of lexical semantic databases has become urgent. WordNet is the most famous English semantic knowledge database at present; many researches of word sense disambiguation adopt it as a standard. Because of the success of WordNet, there is a trend to construct WordNet in different languages. In this paper, we propose a methodology for constructing Chinese WordNet by extracting information from a bilingual terminology bank. We developed an algorithm of word-to-word alignment to extract the English-Chinese translation-equivalent word pairs first. Then, the algorithm disambiguates word senses and maps Chinese word senses to WordNet synsets to achieve the goal. In the word-to-word alignment experiment, this alignment algorithm achieves the f-score of 98.4%. In the word sense disambiguation experiment, the extracted senses cover 36.89% of WordNet synsets and the accuracy of the three proposed disambiguation rules achieve the accuracies of 80%, 83% and 87%, respectively."
O06-1021,Learning to Parse Bilingual Sentences Using Bilingual Corpus and Monolingual {CFG},2006,17,0,2,1,33416,chungchi huang,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,"Abstract We present a new method for learning to parse a bilingual sentence using Inversion Transduction Grammar trained on a parallel corpus and a monolingual treebank. The method produces a parse tree for a bilingual sentence, showing the shared syntactic structures of individual sentence and the differences of word order within a syntactic structure. The method involves estimating lexical translation probability based on a word-aligning strategy and inferring probabilities for CFG rules. At runtime, a bottom-up CYK-styled parser is employed to construct the most probable bilingual parse tree for any given sentence pair. We also describe an implementation of the proposed method. The experimental results indicate the proposed model produces word alignments better than those produced by Giza, a state-of-the-art word alignment system, in terms of alignment error rate and F-measure. The bilingual parse trees produced for the parallel corpus can be exploited to extract bilingual phrases and train a decoder for statistical machine translation."
P05-3010,Learning Source-Target Surface Patterns for Web-based Terminology Translation,2005,7,13,3,1,34550,jiancheng wu,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"This paper introduces a method for learning to find translation of a given source term on the Web. In the approach, the source term is used as query and part of patterns to retrieve and extract translations in Web pages. The method involves using a bilingual term list to learn source-target surface patterns. At runtime, the given term is submitted to a search engine then the candidate translations are extracted from the returned summaries and subsequently ranked based on the surface patterns, occurrence counts, and transliteration knowledge. We present a prototype called TermMine that applies the method to translate terms. Evaluation on a set of encyclopedia terms shows that the method significantly outperforms the state-of-the-art online machine translation systems."
O05-4002,Collocational Translation Memory Extraction Based on Statistical and Linguistic Information,2005,0,3,4,1,50915,thomas chuang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 3, September 2005: Special Issue on Selected Papers from {ROCLING} {XVI}",0,None
O05-1010,"{FAST}ï¼é»è\
¦è¼å©è±æææ³åºé¡ç³»çµ± ({FAST}: Free Assistant of Structural Tests) [In {C}hinese]",2005,0,0,4,0,49886,chiayin chen,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,None
O05-1020,å©ç¨éèªå­¸è¡åè©åº«æ½åä¸­è±å­è©äºè­¯åè©ç¾©è§£æ­§ (Sense Extraction and Disambiguation for {C}hinese Words from Bilingual Terminology Bank) [In {C}hinese],2005,0,0,3,1,34549,minghong bai,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,None
I05-1046,Web-Based Unsupervised Learning for Query Formulation in Question Answering,2005,8,4,4,1,19855,yichia wang,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Converting questions to effective queries is crucial to open-domain question answering systems. In this paper, we present a web-based unsupervised learning approach for transforming a given natural-language question to an effective query. The method involves querying a search engine for Web passages that contain the answer to the question, extracting patterns that characterize fine-grained classification for answers, and linking these patterns with n-grams in answer passages. Independent evaluation on a set of questions shows that the proposed approach outperforms a naive keyword-based approach in terms of mean reciprocal rank and human effort."
P04-3004,Subsentential Translation Memory for Computer Assisted Writing and Translation,2004,4,2,4,1,34550,jiancheng wu,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"This paper describes a database of translation memory, TotalRecall, developed to encourage authentic and idiomatic use in second language writing. TotalRecall is a bilingual concordancer that support search query in English or Chinese for relevant sentences and translations. Although initially intended for learners of English as Foreign Language (EFL) in Taiwan, it is a gold mine of texts in English or Mandarin Chinese. TotalRecall is particularly useful for those who write in or translate into a foreign language. We exploited and structured existing high-quality translations from bilingual corpora from a Taiwan-based Sinorama Magazine and Official Records of Hong Kong Legislative Council to build a bilingual concordance. Novel approaches were taken to provide high-precision bilingual alignment on the subsentential and lexical levels. A browser-based user interface was developed for ease of access over the Internet. Users can search for word, phrase or expression in English or Mandarin. The Web-based user interface facilitates the recording of the user actions to provide data for further research."
P04-3019,{TANGO}: Bilingual Collocational Concordancer,2004,5,19,3,1,50916,jiayan jian,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"In this paper, we describe TANGO as a collocational concordancer for looking up collocations. The system was designed to answer user's query of bilingual collocational usage for nouns, verbs and adjectives. We first obtained collocations from the large monolingual British National Corpus (BNC). Subsequently, we identified collocation instances and translation counterparts in the bilingual corpus such as Sinorama Parallel Corpus (SPC) by exploiting the word-alignment technique. The main goal of the concordancer is to provide the user with a reference tools for correct collocation use so as to assist second language learners to acquire the most eminent characteristic of native-like writing."
O04-2001,Bilingual Collocation Extraction Based on Syntactic and Statistical Analyses,2004,42,4,2,1,49286,chiencheng wu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 9, Number 1, {F}ebruary 2004: Special Issue on Selected Papers from {ROCLING} {XV}",0,"In this paper, we describe an algorithm that employs syntactic and statistical analysis to extract bilingual collocations from a parallel corpus. Collocations are pervasive in all types of writing and can be found in phrases, chunks, proper names, idioms, and terminology. Therefore, automatic extraction of monolingual and bilingual collocations is important for many applications, including natural language generation, word sense disambiguation, machine translation, lexicography, and cross language information retrieval. Collocations can be classified as lexical or grammatical collocations. Lexical collocations exist between content words, while a grammatical collocation exists between a content word and function words or a syntactic structure. In addition, bilingual collocations can be rigid or flexible in both languages. Rigid collocation refers to words in a collocation must appear next to each other, or otherwise (flexible/elastic). We focus in this paper on extracting rigid lexical bilingual collocations. In our method, the preferred syntactic patterns are obtained from idioms and collocations in a machine-readable dictionary. Collocations matching the patterns are extracted from aligned sentences in a parallel corpus. We use a new alignment method based on punctuation statistics for sentence alignment. The punctuation-based approach is found to outperform the length-based approach with precision rates approaching 98%. The obtained collocations are subsequently matched up based on cross-linguistic statistical association. Statistical association between the whole collocations as well as words in collocations is used to link a collocation with its counterpart collocation in the other language. We implemented the proposed method on a very large Chinese-English parallel corpus and obtained satisfactory results."
O04-1020,Using the Web as Corpus for Un-supervised Learning in Question Answering,2004,4,2,4,1,19855,yichia wang,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,None
O04-1027,Collocational Translation Memory Extraction Based on Statistical and Linguistic Information,2004,16,10,3,1,50916,jiayan jian,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,"In this paper, we propose a new method for extracting bilingual collocations from a parallel corpus to provide phrasal translation memories. The method integrates statistical and linguistic information to achieve effective extraction of bilingual collocations. The linguistic information includes parts of speech, chunks, and clauses. The method involves first obtaining an extended list of English collocations from a very large monolingual corpus, then identifying the collocations in a parallel corpus, and finally extracting translation equivalents of the collocations based on word alignment information. Experimental results indicate that phrasal translation memories can be effectively used for computer assisted language learning (CALL) and computer assisted translation (CAT)."
lee-etal-2004-alignment,Alignment of bilingual named entities in parallel corpora using statistical model,2004,12,7,2,1,51803,chunjen lee,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Named entities make up a bulk of documents. Extracting named entities is crucial to various applications of natural language processing. Although efforts to identify named entities within monolingual documents are numerous, extracting bilingual named entities has not been investigated extensively owing to the complexity of the task. In this paper, we describe a statistical phrase translation model and a statistical transliteration model. Under the proposed models, a new method is proposed to align bilingual named entities in parallel corpora. Experimental results indicate that a satisfactory precision rate can be achieved. To enhance the performance, we also describe how to improve the proposed method by incorporating approximate matching and person name recognition. Experimental results show that performance is significantly improved with the enhancement."
lin-etal-2004-extraction,Extraction of name and transliteration in monolingual and parallel corpora,2004,10,6,3,1,50881,tracy lin,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Named-entities in free text represent a challenge to text analysis in Machine Translation and Cross Language Information Retrieval. These phrases are often transliterated into another language with a different sound inventory and writing system. Named-entities found in free text are often not listed in bilingual dictionaries. Although it is possible to identify and translate named-entities on the fly without a list of proper names and transliterations, an extensive list of existing transliterations certainly will ensure high precision rate. We use a seed list of proper names and transliterations to train a Machine Transliteration Model. With the model it is possible to extract proper names and their transliterations in monolingual or parallel corpora with high precision and recall rates."
Y03-1035,A Statistical Approach to {C}hinese-to-{E}nglish Back-Transliteration,2003,12,6,2,1,51803,chunjen lee,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,None
W03-1702,Class Based Sense Definition Model for Word Sense Tagging and Disambiguation,2003,19,0,2,1,50881,tracy lin,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"We present an unsupervised learning strategy for word sense disambiguation (WSD) that exploits multiple linguistic resources including a parallel corpus, a bilingual machine readable dictionary, and a thesaurus. The approach is based on Class Based Sense Definition Model (CBSDM) that generates the glosses and translations for a class of word senses. The model can be applied to resolve sense ambiguity for words in a parallel corpus. That sense tagging procedure, in effect, produces a semantic bilingual concordance, which can be used to train WSD systems for the two languages involved. Experimental results show that CBSDM trained on Longman Dictionary of Contemporary English, English-Chinese Edition (LDOCE E-C) and Longman Lexicon of Contemporary English (LLOCE) is very effectively in turning a Chinese-English parallel corpus into sense tagged data for development of WSD systems."
W03-0317,Acquisition of {E}nglish-{C}hinese Transliterated Word Pairs from Parallel-Aligned Texts using a Statistical Machine Transliteration Model,2003,18,59,2,1,51803,chunjen lee,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"This paper presents a framework for extracting English and Chinese transliterated word pairs from parallel texts. The approach is based on the statistical machine transliteration model to exploit the phonetic similarities between English words and corresponding Chinese transliterations. For a given proper noun in English, the proposed method extracts the corresponding transliterated word from the aligned text in Chinese. Under the proposed approach, the parameters of the model are automatically learned from a bilingual proper name list. Experimental results show that the average rates of word and character precision are 86.0% and 94.4%, respectively. The rates can be further improved with the addition of simple linguistic processing."
P03-2040,{T}otal{R}ecall: A Bilingual Concordance for Computer Assisted Translation and Language Learning,2003,7,18,5,1,34550,jiancheng wu,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"This paper describes a Web-based English-Chinese concordance system, Total-Recall, developed to promote translation reuse and encourage authentic and idiomatic use in second language writing. We exploited and structured existing high-quality translations from the bilingual Sinorama Magazine to build the concordance of authentic text and translation. Novel approaches were taken to provide high-precision bilingual alignment on the sentence, phrase and word levels. A browser-based user interface (UI) is also developed for ease of access over the Internet. Users can search for word, phrase or expression in English or Chinese. The Web-based user interface facilitates the recording of the user actions to provide data for further research."
O03-5003,Building A {C}hinese {W}ord{N}et Via Class-Based Translation Model,2003,7,5,1,1,2389,jason chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 8, Number 2, August 2003",0,"Semantic lexicons are indispensable to research in lexical semantics and word sense disambiguation (WSD). For the study of WSD for English text, researchers have been using different kinds of lexicographic resources, including machine readable dictionaries (MRDs), machine readable thesauri, and bilingual corpora. In recent years, WordNet has become the most widely used resource for the study of WSD and lexical semantics in general. This paper describes the Class-Based Translation Model and its application in assigning translations to nominal senses in WordNet in order to build a prototype Chinese WordNet. Experiments and evaluations show that the proposed approach can potentially be adopted to speed up the construction of WordNet for Chinese and other languages."
O03-3002,Interleaving Text and Punctuations for Bilingual Sub-sentential Alignment,2003,12,0,3,0,52856,wenchi hsie,{ROCLING} 2003 Poster Papers,0,"We present a new approach to aligning bilingual English and Chinese text at sub-sentential level by interleaving alphabetic texts and punctuations matches. With sub-sentential alignment, we expect to improve the effectiveness of alignment at word, chunk and phrase levels and provide finer grained and more reusable translation memory."
O03-3004,Using Punctuations and Lengths for Bilingual Sub-sentential Alignment,2003,8,0,3,0,52858,wenchi hsien,{ROCLING} 2003 Poster Papers,0,"We present a new approach to aligning bilingual English and Chinese text at sub-sentential level by interleaving alphabetic texts and punctuations matches. With sub-sentential alignment, we expect to improve the effectiveness of alignment at word, chunk and phrase levels and provide finer grained and more reusable translation memory."
O03-3005,{T}otal{R}ecall: A Bilingual Concordance in National Digital Learning Project - {CANDLE},2003,-1,-1,3,1,34550,jiancheng wu,{ROCLING} 2003 Poster Papers,0,None
O03-3006,Unsupervised Word Segmentation Without Dictionary,2003,4,3,1,1,2389,jason chang,{ROCLING} 2003 Poster Papers,0,None
O03-1001,Word-Transliteration Alignment,2003,70,9,3,1,50881,tracy lin,Proceedings of Research on Computational Linguistics Conference {XV},0,"The named-entity phrases in free text represent a formidable challenge to text analysis. Translating a named-entity is important for the task of Cross Language Information Retrieval and Question Answering. However, both tasks are not easy to handle because named-entities found in free text are often not listed in a monolingual or bilingual dictionary. Although it is possible to identify and translate named-entities on the fly without a list of proper names and transliterations, an extensive list certainly will ensure the high accuracy rate of text analysis. We use a list of proper names and transliterations to train a Machine Transliteration Model. With the model it is possible to extract proper names and their transliterations in a bilingual corpus with high average precision and recall rates."
O03-1003,Bilingual Collocation Extraction Based on Syntactic and Statistical Analyses,2003,-1,-1,2,1,49286,chiencheng wu,Proceedings of Research on Computational Linguistics Conference {XV},0,None
Y02-1026,An Operator Assisted Call Routing System,2001,-1,-1,2,1,51803,chunjen lee,"Proceedings of the 16th Pacific Asia Conference on Language, Information and Computation",0,None
chuang-etal-2002-adaptive,Adaptive bilingual sentence alignment,2002,13,13,3,1,50915,thomas chuang,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"We present a new approach to the problem of aligning English and Chinese sentences in a bilingual corpus based on adaptive learning. While using length information alone produces surprisingly good results for aligning bilingual French and English sentences with success rates well over 95{\%}, it does not fair as well for the alignment of English and Chinese sentences. The crux of the problem lies in greater variability of lengths and match types of the matched sentences. We propose to cope with such variability via a two-pass scheme under which model parameters can be learned from the data at hand. Experiments show that under the approach bilingual English-Chinese texts can be aligned effectively across diverse domains, genres and translation directions with accuracy rates approaching 99{\%}."
O01-3004,çµ±è¨å¼çèªç¿»è­¯æ¨¡å (Statistical Translation Model for Phrases) [In {C}hinese],2001,19,5,1,1,2389,jason chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 6, Number 2, August 2001",0,None
O01-1003,å¤ç¯æä»¶èªåæè¦ç³»çµ± (Multi-Document Summarization System) [In {C}hinese],2001,0,0,2,0,53887,jiancheng shen,Proceedings of Research on Computational Linguistics Conference {XIV},0,None
O01-1013,çµ±è¨å¼çèªç¿»è­¯æ¨¡å(A Statistical Model of Terminology Translation) [In {C}hinese],2001,0,0,1,1,2389,jason chang,Proceedings of Research on Computational Linguistics Conference {XIV},0,None
P98-1037,A Concept-based Adaptive Approach to Word Sense Disambiguation,1998,11,10,2,0,54361,jen chen,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Word sense disambiguation for unrestricted text is one of the most difficult tasks in the fields of computational linguistics. The crux of the problem is to discover a model that relates the intended sense of a word with its context. This paper describes a general framework for adaptive conceptual word sense disambiguation. Central to this WSD framework is the sense division and semantic relations based on topical analysis of dictionary sense definitions. The process begins with an initial disambiguation step using an MRD-derived knowledge base. An adaptation step follows to combine the initial knowledge base with knowledge gleaned from the partial disambiguated text. Once the knowledge base is adjusted to suit the text at hand, it is then applied to the text again to finalize the disambiguation result. Definitions and example sentences from LDOCE are employed as training materials for WSD, while passages from the Brown corpus and Wall Street Journal are used for testing. We report on several experiments illustrating effectiveness of the adaptive approach."
J98-1003,Topical Clustering of {MRD} Senses Based on Information Retrieval Techniques,1998,39,40,2,0,54361,jen chen,Computational Linguistics,0,"This paper describes a heuristic approach capable of automatically clustering senses in a machine-readable dictionary (MRD). Including these clusters in the MRD-based lexical database offers several positive benefits for word sense disambiguation (WSD). First, the clusters can be used as a coarser sense division, so unnecessarily fine sense distinction can be avoided. The clustered entries in the MRD can also be used as materials for supervised training to develop a WSD system. Furthermore, if the algorithm is run on several MRDs, the clusters also provide a means of linking different senses across multiple MRDs to create an integrated lexical database. An implementation of the method for clustering definition sentences in the Longman Dictionary of Contemporary English (LDOCE) is described. To this end, the topical word lists and topical cross-references in the Longman Lexicon of Contemporary English (LLOCE) are used. Nearly half of the senses in the LDOCE can be linked precisely to a relevant LLOCE topic using a simple heuristic. With the definitions of senses linked to the same topic viewed as a document, topical clustering of the MRD senses bears a striking resemblance to retrieval of relevant documents for a given query in information retrieval (IR) research. Relatively well-established IR techniques of weighting terms and ranking document relevancy are applied to find the topical clusters that are most relevant to the definition of each MRD sense. Finally, we describe an implemented version of the algorithms for the LDOCE and the LLOCE and assess the performance of the proposed approach in a series of experiments and evaluations."
C98-1037,A Concept-based Adaptive Approach to Word Sense Disambiguation,1998,11,10,2,0,54361,jen chen,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Word sense disambiguation for unrestricted text is one of the most difficult tasks in the fields of computational linguistics. The crux of the problem is to discover a model that relates the intended sense of a word with its context. This paper describes a general framework for adaptive conceptual word sense disambiguation. Central to this WSD framework is the sense division and semantic relations based on topical analysis of dictionary sense definitions. The process begins with an initial disambiguation step using an MRD-derived knowledge base. An adaptation step follows to combine the initial knowledge base with knowledge gleaned from the partial disambiguated text. Once the knowledge base is adjusted to suit the text at hand, it is then applied to the text again to finalize the disambiguation result. Definitions and example sentences from LDOCE are employed as training materials for WSD, while passages from the Brown corpus and Wall Street Journal are used for testing. We report on several experiments illustrating effectiveness of the adaptive approach."
chang-etal-1998-taxonomy,Taxonomy and lexical semantics{---}from the perspective of machine readable dictionary,1998,17,7,1,1,2389,jason chang,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Machine-readable dictionaries have been regarded as a rich knowledge source from which various relations in lexical semantics can be effectively extracted. These semantic relations have been found useful for supporting a wide range of natural language processing tasks, from information retrieval to interpretation of noun sequences, and to resolution of prepositional phrase attachment. In this paper, we address issues related to problems in building a semantic hierarchy from machine-readable dictionaries: genus disambiguation, discovery of covert categories, and bilingual taxonomy. In addressing these issues, we will discuss the limiting factors in dictionary definitions and ways of eradicating these problems. We will also compare the taxonomy extracted in this way from a typical MRD and that of the WordNet. We argue that although the MRD-derived taxonomy is considerably flatter than the WordNet, it nevertheless provides a functional core for a variety of semantic relations and inferences which is vital in natural language processing."
P97-1038,An Alignment Method for Noisy Parallel Corpora based on Image Processing Techniques,1997,17,10,1,1,2389,jason chang,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"This paper presents a new approach to bitext correspondence problem (BCP) of noisy bilingual corpora based on image processing (IP) techniques. By using one of several ways of estimating the lexical translation probability (LTP) between pairs of source and target words, we can turn a bitext into a discrete gray-level image. We contend that the BCP, when seen in the light, bears a striking resemblance to the line detection problem in IP. Therefore, BCPs, including sentence and word alignment, can benefit from a wealth of effective, well established IP techniques, including convolution-based filters, texture analysis and Hough transform. This paper describes a new program, PlotAlign that produces a word-level bitext map for noisy or non-literal bitext, based on these techniques."
O97-4004,Aligning More Words with High Precision for Small Bilingual Corpora,1997,0,3,2,0,53075,sue ker,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 2, Number 2, August 1997",0,None
J97-2004,A Class-based Approach to Word Alignment,1997,33,66,2,0,53075,sue ker,Computational Linguistics,0,"This paper presents an algorithm capable of identifying the translation for each word in a bilingual corpus. Previously proposed methods rely heavily on word-based statistics. Under a word-based approach, frequent words with a consistent translation can be aligned at a high rate of precision. However, words that are less frequent or exhibit diverse translations generally do not have statistically significant evidence for confident alignment, thereby leading to incomplete or incorrect alignments. The algorithm proposed herein attempts to broaden coverage by exploiting lexicographic resources. To this end, we draw on the two classification systems of words in Longman Lexicon of Contemporary English (LLOCE) and Tongyici Cilin (Synonym Forest, CILIN). Automatically acquired class-based alignment rules are used to compensate for what is lacking in a bilingual dictionary such as the English-Chinese version of the Longman Dictionary of Contemporary English (LecDOCE). In addition, this alignment method is implemented using LecDOCE examples and their translations for training and testing, while further examples from a technical manual in both English and Chinese are used for an open test. Quantitative results of the closed and open tests are also summarized."
1997.tmi-1.15,{T}op{A}lign: word alignment for bilingual corpora based on topical clusters of dictionary entries and translations,1997,-1,-1,2,1,55467,mathis chen,Proceedings of the 7th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
W96-0305,Acquisition of Computational-Semantic Lexicons from Machine Readable Lexical Resources,1996,15,1,1,1,2389,jason chang,Breadth and Depth of Semantic Lexicons,0,"This paper describes a heuristic algorithm capable of automatically assigning a label to each of the senses in a machine readable dictionary (MRD) for the purpose of acquiring a computational-semantic lexicon for treatment of lexical ambiguity. Including these labels in the MRD-based lexical database offers several positive effects. The labels can be used as a coarser sense division so unnecessarily fine sense distinction can be avoided in word sense disambiguation (WSD).The algorithm is based primarily on simple word matching between an MRD definition sentence and word lists of an LLOCE topic. We also describe an implementation of the algorithm for labeling definition sentences in Longman Dictionary of Contemporary English (LDOCE). For this purpose the topics and sets of related words in Longman Lexicon of Contemporary English (LLOCE) are used in this work. Quantitative results for a 12-word test set are reported. Our discussion entails how the availability of these labels provides the means for treating such problems as: acquisition of a lexicon capable of providing broad coverage, systematic word sense shifts, lexical underspecification, and acquisition of zero-derivatives."
O96-1011,ä»è©ç¿»è­¯æ³åçèªåæ·å (Learning to Translate {E}nglish Prepositions) [In {C}hinese],1996,-1,-1,1,1,2389,jason chang,Proceedings of Rocling {IX} Computational Linguistics Conference {IX},0,None
C96-1037,Aligning More Words with High Precision for Small Bilingual Corpora,1996,27,2,2,1,56025,surjin ker,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"In this paper, we propose an algorithm for aligning words with their translation in a bilingual corpus. Conventional algorithms are based on word-by-word models which require bilingual data with hundreds of thousand sentences for training. By using a word-based approach, less frequent words or words with diverse translations generally do not have statistically significant evidence for confident alignment. Consequently, incomplete or incorrect alignments occur. Our algorithm attempts to handle the problem using class-based rules which are automatic acquired from bilingual materials such as a bilingual corpus or machine readable dictionary. The procedures for acquiring these rules is also described. We found that the algorithm can align over 80% of word pairs while maintaining a comparably high precision rate, even when a small corpus was used in training. The algorithm also poses the advantage of producing a tagged corpus for word sense disambiguation."
1996.amta-1.12,Combining machine readable lexical resources and bilingual corpora for broad word sense disambiguation,1996,-1,-1,1,1,2389,jason chang,Conference of the Association for Machine Translation in the Americas,0,None
Y95-1015,Structural Ambiguity and Conceptual Information Retrieval,1995,0,1,2,1,55467,mathis chen,"Proceedings of the 10th Pacific Asia Conference on Language, Information and Computation",0,None
Y95-1017,The Postprocessing of Optical Character Recognition Based on Statistical Noisy Channel and Language Model,1995,11,3,1,1,2389,jason chang,"Proceedings of the 10th Pacific Asia Conference on Language, Information and Computation",0,"The techniques of image processing have been used in optical character recognition (OCR) for a long time. The recognition method evolved from early pattern recognition to feature extraction recently. The recognition rate is raised from 70% to 90%. But the character by character recognition technique has its limitation. Using language models to assist the OCR system in improving recognition rate is the topic of many recent researches. Recently, the related research on Chinese nature language processing has improved rapidly. These improvement include the Chinese word segmentation, syntax analysis, semantic analysis, collocation analysis, statistical language models. In this paper, we will propose a new techniques for Chinese OCR postprocessing and postediting. We combine noisy channel model and the technique of natural language processing to implement an OCR postprocessing system. From the result of experiments, we found noisy channel model very effective for postprocessing. Under the approach, it is possible to recover the correct character, even when it is not in the candidate list produced by the OCR system."
Y95-1023,Automatic Acquisition of Class-based Rules for Word Alignment,1995,12,1,2,1,56025,surjin ker,"Proceedings of the 10th Pacific Asia Conference on Language, Information and Computation",0,"In this paper, we describe an algorithm for aligning words with their translation in a bilingual corpus. Existing algorithms require enormous bilingual data to train statistical word-to-word translation models. Using word-based approach, frequent words with consistent translation can be aligned at a high precision rate. However, less frequent words or words with diverse translations usually do not have statistically significant evidence for confident alignment. Incomplete or incorrect alignments consequently result. Our algorithm attempts to handle the problem using a hierarchical class-based approximation of translation probabilities. The translation probabilities are estimated using class-based models on 3 levels of specificity. We found that the algorithm can provide translation probability for more word pairs at the cost of slightly lower degree of precision, even when a small corpus was used in training. We have achieved an application rate of 81.8% and precision rate of 93.3%. The algorithm also offer the advantage of producing word-sense disambiguation information."
O93-1007,ä¸­æè¾­å½å²ç¾©ä¹ç ç©¶âæ·è©èè©æ§æ¨ç¤º (The Resolution of Lexicon Ambiguity in {C}hinese - Segmentation and Tagging) [In {C}hinese],1993,0,0,2,0,56735,tsaiyen peng,Proceedings of Rocling {VI} Computational Linguistics Conference {VI},0,None
O91-1004,éå¶å¼æ»¿è¶³åæ©çæä½³åçä¸­ææ·è©æ¹æ³ ({C}hinese Word Segmentation based on Constraint satisfaction and Statistical Optimization) [In {C}hinese],1991,0,0,1,1,2389,jason chang,Proceedings of Rocling {IV} Computational Linguistics Conference {IV},0,None
O90-1011,Computer Generation Of {C}hinese Commentary On Othello Games,1990,0,0,2,0,57504,jenwen liao,Proceedings of Rocling {III} Computational Linguistics Conference {III},0,None
O90-1012,Bi-Lingual Sentence Generation,1990,0,0,2,0,57505,chungcherng chen,Proceedings of Rocling {III} Computational Linguistics Conference {III},0,None
O89-1004,Systemic Generation of {C}hinese Sentences,1989,0,0,2,0,57736,hweiming kuo,Proceedings of Rocling {II} Computational Linguistics Conference {II},0,None
O88-1005,A New Approach to Quality Text Generation,1988,0,0,1,1,2389,jason chang,Proceedings of Rocling {I} Computational Linguistics Conference {I},0,None
