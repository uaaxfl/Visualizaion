2020.lrec-1.82,D15-1075,0,0.0748306,"Missing"
2020.lrec-1.82,bunt-etal-2010-towards,0,0.0353651,"rk experiments. Section 5. concludes our paper. 2. Related work There have been a variety of proposals of dialog systems for health care (Laranjo et al., 2018), as well as those designed for elderly people speaking Japanese (Takahashi et al., 2002; Kobayashi et al., 2010). To the best of our knowledge, however, our work is the ﬁrst to construct large-scale datasets of question-answer pairs and entailment relations dedicated to caregiving. A number of dialog act annotation schemes that have tags for “yes” and “no” responses have been proposed (Core and Allen, 1997; Walker and Passonneau, 2001; Bunt et al., 2010). There are also human-machine dialog corpora annotated for dialog acts (Georgila et al., 2010; El Asri et al., 2017). Existing textual entailment datasets include RTE-3 (Giampiccolo et al., 2007), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015) for English, and the RITE-2 binary classiﬁcation dataset (Watanabe et al., 2013) for Japanese. We did not rely on these existing resources in this study, because in preliminary studies we found that models trained with the data dedicated for our system performed better than domain-general datasets. 3. Data Construction This section explains"
2020.lrec-1.82,N19-1423,0,0.147862,"P (S1 |S2 ) ∝ ∑ P (t|S1 )P (t|S2 ) t P (t) This probability was used as the similarity measure between S1 and S2 . As a set of topic words, we used the 10,000 most frequent nouns in a database that we use for WISDOM X (Mizuno et al., 2016), which is a domain-general questionanswering system being developed by our team. The input to our topic word model is a sentence and the outputs are a probability distribution over 10,001 items, where the ﬁrst 10,000 items correspond to the 10,000 nouns, and the last item is a special output that represents “no noun”. Our topic word model is based on BERT5 (Devlin et al., 2019), where a training instance is a sentence with a list of the nouns that appear either in it, in the previous sentence or in the next sentence. We trained it with 20 million training instances extracted from the same corpus as we used to pretrain the BERT model. For both the word-overlap and topic-word subsets, three annotators independently judged whether each pair has an entailment relation, and the ﬁnal decision was made by majority vote. For the word-overlap subset, the annotators annotated all 33,988 statement pairs. For the topic-word subset, we randomly sampled 47,374 instances from the"
2020.lrec-1.82,W17-5526,0,0.0380284,"Missing"
2020.lrec-1.82,W07-1401,0,0.101008,"r elderly people speaking Japanese (Takahashi et al., 2002; Kobayashi et al., 2010). To the best of our knowledge, however, our work is the ﬁrst to construct large-scale datasets of question-answer pairs and entailment relations dedicated to caregiving. A number of dialog act annotation schemes that have tags for “yes” and “no” responses have been proposed (Core and Allen, 1997; Walker and Passonneau, 2001; Bunt et al., 2010). There are also human-machine dialog corpora annotated for dialog acts (Georgila et al., 2010; El Asri et al., 2017). Existing textual entailment datasets include RTE-3 (Giampiccolo et al., 2007), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015) for English, and the RITE-2 binary classiﬁcation dataset (Watanabe et al., 2013) for Japanese. We did not rely on these existing resources in this study, because in preliminary studies we found that models trained with the data dedicated for our system performed better than domain-general datasets. 3. Data Construction This section explains how we constructed our datasets in detail. 3.1. Data Creation for the Yes/No Response Classiﬁer For the yes/no response classiﬁer, our annotators created 280,467 question-response pairs based on 1"
2020.lrec-1.82,D19-1590,1,0.884705,"Missing"
2020.lrec-1.82,W04-3230,0,0.0609721,"Missing"
2020.lrec-1.82,S14-2001,0,0.0218291,"e (Takahashi et al., 2002; Kobayashi et al., 2010). To the best of our knowledge, however, our work is the ﬁrst to construct large-scale datasets of question-answer pairs and entailment relations dedicated to caregiving. A number of dialog act annotation schemes that have tags for “yes” and “no” responses have been proposed (Core and Allen, 1997; Walker and Passonneau, 2001; Bunt et al., 2010). There are also human-machine dialog corpora annotated for dialog acts (Georgila et al., 2010; El Asri et al., 2017). Existing textual entailment datasets include RTE-3 (Giampiccolo et al., 2007), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015) for English, and the RITE-2 binary classiﬁcation dataset (Watanabe et al., 2013) for Japanese. We did not rely on these existing resources in this study, because in preliminary studies we found that models trained with the data dedicated for our system performed better than domain-general datasets. 3. Data Construction This section explains how we constructed our datasets in detail. 3.1. Data Creation for the Yes/No Response Classiﬁer For the yes/no response classiﬁer, our annotators created 280,467 question-response pairs based on 1,901 carefully designed seed"
2020.lrec-1.82,C16-2055,1,0.851824,"is measured by the probability that S2 occurs in the same context as S1 . We used a topic word model that predicts (t|S), the probability that a topic word t occurs in the context of sentence S, to estimate the similarity between sentences S1 and S2 : P (S1 |S2 ) ≈ ∑ P (S1 |t)P (t|S2 ) t = ∑ P (t|S1 )P (S1 )P (t|S2 ) P (t) t Assuming a uniform distribution for P (S), we obtain: P (S1 |S2 ) ∝ ∑ P (t|S1 )P (t|S2 ) t P (t) This probability was used as the similarity measure between S1 and S2 . As a set of topic words, we used the 10,000 most frequent nouns in a database that we use for WISDOM X (Mizuno et al., 2016), which is a domain-general questionanswering system being developed by our team. The input to our topic word model is a sentence and the outputs are a probability distribution over 10,001 items, where the ﬁrst 10,000 items correspond to the 10,000 nouns, and the last item is a special output that represents “no noun”. Our topic word model is based on BERT5 (Devlin et al., 2019), where a training instance is a sentence with a list of the nouns that appear either in it, in the previous sentence or in the next sentence. We trained it with 20 million training instances extracted from the same cor"
2020.lrec-1.82,P13-1170,1,0.847324,"Missing"
2020.lrec-1.82,H01-1015,0,0.120505,"ur datasets with neural network experiments. Section 5. concludes our paper. 2. Related work There have been a variety of proposals of dialog systems for health care (Laranjo et al., 2018), as well as those designed for elderly people speaking Japanese (Takahashi et al., 2002; Kobayashi et al., 2010). To the best of our knowledge, however, our work is the ﬁrst to construct large-scale datasets of question-answer pairs and entailment relations dedicated to caregiving. A number of dialog act annotation schemes that have tags for “yes” and “no” responses have been proposed (Core and Allen, 1997; Walker and Passonneau, 2001; Bunt et al., 2010). There are also human-machine dialog corpora annotated for dialog acts (Georgila et al., 2010; El Asri et al., 2017). Existing textual entailment datasets include RTE-3 (Giampiccolo et al., 2007), SICK (Marelli et al., 2014) and SNLI (Bowman et al., 2015) for English, and the RITE-2 binary classiﬁcation dataset (Watanabe et al., 2013) for Japanese. We did not rely on these existing resources in this study, because in preliminary studies we found that models trained with the data dedicated for our system performed better than domain-general datasets. 3. Data Construction Th"
2021.acl-long.164,Q17-1010,0,0.0790021,"andom weights while j ≤ t do k←1 while k ≤ b do Sample mini-batch of n examples {(ei ,mi )}n i=1 Generate word embeddings {(ei ,mi )}n i=1 of the examples. Update D and R by ascending their stochastic gradient: n ( ) 1∑ ∇θD ,θR [log D(R(ei )) + log 1 − D(F (mi )) ] n i=1 Update F by descending its stochastic gradient: n ) 1∑ ( log 1 − D(F (mi )) ∇θF n i=1 k ←k+1 end j ←j+1 end For each pair of an entity mention (ei ) and an entity-masked sentence (mi ) in the training data, we first generate two matrices of word embeddings ei and mi using word embeddings pretrained on Wikipedia with fastText (Bojanowski et al., 2017). Then, R and F generate, respectively, a 2106 real entity representation from ei and a fake entity representation from mi . Finally, they are given to D, which is a feed-forward network that judges whether F or R generated the representations, i.e., whether the representations are real or fake, using sigmoid outputs by the final logistic regression layer. The pseudo code of the training scheme is given in Algorithm 1. The training proceeds as follows: R and D as a team try to avoid the possibility that D misjudges F’s output (i.e., a fake entity representation) as a real entity representation"
2021.acl-long.164,P17-1171,0,0.0704455,"91.9 92.6 92.0 91.7 69.1 71.7 74.4 75.3 74.8 71.5 72.3 92.0 92.5 92.6 92.7 92.6 92.6 92.8 91.8 91.8 94.5 94.5 94.5 94.5 94.5 89.4 90.4 90.6 90.7 90.8 90.3 Table 3: GLUE test set results. Our model for test set results incorporates task-specific modification for CoLA and WNLI to improve scores (see Appendix A for details). All results are from the GLUE leaderboard. that only the proposed method with our GANstyle CNNs showed a higher average score than ALBERT. This suggests the effectiveness of our GAN-style pretraining scheme of CNNs. 5.2 Open-domain QA We also tested BERTAC on open-domain QA (Chen et al., 2017) with the publicly available datasets Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). We used the pre-processed version4 of the datasets provided by Lin et al. (2018), which contains passages retrieved for all questions, and followed their data split as described in Table 5. 5.2.1 BERTAC for open-domain QA We implemented our QA model following the approach of Lin et al. (2018), which combines a passage selector to choose relevant passages from retrieved passages and an answer span selector to identify the answer span in the selected passages. For the given question q and the"
2021.acl-long.164,D14-1179,0,0.0433062,"Missing"
2021.acl-long.164,N19-1423,0,0.0775044,"Missing"
2021.acl-long.164,2020.findings-emnlp.207,0,0.0675184,"Missing"
2021.acl-long.164,P18-1161,0,0.0223494,". Our model for test set results incorporates task-specific modification for CoLA and WNLI to improve scores (see Appendix A for details). All results are from the GLUE leaderboard. that only the proposed method with our GANstyle CNNs showed a higher average score than ALBERT. This suggests the effectiveness of our GAN-style pretraining scheme of CNNs. 5.2 Open-domain QA We also tested BERTAC on open-domain QA (Chen et al., 2017) with the publicly available datasets Quasar-T (Dhingra et al., 2017) and SearchQA (Dunn et al., 2017). We used the pre-processed version4 of the datasets provided by Lin et al. (2018), which contains passages retrieved for all questions, and followed their data split as described in Table 5. 5.2.1 BERTAC for open-domain QA We implemented our QA model following the approach of Lin et al. (2018), which combines a passage selector to choose relevant passages from retrieved passages and an answer span selector to identify the answer span in the selected passages. For the given question q and the set of retrieved passages P = {pi }, we computed the probability P r(a|q, P ) of extracting answer span a to question q from P in the following way, and then we extracted the answer sp"
2021.acl-long.164,2021.ccl-1.108,0,0.0782656,"Missing"
2021.acl-long.164,P19-1414,1,0.928508,"asks, possibly as subcomponents of their methods, and/or they have focused on scaling up TLMs or improving their pretraining schemes. As a result, other architectures like Recurrent Neural Networks (RNN) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and Convolutional Neural Networks (CNN) (LeCun et al., 1999) are fading away. In this work, we propose a method for improving TLMs by integrating a simple conventional CNN to them. We pretrained this CNN on Wikipedia using a Generative Adversarial Network (GAN) style training scheme (Goodfellow et al., 2014), and then combined it with TLMs. Oh et al. (2019) similarly used GAN-style training to improve a QA model using a CNN, but their training scheme was applicable only to QAspecific datasets. On the other hand, similarly to TLM, our proposed method for training the CNN is independent of specific tasks. We show that the combination of this CNN with TLMs can achieve higher performance than that of the original TLMs on publicly available datasets for several distinct tasks. We hope that this gives an insight into how to develop novel strong network architectures and training schemes. We call our combination of a TLM and a CNN BERTAC (BERT-style TL"
2021.acl-long.164,D19-1005,0,0.0486941,"Missing"
2021.acl-long.164,2020.semeval-1.271,0,0.0329287,"tary alternative to these existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer representation generator for QA that can guess the vector representation of answers from given whytype questions and answer passages. The generator was trained in a GAN-style manner using QA datasets. We took inspiration from their adversarial training scheme to train task-independent representation generators from unsupervised texts (i.e., Wikipedia sentences in"
2021.acl-long.164,D19-1626,0,0.0212062,"hese existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer representation generator for QA that can guess the vector representation of answers from given whytype questions and answer passages. The generator was trained in a GAN-style manner using QA datasets. We took inspiration from their adversarial training scheme to train task-independent representation generators from unsupervised texts (i.e., Wikipedia sentences in which an entity wa"
2021.acl-long.164,2020.findings-emnlp.278,0,0.366983,"lable at https://github.com/nict-wisdom/bertac. 2 Related Work Pretraining TLMs with entity information: There have been attempts to explicitly learn entity representation from text corpora using TLMs (He et al., 2020; Peters et al., 2019; Sun et al., 2020; Wang et al., 2020a; Xiong et al., 2020; Zhang et al., 2019). Our proposed method is a complementary alternative to these existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer"
2021.acl-long.164,W18-5446,0,0.0763319,"Missing"
2021.acl-long.164,D19-1599,0,0.0172576,"CALBERT-xxlarge Table 5: Number of questions in each dataset. #p is the number of retrieved passages for each question. Non-TLM-based methods O PEN QA (Lin et al., 2018): An RNN-based method that jointly learns passage-selection and answer extraction. O PEN QA+ARG (Oh et al., 2019): An extension of O PEN QA that additionally uses an answer representation generator (ARG) trained by adversarial learning. TLM-based methods WKLM (Xiong et al., 2020): This uses a TLM pretrained with a weakly supervised objective for learning Wikipedia entity information. BERT-base was used for the training. MBERT (Wang et al., 2019): A BERT-based method that extracts answers using globally normalized answer scores across all the passages retrieved by the same question. BERT-large was used for the training. CF ORMER (Wang et al., 2020b): It uses a clusteringbased sparse transformer for long-range dependency encoding. The method was trained using RoBERTa-large. Quasar-T EM F1 42.2 49.3 43.2 49.7 45.8 52.2 51.1 59.1 54.0 63.9 55.8 63.7 58.0 65.8 SearchQA EM F1 58.8 64.5 59.6 65.3 61.7 66.7 65.1 70.7 68.0 75.1 71.9 77.1 74.0 79.2 Table 7: QA test set results. Figures of the previous works were taken from their original paper"
2021.acl-long.164,P19-1226,0,0.0244545,"code and models of BERTAC are available at https://github.com/nict-wisdom/bertac. 2 Related Work Pretraining TLMs with entity information: There have been attempts to explicitly learn entity representation from text corpora using TLMs (He et al., 2020; Peters et al., 2019; Sun et al., 2020; Wang et al., 2020a; Xiong et al., 2020; Zhang et al., 2019). Our proposed method is a complementary alternative to these existing methods in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) pro"
2021.acl-long.164,2020.acl-main.82,0,0.0183674,"ds in the sense that entity representations are integrated into TLMs via CNNs and not directly produced by the TLMs. Fine-tuning TLMs with external resources or other NNs: Yang et al. (2019a) and Liu et al. (2020) have used knowledge graphs for augmenting TLMs with entity representations during finetuning. Unlike these approaches, BERTAC uses unstructured texts rather than clean structured knowledge, such as knowledge graphs, to adversarially train a CNN. Other previous works have proposed combining CNNs or RNNs with BERT for NLP tasks (Lu et al., 2020; Safaya et al., 2020; Shao et al., 2019; Zhang et al., 2020), but their use of CNNs/RNNs was task-specific, so their models were not directly applicable to other tasks. Adversarial learning for improving TLMs: Oh et al. (2019) proposed a CNN-based answer representation generator for QA that can guess the vector representation of answers from given whytype questions and answer passages. The generator was trained in a GAN-style manner using QA datasets. We took inspiration from their adversarial training scheme to train task-independent representation generators from unsupervised texts (i.e., Wikipedia sentences in which an entity was masked in a cloze-t"
2021.acl-long.164,P19-1139,0,0.0533291,"Missing"
C00-1060,J96-1002,0,0.00667717,"Missing"
C00-1060,J93-1002,0,0.0347235,"setsus which follow bunsetsu i as in (7). They report that this kind of contextual information improves accuracy. However, the model has to assume the independency of all the random variables, which may cause some errors. Y def P (i → j) = P (bynd |Φi , Ψk , ∆i,k ) i&lt;k&lt;j ×P (dpnd |Φi , Ψj , ∆i,j ) × Y P (btwn |Φi , Ψk , ∆i,k )(7) k&gt;j The difference between our model and these previous models are discussed in Section 3. 2.2 Statistical Approaches with a grammar There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997). The main issue in this type of research is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for linguistic structures. 2.3 SL"
C00-1060,W98-1114,0,0.0220178,"improved to overcome these problems and compared with other works directly. 4.4 Discussion and Future Work The following are some observations about the speed of our parser. Existing statistical parsers are quite efficient compared to grammar-based systems. Particularly, our system used an HPSG-based grammar, whose speed is said to be slow. However, recent advances in HPSG parsing (Torisawa et al., 2000) enabled us to obtain a unique parse tree with our system in 0.5 sec. in average for sentences in the EDR corpus. Future work shall extend SLUNG so that semantic representations are produced. Carroll et al. (1998) discussed the precision of argument structures. We believe that the focus of our study will shift from a shallow level to such a deeper level for our final aim, realization of intelligent natural language processing systems. 5 Conclusion We presented a hybrid parsing scheme that uses a hand-crafted grammar and a statistical technique. As other hybrid parsing methods, the statistical technique is used for picking up the most preferable parse tree from the parse forest generated by the grammar. The difference from other works is that the precise contextual information needed to estimate the lik"
C00-1060,W98-1511,0,0.0623567,"sed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes Φi and Ψj consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And ∆i,j is the number of intervening bunsetsus between i and j. def (6) P (i → j) = P (T |Φi , Ψj , ∆i,j ) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j) which follow bunsetsu i. That is, instead of learning two output values “T(tru"
C00-1060,P98-1083,0,0.107236,"se Several statistical models for Japanese dependency analysis which do not utilize a hand-crafted grammar have been proposed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes Φi and Ψj consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And ∆i,j is the number of intervening bunsetsus between i and j. def (6) P (i → j) = P (T |Φi , Ψj , ∆i,j ) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but"
C00-1060,P98-2144,1,0.828236,"s to five, but in his case, without a grammar. 3.2 The Triplet/Quadruplet Model The Triplet/Quadruplet Model calculates the likelihood of the dependency between bunsetsu i and bunsetsu cn ; P (i → cn ) with the formulas (8) and (9), where cn denotes the nth candidate among bunsetsu i’s candidates; Φi denotes some attributes of i; and Ψcn denotes attributes of cn (including attributes between i and cn ). def P (i → cn ) = P (n |Φi , Ψc1 , Ψc2 ) (n = 1, 2) (8) def P (i → cn ) = P (n |Φi , Ψc1 , Ψc2 , Ψcl ) (n = 1, 2, l)(9) 2 This heuristics is a Japanese version of a left-association rule: see (Mitsuishi et al., 1998) for detail. As (8) and (9) suggest, the model considers attributes of the modifier bunsetsu and attributes of all modification candidates simultaneously in the conditional parts of the probabilities. Moreover, what is calculated is not the probability of “whether the dependency is correct (T, see Formula(6))”, but the probability of “which of the given candidates is chosen as the modifiee (n =1, 2, or l)”. These characteristics imply the following two advantages. Advantage 1 A new distance metric. The correct modifiee can be chosen by considering relative position among grammatically licensed"
C00-1060,W97-0301,0,0.0181276,"rafted grammars can contribute to parsing accuracy on a shallow level. 1 Introduction There have been many attempts to combine handcrafted high-level grammars, such as FB-LTAG, HPSG and LFG, and statistical disambiguation techniques to obtain precise linguistic structures (Schabes, 1992; Abney, 1996; Carroll et al., 1998). One evident advantage of this approach over purely statistical parsing techniques is that grammars can provide precise semantic representations. However, considering that remarkable parsing accuracy in a shallow level has been achieved by purely statistical techniques (e.g. Ratnaparkhi (1997)), it may be thought more reasonable to use high-level grammars just for postprocessing which maps results of shallow syntactical analyses onto deep analyses. This work was conducted while the first author was a graduate student at Univ. of Tokyo. NH n M H h Figure 1: A tree M with a non-head daughter NH and a head daughter H. In this work we propose that hand-crafted highlevel grammars can be useful in shallow-level analyses and statistical models. In our framework, grammars are used to obtain precise features for probability estimation, which are difficult to obtain without a grammar, and we"
C00-1060,C92-2066,0,0.0342602,"ies for all bunsetsus which follow bunsetsu i as in (7). They report that this kind of contextual information improves accuracy. However, the model has to assume the independency of all the random variables, which may cause some errors. Y def P (i → j) = P (bynd |Φi , Ψk , ∆i,k ) i&lt;k&lt;j ×P (dpnd |Φi , Ψj , ∆i,j ) × Y P (btwn |Φi , Ψk , ∆i,k )(7) k&gt;j The difference between our model and these previous models are discussed in Section 3. 2.2 Statistical Approaches with a grammar There have been many proposals for statistical frameworks particularly designed for parsers with hand-crafted grammars (Schabes, 1992; Briscoe and Carroll, 1993; Abney, 1996; Inui et al., 1997). The main issue in this type of research is how to assign likelihoods to a single linguistic structure generated by a grammar. Some of them (Briscoe and Carroll, 1993; Inui et al., 1997) treat information on contexts, but the contextual information is derived only from a structure to which the parser is trying to assign a likelihood value. Then, the major difference between their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to determine the likelihood for li"
C00-1060,C00-2110,0,0.0191864,"Missing"
C00-1060,E99-1026,0,0.412941,"ndency analysis which do not utilize a hand-crafted grammar have been proposed. We evaluate the accuracy of bunsetsu-dependencies as they do, thus here we introduce them for comparison. All models introduced below are based on the likelihood value of the dependency between two bunsetsus. But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated. There are some models which calculate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes Φi and Ψj consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And ∆i,j is the number of intervening bunsetsus between i and j. def (6) P (i → j) = P (T |Φi , Ψj , ∆i,j ) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j)"
C00-1060,2000.iwpt-1.43,0,0.0481985,"ate the likelihood values of a dependency between bunsetsu i and j as in (6), such as a decision tree model (Haruno et al., 1998), a maximum entropy model (Uchimoto et al., 1999), a model based on distance and lexical information (Fujio and Matsumoto, 1998). Attributes Φi and Ψj consist of a part-of-speech (POS), a lexical item, presence of a comma, and so on. And ∆i,j is the number of intervening bunsetsus between i and j. def (6) P (i → j) = P (T |Φi , Ψj , ∆i,j ) However, these models fail to reflect contextual information because attributes of the surrounding bunsetsus are not considered. Uchimoto et al. (2000) proposed a model using posterior context. The model utilizes not only attributes about bunsetsus i, j but also attributes about all bunsetsus (including j) which follow bunsetsu i. That is, instead of learning two output values “T(true)” or “F(false)” for the dependency between two bunsetsus, three output values are used for learning: the bunsetsu i is “bynd (dependent on a bunsetsu beyond j)”, “dpnd (dependent on the bunsetsu j)” or “btwn (dependent on a bunsetsu between i and j)”. The probability is calculated by multiplying probabilities for all bunsetsus which follow bunsetsu i as in (7)."
C00-1060,J97-4005,0,\N,Missing
C00-1060,C98-1080,0,\N,Missing
C00-1060,C98-2139,1,\N,Missing
C02-1120,C00-1043,0,\N,Missing
C02-1120,C00-1060,1,\N,Missing
C02-1120,P99-1014,0,\N,Missing
C02-1120,J92-4003,0,\N,Missing
C04-1135,P99-1016,0,0.0244343,". Our method is obtained by extending Shinzato’s method (Shinzato and Torisawa, 2004) where a common hypernym for expressions in itemizations in HTML documents is obtained by using statistical measures. By using Japanese HTML documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods. 1 Introduction Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). Most of these techniques have relied on particular linguistic patterns, such as “NP such as NP.” The frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora. The effort of searching for other clues indicating hyponymy relations is thus significant. Our aim is to extract hyponyms of prespecified hypernyms from the WWW. We use itemizations (or lists) in HTML documents, such as the one in"
C04-1135,P03-1001,0,0.0147804,"nding Shinzato’s method (Shinzato and Torisawa, 2004) where a common hypernym for expressions in itemizations in HTML documents is obtained by using statistical measures. By using Japanese HTML documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods. 1 Introduction Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). Most of these techniques have relied on particular linguistic patterns, such as “NP such as NP.” The frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora. The effort of searching for other clues indicating hyponymy relations is thus significant. Our aim is to extract hyponyms of prespecified hypernyms from the WWW. We use itemizations (or lists) in HTML documents, such as the one in Figure 1(A), and their headings (‘Car C"
C04-1135,C92-2082,0,0.0688242,"his assumption. Our method is obtained by extending Shinzato’s method (Shinzato and Torisawa, 2004) where a common hypernym for expressions in itemizations in HTML documents is obtained by using statistical measures. By using Japanese HTML documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods. 1 Introduction Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). Most of these techniques have relied on particular linguistic patterns, such as “NP such as NP.” The frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora. The effort of searching for other clues indicating hyponymy relations is thus significant. Our aim is to extract hyponyms of prespecified hypernyms from the WWW. We use itemizations (or lists) in HTML documents, s"
C04-1135,C00-1060,1,0.847524,"Missing"
C04-1135,N04-1010,1,0.0990669,"Headings in Web Documents Keiji Shinzato Kentaro Torisawa Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Tatsunokuchi-machi, Nomi-gun, Ishikawa, 923-1292 JAPAN {skeiji,torisawa}@jaist.ac.jp Abstract This paper describes a method to acquire hyponyms for given hypernyms from HTML documents on the WWW. We assume that a heading (or explanation) of an itemization (or listing) in an HTML document is likely to contain a hypernym of the items in the itemization, and we try to acquire hyponymy relations based on this assumption. Our method is obtained by extending Shinzato’s method (Shinzato and Torisawa, 2004) where a common hypernym for expressions in itemizations in HTML documents is obtained by using statistical measures. By using Japanese HTML documents, we empirically show that our proposed method can obtain a significant number of hyponymy relations which would otherwise be missed by alternative methods. 1 Introduction Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; A"
C08-1024,P99-1008,0,0.0241378,"d, whose experimental results are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregaw"
C08-1024,W04-3205,0,0.029568,"acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but"
C08-1024,D07-1114,0,0.0149061,"nowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation. While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for theme parks. The acquisition method proposed in this work addresses both these matters. Finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word “trouble”. Although we used the classical lexico-syntactic patterns for hyponymy acquisition (Hearst, 1992; Imasumi, 2001; Ando et al., 2003) to reflect this intuition"
C08-1024,P06-1015,0,0.0820586,"out related work. Section 3 examines the notion of trouble expressions and their evidences. Section 4 describes our method, whose experimental results are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the r"
C08-1024,N04-1010,1,0.817504,"parks. The acquisition method proposed in this work addresses both these matters. Finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word “trouble”. Although we used the classical lexico-syntactic patterns for hyponymy acquisition (Hearst, 1992; Imasumi, 2001; Ando et al., 2003) to reflect this intuition, our experiments show we were unable to attain satisfactory performance using lexico-syntactic patterns alone. Thus, we also use verb-noun dependencies as evidence in learning (Pantel and Ravichandran, 2004; Shinzato and Torisawa, 2004). We treat the evidences uniformly as elements in a feature vector given to a supervised learning method, which allowed us to extract a considerably larger number of trouble expressions than could be acquired by sparse lexicosyntactic patterns alone, while still keeping decent precision. What kind of hyponymy relations can be acquired by noun-verb dependencies is still an open question in NLP. In this work we show that at least trouble expressions can successfully be acquired based on noun-verb dependency information alone. 3 Trouble Expressions and Features for Their Acquisition In section 1"
C08-1024,E06-1026,0,0.0264045,", “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation. While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for theme parks. The acqui"
C08-1024,N06-1008,1,0.852253,"irs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the ac"
C08-1024,P02-1053,0,0.00441826,"et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation. While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for"
C08-1024,W02-1010,0,0.025201,"dences. Section 4 describes our method, whose experimental results are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002;"
C08-1024,doddington-etal-2004-automatic,0,0.0128655,"s underlying the unsupervised method for training sample selection in the first step, and the final filtering mechanism in the third step. The rest of this paper is organized as follows. Section 2 points out related work. Section 3 examines the notion of trouble expressions and their evidences. Section 4 describes our method, whose experimental results are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “p"
C08-1024,J06-1005,0,0.0259437,"s are discussed in Section 5. 2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction. Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text. Typical target relations include “Reaction” and “Production” (Pantel and Pennacchiootti, 2006), “person-affiliation” and “organization-location” (Zelenko et al., 2002), “part-whole” (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly tro"
C08-1024,C92-2082,0,0.310374,"nt-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for theme parks. The acquisition method proposed in this work addresses both these matters. Finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word “trouble”. Although we used the classical lexico-syntactic patterns for hyponymy acquisition (Hearst, 1992; Imasumi, 2001; Ando et al., 2003) to reflect this intuition, our experiments show we were unable to attain satisfactory performance using lexico-syntactic patterns alone. Thus, we also use verb-noun dependencies as evidence in learning (Pantel and Ravichandran, 2004; Shinzato and Torisawa, 2004). We treat the evidences uniformly as elements in a feature vector given to a supervised learning method, which allowed us to extract a considerably larger number of trouble expressions than could be acquired by sparse lexicosyntactic patterns alone, while still keeping decent precision. What kind of"
C08-1024,P06-2059,0,0.0129167,"and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006). Our current task of acquiring “objecttrouble” relations is new and object-trouble relations are inherently more abstract and indirect than relations like “person-affiliation” — they crucially depend on additional knowledge about whether and how a given object’s use might be hampered by a specific trouble. Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006). Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation. While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital camera’s colors — they do not extend to sentiment-related factors external to the object like “traffic jams” for theme parks. The acquisition method proposed in thi"
C08-1024,N04-1041,0,\N,Missing
C10-1095,ando-etal-2004-automatic,0,0.0415362,"on Instances Let S and U represent a source (i.e. corpus) of structured and unstructured text, respectively. In this paper, we use the hierarchical layout of Wikipedia articles and the Wikipedia category system as structured text S (see Section 3.1), and a corpus of ordinary Web pages as unstructured text U . Let XS and XU denote a set of hyponymy relation candidates extracted from S and U , respectively. XS is extracted from the hierarchical layout of Wikipedia articles (Oh et al., 2009) and XU is extracted by lexico-syntactic patterns for hyponymy relations (i.e., hyponym such as hyponymy) (Ando et al., 2004) (see Section 3 for a detailed explanation) We define two types of common instances, called “genuine” common instances (G) and “virtual” common instances (V ). The set of common instances is denoted by Y = G ∪ V . Genuine common instances are hyponymy relation candidates found in both S and U (G = XS ∩ XU ). On 843 the other hand, term pairs are obtained as virtual common instances when: • 1) they are extracted as hyponymy relation candidates in either S or U and; • 2) they do not seem to be a hyponymy relation in the other text and U ) are more likely to hold a hyponymy relation than virtual"
C10-1095,C92-2082,0,0.314899,"Missing"
C10-1095,D07-1073,1,0.666443,"ed text. As in any other supervised system, the cost of preparing the training data is an important issue. We therefore investigated whether Co-STAR can be trained for a lower cost by generating more of its training data automatically. We automatically built training data for Web texts by using definition sentences9 and category names in the Wikipedia articles, while we stuck to manually prepared training data for Wikipedia. To obtain hypernyms from Wikipedia article names, we used definition-specific lexico-syntactic patterns such as “hyponym is hypernym” and “hyponym is a type of hypernym” (Kazama and Torisawa, 2007; Sumida and Torisawa, 2008). Then, we extracted hyponymy relations consisting of pairs of Wikipedia category names and their member articles when the Wikipedia category name and the hypernym obtained from the definition of the Wikipedia article shared the same head word. Next, we selected a subset of the extracted hyponymy relations that are also hyponymy relation candidates in Web texts, as positive instances for hyponymy relation acquisition from Web text. We obtained around 15,000 positive instances in this way. Negative instances were chosen from virtual-common instances, which also origi"
C10-1095,P08-1047,1,0.713712,"of hyponymy relation candidates, these candidates are more likely to be actual hyponymy relations. The pattern feature of hyponymy relation candidates is used for these evidence. We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation. Semantic noun classes have been regarded as useful information in semantic relation acquisition (De Saeger et al., 2009). EM-based clustering (Kazama and Torisawa, 2008) is used for obtaining 500 semantic noun classes4 from 5 × 105 nouns (including single-word and multi-word ones) and their 4 × 108 dependency relations with 5 × 105 verbs and other nouns in our target Web 4 Because EM clustering provides a probability distribution over noun class nc, we obtain discrete classes of each noun n with a probability threshold p(nc|n) ≥ 0.2 (De Saeger et al., 2009). 846 Instance space Feature space Common instances Co-training (Blum and Mitchell, 1998) Same Split by human decision Genuine-common (or All unlabeled) instances Bilingual co-training (Oh et al., 2009) Dif"
C10-1095,P09-1049,1,0.284772,"isition – one handling structured text and the other handling unstructured text – collaborate by repeatedly exchanging the knowledge they acquired about hyponymy relations. Unlike conventional co-training, the two processes in Co-STAR are applied to different source texts and training data. We show the effectiveness of this algorithm through experiments on largescale hyponymy-relation acquisition from Japanese Wikipedia and Web texts. We also show that Co-STAR is robust against noisy training data. 1 Evidence from structured text: topic hierarchy, layout structure of documents, and HTML tags (Oh et al., 2009; Ravi and Pasca, 2008; Sumida and Torisawa, 2008; Shinzato and Torisawa, 2004). Introduction Acquiring semantic knowledge, especially semantic relations between lexical terms, is regarded as a crucial step in developing high-level natural language applications. This paper proposes Co-STAR (a Co-training STyle Algorithm for hyponymy Relation acquisition from structured and unstructured text). Similar to cotraining (Blum and Mitchell, 1998), two hyponymy relation extractors in Co-STAR, one for structured and the other for unstructured text, iteratively collaborate to boost each other’s performa"
C10-1095,N04-1041,0,0.0114666,"n from Wikipedia, lexical features are used to recognize the lexical evidence for hyponymy relations. Lexico-syntactic patterns for hyponymy relation show different coverage and accuracy in hyponymy relation acquisition (Ando et al., 2004). Further if multiple lexico-syntactic patterns support acquisition of hyponymy relation candidates, these candidates are more likely to be actual hyponymy relations. The pattern feature of hyponymy relation candidates is used for these evidence. We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation. Semantic noun classes have been regarded as useful information in semantic relation acquisition (De Saeger et al., 2009). EM-based clustering (Kazama and Torisawa, 2008) is used for obtaining 500 semantic noun classes4 from 5 × 105 nouns (including single-word and multi-word ones) and their 4 × 108 dependency relations with 5 × 105 verbs and other nouns in our target Web 4 Because EM clustering provides a probability distribution over noun class nc, we obtain dis"
C10-1095,C04-1135,1,0.811594,"ructured text – collaborate by repeatedly exchanging the knowledge they acquired about hyponymy relations. Unlike conventional co-training, the two processes in Co-STAR are applied to different source texts and training data. We show the effectiveness of this algorithm through experiments on largescale hyponymy-relation acquisition from Japanese Wikipedia and Web texts. We also show that Co-STAR is robust against noisy training data. 1 Evidence from structured text: topic hierarchy, layout structure of documents, and HTML tags (Oh et al., 2009; Ravi and Pasca, 2008; Sumida and Torisawa, 2008; Shinzato and Torisawa, 2004). Introduction Acquiring semantic knowledge, especially semantic relations between lexical terms, is regarded as a crucial step in developing high-level natural language applications. This paper proposes Co-STAR (a Co-training STyle Algorithm for hyponymy Relation acquisition from structured and unstructured text). Similar to cotraining (Blum and Mitchell, 1998), two hyponymy relation extractors in Co-STAR, one for structured and the other for unstructured text, iteratively collaborate to boost each other’s performance. Many algorithms have been developed to automatically acquire semantic rela"
C10-1095,I08-1025,0,0.34439,"TIGER) can be used as the lexical evidence. Such information is provided along with the words/morphemes and the parts of speech of hyper and hypo, which can be multiword/morpheme nouns. Structure features provide evidence found in layout or tree structures for hyponymy relations. For example, hyponymy relations (T IGER, B EN GAL TIGER ) and (T IGER ,M ALAYAN TIGER ) can be obtained from tree structure “(root node, children nodes of Subspecies)” in Fig 3. 3.2 Acquisition from Web Texts As the target for hyponymy relation acquisition from the Web, we used 5 × 107 pages from the TSUBAKI corpus (Shinzato et al., 2008), a 108 page Japanese Web corpus that was dependency parsed with KNP (Kurohashi-Nagao Parser) (Kurohashi and Kawahara, 2005). Hyponymy relation candidates are extracted from the corpus based on the lexico-syntactic patterns such as “hypo nado hyper (hyper such as hypo)” and “hypo to iu hyper (hyper called hypo)” (Ando 3 MeCab (http://mecab.sourceforge.net/) was used to provide the lexical features. et al., 2004). We extracted 6 × 106 Japanese hyponymy relation candidates from the Japanese Web texts. Features (WebFeature) used for classification are summarized in Table 1. Similar to the hyponym"
C10-1095,P06-1101,0,0.22486,"Missing"
C10-1095,I08-2126,1,0.888695,"and the other handling unstructured text – collaborate by repeatedly exchanging the knowledge they acquired about hyponymy relations. Unlike conventional co-training, the two processes in Co-STAR are applied to different source texts and training data. We show the effectiveness of this algorithm through experiments on largescale hyponymy-relation acquisition from Japanese Wikipedia and Web texts. We also show that Co-STAR is robust against noisy training data. 1 Evidence from structured text: topic hierarchy, layout structure of documents, and HTML tags (Oh et al., 2009; Ravi and Pasca, 2008; Sumida and Torisawa, 2008; Shinzato and Torisawa, 2004). Introduction Acquiring semantic knowledge, especially semantic relations between lexical terms, is regarded as a crucial step in developing high-level natural language applications. This paper proposes Co-STAR (a Co-training STyle Algorithm for hyponymy Relation acquisition from structured and unstructured text). Similar to cotraining (Blum and Mitchell, 1998), two hyponymy relation extractors in Co-STAR, one for structured and the other for unstructured text, iteratively collaborate to boost each other’s performance. Many algorithms have been developed to autom"
C10-1095,D08-1061,0,0.016097,"ctured and unstructured text in different styles, different kinds of evidence have been used for semantic relation acquisition: Recently, researchers have used both structured and unstructured text for semantic-relation acquisition, with the aim of exploiting such different kinds of evidence at the same time. They either tried to improve semantic relation acquisition by putting the different evidence together into a single classifier (Pennacchiotti and Pantel, 2009) or to improve the coverage of semantic relations by combining and ranking the semantic relations obtained from two source texts (Talukdar et al., 2008). In this paper we propose an algorithm called Co-STAR. The main contributions of this work can be summarized as follows. • Co-STAR is a semi-supervised learning method composed of two parallel and iterative processes over structured and unstructured text. It was inspired by bilingual cotraining, which is a framework for hyponymy relation acquisition from source texts in two languages (Oh et al., 2009). Like bilingual co-training, two processes in Co-STAR operate independently on structured text and unstructured text. These two processes are trained in a supervised manner with their initial tr"
C10-1095,D09-1025,0,0.103298,"ce. Many algorithms have been developed to automatically acquire semantic relations from structured and unstructured text. Because term pairs are encoded in structured and unstructured text in different styles, different kinds of evidence have been used for semantic relation acquisition: Recently, researchers have used both structured and unstructured text for semantic-relation acquisition, with the aim of exploiting such different kinds of evidence at the same time. They either tried to improve semantic relation acquisition by putting the different evidence together into a single classifier (Pennacchiotti and Pantel, 2009) or to improve the coverage of semantic relations by combining and ranking the semantic relations obtained from two source texts (Talukdar et al., 2008). In this paper we propose an algorithm called Co-STAR. The main contributions of this work can be summarized as follows. • Co-STAR is a semi-supervised learning method composed of two parallel and iterative processes over structured and unstructured text. It was inspired by bilingual cotraining, which is a framework for hyponymy relation acquisition from source texts in two languages (Oh et al., 2009). Like bilingual co-training, two processes"
C10-1095,D09-1098,0,\N,Missing
C10-2015,W06-2920,0,0.0858243,"accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a sho"
C10-2015,D07-1101,0,0.167428,"d parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph"
C10-2015,I08-1012,1,0.900686,"Missing"
C10-2015,D09-1060,1,0.694572,"by 1.02 points for Chi132 Table 5: Results for English Table 4: Results for Chinese Baseline OURS OURS+STACK Zhao2009 Yu2008 STACK Chen2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 show"
C10-2015,N06-1021,0,0.0247458,"Missing"
C10-2015,P04-1054,0,0.037329,"e long dependencies. The mechanism can easily be implemented by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Ni"
C10-2015,C96-1058,0,0.451123,"s in w10 A w3 . This simple example shows how to use the decision history to help parse the long distance dependencies. 3 Background: graph-based parsing models Before we describe our method, we briefly introduce the graph-based parsing models. We denote input sentence w by w = (w0 , w1 , ..., wn ), where w0 = ROOT is an artificial root token inserted at the beginning of the sentence and does not depend on any other token in w and wi refers to a word. We employ the second-order projective graphbased parsing model of Carreras (2007), which is an extension of the projective parsing algorithm of Eisner (1996). The parsing algorithms used in Carreras (2007) independently find the left and right dependents of a word and then combine them later in a bottomup style based on Eisner (1996). A subtree that spans the words in [s, t] (and roots at s or t) is represented by chart item [s, t, right/lef t, C/I], where right (left) indicates that the root of the subtree is s (t) and C means that the item is complete while I means that the item is incomplete (McDonald, 2006). Here, complete item in the right (left) direction means that the words other than s (t) cannot have dependents outside [s, t] and incompl"
C10-2015,P07-1050,0,0.0219216,"93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are several studies that tried to overcome the limited feature scope of graph-based dependency parsing models . Nakagawa (2007) proposed a method to deal with the intractable inference problem in a graphbased model by introducing the Gibbs sampling algorithm. Compared with their approach, our approach is much simpler yet effective. Hall (2007) used a re-ranking scheme to provide global features while we simply augment the features of an existing parser. Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. One parser uses dependency predictions made by another parser. Our results show that our approach can be used in the stacking frameworks to achieve higher accuracy. 7 Conclusions This paper proposes an approach for improving graph-based dependency parsing by using the decision history. For the graph-based model, we design a set of features over"
C10-2015,P08-1068,0,0.158647,"ub-sentences. • Distance: use the history-based features for the relation of two words within a predefined distance. We set the thresholds to 3, 5, and 10. 5 Experimental results and our new systems OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing,"
C10-2015,J93-2004,0,0.0336939,"history-based features for all the word pairs without any restriction. • Sub-sentences: use the history-based features only for the relation of two words from sub-sentences. Here, we use punctuation marks to split sentences into sub-sentences. • Distance: use the history-based features for the relation of two words within a predefined distance. We set the thresholds to 3, 5, and 10. 5 Experimental results and our new systems OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For"
C10-2015,D07-1013,0,0.0985631,"pendency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as repor"
C10-2015,E06-1011,0,0.179838,"ted by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach"
C10-2015,D07-1100,0,0.0140656,"systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are several studies that tried to overcome the limited feature scope of graph-based dependency parsing models . Nakagawa (2007) proposed a method to deal with the intractable inference problem in a graphbased model by introducing the Gibbs sampling algorithm. Compared with their approach, our approach is much simpler yet effective. Hall (2007) used a re-ranking scheme to provide global features while we simply augment the features of an existing parser. Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. One parser uses dependency predictions made by another parser. Our results show that our approach can be used in the stacking fr"
C10-2015,2006.iwslt-evaluation.9,0,0.0170896,"models and may be used as features to help parse long dependencies. The mechanism can easily be implemented by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for tra"
C10-2015,P08-1108,0,0.0909482,"n2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Cor"
C10-2015,W04-2407,0,0.0205074,"4). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models. Our idea is to use decision history, which is made in previous scans in a bottom-up procedure, to help parse other words in later scans. In the botto"
C10-2015,W96-0213,0,0.215296,"ms OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et"
C10-2015,D09-1058,0,0.0509405,"tem achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Corston-Oliver et al. (2006), Z&C 2008 refers to the combination system of Zhang and Clark (2008), STACK refers to our implementation of the combination parser of Nivre and McDonald (2008), KOO2008 refers to the parser of Koo et al. (2008), Chen2009 refers to the parser of Chen et al. (2009), and Suzuki2009 refers to the parser of Suzuki et al. (2009) that is the best reported result for this data. The results shows that OURS outperformed the first two systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are seve"
C10-2015,W03-3023,0,0.326783,"on (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models. Our idea is to use decision history, which is made in previous scans in a bottom-up procedure, to help parse other words in lat"
C10-2015,C08-1132,0,0.0733273,"ned on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens with the correct HEAD 5 . And we also evaluated on complete dependency analysis. In our experiments, we implemented our systems on the MSTParser6 and extended with the parent-child-grandchild structures (McDonald and Pereira, 2006; Carreras, 2007). For the baseline systems, we used the first- and second-order (parent-sibling) features that were used in McDonald and Pereira (2006) and other second-order features (parent-child-grandchild) that were used in Carreras (2007). In the following se"
C10-2015,D08-1059,0,0.136222,"hat OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Corston-Oliver et al. (2006), Z&C 2008 refers to the combination system of Zhang and Clark (2008), STACK refers to our implementation of the combination parser of Nivre and McDonald (2008), KOO2008 refers to the parser of Koo et al. (2008), Chen2009 refers to the parser of Chen et al. (2009), and Suzuki2009 refers to the parser of Suzuki et al. (2009) that is the best reported result for this data. The results shows that OURS outperformed the first two systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Comple"
C10-2015,P09-1007,0,0.0129002,"s are shown in parentheses. The results show that OURS provided better performance over the Baselines by 1.02 points for Chi132 Table 5: Results for English Table 4: Results for Chinese Baseline OURS OURS+STACK Zhao2009 Yu2008 STACK Chen2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improv"
C10-2015,D07-1096,0,\N,Missing
C12-1169,J92-4003,0,0.0554287,"Missing"
C12-1169,W10-2910,0,0.35934,"to deal with the polarity shifting, 179 Chinese polarity shifting words were collected and used in the CEIA. As for the features, we used the same features as those in Nakagawa et al. (2010). 3 New Features In this section, we describe our approach that effectively employs the dependency information, semantic class and distance information into the above evaluative information extraction (specifically evaluative expression extraction and evaluation target extraction). 3.1 Dependency Features The use of syntactic or deep linguistic features has been tried in opinion analysis in the literature. Johansson and Moschitti (2010) demonstrated that the features derived from grammatical and 2780 w w w i w i w w i+1 (1) w i+1 (2) w w i (3) w i w i+1 h h i+1 w i+1 (4) w h w i (5) w h w w h1 h2 w i i+1 (6) Figure 3: Different dependencies between w i and w i+1 that can be linked by one or two arcs semantic role structure can be used to improve the detection of opinionated expressions in subjectivity analysis. However, based on their evaluation, the precision decreases while the F-measure is increased. In addition, they claimed that a sequence tagging model cannot be used when using syntactic features, and they used reranki"
C12-1169,P06-2059,0,0.122589,"first introduce the specifications of the evaluative information on which this study is focused, and then we explain how an evaluative information corpus is constructed. Finally, we explain each process of CEIA in detail. 2.1 Evaluative Information There is a wide variety of evaluative information on the web, such as reviews of products and criticisms of policies. The information reflects various perspectives of individuals or organizations. Research on evaluative information analysis are conducted from different points of views and at different levels of granularity (Kobayashi et al., 2004; Kaji and Kitsuregawa, 2006; Liu, 2010; Pang and Lee, 2008; Akamine et al., 2010). In this section, we describe the specifications of evaluative information on which this study is focused. We analyze the evaluative information at a fine-grained level. We use a 5-tuple that consists of (1) an evaluative expression, (2) an evaluation holder, (3) an evaluation target, (4) an evaluation type, and (5) sentiment polarity as the basic unit of evaluative information and call it an evaluative information set. Each item is defined as follows. Evaluative expression is a span of text that describes the evaluation. It can be a singl"
C12-1169,P08-1047,1,0.848584,"’s head in a sentence. Since the grammatical information is very important for evaluation target extraction, new features of { depri−2 , depri−1 , depri , depri+1 , depri+2 , depri−1 &depri , depri &d epri+1 } are added for evaluation target extraction. With these features, the grammatical function information can be encoded in both the evaluative expression and evaluation target extraction tasks. 3.2 Semantic Class Features The idea of combining semantic classes of words with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008), dependency parsing (Koo et al., 2008) and Chinese word segmentation and POS tagging (Wang et al., 2011). We adopt and extend these techniques to evaluative information analysis and demonstrate their effectiveness in this task. We produced the semantic classes of various levels of granularity, by using the Brown cluster hierarchy (Brown et al., 1992) at various lengths. Note that a semantic class is represented by a bit string that reflects the branching of the semantic class hierarchy. We designed two kinds of semantic class features: (i) full string feature: full string of the semantic clas"
C12-1169,P08-1068,0,0.0197209,"ormation is very important for evaluation target extraction, new features of { depri−2 , depri−1 , depri , depri+1 , depri+2 , depri−1 &depri , depri &d epri+1 } are added for evaluation target extraction. With these features, the grammatical function information can be encoded in both the evaluative expression and evaluation target extraction tasks. 3.2 Semantic Class Features The idea of combining semantic classes of words with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008), dependency parsing (Koo et al., 2008) and Chinese word segmentation and POS tagging (Wang et al., 2011). We adopt and extend these techniques to evaluative information analysis and demonstrate their effectiveness in this task. We produced the semantic classes of various levels of granularity, by using the Brown cluster hierarchy (Brown et al., 1992) at various lengths. Note that a semantic class is represented by a bit string that reflects the branching of the semantic class hierarchy. We designed two kinds of semantic class features: (i) full string feature: full string of the semantic class for w i ; (ii) 6-bit prefix feature:"
C12-1169,N04-1043,0,0.0182696,"between w i and w i ’s head in a sentence. Since the grammatical information is very important for evaluation target extraction, new features of { depri−2 , depri−1 , depri , depri+1 , depri+2 , depri−1 &depri , depri &d epri+1 } are added for evaluation target extraction. With these features, the grammatical function information can be encoded in both the evaluative expression and evaluation target extraction tasks. 3.2 Semantic Class Features The idea of combining semantic classes of words with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008), dependency parsing (Koo et al., 2008) and Chinese word segmentation and POS tagging (Wang et al., 2011). We adopt and extend these techniques to evaluative information analysis and demonstrate their effectiveness in this task. We produced the semantic classes of various levels of granularity, by using the Brown cluster hierarchy (Brown et al., 1992) at various lengths. Note that a semantic class is represented by a bit string that reflects the branching of the semantic class hierarchy. We designed two kinds of semantic class features: (i) full string feature: full"
C12-1169,N10-1120,0,0.298116,"s to the evaluation expressions and the words next to the evaluative expressions. For CRF model, we use the same features as in Section 2.4.2. If no holder was found by the CRF model, [undefined] was set as the evaluation holder of the current evaluation expression. 2.4.5 Determination of Sentiment Polarity A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002). However, this method cannot consider syntactic structures that seem essential to infer the polarity of a whole sentence. We follow the work of Nakagawa et al. (2010) and use a dependency tree-based method, which was demonstrated to perform better than other methods based on bag-of-words in both English and Japanese sentiment classification tasks. The sentiment polarity is classified using conditional random fields (CRFs) with hidden variables. In the method, the sentiment polarity of each dependency subtree, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated by considering the interactions between the hidden variables. For example in Figure 2, each phrase (indicated by a circle)"
C12-1169,W02-1011,0,0.0115801,"..ws−1 &ws , t s−1 &t s } and bigram n {ws+1+1 &ws+n+2 , t s+n+1 &t s+n+2 ...w l−1 &w l , t l−1 &t l } features to add the bigram information for the words previous to the evaluation expressions and the words next to the evaluative expressions. For CRF model, we use the same features as in Section 2.4.2. If no holder was found by the CRF model, [undefined] was set as the evaluation holder of the current evaluation expression. 2.4.5 Determination of Sentiment Polarity A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002). However, this method cannot consider syntactic structures that seem essential to infer the polarity of a whole sentence. We follow the work of Nakagawa et al. (2010) and use a dependency tree-based method, which was demonstrated to perform better than other methods based on bag-of-words in both English and Japanese sentiment classification tasks. The sentiment polarity is classified using conditional random fields (CRFs) with hidden variables. In the method, the sentiment polarity of each dependency subtree, which is not observable in training data, is represented by a hidden variable. The p"
C12-1169,I11-1035,1,0.926005,"features of { depri−2 , depri−1 , depri , depri+1 , depri+2 , depri−1 &depri , depri &d epri+1 } are added for evaluation target extraction. With these features, the grammatical function information can be encoded in both the evaluative expression and evaluation target extraction tasks. 3.2 Semantic Class Features The idea of combining semantic classes of words with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008), dependency parsing (Koo et al., 2008) and Chinese word segmentation and POS tagging (Wang et al., 2011). We adopt and extend these techniques to evaluative information analysis and demonstrate their effectiveness in this task. We produced the semantic classes of various levels of granularity, by using the Brown cluster hierarchy (Brown et al., 1992) at various lengths. Note that a semantic class is represented by a bit string that reflects the branching of the semantic class hierarchy. We designed two kinds of semantic class features: (i) full string feature: full string of the semantic class for w i ; (ii) 6-bit prefix feature: 6-bit prefix of the semantic class for w i . 3.3 Distance Feature"
C14-1135,P10-2045,0,0.0192264,"temporal ordering except that on causality. From VerbNet (Levin, 1993; Kipper et al., 2006), the hyponymy/synonymy type of entailment relations may be derived using templates in the same verb classes constructed based on shared syntactic behavior, possibly with the help of statistical methods. However, the other types of relations that can be derived from PPTT cannot be derived from VerbNet, since there is no link representing relationships between the verb classes. FrameNet (Fillmore, 1976; Baker et al., 1998) was used to derive hyponymy/synonymy types of entailment (Coyne and Rambow, 2009; Aharon et al., 2010) using information such as a Frameto-frame relation ‘Inheritance’ (is-a relation). In addition, happens-before relations can be derived using ‘Precedes’ (Later-Earlier relations). However, since it does not contain semantic constraints like enablement and necessity that PPTT contains, it is not trivial to derive presupposition type of entailment or anomalous obstruction instances from it. TimeML (Pustejovsky et al., 2003; Puscasu and Mititelu, 2008) contains various temporal information and can be used to derive context-dependent happens-before relations such as the relation between “leaves” a"
C14-1135,P98-1013,0,0.137676,"quires a manual labor cost, which amounted to three man-months in our case; however, we believe that this cost is lower than the cost for developing highly-precise automatic acquisition methods for all of happens-before, entailment, contradiction, and anomalous obstruction relations. We plan to release PPTT and the derived relation instances after the manual annotation of the derived instances to the NLP community. 2 Related Works PPTT might resemble other semantic lexicons created in the long history of NLP (Levin, 1993; Kipper et al., 2006; Fellbaum, 1998; Bond et al., 2009; Fillmore, 1976; Baker et al., 1998; Halliday, 1985; Pustejovsky et al., 2003; Puscasu and Mititelu, 2008; Bejar et al., 1991; Jurgens et al., 2012). PPTT is different in that it primarily aims at deriving various types of semantic relations on a large scale exploiting the notion of the phase of story, rather than being a comprehensive taxonomy like those existing semantic lexicons. As a result, PPTT can derive more varieties of semantic relations between templates than any one of those existing lexicons. From WordNet (Fellbaum, 1998; Bond et al., 2009), we can derive entailment and contradiction relations using synsets and syn"
C14-1135,J12-1003,0,0.0327007,"Missing"
C14-1135,P08-1090,0,0.15633,"pensable knowledge for many NLP applications. For instance, entailment relations are crucial in information extraction and QA (Dagan et al., 2009; Weisman et al., 2012; Berant et al., 2012; Turney and Mohammad, 2014). Temporal relations such as happens-before (Chklovski and Pantel, 2004b; Regneri et al., 2010) are important for enhancing deep semantic processing. A problem, however, is that it is difficult to acquire those relations with a broad coverage. Although many sophisticated machine learning techniques have been applied to various kinds of corpora for this task (Szpektor et al., 2007; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Chambers and Jurafsky, 2009; Hashimoto et al., 2012; Talukdar et al., 2012; Kloetzer et al., 2013), no satisfactory coverage has been achieved, probably due to data sparseness in the input data. In this work we take a completely different approach: we manually construct a semantic lexicon called Phased Predicate Template Taxonomy (PPTT), and derive various types of semantic relations on a large-scale by using it. Our target language is Japanese, but examples are given in English for simplicity throughout this paper. PPTT is a taxonomy of predicate templates (predicate"
C14-1135,P09-1068,0,0.130912,"tance, entailment relations are crucial in information extraction and QA (Dagan et al., 2009; Weisman et al., 2012; Berant et al., 2012; Turney and Mohammad, 2014). Temporal relations such as happens-before (Chklovski and Pantel, 2004b; Regneri et al., 2010) are important for enhancing deep semantic processing. A problem, however, is that it is difficult to acquire those relations with a broad coverage. Although many sophisticated machine learning techniques have been applied to various kinds of corpora for this task (Szpektor et al., 2007; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Chambers and Jurafsky, 2009; Hashimoto et al., 2012; Talukdar et al., 2012; Kloetzer et al., 2013), no satisfactory coverage has been achieved, probably due to data sparseness in the input data. In this work we take a completely different approach: we manually construct a semantic lexicon called Phased Predicate Template Taxonomy (PPTT), and derive various types of semantic relations on a large-scale by using it. Our target language is Japanese, but examples are given in English for simplicity throughout this paper. PPTT is a taxonomy of predicate templates (predicates with one argument slot like rescue X, “Template” he"
C14-1135,P11-1098,0,0.0449869,"Missing"
C14-1135,W04-3205,0,0.05906,", rescue X⊃X is alive), happens-before (e.g., buy X⇒drink X), and a novel relation type anomalous obstruction (e.g., X is sold out;cannot buy X). Such derivation became possible thanks to PPTT’s design and the use of statistical methods. 1 Introduction Databases of various semantic relations between natural language expressions are indispensable knowledge for many NLP applications. For instance, entailment relations are crucial in information extraction and QA (Dagan et al., 2009; Weisman et al., 2012; Berant et al., 2012; Turney and Mohammad, 2014). Temporal relations such as happens-before (Chklovski and Pantel, 2004b; Regneri et al., 2010) are important for enhancing deep semantic processing. A problem, however, is that it is difficult to acquire those relations with a broad coverage. Although many sophisticated machine learning techniques have been applied to various kinds of corpora for this task (Szpektor et al., 2007; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Chambers and Jurafsky, 2009; Hashimoto et al., 2012; Talukdar et al., 2012; Kloetzer et al., 2013), no satisfactory coverage has been achieved, probably due to data sparseness in the input data. In this work we take a completely diffe"
C14-1135,D09-1122,1,0.903842,"Missing"
C14-1135,D12-1057,1,0.91074,"Missing"
C14-1135,P14-1093,1,0.843683,"Missing"
C14-1135,S12-1047,0,0.175429,"is lower than the cost for developing highly-precise automatic acquisition methods for all of happens-before, entailment, contradiction, and anomalous obstruction relations. We plan to release PPTT and the derived relation instances after the manual annotation of the derived instances to the NLP community. 2 Related Works PPTT might resemble other semantic lexicons created in the long history of NLP (Levin, 1993; Kipper et al., 2006; Fellbaum, 1998; Bond et al., 2009; Fillmore, 1976; Baker et al., 1998; Halliday, 1985; Pustejovsky et al., 2003; Puscasu and Mititelu, 2008; Bejar et al., 1991; Jurgens et al., 2012). PPTT is different in that it primarily aims at deriving various types of semantic relations on a large scale exploiting the notion of the phase of story, rather than being a comprehensive taxonomy like those existing semantic lexicons. As a result, PPTT can derive more varieties of semantic relations between templates than any one of those existing lexicons. From WordNet (Fellbaum, 1998; Bond et al., 2009), we can derive entailment and contradiction relations using synsets and synset-links that represent relations such as ‘troponym’, ‘antonym’ and ‘entailment’. However, happens-before and an"
C14-1135,kipper-etal-2006-extending,0,0.02025,"om the original one in Hashimoto et al. (2012). We inserted the verb “prepared” into the original definition. This clarifies that various preparation processes for X, such as buy X, can be regarded as excitatory templates. We also assume that such templates as X exists and have X, which mean little more than just existence, are regarded as excitatory templates in PPTT based on the assumption that existence can be regarded as preparation for the function of X. 1424 cannot be derived from it, since there is no information on temporal ordering except that on causality. From VerbNet (Levin, 1993; Kipper et al., 2006), the hyponymy/synonymy type of entailment relations may be derived using templates in the same verb classes constructed based on shared syntactic behavior, possibly with the help of statistical methods. However, the other types of relations that can be derived from PPTT cannot be derived from VerbNet, since there is no link representing relationships between the verb classes. FrameNet (Fillmore, 1976; Baker et al., 1998) was used to derive hyponymy/synonymy types of entailment (Coyne and Rambow, 2009; Aharon et al., 2010) using information such as a Frameto-frame relation ‘Inheritance’ (is-a"
C14-1135,D13-1065,1,0.854198,"Missing"
C14-1135,P13-1170,1,0.860734,"Missing"
C14-1135,puscasu-mititelu-2008-annotation,0,0.0701733,"in our case; however, we believe that this cost is lower than the cost for developing highly-precise automatic acquisition methods for all of happens-before, entailment, contradiction, and anomalous obstruction relations. We plan to release PPTT and the derived relation instances after the manual annotation of the derived instances to the NLP community. 2 Related Works PPTT might resemble other semantic lexicons created in the long history of NLP (Levin, 1993; Kipper et al., 2006; Fellbaum, 1998; Bond et al., 2009; Fillmore, 1976; Baker et al., 1998; Halliday, 1985; Pustejovsky et al., 2003; Puscasu and Mititelu, 2008; Bejar et al., 1991; Jurgens et al., 2012). PPTT is different in that it primarily aims at deriving various types of semantic relations on a large scale exploiting the notion of the phase of story, rather than being a comprehensive taxonomy like those existing semantic lexicons. As a result, PPTT can derive more varieties of semantic relations between templates than any one of those existing lexicons. From WordNet (Fellbaum, 1998; Bond et al., 2009), we can derive entailment and contradiction relations using synsets and synset-links that represent relations such as ‘troponym’, ‘antonym’ and ‘"
C14-1135,P10-1100,0,0.0609641,"ns-before (e.g., buy X⇒drink X), and a novel relation type anomalous obstruction (e.g., X is sold out;cannot buy X). Such derivation became possible thanks to PPTT’s design and the use of statistical methods. 1 Introduction Databases of various semantic relations between natural language expressions are indispensable knowledge for many NLP applications. For instance, entailment relations are crucial in information extraction and QA (Dagan et al., 2009; Weisman et al., 2012; Berant et al., 2012; Turney and Mohammad, 2014). Temporal relations such as happens-before (Chklovski and Pantel, 2004b; Regneri et al., 2010) are important for enhancing deep semantic processing. A problem, however, is that it is difficult to acquire those relations with a broad coverage. Although many sophisticated machine learning techniques have been applied to various kinds of corpora for this task (Szpektor et al., 2007; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Chambers and Jurafsky, 2009; Hashimoto et al., 2012; Talukdar et al., 2012; Kloetzer et al., 2013), no satisfactory coverage has been achieved, probably due to data sparseness in the input data. In this work we take a completely different approach: we manual"
C14-1135,P07-1058,0,0.193074,"e expressions are indispensable knowledge for many NLP applications. For instance, entailment relations are crucial in information extraction and QA (Dagan et al., 2009; Weisman et al., 2012; Berant et al., 2012; Turney and Mohammad, 2014). Temporal relations such as happens-before (Chklovski and Pantel, 2004b; Regneri et al., 2010) are important for enhancing deep semantic processing. A problem, however, is that it is difficult to acquire those relations with a broad coverage. Although many sophisticated machine learning techniques have been applied to various kinds of corpora for this task (Szpektor et al., 2007; Chambers and Jurafsky, 2008; Hashimoto et al., 2009; Chambers and Jurafsky, 2009; Hashimoto et al., 2012; Talukdar et al., 2012; Kloetzer et al., 2013), no satisfactory coverage has been achieved, probably due to data sparseness in the input data. In this work we take a completely different approach: we manually construct a semantic lexicon called Phased Predicate Template Taxonomy (PPTT), and derive various types of semantic relations on a large-scale by using it. Our target language is Japanese, but examples are given in English for simplicity throughout this paper. PPTT is a taxonomy of p"
C14-1135,P05-1017,0,0.0226927,"ived are different from the ones given to the links. 4 Construction of PPTT and Relation Derivation Using the automatic acquisition method proposed by Hashimoto et al. (2012), we collected 10,825 candidates of excitatory/inhibitory templates from a 600-million-page web corpus (hereafter, W Corpus). Hashimoto et al.’s method constructs a network of templates based on their co-occurrence in sentences with a small number of seed templates of which excitation polarity are assigned manually, and infers the polarity of all the templates in the network by a constraint solver based on the spin model (Takamura et al., 2005). Then, we added the 20,000 most frequent templates in the corpus that could not be extracted automatically for a total of 30,825 templates. Three human annotators (not the authors) judged the polarity of the templates, and we included the excitatory and the inhibitory templates but excluded the neutral templates in PPTT due to the reason discussed in Section 3.1. We also excluded templates whose variable X is the subject of a transitive verb. This is because the subject position is often occupied by living things, and since the functions/objectives of such subjects seem difficult to identify,"
C14-1135,D12-1018,0,0.0245792,"Missing"
C14-1135,W09-3401,0,\N,Missing
C14-1135,C98-1013,0,\N,Missing
C14-1135,P13-1159,1,\N,Missing
C16-2055,D09-1122,1,0.696804,"Missing"
C16-2055,P11-1109,1,0.903764,"Missing"
C16-2055,D12-1057,1,0.908007,"Missing"
C16-2055,P14-1093,1,0.868857,"he what-happens-if QA system. Figure 1: Example screenshots of WISDOM X. In the following, we provide an overview of WISDOM X, DISAANA, and D-SUMM. 2 WISDOM X: Information Analysis System WISDOM X, which discovers answers to given questions from about 4-billion web pages by several kinds of deep semantic processing, consists of four QA systems, each of which deals with different types of questions: factoid (e.g., What prevents global warming?), why-type (Oh et al., 2012; Oh et al., 2013; Oh et al., 2016) (e.g., Why did the global warming worsen?), what-happens-if-type (Hashimoto et al., 2012; Hashimoto et al., 2014) (e.g., What happens if global warming worsens?), and definition type (e.g., What is global warming?). It also has a functionality that suggests questions to users. These QA systems use a large-scale knowledge base for entailment recognition(Saeger et al., 2009; Hashimoto et al., 2009; Saeger et al., 2011; Hashimoto et al., 2011; Kloetzer et al., 2013; Sano et al., 2014; Kloetzer et al., 2015) and semantic noun clusters (Kazama and Torisawa, 2008). We also developed a middleware RaSC (Tanaka et al., 2016) to efficiently run various NLP tools on hundreds of computation nodes. We designed WISDOM"
C16-2055,D16-1132,1,0.798333,"Missing"
C16-2055,P08-1047,1,0.843363,"why-type (Oh et al., 2012; Oh et al., 2013; Oh et al., 2016) (e.g., Why did the global warming worsen?), what-happens-if-type (Hashimoto et al., 2012; Hashimoto et al., 2014) (e.g., What happens if global warming worsens?), and definition type (e.g., What is global warming?). It also has a functionality that suggests questions to users. These QA systems use a large-scale knowledge base for entailment recognition(Saeger et al., 2009; Hashimoto et al., 2009; Saeger et al., 2011; Hashimoto et al., 2011; Kloetzer et al., 2013; Sano et al., 2014; Kloetzer et al., 2015) and semantic noun clusters (Kazama and Torisawa, 2008). We also developed a middleware RaSC (Tanaka et al., 2016) to efficiently run various NLP tools on hundreds of computation nodes. We designed WISDOM X to provide a wide range of pin-point answers, e.g., a noun phrase for factoid questions and a sentence for what-happens-if-type questions. This feature constitutes a major difference from commercial search engines, which merely give web pages for a given query and rely on human effort to ascertain pin-point answers. In addition, WISDOM X can provide numerous answers to a given question. For instance, the current version of WISDOM X provides aro"
C16-2055,D13-1065,1,0.898003,"Missing"
C16-2055,D15-1190,1,0.821981,"ons: factoid (e.g., What prevents global warming?), why-type (Oh et al., 2012; Oh et al., 2013; Oh et al., 2016) (e.g., Why did the global warming worsen?), what-happens-if-type (Hashimoto et al., 2012; Hashimoto et al., 2014) (e.g., What happens if global warming worsens?), and definition type (e.g., What is global warming?). It also has a functionality that suggests questions to users. These QA systems use a large-scale knowledge base for entailment recognition(Saeger et al., 2009; Hashimoto et al., 2009; Saeger et al., 2011; Hashimoto et al., 2011; Kloetzer et al., 2013; Sano et al., 2014; Kloetzer et al., 2015) and semantic noun clusters (Kazama and Torisawa, 2008). We also developed a middleware RaSC (Tanaka et al., 2016) to efficiently run various NLP tools on hundreds of computation nodes. We designed WISDOM X to provide a wide range of pin-point answers, e.g., a noun phrase for factoid questions and a sentence for what-happens-if-type questions. This feature constitutes a major difference from commercial search engines, which merely give web pages for a given query and rely on human effort to ascertain pin-point answers. In addition, WISDOM X can provide numerous answers to a given question. For"
C16-2055,Y15-1063,1,0.853113,"by DISAANA. Similar problem reports such as “buildings collapsed” and “houses were demolished” were merged into a single problem report. In addition, the reports were classified according to their subparts of a specified area. Another important issue is false rumors. In past disaster situations, numerous false rumors were spread widely on Twitter (e.g., “Drinking iodine protects against radiation” during the 2011 Great East Japan Earthquake). DISAANA and D-SUMM give an alert to users by retrieving not only answers but also information that contradicts the answers by using a modality analyzer (Mizuno et al., 2015) and contradictory patterns (Kloetzer et al., 2013). For example, when “acid rain” is one of the answers to the question, “What happened in a petrochemical complex?” and there is a tweet that contradicts the answer such as “Acid rain in the petrochemical complex is a false rumor,” DISAANA presents it alongside the original tweet: the source of the answer. By examining such contradictory information, users can notice 265 SELECTED PLACE: Kumamoto prefecture QUESTION: What is in short supply in Kumamoto? CATEGORY: daily necessities portable toilets aftershocks continue CATEGORY: medical supplies"
C16-2055,P13-1170,1,0.914971,"Missing"
C16-2055,C14-1135,1,0.876252,"Missing"
C96-2160,P95-1013,0,\N,Missing
C96-2160,P85-1018,0,\N,Missing
C98-2128,1995.iwpt-1.9,0,0.0445456,"ms is able to achieve the efficiency we established as our goal. Moreover, these two systems have serious disadvantages as a framework for practical applications. The ProFIT approach, for example, tends to consume too much memory for execution. It is also difficult, if not impossible, to combine them with other techniques like parallel parsing, etc., because these two systems have been embedded in Prolog. 1.2 O u r Approach One of the promising directions of improving the efficiency of handling TFSs while retaining a necessary amount of flexibility is to take up the idea of AMAVL proposed in (Carpenter and Qu, 1995) to design a general programming system based on TFS. LiLFeS is a logic programming system thus designed and developed by our group, based on AMAVLimplementation. LiLFeS can be characterized as follows. • Architecture based on an AMAVL implementation, which compiles a TFS into a sequence of abstract machine instructions, and performs unification of the TFS by emulating the execution of those instructions. Although the proposal of such an A M A V L was already made in 1995, no serious implementation has been reported. We believe that LiLFeS is the first serious treatment of the proposal. • Rich"
C98-2128,E95-1025,0,0.0251135,"y the project of Japan Society for the Promotion of Science (JSPS-RFTF96P00502). I LIFE (AYt-kaci et al., 1994) is also famous, but we do not discuss it because it does not follow Carpenter's TFS definition. Moreover, our separate experiments show that LIFE is more than 10 times slower than emulator-based LiLFeS. As for AMALIA (Wintner, 1997), we cannot make experiments since it is not freely distributed. His experiments in his dissertation shows that AMALIA is 15 time faster than ALE at maximum; it is close to emulator-based LiLFeS, and is outperformed by native-code compiler of LiLFeS. 807 (Erbach, 1995), a TFS-to-Prolog-term compiler. However, as the comparison of these systems with our system (Section 3.2) shows, neither of these two systems is able to achieve the efficiency we established as our goal. Moreover, these two systems have serious disadvantages as a framework for practical applications. The ProFIT approach, for example, tends to consume too much memory for execution. It is also difficult, if not impossible, to combine them with other techniques like parallel parsing, etc., because these two systems have been embedded in Prolog. 1.2 O u r Approach One of the promising directions"
C98-2128,C96-2160,1,0.413816,"• A na'fve parser using a CYK-like algorithm. Although using a simple algorithm, the parser utilizes the full capabilities provided by LiLFeS, such as built-in predicates (TFS copy, array op4 The grammar does not contain semantic analysis such as coreference resolution. 809 Condition: 600 sentences from EDR Japanese corpus (average length 21 words), Average in the parsing of successflflly parsed 539 sentences Environment: DEC Alpha 500/400MHz with 256MB memory Table 2 Parsing Performance Evaluation with a Practical Grammar eration, etc.). • A parser based on the Torisawa's parsing algorithm (Torisawa and Tsujii, 1996). This algorithm compiles an HPSG grammar into 2 parts: its CFG skeletons and a remaining part, and parses a sentence in two phases. Although the parser is not a complete implementation of the algorithm, its efficiency benefits from its 2phase parsing, which reduces the amount of unification. These parsers and grammars are used for the performance evaluations in the next section. 4.2 P e r f o r m a n c e E v a l u a t i o n We evaluated the pertbrmance of the LiLFeS system over three aspects: Parsing performance of LiLFeS, comparison to other TFS systems, and comparison to different Prolog sy"
C98-2128,P98-2144,1,0.686497,"Missing"
C98-2128,J94-2001,0,\N,Missing
C98-2128,C94-1027,0,\N,Missing
C98-2128,C98-2139,1,\N,Missing
C98-2139,J94-4001,0,0.16372,"e. The way how we abandon the t r e a t m e n t of rare linguistic p h e n o m e n a is by introducing additional constraints in feature structures. Regarding (i) and (ii), we introduce &apos;pseudo-principles&apos;, which are unified with ID schemata in the same way principles are unified. Regarding (iii), we add some feature structures to LEs/LETs. 3.1 P o s t p o s i t i o n &apos;Wa&apos; The main usage of the postposition &apos;wa&apos; is divided into the following two patternsS: • If two PPs with the postposition &apos;wa&apos; appear consecutively, we treat the first PP as 5These patterns are almost similar to tile ones in (Kurohashi and Nagao, 1994). (a) (b)* ......... ~. . . . . . . . . . . . . . . . + ...... d>....... ....... lyOtl (c) WX d.d,. &apos; T..... i ÷--+--+ +--+--+ &apos; l .... &|~itX ! • ...... + I 4--4--4 +-+-+ I lq &apos; d.., IX Figure 2: Correct parse tree for Sentence (4) (d)* ......... l ........ ...... i +-+-+ +-+-+ &apos; [ ; ....... i........... *1 +_+_+ +-+-+ ,._ho@,N E} where Figure 1: (a) Correct / (b) incorrect parse tree for Sentence (2); (c) correct / (d) incorrect parse tree for Sentence (3) a complement of a predicate just before the second PP. • Otherwise, P P with the postposition &apos;wa&apos; is treated as the complement of the la"
C98-2139,P98-2132,1,0.726761,"such a nominal suffix and comma are often used adverbially (Sentence (6) & Figure 4(a) ), while general NPs with a comma are used in coordinate structures (Sentence (7) gz Figure 4(b) ). I went to Kyoto andNara. In order to restrict the behavior of NPs with nonfinal time suffixes and commas to adverbial usage only, we added the following constraint to the LE of a comma, constructing a coordinate structure: [ MARKISYNILOCALIN-SUFFIX - ] This prohibits an NP with a nominal suffix fl&apos;om being marked by a comma for coordination. 4 Experiments We implemented our parser and g r a m m a r in LiLFeS (Makino et al., 1998) s, a featurestructure descrit)tion language developed 1).~ our group. We tested randomly selected 10000 sentences from the Japanese E D R corpus (EDR, 1996). The EDR Corl)us is a Japanese version of treebank with morphological, structural, and semantic information. In our experiments, we used only the structural information, that is, parse trees. Both the parse trees in our parser and the parse trees in the E D R Corpus are first converted into bunsetsu dependencies, and they are compared when calculating accuracy. Note that the internal structures of bunsetsus, e.8. structm&apos;es of compound no"
C98-2139,C96-2160,1,0.795733,"s. And grammar (e) using the combination of the three constraints still works with no side effect. We also measured average parsing time per sentence for the original grammar (a) and the fully augmented grammar (e). The parser we adopted is a naive CKY-style parser. Table 3 gives the average parsing time per sentence for those 2 grammars. Pseudo-principles and further constraints on LEs/LETs also make parsing more time-efficient. Even though they are sometimes considered to be slow in practical application because of their heavy feature structures, actually we found them to improve speed. In (Torisawa and Tsujii, 1996), an efficient HPSG parser is proposed, and our preliminary experiments show that the parsing time of the efficient parser is about three times shorter than that of the naive one. Thus, the average parsing time per sentence will be about 300 msec., and we believe our grammar will achive a practical speed. Other techniques to speed-up the parser are proposed in (Makino et al., 1998). 5 Average parsing time per sentence 1277 (msec) 838 (msec) Discussion This section focuses on the behavior of commas. Out of randomly selected 119 errors in experiment (e), 34 errors are considered to have been cau"
C98-2139,C98-2128,1,\N,Missing
C98-2154,A88-1010,0,0.0160053,"Missing"
C98-2154,P98-2132,1,0.905963,"PS-RFTF96P00502). 968 J u n &apos; i c h i t* / ] Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures ( T F S s ) ( C a r p e n t e r , 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers&apos; point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describ"
C98-2154,P98-2144,1,\N,Missing
C98-2154,C98-2139,1,\N,Missing
C98-2154,C98-2128,1,\N,Missing
D07-1033,P04-1056,0,0.0159494,"to “local” features, which only depend on a very small number of labels (usually two: the previous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not We propose a new perceptron algorithm in this paper that can use non-local features along with local features. Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained. For example, Finkel et al. (2005) enabled the use of non-local features by using Gibbs sampling. However, it is unclear how to apply their method of determining the parameters of a non-local model to other types of non-local features, which they did not used. Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming. However, this is equivalent to only allowing non"
D07-1033,P02-1034,0,0.253726,"ctable for the same reason. To deal with this problem, we ﬁrst relaxed our objective. The modiﬁed objective was to ﬁnd a good model from those with the form: {y n } = n-besty Φl (x, y) · α y ′ = argmaxy ∈{y n } Φa (x, y) · α, (1) That is, we ﬁrst generate n-best candidates {y n } under the local model, Φl (x, y) · α. This can be done efﬁciently using the A* algorithm. We then ﬁnd the best scoring candidate under the total model, Φa (x, y)·α, only from these n-best candidates. If n is moderately small, this can also be done in a practical amount of time. This resembles the re-ranking approach (Collins and Duffy, 2002; Collins, 2002b). However, unlike the re-ranking approach, the local model, Φl (x, y) · α, and the total model, Φa (x, y) · α, correlate since they share a part of the vector and are trained at the same time in our algorithm. The re-ranking approach has the disadvantage that it is necessary to use different training corpora for the ﬁrst model and for the second, or to use cross validation type training, to make the training for the second meaningful. This reduces the effective size of training data or increases training time substantially. On the other hand, our algorithm has no such disadvan"
D07-1033,P04-1015,0,0.080534,"local features. Therefore, our objective in this study was to establish a framework, where all 315 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 315–324, Prague, June 2007. 2007 Association for Computational Linguistics types of non-local features are allowed. With non-local features, we cannot use efﬁcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs (Lafferty et al., 2001) and perceptrons (Collins, 2002a). Recently, several methods (Collins and Roark, 2004; Daum´e III and Marcu, 2005; McDonald and Pereira, 2006) have been proposed with similar motivation to ours. These methods alleviate this problem by using some approximation in perceptron-type learning. In this paper, we follow this line of research and try to solve the problem by extending Collins’ perceptron algorithm (Collins, 2002a). We exploited the not-so-familiar fact that we can design a perceptron algorithm with guaranteed convergence if we can ﬁnd at least one wrong labeling candidate even if we cannot perform exact inference. We ﬁrst ran the A* search only using local features to g"
D07-1033,W02-1001,0,0.0818314,"are determined from the sequence and the labels. The weights of local and non-local features are learned together in the training process with guaranteed convergence. We present experimental results from the CoNLL 2003 named entity recognition (NER) task to demonstrate the performance of the proposed algorithm. 1 Introduction Many NLP tasks such as POS tagging and named entity recognition have recently been solved as sequence labeling. Discriminative methods such as Conditional Random Fields (CRFs) (Lafferty et al., 2001), Semi-Markov Random Fields (Sarawagi and Cohen, 2004), and perceptrons (Collins, 2002a) have been popular approaches for sequence labeling because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features. However, the common limitation of these methods is that the features are limited to “local” features, which only depend on a very small number of labels (usually two: the previous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-"
D07-1033,P06-1096,0,0.052969,"Missing"
D07-1033,E06-1011,0,0.352989,"y was to establish a framework, where all 315 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 315–324, Prague, June 2007. 2007 Association for Computational Linguistics types of non-local features are allowed. With non-local features, we cannot use efﬁcient procedures such as forward-backward procedures and the Viterbi algorithm that are required in training CRFs (Lafferty et al., 2001) and perceptrons (Collins, 2002a). Recently, several methods (Collins and Roark, 2004; Daum´e III and Marcu, 2005; McDonald and Pereira, 2006) have been proposed with similar motivation to ours. These methods alleviate this problem by using some approximation in perceptron-type learning. In this paper, we follow this line of research and try to solve the problem by extending Collins’ perceptron algorithm (Collins, 2002a). We exploited the not-so-familiar fact that we can design a perceptron algorithm with guaranteed convergence if we can ﬁnd at least one wrong labeling candidate even if we cannot perform exact inference. We ﬁrst ran the A* search only using local features to generate n-best candidates (this can be efﬁciently perform"
D07-1033,P05-1012,0,0.0453541,"een, the margin approaches at least half of true 317 margin δ (at the cost of inﬁnite training time), as C → ∞. Note that if the features are all local, the secondbest candidate (generally n-best candidates) can also be found efﬁciently by using an A* search that uses the best scores calculated during a Viterbi search as the heuristic estimation (Soong and Huang, 1991). There are other methods for improving robustness by making margin larger for the structural output problem. Such methods include ALMA (Gentile, 2001) used in (Daum´e III and Marcu, 2005)2 , MIRA (Crammer et al., 2006) used in (McDonald et al., 2005), and Max-Margin Markov Networks (Taskar et al., 2003). However, to the best of our knowledge, there has been no prior work that has applied a perceptron with a margin (Krauth and M´ezard, 1987) to structured output.3 Our method described in this section is one of the easiest to implement, while guaranteeing a large margin. We found in the experiments that our method outperformed the Collins’ averaged perceptron by a large margin. 4 Algorithm 4.1 Deﬁnition and Basic Idea Having described the basic perceptron algorithms, we will know explain our algorithm that learns the weights of local and no"
D07-1033,P06-1089,0,0.0217222,"ous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not We propose a new perceptron algorithm in this paper that can use non-local features along with local features. Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained. For example, Finkel et al. (2005) enabled the use of non-local features by using Gibbs sampling. However, it is unclear how to apply their method of determining the parameters of a non-local model to other types of non-local features, which they did not used. Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming. However, this is equivalent to only allowing non-local features whose weights are ﬁxed to negative inﬁnity. Krishnan and Manning (2006) divided the"
D07-1033,W95-0107,0,0.0226383,"ﬂing the training examples.4 However, it is very time consuming to run the complete training process several times. We thus ran the training in only one pass over the shufﬂed examples several times, and used the averaged output weight vectors as a new initial weight vector, because we thought that the early part of training would be more seriously affected by the order of examples. We call this “BPM initialization”. 5 5 Named Entity Recognition and Non-Local Features We evaluated the performance of the proposed algorithm using the named entity recognition task. We adopted IOB (IOB2) labeling (Ramshaw and Marcus, 1995), where the ﬁrst word of an entity of class “C” is labeled “B-C”, the words in the entity are labeled “I-C”, and other words are labeled “O”. We used non-local features based on Finkel et al. (2005). These features are based on observations such as “same phrases in a document tend to have the same entity class” (phrase consistency) and “a sub-phrase of a phrase tends to have the same entity class as the phrase” (sub-phrase consistency). We also implemented the “majority” version of these features as used in Krishnan and Manning (2006). In addition, we used non-local features, which are based o"
D07-1033,P05-1045,0,0.221453,"only depend on a very small number of labels (usually two: the previous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not We propose a new perceptron algorithm in this paper that can use non-local features along with local features. Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained. For example, Finkel et al. (2005) enabled the use of non-local features by using Gibbs sampling. However, it is unclear how to apply their method of determining the parameters of a non-local model to other types of non-local features, which they did not used. Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming. However, this is equivalent to only allowing non-local features whose"
D07-1033,P06-1141,0,0.350627,"bels (usually two: the previous and the current). Although this limitation makes training and inference tractable, it also excludes the use of possibly useful “non-local” features that are accessible after all labels are determined. For example, non-local features such as “same phrases in a document do not We propose a new perceptron algorithm in this paper that can use non-local features along with local features. Although several methods have already been proposed to incorporate non-local features (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005; Roth and Yih, 2005; Krishnan and Manning, 2006; Nakagawa and Matsumoto, 2006), these present a problem that the types of non-local features are somewhat constrained. For example, Finkel et al. (2005) enabled the use of non-local features by using Gibbs sampling. However, it is unclear how to apply their method of determining the parameters of a non-local model to other types of non-local features, which they did not used. Roth and Yih (2005) enabled the use of hard constraints on labels by using integer linear programming. However, this is equivalent to only allowing non-local features whose weights are ﬁxed to negative inﬁnity. Krishnan"
D07-1033,W03-0419,0,0.0443427,"Missing"
D07-1033,P02-1062,0,\N,Missing
D07-1073,E06-1002,0,0.397121,"Missing"
D07-1073,W02-1028,0,0.0347985,". These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER. 1 Introduction It has been known that Gazetteers, or entity dictionaries, are important for improving the performance of named entity recognition. However, building and maintaining high-quality gazetteers is very time consuming. Many methods have been proposed for solving this problem by automatically extracting gazetteers from large amounts of texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). However, these methods require complicated induction of patterns or statistical methods to extract high-quality gazetteers. We have recently seen a rapid and successful growth of Wikipedia (http://www.wikipedia.org), which is an open, collaborative encyclopedia on the Web. Wikipedia has now more than 1,700,000 articles on the English version (March 2007) and the number is still increasing. Since Wikipedia aims to be an encyclopedia, most articles are about named entities and they are more structured tha"
D07-1073,W03-0419,0,0.0793043,"Missing"
D07-1073,W06-2809,0,0.267886,"Missing"
D07-1073,D07-1033,1,0.327366,"ost $17,000”, as: RareO JimiB-guitarist HendrixI-guitarist songO draftO forO almostO $17,000O .O Note that we adopted the leftmost longest match if there were several possible matchings. These IOB2 tags were used in the same way as other features 7 http://www.cs.utah.edu/˜hal/TagChunk/ We use bare “B”, “I”, and “O” tags if we want to represent only the matching information. 8 701 4 Experiments In this section, we demonstrate the usefulness of the extracted category labels for NER. 4.1 Data and setting 9 We used sentence concatenation because we found it improves the accuracy in another study (Kazama and Torisawa, 2007). 10 http://www.cs.utah.edu/˜hal/TagChunk/ 11 This is not because TagChunk overﬁts the CoNLL 2003 dataset (TagChunk is trained on the Penn Treebank (Wall Street Journal), while the CoNLL 2003 data are taken from the Reuters corpus). Table 1: Baseline features. The value of a node feature is determined from the current label, y0 , and a surface feature determined only from x. The value of an edge feature is determined by the previous label, y−1 , the current label, y0 , and a surface feature. Used surface features are the word (w), the downcased word (wl), the POS tag (pos), the chunk tag (chk)"
D07-1073,N06-1025,0,0.0722441,"rce, extracting knowledge such as gazetteers from Wikipedia will be much easier than from raw texts or from usual Web texts because of its structure. It is also important that Wikipedia is updated every day and therefore new named entities are added constantly. We think that extracting knowledge from Wikipedia for natural language processing is one of the promising ways towards enabling large-scale, real-life applications. In fact, many studies that try to exploit Wikipedia as a knowledge source have recently emerged (Bunescu and Pas¸ca, 2006; Toral and Mu˜noz, 2006; Ruiz-Casado et al., 2006; Ponzetto and Strube, 2006; Strube and Ponzetto, 2006; Zesch et al., 2007). We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER). Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the ﬁrst sentence of the entry, which can be thought of as a deﬁnition part. These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER. 1 Introduction It has been known"
D07-1073,sekine-etal-2002-extended,0,0.172569,"Missing"
D07-1073,W06-2919,0,0.0298674,"We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER. 1 Introduction It has been known that Gazetteers, or entity dictionaries, are important for improving the performance of named entity recognition. However, building and maintaining high-quality gazetteers is very time consuming. Many methods have been proposed for solving this problem by automatically extracting gazetteers from large amounts of texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). However, these methods require complicated induction of patterns or statistical methods to extract high-quality gazetteers. We have recently seen a rapid and successful growth of Wikipedia (http://www.wikipedia.org), which is an open, collaborative encyclopedia on the Web. Wikipedia has now more than 1,700,000 articles on the English version (March 2007) and the number is still increasing. Since Wikipedia aims to be an encyclopedia, most articles are about named entities and they are more structured than raw As a ﬁrst step towards such approach, we demonstrate in this p"
D09-1060,W08-2102,0,0.197319,"orating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et al. (2008) uses both sibling and grandparent interactions, and uses cluster-based features. Carreras et al. (2008) reported a very high accuracy using information of constituent structure of the TAG grammar formalism. In our systems, we did not use such knowledge. Our subtree-based features could be combined 4.1.1 Main results of English dat"
D09-1060,N06-1021,0,0.0749069,"Missing"
D09-1060,A00-1031,0,0.0251006,"or testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. And we also evaluated on complete dependency analysis. The results are shown in Table 1, where Ord1/Ord2 refers to a first-/second-order MSTParser with basic features, Ord1s/Ord2s refers to a first-/second-order MSTParser with basic+subtree-based features, and the improvements by the subtree-based features over the basic features are"
D09-1060,W06-2920,0,0.0596271,"features for the parsing models. 3.1 Subtrees extraction To ease explanation, we transform the dependency structure into a more tree-like structure as shown in Figure 2, the sentence is the same as the one in Figure 1. ROOT ROOT I ate the fish with a fork ate . Figure 1: Example for dependency structure I fish with . the fork 2.1 Parsing approach For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008): graph-based model and transition-based model, which achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). Our subtree-based features can be applied in both of the two parsing models. In this paper, as the base parsing system, we employ the graph-based MST parsing model proposed by McDonald et al. (2005) and McDonald and Pereira (2006), which uses the idea of Maximum Spanning Trees of a graph and large margin structured learning algorithms. The details a I ate the fish with a fork . Figure 2: Example for dependency structure in tree-format Our task is to extract subtrees from dependency trees. If a subtree contains two nodes, we call it a bigram-subtree. If a subtree contains"
D09-1060,D07-1101,0,0.315431,"forms of heads. Specifically, for any feature related to word form, we remove this feature if the word is not one of the Top-N most frequent words in the training data. We used N=1000 for the experiments in this paper. This method can reduce the size of the feature sets. In this paper, we only used bigram-subtrees and the limited form of trigram-subtrees, though in theory we can use k-gram-subtrees, which are limited in the same way as our trigram subtrees, in (k-1)th-order MST parsing models mentioned in McDonald and Pereira (2006) or use grandparenttype trigram-subtrees in parsing models of Carreras (2007). Although the higher-order MST parsing models will be slow with exact inference, requiring O(nk ) time (McDonald and Pereira, 2006), it might be possible to use higher-order kgram subtrees with approximated parsing model in the future. Of course, our method can also be easily extended to the labeled dependency case. d+1 … (a) …h … d1 … d2 … (b) Figure 4: Word pairs and triple for feature representation in Figure 5, where h is “ate” and d is “with”. We can generate the features for the pairs linked by dashed-lines, such as h − d, h − d+1 and so on. Then we have the temporary bigram-subtrees “a"
D09-1060,I08-1012,1,0.908673,"he BLLIP corpus. For Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 8 (CTB) version 4.09 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of token"
D09-1060,P06-2041,0,0.0127447,"r MST parsing models. For baseline systems, we used the first- and second-order basic features, which were the same as the features used by McDonald and Pereira (2006), and we used the default settings of MSTParser throughout the paper: iters=10; training-k=1; decode-type=proj. We implemented our systems based on the MSTParser by incorporating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et a"
D09-1060,C08-1054,0,0.0118978,"rds because their numbers were very small. The Better and Worse curves showed that our approach always provided better results. The results indicated that the improvements apparently became larger when the sentences had more unknown words for the Chinese data. And for the English data, the graph also showed the similar trend, although the improvements for the sentences have three and four unknown words were slightly less than the others. 4.2.2 Coordinating conjunctions We analyzed our new parsers’ behavior for coordinating conjunction structures, which is a very difficult problem for parsing (Kawahara and Kurohashi, 2008). Here, we compared the Ord2 system with the Ord2s system. Figures 9 and 10 show how the subtree-based features affect accuracy as a function of the number of conjunctions, where the x axis refers to the number of conjunctions in one sentence and the y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved the coordinating conjunction problem. In the trigram-subtree list, many subtrees are related to coordinating conjunctions, such as “utilities:1:3 and:2:3 businesses:3:0” and “pull:1:0 and:2:1 protect:3:1”. These subtrees can provide ad"
D09-1060,P08-1068,0,0.550631,"ive idea to improve dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570–579, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP glish and Chinese data. We show that this simple approach greatly improves the accuracy and that the use of richer structures (i.e, word triples) indeed gives additional improvement. We also demonstrate that our approach and other improvement techniques (Koo et al., 2008; Nivre and McDonald, 2008) are complementary and that we can achieve very high accuracies when we combine our method with other improvement techniques. Specifically, we achieve the best accuracy for the Chinese data. The rest of this paper is as follows: Section 2 introduces the background of dependency parsing. Section 3 proposes an approach for extracting subtrees and represents the subtree-based features for dependency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing"
D09-1060,J93-2004,0,0.0327919,"m-subtree is formed by the word forms of h, d1, and d2. Then we retrieve the subtree in Lst to get its set ID. In addition, we consider the triples of “h-NULL”6 , d1, and d2, which means that we only check the words of sibling nodes without checking the head word. Then, we generate second-order subtree-based features, consisting of indicator functions for set IDs of the retrieved trigram-subtrees. 6 h-NULL is a dummy token 573 In order to evaluate the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for"
D09-1060,P06-1043,0,0.179445,"Missing"
D09-1060,D08-1059,0,0.606476,"d Pereira (2006), and we used the default settings of MSTParser throughout the paper: iters=10; training-k=1; decode-type=proj. We implemented our systems based on the MSTParser by incorporating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et al. (2008) uses both sibling and grandparent interactions, and uses cluster-based features. Carreras et al. (2008) reported a very high accuracy using infor"
D09-1060,E06-1011,0,0.206252,"explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent relations between the words in a sentence. A simple example is shown in Figure 1, where an arc between two words indicates a dependency relation between them. For example, the arc between “ate” and “fish” indicates that “ate” is the head of “fish” and “fish” is the dependent. The arc between “ROOT” and “ate” indicates that “ate” is the ROOT of the sentence. of parsing model were presented in McDonald et al. (2005) and McDonald and Pereira (2006). 2.2 Baseline Parser In the MST parsing model, there are two well-used modes: the first-order and the second-order. The first-order model uses first-order features that are defined over single graph edges and the secondorder model adds second-order features that are defined on adjacent edges. For the parsing of unannotated data, we use the first-order MST parsing model, because we need to parse a large number of sentences and the parser must be fast. We call this parser the Baseline Parser. 3 Our approach In this section, we describe our approach of extracting subtrees from unannotated data."
D09-1060,W08-2127,0,0.0175142,"cy parsing results for English, for our parsers and previous work To demonstrate that our approach and other work are complementary, we thus implemented a system using all the techniques we had at hand that used subtree- and cluster-based features and applied the integrating method of Nivre and McDonald (2008). We used the word clustering tool12 , which was used by Koo et al. (2008), to produce word clusters on the BLLIP corpus. The cluster-based features were the same as the features used by Koo et al. (2008). For the integrating method, we used the transition MaxEnt-based parser of Zhao and Kit (2008) because it was faster than the MaltParser. The results are shown in the bottom part of Table 2, where Ord1c/Ord2c refers to a first-/second-order MSTParser with cluster-based features, Ord1i/Ordli refers to a first/second-order MSTParser with integrating-based features, Ord1sc/Ord2sc refers to a first-/secondorder MSTParser with subtree-based+clusterbased features, and Ord1sci/Ord2sci refers to a first-/second-order MSTParser with subtreebased+cluster-based+integrating-based features. Ord1c/Ord2c was worse than KOO08-dep1c/dep2c, but Ord1sci outperformed KOO08-dep1c Chinese UAS 86.38 87.68(+1"
D09-1060,P05-1012,0,0.843284,"endency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent relations between the words in a sentence. A simple example is shown in Figure 1, where an arc between two words indicates a dependency relation between them. For example, the arc between “ate” and “fish” indicates that “ate” is the head of “fish” and “fish” is the dependent. The arc between “ROOT” and “ate” indicates that “ate” is the ROOT of the sentence. of parsing model were presented in McDonald et al. (2005) and McDonald and Pereira (2006). 2.2 Baseline Parser In the MST parsing model, there are two well-used modes: the first-order and the second-order. The first-order model uses first-order features that are defined over single graph edges and the secondorder model adds second-order features that are defined on adjacent edges. For the parsing of unannotated data, we use the first-order MST parsing model, because we need to parse a large number of sentences and the parser must be fast. We call this parser the Baseline Parser. 3 Our approach In this section, we describe our approach of extracting"
D09-1060,2006.iwslt-evaluation.9,0,0.0600461,"onstrate the effectiveness of our proposed approach, we present the experimental results on the English Penn Treebank and the Chinese Penn Treebank. These results show that our approach significantly outperforms baseline systems. And, it achieves the best accuracy for the Chinese data and an accuracy which is competitive with the best known systems for the English data. 1 Introduction Dependency parsing, which attempts to build dependency links between words in a sentence, has experienced a surge of interest in recent times, owing to its usefulness in such applications as machine translation (Nakazawa et al., 2006) and question answering (Cui et al., 2005). To obtain dependency parsers with high accuracy, supervised techniques require a large amount of handannotated data. While hand-annotated data are very expensive, large-scale unannotated data can be obtained easily. Therefore, the use of largescale unannotated data in training is an attractive idea to improve dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Langu"
D09-1060,P08-1108,0,0.351178,"e dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570–579, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP glish and Chinese data. We show that this simple approach greatly improves the accuracy and that the use of richer structures (i.e, word triples) indeed gives additional improvement. We also demonstrate that our approach and other improvement techniques (Koo et al., 2008; Nivre and McDonald, 2008) are complementary and that we can achieve very high accuracies when we combine our method with other improvement techniques. Specifically, we achieve the best accuracy for the Chinese data. The rest of this paper is as follows: Section 2 introduces the background of dependency parsing. Section 3 proposes an approach for extracting subtrees and represents the subtree-based features for dependency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent rel"
D09-1060,H94-1048,0,0.0447416,"the x axis refers to the number of conjunctions in one sentence and the y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved the coordinating conjunction problem. In the trigram-subtree list, many subtrees are related to coordinating conjunctions, such as “utilities:1:3 and:2:3 businesses:3:0” and “pull:1:0 and:2:1 protect:3:1”. These subtrees can provide additional information for parsing models. 4.2.3 PP attachment We analyzed our new parsers’ behavior for preposition-phrase attachment, which is also a difficult task for parsing (Ratnaparkhi et al., 1994). We compared the Ord2 system with the Ord2s system. Figures 11 and 12 show how the subtreebased features affect accuracy as a function of the number of prepositions, where the x axis refers to the number of prepositions in one sentence and the 577 y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved preposition-phrase attachment. 5 Related work Our approach is to incorporate unannotated data into parsing models for dependency parsing. Several previous studies relevant to our approach have been conducted. Chen et al. (2008) previousl"
D09-1060,W96-0213,0,0.611923,"te the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.8 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Basic Parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees"
D09-1060,D07-1111,0,0.0905789,"Missing"
D09-1060,E03-1008,0,0.0906958,"Missing"
D09-1060,W07-2201,0,0.202814,"Missing"
D09-1060,W03-3023,0,0.921881,"f “h-NULL”6 , d1, and d2, which means that we only check the words of sibling nodes without checking the head word. Then, we generate second-order subtree-based features, consisting of indicator functions for set IDs of the retrieved trigram-subtrees. 6 h-NULL is a dummy token 573 In order to evaluate the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.8 We used the"
D09-1060,C08-1132,0,0.5151,"r Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 8 (CTB) version 4.09 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all p"
D09-1060,D07-1096,0,\N,Missing
D09-1069,P02-1051,0,0.0149479,"nt of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP , JCP }, x1 = x2 , and x1 , x2 ∈ X = {EG , EP , EGP }. In this paper, we define three types of hybrid transliteration models in each class: M (EG + EP , y, α), M (EG + EGP , y, α), and M (EP + EGP , y, α). PM (EG ,φ) (CG |EG ) = P (CG |EG ) (7) PM (x1 +x2 ,y,α) (CG |EG ) PM (EP ,φ) (CG |EG )  P (EP |EG ) × P (CG |EP ) = (8) = α × PM (x1 ,y) (CG |EG ) + (1 − α) × PM (x2 ,y) (CG |EG ) ∀EP PM (EGP ,φ) (CG |EG"
D09-1069,W97-0301,0,0.0345717,"iple1 (egi , cpi , cgi−1 ) egi = “r”, cpi−1 = “GE”, cgi−1 = “ò:B” cgi = “/:B” f6 triple2 (egi−1 , cgi−1 , cpi−1 ) egi−1 = “g”, cpii−1 = “GE L”, cgi−1 = “ò:B” cgi = “/:B” they can be simplified into a series of products in Eqs. (11)–(13). The maximum entropy model is used to estimate the probabilities in Eqs. (11)–(13) (Berger et al., 1996). Generally, a conditional maximum entropy model is an exponential model that gives the conditional probability, as described in Eq. (14), where λi is the parameter to be estimated and fi (a, b) is a feature function corresponding to λi (Berger et al., 1996; Ratnaparkhi, 1997):  i+k P (EP |EG ) ≈ P (epi |epi−1 (11) i−k , egi−k ) Table 4: Context predicates and their descriptions Category N- GRAM PAIR T RIPLE Context predicates gram1 (uj ) gram2 (uj ) gram3 (uj ) pair11 (uj , vk ) pair12 (uj , vk ) pair22 (uj , vk ) triple1 (uj , vk , wl ) triple2 (uj , vk , wl ) Description uj uj+1 j uj+2 j u j , vk uj , vkk+1 k+1 uj+1 j , vk u j , vk , wl uj , vk , wll+1 i P (CP |EG , EP )  i+k P (cpi |cpi−1 ≈ i−k , eg, epi−k ) (12) P (CG |EG , EP , CP )  i−1 P (cgi |cgi−k , eg, ep, cpi+k ≈ i−k ) (13) (e.g., pair12 (uj , uj+1 ) = trigram(uj ) = uj+2 j ). Table 3 represents"
D09-1069,J96-1002,0,0.0696923,"n Table 2, where i = 2 i+2 f1 gram3 (egi ) egi = “ree” cgi = “/:B” cpi−1 = “G”, cgi−1 = “ò:B” cgi = “/:B” f2 pair11 (cpi−1 , cgi−1 ) i f3 pair12 (cgi−1 , cpi−1 ) cpi−1 = “GE L”, cgi−1 = “ò:B” cgi = “/:B” i egi−1 = “gr”, epii−1 = “G R” cgi = “/:B” f4 pair22 (cpi−1 , cgi−2 ) f5 triple1 (egi , cpi , cgi−1 ) egi = “r”, cpi−1 = “GE”, cgi−1 = “ò:B” cgi = “/:B” f6 triple2 (egi−1 , cgi−1 , cpi−1 ) egi−1 = “g”, cpii−1 = “GE L”, cgi−1 = “ò:B” cgi = “/:B” they can be simplified into a series of products in Eqs. (11)–(13). The maximum entropy model is used to estimate the probabilities in Eqs. (11)–(13) (Berger et al., 1996). Generally, a conditional maximum entropy model is an exponential model that gives the conditional probability, as described in Eq. (14), where λi is the parameter to be estimated and fi (a, b) is a feature function corresponding to λi (Berger et al., 1996; Ratnaparkhi, 1997):  i+k P (EP |EG ) ≈ P (epi |epi−1 (11) i−k , egi−k ) Table 4: Context predicates and their descriptions Category N- GRAM PAIR T RIPLE Context predicates gram1 (uj ) gram2 (uj ) gram3 (uj ) pair11 (uj , vk ) pair12 (uj , vk ) pair22 (uj , vk ) triple1 (uj , vk , wl ) triple2 (uj , vk , wl ) Description uj uj+1 j uj+2 j u"
D09-1069,N09-1034,0,0.0434359,"Missing"
D09-1069,P07-1119,0,0.0524423,"ee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ , except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG |CP ) in Eqs. (4)–(6). els are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP , JCP }, x1 = x2 , and x1 , x2 ∈ X = {EG , EP , EGP }. In"
D09-1069,W03-1508,0,0.493219,"mes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 1 Introduction 1.1 Motivation Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of Englishto-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007). Inspired by the success of English grapheme-tophoneme research in speech synthesis, many researchers have proposed phoneme-based English1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chinese phoneme-to-grapheme conversion. However, the simple use of Chinese phonemes doesn"
D09-1069,P98-2220,0,0.760576,"int use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 1 Introduction 1.1 Motivation Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of Englishto-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007). Inspired by the success of English grapheme-tophoneme research in speech synthesis, many researchers have proposed phoneme-based English1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chinese phoneme-to-grapheme conversion. However, the simple u"
D09-1069,P07-1082,0,0.0300719,"al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ , except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG |CP ) in Eqs. (4)–(6). els are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP ,"
D09-1069,W03-0317,0,0.251343,"onventional models. Our proposed model relies on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 1 Introduction 1.1 Motivation Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of Englishto-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007). Inspired by the success of English grapheme-tophoneme research in speech synthesis, many researchers have proposed phoneme-based English1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chi"
D09-1069,P07-1015,0,0.0218881,"d Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ , except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG |CP ) in Eqs. (4)–(6). els are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP , JCP }, x1 = x2 , and x1 , x2 ∈ X = {EG , EP , EGP }. In this paper, we defi"
D09-1069,P04-1021,0,0.192847,".jp Abstract to-Chinese transliteration models. In these approaches, Chinese phonemes are generated from English graphemes or phonemes, and then the Chinese phonemes are converted into Chinese graphemes (or characters), where Chinese Pinyin strings1 are used for representing a syllable-level Chinese phoneme sequence. Despite its high accuracy in generating Chinese phonemes from English, this approach has severely suffered from errors in Chinese phoneme-to-grapheme conversion, mainly caused by Chinese homophone confusion – one Chinese Pinyin string can correspond to several Chinese characters (Li et al., 2004). For example, the Pinyin string “LI” corresponds to such different Chinese characters as (, :, and /. For this reason, it has been reported that English-toChinese transliteration without Chinese phonemes outperforms that with Chinese phonemes (Li et al., 2004). Then “Can Chinese phonemes improve English-to-Chinese transliteration, if we can reduce the errors in Chinese phoneme-to-grapheme conversion?” Our research starts from this question. Inspired by the success of English grapheme-to-phoneme research in speech synthesis, many researchers have proposed phoneme-based English-to-Chinese trans"
D09-1069,P07-1016,0,0.439781,"relies on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 1 Introduction 1.1 Motivation Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of Englishto-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007). Inspired by the success of English grapheme-tophoneme research in speech synthesis, many researchers have proposed phoneme-based English1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chinese phoneme-to-grapheme conversio"
D09-1069,P06-1143,0,0.0302869,"., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ , except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG |CP ) in Eqs. (4)–(6). els are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP , JCP }, x1 ="
D09-1069,H89-2027,0,\N,Missing
D09-1069,C98-2215,0,\N,Missing
D09-1069,W09-3502,0,\N,Missing
D09-1069,Y06-1050,0,\N,Missing
D09-1097,bond-etal-2008-boot,1,0.581211,"Missing"
D09-1097,P99-1016,0,0.0133585,"ctic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities or hierarchical structures in HTML documents. Etzioni et Documents hypernym ..… word The same hypernym is selected for all words in a class. Pattern-based method (Hearst 1992, Pantel et al. 2004a, Ando et al. 2003, Snow et al. 2005, Snow et al. 2006, and Etzioni et al. 2005) word word word word word Word Class Clustering-based method (Caraballo 1999, Pantel et al. 2004b, Shinzato et al. 2004, and Etzioni e"
D09-1097,C92-2082,0,0.762187,"pproach. duce some related works in Section 2. Section 3 describes the Wikipedia relation database. Section 4 describes the distributional similarity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Corpus/documents hypernym such as word Co-occurrences in a pattern are needed Related Works Most previous researchers have relied on lexico-syntactic patterns for hyponymy acquisition. Lexico-syntactic patterns were first used by Hearst (1992). The patterns used by her included “NP0 such as NP1,” in which NP0 is a hypernym of NP1. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method wa"
D09-1097,P08-1047,1,0.864076,"Missing"
D09-1097,P99-1004,0,0.0542805,"etween v and n. In Japanese, a relation rel is represented by postpositions attached to n and the phrase composed of n and rel modifies v. Each triple is divided into two parts. The first is &lt;v, rel&gt; and the second is n. Then, we consider the conditional probability of occurrence of the pair &lt;v, rel&gt;: P(&lt;v, rel&gt;|n). P(&lt;v, rel&gt;|n) can be regarded as the distribution of the grammatical contexts of the noun phrase n. The distributional similarity can be defined as the distance between these distributions. There are several kinds of functions for evaluating the distance between two distributions (Lee 1999). Our method uses the Jensen-Shannon divergence. The Jensen-Shannon divergence between two probability distributions, P (⋅ |n1 ) and P (⋅ |n2 ) , can be calculated as follows: DJS ( P (⋅ |n1 ) ||P (⋅ |n2 )) P (⋅ |n1 ) + P (⋅ |n2 ) 1 ) ( DKL ( P (⋅ |n1 ) || 2 2 P (⋅ |n1 ) + P (⋅ |n2 ) + DKL ( P (⋅ |n2 ) || )), 2 = &lt;v ,rel &gt;∈D if f (&lt; v, rel , n &gt;) &gt; 0, where f(&lt;v, rel, n&gt;) is the frequency of a triple &lt;v, rel, n&gt; and D is the set defined as { &lt;v, rel &gt; | f(&lt;v, rel, n&gt;) &gt; 0 }. In the case of f(&lt;v, rel, n&gt;) = 0, P(&lt;v, rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the"
D09-1097,P06-1101,0,0.0472608,"P1,” in which NP0 is a hypernym of NP1. Using these patterns as seeds, Hearst discovered new patterns by which to semiautomatically extract hyponymy relations. Pantel et al. (2004a) proposed a method to automatically discover the patterns using a minimal edit distance. Ando et al. (2003) applied predefined lexico-syntactic patterns to Japanese news articles. Snow et al. (2005) generalized these lexicosyntactic pattern-based methods by using dependency path features for machine learning. Then, they extended the framework such that this method was capable of making use of heterogenous evidence (Snow et al. 2006). These pattern-based methods require the co-occurrences of a target word and the hypernym in a document. It should be noted that the requirement of such cooccurrences actually poses a problem when we extract a large set of hyponymy relations since they are not frequently observed (Shinzato et al. 2004, Pantel et al. 2004b). Clustering-based methods have been proposed as another approach. Caraballo (1999), Pantel et al. (2004b), and Shinzato et al. (2004) proposed a method to find a common hypernym for word classes, which are automatically constructed using some measures of word similarities o"
D09-1097,sumida-etal-2008-boosting,1,0.937771,"aper, we use the term “word” for both “a single-word word” and “a multi-word word.” 929 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929–937, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP A hypernym is selected for each word independently. Wikipedia relation database Wikipedia Hyponymy relations are extracted using the layout information of Wikipedia. hypernym : Selected from hypernyms in the Wikipedia relation database. No direct co-occurrences of hypernym and hyponym in corpora are needed. Wikipedia-based approach (Ponzetto et al. 2007 and Sumida et al. 2008) Target word: Selected from the Web : word k similar words Figure 1: Overview of the proposed approach. duce some related works in Section 2. Section 3 describes the Wikipedia relation database. Section 4 describes the distributional similarity calculated by the two methods. In Section 5, we describe a method to discover an appropriate hypernym for each target word. The experimental results are presented in Section 6 before concluding the paper in Section 7. 2 Corpus/documents hypernym such as word Co-occurrences in a pattern are needed Related Works Most previous researchers have relied on le"
D09-1097,N04-1041,0,0.0180963,"Missing"
D09-1097,P99-1014,0,0.0117575,"rel&gt;|n) is set to 0. Instead of using the observed frequency directly as in the usual maximum likelihood estimation, we modified it as above. Although this might seems strange, this kind of modification is common in information retrieval as a term weighing method (Manning et al. 1999) and it is also applied in some studies to yield better word similarities (Terada et al. 2006, Kazama et al. 2009). We also adopted this idea in this study. 4.2 P(⋅ |n1 ) . P(⋅ |n2 ) Finally, the distributional similarity between two words, n1 and n2, is defined as follows: Distributional Similarity Based on CVD Rooth et al. (1999) and Torisawa (2001) showed that EM-based clustering using verb-noun dependencies can produce semantically clean noun clusters. We exploit these EM-based clustering results as the smoothed contexts for noun n. In Torisawa’s model (2001), the probability of occurrence of the triple &lt;v, rel, n&gt; is defined as follows: P(&lt; v, rel, n &gt;) where DKL indicates the Kullback-Leibler divergence and is defined as follows: DKL ( P(⋅ |n1 ) ||P(⋅ |n2 ))= ∑ P(⋅ |n1 ) log log( f (&lt; v, rel , n &gt;)) + 1 ∑ log( f (&lt; v, rel , n &gt;) + 1 =def ∑a∈A P(&lt; v, rel &gt; |a) P(n |a) P(a), where a denotes a hidden class of &lt;v,rel&gt;"
D09-1097,N04-1010,1,0.869582,"Missing"
D09-1097,shinzato-etal-2008-large,0,\N,Missing
D09-1122,D07-1017,0,\N,Missing
D09-1122,W03-1608,0,\N,Missing
D09-1122,C08-1107,0,\N,Missing
D09-1122,N06-1007,0,\N,Missing
D09-1122,N06-1008,1,\N,Missing
D09-1122,N06-1023,0,\N,Missing
D09-1122,W03-1011,0,\N,Missing
D09-1122,C08-1029,0,\N,Missing
D09-1122,N03-1003,0,\N,Missing
D09-1122,P05-1014,0,\N,Missing
D09-1122,P05-1074,0,\N,Missing
D09-1122,P98-2127,0,\N,Missing
D09-1122,C98-2122,0,\N,Missing
D09-1122,P06-1107,0,\N,Missing
D09-1122,kawahara-kurohashi-2006-case,0,\N,Missing
D09-1122,W04-3206,0,\N,Missing
D11-1007,D08-1092,0,0.318764,"design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual tre"
D11-1007,D07-1101,0,0.144195,"sponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x an"
D11-1007,D09-1060,1,0.927841,"list of the target monolingual subtrees 1 For the second order features, Dir is the combination of the directions of two dependencies. or bilingual subtrees, this constraint will probably be reliable. We first parse the large-scale unannotated monolingual and bilingual data. Subsequently, we extract the monolingual and bilingual subtrees from the parsed data. We then verify the bilingual constraints using the extracted subtrees. Finally, we generate the bilingual features based on the verified results for the parsing models. 5.1 Verified constraint functions 5.1.1 Monolingual target subtrees Chen et al. (2009) proposed a simple method to extract subtrees from large-scale monolingual data and used them as features to improve monolingual parsing. Following their method, we parse large unannotated data with the Parsert and obtain the subtree list (STt ) on the target side. We extract two types of subtrees: bigram (two words) subtree and trigram (three words) subtree. 5.1.2 Verified target constraint function: Fvt (rtk ) We use the extracted target subtrees to verify the rtk of the bilingual constraints. In fact, rtk is a candidate subtree. If the rtk is included in STt , function Fvt (rtk ) = T ype(rt"
D11-1007,P10-1003,1,0.243719,"based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both si"
D11-1007,P07-1003,0,0.126522,"ale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that"
D11-1007,D09-1127,0,0.0657088,"Missing"
D11-1007,N03-1017,0,0.0160094,"Missing"
D11-1007,P10-1001,0,0.0121108,"ley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that we have a (candidate) dependency relation rs that can be a bigram or trigram dependency. We examine whether the corresponding words of the source words of rs have a dependency relation rt in the target trees. We also consider the direction of the dependency relation. The corresponding word of the head should also be the head in rt . We define a binary function for this bilingual constraint: Fbn (rsn : rtk ), where n and k refers to the types of the dependencies (2 for bigram and 3 for trigram). For example, in rs2 : rt3 , rs2 is a bigram dependency on the sour"
D11-1007,P09-1058,1,0.820133,"//www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same procedure as we did for the BLLIP corpus. Word alignment was performed using the Berkeley Aligner. We reported the parser quality by the UAS, i.e., the percentage of tokens (excluding all punctuation tokens) with correct HEADs. 6.1 Experimental settings For baseline systems, we used the monolingual features mentioned in Section 3. We called these features basic features. To compare the results of (Burk"
D11-1007,N06-1014,0,0.079112,"ed by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a fu"
D11-1007,P10-5002,0,0.0237031,"able is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both sides. Huang et al. (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. It uses another type of bilingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen"
D11-1007,J93-2004,0,0.0430468,"ingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen et al. (2010) also used bilingual treebanks and made use of tree structures on the target side. However, the bilingual treebanks are hard to obtain, partly because of the high cost of human translation. Thus, in their experiments, they applied their methods to a small data set, the manually translated portion of the Chinese Treebank (CTB) which contains only about 3,000 sentences. On the other hand, many large-scale monolingual treebanks exist, such as the Penn English Treebank (PTB) (Marcus et al., 1993) (about 40,000 sentences in Version 3) and the latest version of CTB (over 50,000 sentences in Version 7). In this paper, we propose a bitext parsing approach in which we produce the bilingual constraints on existing monolingual treebanks with the help of SMT systems. In other words, we aim to improve source-language parsing with the help of automatic translations. In our approach, we first use an SMT system to translate the sentences of a source monolingual treebank into the target language. Then, the target sentences are parsed by a parser trained on a target monolingual treebank. We then ob"
D11-1007,E06-1011,0,0.17547,"ure, the word “skills” corresponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the in"
D11-1007,W03-3017,0,0.0373708,"monstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Si"
D11-1007,J03-1002,0,0.00306085,"ased on the bilingual constraints verified by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order"
D11-1007,W96-0213,0,0.0608525,"e trained first-order and second-order Parsert on the training data. The unlabeled attachment score (UAS) of the second-order Parsert was 91.92, indicating state-of-the-art accuracy on the test data. We used the second-order Parsert to parse the autotranslated/human-made target sentences in the CTB 3 http://www.statmt.org/moses/ http://www.speech.sri.com/projects/srilm/download.html 5 http://www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same proced"
D11-1007,W04-3207,0,0.0134497,"ify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their"
D11-1007,W03-3023,0,0.0472776,"his is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Singer, 2003) during training."
D11-1007,P09-1007,0,0.0415887,"for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree s"
D11-1076,P08-1004,0,0.251978,"5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alleviates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of r"
D11-1076,P99-1008,0,0.0342991,"t in combination they do give a marked improvement over the base features, at least for some relations like causation and material. In other words, the main contribution of semantic word classes and partial patterns to our method’s performance lies not in the final classification step but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alleviates pattern sparseness by using infix patterns that are generalized using classes of distri"
D11-1076,H05-1091,0,0.0559931,"ng infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only rela"
D11-1076,N06-1038,0,0.0282956,"ses of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the"
D11-1076,P04-1054,0,0.00902849,"arseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e.,"
D11-1076,P07-1088,0,0.0220274,"but seems to occur at earlier stages of the process, in the candidate and training data generation steps. 5 Related Work Using lexico-syntactic patterns to extract semantic relations was first explored by Hearst (Hearst, 1992), and has inspired a large body of work on semisupervised relation acquisition methods (Berland and Charniak, 1999; Agichtein and Gravano, 2000; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009), two of which were used in this work. Some researchers have addressed the sparseness problems inherent in pattern based methods. Downey et al. (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. Pas¸ca et al. (2006) alleviates pattern sparseness by using infix patterns that are generalized using classes of distributionally similar words. In addition, their method employs clustering based semantic similarities to filter newly extracted instances in each iteration of the bootstrapping process. A comparison with our method would have been instructive, but we were unable to implement their m"
D11-1076,P05-1053,0,0.0215326,"he noun, which is captured by the unigram features. In addition to these base features, we include the semantic classes to which the candidate noun pair belongs, the partial patterns they match in this sentence, and the infix words inbetween the target noun pair. Note that this feature set is not intended to be optimal beyond the actual claims of this paper, and we have deliberately avoided exhaustive feature engineering so as not to obscure the contribution of semantic classes and partial pattern to our approach. Clearly an optimal classifier will incorporate many more advanced features (see GuoDong et al. (2005) for a comprehensive overview), but even without sophisticated feature engineering our classifier achieved sufficient performance levels to support our claims. An overview of the feature set is given in Table 1. The relative contribution of each type of features is discussed in section 4. In preliminary experiments we found a polynomial kernel of degree 3 gave the best results, which suggests the effectiveness of combining different types of indirect evidence. The SVM classifier outputs (noun pair, sentence) triples, ranked by SVM score. To obtain the final output of our method we assign each"
D11-1076,C92-2082,0,0.300524,"that our method performs on par with state-of-the-art pattern based methods, and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns. This ability to acquire long tail instances is crucial for risk management and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance. “Curing hypertension alleviates the deterioration speed of the renal function, thereby lowering the risk of causing intracranial bleeding” 1 Introduction Pattern based relation acquisition methods rely on lexico-syntactic patterns (Hearst, 1992) for extracting relation instances. These are templates of natural language expressions such as “X causes Y ” that signal an instance of some semantic relation (i.e., causality). Pattern based methods (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009) learn many ∗ This work was done when all authors were at the National Institute of Information and Communications Technology. Humans can infer that this sentence expresses a causal relation between the underlined noun phrases. But the actual pattern connecting them, i.e., “Curing X alleviat"
D11-1076,P08-1047,1,0.772197,", together with the highest scoring class dependent pattern each noun pair co-occurs with. This list becomes the input to Stage 2 of our method, as shown in Figure 1. We adopted CDP as Stage 1 extractor because, besides having generally good performance, the class dependent patterns provide the two fundamental ingredients for Stage 2 of our method — the target semantic word classes for a given relation (in the form of the semantic class restrictions attached to patterns), and partial patterns. To obtain fine-grained semantic word classes we used the large scale word clustering algorithm from (Kazama and Torisawa, 2008), which uses the EM algorithm to compute the probability that a word w belongs to class c, i.e., P (c|w). Probabilistic clustering defines no discrete boundary between members and non-members of a semantic class, so we simply assume w belongs to c whenever P (c|w) ≥ 0.2. For this work we clustered 106 nouns into 500 classes. Finally, we adopt the structural representation of patterns introduced in (Lin and Pantel, 2001). All sentences in our corpus are dependency parsed, and patterns consist of words on the path of dependency relations connecting two nouns. 827 3 Stage 2 Extractor We use CDP a"
D11-1076,D08-1106,0,0.030624,"Missing"
D11-1076,P09-1113,0,0.0421639,"d have been instructive, but we were unable to implement their method because the original paper contains insufficient detail to allow replication. There is a large body of research in the supervised tradition that does not use explicit pattern representations — kernel based methods (Zelenko et al., 2003; Culotta, 2004; Bunescu and Mooney, 2005) and CRF based methods (Culotta et al., 2006). These approaches are all fully supervised, whereas in our work the automatic generation of candidates and training data is an integral part of the method. An interesting alternative is distant supervision (Mintz et al., 2009), which trains a classifier using an existing database (Freebase) containing thousands of semantic relations, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the relations in this work) and languages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such"
D11-1076,P06-1102,0,0.161549,"Missing"
D11-1076,P06-1015,0,0.697909,"gement and innovation, where an exhaustive database of high-level semantic relations like causation is of vital importance. “Curing hypertension alleviates the deterioration speed of the renal function, thereby lowering the risk of causing intracranial bleeding” 1 Introduction Pattern based relation acquisition methods rely on lexico-syntactic patterns (Hearst, 1992) for extracting relation instances. These are templates of natural language expressions such as “X causes Y ” that signal an instance of some semantic relation (i.e., causality). Pattern based methods (Agichtein and Gravano, 2000; Pantel and Pennacchiotti, 2006b; Pas¸ca et al., 2006; De Saeger et al., 2009) learn many ∗ This work was done when all authors were at the National Institute of Information and Communications Technology. Humans can infer that this sentence expresses a causal relation between the underlined noun phrases. But the actual pattern connecting them, i.e., “Curing X alleviates the deterioration speed of the renal function, thereby lowering the risk of causing Y ”, is rarely observed more than once even in a 108 page Web corpus. In the sense that the term pattern implies a recurring event, this expression contains no pattern for de"
D11-1076,D10-1106,0,0.0805018,"tions, with millions of instances. We believe our method is more general, as depending on external resources like a database of semantic relations limits both the range of semantic relations (i.e., Freebase contains only relations between named entities, and none of the relations in this work) and languages (i.e., no resource comparable to Freebase exists for Japanese) to which the technology can be applied. Furthermore, it is unclear whether distant supervision can deal with noisy input such as automatically acquired relation instances. Finally, inference based methods (Carlson et al., 2010; Schoenmackers et al., 2010; Tsuchida et al., 2010) are another attempt at relation acquisition that goes beyond pattern matching. Carlson et al. (2010) proposed a method based on inductive logic programming (Quinlan, 1990). Schoenmackers et al. (2010) takes relation instances produced by TextRunner (Banko and Etzioni, 2008) as input and induces first-order Horn clauses, and new instances are infered using a Markov Logic Network (Richardson and Domingo, 2006; Huynh and Mooney, 2008). Tsuchida et al. (2010) generated new relation hypotheses by substituting words in seed instances with distributionally similar words. The"
D11-1076,I08-1025,1,0.836691,"4.1 Experimental Setting We evaluate our method on three semantic relation acquisition tasks: causality, prevention and material. Two concepts stand in a causal relation when the source concept (the “cause”) is directly or indirectly responsible for the subsequent occurrence of the target concept (its “effect”). In a prevention relation the source concept directly or indirectly acts to avoid the occurrence of the target concept, and in a material relation the source concept is a material or ingredient of the target concept. For our experiments we used the latest version of the TSUBAKI corpus (Shinzato et al., 2008), a collection of 600 million Japanese Web pages dependency parsed by the Japanese dependency parser KNP2 . In our implementation of CDP, lexicosyntactic patterns consist of words on the path connecting two nouns in a dependency parse tree. We discard patterns from dependency paths longer than 8 constituent nodes. Furthermore, we estimated pattern frequencies in a subset of the corpus (50 million pages, or 1/12th of the entire corpus) and discarded patterns that co-occur with less than 10 unique noun pairs in this smaller corpus. These restrictions do not apply to the proposed method, which ca"
D12-1034,D12-1057,1,0.152513,"Missing"
D12-1034,I08-1055,0,0.769914,"queries for obtaining answer candidates. Each document in the result of document retrieval is split into a set of answer candidates consisting of five subsequent sentences4 . Subsequent answer candidates can share up to two sentences to avoid errors due to wrong document segmentation. 2 http://lucene.apache.org/solr To the best of our knowledge, few Japanese non-factoid QA systems in the literature have used such a large-scale corpus. 4 The length of acceptable answer candidates for whyQA in the literature ranges from one sentence to two paragraphs (Fukumoto et al., 2007; Murata et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2007; Verberne et al., 2010). 3 Answer candidate ac for question q is ranked according to scoring function S(q, ac) given in Eq. (1) (Murata et al., 2007). Murata et al. (2007)’s method uses text search to look for answer candidates containing terms from the question with additional clue terms referring to “reason” or “cause.” Following the original method we used riyuu (reason), genin (cause) and youin (cause) as clue terms. The top-20 answer candidates for each question are passed on to the next step, which is answer reranking. S(q, ac) assigns a score to answer candidates"
D12-1034,P08-1047,1,0.530577,"e-ranker candidates that themselves contain a term from the question (e.g., “types of cancer” in example A1-2). MSA3 is the n-gram feature that contains one of the clue terms used for answer retrieval (riyuu (reason), genin (cause) or youin (cause)). Here too, n-grams obtained from the questions and answer candidates are distinguished. Finally, MSA4 is the percentage of the question terms found in an answer candidate. 3.2 Semantic Word Class Semantic word classes are sets of semantically similar words. We construct these semantic word classes by using the noun clustering algorithm proposed in Kazama and Torisawa (2008). The algorithm follows the distributional hypothesis, which states that semantically similar words tend to appear in similar contexts (Harris, 1954). By treating syntactic dependency relations between words as “contexts,” the clustering method defines a probabilistic model of noun-verb dependencies with hidden classes as: p(n, v, r) = X p(n|c)p(hv, ri|c)p(c) (2) c 371 Here, n is a noun, v is a verb or noun on which n depends via a grammatical relation r (post-positions in Japanese), and c is a hidden class. Dependency relation frequencies were obtained from our 600-million page web corpus, an"
D12-1034,P09-1083,0,0.0131861,"features and Verberne et al. (2010) exploited WordNet features as a kind of semantic features for training their re-ranker, where we used these features, respectively, for B-Ranker+CR and B-Ranker+WN in our experiment. Our work differs from the above approaches in that we propose semantic word classes and sentiment analysis as a new type of semantic features, and show their usefulness in why-QA. Sentiment analysis has been used before on the slightly unusual task of opinion question answering, where the system is asked to answer subjective opinion questions (Stoyanov et al., 2005; Dang, 2008; Li et al., 2009). To the best of our knowledge though, no previous work has systematically explored the use of sentiment analysis in a general QA setting beyond opinion questions. 7 Conclusion In this paper, we have explored the utility of sentiment analysis and semantic word classes for ranking answer candidates to why-questions. We proposed a set of semantic features that exploit sentiment analysis and semantic word classes obtained from largescale noun clustering, and used them to train an answer candidate re-ranker. Through a series of experiments on 850 why-questions, we showed that the proposed semantic"
D12-1034,N10-1120,0,0.0119335,"their respective semantic word classes. For example, Wdisease in word class 2-gram “cause Wdisease ” from A2 is the semantic word class of rickets, one of the question 372 terms. These features capture the correspondence between semantic word classes in the question and answer candidates. 3.3 Sentiment Analysis Sentiment analysis (SA) features are classified into word-polarity and phrase-polarity features. We use opinion extraction tool8 and sentiment orientation lexicon in the tool for these features. 3.3.1 Opinion Extraction Tool Opinion extraction tool is a software, the implementation of Nakagawa et al. (2010). It extracts linguistic expressions representing opinions (henceforth, we call them sentiment phrases) from a Japanese sentence and then identifies the polarity of these sentiment phrases using machine learning techniques. For example, rickets occur in Q2 and Deficiency of vitamin D can cause rickets in A2 can be identified as sentiment phrases with a negative polarity. The tool identifies sentiment phrases and their polarity by using polarities of words and dependency subtrees as evidence, where these polarities are given in a word polarity dictionary. In this paper, we use a trained model a"
D12-1034,W02-1011,0,0.0168213,"Missing"
D12-1034,N04-3012,0,0.0542856,"these CR features are introduced only for comparing our semantic features with ones in Higashinaka and Isozaki (2008) and they are not a part of our method. B-Ranker+WN: its re-ranker is trained with our MSA features and the WordNet features in Verberne et al. (2010). The WordNet features include the percentage of the question terms and their synonyms in WordNet synsets found in an answer candidate and the semantic relatedness score between a question and its answer candidate, the average of the concept similarity between each question term and all of the answer terms by WordNet::Similarity (Pedersen et al., 2004). We used the Japanese WordNet 1.1 (Bond et al., 2009) for these WordNet features. Note that the Japanese WordNet 1.1 has 93,834 Japanese words linked to 57,238 WordNet synsets, while the English WordNet 3.0 covers 155,287 words linked to 117,659 synsets. Due to this lower coverage, the WordNet features in Japanese may have a less power for finding a correct answer than those in English used in Verberne et al. (2010). Proposed: our proposed method. All of the MSA, SWC and SA features are used for training our 376 re-ranker. UpperBound: a system that ranks all n correct answers as the top n res"
D12-1034,H05-1116,0,0.0201051,"sozaki (2008) used causal relation features and Verberne et al. (2010) exploited WordNet features as a kind of semantic features for training their re-ranker, where we used these features, respectively, for B-Ranker+CR and B-Ranker+WN in our experiment. Our work differs from the above approaches in that we propose semantic word classes and sentiment analysis as a new type of semantic features, and show their usefulness in why-QA. Sentiment analysis has been used before on the slightly unusual task of opinion question answering, where the system is asked to answer subjective opinion questions (Stoyanov et al., 2005; Dang, 2008; Li et al., 2009). To the best of our knowledge though, no previous work has systematically explored the use of sentiment analysis in a general QA setting beyond opinion questions. 7 Conclusion In this paper, we have explored the utility of sentiment analysis and semantic word classes for ranking answer candidates to why-questions. We proposed a set of semantic features that exploit sentiment analysis and semantic word classes obtained from largescale noun clustering, and used them to train an answer candidate re-ranker. Through a series of experiments on 850 why-questions, we sho"
D12-1034,J11-2003,0,0.0266434,"their score given by SVMs. We trained and tested the re-ranker using 10-fold cross validation on a corpus composed of 850 why-questions and their top20 answer candidates provided by the answer retrieval procedure in Section 2.1. The answer candidates were manually annotated by three human annotators (not by the authors). Our corpus construction method is described in more detail in Section 4. 370 Morphological and Syntactic Analysis MSA including n-grams of morphemes, words, and syntactic dependencies has been widely used for reranking answers in non-factoid QA (Higashinaka and Isozaki, 2008; Surdeanu et al., 2011; Verberne et al., 2007; Verberne et al., 2010). We use MSA as a baseline feature set in this work. We represent all sentences in a question and its answer candidate in three ways: morphemes, word phrases (bunsetsu5 ) and syntactic dependency chains. These are obtained using a morphological analyzer6 and a dependency parser7 . From each question and answer candidate we extract n-grams of morphemes, word phrases, and syntactic dependencies, where n ranges from 1 to 3. Syntactic dependency n-grams are defined as a syntactic dependency chain containing n word phrases. Syntactic dependency 1-grams"
D12-1034,P02-1053,0,0.0123562,"Missing"
D12-1034,J10-2003,0,0.23433,"in the result of document retrieval is split into a set of answer candidates consisting of five subsequent sentences4 . Subsequent answer candidates can share up to two sentences to avoid errors due to wrong document segmentation. 2 http://lucene.apache.org/solr To the best of our knowledge, few Japanese non-factoid QA systems in the literature have used such a large-scale corpus. 4 The length of acceptable answer candidates for whyQA in the literature ranges from one sentence to two paragraphs (Fukumoto et al., 2007; Murata et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2007; Verberne et al., 2010). 3 Answer candidate ac for question q is ranked according to scoring function S(q, ac) given in Eq. (1) (Murata et al., 2007). Murata et al. (2007)’s method uses text search to look for answer candidates containing terms from the question with additional clue terms referring to “reason” or “cause.” Following the original method we used riyuu (reason), genin (cause) and youin (cause) as clue terms. The top-20 answer candidates for each question are passed on to the next step, which is answer reranking. S(q, ac) assigns a score to answer candidates like tf -idf , where 1/dist(t1 , t2 ) function"
D12-1034,W09-3401,0,\N,Missing
D12-1057,C08-1001,0,0.124343,"rientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting semantic knowledge"
D12-1057,andreevskaia-bergler-2006-semantic,0,0.0104394,"e latter is negative. General Inquirer (Stone et al., 1966) deals with semantic factors some of which were proposed by Osgood et al. (1957). Their ‘activity’ factor involves binary opposition between ‘active’ and ‘passive.’ Notice that activity and Excitation are independent. In General Inquirer, both accelerate X and abolish X are active, but only the former is excitatory. Both accept X and abate X are passive, but only the latter is inhibitory. Pustejovsky (1995) proposed telic 620 and agentive roles, which inspired our excitatory notion, but they have no corresponding notion of inhibitory. Andreevskaia and Bergler (2006) acquired the increase/decrease semantic orientation, which is a subclass of Excitation. Excitation is inverted if a template’s predicate is negated. For example, preserve X is excitatory, while don’t preserve X is inhibitory. We acknowledge that this may seem somewhat counter-intuitive and will address this issue in future work. 3 Excitation Template Acquisition This section presents our acquisition method of Excitation templates. We introduce constraints in the co-occurrence of templates in text that seem both robust and language independent in Section 3.1. Our method exploits these constrai"
D12-1057,P09-1068,0,0.00625682,"revious methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting semantic knowledge base is important for the dis"
D12-1057,D11-1027,0,0.0828657,"ion extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting semantic knowledge base is important for the discovery of so-call"
D12-1057,W03-1210,0,0.388258,"itation represents a genuinely new semantic orientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpor"
D12-1057,P97-1023,0,0.700856,"ate X, close to X) For example, when fire fills the X slot of cause X, it suggests that the effect of fire is activated. If prevent X’s slot is filled with flu, the effect of flu is suppressed. In this study, we aim to acquire excitatory and inhibitory templates that are useful for extracting contradiction and causality, though neutral templates are the most frequent in our data (See Section 5.1). Collectively we call excitatory and inhibitory templates Excitation templates, and excitatory and inhibitory two opposite polarities. Excitation is independent of the good/bad semantic orientation. (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009). For example, sophisticate X and complicate X are both excitatory, but only the former has a positive connotation. Similarly, remedy X and degrade X are both inhibitory but only the latter is negative. General Inquirer (Stone et al., 1966) deals with semantic factors some of which were proposed by Osgood et al. (1957). Their ‘activity’ factor involves binary opposition between ‘active’ and ‘passive.’ Notice that activity and Excitation are independent. In General Inquirer, both accelerate X and abolish X are active, but only the former is excitatory."
D12-1057,N06-1023,0,0.017355,"Missing"
D12-1057,P98-2127,0,0.15841,"contrast in Excitation value. Concretely, we 622 extract two phrases as a contradiction pair if (a) their templates have opposite Excitation polarities, (b) they share the same argument noun, and (c) the part-of-speech of their predicates is the same. Then the contradiction pairs are ranked by Ct: Ct(p1 , p2 ) = |s1 |× |s2 |× sim(t1 , t2 ) Here p1 and p2 are two phrases that satisfy conditions (a), (b) and (c) above, t1 and t2 are their respective templates, and |s1 |and |s2 |are the absolute values of t1 and t2 ’s excitation values. sim(t1 , t2 ) is the distributional similarity proposed by Lin (1998). Note that “contradiction” here includes what we call “quasi-contradiction.” This consists of two phrases such that, if the tendencies of the events they describe get stronger, they eventually become contradictions. For example, the pair emit smells ⊥ reduce smells is not logically contradictory since the two events can happen at the same time. However, they become almost contradictory when their tendencies get stronger (i.e., emit smells more strongly ⊥ thoroughly reduce smells). We believe quasicontradictions are useful for NLP tasks. 4.2 Causality Extraction Our second knowledge acquisitio"
D12-1057,D08-1103,0,0.0497279,"Missing"
D12-1057,D12-1034,1,0.153229,"Missing"
D12-1057,E09-1077,0,0.0403667,"e X slot of cause X, it suggests that the effect of fire is activated. If prevent X’s slot is filled with flu, the effect of flu is suppressed. In this study, we aim to acquire excitatory and inhibitory templates that are useful for extracting contradiction and causality, though neutral templates are the most frequent in our data (See Section 5.1). Collectively we call excitatory and inhibitory templates Excitation templates, and excitatory and inhibitory two opposite polarities. Excitation is independent of the good/bad semantic orientation. (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009). For example, sophisticate X and complicate X are both excitatory, but only the former has a positive connotation. Similarly, remedy X and degrade X are both inhibitory but only the latter is negative. General Inquirer (Stone et al., 1966) deals with semantic factors some of which were proposed by Osgood et al. (1957). Their ‘activity’ factor involves binary opposition between ‘active’ and ‘passive.’ Notice that activity and Excitation are independent. In General Inquirer, both accelerate X and abolish X are active, but only the former is excitatory. Both accept X and abate X are passive, but"
D12-1057,I11-1115,0,0.0147558,"quire either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting semantic knowledge base is important for the discovery of so-called “unknown unknowns” (Torisaw"
D12-1057,P05-1017,0,0.236301,"Missing"
D12-1057,N06-1008,1,0.773225,"y new semantic orientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisition of semantic knowledge that is not explicitly written in corpora, Tsuchida et al. (2011) proposed a novel method to generate semantic relation instances as hypotheses using automatically discovered inference rules. We think that automatically generating plausible semantic knowledge that is not written (explicitly) in corpora as hypotheses and augmenting s"
D12-1057,I11-1101,1,0.787648,"Missing"
D12-1057,P02-1053,0,0.0137921,"fire fills the X slot of cause X, it suggests that the effect of fire is activated. If prevent X’s slot is filled with flu, the effect of flu is suppressed. In this study, we aim to acquire excitatory and inhibitory templates that are useful for extracting contradiction and causality, though neutral templates are the most frequent in our data (See Section 5.1). Collectively we call excitatory and inhibitory templates Excitation templates, and excitatory and inhibitory two opposite polarities. Excitation is independent of the good/bad semantic orientation. (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009). For example, sophisticate X and complicate X are both excitatory, but only the former has a positive connotation. Similarly, remedy X and degrade X are both inhibitory but only the latter is negative. General Inquirer (Stone et al., 1966) deals with semantic factors some of which were proposed by Osgood et al. (1957). Their ‘activity’ factor involves binary opposition between ‘active’ and ‘passive.’ Notice that activity and Excitation are independent. In General Inquirer, both accelerate X and abolish X are active, but only the former is excitatory. Both accept X"
D12-1057,C08-1114,0,0.0198351,"results, we conclude that our assumption on causality hypothesis generation is valid. 6 Related Work While the semantic orientation involving good/bad (or desirable/undesirable) has been extensively stud628 ied (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009; Velikovich et al., 2010), we believe Excitation represents a genuinely new semantic orientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge between events exist (Girju, 2003; Torisawa, 2005; Torisawa, 2006; Abe et al., 2008; Chambers and Jurafsky, 2009; Do et al., 2011; Shibata and Kurohashi, 2011), but none uses a notion similar to Excitation. As we have shown, we expect that Excitation will improve their performance. Regarding the acquisi"
D12-1057,N10-1119,0,0.0194522,"the case was erroneous since its original causality was erroneous. The second 6 case was due to the fact that one of the contradiction phrase pairs used to generate the hypothesis was in fact not contradictory (景 気をコントロールする 6⊥ 景気が良くなる ‘control economic conditions 6⊥ economic conditions improve’). From these results, we conclude that our assumption on causality hypothesis generation is valid. 6 Related Work While the semantic orientation involving good/bad (or desirable/undesirable) has been extensively stud628 ied (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Rao and Ravichandran, 2009; Velikovich et al., 2010), we believe Excitation represents a genuinely new semantic orientation. Most previous methods of contradiction extraction require either thesauri like Roget’s or WordNet (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008) or large training data for supervision (Turney, 2008). In contrast, our method requires only a few seed templates. Lin et al. (2003) used a few “incompatibility” patterns to acquire antonyms, but they did not report their method’s performance on the incompatibility identification task. Many methods for extracting causality or scriptlike knowledge betwee"
D12-1057,P08-1008,0,0.088906,"Missing"
D12-1057,P08-1118,0,\N,Missing
D12-1057,C98-2122,0,\N,Missing
D13-1065,P11-1062,0,0.324324,"e, like “X causes Y”, and we say a noun pair co-occurs with a pattern if the two nouns are connected by this pattern in the dependency tree of a sentence in the corpus. We focus on typed binary patterns, which place semantic class restrictions on the noun pairs they co-occur with, e.g., “Yorganization is in Xlocation ”. Subscripts organization and location indicate the semantic classes of the X and Y slots. Since typed patterns can distinguish between multiple senses of ambiguous patterns, they greatly reduce errors due to pattern ambiguity (De Saeger et al., 2009; Schoenmackers et al., 2010; Berant et al., 2011). We automatically induced semantic classes from our corpus using the EM-based noun clustering algorithm presented in Kazama and Torisawa (2008), and clustered one million nouns into 500 relatively clean semantic classes, including for example classes of diseases and of chemical substances. The binary patterns and their co-occurring noun pairs were extracted from our corpus of 600 million Japanese web pages dependency parsed with KNP (Kurohashi and Nagao, 1994). We restricted our patterns to the most frequent 3.9 million patterns of the form “X-[case particle] Y-[case particle] predicate” such"
D13-1065,W07-1403,0,0.0702933,"Missing"
D13-1065,P08-1118,0,0.255977,"Missing"
D13-1065,D09-1122,1,0.787548,"Missing"
D13-1065,D12-1057,1,0.690246,"Missing"
D13-1065,C10-2061,0,0.0722232,"erns directly leads to the discovery of unreliable information to be flagged or to a meaningful survey of complex problems. As exemplified by the agaricus and TPP examples, contradictions between binary patterns that include two variables such as “X promotes Y” or “X will wipe out Y” are more useful than those between unary patterns. We also show that it is not trivial to recognize contradictions between binary patterns using contradictions between unary patterns. Most works dealing with contradiction recognition up till now (Harabagiu et al., 2006; Bobrow et al., 2007; Kawahara et al., 2008; Kawahara et al., 2010; Ohki et al., 2011) focus on recognizing contradictions between full sentences or documents, not text fragments that match our relatively short patterns (survey in Section 5). We expect that the contradictory pattern pairs we acquired can be used as building blocks in such full-fledged contradiction recognition for full sentences or documents, similarly to antonym pairs in Harabagiu et al. (2006). Also, we should emphasize that our method focuses on the most challenging part of contradiction recognition according to the classification of De Marneffe et al. (2008). Since we discard patterns wi"
D13-1065,P08-1047,1,0.788432,"e of a sentence in the corpus. We focus on typed binary patterns, which place semantic class restrictions on the noun pairs they co-occur with, e.g., “Yorganization is in Xlocation ”. Subscripts organization and location indicate the semantic classes of the X and Y slots. Since typed patterns can distinguish between multiple senses of ambiguous patterns, they greatly reduce errors due to pattern ambiguity (De Saeger et al., 2009; Schoenmackers et al., 2010; Berant et al., 2011). We automatically induced semantic classes from our corpus using the EM-based noun clustering algorithm presented in Kazama and Torisawa (2008), and clustered one million nouns into 500 relatively clean semantic classes, including for example classes of diseases and of chemical substances. The binary patterns and their co-occurring noun pairs were extracted from our corpus of 600 million Japanese web pages dependency parsed with KNP (Kurohashi and Nagao, 1994). We restricted our patterns to the most frequent 3.9 million patterns of the form “X-[case particle] Y-[case particle] predicate” such as “X-ga Y-ni aru” (“X is in Y”) which do not contain any negation, number, symbol or punctuation character. Based on our observation that patt"
D13-1065,W10-3114,0,0.0167745,"e slot, and with each of the pattern’s unary sub-patterns. We also added a few more uncategorizable features. See Table 4 for more details. 5 as antonyms and negations in these works. Closer to our work, Ritter et al. (2008) presented a method for detecting contradictions between functional relations like “X was born in Y”, but these constitute only a part of the semantic relations expressed by the binary patterns we deal with in this paper. Other works analyzed contradictions from linguistic/semantic viewpoints. Voorhees (2008) analyzed the contradiction recognition-task of the RTE3 contest. Magnini and Cabrio (2010) examined relations between contradictions and textual entailment samples. De Marneffe et al. (2008) presented a typology of contradictions, and showed that contradictions can arise from a multitude of phenomena. They showed contradictions based on lexical or world knowledge are challenging and require a highlevel understanding of language and/or the world. As stated in the introduction, these are the types of contradictions our method focuses on. Related Work A number of previous work dealt with the recognition of contradictions between sentences. Harabagiu et al. (2006) proposed a contradict"
D13-1065,W07-1407,0,0.0834851,"ternal lexical resources, and distributional similarity based features; all features are listed in Table 4. ENT uses all the features while BASE and EXP use all except for the distributional similarity based ones. The optimality of the feature sets was confirmed through ablation tests using the development set (results omitted for the sake of space). Since patterns with a contradiction or entailment relation are often superficially similar, for instance, in case structure or inflection, we use a number of surface features based on string similarity measures, extending the feature sets used by Malakasiotis and Androutsopoulos (2007) for entailment recognition. They include bag-of-words features such as n-grams and similarity scores concerning the bag-of-words such as their Euclidian distance. To complement the surface features with knowledge about the content words, we used lexical databases including such as antonymy, synonymy, entailment, or allography. The presence of such word pairs is usually a good indicator of (non-)contradiction or (non-)entailment at the pattern level. More specifically, for any word pair hwp, wqi taken from a pattern pair hp, qi we mark the presence of hwp, wqi in each of the lexical resources"
D13-1065,W11-0123,0,0.116636,"the discovery of unreliable information to be flagged or to a meaningful survey of complex problems. As exemplified by the agaricus and TPP examples, contradictions between binary patterns that include two variables such as “X promotes Y” or “X will wipe out Y” are more useful than those between unary patterns. We also show that it is not trivial to recognize contradictions between binary patterns using contradictions between unary patterns. Most works dealing with contradiction recognition up till now (Harabagiu et al., 2006; Bobrow et al., 2007; Kawahara et al., 2008; Kawahara et al., 2010; Ohki et al., 2011) focus on recognizing contradictions between full sentences or documents, not text fragments that match our relatively short patterns (survey in Section 5). We expect that the contradictory pattern pairs we acquired can be used as building blocks in such full-fledged contradiction recognition for full sentences or documents, similarly to antonym pairs in Harabagiu et al. (2006). Also, we should emphasize that our method focuses on the most challenging part of contradiction recognition according to the classification of De Marneffe et al. (2008). Since we discard patterns with negations, an evi"
D13-1065,D08-1002,0,0.0889012,"istributional similarity values between patterns are based on the idea that patterns that appear in similar contexts tend to have similar meanings and as such are useful to recognize entailment (Lin and Pantel, 2001). We computed as features several distributional similarity measures on the sets of each pattern’s co-occurring noun pairs and their POS tags, of nouns co-occurring in each variable slot, and with each of the pattern’s unary sub-patterns. We also added a few more uncategorizable features. See Table 4 for more details. 5 as antonyms and negations in these works. Closer to our work, Ritter et al. (2008) presented a method for detecting contradictions between functional relations like “X was born in Y”, but these constitute only a part of the semantic relations expressed by the binary patterns we deal with in this paper. Other works analyzed contradictions from linguistic/semantic viewpoints. Voorhees (2008) analyzed the contradiction recognition-task of the RTE3 contest. Magnini and Cabrio (2010) examined relations between contradictions and textual entailment samples. De Marneffe et al. (2008) presented a typology of contradictions, and showed that contradictions can arise from a multitude"
D13-1065,D10-1106,0,0.0254975,"n a syntactic dependency tree, like “X causes Y”, and we say a noun pair co-occurs with a pattern if the two nouns are connected by this pattern in the dependency tree of a sentence in the corpus. We focus on typed binary patterns, which place semantic class restrictions on the noun pairs they co-occur with, e.g., “Yorganization is in Xlocation ”. Subscripts organization and location indicate the semantic classes of the X and Y slots. Since typed patterns can distinguish between multiple senses of ambiguous patterns, they greatly reduce errors due to pattern ambiguity (De Saeger et al., 2009; Schoenmackers et al., 2010; Berant et al., 2011). We automatically induced semantic classes from our corpus using the EM-based noun clustering algorithm presented in Kazama and Torisawa (2008), and clustered one million nouns into 500 relatively clean semantic classes, including for example classes of diseases and of chemical substances. The binary patterns and their co-occurring noun pairs were extracted from our corpus of 600 million Japanese web pages dependency parsed with KNP (Kurohashi and Nagao, 1994). We restricted our patterns to the most frequent 3.9 million patterns of the form “X-[case particle] Y-[case par"
D13-1065,P07-1058,0,0.0502129,"Popp with EXP to produce the final output of the whole method. Note that while this expansion process can be re-iterated with EXP’s output, our experiments failed to show any improvement with subsequent iterations. Figure 4: Precision of all the compared methods al. (2008). We then say binary patterns such as “X causes Y” and “X prevents Y” are contradictory if the above definition holds for any noun pair that can instantiate the patterns’ variables in the provided semantic class pair. Because our semantic classes are obtained by automatic clustering and have no meaningful labels, we followed Szpektor et al. (2007) and provided the annotators with three random noun pairs that cooccur with the patterns as a proxy for the class pair. The annotators marked a given pattern pair as positive if the contradiction relation between the patterns held for all three noun pairs presented. 3.2 Experimental Results 3 Evaluation This section presents our experimental results. We describe first how we constructed test and development data, and then report comparison results between our method and others including BASE and an Integer Linear Programming-based (ILP) method. Here we show how our proposed method outperforms"
D13-1065,P08-1008,0,0.0256214,"co-occurring noun pairs and their POS tags, of nouns co-occurring in each variable slot, and with each of the pattern’s unary sub-patterns. We also added a few more uncategorizable features. See Table 4 for more details. 5 as antonyms and negations in these works. Closer to our work, Ritter et al. (2008) presented a method for detecting contradictions between functional relations like “X was born in Y”, but these constitute only a part of the semantic relations expressed by the binary patterns we deal with in this paper. Other works analyzed contradictions from linguistic/semantic viewpoints. Voorhees (2008) analyzed the contradiction recognition-task of the RTE3 contest. Magnini and Cabrio (2010) examined relations between contradictions and textual entailment samples. De Marneffe et al. (2008) presented a typology of contradictions, and showed that contradictions can arise from a multitude of phenomena. They showed contradictions based on lexical or world knowledge are challenging and require a highlevel understanding of language and/or the world. As stated in the introduction, these are the types of contradictions our method focuses on. Related Work A number of previous work dealt with the rec"
D13-1065,W03-1011,0,0.0601971,"ted over: the patterns’ 1-, 2- and 3-grams sets of: characters, morphemes, their stems & POS; content words and stems binary feature for each of the patterns’ subtrees, 1- and 2-grams ; patterns’ lengths and length ratios entries in databases of verb entailments and non-entailments, synonyms, antonyms, allographs ; checked over: pairs of content words, pairs of content word stems, same for the reverse pattern pair hq, pi Distributional similarity measures: Common elements ratios, Jaccard and discounted Jaccard scores, sets and sets intersection cardinality, DIRT (Lin and Pantel, 2001), Weeds (Weeds and Weir, 2003) and Hashimoto (Hashimoto et al., 2009) scores; computed over: patterns’ co-occurring noun pairs, POS tags of those, nouns co-occurring in each variable slot, nouns co-occurring with each unary sub-patterns binary feature for each semantic class pair and individual semantic classes patterns frequency rank in the given semantic class pair pairs; Alagin ID A-2), the databases of synonyms, antonyms and meronyms (respectively 111,000, 5000 and 2500 pairs; Alagin ID A-9), and the allographic word database (2.7 million pairs; Alagin ID A-7). We also used the information concerning allographic words"
D15-1190,P11-1062,0,0.69887,"nly present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1 Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as buil"
D15-1190,P05-1014,0,0.270325,"Missing"
D15-1190,P06-1114,0,0.19082,"Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1 Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir, 2003b), although we showed that supervised techniques naturally obtain stronger performance (Kloetzer et al., 2013b). The two works that are most closely related to our work are Berant et al. (2011) and Kloetzer et al"
D15-1190,D09-1122,1,0.850399,"Missing"
D15-1190,P08-1047,1,0.885426,"Missing"
D15-1190,D13-1065,1,0.724206,"Missing"
D15-1190,D15-1190,1,0.0512755,"Missing"
D15-1190,W07-1407,0,0.108118,"tribute Y”→“Y is available on X”, “underwent X on Y”→“X carried out on Y”, “start X at Y”→“Y’s X” or “attach X to Y”→“put X on Y”. Even though we only present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1 Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et a"
D15-1190,E06-1052,0,0.0926813,"ntailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1 Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir, 2003b), although we showed that supervised techniques naturally obtain stronger performance (Kloetzer et al., 2013b). T"
D15-1190,I08-1025,1,0.857562,"dictions by exploiting transitivity and adds the highest scoring contradictions based on a novel score (CDP) to the training data of the original classifier. The score is based on the assumption that if pair hP ,Qi, when chained by transitivity to other pairs hQ,Ri i, generally leads to correct entailment pairs hP ,Ri i, then all pairs hP ,Ri i should be correct entailment pairs. Although this work was designed for contradiction recognition, it is easily adapted to entailment. 3 Target Data and Baseline Classifiers Target Pattern Pairs We extracted our binary patterns from the TSUBAKI corpus (Shinzato et al., 2008) of 600 million Japanese web pages. Binary patterns are defined as sequences of words on the path of dependency relations connecting two nouns in a sentence and have two variables. “use Y to distribute X” and “X is available on Y” are such 1650 binary patterns. Like previous works (De Saeger et al., 2009; Berant et al., 2011; Kloetzer et al., 2013a), we pose restrictions on the noun-pairs that co-occur with each pattern using word classes to disambiguate their various potential meanings: “Xbook by Yauthor ” and “Xbuilding by Ylocation ”. We used the EM-based noun clustering algorithm presented"
D15-1190,P11-2098,0,0.133136,"or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1 Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Wei"
D15-1190,W04-3206,0,0.0791447,"went X on Y”→“X carried out on Y”, “start X at Y”→“Y’s X” or “attach X to Y”→“put X on Y”. Even though we only present results for the Japanese language, we believe that our method should be applicable to other languages as well. This is because none of the few language dependent features of our classifier are strictly needed by the baseline or our proposed method, and its performance boost is unrelated to these features. 2 Related Works The task of recognizing entailment between texts has been proposed by Dagan et al. (2006) and intensively researched (Malakasiotis and Androutsopoulos, 2007; Szpektor et al., 2004; Androutsopoulos and Malakasiotis, 2009; Dagan et al., 2009; Hashimoto et al., 2009; Berant et al., 2011) using a various range of techniques, including Integer Linear Programming (Berant et al., 2011), machine learning with SVMs (Malakasiotis and Androutsopoulos, 2007), and probabilistic models (Wang and Manning, 2010; Shnarch et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1 Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work"
D15-1190,C10-1131,0,0.169913,"Missing"
D15-1190,W03-1011,0,0.56524,"et al., 2011). Entailment recognizer or entailment data sets have been used in such fields as relation extraction (Romano et al., 2006) and 1 Examples are given in English for convenience question-answering (Harabagiu and Hickl, 2006; Tanaka et al., 2013). In this work, we are interested into recognizing entailment between syntactic patterns, which can then be used as building blocks in a complete entailment recognition system (Shnarch et al., 2011). Recognizing entailment between patterns has generally been studied using unsupervised techniques (Szpektor et al., 2004; Hashimoto et al., 2009; Weeds and Weir, 2003b), although we showed that supervised techniques naturally obtain stronger performance (Kloetzer et al., 2013b). The two works that are most closely related to our work are Berant et al. (2011) and Kloetzer et al. (2013a), both of which exploit transitivity to improve the result of a baseline classifier. Berant et al. (2011) proposed an entailment recognition method for binary patterns that exploits Integer Linear Programming techniques (ILP) to expand the results of an SVM classifier. This method encodes into an ILP problem an entailment graph, which is a valued graph where nodes and edges r"
D15-1190,I13-2012,1,\N,Missing
D15-1260,D13-1135,0,0.228295,"tic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint"
D15-1260,J95-2003,0,0.789369,"Missing"
D15-1260,D13-1095,0,0.205399,"from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or coreference resolution ("
D15-1260,P14-1093,1,0.808665,"Missing"
D15-1260,I11-1023,0,0.55312,"5). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for"
D15-1260,P11-1081,1,0.910037,"h trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Y"
D15-1260,W07-1522,1,0.837638,"Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution."
D15-1260,P09-2022,0,0.734906,"Missing"
D15-1260,W03-1024,0,0.669377,"kumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero an"
D15-1260,P86-1031,0,0.769488,"sharing recognition for the three types in Section 4. We evaluate how effectively our method recognizes subject sharing relations for these types in Section 5. After that, we investigate the impact of explicitly introducing SSPNs in Section 6 and compare our zero anaphora resolution method with a state-of-the-art ILP-based method on the task of intra-sentential subject zero anaphora resolution in Section 7. Finally, in Section 8 we summarize this work and discuss future directions. 2 Related work Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japan"
D15-1260,P08-1047,1,0.795694,"pe of subject sharing relations, whether certain nouns appear between two predicates is an important clue, e.g., koto (complementizer) in example (6) and nouryoku (ability) in example (7). 2183 Name Description PoSi (PoSj ) PoS of pi (pj ) lemmai (lemmaj ) lemma of pi (pj ) func wi (func wj ) function words following pi (pj ) casei (casej ) case marker of arguments of pi (pj ) btw case case marker of arguments that appeared between pi and pj NpPoS* PoS of np Np lemma* lemma of np func wnp * function words following np casenp * case marker of dependents of np n class* noun class of np based on Kazama and Torisawa (2008) pi and pj stand for the left and right predicates in predicate pairs. np is the noun phrase between pi and pj . bi (bj ) stands for the bunsetsu-unit4 including pi (pj ). The features marked with * are only used for PNP type. Table 1: Features of subject sharing recognition (6) seifui -wa (ϕi -ga) sono isetsu-o government-TOP iti -SUBJ the relocation-OBJ mitomeru koto-o kime-ta . admit decide-PAST period COMP - OBJ by assigning noun n to class c when the model parameter p(c|n) > θ (θ = 0.2). 5 Experiment 1: pairwise subject sharing recognition The governmenti decided that (iti ) admits the re"
D15-1260,C96-2137,0,0.0578603,"te how effectively our method recognizes subject sharing relations for these types in Section 5. After that, we investigate the impact of explicitly introducing SSPNs in Section 6 and compare our zero anaphora resolution method with a state-of-the-art ILP-based method on the task of intra-sentential subject zero anaphora resolution in Section 7. Finally, in Section 8 we summarize this work and discuss future directions. 2 Related work Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira"
D15-1260,C96-2147,0,0.195854,"es in Section 4. We evaluate how effectively our method recognizes subject sharing relations for these types in Section 5. After that, we investigate the impact of explicitly introducing SSPNs in Section 6 and compare our zero anaphora resolution method with a state-of-the-art ILP-based method on the task of intra-sentential subject zero anaphora resolution in Section 7. Finally, in Section 8 we summarize this work and discuss future directions. 2 Related work Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003"
D15-1260,C14-1135,1,0.876749,"Missing"
D15-1260,C08-1097,0,0.499313,"nly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseli"
D15-1260,N09-1059,0,0.389782,"rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Conc"
D15-1260,C02-1078,0,0.85003,"ker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overa"
D15-1260,D08-1055,0,0.447225,"1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of th"
D15-1260,J94-2003,0,0.105941,"ion for the three types in Section 4. We evaluate how effectively our method recognizes subject sharing relations for these types in Section 5. After that, we investigate the impact of explicitly introducing SSPNs in Section 6 and compare our zero anaphora resolution method with a state-of-the-art ILP-based method on the task of intra-sentential subject zero anaphora resolution in Section 7. Finally, in Section 8 we summarize this work and discuss future directions. 2 Related work Traditional approaches to zero anaphora resolution are based on manually created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 200"
D15-1260,P10-2018,0,0.0738626,"eory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related method"
D15-1260,P05-1021,0,0.0131313,"1; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or coreference resolution (Bean and Riloff, 2004; Bansal and Klein, 2012). In these methods, the semantic compatibility between the contexts surrounding an anaphor and its antecedent (e.g., the compatibility of verbs kidnap and release given some arguments) was automatically extracted from raw texts in an unsupervised manner and used as features in a machine learning-based approach. However, because the automatically acquired semantic compatibility is not always true or applicable in the context of any pair of an anaphor and its antecedent, the effectiveness of the compatibility features migh"
D15-1260,I11-1126,0,0.37852,"a resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or cor"
D15-1260,D09-1160,0,0.196514,"Missing"
D15-1260,I13-1126,0,0.395716,"approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011) revealed that joint inference improves the overall performance of zero anaphora resolution. We employed one of these works as a baseline in Section 6. Concerning subject sharing recognition, related methods have been explored for pronominal anaphora (Yang et al., 2005) or coreference resolution (Bean and Riloff, 2004;"
D15-1260,D07-1057,0,0.640123,"ally created heuristic rules (Kameyama, 1986; Walker et al., 1994; Okumura and Tamura, 1996; Nakaiwa and Shirai, 1996), which are mainly motivated by the rules and preferences introduced in Centering Theory (Grosz et al., 1995). However, the research trend of zero anaphora resolution has shifted from such rule-based approaches to machine learningbased approaches because in machine learning we can easily integrate many different types of information, such as morpho-syntactic, semantic and discourse-related information. Researchers have developed methods of zero anaphora resolution for Chinese (Zhao and Ng, 2007; Chen and Ng, 2013), Japanese (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Sasano et al., 2008; Sasano et al., 2009; Imamura 2180 et al., 2009; Watanabe et al., 2010; Hayashibe et al., 2011; Iida and Poesio, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013; Yoshino et al., 2013) and Italian (Iida and Poesio, 2011). One critical issue in zero anaphora resolution is optimizing the outputs of sub-problems (e.g., zero anaphor detection and antecedent identification). Recent works by Watanabe et al. (2010), Iida and Poesio (2011) and Yoshikawa et al. (2011)"
D15-1260,N04-1038,0,\N,Missing
D15-1260,P12-1041,0,\N,Missing
D16-1132,P15-1026,0,0.0321947,"cture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent, ⟨predi , candi ⟩, that appears in a target sentence. Step 2 Predict the probability of each pair using our MCNN. Step 3 Rank in descending order all the pairs by their probabilities obtained in Step 2. Step 4 Cho"
D16-1132,P15-1061,0,0.0128412,". (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent, ⟨predi , candi ⟩, that a"
D16-1132,J95-2003,0,0.781794,"eve such high precision is crucial to realworld applications, even though the recall remains low, and thus our method is preferable to Ouchi et al.’s method in that sense. In our proposed method, we use a Multi-column Convolutional Neural Network (MCNN) (Cires¸an et al., 2012), which is a variant of a Convolutional Neural Network (CNN) (LeCun et al., 1998). An MCNN has several independent columns, each of which has its own convolutional and pooling layers. The outputs of all the columns are combined in the final layer to provide a final prediction. In this work, motivated by Centering Theory (Grosz et al., 1995) and other previous works, we exploit as distinct columns the word sequences obtained from the surface word 1245 sequence and the dependency tree of a target sentence in our MCNN. Although the existing works also exploited such word sequences, they used only particular types of information from them as features based on the researchers’ linguistic insights. In contrast, we minimized such feature engineering due to using an MCNN. The rest of this paper is organized as follows. In Section 2, we briefly overview previous work on zero anaphora resolution. In Section 3, we present the procedure of"
D16-1132,I11-1023,0,0.150923,"tion method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting other structural fragments in the dependency tree"
D16-1132,P11-1081,1,0.949442,"tial) zero anaphoric relations in the same sentence. The final determination of zero anaphoric relations for each zero anaphor in a given sentence is done in a greedy way; only the most likely candidate antecedent for each zero anaphor is selected as its antecedent as far as the likelihood score exceeds a 1244 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1244–1254, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics given threshold. This approach contrasts with global optimization methods (Yoshikawa et al., 2011; Iida and Poesio, 2011; Ouchi et al., 2015), which have recently become popular. These methods use the constraints among possible zero anaphoric relations, such as “if a candidate antecedent is identified as the antecedent of a subject zero anaphor of a predicate, the candidate cannot be referred to by the object zero anaphor of the same predicate”, and determine an optimal set of zero anaphoric relations in an entire sentence while satisfying such constraints, using such optimization techniques as sentence-wise global learning (Ouchi et al., 2015) and integer linear programming (Iida and Poesio, 2011). Although th"
D16-1132,W03-2604,1,0.730012,"briefly overview previous work on zero anaphora resolution. In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the de"
D16-1132,P06-1079,1,0.915957,"ora resolution. In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a can"
D16-1132,W07-1522,1,0.889983,"tated in the NAIST Text Corpus. In this revision phase, both the subject sharing and zero anaphora relations for such suspicious instances were independently re-annotated by three annotators, and their final labels of both relations were determined by a majority of the their decisions.3 As a result, 2,120 zero anaphoric instances were newly added to the corpus and 1,184 instances were removed from it for a total of 19,049 instances of intra-sentential subject zero anaphoric relations.4 3 In our preliminary investigation of the intrasentential zero anaphoric relations in the NAIST Text Corpus (Iida et al., 2007), since we found more annotation errors than we expected, we decided to 2 We use zero padding for dealing with text fragments of variable length (Kim, 2014). 1249 We are planning to release the annotated results and information on the data separation used in our evaluation from https://alaginrc.nict.go.jp/. 4 After this revision, a small number of inconsistent annotated results have both a syntactically dependent subject and a subject zero anaphor because the revision was performed locally. There were 30 inconsistent instances in the testing set and 100 in the training and development sets. We"
D16-1132,D15-1260,1,0.911685,"he rules and principles regarding the recency and saliency of candidate antecedents. Okumura and Tamura (1996) developed a rule-based method based on the idea of Centering Theory. Iida et al. (2003) and Imamura et al. (2009) used as features for machine learning the results of rule-based antecedent identification based on a variant of Centering Theory (Nariyama, 2002). However, we observed that actual anaphoric phenomena often do not obey Centering Theory. To robustly resolve zero anaphora, we need to explore additional clues that are represented in a target sentence (or text). Recent work by Iida et al. (2015) newly introduced a sub-problem of zero anaphora resolution, subject sharing recognition, which is the task that judges whether two predicates have the same subject. In their method, a network of subject sharing predicates is created by their subject sharing recognizer, and then zero anaphora resolution is performed by propagating a subject to the unrealized subject positions through the path in the network. Even though the accuracy of subject sharing recognition exceeds that of zero anaphora resolution, the zero anaphoric relations identified using the results of subject sharing recognition a"
D16-1132,P09-2022,0,0.464996,"r zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting other structural fragments"
D16-1132,W03-1024,0,0.698287,"evious work on zero anaphora resolution. In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a"
D16-1132,N15-1011,0,0.0101775,"tential zero anaphoric relations) as features to directly decide more than one predicate-argument relation simultaneously. We adopted Ouchi et al. (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our propose"
D16-1132,P14-1062,0,0.0138566,"tions (e.g., the combination of two potential zero anaphoric relations) as features to directly decide more than one predicate-argument relation simultaneously. We adopted Ouchi et al. (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at i"
D16-1132,D14-1181,0,0.0411336,"n of two potential zero anaphoric relations) as features to directly decide more than one predicate-argument relation simultaneously. We adopted Ouchi et al. (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 P"
D16-1132,W04-3230,0,0.0574198,"of 0.5 to the final layer. We used an SGD with mini-batches of 100 and a learning rate decay of 0.95. We ran ten epochs through all of the training data, where each epoch consisted of many mini-batch updates. We utilized 3-, 4- and 5-grams with 100 filters each and used the F-score of positive instances as our evaluation metric. The total number of the nodes in the final layers of our MCNN was 3,300: 11 columns × 3 N -gram × 100 filters. Word segmentation, PoS tagging and dependency parsing of the sentences in the NAIST Text Corpus were performed by a Japanese morphological analyzer, MeCab8 (Kudo et al., 2004), and a depentwo sets. 5 Words occurring less than five times in all the sentences were ignored to train the word embedding vectors. 6 We set the skip distance to 5 and the number of negative samples to 10. 7 https://archive.org/details/jawiki-20150118 8 http://taku910.github.io/mecab/ 1250 We compared our method with three baseline methods. The first baseline is a single-column convolutional neural network in which the column includes the entire surface word sequence of a sentence. To give the positions of predi and candi to the network, we concatenated to each word vector an additional 2-dim"
D16-1132,2002.tmi-papers.15,0,0.0682717,"method uses the text fragments that cover the entire dependency tree. Another important clue was derived from discourse theories, such as Centering Theory (Grosz et al., 1995). In this theory, (zero) anaphoric phenomenon is explained based on the rules and principles regarding the recency and saliency of candidate antecedents. Okumura and Tamura (1996) developed a rule-based method based on the idea of Centering Theory. Iida et al. (2003) and Imamura et al. (2009) used as features for machine learning the results of rule-based antecedent identification based on a variant of Centering Theory (Nariyama, 2002). However, we observed that actual anaphoric phenomena often do not obey Centering Theory. To robustly resolve zero anaphora, we need to explore additional clues that are represented in a target sentence (or text). Recent work by Iida et al. (2015) newly introduced a sub-problem of zero anaphora resolution, subject sharing recognition, which is the task that judges whether two predicates have the same subject. In their method, a network of subject sharing predicates is created by their subject sharing recognizer, and then zero anaphora resolution is performed by propagating a subject to the un"
D16-1132,P04-1020,0,0.0182292,"ished as a subtask of coreference recency can be estimated by consulting the surface resolution. This problem was basically solved by word sequence between France and increase: no exploring the possible candidate antecedents for a other salient candidates are included in the word se- given anaphor candidate in its search space, and the quence. Also, the other two types of word sequences results were used for improving the overall perfor(i.e., the sequence that spans from the beginning of mance of coreference resolution, especially in Enthe sentence to candi and that spans from predi to glish (Ng, 2004; Wiseman et al., 2015). Inspired its end) are important for confirming whether a more by such previous works, we designed the P RED salient candidate than candi appears in each word C ONTEXT set to determine the anaphoricity of zero sequence. If such a more salient candidate is found, anaphors, i.e., to judge whether a zero anaphor canit should be a stronger candidate of the antecedent. didate has its antecedent in a sentence, by consulting The D EP T REE column set is introduced for cap- the surface word sequences before and after predi . turing a different aspect of intra-sentential zero an"
D16-1132,W15-1506,0,0.0142481,"a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent, ⟨predi , candi ⟩, that appears in a target sentence."
D16-1132,C96-2147,0,0.531054,"iminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting other structural fragments in the dependency tree representing a target sentence, whereas our method uses the text fragments that cover the entire dependency tree. Another important clue was derived from discourse theories, such as Centering Theory (Grosz et al., 1995). In this theory, (zero) anaphoric phenomenon is explained based on the rules and principles regarding the recency and saliency of candidate antecedents. Okumura and Tamura (1996) developed a rule-based method based on the idea of Centering Theory. Iida et al. (2003) and Imamura et al. (2009) used as features for machine learning the results of rule-based antecedent identification based on a variant of Centering Theory (Nariyama, 2002). However, we observed that actual anaphoric phenomena often do not obey Centering Theory. To robustly resolve zero anaphora, we need to explore additional clues that are represented in a target sentence (or text). Recent work by Iida et al. (2015) newly introduced a sub-problem of zero anaphora resolution, subject sharing recognition, wh"
D16-1132,P15-1093,0,0.461032,"lations in the same sentence. The final determination of zero anaphoric relations for each zero anaphor in a given sentence is done in a greedy way; only the most likely candidate antecedent for each zero anaphor is selected as its antecedent as far as the likelihood score exceeds a 1244 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1244–1254, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics given threshold. This approach contrasts with global optimization methods (Yoshikawa et al., 2011; Iida and Poesio, 2011; Ouchi et al., 2015), which have recently become popular. These methods use the constraints among possible zero anaphoric relations, such as “if a candidate antecedent is identified as the antecedent of a subject zero anaphor of a predicate, the candidate cannot be referred to by the object zero anaphor of the same predicate”, and determine an optimal set of zero anaphoric relations in an entire sentence while satisfying such constraints, using such optimization techniques as sentence-wise global learning (Ouchi et al., 2015) and integer linear programming (Iida and Poesio, 2011). Although the global optimization"
D16-1132,I11-1085,0,0.700523,"our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting other structural fragments in the dependency tree representing a target sentence, whereas our method"
D16-1132,C08-1097,0,0.267866,"t the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods just focus on the dependency paths between a predicate and a candidate antecedent without exploiting othe"
D16-1132,C02-1078,0,0.332527,". In Section 2, we briefly overview previous work on zero anaphora resolution. In Section 3, we present the procedure of our zero anaphora resolution method and explain the column sets used in our MCNN architecture. We evaluate how effectively our method recognizes intra-sentential subject zero anaphora in Section 4 and summarize this work and discuss future directions in Section 5. 2 Related work The typical zero anaphora resolution algorithms proposed so far have exploited the information of a predicate that potentially has a zero anaphor and its candidate antecedent in a supervised manner (Seki et al., 2002; Iida et al., 2003; Isozaki and Hirao, 2003; Iida et al., 2006; Taira et al., 2008; Sasano et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Yoshikawa et al., 2011). In addition, existing works have exploited the dependency path between a predicate and a candidate antecedent either by encoding such paths to the set of binary features of the words that appear in the path (Iida and Poesio, 2011) or by mining from the paths the sub-trees that effectively discriminate zero anaphoric relations (Iida et al., 2006). However, both methods j"
D16-1132,D08-1055,0,0.761888,"Missing"
D16-1132,P15-1137,0,0.0245863,"subtask of coreference recency can be estimated by consulting the surface resolution. This problem was basically solved by word sequence between France and increase: no exploring the possible candidate antecedents for a other salient candidates are included in the word se- given anaphor candidate in its search space, and the quence. Also, the other two types of word sequences results were used for improving the overall perfor(i.e., the sequence that spans from the beginning of mance of coreference resolution, especially in Enthe sentence to candi and that spans from predi to glish (Ng, 2004; Wiseman et al., 2015). Inspired its end) are important for confirming whether a more by such previous works, we designed the P RED salient candidate than candi appears in each word C ONTEXT set to determine the anaphoricity of zero sequence. If such a more salient candidate is found, anaphors, i.e., to judge whether a zero anaphor canit should be a stronger candidate of the antecedent. didate has its antecedent in a sentence, by consulting The D EP T REE column set is introduced for cap- the surface word sequences before and after predi . turing a different aspect of intra-sentential zero anaphora. In the explanat"
D16-1132,N15-1091,0,0.0611928,"Missing"
D16-1132,I11-1126,0,0.522837,"idering the other (potential) zero anaphoric relations in the same sentence. The final determination of zero anaphoric relations for each zero anaphor in a given sentence is done in a greedy way; only the most likely candidate antecedent for each zero anaphor is selected as its antecedent as far as the likelihood score exceeds a 1244 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1244–1254, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics given threshold. This approach contrasts with global optimization methods (Yoshikawa et al., 2011; Iida and Poesio, 2011; Ouchi et al., 2015), which have recently become popular. These methods use the constraints among possible zero anaphoric relations, such as “if a candidate antecedent is identified as the antecedent of a subject zero anaphor of a predicate, the candidate cannot be referred to by the object zero anaphor of the same predicate”, and determine an optimal set of zero anaphoric relations in an entire sentence while satisfying such constraints, using such optimization techniques as sentence-wise global learning (Ouchi et al., 2015) and integer linear programming (Iida and Poe"
D16-1132,D09-1160,0,0.094722,"tion on the data separation used in our evaluation from https://alaginrc.nict.go.jp/. 4 After this revision, a small number of inconsistent annotated results have both a syntactically dependent subject and a subject zero anaphor because the revision was performed locally. There were 30 inconsistent instances in the testing set and 100 in the training and development sets. We only removed such instances from the testing set without changing the other Type #docs #sentences train dev test 1,757 586 586 23,152 7,526 7,705 #zero anaphors (intra-sentential) 11,453 3,691 3,875 dency parser, J.DepP9 (Yoshinaga and Kitsuregawa, 2009). 4.3 Baselines Table 1: Statistics of our data set 4.2 Experimental settings The documents in the corpus were divided into five subsets, three of which were used as a training data set, one as a development data set, and one as a testing data set. The statistics of our data set are summarized in Table 1. We evaluated the performance of our intra-sentential subject zero anaphora resolution method and three baseline methods described below using the revised annotated results in our data set. We implemented our MCNN using Theano (Bastien et al., 2012). We pre-trained 300dimensional word embeddin"
D16-1132,C14-1220,0,0.0304154,"We adopted Ouchi et al. (2015)’s method as a baseline in Section 4 because it achieved the state-of-the-art performance for intra-sentential zero anaphora resolution. Collobert et al. (2011) proposed CNN architecture that can be applied to various NLP tasks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent,"
D16-1132,D15-1203,0,0.0287412,"ks, such as PoS tagging, chunking, named entity recognition 1246 and semantic role labeling. Following this work, CNNs have been utilized in such NLP tasks as document classification (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2015), paraphrase (Hu et al., 2014; Yin and Sch¨utze, 2015) and relation extraction (Liu et al., 2013; Zeng et al., 2014; dos Santos et al., 2015; Nguyen and Grishman, 2015). MCNNs were first introduced for image classification (Cires¸an et al., 2012). In NLP tasks, they have been utilized for question-answering (Dong et al., 2015) and relation extraction (Zeng et al., 2015). Our MCNN architecture was inspired by a Siamese architecture (Chopra et al., 2005), which we extend to a multi-column network and replace its similarity measure with a softmax function at its top. 3 Proposed method Our proposed method consists of the following four steps: Step 1 Extract every pair of a predicate and a candidate antecedent, ⟨predi , candi ⟩, that appears in a target sentence. Step 2 Predict the probability of each pair using our MCNN. Step 3 Rank in descending order all the pairs by their probabilities obtained in Step 2. Step 4 Choose the top pair ⟨predi , candi ⟩ in the ran"
D19-1590,N19-1423,0,0.189337,"rds (Torisawa, 5816 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5816–5822, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2006; Riaz and Girju, 2010; Do et al., 2011), semantic polarities (Hashimoto et al., 2012), answers obtained from a web-based open-domain why-QA system and other causality related texts (Kruengkrai et al., 2017), and causality-related word embeddings (Xie and Mu, 2019). In this work, we investigate whether BERT (Devlin et al., 2019) (especially its pre-training) enables novel ways to exploit background knowledge. Our assumption is that if a BERT model is pre-trained using a large amount of causalityrich texts, it can learn some sort of background knowledge from the text. If the pre-training is adequately performed, background knowledge in the form of text fragments and a special mechanism for dealing with them might become obsolete. Our experimental results show that a BERT model pre-trained with causality-rich texts achieved significantly better performance than models using Wikipedia articles or randomly sampled web te"
D19-1590,D11-1027,0,0.272643,"Missing"
D19-1590,C16-1168,0,0.0440712,"Missing"
D19-1590,D12-1057,1,0.874603,"Missing"
D19-1590,P14-1093,1,0.907121,"rmance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction. 1 Introduction Event causality, such as “smoke cigarettes” → “die of lung cancer,” is critical knowledge for NLP applications such as machine reading (Rajpurkar et al., 2016). For the task of recognizing event causality written in web texts, we propose new BERT-based methods that exploit independent labels in a gold dataset provided by multiple annotators. In the creation of the dataset we used (Hashimoto et al., 2014), three annotators independently labeled the data and the final labels were determined by majority vote. In the previous work, each annotator’s independent judgments were ignored, but in our proposed method, we exploit each annotator’s judgments in predicting the majority vote labels. The dataset we used had a reasonable degree of inter-annotator agreement (Fleiss’ Kappa value was 0.67), but a discrepancy remained among the annotators. Despite this discrepancy, we assume that their judgments are more or less consistent and that we can improve performance by training multiple classifiers, each"
D19-1590,D15-1035,0,0.0128885,"a discrepancy remained among the annotators. Despite this discrepancy, we assume that their judgments are more or less consistent and that we can improve performance by training multiple classifiers, each from the labels provided by an individual annotator to grasp her/his policy, and by combining the resulting outputs of these classifiers. Researchers have studied how to exploit the differences between the behaviors of annotators and crowd workers to improve the quality of gold datasets (Snow et al., 2008; Zaidan and CallisonBurch, 2011; Zhou et al., 2012; Jurgens, 2013; Plank et al., 2014; Jamison and Gurevych, 2015; Felt et al., 2016; Li et al., 2017). In an attempt that resembles ours, one study (Jamison and Gurevych, 2015) successfully improved the performance of several NLP tasks by computing the agreement ratio of each training instance and using only those instances with high agreement. Another work (Plank et al., 2014) improved part-of-speech tagging by measuring the inter-annotator agreement on a small number of sampled data and incorporating this value during training via a modified loss function. However, neither of them directly used each annotator’s judgments, as we did in this work. As anoth"
D19-1590,N13-1062,0,0.0233135,"(Fleiss’ Kappa value was 0.67), but a discrepancy remained among the annotators. Despite this discrepancy, we assume that their judgments are more or less consistent and that we can improve performance by training multiple classifiers, each from the labels provided by an individual annotator to grasp her/his policy, and by combining the resulting outputs of these classifiers. Researchers have studied how to exploit the differences between the behaviors of annotators and crowd workers to improve the quality of gold datasets (Snow et al., 2008; Zaidan and CallisonBurch, 2011; Zhou et al., 2012; Jurgens, 2013; Plank et al., 2014; Jamison and Gurevych, 2015; Felt et al., 2016; Li et al., 2017). In an attempt that resembles ours, one study (Jamison and Gurevych, 2015) successfully improved the performance of several NLP tasks by computing the agreement ratio of each training instance and using only those instances with high agreement. Another work (Plank et al., 2014) improved part-of-speech tagging by measuring the inter-annotator agreement on a small number of sampled data and incorporating this value during training via a modified loss function. However, neither of them directly used each annotat"
D19-1590,P19-1414,1,0.849315,"Missing"
D19-1590,P13-1170,1,0.946064,"Missing"
D19-1590,E14-1078,0,0.0316585,"value was 0.67), but a discrepancy remained among the annotators. Despite this discrepancy, we assume that their judgments are more or less consistent and that we can improve performance by training multiple classifiers, each from the labels provided by an individual annotator to grasp her/his policy, and by combining the resulting outputs of these classifiers. Researchers have studied how to exploit the differences between the behaviors of annotators and crowd workers to improve the quality of gold datasets (Snow et al., 2008; Zaidan and CallisonBurch, 2011; Zhou et al., 2012; Jurgens, 2013; Plank et al., 2014; Jamison and Gurevych, 2015; Felt et al., 2016; Li et al., 2017). In an attempt that resembles ours, one study (Jamison and Gurevych, 2015) successfully improved the performance of several NLP tasks by computing the agreement ratio of each training instance and using only those instances with high agreement. Another work (Plank et al., 2014) improved part-of-speech tagging by measuring the inter-annotator agreement on a small number of sampled data and incorporating this value during training via a modified loss function. However, neither of them directly used each annotator’s judgments, as w"
D19-1590,D16-1264,0,0.0256425,"erformance improved when we pretrained the BERT models with web texts containing a large number of event causalities instead of Wikipedia articles or randomly sampled web texts. However, this effect was limited. Therefore, we further improved performance by simply adding texts related to an input causality candidate as background knowledge to the input of the BERT models. We believe these findings indicate a promising future research direction. 1 Introduction Event causality, such as “smoke cigarettes” → “die of lung cancer,” is critical knowledge for NLP applications such as machine reading (Rajpurkar et al., 2016). For the task of recognizing event causality written in web texts, we propose new BERT-based methods that exploit independent labels in a gold dataset provided by multiple annotators. In the creation of the dataset we used (Hashimoto et al., 2014), three annotators independently labeled the data and the final labels were determined by majority vote. In the previous work, each annotator’s independent judgments were ignored, but in our proposed method, we exploit each annotator’s judgments in predicting the majority vote labels. The dataset we used had a reasonable degree of inter-annotator agr"
D19-1590,D08-1027,0,0.0785427,"Missing"
D19-1590,N06-1008,1,0.756024,"Missing"
D19-1590,P11-1122,0,0.0740377,"Missing"
H05-1018,P04-1054,0,0.116891,"788 84.79 λ = 0.25, C = 8.647 86.20 λ = 0.20, C = 3.344 83.58 λ = 0.15, C = 20.57 91.11 λ = 0.15, C = 13.95 89.59 F1 (test) 87.90 85.56 84.70 83.51 86.64 83.72 92.92 91.32 marking where we determine the mark (tag) of a node. Kashima and Koyanagi (2002) dealt with this task by inserting the node representing the mark above the node to be tagged and classifying the transformed tree using SVMs with tree kernels such as Klo . For the SRL task, Moschitti (2004) applied the tree kernel (Kc ) to tree fragments that are heuristically extracted to reflect the role of interest. For relation extraction, Culotta and Sorensen (2004) proposed a tree kernel that operates on only the smallest tree fragment including two entities for which a relation is assigned. Our kernels on marked labeled ordered trees differ in what subtrees are permitted. Although comparisons are needed, we think our kernels are intuitive and general. There are many possible structures for which tree kernels can be defined. Shen et al. (2003) proposed a tree kernel for LTAG derivation trees to focus only on linguistically meaningful structures. Culotta and Sorensen (2004) proposed a tree kernel for dependency trees. An important future task is to find"
H05-1018,J02-3001,0,0.176394,", 1995). Previous studies (Collins and Duffy, 2001; Kashima and Koyanagi, 2002) showed that although it is difficult to explicitly calculate the inner product in Eq. (1) because we need to consider an exponential number of possible subtrees, the tree kernels can be computed in O(|T1 ||T2 |) time by using dynamic programming (DP) procedures. However, these DP procedures are time-consuming in practice. In this paper, we present a method for speeding up the training with tree kernels. Our target application is node relation labeling, which includes NLP tasks such as semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004). For this purpose, we designed kernels on marked labeled ordered trees and derived O(|T1 ||T2 |) procedures. However, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast. Our speed-up method is based on the observation that very few pairs in the training set have a great many common subtrees (we call such pairs malicious pairs) and most pairs have a very small number of common subtrees. This leads to a drastic variance in kernel va"
H05-1018,W04-2416,0,0.042368,", 2001; Kashima and Koyanagi, 2002) showed that although it is difficult to explicitly calculate the inner product in Eq. (1) because we need to consider an exponential number of possible subtrees, the tree kernels can be computed in O(|T1 ||T2 |) time by using dynamic programming (DP) procedures. However, these DP procedures are time-consuming in practice. In this paper, we present a method for speeding up the training with tree kernels. Our target application is node relation labeling, which includes NLP tasks such as semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004). For this purpose, we designed kernels on marked labeled ordered trees and derived O(|T1 ||T2 |) procedures. However, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast. Our speed-up method is based on the observation that very few pairs in the training set have a great many common subtrees (we call such pairs malicious pairs) and most pairs have a very small number of common subtrees. This leads to a drastic variance in kernel values, e.g., when W (Si ) = 1. We thus cal"
H05-1018,P03-1004,0,0.18425,"ate the kernel values with all the training examples for a given example Ti : KS(Ti ) = {K(Ti , T1 ), . . . , K(Ti , TL )}, where L is the number of training examples. Using occurrence pattern OP (Fi ) = {(k, #Fi (Tk ))|#Fi (Tk ) 6= 0} preAlgorithm 4.2: FREQTM(D, R) Algorithm 4.1: C ALCULATE KS(Ti ) for each F such that #F (Ti ) 6= 0 do for each (j, #F (Tj )) ∈ OP (F ) do KS(j) ← KS(j) + W (F ) · #F (Ti ) · #F (Tj ) (A) for j = 1 to L do if (i, j) is malicious then KS(j) ← K(Ti , Tj ) (DP) pared beforehand, we can calculate KS(Ti ) efficiently (Algorithm 4.1). A similar technique was used in (Kudo and Matsumoto, 2003a) to speed up the calculation of inner products. We can show that the per-pair cost of Algorithm 4.1 is O(c1 Q + rm c2 |Ti ||Tj |), where Q is the average number of common feature subtrees in a tree pair, rm is the rate of malicious pairs, c1 and c2 are the constant factors for vector operations and DP operations. This cost is independent of the number of training examples. We expect from our observations that both Q and rm are very small and that c1 ¿ c2 . 4.2 Feature Subtree Enumeration with Malicious Pair Detection To detect malicious pairs and enumerate feature subtrees F (and to convert"
H05-1018,W04-3239,0,0.147842,"are very small and that c1 ¿ c2 . 4.2 Feature Subtree Enumeration with Malicious Pair Detection To detect malicious pairs and enumerate feature subtrees F (and to convert each tree to a feature vector), we developed an algorithm based on the FREQT algorithm (Asai et al., 2002). The FREQT algorithm can efficiently enumerate subtrees that are included (Definition 2.1) in more than a pre-specified number of trees in the training examples by generating candidate subtrees using right most expansions (RMEs). FREQT-based algorithms have recently been used in methods that treat subtrees as features (Kudo and Matsumoto, 2004; Kudo and Matsumoto, 2003b). To develop the algorithm, we made the definition of maliciousness more search-oriented since it is costly to check for maliciousness based on the exact number of common subtrees or the kernel values (i.e., by using the DP procedure for all L2 pairs). Whatever definition we use, the correctness is preserved as long as we do not fail to enumerate the subtrees that appear in the pairs we consider nonmalicious. First, we consider pairs (i, i) to always be malicious. Then, we use a FREQT search that enumerates the subtrees that are included in at least two trees as a b"
H05-1018,P04-1043,0,0.16701,"Collins and Duffy, 2001; Kashima and Koyanagi, 2002) showed that although it is difficult to explicitly calculate the inner product in Eq. (1) because we need to consider an exponential number of possible subtrees, the tree kernels can be computed in O(|T1 ||T2 |) time by using dynamic programming (DP) procedures. However, these DP procedures are time-consuming in practice. In this paper, we present a method for speeding up the training with tree kernels. Our target application is node relation labeling, which includes NLP tasks such as semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004). For this purpose, we designed kernels on marked labeled ordered trees and derived O(|T1 ||T2 |) procedures. However, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast. Our speed-up method is based on the observation that very few pairs in the training set have a great many common subtrees (we call such pairs malicious pairs) and most pairs have a very small number of common subtrees. This leads to a drastic variance in kernel values, e.g., when"
H05-1018,W03-1012,0,0.131291,"ith tree kernels such as Klo . For the SRL task, Moschitti (2004) applied the tree kernel (Kc ) to tree fragments that are heuristically extracted to reflect the role of interest. For relation extraction, Culotta and Sorensen (2004) proposed a tree kernel that operates on only the smallest tree fragment including two entities for which a relation is assigned. Our kernels on marked labeled ordered trees differ in what subtrees are permitted. Although comparisons are needed, we think our kernels are intuitive and general. There are many possible structures for which tree kernels can be defined. Shen et al. (2003) proposed a tree kernel for LTAG derivation trees to focus only on linguistically meaningful structures. Culotta and Sorensen (2004) proposed a tree kernel for dependency trees. An important future task is to find suitable structures for each task (the SRL task in our case). Our speed-up method will be beneficial as long as there is unbalanced similarity. 7 Conclusion We have presented a method for speeding up the training with tree kernels. Using the SRL task, we demonstrated that our speed-up method made the training substantially faster. training set size = 8,000 best setting F1 (dev) λ = 0"
H05-1018,P04-1016,0,0.0199587,"be made faster by exploiting the mark information for pruning. Although our method is not a complete solution in a classification setting, it might be in a clustering setting (in a sense it is training only). However, it is an open question whether unbalanced similarity, which is the key to our speed-up, is ubiquitous in NLP tasks and under what conditions our method scales better than the SVMs or other kernel-based methods. Several studies claim that learning using tree kernels and other convolution kernels tends to overfit and propose selecting or restricting features (Cumby and Roth, 2003; Suzuki et al., 2004; Kudo and Matsumoto, 2004). Sometimes, the classification becomes faster as a result (Suzuki et al., 2004; Kudo and Matsumoto, 2004). We do not disagree with these studies. The fact that small λ values resulted in the highest accuracy in our experiment implies that too large subtrees are not so useful. However, since this tendency depends on the task, we need to assess the performance of full tree kernels for comparison. In this sense, our method is of great importance. Node relation labeling is a generalization of node 6 We used 106 as the maximum number of iterations. r and K r . Table 4: C"
H05-1018,W05-0620,0,0.235376,"Missing"
I05-1010,P03-1001,0,0.0346989,"y understood or described, is one of such type of knowledge. For example, the attributes of car objects will be weight, engine, steering wheel, driving feel, and manufacturer. In other words, attributes are items whose values we want to know when we want to know about the object. More analytically, we tend to regard A as an attribute for objects of class C when A works as if function v = A(o), o ∈ C where v is necessary to us to identify o (especially to distinguish o from o (= o) ∈ C). Therefore, obvious applications of attributes are ones such as summarization [1,2] and question-answering [3]. Moreover, they can be useful as features in word clustering [4] or machine learning. Although the knowledge base for attributes can be prepared manually (e.g., WordNet [5]), problems are cost and coverage. To overcome these, we propose a method that automatically acquires attribute knowledge from the Web. To acquire the attributes for a given class, C (e.g., car ), the proposed method ﬁrst downloads documents that contain class label C (e.g., “car”) from the Web.1 We extract the candidates of attribute words from these documents and score them according to the statistics of words, lexico-syn"
I05-1010,W04-3221,0,0.0219875,"example, the attributes of car objects will be weight, engine, steering wheel, driving feel, and manufacturer. In other words, attributes are items whose values we want to know when we want to know about the object. More analytically, we tend to regard A as an attribute for objects of class C when A works as if function v = A(o), o ∈ C where v is necessary to us to identify o (especially to distinguish o from o (= o) ∈ C). Therefore, obvious applications of attributes are ones such as summarization [1,2] and question-answering [3]. Moreover, they can be useful as features in word clustering [4] or machine learning. Although the knowledge base for attributes can be prepared manually (e.g., WordNet [5]), problems are cost and coverage. To overcome these, we propose a method that automatically acquires attribute knowledge from the Web. To acquire the attributes for a given class, C (e.g., car ), the proposed method ﬁrst downloads documents that contain class label C (e.g., “car”) from the Web.1 We extract the candidates of attribute words from these documents and score them according to the statistics of words, lexico-syntactic patterns, and HTML tags. Highly scored words are output as"
I05-1010,C92-2082,0,0.0212173,"erage. To overcome these, we propose a method that automatically acquires attribute knowledge from the Web. To acquire the attributes for a given class, C (e.g., car ), the proposed method ﬁrst downloads documents that contain class label C (e.g., “car”) from the Web.1 We extract the candidates of attribute words from these documents and score them according to the statistics of words, lexico-syntactic patterns, and HTML tags. Highly scored words are output as attributes for the class. Lexico-syntactic patterns and other statistics have been used in other lexical knowledge acquisition systems [3,4,6,7,8]. We speciﬁcally used lexico-syntactic patterns involving the Japanese postposition “no” as used in [8] such as “C no A” where A is an attribute word, which is almost equivalent to pattern “A of C” used in [7] to 1 We use C to denote both the class and its class label (the word representing the class). We also use A to denote both the attribute and the word representing it. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 106–118, 2005. c Springer-Verlag Berlin Heidelberg 2005  Automatic Discovery of Attribute Words from Web Documents 107 ﬁnd part-whole relations. Novel features of our meth"
I05-1010,P99-1008,0,0.116221,"erage. To overcome these, we propose a method that automatically acquires attribute knowledge from the Web. To acquire the attributes for a given class, C (e.g., car ), the proposed method ﬁrst downloads documents that contain class label C (e.g., “car”) from the Web.1 We extract the candidates of attribute words from these documents and score them according to the statistics of words, lexico-syntactic patterns, and HTML tags. Highly scored words are output as attributes for the class. Lexico-syntactic patterns and other statistics have been used in other lexical knowledge acquisition systems [3,4,6,7,8]. We speciﬁcally used lexico-syntactic patterns involving the Japanese postposition “no” as used in [8] such as “C no A” where A is an attribute word, which is almost equivalent to pattern “A of C” used in [7] to 1 We use C to denote both the class and its class label (the word representing the class). We also use A to denote both the attribute and the word representing it. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 106–118, 2005. c Springer-Verlag Berlin Heidelberg 2005  Automatic Discovery of Attribute Words from Web Documents 107 ﬁnd part-whole relations. Novel features of our meth"
I05-1010,C00-1060,1,0.757873,"Missing"
I05-1010,N04-1010,1,0.701575,"Missing"
I08-2126,P03-1001,0,0.0267767,"Missing"
I08-2126,C92-2082,0,0.515763,"iller et al., 1990). Linguistic literature, e.g. (A.Cruse, 1998), distinguishes hyponymy relations, such as “national university” and “university”, and concept-instance relations, such as “Tokyo University” and “university”. However, we regard concept-instance Currently, most useful sources of hyponymy relations are hand-crafted thesauri, such as WordNet (Fellbaum, 1998). Such thesauri are highly reliable, but their coverage is not large and the costs of extension and maintenance is prohibitively high. To reduce these costs, many methods have been proposed for automatically building thesauri (Hearst, 1992; Etzioni et al., 2005; Shinzato and Torisawa, 2004; Pantel and Pennacchiotti, 2006). But often these methods need a huge amount of documents and computational resources to obtain a reasonable number of hyponymy relations, and we still do not have a thesaurus with sufficient coverage. In this paper, we attempt to extract a large number of hyponymy relations without a large document collection or great computational power. The key idea is to focus on Wikipedia2 , which is much more consistently organized than normal documents. Actually, some studies have already attempted to extract hyponymy re"
I08-2126,D07-1073,1,0.703864,"nd computational resources to obtain a reasonable number of hyponymy relations, and we still do not have a thesaurus with sufficient coverage. In this paper, we attempt to extract a large number of hyponymy relations without a large document collection or great computational power. The key idea is to focus on Wikipedia2 , which is much more consistently organized than normal documents. Actually, some studies have already attempted to extract hyponymy relations or semantic classifications from Wikipedia. Hyponymy relations were extracted from definition sentences (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007). Disambiguation of named entities was also attempted (Bunescu and Pasca, 2006). Category pages were used to extract semantic relations (Suchanek et al., 2007). Lexical patterns for semantic relations were learned (RuizCasado et al., 2005). The difference between our work and these attempts is that we focus on the hierarchical layout of normal articles in Wikipedia. For instance, the article titled “Penguin” is shown in Fig. 1(b). This article has a quite consistently organized hierarchical structure. The whole article is divided into the sections “Anatomy”, “Mating habits”, “Systematics and e"
I08-2126,P06-1015,0,0.0609126,"distinguishes hyponymy relations, such as “national university” and “university”, and concept-instance relations, such as “Tokyo University” and “university”. However, we regard concept-instance Currently, most useful sources of hyponymy relations are hand-crafted thesauri, such as WordNet (Fellbaum, 1998). Such thesauri are highly reliable, but their coverage is not large and the costs of extension and maintenance is prohibitively high. To reduce these costs, many methods have been proposed for automatically building thesauri (Hearst, 1992; Etzioni et al., 2005; Shinzato and Torisawa, 2004; Pantel and Pennacchiotti, 2006). But often these methods need a huge amount of documents and computational resources to obtain a reasonable number of hyponymy relations, and we still do not have a thesaurus with sufficient coverage. In this paper, we attempt to extract a large number of hyponymy relations without a large document collection or great computational power. The key idea is to focus on Wikipedia2 , which is much more consistently organized than normal documents. Actually, some studies have already attempted to extract hyponymy relations or semantic classifications from Wikipedia. Hyponymy relations were extracte"
I08-2126,N04-1010,1,0.904335,"rature, e.g. (A.Cruse, 1998), distinguishes hyponymy relations, such as “national university” and “university”, and concept-instance relations, such as “Tokyo University” and “university”. However, we regard concept-instance Currently, most useful sources of hyponymy relations are hand-crafted thesauri, such as WordNet (Fellbaum, 1998). Such thesauri are highly reliable, but their coverage is not large and the costs of extension and maintenance is prohibitively high. To reduce these costs, many methods have been proposed for automatically building thesauri (Hearst, 1992; Etzioni et al., 2005; Shinzato and Torisawa, 2004; Pantel and Pennacchiotti, 2006). But often these methods need a huge amount of documents and computational resources to obtain a reasonable number of hyponymy relations, and we still do not have a thesaurus with sufficient coverage. In this paper, we attempt to extract a large number of hyponymy relations without a large document collection or great computational power. The key idea is to focus on Wikipedia2 , which is much more consistently organized than normal documents. Actually, some studies have already attempted to extract hyponymy relations or semantic classifications from Wikipedia."
I08-2126,I05-1010,1,0.782305,"Missing"
I08-2126,E06-1002,0,\N,Missing
I08-2126,C86-1105,0,\N,Missing
I11-1035,D08-1017,0,0.0345737,"t. 3.2 New Features for POS Tagging We generate n-gram and lexicon features for POS tagging as well. In addition, the features that incorporate word clusters derived from a large autoanalyzed corpus (referred to as cluster features) are introduced. • For the development and test sets, we collect a lexicon using the entire training corpus and use it for feature generation. Because the lexicon is extracted from other sets, the weights for this feature will not be overestimated by the learning algorithm. This kind of cross-validation-like techniques are used in studies such as Collins (2002) and Martins et al. (2008) to avoid over-fitting to the training data. Our method can be considered as its application to lexicon extraction. Using the extracted lexicon, we generate lexicon features as follows. If a character sequence starting with character c0 matches some words in the lexicon, we greedily choose the longest such matching word w. Letting LEN (w) be the length (the number of characters) of w, we add the following feature for each character ck in c0 , c1 , ..., cLEN (w) : (b) P (ck )/LEN (w)-P OSs(w) Here, P (ck ) is the position number (i.e., k) of the character ck in w and P OSs(w) represents the POS"
I11-1035,P09-1058,1,0.736673,"Missing"
I11-1035,W03-1719,0,0.0122318,"81 0.9112 CTB7 0.8996 0.9017 0.9020 0.9019 0.9046 Table 7: Results of word segmentation POS tag method Baseline +(c) n-gram +(d) cluster +(e) lexicon +(c)+(d)+(e) CTB5 0.9318 0.9333 0.9350 0.9346 0.9359 CTB6 0.8999 0.9014 0.9026 0.9015 0.9048 CTB7 0.8937 0.8958 0.8959 0.8959 0.8985 POS tag method Baseline +(c) n-gram +(d) cluster +(e) lexicon +(c)+(d)+(e) Table 8: F1 results of segmentation and POS tagging (baseline model for word segmentation) Table 9: F1 results of segmentation and POS tagging (our best model for word segmentation) the words in the test set that are not in the training set (Sproat and Emerson, 2003). The development sets were used to obtain the optimal values of tunable parameters and feature configurations. The unlabeled data for our experiments were taken from the XIN_CMN portion of Chinese Gigaword Version 2.0 (LDC2009T14), which has approximately 311 million words. Some of CTB data and Chinese Gigaword data are from the same source: Xinhua newswire between 1994 and 1998. In order to avoid overlap between the CTB data and the unlabeled data, we used only the articles published in 1991- 1993 and 1999-2004 as unlabeled data, with 204 million words.8 Note that we only used one million wo"
I11-1035,Y06-1012,0,0.122089,"(and labeled) data into the above baseline models through features. We preprocess unlabeled data with our baseline models and obtain wordsegmented sentences with POS tags, and generate new features from the auto-analyzed data. Although the focus of the paper is semi-supervised learning, we also extract a lexicon from the training corpus and use it to generate features. Figure 1 shows an overview of our approach. The rest of this section describes our features in detail. Segmentation and POS tagging Models We implement our approach using sequential tagging models. Following the previous work (Zhao et al., 2006; Zhao et al., 2010), we employ the linear chain CRFs (Lafferty et al., 2001) as our learning model. Specifically, we use its CRF++ (version 0.54) implementation by Taku Kudo. 1 2.1 Baseline Segmentation Model 3.1 New features for Word Segmentation We employ character-based sequence labeling for word segmentation. In addition to its simplicity, the advantage of a character-based model is its robustness to the unknown word problem (Xue, 2003). In a character-based Chinese word segmentation task, a character in a given sequence is labeled by a tag that stands for its position in the word that th"
I11-1035,P08-1068,0,0.0436031,"Missing"
I11-1035,P07-2055,0,0.142272,"ry is correctly identified. For Seg &Tag, a word is considered correct only when both the word boundary and its POS tag are correctly identified. Table 13 summarizes the results on test sets. These tests suggest that although the difference from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was"
I11-1035,P08-1102,0,0.21839,"rence from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the version with Yates’ correction, using correction factor 0.5 315 Sentences added 0(Baseline) 5k 10k 30k 150k 300k 600k Segmentation F1 0.9498"
I11-1035,C08-1049,0,0.103149,"rence from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the version with Yates’ correction, using correction factor 0.5 315 Sentences added 0(Baseline) 5k 10k 30k 150k 300k 600k Segmentation F1 0.9498"
I11-1035,I08-1012,1,0.81753,"gging by incorporating large unlabeled data. We first preprocess unlabeled data with our baseline models. We then extract various items of dictionary information from the auto-analyzed data. Finally, we generate new features that incorporate the extracted information for both word segmentation and POS tagging. We also perform word clustering on the auto-segmented data and use word clusters as features in POS tagging. In addition, we introduce lexicon features by using a crossvalidation technique. The use of sub-structures from the autoannotated data has been presented previously (Noord, 2007; Chen et al., 2008; Chen et al., 2009). Chen et al. (2009) extracted different types of subtrees from the auto-parsed data and used them as new features in standard learning methods. They showed this simple method greatly improves the accuracy of dependency parsing. The idea of combining word clusters with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008) and dependency parsing (Koo et al., 2008). We adapt and extend these techniques to Chinese word segmentation and POS tagging, and demonstrate their effectiveness in"
I11-1035,D09-1060,1,0.381889,"Missing"
I11-1035,I08-4029,0,0.134356,"be a character n-gram (e.g., uni-gram ci , bi-gram ci ci+1 , trigram ci−1 ci ci+1 and so on)2 , and seg be a segmentation profile for n-gram g observed at each position. The segmentation profile can be tag ti or the combination of tags. Take a bi-gram for example, seg may be ti or ti ti+1 . Then, 2.2 Baseline POS Tagging Model Since we employ a pipelined method, the POS tagging can be performed as a word labeling task, where the input is a sequence of segmented words. We use a CRF here as well. The feature set of our baseline POS tagger, is listed in Table 3. These are adopted from Wu et al. (2008). 1 2 Note that there are several alternative ways for extracting n-grams at position i, for example ci−1 ci for a bi-gram. In this paper, we used the way as explained here. Available from http://crfpp.sourceforge.net/ 310 Feature Type Word Unigram Nearing Word Bigram Jump Word Bigram First Character Last Character Length Context Position w−2 ,w−1 ,w0 ,w1 ,w2 (w−2 w−1 ),(w−1 w0 ),(w1 w0 ),(w1 w2 ) (w−1 ,w1 ) F c(w0 ) Lc(w0 ) Len(w0 ) Description Word unigram Word bigram Previous word and next word First character of current word Last character of current word Length of current word Table 3: Fe"
I11-1035,D09-1058,0,0.0118399,"r-level NLP tasks such as parsing and information extraction. Although the performance of Chinese word segmentation and POS tagging has been greatly improved over the past years, the task is still challenging. To improve the accuracy of NLP systems, one of the current trends is semi-supervised learning, which utilizes large unlabeled data in supervised learning. Several studies have demonstrated that the use of unlabeled data can improve the performance of NLP tasks, such as text chunking (Ando and Zhang, 2005), POS tagging and named entity recognition (Suzuki and Isozaki, 2008), and parsing (Suzuki et al., 2009; Chen et al., 2009; Koo et al., 2008). Therefore, it is attractive to consider adopting semi-supervised learning in Chinese word segmentation and POS tagging tasks. 309 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 309–317, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Word Length Tags 1 S 2 BE 3 BB2 E 4 BB2 B3 E 5 BB2 B3 M E 6 BB2 B3 M M E 7 or more BB2 B3 M...M E Table 1: Word representation with a 6-tag tagset : S, B, B2 , B3 , M, E Type Character Unigram Nearing Character Bigram Jump Character Bigram Punctuation Character Type Feat"
I11-1035,D10-1082,0,0.4394,"boundary and its POS tag are correctly identified. Table 13 summarizes the results on test sets. These tests suggest that although the difference from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the ver"
I11-1035,N04-1043,0,\N,Missing
I11-1035,W07-2201,0,\N,Missing
I11-1035,P08-1047,1,\N,Missing
I11-1035,P08-1076,0,\N,Missing
I11-1035,O03-4002,0,\N,Missing
I11-1035,I05-3025,0,\N,Missing
I11-1035,W03-1726,0,\N,Missing
I11-1035,P02-1062,0,\N,Missing
I11-1035,N06-1020,0,\N,Missing
I11-1060,J92-4003,0,0.100893,"Missing"
I11-1060,N03-2003,0,0.0396781,"and the resulting speech recognition performance was worse than that obtained by a corpus randomly sampled from our Web archive. To achieve the performance compatible to our method, thorough research must be conducted on generating only natural questions. 6 Related Work The Web has been used as a relatively inexpensive source of large-scale data. “Just-in-time” language modeling (Berger et al., 1998) submits content words from previous user sentences as queries to a web search engine, Zhu et al. (2001) use a search engine to update the probabilities of already existing n-grams. More recently Bulyko et al. (2003) use frequent n-grams of the seed corpus as queries to retrieve similar text from the Web. Sarikaya et al. (2005) retrieves relevant text based on the BLEU score. Word perplexity is another frequently used similarity measure (Misu and Kawahara, 2006; Creutz et al., 2009). Some 7 Conclusions We have proposed a similarity based language model construction method for Ikkyu, a voice driven open-domain QA system. We used the combination of a distributional similarity based noun replacement method and a statistical domain adaptation method. Our best language model outperformed the baseline model con"
I11-1060,E09-1019,0,0.0309227,"Missing"
I11-1060,P01-1068,0,\N,Missing
I11-1060,P10-1026,1,\N,Missing
I11-1060,I08-1025,0,\N,Missing
I11-1098,I08-1025,1,0.92656,"ven corpus. In the E-step, probability P (a |< v, p &gt;) is calculated. In the M-step, probabilities P (< v, p &gt; |a), P (n|a), and P (a) are updated to arrive at the maximum likelihood using the results of the E-step. From the results of estimation by this EM-based clustering method, probabilities P (< v, p &gt; |a), P (n|a), and P (a) for < v, p &gt;, n, and a are obtained. P (a|n) is then calculated by the following equation: P (a|n) = P P (n|a)P (a) a∈A P (n|a)P (a) (5) With the aim of enabling large-scale clustering and using the resulting clusters in named entity recognition, Kazama and Torisawa (2008) proposed parallelization of this EM-based clustering method. Kazama et al. (2009) then reported the calculation of distributional similarity by using the clustering results. We applied their method to the TSUBAKI corpus (Shinzato et al., 2008), a collection of 100-million Japanese Web pages containing 6 × 109 sentences. We prepared 3.2.2 Measuring Distributional Similarity The distributional similarity between two terms (n1 and n2 ) is defined as sim(n1 , n2 ) = 1 − DJS (P (a|n1 )kP (a|n2 )) (3) 877 3.3 about 1,000,000 terms for calculating the distributional similarity. These one million ter"
I11-1098,P06-1101,0,0.225254,"rmance of the classifier when one of the features was ignored) were conducted. Table 3 lists the results of these tests. This table shows that Feature set All w/o Hyper (f1) w/o Name of SIM (f2) w/o Value of P S(I, syn) (f3) w/o Synset ID (f4) w/o Suffix of the trg (f5) w/o Suffix of the hyper (f6) freq. 336 336 132 68 44 44 28 25 25 23 22 22 19 17 17 4.6 Analysis 4.6.1 Advantage of Proposed Method The advantage of the proposed method compared to previous methods that use nothing but either distributional similarity of a trg or co-occurrence with their hypernyms via lexico-syntactic patterns (Snow et al., 2006; Yamada et al., 2009) (or both) was demonstrated as follows. Specifically, it was shown that many terms do not have reliable distributional similarity (owing to their infrequency in a corpus) and do not co-occur with their hypernyms via any lexico-syntactic pattern in a sentence. Even so, our method can correctly identify the synset of such terms thanks to their hyPrecision 82.6 (661/800) 82.1 (657/800) 82.5 (660/800) 82.0 (656/800) 81.1 (649/800) 82.1 (657/800) 82.4 (659/800) Table 3: Results of ablation test. 880 pers and sibs acquired from the internal structure of Wikipedia articles. Firs"
I11-1098,C92-2082,0,0.271005,"Missing"
I11-1098,P08-1047,1,0.858574,"lities by using a given corpus. In the E-step, probability P (a |< v, p &gt;) is calculated. In the M-step, probabilities P (< v, p &gt; |a), P (n|a), and P (a) are updated to arrive at the maximum likelihood using the results of the E-step. From the results of estimation by this EM-based clustering method, probabilities P (< v, p &gt; |a), P (n|a), and P (a) for < v, p &gt;, n, and a are obtained. P (a|n) is then calculated by the following equation: P (a|n) = P P (n|a)P (a) a∈A P (n|a)P (a) (5) With the aim of enabling large-scale clustering and using the resulting clusters in named entity recognition, Kazama and Torisawa (2008) proposed parallelization of this EM-based clustering method. Kazama et al. (2009) then reported the calculation of distributional similarity by using the clustering results. We applied their method to the TSUBAKI corpus (Shinzato et al., 2008), a collection of 100-million Japanese Web pages containing 6 × 109 sentences. We prepared 3.2.2 Measuring Distributional Similarity The distributional similarity between two terms (n1 and n2 ) is defined as sim(n1 , n2 ) = 1 − DJS (P (a|n1 )kP (a|n2 )) (3) 877 3.3 about 1,000,000 terms for calculating the distributional similarity. These one million ter"
I11-1098,I08-2126,1,0.899772,"Missing"
I11-1098,sumida-etal-2008-boosting,1,0.84964,"ned by the judges’ majority vote. The interrater agreement between the three judges (Siegel’s Kappa) was 0.785, indicating substantial agreement. We performed parameter optimization by using the development data. The parameters used in our method showing the best performance for development data were used in our experiments, namely, the number of similar terms k = 60, the parameter for score propagation λ = 0.6, and weights for S(n, syn) in P S(I, syn) (α = 0.6 and β = 0.4 for SIM2 and α = 0.5, β = 0.4, and γ = 0.1 for SIM3 ). 4.2 can be considered simple extensions of an existing research of Sumida et al. (2008) for estimating Wordnet synsets. Table 2 shows the precision rate of each system. We could not evaluate all 1,800 samples for B1, SB1, EB1 and EB2. B1 and SB1 were able to generate outputs for 614 Is, where trg in Is was included in the target terms for calculating the distributional similarity. EB1 and EB2 can select the synset when hyper or the suffix of hyper is registered in WordNet. For this reason, it was not possible to select a synset for 174 trgs out of the 1,800 samples in EB1 and EB2. As a result, we used 1,636 samples in evaluating EB1 and EB2. SB1–SB3, CB2, EB2, and the proposed m"
I11-1098,P09-1049,1,0.90431,"Missing"
I11-1098,toral-etal-2008-named,0,0.183001,"Missing"
I11-1098,R09-1080,0,0.0502895,"Missing"
I11-1098,C10-1095,1,0.813257,"Missing"
I11-1098,D09-1097,1,0.867635,"Missing"
I11-1098,N04-1010,1,0.875766,"Missing"
I11-1098,W09-3401,0,\N,Missing
I11-1101,P08-1004,0,0.294231,"tances, which are not written in any single sentence and may not be even written in a large corpus. We develop a method to infer new semantic relation instances by applying auto-discovered inference rules, and show that our method inferred a considerable number of valid instances that were not written in single sentences even in 600 million Web pages. 1 Most existing relation acquisition methods acquire relation instances using lexico-syntactic patterns (Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006; De Saeger et al., 2009) such as “X causes Y” or probabilistic sequence labeling models (Banko and Etzioni, 2008). These methods basically rely on the structure of single sentences and they are for acquiring SS instances. Thus we consider that NS instances are practically beyond their reach. A few attempts to overcome this limitation include inference-based methods. These methods take SS instances provided by other methods as input and infer relation instances including NS ones using auto-discovered inference rules (Schoenmackers et al., 2010; Carlson et al., 2010) or distributional similarities (Tsuchida et al., 2010). We consider such inference approaches to be promising for acquiring NS instances. Int"
I11-1101,D10-1106,0,0.134826,"exico-syntactic patterns (Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006; De Saeger et al., 2009) such as “X causes Y” or probabilistic sequence labeling models (Banko and Etzioni, 2008). These methods basically rely on the structure of single sentences and they are for acquiring SS instances. Thus we consider that NS instances are practically beyond their reach. A few attempts to overcome this limitation include inference-based methods. These methods take SS instances provided by other methods as input and infer relation instances including NS ones using auto-discovered inference rules (Schoenmackers et al., 2010; Carlson et al., 2010) or distributional similarities (Tsuchida et al., 2010). We consider such inference approaches to be promising for acquiring NS instances. Introduction Recent advances in automatic relation acquisition methods (Agichtein and Luis, 2001; Etzioni et al., 2004; Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006; Banko and Etzioni, 2008; De Saeger et al., 2009) have opened the way to build large knowledge bases containing a huge number of semantic relation instances such as “CAUSE(allergen, allergy)” and “PREVENTION(coffee, drowsiness)”. Such a massive knowledge base is val"
I11-1101,D11-1076,1,0.877572,"Missing"
I11-1101,P08-1047,1,0.850247,"cquire SS instances from highly complex and infrequent expressions, using word classes and lexico-syntactic pattern fragments, which they call partial patterns. Such approach may prove useful for acquiring NS instances too, as the method can acquire relation instances without considering any pattern connecting the two words of the instance. Yet their work focused only on SS instances. tions of the same pattern (e.g., “Y:products from X:company”, as in “iPhone from Apple”) may not yield a valid paraphrase of “X causes Y”. To obtain word classes they use a large-scale word clustering algorithm (Kazama and Torisawa, 2008), and rank each instance in the corpus according to a score based on the semantic similarity between the seed patterns and each class dependent pattern the instance co-occurs with. Although much work in this category successfully extracts the instances implicitly written in a single sentence based on a wide range of nontrivial evidence including paraphrases (e.g., “Y by X” for causality), the applicability of these methods is restricted to SS instances. In the second category, Schoenmackers et al. (2010), which is the most relevant to our work, takes relation instances provided by TextRunner ("
I11-1101,P06-1102,0,0.0646067,"Missing"
I11-1101,P06-1015,0,0.267203,"sed on semantic relations explicitly expressed in single sentences. Our goal in this work is to obtain valid non-single sentence relation instances, which are not written in any single sentence and may not be even written in a large corpus. We develop a method to infer new semantic relation instances by applying auto-discovered inference rules, and show that our method inferred a considerable number of valid instances that were not written in single sentences even in 600 million Web pages. 1 Most existing relation acquisition methods acquire relation instances using lexico-syntactic patterns (Pantel and Pennacchiotti, 2006; Pas¸ca et al., 2006; De Saeger et al., 2009) such as “X causes Y” or probabilistic sequence labeling models (Banko and Etzioni, 2008). These methods basically rely on the structure of single sentences and they are for acquiring SS instances. Thus we consider that NS instances are practically beyond their reach. A few attempts to overcome this limitation include inference-based methods. These methods take SS instances provided by other methods as input and infer relation instances including NS ones using auto-discovered inference rules (Schoenmackers et al., 2010; Carlson et al., 2010) or dis"
I13-2008,P13-1170,1,0.776241,"Missing"
I13-2008,I13-2012,1,\N,Missing
I13-2012,D12-1057,1,0.870597,"Missing"
I13-2012,I13-2008,1,0.880435,"Missing"
I13-2012,I11-1060,1,0.810839,"Missing"
I13-2012,P09-4001,0,\N,Missing
I13-2012,P13-1170,1,\N,Missing
I13-2012,I11-1101,1,\N,Missing
I13-2012,P13-1159,1,\N,Missing
L18-1556,doddington-etal-2004-automatic,0,0.0457969,"] while drinking coffee at a i caf´e. Table 1: Examples of -de zero anaphora in our data our first annotation scheme alone, while the Kyoto University Text Corpus (Kawahara et al., 2002), the largest existing resource that we are aware of, has only 333 instances of -de zero anaphora of the equivalent type. Table 1 shows a few illustrative examples of zero anaphora successfully collected in our work.1 2. Related work Anaphora or coreference has been annotated in several projects including Message Understanding Conference (MUC) (Hirschman and Chinchor, 1997), Automatic Context Extraction (ACE) (Doddington et al., 2004) and OntoNotes (Hovy et al., 2006), but zero anaphora is not annotated in their English corpora. The OntoNotes corpora for pro-drop languages like Chinese and Arabic contain coreference annotations for certain types of zero pronouns. They do not, however, include adjunct zero pronouns, which we deal with in this paper. Another kind of resources that are relevant to our work is annotated corpora of semantic roles or frame elements such as PropBank (Palmer et al., 2005) and FrameNet 3523 1 We only deal with intra-sentential anaphora in this paper. (Baker et al., 1998). In FrameNet, for example,"
L18-1556,N06-2015,0,0.0670484,"ble 1: Examples of -de zero anaphora in our data our first annotation scheme alone, while the Kyoto University Text Corpus (Kawahara et al., 2002), the largest existing resource that we are aware of, has only 333 instances of -de zero anaphora of the equivalent type. Table 1 shows a few illustrative examples of zero anaphora successfully collected in our work.1 2. Related work Anaphora or coreference has been annotated in several projects including Message Understanding Conference (MUC) (Hirschman and Chinchor, 1997), Automatic Context Extraction (ACE) (Doddington et al., 2004) and OntoNotes (Hovy et al., 2006), but zero anaphora is not annotated in their English corpora. The OntoNotes corpora for pro-drop languages like Chinese and Arabic contain coreference annotations for certain types of zero pronouns. They do not, however, include adjunct zero pronouns, which we deal with in this paper. Another kind of resources that are relevant to our work is annotated corpora of semantic roles or frame elements such as PropBank (Palmer et al., 2005) and FrameNet 3523 1 We only deal with intra-sentential anaphora in this paper. (Baker et al., 1998). In FrameNet, for example, frame elements that are not overtl"
L18-1556,W07-1522,1,0.614857,"ra-sentential anaphora in this paper. (Baker et al., 1998). In FrameNet, for example, frame elements that are not overtly encoded are annotated as Null Instantiation, some of which can be regarded as adjunct zero anaphors, although their antecedents are not annotated. As for Japanese resources, zero anaphora was annotated in 5,000 sentences of the Kyoto University Text Corpus (Kawahara et al., 2002), including -de zero anaphora, although its size is small; it has only 333 instances of intra-sentential -de zero anaphora. Zero anaphora is also annotated for 20,000 sentences in the NAIST corpus (Iida et al., 2007), but only for -ga (nominative), -o (accusative) and -ni (dative). While zero anaphora resolution has been recognized as an important task in pro-drop languages such as Chinese and Japanese, the task has been less prominent in languages like English, in which core arguments are usually realized as overt forms. However, adjuncts can be omitted in any language, and in such cases, they must be inferred from the context. For example, consider the following English sentence: Shortly after her arrival in Tokyo, she began her career as a journalist. Given this sentence, we can infer that she began he"
L18-1556,D16-1132,1,0.842636,"notate zero anaphora in a corpus to achieve our goal, such an approach might be inefficient when we are interested only in instances that are useful in a specific application. Given this, we propose two different annotation schemes in this paper. In the first scheme, annotators annotate QA instances that potentially involve zero anaphora. In the second scheme, annotators directly annotate noun-predicate pairs with regard to whether they are in a zero anaphora relationship. We evaluated the performance of these two schemes using an existing neural network-based zero anaphora resolution method (Iida et al., 2016). Our experimental results show that the first scheme achieved better performance when it is used to train a module for a QA system, suggesting that the effectiveness of an annotation scheme depends on applications even in a relatively well studied task like zero anaphora resolution. Conversely, the model trained with annotation results of the second scheme achieved better performance in identifying zero anaphora for sentences randomly sampled from a corpus. We collected 20,830 instances of -de zero anaphora with レンタルバイクを借りて島をまわる。I rent a motorcyclei and travel the island [by ∅ ]. i ソーラー発電が拡大す"
L18-1556,kawahara-etal-2002-construction,0,0.238288,"Missing"
L18-1556,W04-3230,0,0.0936705,"dicate. We also created a small third dataset for evaluation, which we refer to as General. For this dataset, sentences were randomly sampled from the four-billion-page web corpus. For each noun-predicate pair that was identified as not being in a dependency relationship, annotators annotated whether it was in a -de anaphora relationship or not. In order to restrict our data to Japanese body texts, we only used sentences that (i) have at least two postpositions and (ii) end with the Japanese full stop (。). To identify nouns and predicates to annotate, we used the morphological analyzer MeCab (Kudo et al., 2004), as well as the dependency parser J.DepP (Yoshinaga and Kitsuregawa, 2009). 5. Annotation results In this section, we describe annotation results obtained from the annotation tasks described above. Table 2 summarizes the sizes of our datasets as well as our annotation results. For each dataset, three annotators independently evaluate each instance, and the final judgment was determined by a majority vote. An annotator was sometimes replaced by another person after completing a set of 1,000 instances. The total numbers of annotators participated in our work, as well as the time required to cre"
L18-1556,C16-2055,1,0.845435,"ectly modify improve, but we can clearly see that it is the means of the action denoted by the predicate. Solving -de zero anaphora is useful in a variety of applications including question answering. For example, given sentence (4), to build a QA system that can correctly answer to the question ‘What can we improve medicine with?’, the system must resolve -de zero anaphora. More generally, -de anaphora resolution plays a crucial role when we would like to extract information like location and means that must be inferred from context.2 4. To generate datasets for annotation, we used WISDOM X (Mizuno et al., 2016), a question-answering system that our team has been developing.3 The factoid QA module of WISDOM X accepts a question in Japanese and returns nouns as answers, as well as original sentences from the web corpus that support the answers. An example is below. (5) Question AI-de nani-ga jitsugensuru AI-INSTR what-NOM be.realized ‘What will be realized by AI?’ Answer kaji-robotto ‘housekeeping robot’ Sentence AI-ga sarani hattensureba, AI-NOM further develop.if kaji-robotto-ga jitsugensuru daroo. housekeeping-robot-NOM be.realized will ‘If AI develops further, housekeeping robots will be brought i"
L18-1556,P17-1146,0,0.0327632,"Missing"
L18-1556,J05-1004,0,0.0726498,"ojects including Message Understanding Conference (MUC) (Hirschman and Chinchor, 1997), Automatic Context Extraction (ACE) (Doddington et al., 2004) and OntoNotes (Hovy et al., 2006), but zero anaphora is not annotated in their English corpora. The OntoNotes corpora for pro-drop languages like Chinese and Arabic contain coreference annotations for certain types of zero pronouns. They do not, however, include adjunct zero pronouns, which we deal with in this paper. Another kind of resources that are relevant to our work is annotated corpora of semantic roles or frame elements such as PropBank (Palmer et al., 2005) and FrameNet 3523 1 We only deal with intra-sentential anaphora in this paper. (Baker et al., 1998). In FrameNet, for example, frame elements that are not overtly encoded are annotated as Null Instantiation, some of which can be regarded as adjunct zero anaphors, although their antecedents are not annotated. As for Japanese resources, zero anaphora was annotated in 5,000 sentences of the Kyoto University Text Corpus (Kawahara et al., 2002), including -de zero anaphora, although its size is small; it has only 333 instances of intra-sentential -de zero anaphora. Zero anaphora is also annotated"
L18-1556,I08-1025,0,0.0334482,"ce, our target is limited to intrasentential anaphora. We created two datasets for annotation, QAAnnot and AllNouns, based on the QA instances generated by the procedure above. QAAnnot Annotators directly evaluate QA instances that potentially involve zero anaphora. This task can be simultaneously interpreted as both a QA evaluation task and a zero anaphora annotation task. For this task, we obtained 100,000 QA instances in the following manner. First, we generated questions for 10,000 nouns randomly sampled from the nouns that most frequently appear in the -de position in the TSUBAKI corpus (Shinzato et al., 2008) of 600 million web pages. Next, we randomly sampled questions according to the frequency distribution of the predicates; these questions were then input into WISDOM X until we obtained 100,000 QA instances. Finally, human annotators judge each QA instance for its correctness. AllNouns While QAAnnot may be optimized for QA, its special annotation scheme may have a negative impact on performance when it is used to train a model for identifying -de zero anaphora in general. To investigate this, we created the second dataset called AllNouns; for this dataset, we obtained 10,000 QA instances using"
L18-1556,D09-1160,0,0.071958,"Missing"
N04-1010,P99-1016,0,0.0892442,"however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical HTML documents o"
N04-1010,P03-1001,0,0.119246,"istic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical HTML documents on the WWW. The second consists of statis"
N04-1010,C92-2082,0,0.520684,". Our method, however, does not use such linguistic patterns, and we expect that our procedure can be applied to a wide range of expressions for which existing methods cannot be used. Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences. 1 Introduction The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003). The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical"
N04-1010,C00-1060,1,0.736545,"es, while the behavior of true hypernyms should be semantically similar to the hyponyms. If we rank the pairs of hypernym candidates and HCSs according to their semantic similarities, the low ranked pairs are likely to have an erroneous hypernym candidate. We can then obtain relatively precise hypernyms by discarding the low ranked pairs. The similarities are computed through the following steps. First, we parse all the texts in the local document set, and check the argument positions of verbs where hyponym candidates appear. (To parse texts, we use a downgraded version of an existing parser (Kanayama et al., 2000) throughout this work.) Let us denote the frequency of the hyponym candidates in an HCS C occupying an argument position p of a verb v as fhypo (C, p, v). Assume that all possible argument positions are denoted as {p1 , · · · , pl } and all the verbs as {v1 , · · · , vm }. We then define the co-occurrence vector of hyponym candidates as follows. hypov(C) = hfhypo (C, p1 , v1 ), fhypo (C, p2 , v1 ), · · · , fhypo (C, pl−1 , vm ), fhypo (C, pl , vm )i In the same way, we can define the co-occurrence vector of a hypernym candidate n. hyperv(n) = hf (n, p1 , v1 ), · · · , f (n, pl , vm )i Here, f"
N06-1008,N03-1003,0,0.0108701,"relying on word co-occurrence was proposed by Geffet and Dagan (Geffet and Dagan, 2005) but our method is simpler and we expect it to be applicable to a wider range of vocabularies. Research on the automatic acquisition of inference rules, paraphrases and entailments has received much attention. Previous attempts have used, for instance, the similarities between case frames (Lin and Pan57 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 57–64, c New York, June 2006. 2006 Association for Computational Linguistics tel, 2001), anchor words (Barzilay and Lee, 2003; Shinyama et al., 2002; Szepektor et al., 2004), and a web-based method (Szepektor et al., 2004; Geffet and Dagan, 2005). There is also a workshop devoted to this task (Dagan et al., 2005). The obtained accuracies have still been low, however, and we think searching for other clues, such as coordinated sentences and the bias we have just mentioned, is necessary. In addition, research has also been done on the acquisition of the temporal relations (Fujiki et al., 2003; Chklovski and Pantel, 2004) by using coordinated sentences as we did, but these works did not consider the implications betwee"
N06-1008,W04-3205,0,0.215603,"57–64, c New York, June 2006. 2006 Association for Computational Linguistics tel, 2001), anchor words (Barzilay and Lee, 2003; Shinyama et al., 2002; Szepektor et al., 2004), and a web-based method (Szepektor et al., 2004; Geffet and Dagan, 2005). There is also a workshop devoted to this task (Dagan et al., 2005). The obtained accuracies have still been low, however, and we think searching for other clues, such as coordinated sentences and the bias we have just mentioned, is necessary. In addition, research has also been done on the acquisition of the temporal relations (Fujiki et al., 2003; Chklovski and Pantel, 2004) by using coordinated sentences as we did, but these works did not consider the implications between events. 2 Algorithm with a SimpliÞed Score In the following, we begin by providing an overview of our algorithm. We specify the basic steps in the algorithm and the form of the rules to be acquired. We also examine the direction of implications and temporal ordering described by the rules. After that, we describe a simpliÞed version of the scoring function that our algorithm uses and then discuss a problem related to it. The bias mechanism, which we mentioned in the introduction, is described i"
N06-1008,E03-1061,0,0.0261621,"ter of the ACL, pages 57–64, c New York, June 2006. 2006 Association for Computational Linguistics tel, 2001), anchor words (Barzilay and Lee, 2003; Shinyama et al., 2002; Szepektor et al., 2004), and a web-based method (Szepektor et al., 2004; Geffet and Dagan, 2005). There is also a workshop devoted to this task (Dagan et al., 2005). The obtained accuracies have still been low, however, and we think searching for other clues, such as coordinated sentences and the bias we have just mentioned, is necessary. In addition, research has also been done on the acquisition of the temporal relations (Fujiki et al., 2003; Chklovski and Pantel, 2004) by using coordinated sentences as we did, but these works did not consider the implications between events. 2 Algorithm with a SimpliÞed Score In the following, we begin by providing an overview of our algorithm. We specify the basic steps in the algorithm and the form of the rules to be acquired. We also examine the direction of implications and temporal ordering described by the rules. After that, we describe a simpliÞed version of the scoring function that our algorithm uses and then discuss a problem related to it. The bias mechanism, which we mentioned in the"
N06-1008,P05-1014,0,0.0478378,"ences will be observed. However, it is weighted by a bias, which embodies our assumption that frequently observed verbs are likely to appear as the consequence of a proper inference rule. This is based on our intuition that frequently appearing verbs have a generic meaning and tend to describe a wide range of situations, and that natural language expressions referring to a wide range of situations are more likely to be a consequence of a proper rule than speciÞc expressions describing only a narrow range of events. A similar idea relying on word co-occurrence was proposed by Geffet and Dagan (Geffet and Dagan, 2005) but our method is simpler and we expect it to be applicable to a wider range of vocabularies. Research on the automatic acquisition of inference rules, paraphrases and entailments has received much attention. Previous attempts have used, for instance, the similarities between case frames (Lin and Pan57 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 57–64, c New York, June 2006. 2006 Association for Computational Linguistics tel, 2001), anchor words (Barzilay and Lee, 2003; Shinyama et al., 2002; Szepektor et al., 2004), and a web-based"
N06-1008,C00-1060,1,0.785819,"letely random manner and by 2) assuming that the occurrence probability of the event instances describable by exp2 can be approximated by the probability that exp2 is observed in text corpora. This means that P (exp2 ) is one of the metrics indicating how easily we can establish the mapping f in Φ. Then, the next question is what kind of expressions should be regarded as the event description exp2 . A 61 Settings We parsed 35 years of newspaper articles (Yomiuri 87-01, Mainichi 91-99, Nikkei 90-00, 3.24GB in total) and 92.6GB of HTML documents downloaded from the WWW using an existing parser (Kanayama et al., 2000) to obtain the word (co-occurrence) frequencies. All the probabilities used in our method were estimated by maximum likelihood estimation from these frequencies. We randomly picked 600 nouns as a development set. We prepared three test sets, namely test sets A, B, and C, which consisted of 100 nouns, 250 nouns and 1,000 nouns respectively. Note that all the nouns in the test sets were randomly picked and did not have any common items with the development set. In all the experiments, four human judges checked if each produced rule was a proper one without knowing how each rule was produced. 4.2"
N06-1008,W04-3206,0,0.126863,"Geffet and Dagan (Geffet and Dagan, 2005) but our method is simpler and we expect it to be applicable to a wider range of vocabularies. Research on the automatic acquisition of inference rules, paraphrases and entailments has received much attention. Previous attempts have used, for instance, the similarities between case frames (Lin and Pan57 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 57–64, c New York, June 2006. 2006 Association for Computational Linguistics tel, 2001), anchor words (Barzilay and Lee, 2003; Shinyama et al., 2002; Szepektor et al., 2004), and a web-based method (Szepektor et al., 2004; Geffet and Dagan, 2005). There is also a workshop devoted to this task (Dagan et al., 2005). The obtained accuracies have still been low, however, and we think searching for other clues, such as coordinated sentences and the bias we have just mentioned, is necessary. In addition, research has also been done on the acquisition of the temporal relations (Fujiki et al., 2003; Chklovski and Pantel, 2004) by using coordinated sentences as we did, but these works did not consider the implications between events. 2 Algorithm with a SimpliÞed Score In"
N13-1007,N03-1003,0,0.255464,"原文意思的前提下，完全改变原 文 的 句 子 结 构 。 (Paraphrasing refers to the transformation of sentence structure by the translator without changing the meaning of original text.) b. 意译是指只保持原文内容，不保持原文形式的翻译方法。 (Paraphrasing is a translation method of keeping the content of original text but not keeping the expression.) Figure 1: Multilingual definition pairs on “paraphrasing.” Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal paraphrases from the definition sentences. Both steps require supervised classifiers trained by manually ann"
N13-1007,P01-1008,0,0.129702,"ase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) 64 extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel corpus. However, our method does not suffer from a high labor cost of preparing parallel corpora, since it can automa"
N13-1007,D07-1017,0,0.0245304,"otation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) 64 extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition pairs as a parallel c"
N13-1007,D08-1021,0,0.21472,"efers to the transformation of sentence structure by the translator without changing the meaning of original text.) b. 意译是指只保持原文内容，不保持原文形式的翻译方法。 (Paraphrasing is a translation method of keeping the content of original text but not keeping the expression.) Figure 1: Multilingual definition pairs on “paraphrasing.” Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal paraphrases from the definition sentences. Both steps require supervised classifiers trained by manually annotated data, and heavily depend on their t"
N13-1007,D09-1060,1,0.872534,"Missing"
N13-1007,C04-1051,0,0.595873,"构 。 (Paraphrasing refers to the transformation of sentence structure by the translator without changing the meaning of original text.) b. 意译是指只保持原文内容，不保持原文形式的翻译方法。 (Paraphrasing is a translation method of keeping the content of original text but not keeping the expression.) Figure 1: Multilingual definition pairs on “paraphrasing.” Introduction Automatic paraphrasing has been recognized as an important component for NLP systems, and many methods have been proposed to acquire paraphrase knowledge (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008; Hashimoto et al., 2011; Fujita et al., 2012). We propose a minimally supervised method for multilingual paraphrase extraction. Hashimoto et al. (2011) developed a method to extract paraphrases from definition sentences on the Web, based on their observation that definition sentences defining the same concept tend to contain many paraphrases. Their method consists of two steps; they extract definition sentences from the Web, and extract phrasal paraphrases from the definition sentences. Both steps require supervised classifiers trained by manually annotated data, and hea"
N13-1007,P05-1045,0,0.0139239,"Missing"
N13-1007,D12-1058,0,0.338671,"Missing"
N13-1007,P05-1014,0,0.029409,"iminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) 64 extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from antonymous pairs (Lin et al., 2003). The latter rarely mistakes antonymous pairs for paraphrases, but preparing parallel corpora is expensive. As with Hashimoto et al. (2011), our method is a kind of parallel corpus approach in that it uses definition"
N13-1007,D09-1122,1,0.90277,"Missing"
N13-1007,P11-1109,1,0.835368,"Missing"
N13-1007,D07-1073,1,0.788208,"istinguishing definition from non-definition. The classifier is learnt from the first sentences in 63 Proceedings of NAACL-HLT 2013, pages 63–73, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Web Wikipedia Ranking by Score Automa&cally constructed training data Web Classiﬁer Deﬁni&on sentences Deﬁni&on Extrac&on (Sec&on 2.1) Deﬁni&on pairs Paraphrase candidates Ranked paraphrase candidates Paraphrase Extrac&on (Sec&on 2.2) Figure 2: Overall picture of our method. Wikipedia articles, which can be regarded as the definition of the title of Wikipedia article (Kazama and Torisawa, 2007) and hence can be used as positive examples. Our method relies on a POS tagger, a dependency parser, a NER tool, noun phrase chunking rules, and frequency thresholds for each language, in addition to Wikipedia articles, which can be seen as a manually annotated knowledge base. However, our method needs no additional manual annotation particularly for this task and thus we categorize our method as a minimally supervised method. On the other hand, Hashimoto et al.’s method heavily depends on the properties of Japanese like the assumption that characteristic expressions of definition sentences te"
N13-1007,P07-2045,0,0.0134423,"to develop a minimally supervised method for multilingual paraphrase extraction from definition sentences. Again, Hashimoto et al.’s method utilizes a supervised classifier trained with annotated data particularly prepared for this task. We eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fragments in global contexts. Our paraphrase extraction method is mostly language-independent and, through experiments for the three languages, we show that it outperforms unsupervised methods (Pas¸ca and Dienes, 2005; Koehn et al., 2007) and is comparable to Hashimoto et al.’s supervised method for Japanese. Previous methods for paraphrase (and entailment) 64 extraction can be classified into a distributional similarity based approach (Lin and Pantel, 2001; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009) and a parallel corpus based approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004; Callison-Burch, 2008). The former can exploit large scale monolingual corpora, but is known to be unable to distinguish paraphrase pairs from anton"
N13-1007,P09-1058,1,0.885519,"Missing"
N13-1007,W06-2932,0,0.10292,"Missing"
N13-1007,P10-1134,0,0.0562029,"ikipedia articles, which can be seen as a manually annotated knowledge base. However, our method needs no additional manual annotation particularly for this task and thus we categorize our method as a minimally supervised method. On the other hand, Hashimoto et al.’s method heavily depends on the properties of Japanese like the assumption that characteristic expressions of definition sentences tend to appear at the end of sentence in Japanese. We show that our method is applicable to English, Japanese, and Chinese, and that its performance is comparable to state-of-the-art supervised methods (Navigli and Velardi, 2010). Since the three languages are very different we believe that our definition extraction method is applicable to any language as long as Wikipedia articles of the language exist. The second contribution of our work is to develop a minimally supervised method for multilingual paraphrase extraction from definition sentences. Again, Hashimoto et al.’s method utilizes a supervised classifier trained with annotated data particularly prepared for this task. We eliminate the need for annotation and instead introduce a method that uses a novel similarity measure considering the occurrence of phrase fr"
N13-1007,navigli-etal-2010-annotated,0,0.102681,"Missing"
N13-1007,I05-1011,0,0.219049,"Missing"
N13-1007,I08-4017,0,0.0123272,"samples were shuffled so that human annotators could not know which sample was from which method. Annotators were the same as those who conducted the evaluation in Section 3.1.3. Cohen’s kappa (Cohen, 1960) was 0.83 for English, 0.88 for Japanese, 5 We filtered out phrase pairs in which one phrase contained a named entity but the other did not contain the named entity from the output of ProposedScore , Proposedlocal , SMT, and P&D, since most of them were not paraphrases. We used Stanford NER (Finkel et al., 2005) for English named entity recognition (NER), KNP for Japanese NER, and BaseNER (Zhao and Kit, 2008) for Chinese NER. Hashisup and Hashiuns did the named entity filtering of the same kind (footnote 3 of Hashimoto et al. (2011)), and thus we did not apply the filter to them any further. 70 Exp2 We compared ProposedScore and P&D. Since P&D restricted its output to phrase pairs in which each phrase consists of two to four words, we restricted the output of ProposedScore to 2-to-4words phrase pairs, too. We randomly sampled 200 from the top 3,000 phrase pairs from each method for evaluation, and the annotators checked entailment relation of both directions between two phrases using Web sentence"
N13-1007,C08-1107,0,\N,Missing
P03-2036,P81-1022,0,0.0851703,"Missing"
P03-2036,2000.iwpt-1.15,0,0.76089,"ations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; Yoshinaga et al., 2001). If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations. This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000), following an approach to parsing comparison among different grammar formalisms (Yoshinaga et al., 2001). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by Yoshinaga and Miyao (2001). The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating"
P03-2036,J93-2004,0,0.0307471,"from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques. We performed a comparison between the existing CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000), using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style. We compared the parsers with respect to the size of the approximated CFG and its effectiveness as a filter. 2 Background In this section, we introduce a grammar conversion (Yoshinaga and Miyao, 2001) and CFG filtering (Harbusch, 1990; Poller and Becker, 1998; Torisawa et al., 2000; Kiefer and Krieger, 2000). 2.1 Grammar conversion The grammar conversion consists of a conversion of LTAG elementary trees to HPSG lexical entries and an emulation of substitution and adjunction by Tree 5: Tree 9: S 5.ε NP5.1 VP5.2 V 5.2.1 NP5.2.2 S 9.ε NP9.1 VP 9.2 V 9.2.1 S 9.2.2 CFG rules NP VP"
P03-2036,J93-4001,0,0.584153,"Missing"
P03-2036,E03-1047,1,0.493609,",115 58,356 68,239 118,464 Table 2: Parsing performance (sec.) with the strongly equivalent grammars for Section 2 of WSJ Parser PB TNT 3 G2 1.4 0.044 G2-4 9.1 0.097 G2-6 17.4 0.144 G2-8 24.0 0.182 G2-10 34.2 0.224 G2-21 124.3 0.542 Comparison with CFG filtering In this section, we compare a pair of CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000) described in Section 2.2.1 and 2.2.2. We hereafter refer to PB and TNT for the C++ implementations of the former and a valiant1 of the latter, respectively.2 We first acquired LTAGs by a method proposed in Miyao et al. (2003) from Sections 2-21 of the Wall Street Journal (WSJ) in the Penn Treebank (Marcus et al., 1993) and its subsets.3 We then converted them into strongly equivalent HPSG-style grammars using the grammar conversion described in Section 2.1. Table 1 shows the size of CFG approximated from the strongly equivalent grammars. Gx , CFGPB , and CFGTNT henceforth refer to the LTAG extracted from Section x of WSJ and CFGs approximated from Gx by PB and TNT, respectively. The size of CFGTNT is much larger than that of CFGPB . By investigating parsing performance using these CFGs, we show that the larger siz"
P03-2036,W98-0134,0,0.552982,"t al., 1999; Torisawa et al., 2000). However, these realizations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; Yoshinaga et al., 2001). If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations. This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000), following an approach to parsing comparison among different grammar formalisms (Yoshinaga et al., 2001). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by Yoshinaga and Miyao (2001). The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity"
P03-2036,C88-2121,0,0.322459,"Missing"
P03-2036,P90-1036,0,\N,Missing
P08-1047,N03-1002,0,0.438359,"than UNK). Various statistics for this extraction are shown in Table 1. The number of distinct hypernyms in the gazetteer was 12,786. Although this Wikipedia gazetteer is much smaller than the English version used by Kazama and Torisawa (2007) that has over 2,000,000 entries, it is the largest gazetteer that can be freely used for Japanese NER. Our experimental results show that this Wikipedia gazetteer can be used to improve the accuracy of Japanese NER. 3 Using Gazetteers as Features of NER Since Japanese has no spaces between words, there are several choices for the token unit used in NER. Asahara and Motsumoto (2003) proposed using characters instead of morphemes as the unit to alleviate the effect of segmentation errors in morphological analysis and we also used their character-based method. The NER task is then treated as a tagging task, which assigns IOB tags to each character in a sentence.10 We use Conditional Random Fields (CRFs) (Lafferty et al., 2001) to perform this tagging. The information of a gazetteer is incorporated 8 They handled “redirections” as well by following redirection links and extracting a hypernym from the article reached. 9 http://mecab.sourceforge.net 10 Precisely, we use IOB2"
P08-1047,J92-4003,0,0.085671,"ions extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs and Brown’s algorithm (Brown et al., 1992), seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1 We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2 Although several categories can be associated in general, we assume that only one category is associated. 407 Proceedings of ACL-08: HLT, pages 407–415, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics syntactic/semantic structures, i.e., verb-MN dependencies to make clean MN clusters. Roo"
P08-1047,C92-2082,0,0.0144215,"al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004)."
P08-1047,D07-1073,1,0.617175,"N)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs"
P08-1047,W02-2016,0,0.0222009,"ano and Hirai, 2004). head-of-bunsetsu features. See (Nakano and Hirai, 2004). Table 2: Atomic features used in baseline model. velopment set (1,446 sentences), and the testing set (1,446 sentences). 4.2 Baseline Model We extracted the atomic features listed in Table 2 at each character for our baseline model. Though there may be slight differences, these features are based on the standard ones proposed and used in previous studies on Japanese NER such as those by Asahara and Motsumoto (2003), Nakano and Hirai (2004), and Yamada (2007). We used MeCab as a morphological analyzer and CaboCha14 (Kudo and Matsumoto, 2002) as the dependency parser to ﬁnd the boundaries of the bunsetsu. We generated the node and the edge features of a CRF model as described in Table 3 using these atomic features. 4.3 Training To train CRF models, we used Taku Kudo’s CRF++ (ver. 0.44) 15 with some modiﬁcations.16 We http://chasen.org/∼taku/software/ CaboCha 15 http://chasen.org/˜taku/software/CRF++ 16 We implemented scaling, which is similar to that for HMMs (Rabiner, 1989), in the forward-backward phase and replaced the optimization module in the original package with the 14 Node features: {””, x−2 , x−1 , x0 , x+1 , x+2 } × y0"
P08-1047,N04-1043,0,0.421618,"lations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs and Brown’s algorithm (Brown et al., 1992), seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer 1 We used the term, “multi-word”, to emphasize that a gazetteer includes not only one-word expressions but also multi-word expressions. 2 Although several categories can be associated in general, we assume that only one category is associated. 407 Proceedings of ACL-08"
P08-1047,P99-1014,0,0.77808,"a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot produce the MN clusters required for constructing gazette"
P08-1047,I08-2080,0,0.018718,"1.77-point improvement from the baseline for the testing set. 5 Comparison with Previous Studies Since many previous studies on Japanese NER used 5-fold cross validation for the IREX dataset, we also performed it for some our models that had the best σ 2 found in the previous experiments. The results are listed in Table 7 with references to the results of recent studies. These results not only reconﬁrmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. It uses many structural features that are not used in our model. Incorporating such features might improve our model further. 6 Related Work and Discussion There are several studies that used automatically extracted gazetteers for NER (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006; Kazama and Torisawa, 2007). Most of the methods (Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006) are oriented at the NE category. They extracted a gazetteer for each NE category and utilized it in a NE tagger. On the other han"
P08-1047,sekine-isahara-2000-irex,0,0.0616897,"the accuracy of NER is difﬁcult. We enabled such large-scale clustering by parallelizing the clustering algorithm, and we demonstrate the usefulness of the gazetteer constructed. We parallelized the algorithm of (Torisawa, 2001) using the Message Passing Interface (MPI), with the prime goal being to distribute parameters and thus enable clustering with a large vocabulary. Applying the parallelized clustering to a large set of dependencies collected from Web documents enabled us to construct gazetteers with up to 500,000 entries and 3,000 classes. In our experiments, we used the IREX dataset (Sekine and Isahara, 2000) to demonstrate the usefulness of cluster gazetteers. We also compared the cluster gazetteers with the Wikipedia gazetteer constructed by following the method of (Kazama and Torisawa, 2007). The improvement was larger for the cluster gazetteer than for the Wikipedia gazetteer. We also investigated whether these gazetteers improve the accuracies further when they are used in combination. The experimental results indicated that the accuracy improved further in several cases and showed that these gazetteers complement each other. The paper is organized as follows. In Section 2, we explain the con"
P08-1047,N04-1010,1,0.533019,"st studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001). For instance, Kazama and Torisawa (2007) used the hyponymy relations extracted from Wikipedia for the English NER, and reported improved accuracies with such a gazetteer. We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004). However, these methods cannot"
P08-1047,W06-2919,0,0.404639,"ng the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of gazetteers. For example, we can use automatically extracted hyponymy"
P08-1047,W02-1028,0,0.0102524,"sters as a gazetteer (cluster gazetteer) is a effective way of improving the accuracy of NER. Moreover, we demonstrate that the combination of the cluster gazetteer and a gazetteer extracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. 1 Introduction Gazetteers, or entity dictionaries, are important for performing named entity recognition (NER) accurately. Since building and maintaining high-quality gazetteers by hand is very expensive, many methods have been proposed for automatic extraction of gazetteers from texts (Riloff and Jones, 1999; Thelen and Riloff, 2002; Etzioni et al., 2005; Shinzato et al., 2006; Talukdar et al., 2006; Nadeau et al., 2006). Most studies using gazetteers for NER are based on the assumption that a gazetteer is a mapping from a multi-word noun (MN)1 to named entity categories such as “Tokyo Stock Exchange → {ORGANIZATION}”.2 However, since the correspondence between the labels and the NE categories can be learned by tagging models, a gazetteer will be useful as long as it returns consistent labels even if those returned are not the NE categories. By changing the perspective in such a way, we can explore more broad classes of"
P09-1049,P07-1000,0,0.131789,"ived from (T IGER , S IBERIAN TIGER) in Figure 4. English and Japanese terms are used for building bilingual instance dictionary DBI for hyponymyrelation acquisition, where DBI is composed of translation pairs between English and Japanese hyponymy-relation candidates5 . Wikipedia infobox, a special kind of template, that describes a tabular summary of an article subject expressed by attribute-value pairs. An attribute type coupled with the infobox name to which it belongs provides the semantic properties of its value that enable us to easily understand what the attribute value means (Auer and Lehmann, 2007; Wu and Weld, 2007). For example, infobox template City Japan in Wikipedia article Kyoto contains several attribute-value pairs such as “Mayor=Daisaku Kadokawa” as attribute=its value. What Daisaku Kadokawa, the attribute value of mayor in the example, represents is hard to understand alone if we lack knowledge, but its attribute type, mayor, gives a clue–Daisaku Kadokawa is a mayor related to Kyoto. These semantic properties enable us to discover semantic evidence for hyponymy relations. We extract triples (infobox name, attribute type, attribute value) from the Wikipedia infoboxes and encod"
P09-1049,I08-2126,1,0.731186,"Missing"
P09-1049,sumida-etal-2008-boosting,1,0.924703,"roblems, especially when the student also has a certain level of confidence in his opinion on a class label but disagrees with the teacher: rT &gt; θ and clS = clT . In that case, the teacher does nothing Wikipedia Articles in J Hyponymy-relation candidate extraction Translation dictionary Candidates in J Bilingual instance dictionary Classifier in E Labeled instances Unlabeled instances in E Newly labeled instances for E Unlabeled instances in J Newly labeled instances for J Classifier in J Labeled instances Bilingual Co-Training Figure 3: System architecture 3.1 Candidate Extraction We follow Sumida et al. (2008) to extract hyponymy-relation candidates from English and Japanese Wikipedia. A layout structure is chosen 434 used in Sumida et al. (2008) but LF1 –LF5 and SF1 –SF2 are the same as their feature set. Let us provide an overview of the feature sets used in Sumida et al. (2008). See Sumida et al. (2008) for more details. Lexical features LF1 –LF5 are used to recognize the lexical evidence encoded in hyper and hypo for hyponymy relations. For example, (hyper,hypo) is often a proper hyponymy relation if hyper and hypo share the same head morpheme or word. In LF1 and LF2 , such information is provi"
P09-1049,S07-1003,0,0.059436,"Missing"
P09-1049,D07-1073,1,0.830981,"in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effectiv"
P09-1049,P02-1044,0,0.0476847,"Missing"
P09-1049,P02-1027,0,0.0325283,"ote that F1 of INIT in Table 2 was 72.2 in English and 76.6 in Japanese.) 5 bilingual bootstrapping. However, the two classifiers in bilingual bootstrapping were for a bilingual task but did different tasks from the monolingual viewpoint. A classifier in each language is for word sense disambiguation, where a class label (or word sense) is different based on the languages. On the contrary, classifiers in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods f"
P09-1049,J04-1001,0,\N,Missing
P09-1058,J96-2001,0,0.0305502,"of unknown words (with characterlevel nodes) as well as those of known words (with word-level nodes). We can directly estimate the statistics of known words from an annotated corpus where a sentence is already segmented into words and assigned POS tags. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level node"
P09-1058,J95-4004,0,0.503119,"Missing"
P09-1058,P99-1036,0,0.028539,"s. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned. For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes. As a result, the correct path yt can contain"
P09-1058,W02-1001,0,0.0750953,"lgorithm has two main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iteration. As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting. For inference, we can use Viterbi-style decoding to search for the most likely path y∗ for a given sentence x where: Input: Training set S = {(xt , yt )}Tt=1 Output: Model weight vector w 1: w(0) = 0; v = 0; i = 0 2: for iter = 1 to N do 3: for t = 1 to T do 4: w(i+1) = update w(i) according to (xt , yt ) 5: v = v + w(i+1) 6: i=i+1 7: end for 8: end for 9: w = v/(N × T ) within a few iterations (McDonald, 2006). Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework. 4.2"
P09-1058,P07-2055,1,0.292461,"taka Uchimoto‡ and Jun’ichi Kazama‡ Yiou Wang‡ and Kentaro Torisawa‡ and Hitoshi Isahara†‡ † Graduate School of Engineering, Kobe University 1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan ‡ National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan {canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp tem’s word dictionary1 . The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors. The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem. However, it suffers from structural complexity. Nakagawa (2004) described a training method based on a word-based Markov model and a character-based maximum entropy model that can be completed in a reasonable time. However, this training method is limited by the generatively-trained Markov model in which informative features are hard to exploit. In this paper, we overcome such limitations concerning both efficiency and effectiveness. We propose a new framework for training the wordcharacter hybrid model based on the Margin Infused Relaxed A"
P09-1058,C04-1067,0,0.417122,"gkrai†‡ and Kiyotaka Uchimoto‡ and Jun’ichi Kazama‡ Yiou Wang‡ and Kentaro Torisawa‡ and Hitoshi Isahara†‡ † Graduate School of Engineering, Kobe University 1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan ‡ National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan {canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp tem’s word dictionary1 . The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors. The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem. However, it suffers from structural complexity. Nakagawa (2004) described a training method based on a word-based Markov model and a character-based maximum entropy model that can be completed in a reasonable time. However, this training method is limited by the generatively-trained Markov model in which informative features are hard to exploit. In this paper, we overcome such limitations concerning both efficiency and effectiveness. We propose a new framework for training the wordcharacter hybrid model based o"
P09-1058,W04-3236,0,0.900606,"our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2"
P09-1058,W96-0213,0,0.439508,"nd assigned POS tags. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned. For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes. As a result, the correct path y"
P09-1058,W03-1719,0,0.0612588,"nese Treebank (CTB) (Xia et al., 2000) in experiments. However, versions of CTB and experimental settings vary across different studies. In this paper, we used CTB 5.0 (LDC2005T01) as our main corpus, defined the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b), and designed our experiments to explore the impact of the training corpus size on our approach. Table 5 provides the statistics of our experimental settings on the small and large training data. The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). Note that the development set was only used for evaluating the trained model to obtain the optimal values of tunable parameters. 5.4 Impact of policies for correct path selection Table 6 shows the results of our word-character hybrid model using the error-driven and baseline policies. The third and fourth columns indicate the numbers of known and artificial unknown words in the training phase. The total number of words is the same, but the different policies yield different balances between the known and artificial unknown words for learning the hybrid model. Optimal balances were selected u"
P09-1058,P08-1102,0,0.671136,"Missing"
P09-1058,W01-0512,1,0.806822,"cond is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model. Based on extensive comparisons, we showed that our approach is superior to the existing approaches reported in the literature. In future work, we plan to apply our framework to other Asian languages, including Thai and Japanese. 7 Related work In this section, we discuss related approaches based on several aspects of learning algorithms and search space representation methods. Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance. Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems. However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model. Online learning has recently gained popularity for many NLP tasks since it performs compa"
P09-1058,C08-1049,0,0.775572,"uperior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Figure 1: Lattice used in word-charac"
P09-1058,W04-3230,0,0.04645,"n future work, we plan to apply our framework to other Asian languages, including Thai and Japanese. 7 Related work In this section, we discuss related approaches based on several aspects of learning algorithms and search space representation methods. Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance. Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems. However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model. Online learning has recently gained popularity for many NLP tasks since it performs comparably or better than batch learning using shorter training times (McDonald, 2006). For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging (Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). Another potent"
P09-1058,xia-etal-2000-developing,0,0.388731,"2005; McDonald, 2006). We describe k-best decoding for our hybrid model and design its loss function and the features appropriate for our task. In our word-character hybrid model, allowing the model to learn the characteristics of both known and unknown words is crucial to achieve optimal performance. Here, we describe our strategies that yield good balance for learning these two characteristics. We propose an errordriven policy that delivers this balance by acquiring examples of unknown words from particular errors in a training corpus. We conducted our experiments on Penn Chinese Treebank (Xia et al., 2000) and compared our approach with the best previous approaches reported in the literature. Experimental results indicate that our approach can achieve state-of-the-art performance. Abstract In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiri"
P09-1058,P08-1101,0,0.620356,"show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Figure 1: Lattice"
P09-1058,H05-1066,0,0.00424798,"o main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iteration. As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting. For inference, we can use Viterbi-style decoding to search for the most likely path y∗ for a given sentence x where: Input: Training set S = {(xt , yt )}Tt=1 Output: Model weight vector w 1: w(0) = 0; v = 0; i = 0 2: for iter = 1 to N do 3: for t = 1 to T do 4: w(i+1) = update w(i) according to (xt , yt ) 5: v = v + w(i+1) 6: i=i+1 7: end for 8: end for 9: w = v/(N × T ) within a few iterations (McDonald, 2006). Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework. 4.2 k-best MIRA We focus on"
P09-1058,C94-1032,0,0.27228,"hybrid model since it quickly converges 3 We consider a word and its POS tag a single entry. In our experiments, the optimal threshold value r is selected by evaluating the performance of joint word segmentation and POS tagging on the development set. 4 515 Algorithm 1 Generic Online Learning Algorithm above quadratic programming (QP) problem can be solved using Hildreth’s algorithm (Yair Censor, 1997). Replacing Eq. (2) into line 4 of Algorithm 1, we obtain k-best MIRA. The next question is how to efficiently generate bestk (xt ; w(i) ). In this paper, we apply a dynamic programming search (Nagata, 1994) to kbest MIRA. The algorithm has two main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iterat"
P09-1058,W03-1726,0,\N,Missing
P10-1003,N03-1017,0,0.00598786,"al Linguistics the target side that might be useful for ambiguity resolution. Our method achieves much greater improvement because it uses the richer subtree constraints. Our approach takes the same input as Huang et al. (2009) and exploits the subtree structure on the target side to provide the bilingual constraints. The subtrees are extracted from large-scale autoparsed monolingual data on the target side. The main problem to be addressed is mapping words on the source side to the target subtree because there are many to many mappings and reordering problems that often occur in translation (Koehn et al., 2003). We use an automatic way for generating mapping rules to solve the problems. Based on the mapping rules, we design a set of features for parsing models. The basic idea is as follows: if the words form a subtree on one side, their corresponding words on the another side will also probably form a subtree. Experiments on the translated portion of the Chinese Treebank (Xue et al., 2002; Bies et al., 2007) show that our system outperforms state-ofthe-art monolingual parsers by 2.93 points for Chinese and 1.64 points for English. The results also show that our system provides higher accuracies than"
P10-1003,P09-1058,1,0.81326,"n a bilingual corpus having approximately 0.8M sentence pairs. We removed notoriously bad links in {a, an, the}×{的(DE), 了(LE)} following the work of Huang et al. (2009). For Chinese unannotated data, we used the XIN CMN portion of Chinese Gigaword Version 2.0 (LDC2009T14) (Huang, 2009), which has approximately 311 million words whose segmentation and POS tags are given. To avoid unfair comparison, we excluded the sentences of the CTB data from the Gigaword data. We discarded the annotations because there are differences in annotation policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. For English unannotated data, we used the BLLIP corpus that contains about 43 million words of WSJ text. The POS tags were assigned by the MXPOST tagger trained on training data. Then we used the Baseline Parser to parse all the sentences in the data. We reported the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with correct HEADs. 4.3.2 Features for 2to3 In the 2to3 case, a ne"
P10-1003,N06-1014,0,0.266158,"n the above cases. For example, in Figure 6, trigram-subtree “在(NULL):3-上(at):1-说(say):0” is mapped onto bigram-subtree “said:0-at:1”. Since asking linguists to define the mapping rules is very expensive, we propose a simple method to easily obtain the mapping rules. 4.2.2 Bilingual subtree mapping 4.2.3 Generalized mapping rules To solve the mapping problems, we use a bilingual corpus, which includes sentence pairs, to automatically generate the mapping rules. First, the sentence pairs are parsed by monolingual parsers on both sides. Then we perform word alignment using a word-level aligner (Liang et al., 2006; DeNero and Klein, 2007). Figure 8 shows an example of a processed sentence pair that has tree structures on both sides and word alignment links. ROOT ROOT ԆԜ ༴Ҿ ⽮Պ 䗩㕈 To increase the mapping coverage, we generalize the mapping rules from the extracted subtree pairs by using the following procedure. The rules are divided by “=&gt;” into two parts: source (left) and target (right). The source part is from the source subtree and the target part is from the target subtree. For the source part, we replace nouns and verbs using their POS tags (coarse grained tags). For the target part, we use the wor"
P10-1003,D08-1092,0,0.602536,"nguage sentence is mapped to a subtree in the corresponding target-language sentence by using word alignment and mapping rules that are automatically learned. The target subtree is verified by checking the subtree list that is collected from unlabeled sentences in the target language parsed by a usual monolingual parser. The result is used as additional features for the source side dependency parser. In this paper, our task is to improve the source side parser with the help of the translations on the target side. Many researchers have investigated the use of bilingual constraints for parsing (Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009). For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. However, their methods require that the training data have tree structures on both sides, which are hard to obtain. Our method only requires dependency annotation on the source side and is much simpler and faster. Huang et al. (2009) proposes a method, bilingual-constrained monolingual parsing, in which a source-language parser is extended to use the re-ordering of words between two sides’ sentences as additional informatio"
P10-1003,E06-1011,0,0.712091,"odes, we call it a trigram-subtree. From the dependency tree of Figure 3, we obtain the subtrees, as shown in Figure 4 and Figure 5. Figure 4 shows the extracted bigram-subtrees and Figure 5 shows the extracted trigram-subtrees. After extraction, we obtain a set of subtrees. We remove the subtrees occurring only once in the data. Following Chen et al. (2009), we also group the subtrees into different sets based on their frequencies. . Figure 3: Example of dependency tree In our systems, the monolingual features include the first- and second- order features presented in (McDonald et al., 2005; McDonald and Pereira, 2006) and the parent-child-grandchild features used in (Carreras, 2007). We call the parser with the monolingual features monolingual parser. 3.2 Parsing with bilingual features In this paper, we parse source sentences with the help of their translations. A set of bilingual features are designed for the parsing model. 3.2.1 Bilingual subtree features We design bilingual subtree features, as described in Section 4, based on the constraints between the source subtrees and the target subtrees that are verified by the subtree list on the target side. The source subtrees are from the possible dependency"
P10-1003,P05-1012,0,0.689073,"d undirected links are word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on the target side. We use large-scale aut"
P10-1003,D07-1101,0,0.602173,"word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on the target side. We use large-scale auto-parsed data to"
P10-1003,2006.iwslt-evaluation.9,0,0.0149817,"res gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English. 1 Introduction Parsing bilingual texts (bitexts) is crucial for training machine translation systems that rely on syntactic structures on either the source side or the target side, or the both (Ding and Palmer, 2005; Nakazawa et al., 2006). Bitexts could provide more information, which is useful in parsing, than a usual monolingual texts that can be called “bilingual constraints”, and we expect to obtain more accurate parsing results that can be effectively used in the training of MT systems. With this motivation, there are several studies aiming at highly 21 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21–29, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics the target side that might be useful for ambiguity resolution. Our method achieves much"
P10-1003,D09-1060,1,0.951248,"sign the bilingual subtree features based on the mapping rules for the parsing model. These features indicate the information of the constraints between bilingual subtrees, that are called bilingual subtree constraints. 3.1 Parsing with monolingual features Figure 3 shows an example of dependency parsing. In the graph-based parsing model, features are represented for all the possible relations on single edges (two words) or adjacent edges (three words). The parsing algorithm chooses the tree with the highest score in a bottom-up fashion. 4.1 Subtree extraction ROOT He ate the meat with a fork Chen et al. (2009) propose a simple method to extract subtrees from large-scale monolingual data and use them as features to improve monolingual parsing. Following their method, we parse large unannotated data with a monolingual parser and obtain a set of subtrees (STt ) in the target language. We encode the subtrees into string format that is expressed as st = w : hid(−w : hid)+1 , where w refers to a word in the subtree and hid refers to the word ID of the word’s head (hid=0 means that this word is the root of a subtree). Here, word ID refers to the ID (starting from 1) of a word in the subtree (words are ord"
P10-1003,P08-1108,0,0.0267451,"have an input sentence pair as shown in Figure 1, where the source sentence is in English, the target is in Chinese, the dashed undirected links are word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bil"
P10-1003,P07-1003,0,0.141246,"or example, in Figure 6, trigram-subtree “在(NULL):3-上(at):1-说(say):0” is mapped onto bigram-subtree “said:0-at:1”. Since asking linguists to define the mapping rules is very expensive, we propose a simple method to easily obtain the mapping rules. 4.2.2 Bilingual subtree mapping 4.2.3 Generalized mapping rules To solve the mapping problems, we use a bilingual corpus, which includes sentence pairs, to automatically generate the mapping rules. First, the sentence pairs are parsed by monolingual parsers on both sides. Then we perform word alignment using a word-level aligner (Liang et al., 2006; DeNero and Klein, 2007). Figure 8 shows an example of a processed sentence pair that has tree structures on both sides and word alignment links. ROOT ROOT ԆԜ ༴Ҿ ⽮Պ 䗩㕈 To increase the mapping coverage, we generalize the mapping rules from the extracted subtree pairs by using the following procedure. The rules are divided by “=&gt;” into two parts: source (left) and target (right). The source part is from the source subtree and the target part is from the target subtree. For the source part, we replace nouns and verbs using their POS tags (coarse grained tags). For the target part, we use the word alignment information t"
P10-1003,W03-3017,0,0.118478,"ntence is in English, the target is in Chinese, the dashed undirected links are word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentence"
P10-1003,P05-1067,0,0.0374513,"sing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because of richer bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English. 1 Introduction Parsing bilingual texts (bitexts) is crucial for training machine translation systems that rely on syntactic structures on either the source side or the target side, or the both (Ding and Palmer, 2005; Nakazawa et al., 2006). Bitexts could provide more information, which is useful in parsing, than a usual monolingual texts that can be called “bilingual constraints”, and we expect to obtain more accurate parsing results that can be effectively used in the training of MT systems. With this motivation, there are several studies aiming at highly 21 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21–29, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics the target side that might be useful for ambiguity resolution. O"
P10-1003,W04-3207,0,0.350187,"Missing"
P10-1003,C02-1145,0,0.142006,"ata on the target side. The main problem to be addressed is mapping words on the source side to the target subtree because there are many to many mappings and reordering problems that often occur in translation (Koehn et al., 2003). We use an automatic way for generating mapping rules to solve the problems. Based on the mapping rules, we design a set of features for parsing models. The basic idea is as follows: if the words form a subtree on one side, their corresponding words on the another side will also probably form a subtree. Experiments on the translated portion of the Chinese Treebank (Xue et al., 2002; Bies et al., 2007) show that our system outperforms state-ofthe-art monolingual parsers by 2.93 points for Chinese and 1.64 points for English. The results also show that our system provides higher accuracies than the parser of Huang et al. (2009). The rest of the paper is organized as follows: Section 2 introduces the motivation of our idea. Section 3 introduces the background of dependency parsing. Section 4 proposes an approach of constructing bilingual subtree constraints. Section 5 explains the experimental results. Finally, in Section 6 we draw conclusions and discuss future work. 2 He"
P10-1003,C96-1058,0,0.00990233,"e it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on the target side. We use large-scale auto-parsed data to obtain subtrees on the target side. Then we generate the mapping rules to map the source subtrees onto the extracted target subtrees. Finally, we design the bilingual subtree features based on the mapping rules for the parsing model. These features indicate the i"
P10-1003,W03-3023,0,0.2085,"English, the target is in Chinese, the dashed undirected links are word alignment links, and the directed links between words indicate that they have a (candidate) dependency relation. In the English side, it is difficult for a parser to determine the head of word “with” because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the information on the Chinese side to help disambigua3 Dependency parsing For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graphbased (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing models. In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira 22 4 Bilingual subtree constraints (2006), which is an extension of the projective parsing algorithm of Eisner (1996). To use richer second-order information, we also implement parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on t"
P10-1003,P09-1007,0,0.117236,"to a subtree in the corresponding target-language sentence by using word alignment and mapping rules that are automatically learned. The target subtree is verified by checking the subtree list that is collected from unlabeled sentences in the target language parsed by a usual monolingual parser. The result is used as additional features for the source side dependency parser. In this paper, our task is to improve the source side parser with the help of the translations on the target side. Many researchers have investigated the use of bilingual constraints for parsing (Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009). For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. However, their methods require that the training data have tree structures on both sides, which are hard to obtain. Our method only requires dependency annotation on the source side and is much simpler and faster. Huang et al. (2009) proposes a method, bilingual-constrained monolingual parsing, in which a source-language parser is extended to use the re-ordering of words between two sides’ sentences as additional information. The input of the"
P10-1003,D09-1127,0,0.136762,"Missing"
P10-1026,P94-1038,0,0.168634,"roblem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. T"
P10-1026,I08-1025,0,0.0193862,"and set the output variable to the default value. Then, we iterate over the sparse vectors c(w1 , fk ) and c(w2 , fk ). If We used the GNU (www.gnu.org/software/gsl/), function. = T 1 X δ(wi ∈ ans), T i=1 AP = N 1 X δ(wi ∈ ans)P@i. R i=1 δ(wi ∈ ans) returns 1 if the output word wi is in the answers, and 0 otherwise. N is the number of outputs and R is the number of the answers. MP@T and MAP are the averages of these values over all input words. For each k: (D): exp(2(lnΓ(αk + 12 )). 2 P@T 5.2 Collecting context proﬁles Dependency relations are used as context proﬁles as in Kazama and Torisawa (2008) and Kazama et al. (2009). From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million Scientiﬁc Library (GSL) which implements this 250 “B” is for the validation of the parameter tuning. documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus. If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, 〈w, r〉). That is, a context fk , is 〈w, r〉 here. For noun-verb dependencies, postpositions in Japanese represent relation"
P10-1026,P97-1008,0,0.0366308,"known that p(φ|D) is also a Dirichlet distribution for this simplest case, and it can be analytically calculated as follows. p(φ|D) = Dir(φ|{αk + c(k)}), (6) where c(k) is the frequency of choice k in data D. For example, c(k) = c(wi , fk ) in the estimation of p(fk |wi ). This is very simple: we just need to add the observed counts to the hyperparameters. 248 2.2 Bhattacharyya coefﬁcient When the context proﬁles are probability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al., 1994; Dagan et al., 1997). The JS divergence is deﬁned as follows. JS(p1 ||p2 ) = = 2 where pavg = p1 +p is a point-wise average of p1 2 and p2 and KL(.) is the Kullback-Leibler divergence. Although we found that the JS divergence is a good measure, it is difﬁcult to derive an efﬁcient calculation of Eq. 2, even in the Dirichlet prior case.1 In this study, we employ the Bhattacharyya coefﬁcient (Bhattacharyya, 1943) (BC for short), which is deﬁned as follows: K X √ BCb (w1 , w2 ) = Γ(α0 + a0 )Γ(β0 + b0 ) × Γ(α0 + a0 + 12 )Γ(β0 + b0 + 12 ) (8) K X Γ(αk + c(w1 , fk ) + 21 )Γ(βk + c(w2 , fk ) + 12 ) , Γ(αk + c(w1 , fk ))"
P10-1026,P06-1124,0,0.012983,"3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. Then, Section 5 reports the experimental results. has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation. The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995). Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009). In this paper, we apply the Bayesian framework to the calculation of distributional similarity. The method is straightforward: Instead of using the point estimation of v(wi ), we ﬁrst estimate the distribution of the context proﬁle, p(v(wi )), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: simb (w1 , w2 ) 2 Background 2.1 Bayesian estimation with Dirichlet prior Assume that we estimate a probabilistic model for the observed data D, p(D|φ), which is parameterized with parameters φ. In the maximum li"
P10-1026,P90-1034,0,0.43827,"has been recognized as a serious problem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some im"
P10-1026,P08-1047,1,0.897456,"all c(wi , fk ) = 0 and set the output variable to the default value. Then, we iterate over the sparse vectors c(w1 , fk ) and c(w2 , fk ). If We used the GNU (www.gnu.org/software/gsl/), function. = T 1 X δ(wi ∈ ans), T i=1 AP = N 1 X δ(wi ∈ ans)P@i. R i=1 δ(wi ∈ ans) returns 1 if the output word wi is in the answers, and 0 otherwise. N is the number of outputs and R is the number of the answers. MP@T and MAP are the averages of these values over all input words. For each k: (D): exp(2(lnΓ(αk + 12 )). 2 P@T 5.2 Collecting context proﬁles Dependency relations are used as context proﬁles as in Kazama and Torisawa (2008) and Kazama et al. (2009). From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million Scientiﬁc Library (GSL) which implements this 250 “B” is for the validation of the parameter tuning. documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus. If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, 〈w, r〉). That is, a context fk , is 〈w, r〉 here. For noun-verb dependencies, postpositions in Japanese represent relation"
P10-1026,P98-2127,0,0.309119,"guage modeling and supervised machine learning. However, to our knowledge, there Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the ﬁeld. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗ (1) The work was done while the author was at NICT. 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics The rest of the paper is organized as follows. In Section 2, we brieﬂy introduce the Bayesian estimation and the Bhattacharyya coefﬁcient. Section 3 proposes our new Bayesian Bhattacharyya coefﬁcient for robust similarity calculation. Section 4 mentions some implementation issues and the solutions. Then, Section 5 reports the expe"
P10-1026,D09-1098,0,\N,Missing
P10-1026,P09-1012,0,\N,Missing
P10-1026,C98-2122,0,\N,Missing
P10-1026,P08-1000,0,\N,Missing
P11-1109,P05-1074,0,0.327239,"Missing"
P11-1109,N03-1003,0,0.822953,"he same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same 1087 concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define the same concept “osteoporosis”, include two pai"
P11-1109,P01-1008,0,0.454546,"ntroduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same 1087 concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define"
P11-1109,D07-1017,0,0.0267206,"cription of related work on the same research issue. Section 2 describes related works. Section 3 presents our proposed method. Section 4 reports on evaluation results. Section 5 concludes the paper. 2 Related Work The existing work for paraphrase extraction is categorized into two groups. The first involves a distributional similarity approach pioneered by Lin and Pantel (2001). Basically, this approach assumes that two expressions that have a large distributional similarity are paraphrases. There are also variants of this approach that address entailment acquisition (Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009). These methods can be applied to a normal monolingual corpus, and it has been shown that a large number of paraphrases or entailment rules could be extracted. How1088 ever, the precision of these methods has been relatively low. This is due to the fact that the evidence, i.e., distributional similarity, is just indirect evidence of paraphrase/entailment. Accordingly, these methods occasionally mistake antonymous pairs for paraphrases/entailment pairs, since an expression and its antonymous counterpart are also likely to have a large distribut"
P11-1109,N06-1003,0,0.101957,"Missing"
P11-1109,C04-1051,0,0.665037,". Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept. We propose a supervised method that exploits various kinds of lexical similarity features and contextual features. Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method. Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). The former requires a large amount of manual labor to translate the same texts Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics in several ways. The latter would suffer from the fact that it is not easy to automatically retrieve large bodies of parallel news text with high accuracy. On the contrary, recognizing definition sentences for the same concept is quite an easy task at least for Japanese, as we will show, and we were able to find a huge amount"
P11-1109,P05-1014,0,0.0131216,"same cuisine, or the description of related work on the same research issue. Section 2 describes related works. Section 3 presents our proposed method. Section 4 reports on evaluation results. Section 5 concludes the paper. 2 Related Work The existing work for paraphrase extraction is categorized into two groups. The first involves a distributional similarity approach pioneered by Lin and Pantel (2001). Basically, this approach assumes that two expressions that have a large distributional similarity are paraphrases. There are also variants of this approach that address entailment acquisition (Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009). These methods can be applied to a normal monolingual corpus, and it has been shown that a large number of paraphrases or entailment rules could be extracted. How1088 ever, the precision of these methods has been relatively low. This is due to the fact that the evidence, i.e., distributional similarity, is just indirect evidence of paraphrase/entailment. Accordingly, these methods occasionally mistake antonymous pairs for paraphrases/entailment pairs, since an expression and its antonymous counterpart are also likely to h"
P11-1109,D09-1122,1,0.885272,"Missing"
P11-1109,N06-1058,0,0.0634,"Missing"
P11-1109,D07-1073,1,0.816716,"We describe them below. 3.1 Definition sentence acquisition We acquire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences from Wikipedia articles, which are typi(Osteoporosis is a disease that makes bones fragile.) cally the first sentence of the body of each article (Kazama and Torisawa, 2007), we obtained a total Fujii and Ishikawa (2002) developed an unsuper- of 2,141,878 definition sentence candidates, which vised method to find definition sentences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use"
P11-1109,P08-1047,1,0.825091,"e defined similarly. 1090 Figure 1: Illustration of features f8-12. didate phrase of s2 but do appear in the other part of s2 , i.e. they are extra morphemes for s1 ’s candidate phrase. On the other hand, f9 is zero since there is no such extra morpheme in s2 ’s candidate phrase. Also, features f10-12 have positive values since the two candidate phrases share two parent dependency tree fragments, (that increases) and (of fracture). We have also tried the following features, which we do not detail due to space limitation: the similarity of candidate phrases based on semantically similar nouns (Kazama and Torisawa, 2008), entailing/entailed verbs (Hashimoto et al., 2009), and the identity of the pronunciation and base form of the head morpheme; N -grams (N =1,2,3) of child and parent contexts represented by either the inflected form, base form, pronunciation, or POS of morOriginal definition sentence pair (s1 , s2 ) s1 : Osteoporosis is a disease that reduces bone mass and makes bones fragile. s2 : Osteoporosis is a disease that decreases the quantity of bone and increases the risk of bone fracture. Paraphrased definition sentence pair (s01 , s02 ) s01 : Osteoporosis is a disease that decreases the quantity o"
P11-1109,D08-1084,0,0.0310218,"Missing"
P11-1109,J10-3003,0,0.0405564,"unt of parallel corpora, as noted before. We avoid this by using definition sentences, which can be easily acquired on a large scale from the Web, as parallel corpora. Murata et al. (2004) used definition sentences in two manually compiled dictionaries, which are considerably fewer in the number of definition sentences than those on the Web. Thus, the coverage of their method should be quite limited. Furthermore, the precision of their method is much poorer than ours as we report in Section 4. For a more extensive survey on paraphrasing methods, see Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). 3 Proposed method Our method, targeting the Japanese language, consists of two steps: definition sentence acquisition and paraphrase extraction. We describe them below. 3.1 Definition sentence acquisition We acquire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences f"
P11-1109,P10-1134,0,0.0253121,"ences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use one sentential template and an SVM clas- a language-independent method of definition acquisifier. Specifically, we first collect definition sen- sition, see Navigli and Velardi (2010) as an example. tence candidates by a template “ˆNP とは.*”, where ˆ is the beginning of sentence and NP is the noun 3.2 Paraphrase extraction phrase expressing the concept to be defined followed Paraphrase extraction proceeds as follows. First, by a particle sequence, “と” (comitative) and “は” each sentence in a pair is parsed by the depen(topic) (and optionally followed by comma), as ex- dency parser KNP2 and dependency tree fragemplified in (2). As a result, we collected 3,027,101 ments that constitute linguistically well-formed consentences. Although the particle sequence tends stituents are"
P11-1109,W04-3219,0,0.024729,"Missing"
P11-1109,P02-1006,0,0.0207055,"uire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia. 92.2, and 91.4, respectively. Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences. After adding definition (2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 sentences from Wikipedia articles, which are typi(Osteoporosis is a disease that makes bones fragile.) cally the first sentence of the body of each article (Kazama and Torisawa, 2007), we obtained a total Fujii and Ishikawa (2002) developed an unsuper- of 2,141,878 definition sentence candidates, which vised method to find definition sentences from the covered 867,321 concepts ranging from weapons to Web using 18 sentential templates and a language rules of baseball. Then, we coupled two definition model constructed from an encyclopedia. On the sentences whose defined concepts were the same other hand, we developed a supervised method to and obtained 29,661,812 definition sentence pairs. achieve a higher precision. Obviously, our method is tailored to Japanese. For We use one sentential template and an SVM clas- a lang"
P11-1109,I05-5011,0,0.0171765,"Missing"
P11-1109,P07-1058,0,0.0128043,"the Web are a treasure trove of paraphrase knowledge (Section 4.2). II. Our method of paraphrase acquisition from definition sentences is more accurate than wellknown competing methods (Section 4.1). We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al., 2007) (SMT method), and that of Murata et al. (2004) (Mrt method). The first two methods are well known for accurately extracting semantically equivalent phrase pairs from parallel corpora.8 Then, we verify claim This scheme is similar to the one proposed by Szpektor et al. (2007). We adopt this scheme since paraphrase judgment might be unstable between annotators unless they are given a particular context based on which they make a judgment. As de6 The remaining 36 pairs were discarded as they contained scribed below, we use definition sentences as congarbled characters of Japanese. texts. We admit that annotators might be biased by 7 http://www.statmt.org/moses/ 8 this in some unexpected way, but we believe that As anonymous reviewers pointed out, they are unsuperthis is a more stable method than that without con- vised methods and thus unable to be adapted to defini"
P11-1109,C08-1107,0,\N,Missing
P11-1109,P07-2045,0,\N,Missing
P13-1159,D11-1145,0,0.220534,"Missing"
P13-1159,C08-1024,1,0.876106,"Missing"
P13-1159,D11-1076,1,0.852401,"Missing"
P13-1159,D12-1057,1,0.626382,"Missing"
P13-1159,C08-1052,0,0.0587294,"Missing"
P13-1159,P08-1047,1,0.3007,"ve and 27,951 negative words. Note that we used the Opinion Extraction Tool in the experiments to check the effectiveness of the full-fledged sentiment analysis in this task. Semantic Word Class (SWC) We assume that nouns in the same semantic class behave simi2 Provided at the ALAGIN Forum (http://www.alagin.jp/). larly in crisis situations. For example, if “infection” appears in a problem report, the tweets including “pulmonary embolism” are also likely to be problem reports. Semantic word class features are used to capture such tendencies. We applied an EM-style word clustering algorithm in Kazama and Torisawa (2008) to 600 million Web pages and clustered 1 million nouns into 500 classes. This algorithm has been used in many works, such as relation extraction (De Saeger et al., 2011) and Why-QA (Oh et al., 2012), and can generate various kinds of semantically clean word classes, such as foods, disease names, and natural disasters. We used the word classes in tweets as features.3 Geographical Locations (GL) Our location recognizer matches tweets against our location dictionary. Location names and their existence/non-existence in tweets constitute evidence, thus we encoded such information into our features"
P13-1159,P10-1026,1,0.358914,"Missing"
P13-1159,W12-2104,0,0.0724532,"Missing"
P13-1159,W11-0309,0,0.0103484,"atform for situational awareness during various crisis situations (Starbird et al., 2010; Vieweg et al., 2010), as sensors for an earthquake reporting system (Sakaki et al., 2010; Okazaki and Matsuo, 2010) or to detect epidemics (Aramaki et al., 2011). Besides Twitter, blogs or forums have also been the target of community response analysis (Qu et al., 2009; Torrey et al., 2007). Similar to our work are the ones of Neubig et al. (2011) and Ishino et al. (2012), who tackle specific problems that occur during disasters (i.e., safety information and transportation information, respectively); and Munro (2011), who extracted “actionable messages” (requests and aids, indiscriminately), matching being performed manually. Our work differs from (Neubig et al., 2011) and (Ishino et al., 2012) in that we do not restrict the range of problem reports, and as opposed to (Munro, 2011), matching is automatic. Systems such as that of Seki (2011)11 or Munro (2013)12 are successful examples of crisis crowdsourcing, but these require extensive human intervention to coordinate useful information. Another category of related work relevant to our task is troubleshooting. Baldwin et al. (2007) and Raghavan et al. (20"
P13-1159,N10-1120,0,0.00734294,"and contradiction extraction (Hashimoto et al., 2012) or Why-QA (Oh et al., 2013). Word Sentiment Polarity (WSP) As we suggested before, full-fledged sentiment analysis to recognize the expressions, including clauses and phrases, that refer to something good or bad was not effective in our task. However, the sentiment polarity, assigned to single words turned out to be effective. To identify the sentiment polarity of words, we employed the word sentiment polarity dictionary used with a sentiment analysis tool for Japanese, the Opinion Extraction Tool software2 , which is an implementation of Nakagawa et al. (2010). The dictionary includes 9,030 positive and 27,951 negative words. Note that we used the Opinion Extraction Tool in the experiments to check the effectiveness of the full-fledged sentiment analysis in this task. Semantic Word Class (SWC) We assume that nouns in the same semantic class behave simi2 Provided at the ALAGIN Forum (http://www.alagin.jp/). larly in crisis situations. For example, if “infection” appears in a problem report, the tweets including “pulmonary embolism” are also likely to be problem reports. Semantic word class features are used to capture such tendencies. We applied an"
P13-1159,D12-1034,1,0.673608,"Missing"
P13-1159,P13-1170,1,0.645467,"Missing"
P13-1159,I11-1108,0,\N,Missing
P13-1170,blanco-etal-2008-causal,0,0.028575,"Missing"
P13-1170,D11-1027,0,0.13572,"Missing"
P13-1170,W05-0306,0,0.0250648,"se causal relation features generated from intra- and inter-sentential causal relations in answer candidates and use them along with the features proposed in our previous work for training our re-ranker. 4 Causal Relations for Why-QA We describe causal relation recognition in Section 4.1 and describe the features (of our re-ranker) generated from causal relations in Section 4.2. 4.1 Causal Relation Recognition We restrict causal relations to those expressed by such cue phrases for causality as (the Japanese counterparts of) because and as a result like in the previous work (Khoo et al., 2000; Inui and Okumura, 2005) and recognize them in the following two steps: extracting causal relation candidates and recognizing causal relations from these candidates. 4.1.1 Extracting Causal Relation Candidates We identify cue phrases for causality in answer candidates using the regular expressions in Table 2. Then, for each identified cue phrase, we extract three sentences as a causal relation candidate, where one contains the cue phrase and the other two are the previous and next sentences in the answer candidates. When there is more than one cue phrase in an answer candidate, we use all of them for extracting the c"
P13-1170,P08-1047,1,0.887907,"of clue terms, including the Japanese counterparts of cause and reason, as query terms for the ranking. The top ranked passages are regarded as answer candidates in the answer re-ranking. See Murata et al. (2007) for more details. Answer re-ranking: Re-ranking the answer candidates is done by a supervised classifier (SVMs) (Vapnik, 1995). In our previous work, we employed three types of features for training the re-ranker: morphosyntactic features (n-grams of morphemes and syntactic dependency chains), semantic word class features (semantic word classes obtained by automatic word clustering (Kazama and Torisawa, 2008)) and sentiment polarity features (word and phrase polarities). Here, we used semantic word classes and sentiment polarities for identifying such semantic associations between a why-question and its answer as “if a disease’s name appears in a question, then answers that include nutrient names are more likely to be correct” by semantic word classes and “if something undesirable happens, the reason is often also something undesirable” by sentiment polarities. In this work, we propose causal relation features generated from intra- and inter-sentential causal relations in answer candidates and use"
P13-1170,P00-1043,0,0.201126,"this work, we propose causal relation features generated from intra- and inter-sentential causal relations in answer candidates and use them along with the features proposed in our previous work for training our re-ranker. 4 Causal Relations for Why-QA We describe causal relation recognition in Section 4.1 and describe the features (of our re-ranker) generated from causal relations in Section 4.2. 4.1 Causal Relation Recognition We restrict causal relations to those expressed by such cue phrases for causality as (the Japanese counterparts of) because and as a result like in the previous work (Khoo et al., 2000; Inui and Okumura, 2005) and recognize them in the following two steps: extracting causal relation candidates and recognizing causal relations from these candidates. 4.1.1 Extracting Causal Relation Candidates We identify cue phrases for causality in answer candidates using the regular expressions in Table 2. Then, for each identified cue phrase, we extract three sentences as a causal relation candidate, where one contains the cue phrase and the other two are the previous and next sentences in the answer candidates. When there is more than one cue phrase in an answer candidate, we use all of"
P13-1170,P07-1098,0,0.029092,"features: As our c-marker features, we use a pair composed of c-marker cm and one j+1 of the following: mj , mj+1 j , sj , or sj . ef4 dep 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1 –tf4 ), partial tree matching (pf1 – pf4 ) and excitation polarity matching (ef1 –ef4 ). We call these causal relations used for generating our causal relation features candidates of an appropriate causal relation in th"
P13-1170,C04-1100,0,0.0166525,"rser. Type tf1 tf2 tf3 C-marker features: As our c-marker features, we use a pair composed of c-marker cm and one j+1 of the following: mj , mj+1 j , sj , or sj . ef4 dep 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1 –tf4 ), partial tree matching (pf1 – pf4 ) and excitation polarity matching (ef1 –ef4 ). We call these causal relations used for generating our causal relation features candidates of an appropria"
P13-1170,W03-1210,0,0.78918,"tsunami hit their country on Friday, March 11, 2011.]cause Table 1: Examples of intra/inter-sentential causal relations. Cause and effect parts of each causal relation, marked with [..]cause and [..]ef f ect , are connected by the underlined cue phrases for causality, such as because, this causes, and are caused by. Introduction “Why-question answering” (why-QA) is a task to retrieve answers from a given text archive for a why-question, such as “Why are tsunamis generated?” The answers are usually text fragments consisting of one or more sentences. Although much research exists on this task (Girju, 2003; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Verberne et al., 2011; Oh et al., 2012), its performance remains much lower than that of the state-of-the-art factoid QA systems, such as IBM’s Watson (Ferrucci et al., 2010). In this work, we propose a quite straightforward but novel approach for such difficult whyQA task. Consider the sentence A1 in Table 1, which represents the causal relation between the cause, “the ocean’s water mass ..., waves are generated,” and its effect, “Tsunamis ... are generated.” This is a good answer to the question, “Why are tsunamis generated?”, since the"
P13-1170,D12-1057,1,0.460425,"Missing"
P13-1170,I08-1055,0,0.271221,"their country on Friday, March 11, 2011.]cause Table 1: Examples of intra/inter-sentential causal relations. Cause and effect parts of each causal relation, marked with [..]cause and [..]ef f ect , are connected by the underlined cue phrases for causality, such as because, this causes, and are caused by. Introduction “Why-question answering” (why-QA) is a task to retrieve answers from a given text archive for a why-question, such as “Why are tsunamis generated?” The answers are usually text fragments consisting of one or more sentences. Although much research exists on this task (Girju, 2003; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Verberne et al., 2011; Oh et al., 2012), its performance remains much lower than that of the state-of-the-art factoid QA systems, such as IBM’s Watson (Ferrucci et al., 2010). In this work, we propose a quite straightforward but novel approach for such difficult whyQA task. Consider the sentence A1 in Table 1, which represents the causal relation between the cause, “the ocean’s water mass ..., waves are generated,” and its effect, “Tsunamis ... are generated.” This is a good answer to the question, “Why are tsunamis generated?”, since the effect part is more or less eq"
P13-1170,J11-2003,0,0.00888305,"one j+1 of the following: mj , mj+1 j , sj , or sj . ef4 dep 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1 –tf4 ), partial tree matching (pf1 – pf4 ) and excitation polarity matching (ef1 –ef4 ). We call these causal relations used for generating our causal relation features candidates of an appropriate causal relation in this section. Note that if one answer candidate has more than one candidate of"
P13-1170,N06-1008,1,0.878062,"Missing"
P13-1170,C08-1120,0,0.0716113,"11, 2011.]cause Table 1: Examples of intra/inter-sentential causal relations. Cause and effect parts of each causal relation, marked with [..]cause and [..]ef f ect , are connected by the underlined cue phrases for causality, such as because, this causes, and are caused by. Introduction “Why-question answering” (why-QA) is a task to retrieve answers from a given text archive for a why-question, such as “Why are tsunamis generated?” The answers are usually text fragments consisting of one or more sentences. Although much research exists on this task (Girju, 2003; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Verberne et al., 2011; Oh et al., 2012), its performance remains much lower than that of the state-of-the-art factoid QA systems, such as IBM’s Watson (Ferrucci et al., 2010). In this work, we propose a quite straightforward but novel approach for such difficult whyQA task. Consider the sentence A1 in Table 1, which represents the causal relation between the cause, “the ocean’s water mass ..., waves are generated,” and its effect, “Tsunamis ... are generated.” This is a good answer to the question, “Why are tsunamis generated?”, since the effect part is more or less equivalent to the (propos"
P13-1170,D12-1034,1,\N,Missing
P13-1170,D11-1076,1,\N,Missing
P13-1170,P13-1159,1,\N,Missing
P14-1093,C08-1001,0,0.0935422,"egarded as event causality only phrase pairs that were interpretable as event causality without contexts (i.e., self-contained). From the training data, our method seemed to successfully learn what selfcontained event causality is. Our scenario generation method generates scenarios by chaining extracted event causality; generating A→B→C from A→B and B→C. The challenge is that many acceptable scenarios are overlooked if we require the joint part of the chain (B 2 Related Work For event causality extraction, clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2 A bacterium in the sea causing food-poisoning. Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality in"
P14-1093,P08-1090,0,0.00970086,"Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2 A bacterium in the sea causing food-poisoning. Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 3 988 feature extraction, the event causality candidates are accompanied by the original sentences from which they were extracted. Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore since we target event causality about two distinct entities. To the best of our knowledge, future scenario generation is a new task, although previous works have addressed similar tasks (Radinsky et al., 2012; Radinsky and Horvitz, 2013). Neither involves chaining and restricts themselves to only one event causality step. Besides, the events they predict must be those for which similar events have previously been observed, and their method only applies to news domain. Some of the scenarios we generated are written on no page in our input web corp"
P14-1093,P11-1109,1,0.90519,"Missing"
P14-1093,D12-1057,1,0.824353,"Missing"
P14-1093,P08-1047,1,0.168244,"Section 3.1. We omit Do et al.’s Dist, which is a constant since we set our window size to one. AC3: Do et al.’s Spa. This is the association measure between arguments and predicates, which is the sum of AC8 and AC9. They are calculated from the 132,528,706 event causality candidates. AC4: Do et al.’s Saa , which is PMI between arguments. We obtained it in the same way as Filter 5 in the supplementary notes. AC5: PMI between predicates. AC6 / AC7: Do et al.’s max / IDF . AC8: PMI between a cause noun and an effect predicate. AC9: PMI between a cause predicate and an effect noun. the method of Kazama and Torisawa (2008). 3.3 Event Causality Scoring Using the above features, a classifier6 classifies each event causality candidate into causality and non-causality. An event causality candidate is given a causality score CScore, which is the SVM score (distance from the hyperplane) that is normalized to [0, 1] by the sigmoid function 1+e1−x . Each event causality candidate may be given multiple original sentences, since a phrase pair can appear in multiple sentences, in which case it is given more than one SVM score. For such candidates, we give the largest score and keep only one original sentence that correspo"
P14-1093,D13-1065,1,0.850567,"Missing"
P14-1093,P09-1068,0,0.0159819,"hose described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2 A bacterium in the sea causing food-poisoning. Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 3 988 feature extraction, the event causality candidates are accompanied by the original sentences from which they were extracted. Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore since we target event causality about two distinct entities. To the best of our knowledge, future scenario generation is a new task, although previous works have addressed similar tasks (Radinsky et al., 2012; Radinsky and Horvitz, 2013). Neither involves chaining and restricts themselves to only one event causality step. Besides, the events they predict must be those for which similar events have previously been observed, and their method only applies to news domain. Some of the scenarios we generated are written on no page in our input web corpus. Similarly, Tsuchida et al."
P14-1093,D11-1076,1,0.91973,"Missing"
P14-1093,D11-1027,0,0.639108,"ning data, our method seemed to successfully learn what selfcontained event causality is. Our scenario generation method generates scenarios by chaining extracted event causality; generating A→B→C from A→B and B→C. The challenge is that many acceptable scenarios are overlooked if we require the joint part of the chain (B 2 Related Work For event causality extraction, clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2 A bacterium in the sea causing food-poisoning. Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 3 988 feature extraction, the event causality candidates are accompanied by the original sentences from which they were extracted. Ot"
P14-1093,D09-1122,1,0.557063,"e high are excitatory and hence sea temperatures are high and sea temperatures rise are causally-compatible.8 Base Features Base features represent the basic properties of event causality like nouns, templates, and their excitation polarities (See Section E in the supplementary notes). For B3 and B4, 500 semantic classes were obtained from our web corpus using 6 We used SVMlight with the polynominal kernel (d = 2), available at http://svmlight.joachims.org. 7 Future work will exploit other original sentences, as suggested by an anonymous reviewer. 8 Using other knowledge like verb entailment (Hashimoto et al., 2009) can be helpful too, which is further future work. 991 Method Proposed w/o Context features w/o Association features w/o Semantic relation features Base features only Scenarios (scs) generated by chaining causallycompatible phrase pairs are scored by Score(sc), which embodies our assumption that an acceptable scenario consists of plausible event causality pairs: Y Score(sc) = CScore(cs) Table 3: Ablation tests. cs∈CAU S(sc) Semantic relations All semantic relations (Proposed) C AUSATION C AUSATION and P REVENTION None (w/o Semantic relation features) where CAU S(sc) is a set of event causality"
P14-1093,P13-1170,1,0.650525,"Missing"
P14-1093,P05-1017,0,0.0586632,"ary patterns like A CAUSES B. Deforestation and global warming might complete the A and B slots. We manually collected 748 binary patterns for this relation. (See Section B in the supplementary notes for examples of our binary patterns.) M ATERIAL is the relation between a material and a product made of it (e.g. plutonium and 5 Hashimoto et al.’s method constructs a network of templates based on their co-occurrence in web sentences with a small number of polarity-assigned seed templates and infers the polarity of all the templates in the network by a constraint solver based on the spin model (Takamura et al., 2005). 4 We used a Japanese dependency parser called J.DepP (Yoshinaga and Kitsuregawa, 2009), available at http:// www.tkl.iis.u-tokyo.ac.jp/∼ynaga/jdepp/. 989 SR1: Binary pattern of our semantic relations that cooccurs with two nouns of an event causality candidate in our web corpus. SR2: Semantic relation types (e.g C AUSATION and E N TAILMENT) of the binary pattern of SR1. E XCITA TION is divided into six sub types based on the excitation polarity of the binary patterns, the argument positions, and the existence of causative markers. A C AUSATION pattern, B BY A, constitutes an independent rela"
P14-1093,C12-2118,0,0.0115642,"ethods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2 A bacterium in the sea causing food-poisoning. Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 3 988 feature extraction, the event causality candidates are accompanied by the original sentences from which they were extracted. Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore since we target event causality about two distinct entities. To the best of our knowledge, future scenario generation is a new task, although previous works have addressed similar tasks (Radinsky et al., 2012; Radinsky and Horvitz, 2013). Neither involves chaining and re"
P14-1093,N06-1008,1,0.787463,"(i.e., self-contained). From the training data, our method seemed to successfully learn what selfcontained event causality is. Our scenario generation method generates scenarios by chaining extracted event causality; generating A→B→C from A→B and B→C. The challenge is that many acceptable scenarios are overlooked if we require the joint part of the chain (B 2 Related Work For event causality extraction, clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2 A bacterium in the sea causing food-poisoning. Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 3 988 feature extraction, the event causality candidates are accompanied by the original sent"
P14-1093,I11-1101,1,0.892688,"Missing"
P14-1093,D09-1160,0,0.0343114,"Missing"
P14-1093,P13-1159,1,\N,Missing
P14-1093,I13-2012,1,\N,Missing
P19-1414,P17-1171,0,0.151559,"posed of a generator and a discriminator: the generator network is trained for generating (from answer passages) fake representations to make it hard for the discriminator network to distinguish these fake representations from the true representations derived from manually created compact-answers. Logistic regression +softmax+dropout ? R: Realrepresentation generator ?(?|?) Real D: Discriminator ? F: Fake? Realrepresentation representation generator Fake Fakerepresentation ?(?|?) (b) AGR and its three subnetworks F , R, and D Figure 1: System architecture pervised open-domain QA (DS-QA) task (Chen et al., 2017), which is an extension of a machinereading task, to check whether it is applicable to other datasets. We combined our generator network with a state-of-the-art DS-QA method, OpenQA (Lin et al., 2018), and used a generated compact-answer representation from a given passage as evidence to 1) select relevant passages from the retrieved ones and 2) find an answer from the selected passages. Although the task was not our initial target (why-QA) and the answers in the DS-QA task were considerably shorter than those in the why-QA, experiments using three publicly available datasets (Quasar-T (Dhingr"
P19-1414,N19-1423,0,0.198974,"were disappointed by the small performance improvement, as shown in our experimental results. We combined the generator network in the AGR with an extension of the state-of-the-art why-QA method (Oh et al., 2017). Our evaluation against a Japanese open-domain why-QA dataset, which was created using general web texts as a source of answer passages, revealed that the generator network significantly improved the accuracy of the top-ranked answer passages and that the combination significantly outperformed several strong baselines, including a combination of a generator network and a BERT model (Devlin et al., 2019). This combination also outperformed a vanilla BERT model, suggesting that the generator network in our AGR may be effective even if it is combined with many types of NN architectures. Another interesting point is that the performance improved even when we replaced, as the inputs to AGR, the word embedding vectors that represent an answer passage, with a random vector. This observation warrants further exploration in our future work. Finally, we applied our AGR to a distantly suCompact-answer representation Passage representation Question representation rc rp rq F in AGR (Pretrained) Passage e"
P19-1414,W03-1210,0,0.154125,"is thought to help prevent the growth of microbes in honey. Despite these properties, honey can be contaminated under certain circumstances. Because its acidity, low water activity, and hydrogen peroxide together hinder the growth of microbes. Table 1: Answer passage A to why-question Q and its compact answer C Introduction Why-question answering (why-QA) tasks retrieve from a text archive answers to such why-questions as “Why does honey last such a long time?” Previous why-QA methods retrieve from a text archive answer passages, each of which consists of several sentences, like A in Table 1 (Girju, 2003; Higashinaka and Isozaki, 2008; Oh et al., 2012, 2013, 2016, 2017; dos Santos et al., 2016; Sharp et al., 2016; Tan et al., 2016; Verberne et al., 2011), and then determine whether the passages answer the question. A proper answer passage must contain (1) a paraphrase of the why-question (e.g., the underlined texts in Table 1) and (2) the reasons or the causes (e.g., the bold texts in Table 1) of the events described in the why-question, both of which are often written in multiple non-adjacent sentences. This multi-sentenceness implies that the answer passages often contain redundant parts th"
P19-1414,D12-1057,1,0.854796,"Missing"
P19-1414,I08-1055,0,0.367783,"help prevent the growth of microbes in honey. Despite these properties, honey can be contaminated under certain circumstances. Because its acidity, low water activity, and hydrogen peroxide together hinder the growth of microbes. Table 1: Answer passage A to why-question Q and its compact answer C Introduction Why-question answering (why-QA) tasks retrieve from a text archive answers to such why-questions as “Why does honey last such a long time?” Previous why-QA methods retrieve from a text archive answer passages, each of which consists of several sentences, like A in Table 1 (Girju, 2003; Higashinaka and Isozaki, 2008; Oh et al., 2012, 2013, 2016, 2017; dos Santos et al., 2016; Sharp et al., 2016; Tan et al., 2016; Verberne et al., 2011), and then determine whether the passages answer the question. A proper answer passage must contain (1) a paraphrase of the why-question (e.g., the underlined texts in Table 1) and (2) the reasons or the causes (e.g., the bold texts in Table 1) of the events described in the why-question, both of which are often written in multiple non-adjacent sentences. This multi-sentenceness implies that the answer passages often contain redundant parts that are not directly related to"
P19-1414,P17-1147,0,0.0252233,"ether it is applicable to other datasets. We combined our generator network with a state-of-the-art DS-QA method, OpenQA (Lin et al., 2018), and used a generated compact-answer representation from a given passage as evidence to 1) select relevant passages from the retrieved ones and 2) find an answer from the selected passages. Although the task was not our initial target (why-QA) and the answers in the DS-QA task were considerably shorter than those in the why-QA, experiments using three publicly available datasets (Quasar-T (Dhingra et al., 2017), SearchQA (Dunn et al., 2017), and TriviaQA (Joshi et al., 2017)) revealed that the generator network improved the performance in most cases. This suggests that AGR may be applicable to many QA-like tasks. 2 Why-QA Model Figure 1 illustrates the architecture of our why-QA model and the AGR. Our why-QA model computes the probability that a given answer passage describes a proper answer to a given why-question 4228 using the representations of a question, an answer passage, and a compact answer. The probability (the why-QA model’s final output) is computed from these representations by our answer selection module, which is a logistic regression layer with dr"
P19-1414,D14-1181,0,0.00671335,"or word tj . Finally, we form two attention feature vectors, s a = [as1 , · · · , as|t |] and ac = [ac1 , · · · , ac|t |], concatenate them into a = [as ; ac ] ∈ R2×|t |, and produce attention-weighted word embedding tatt of given text t, which is either an answer passage or a compact answer: tatt = ReLU(Wt t + Wa a) where Wt ∈ R2d×2d and Wa ∈ R2d×2 are trainable parameters, t is the representation of text t, and ReLU represents the rectified linear units. 3.3.3 CNNs tatt is given to CNNs to generate final representation rt of a given passage/compact-answer t. 4230 The CNNs resembles those in Kim (2014). Convolutions are performed over the word embeddings using both multiple filters and multiple filter windows (e.g., sliding over 1, 2, or 3 word windows at a time and 100 filters for each window). An average pooling operation is applied to the convolution results to generate representation rt , which is the output/value of Encoder(t; θ, q); rt = Encoder(t; θ, q). In our experiments, we set the dimension of representation rt to 300. 4 4.1 Why-QA Experiments Datasets We used three datasets, W hySet, CmpAns, and AddT r, for our why-QA experiments. W hySet and AddT r were used for training and ev"
P19-1414,P14-1093,1,0.860546,"Missing"
P19-1414,P18-1161,0,0.436382,"fake representations from the true representations derived from manually created compact-answers. Logistic regression +softmax+dropout ? R: Realrepresentation generator ?(?|?) Real D: Discriminator ? F: Fake? Realrepresentation representation generator Fake Fakerepresentation ?(?|?) (b) AGR and its three subnetworks F , R, and D Figure 1: System architecture pervised open-domain QA (DS-QA) task (Chen et al., 2017), which is an extension of a machinereading task, to check whether it is applicable to other datasets. We combined our generator network with a state-of-the-art DS-QA method, OpenQA (Lin et al., 2018), and used a generated compact-answer representation from a given passage as evidence to 1) select relevant passages from the retrieved ones and 2) find an answer from the selected passages. Although the task was not our initial target (why-QA) and the answers in the DS-QA task were considerably shorter than those in the why-QA, experiments using three publicly available datasets (Quasar-T (Dhingra et al., 2017), SearchQA (Dunn et al., 2017), and TriviaQA (Joshi et al., 2017)) revealed that the generator network improved the performance in most cases. This suggests that AGR may be applicable t"
P19-1414,C16-2055,1,0.851719,"e AGR, we used CmpAns, the training data set created in Ishida et al. (2018) for compact-answer generation; CmpAns consists of 15,130 triples of a why-question, an answer passage, and a manually-created compact answer. These cover 2,060 unique why-questions. Note that there was no overlap between the questions in CmpAns and those in W hySet. CmpAns was created in the following manner: 1) human annotators manually came up with open-domain whyquestions, 2) retrieved the top-20 passages for each why-question using the open-domain whyQA module of a publicly available web-based QA system WISDOM X (Mizuno et al., 2016; Oh et al., 2016), and 3) three annotators created (when possible) a compact answer for each of the retrieved passages. The passages for which no annotator could create a compact answer were discarded, and were not included in the 15,130 triples mentioned previously. The average lengths of questions, passages, and compact answers in CmpAns were 10.5 words, 184.4 words, and 8.3 words, respectively. Finally, we created additional training data AddT r for training the why-QA models. If an annotator could write a compact answer for a question and an answer passage, she/he probably recognized the"
P19-1414,D12-1034,1,0.888127,"Missing"
P19-1414,P13-1170,1,0.940027,"Missing"
P19-1414,D14-1162,0,0.0821098,"ose majority are noun phrases, unlike the compact answers for our why-QA experiment, i.e., sentences or phrases (8.3 words on average). We trained our AGR with all the triples of 4234 a question, an answer, and a paragraph in the training data of SQuAD-v1.1 under the same settings for the AGR’s hyperparameters as in our why-QA experiment except that we use neither causal word embeddings nor causality-attention. In this experiment, we used the AGR training schemes for Ours(OP) and Ours(RV). We used the 300-dimensional GloVe word embeddings learned from 840 billion tokens in the web crawl data (Pennington et al., 2014), as general word embeddings. Then we combined the resulting fake-representation generator F in the AGR with the state-of-the-art DS-QA method, OpenQA (Lin et al., 2018)7 . We also used the hyperparameters presented in Lin et al. (2018). OpenQA is composed of two components: a paragraph selector to choose relevant paragraphs (or answer passages in our terms) from a set of paragraphs and a paragraph reader to extract answers from the selected paragraphs. For identifying answer a to given question q from set of paragraphs P = {pi }, the paragraph selector and the paragraph reader respectively co"
P19-1414,D16-1264,0,0.0608345,"of a compact answer than on other words. We also in out computed {rorg i }, {ri }, and {ri } with fakerepresentation generator FOP in the same way and observed the same tendency. 5 DS-QA Experiments We tested our framework on another task, the distantly supervised open-domain question answering (DS-QA) task (Chen et al., 2017), to check its generalizability. Table 4 shows the statistics for the datasets used in this experiment. The first three, Quasar-T, SearchQA, and TriviaQA provided by Lin et al. (2018), were used for training and evaluating DS-QA methods. The training data of SQuAD v1.1 (Rajpurkar et al., 2016) was used for training our AGR. The SQuAD dataset consisted of the triples of a question, an answer, and a paragraph that includes the answer. We assume that the answers are our compact answers, although the answers in the dataset are consecutive short word sequences (2.8 words on average), whose majority are noun phrases, unlike the compact answers for our why-QA experiment, i.e., sentences or phrases (8.3 words on average). We trained our AGR with all the triples of 4234 a question, an answer, and a paragraph in the training data of SQuAD-v1.1 under the same settings for the AGR’s hyperparam"
P19-1414,D16-1014,0,0.398581,"minated under certain circumstances. Because its acidity, low water activity, and hydrogen peroxide together hinder the growth of microbes. Table 1: Answer passage A to why-question Q and its compact answer C Introduction Why-question answering (why-QA) tasks retrieve from a text archive answers to such why-questions as “Why does honey last such a long time?” Previous why-QA methods retrieve from a text archive answer passages, each of which consists of several sentences, like A in Table 1 (Girju, 2003; Higashinaka and Isozaki, 2008; Oh et al., 2012, 2013, 2016, 2017; dos Santos et al., 2016; Sharp et al., 2016; Tan et al., 2016; Verberne et al., 2011), and then determine whether the passages answer the question. A proper answer passage must contain (1) a paraphrase of the why-question (e.g., the underlined texts in Table 1) and (2) the reasons or the causes (e.g., the bold texts in Table 1) of the events described in the why-question, both of which are often written in multiple non-adjacent sentences. This multi-sentenceness implies that the answer passages often contain redundant parts that are not directly related to a why-question or its reason/cause and whose presence complicates the why-QA tas"
P19-1414,P16-1044,0,0.415181,"n circumstances. Because its acidity, low water activity, and hydrogen peroxide together hinder the growth of microbes. Table 1: Answer passage A to why-question Q and its compact answer C Introduction Why-question answering (why-QA) tasks retrieve from a text archive answers to such why-questions as “Why does honey last such a long time?” Previous why-QA methods retrieve from a text archive answer passages, each of which consists of several sentences, like A in Table 1 (Girju, 2003; Higashinaka and Isozaki, 2008; Oh et al., 2012, 2013, 2016, 2017; dos Santos et al., 2016; Sharp et al., 2016; Tan et al., 2016; Verberne et al., 2011), and then determine whether the passages answer the question. A proper answer passage must contain (1) a paraphrase of the why-question (e.g., the underlined texts in Table 1) and (2) the reasons or the causes (e.g., the bold texts in Table 1) of the events described in the why-question, both of which are often written in multiple non-adjacent sentences. This multi-sentenceness implies that the answer passages often contain redundant parts that are not directly related to a why-question or its reason/cause and whose presence complicates the why-QA task. Highly accurate"
P98-2132,H90-1055,0,0.0393049,"Missing"
P98-2132,W96-0102,0,0.0235503,"Missing"
P98-2132,J94-2001,0,0.0240852,"Missing"
P98-2132,C94-1027,0,0.0136296,"in the inputs for tagging and that there are 50 POSs. The n-gram models must estin]ate 50 T = 7.8e + 11 n-grams, while the single-neuro tagger with the longest input uses 805 only 70,000 weights, which can be calculated by nipt • n h i d q- n h i d • nopt w h e r e n i p t , n h i d , and nopt are, respectively, the number of units in the input, the hidden, and the output layers, and nhid is set to be nipt/2. T h a t neuro models require few parameters m a y offer another advantage: their performance is less affected by a small amount of training d a t a than that of the statistical methods (Schmid, 1994). Neuro taggers also offer fast tagging compared to other models, although its training stage is longer. 5 Experimental Results The Thai corpus used in the computer experiments contains 10,452 sentences that are randomly divided into two sets: one with 8,322 sentences for training and another with 2,130 sentences for testing. The training and testing sets contain, respectively, 22,311 and 6,717 ambiguous words that serve as more than one POS and were used for training and testing. Because there are 47 types of POSs in Thai (Charoenporn et al., 1997), n in (6), (10), and (14) was set at 47. The"
P98-2132,C96-2160,1,\N,Missing
P98-2132,E95-1025,0,\N,Missing
P98-2132,P98-2144,1,\N,Missing
P98-2132,C98-2139,1,\N,Missing
P98-2144,J94-4001,0,0.213984,"abandon the t r e a t m e n t of rare linguistic p h e n o m e n a is by i n t r o d u c i n g a d d i t i o n a l constraints in feature structures. Regarding (i) and (ii), we introduce 'pseudo-principles', which are unified with ID schemata in the same way principles are unified. Regarding (iii), we add some feature structures to LEs/LETs. 3.1 P o s t p o s i t i o n 'Wa' The main usage of the postposition 'wa' is divided into the following two patternsS: • If two PPs with the postposition 'wa' appear consecutively, we treat the first PP as 5These patterns are almost similar to the ones in (Kurohashi and Nagao, 1994). (a) / (b)* ......... 1. . . . . . . . . . . . . . . . .... ........ .... (c) (~)....... l' I Chil~ . . . . ' .... l_-. I ............i ....... '-"" I tt&x"" g~., ko u. Figure 2: Correct parse tree for Sentence (4) (d)* ......... l ........ T 1 ........ (i) ........ *........... ; *------'t----4 ....... '-'1 ! .._ho(N El D Figure 1: (a) Correct / (b) incorrect parse tree for Sentence (2); (c) correct / (d) incorrect parse tree for Sentence (3) where .a_hc(-, --, --). .a_hc(+, --, 4-). .a_hc(-, +, +). (B) W h e n applying head-modifier schema, also apply: a complement of a predicate just before"
P98-2144,P98-2132,1,0.744407,"epresenting Time and Commas Noun phrases (NPs) with nominal suffixes such as nen (year), gatsu (month), and ji (hour) represent information about time. Such NPs are sometimes used adverbially, rather t h a n nominally. Especially NPs with such a nominal suffix and c o m m a are often used adverbially (Sentence (6) & Figure 4(a) ), while general S P s with a c o m m a are used in coordinate structures (Sentence (7) & Figure 4(b) ). (6) 1995 nen, jishin ga okita. year earthquake -SUBJ Occur-PAST An earthquake occurred in 1995. 4 Experiments We implemented our parser and g r a m m a r in LiLFeS (Makino et al., 1998) s, a featurestructure description language developed by our group. We tested randomly selected 10000 sentences fi'om the Japanese E D R corpus (EDR, 1996). Tile EDR Corpus is a Japanese version of treebank with morphological, structural, and semantic information. In our experiments, we used only the structural information, that is, parse trees. Both the parse trees in our parser and the parse trees in the E D R Corpus are first converted into bunsetsu dependencies, and they are compared when calculating accuracy. Note that the internal structures of bunsetsus, e.~. structures of c o m p o u n"
P98-2144,C96-2160,1,0.798548,"s. And grammar (e) using the combination of the three constraints still works with no side effect. We also measured average parsing time per sentence for the original grammar (a) and the fully augmented grammar (e). The parser we adopted is a naive CKY-style parser. Table 3 gives the average parsing time per sentence for those 2 grammars. Pseudo-principles and further constraints on LEs/LETs also make parsing more time-efficient. Even though they are sometimes considered to be slow in practical application because of their heavy feature structures, actually we found them to improve speed. In (Torisawa and Tsujii, 1996), an efficient HPSG parser is proposed, and our preliminary experiments show that the parsing time of the effident parser is about three times shorter than that of the naive one. Thus, the average parsing time per sentence will be about 300 msec., and we believe our g r a m m a r will achive a practical speed. Other techniques to speed-up the parser are proposed in (Makino et al., 1998). 5 Average parsing time per sentence 1277 (msec) 838 (msec) Discussion This section focuses on the behavior of commas. Out of randomly selected 119 errors in experiment (e), 34 errors are considered to have bee"
P98-2144,C98-2128,1,\N,Missing
P98-2159,P98-2132,1,0.896804,"SPS(JSPS-RFTF96P00502). 968 J u n ' i c h i t$ Rep~ Order~! Figure 1: Agent-based System with the PSTFS ing or semantic processing, are divided into several pieces which can be simultaneously computed by several agents. Several parallel NLP systems have been developed previously. But most of them have been neither efficient nor practical enough (Adriaens and Hahn, 1994). On the other hand, our PSTFS provides the following features. • An efficient communication scheme for messages including Typed Feature Structures (TFSs) (Carpenter, 1992). • Efficient treatment of TFSs by an abstract machine (Makino et al., 1998). Another possible way to develop parallel NLP systems with TFSs is to use a full concurrent logic programming language (Clark and Gregory, 1986; Ueda, 1985). However, we have observed that it is necessary to control parallelism in a flexible way to achieve high-performance. (Fixed concurrency in a logic programming language does not provide sufficient flexibility.) Our agent-based architecture is suitable for accomplishing such flexibility in parallelism. The next section discusses PSTFS from a programmers' point of view. Section 3 describes the PSTFS architecture in detail. Section 4 describ"
P98-2159,P98-2144,1,0.781453,"nally. when computation of Fi (using Fi k and Fk j for all k(i &lt; k &lt; j ) ) is completed, Ci,J d]strlbutes • . . Fi,3 to other agents waiting for Fij. Parsing ]s completed when the computation of F0 n is completed. We have done a series of experiments on a shared-memory parallel machine, SUN Ultra Enterprise 10000 consisting of 64 nodes (each node is a 250 MHz UltraSparc) and 6 GByte shared memory. The corpus consists of 879 random sentences from the EDR Japanese corpus written in Japanese (average length of sentences is 20.8) 4 . The grammar we used is an underspecified Japanese HPSG grammar (Mitsuishi et al., 1998) consisting of 6 ID-schemata and 39 lexical entries (assigned to functional words) and 41 lexical-entry-templates (assigned to parts of speech)• This grammar has wide coverage and high accuracy for real-world texts s. Table 1 shows the result and comparison with a parser written in LiLFeS. Figure 6 shows its speed-up. From the Figure 6, we observe that the maximum speedup reaches up to 12.4 times. The average parsing time is 85 msec per 3CSAs cannot be added dynamically in our implementation. So, to gain the maximum parallelism, we assigned a CSA to each processor. Each Cij asks the CSA on the"
P98-2159,A88-1010,0,\N,Missing
P98-2159,C98-2139,1,\N,Missing
P98-2159,C98-2128,1,\N,Missing
sumida-etal-2008-boosting,D07-1073,1,\N,Missing
sumida-etal-2008-boosting,C92-2082,0,\N,Missing
sumida-etal-2008-boosting,P99-1016,0,\N,Missing
sumida-etal-2008-boosting,W02-1111,0,\N,Missing
sumida-etal-2008-boosting,P06-1015,0,\N,Missing
sumida-etal-2008-boosting,I08-2126,1,\N,Missing
sumida-etal-2008-boosting,W04-3221,0,\N,Missing
sumida-etal-2008-boosting,N04-1010,1,\N,Missing
sumida-etal-2008-boosting,P03-1001,0,\N,Missing
W01-1510,W00-2006,0,0.022452,"to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages. Section 2 reviews the source and the target grammar formalisms of the conversion algorithm. Section 3 describes the conversion algorithm which the core module in the RenTAL system uses. Section"
W01-1510,2000.iwpt-1.9,0,0.035923,"Missing"
W01-1510,P00-1058,0,0.0138769,"r (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2: Elementary trees Derivation trees Derivation translator Parse trees F"
W01-1510,C00-1060,1,0.921494,"nd Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2: Elementary trees Derivation trees Derivation translator Parse trees Figure 1: The RenTAL Syst"
W01-1510,W98-0134,0,0.0195913,"matically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2"
W01-1510,W00-1605,0,0.0466259,"Missing"
W01-1510,C88-2121,0,0.469209,"Missing"
W01-1510,W98-0141,1,0.83789,"rarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them. The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. The derivation translator module takes HPSG parse trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions"
W01-1510,P95-1013,0,0.83404,"trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages. Section 2 reviews the source and the target grammar formalisms of the conversion algorithm. Section 3 describes the conversion algorithm which the core module in the Re"
W01-1510,P98-2144,1,0.85278,"ure 6 shows a rule application to “can run” and “we”. There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts (Flickinger et al., 2000). Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000). In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994). Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000). S anchor * foot node substitution node trunk NP VP V S* think think: Sym: V Sym : VP Arg: Leaf : S Dir : right Foot?: + , Sym : S Leaf : NP Dir : left Foot?: _ Figure 8: A conversion from a canonical elementary tree into an HPSG lexical entry  h mother Sym : Arg :  1 i 2 X*X2XXX 3 4 Arg : 4 5j Sym : 3 Arg : h i substitution node 2 Sym : 1 Leaf : 3 Dir : lef t Foot? : 2 + 3 5 trunk node Figure 9: Left substitution rule 3 Grammar conversion The grammar conversion from LTAG to HPSG (Yoshinaga and Miyao"
W01-1510,W98-0131,0,0.0541757,"Missing"
W01-1510,C88-2147,0,0.0507767,"rammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar. The system is applied to the latest version of the XTAG English grammar. Experimental results show that the obtained HPSG-style grammar successfully worked with an HPSG parser, and achieved a drastic speed-up against an LTAG parser. This system enables to share not only grammars and lexicons but also parsing techniques. 1 Introduction This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar (FB-LTAG1 ) (Vijay-Shanker, 1987; Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion. The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly"
W01-1510,J93-2004,0,\N,Missing
W01-1510,C98-2139,1,\N,Missing
W01-1510,C98-2128,1,\N,Missing
W01-1510,P98-2132,1,\N,Missing
W04-1210,J94-4002,0,\N,Missing
W04-1210,P98-2143,0,\N,Missing
W04-1210,C98-2138,0,\N,Missing
W04-1210,W95-0107,0,\N,Missing
W06-2908,W05-0620,0,0.0416402,"Missing"
W06-2908,J02-3001,0,0.0131431,"e give a solution that uses an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004; Punyakanok et al., 2004; Pradhan et al., 2005a; Pradhan et al., 2005b; Toutanova et al., 2005). Most of the studies have focused on machine learning because of the availability of standard datasets, such as PropBank (Kingsbury and Palmer, 2002). Naturally, the usefulness of parse trees in this task can be anticipated. For example, the recent CoNLL 2005 shared task (Carreras and M`arquez, 2005) provided parse trees for use and their usefulness was ensured. Most of the methods heuristically extract features from parse trees, and from other sources, and u"
W06-2908,W04-2416,0,0.0213929,"pdating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004; Punyakanok et al., 2004; Pradhan et al., 2005a; Pradhan et al., 2005b; Toutanova et al., 2005). Most of the studies have focused on machine learning because of the availability of standard datasets, such as PropBank (Kingsbury and Palmer, 2002). Naturally, the usefulness of parse trees in this task can be anticipated. For example, the recent CoNLL 2005 shared task (Carreras and M`arquez, 2005) provided parse trees for use and their usefulness was ensured. Most of the methods heuristically extract features from parse trees, and from other sources, and use them in machine learning methods base"
W06-2908,H05-1018,1,0.0719837,"s Jun’ichi Kazama and Kentaro Torisawa Japan Advanced Institute of Science and Technology (JAIST) Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan {kazama, torisawa}@jaist.ac.jp Abstract We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel). We extend the kernels on marked ordered labeled trees (Kazama and Torisawa, 2005) so that the mark can be weighted according to its importance. We improve the accuracy by giving more weights on subtrees that contain the predicate and the argument nodes with this ability. Although Kazama and Torisawa (2005) presented fast training with tree kernels, the slow classiﬁcation during runtime remained to be solved. In this paper, we give a solution that uses an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step afte"
W06-2908,kingsbury-palmer-2002-treebank,0,0.0533911,"Missing"
W06-2908,W05-0407,0,0.029547,"Missing"
W06-2908,P04-1043,0,0.0414869,"an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004; Punyakanok et al., 2004; Pradhan et al., 2005a; Pradhan et al., 2005b; Toutanova et al., 2005). Most of the studies have focused on machine learning because of the availability of standard datasets, such as PropBank (Kingsbury and Palmer, 2002). Naturally, the usefulness of parse trees in this task can be anticipated. For example, the recent CoNLL 2005 shared task (Carreras and M`arquez, 2005) provided parse trees for use and their usefulness was ensured. Most of the methods heuristically extract features from parse trees, and from other sources, and use them in machin"
W06-2908,P05-1072,0,0.0129971,"tion. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al., 2004; Punyakanok et al., 2004; Pradhan et al., 2005a; Pradhan et al., 2005b; Toutanova et al., 2005). Most of the studies have focused on machine learning because of the availability of standard datasets, such as PropBank (Kingsbury and Palmer, 2002). Naturally, the usefulness of parse trees in this task can be anticipated. For example, the recent CoNLL 2005 shared task (Carreras and M`arquez, 2005) provided parse trees for use and their usefulness was ensured. Most of the methods heuristically extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation. As a result"
W06-2908,C04-1197,0,0.0707445,"of the methods heuristically extract features from parse trees, and from other sources, and use them in machine learning methods based on feature vector representation. As a result, these methods depend on feature engineering, which is time-consuming. Tree kernels (Collins and Duffy, 2001; Kashima and Koyanagi, 2002) have been proposed to directly handle trees in kernel-based methods, such as SVMs (Vapnik, 1995). Tree kernels calculate the similarity between trees, taking into consideration all of the subtrees, and, therefore there is no need for such feature engineering. Moschitti and Bejan (2004) extensively studied tree kernels for semantic role labeling. However, they reported that they could not successfully build an accurate argument recognizer, although the role assignment was improved. Although Moschitti et al. (2005) reported on argument recognition using tree kernels, it was a preliminary evaluation because they used oracle parse trees. Kazama and Torisawa (2005) proposed a new tree kernel for node relation labeling, as which SRL can be cast. This kernel is deﬁned on marked ordered labeled trees, where a node can have a mark to indicate the existence of a relation. We refer to"
W06-2908,W03-1012,0,0.0610052,"Missing"
W06-2908,P05-1073,0,0.0642226,"d Kentaro Torisawa Japan Advanced Institute of Science and Technology (JAIST) Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan {kazama, torisawa}@jaist.ac.jp Abstract We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel). We extend the kernels on marked ordered labeled trees (Kazama and Torisawa, 2005) so that the mark can be weighted according to its importance. We improve the accuracy by giving more weights on subtrees that contain the predicate and the argument nodes with this ability. Although Kazama and Torisawa (2005) presented fast training with tree kernels, the slow classiﬁcation during runtime remained to be solved. In this paper, we give a solution that uses an efﬁcient DP updating procedure applicable in argument recognition. We demonstrate that the WMOLT kernel improves the accuracy, and our speed-up method makes the recognition more than 40 times faster than the naive classiﬁcation. 1 Introduction Semantic role labeling (SRL) is a task that recognizes the arguments of a predicate (verb) in a sentence and assigns the correct role to each argument. As this task is recognized as an important step afte"
W09-1209,burchardt-etal-2006-salsa,0,0.0198371,"Missing"
W09-1209,I08-1012,1,0.714872,"ods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the l"
W09-1209,D07-1097,0,0.04916,"Missing"
W09-1209,kawahara-etal-2002-construction,0,0.011934,"hnology (NICT) and City University of Hong Kong (CityU) for the joint learning task of CoNLL-2009 shared task (Hajiˇc et al., 2009)1 . The system is basically a pipeline of syntactic parser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their metho"
W09-1209,P08-1068,0,0.10612,"c parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT i"
W09-1209,E06-1011,0,0.013162,"rser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use"
W09-1209,W06-2932,0,0.0235357,"atures for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, number, gender, etc. We try to align different types of morphological-features. For example, 2 http://mstparser.sourceforge.net Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61–66, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics we can obtain a sequence of gender tags of all words from a head h to its d"
W09-1209,P08-1108,0,0.0780057,"ks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, nu"
W09-1209,W03-3017,0,0.0110636,"graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for graphbased parser. Based on the results on development data, we choose the MaltParser for Catalan, Czech, German, and Spanish, and choose another MaxEntbased parser for Chinese, English, and Japanese. 2.4.1 A Transition-based Parser: MaltParser For Catalan, Czech, German, and Spanish, we use the MaltParser, a freely available implementa4 http://www.cs.berkeley.edu/˜pliang/software/browncluster-1.2.zip tion5 , whose details are presented in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use"
W09-1209,W08-2121,0,0.0249233,"Missing"
W09-1209,taule-etal-2008-ancora,0,0.0239121,"Missing"
W09-1209,W08-2127,1,0.761054,"nted in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GT ran refers t"
W09-1209,W09-1208,1,0.895734,"omputational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GT ran refers to dependency parsing trees generated by the MaltParser or MaxEnt-base Parser and ∗ refers to any label. All features are"
W09-3506,J96-1002,0,0.0799941,"). The difference between the two models lies in whether or not a machine transliteration process depends on target-language phonemes. TM-G directly converts source-language graphemes into targetlanguage graphemes, while TM-GP first transforms source language graphemes into targetlanguage phonemes and then target-language phonemes coupled with their corresponding source-language graphemes are converted into target-language graphemes. We used three different machine learning algorithms (conditional random fields (CRFs), margin infused relaxed algorithm (MIRA), and maximum entropy model (MEM)) (Berger et al., 1996; Crammer and Singer, 2003; Lafferty et al., 2001) for building multiple machine transliteration engines. We PT M −G (T |S) = P (TG |S) (1) PT M −GP (T |S)  P (TP |S) × P (TG |TP , S) = (2) ∀TP En S C l i n t o n Ch TP KE L I N D U N Ja TG 克:B 林:B 林:I TP KU TG ク:B R リ:B リ:I TM-G Clinton 克林顿 I 林:I 顿:B 顿:I 顿:I N T O N ン:B ト:B ト:I ン:B TM-GP Clinton クリントン Clinton Clinton KELINDUN KURINTON 克林顿 クリントン Figure 1: Illustration of the two transliteration models 36 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 36–39, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP Figure"
W09-3506,P04-1021,0,0.429966,"Missing"
W09-3506,W03-0430,0,0.0152691,"produces 30-best transliterations for a given source-language word. 3.1 Maximum Entropy Model Machine transliteration based on the maximum entropy model was described in detail in Oh et al. (2006) along with comprehensive evaluation of its performance. We used the same way as that proposed by Oh et al. (2006), thus its full description is not presented here. 3.2 Conditional Random Fields (CRFs) CRFs, a statistical sequence modeling framework, was first introduced by Lafferty et al. (2001). CRFs has been used for sequential labeling problems such as text chunking and named entity recognition (McCallum and Li, 2003). CRF++1 was used in our experiment. TM-G TM-GP 3.3 Margin Infused Relaxed Algorithm Source-language transliteration unit Grapheme Syllable ME-G, CRF-G MIRA-G ME-GP N/A Table 1: Design strategy for multiple transliteration engines The Margin Infused Relaxed Algorithm (MIRA) has been introduced by Crammer and Singer (2003) for large-margin multi-class classification. Kruengkrai et al. (2008) proposed a discriminative model for joint Chinese segmentation and POS tagging, where MIRA was used as their machine learning algorithm. We used the same model for our machine transliteration, exactly joint"
W09-3506,2007.mtsummit-papers.47,1,0.670215,"ative model for joint Chinese segmentation and POS tagging, where MIRA was used as their machine learning algorithm. We used the same model for our machine transliteration, exactly joint syllabication2 and transliteration. 4.2 Combining Methodology We combined the outputs of multiple transliteration engines by means of a re-ranking function, g(x). Let X be a set of transliterations generated by multiple transliteration engines for sourcelanguage word s and ref be a reference transliteration of s. A re-ranking function is defined as Eq. (3), where it ranks ref in X higher and the others lower (Oh and Isahara, 2007). 3.4 Features We used the following features within the ±3 context window3 for the above mentioned three mag(x) : X → {r : r is ordering of x ∈ X} (3) 1 Available at http://crfpp.sourceforge.net/ A syllable in English is defined as a sequence of English grapheme corresponding to one target-language grapheme. 3 The unit of context window is source-language grapheme or syllable. 2 We designed two types of re-ranking functions by using the rank of each individual engine and machine learning algorithm. 37 • grank (x), gF score (x), Rank1 i (x) , P (T |S) 4.2.1 Re-ranking Based on the Rank of In"
W09-3506,P09-1058,1,\N,Missing
W09-3506,W09-3501,0,\N,Missing
W09-3506,W09-3502,0,\N,Missing
W10-3907,P90-1034,0,0.480261,"he type counts of dependency relations, which is roughly equated with the “frequencies” of the terms. Each base term in B is associated with up to 500 of the most distributionally similar terms. This defines T . For M, we used the Jensen-Shannon divergence (JS-divergence) base on the probability distributions derived by an EM-based soft clustering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency relations as distributional information. This makes our method comparable to that used in Hindle (1990). The statistics of the distributional data used were as follows: roughly 920 million types of dependency relations1) were automatically acquired 1) The 920 million types come in two kinds of context triples: 590 million types of (t, p, v) and 320 million types 41 from a large-scale Japanese Web-corpus called the Tsubaki corpus (Shinzato et al., 2008) which consists of roughly 100 million Japanese pages with six billion sentences. After excluding hapax nouns, we had about 33 million types of nouns (in terms of string) and 27 million types of verbs. These nouns were ranked by type count of the"
W10-3907,P08-1047,1,0.851243,"list of n terms T = [ti,1 , ti,2 ,. . . , ti, j , . . . , ti,n ] where ti, j denotes the jth most similarity term in T against bi ∈ B. P(k) are pairs of bi and ti,k , i.e., the k-th most similar term to bi . c. Human raters classify a portion Q of the pairs in P(k) with reference to a classification guideline prepared for the task. Note that the selection of base set B can be independent of the selection of T . Note also that T is indexed by terms in B. To encode this, we write: T [bi ] = [ti,1 , ti,2 ,. . . , ti, j , . . . , ti,n ]. 2.2 Data For T , we used Kazama’s nominal term clustering (Kazama and Torisawa, 2008; Kazama et al., 2009). In this data, base set B for T is one million terms defined by the type counts of dependency relations, which is roughly equated with the “frequencies” of the terms. Each base term in B is associated with up to 500 of the most distributionally similar terms. This defines T . For M, we used the Jensen-Shannon divergence (JS-divergence) base on the probability distributions derived by an EM-based soft clustering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency re"
W10-3907,P98-2127,0,0.390989,"Missing"
W10-3907,P99-1014,0,0.0288406,"Missing"
W10-3907,I08-1025,0,0.0162148,"ering (Kazama and Torisawa, 2008). For convenience, some relevant details of the data construction are described in Appendix A, but in a nutshell, we used dependency relations as distributional information. This makes our method comparable to that used in Hindle (1990). The statistics of the distributional data used were as follows: roughly 920 million types of dependency relations1) were automatically acquired 1) The 920 million types come in two kinds of context triples: 590 million types of (t, p, v) and 320 million types 41 from a large-scale Japanese Web-corpus called the Tsubaki corpus (Shinzato et al., 2008) which consists of roughly 100 million Japanese pages with six billion sentences. After excluding hapax nouns, we had about 33 million types of nouns (in terms of string) and 27 million types of verbs. These nouns were ranked by type count of the two context triples, i.e., (t, p, v) and (n∗ , p∗ ,t). B was determined by selecting the top one million terms with the most variations of context triples. 2.2.1 Sample of T [b] For illustration, we present examples of the Web-derived distributional similar terms. (2) shows the 10 most distributionally similar terms (i.e., [t1070,1 , t1070,2 , . . . ,"
W10-3907,C98-2122,0,\N,Missing
W98-0127,P87-1033,0,0.0223494,"Missing"
W98-0127,P98-2132,1,0.604697,"ssigned to a word, and many constituents are produced during parsing. The experimental results show that our method leads to a significant speed-up. The results also suggest the possihility of optimizing the XTAG system by introducing packing offeature structures and packing of tree structures, although these operations are not currently so apparent. 2 The XHPSG System This section describes the current status of the XHPSG system and the efficiency problem in the system. Both of the grammar and the parser in the XHPSG system are implemented with feature structure description language, LiLFeS (Makino et al., 1998). The grammar consists of lexical entries for about 317 ,000 words, and 10 schemata, which follows schemata of the &apos;This work is partially founded by Japan Society for the Promotion of Science (JSPS-RFTF96P00502). 104 twfica5on ol Pad<ed FllSllK9 Sltvetvn&quot; CKYTable Orir;lnLl XllPSG SJ&gt;lcm SJ&gt;ltm w!lh lhe Pacldo& Moowo Figure 1: Data flow in the parsers for the X11PSG system. HPSG framework in (Pollard and Sag, 199~) with slight modifications. The parser is a simpltCKY-based parser. Currently, the parsing speed of this system is not satisfactory, and we need further impro&apos;e· ment of the parsin"
W98-0127,W98-0141,1,0.86765,"Missing"
W98-0127,C96-2160,1,0.340345,"Missing"
W98-0141,P98-2132,1,0.831682,"Missing"
W98-0141,W98-0127,1,0.868547,"Missing"
W98-0141,C96-2160,1,0.844226,"Missing"
W98-0141,C98-2128,1,\N,Missing
wang-etal-2010-adapting,C02-1148,0,\N,Missing
wang-etal-2010-adapting,P09-1058,1,\N,Missing
wang-etal-2010-adapting,W08-0336,0,\N,Missing
wang-etal-2010-adapting,N03-1017,0,\N,Missing
wang-etal-2010-adapting,P08-1115,0,\N,Missing
wang-etal-2010-adapting,I08-1033,0,\N,Missing
wang-etal-2010-adapting,D08-1076,0,\N,Missing
Y10-1075,I08-1007,1,0.872999,"Missing"
Y10-1075,P94-1038,0,0.256331,"Missing"
Y10-1075,P08-1047,1,0.887762,"Missing"
Y10-1075,D08-1047,0,0.0534269,"Missing"
Y10-1075,I08-1025,0,0.0329099,"Missing"
Y10-1075,C02-1119,0,0.053663,"Missing"
Y10-1075,J01-2002,0,0.052682,"Missing"
Y10-1075,P97-1017,0,\N,Missing
Y10-1080,P00-1041,0,0.0423245,"ncluding plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech other than verbs by using co-occurring words. We can also perform paraphrasing of various kinds of information by using co-occurring words. Banko et al. generated newspaper headlines by using statistical machine translation (Banko et al., 2000). In this study, we handled only the case in which the output summary is one word for simplicity; however, we aim to generate a summary comprising sentences as the output in the future. The case where the output summary comprises two words or sentences can be handled by using an extended version of this method. In this paper, we have only described our idea for handling twoword summaries in Section 4. The summarization process that outputs one-word summaries can also be used to categorize documents, because that one word is representative of the main content and theme of the document. We handl"
Y10-1080,W97-0703,0,0.097702,"of elements in our method was beneficial. Our method obtained 0.75 as the ratio where the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system jud"
Y10-1080,N04-1015,0,0.0356035,"ument include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as much as possible is a suitable summary: (i) Th"
Y10-1080,1993.mtsummit-1.10,0,0.63602,"rds, “strict,” and “lenient.” In “strict,” we evaluated only correct candidates as the correct summary. In “lenient” evaluation, we evaluated candidates similar to a correct candidate as the correct summary. We obtained a high Kappa value of 0.78 and 0.75 for “strict” and “lenient,” respectively, when we evaluated the results using two test subjects in a preliminary experiment (Landis and Koch, 1977). 3.2 Results Evaluated results are shown in Tables 1 and 2. Methods 1 to 4 are described in Section 2.6. Method 5 is a comparison method that uses definition sentences in the EDR word dictionary (EDR, 1993). In Method 5, we selected a candidate word whose definition sentence contained a noun that was also present in the input document; we used these words in the definition sentence of a word x as the related words of the word x. The subsequent working is the same as that of our proposed method. Since Method 5 used a definition sentence, it is related to Kondo et al.’s studies (Kondo and Okumura, 1997). However, Kondo et al. did not propose the method of selecting a candidate from plural candidates. We used our candidate scores that were obtained on the basis of criteria (i) and (ii) for selectin"
Y10-1080,A00-1043,0,0.0246578,"e the top 10 summaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as mu"
Y10-1080,P08-4007,0,0.023681,"by the candidate. In this study, we use co-occurring words as word-association knowledge. By using cooccurring words, a summary can be generated containing words that do not appear in the original document (summarization by paraphrasing). In this aspect, our method is completely different Copyright 2010 by Kazuki Takigawa, Masaki Murata, Masaaki Tsuchida, Stijn De Saeger, Kazuhide Yamamoto, and Kentaro Torisawa 693 694 Student Papers from the existing methods that extract some sentences or words from the original document for generating its summary (Knight and Marcu, 2002; Lin and Hovy, 2003; Kang et al., 2008). In terms of related studies pertaining to summarization by paraphrasing, Kondo et al. proposed a method of paraphrasing plural verbs in the original document into a verb having superordinate concepts including plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech other than verbs by us"
Y10-1080,W00-1100,0,0.390754,"Missing"
Y10-1080,W03-0510,0,0.0335233,"ent is not conveyed by the candidate. In this study, we use co-occurring words as word-association knowledge. By using cooccurring words, a summary can be generated containing words that do not appear in the original document (summarization by paraphrasing). In this aspect, our method is completely different Copyright 2010 by Kazuki Takigawa, Masaki Murata, Masaaki Tsuchida, Stijn De Saeger, Kazuhide Yamamoto, and Kentaro Torisawa 693 694 Student Papers from the existing methods that extract some sentences or words from the original document for generating its summary (Knight and Marcu, 2002; Lin and Hovy, 2003; Kang et al., 2008). In terms of related studies pertaining to summarization by paraphrasing, Kondo et al. proposed a method of paraphrasing plural verbs in the original document into a verb having superordinate concepts including plural verbs concepts by using definition sentences from a word dictionary (Kondo and Okumura, 1997). This method was useful for handling only verbs and could not handle nouns. Another problem is that it cannot handle summarization of the information that is not described in the definition sentences of a word dictionary. In contrast, we can handle parts of speech ot"
Y10-1080,H01-1056,0,0.0238079,"mmaries for each document include a correct summary and 0.45 as the mean reciprocal rank (MRR) in the “lenient” case of experiments. Keywords: Summarization, Word-Association Knowledge, Precision, Recall, Generation 1 Introduction Summarization is one of the important techniques in natural language processing more so because of the development of internet-based technologies and the existence of many documents and many kinds of information on the Web (Hovy and Mareu, 1988; Mani and Maybury, 1999; Barzilay and Elhada, 1997; Goldstein et al., 1999; Marcu, 2000; Ker and Chen, 2000; Hongyan, 2000; Radev et al., 2001; Barzilay and Lee, 2004; Radev et al., 2004; Kato et al., 2005). In this study, our purpose was to make a short summary for sentences. For example, we aimed to make a short summary “terror” for sentences “A bomb went off. Some people were killed. This was triggered by rebel campaign.” In this paper, we have proposed a new method that generates summaries that appropriately and adequately express the contents of the original documents using word-association knowledge. In our proposed method, a system judges that a candidate summary that satisfies the following criteria as much as possible is a"
Y10-1080,W00-1108,0,\N,Missing
Y10-1080,I05-2047,0,\N,Missing
Y15-1063,J96-1002,0,0.0224266,"“may” or “likely,” and situation selecting predicates such as “prevent” or “suggest.” They used these cue words to detect polarity (positive, negative, or unknown) and epistemic modality (certain, probable, possible, or unknown) and combined these two values to indicate an event’s factuality. de Marneffe et al. (2012) annotated the veridicality which roughly corresponds to the factuality for each event by ten annotators to the FactBank corpus (Saur´ı and Pustejovsky, 2009) and trained a classifier to predict the probabilistic distribution of event veridicality using a maximum entropy method (Berger et al., 1996). They compared the distributions predicted by the classifier and the distributions annotated by human annotators. Soni et al. (2014) examined the factuality of quoted statements in tweets. They used the cue words defined in Saur´ı (2008) that introduced the quoted event negation or speculation and the tweet’s author. They reported that conducting factuality analysis for quoted statements is quite difficult due to the error rate. PACLIC 29 核爆発 が 起きる なんて nuclear explosion occur caseadverbial noun marker verb particle デマ だ false rumor be noun verb Figure 1: Japanese bunsetsu example 3 Approach O"
Y15-1063,J12-2003,0,0.0367224,"Missing"
Y15-1063,W07-1522,1,0.725345,"hquake (from March 11 2011 to April 11 2011) and instances whose source is artificial in the table were manually composed of tweetlike texts that included typical examples of complex forms of negations to expand the number of complex negations. Note that we also used the training set as the development set for parameter tuning by 5-fold cross-validation. All of the test set instances were extracted from tweets and there was no overlap between the training and test sets. In both sets, each predicate was annotated by a single annotator by the following steps: 1. We annotated predicates based on Iida et al. (2007). All of the verbs and adjectives were annotated as predicates, and some nouns were annotated as nominal predicates. 2. We annotated negation by the negation cues surrounding the predicate. Both such functional expressions as “ない nai (not)” and such content words as “嘘 だ uso da (it is doubtful that)” are used as negation cues. 3. The predicates (interpreted by the annotator as negated by the cues) are annotated as negated predicates and used as positive instances for SVM, and the others are used as negative instances. Note that recognizing negated instances as either simple or complex is done"
Y15-1063,P08-1047,1,0.870758,"f using the n-gram cluster features with three types of corpora. The n-gram cluster feature generated from Twitter outperforms the other 549 two corpora. The “ML+all” column indicates using three corpora at once. These features are distinguished by their sources. It outperforms the other settings of using each corpora. The comparisons between the “ML” and “ML+all” and “Marneffe12” and “ML+all” suggest that n-gram clusters successfully generalize complex negations forms by their cluster IDs. We compare our n-gram clustering method with “noun-cls,” another clustering method that was proposed by Kazama and Torisawa (2008). We applied their clustering algorithm to nouns extracted from roughly six hundred million Web documents. The Web documents that we used for our clustering are a subset of these documents. The variation of words in the noun-cluster is wider than in other clusters. We set the clustering number to 2,000 in our n-gram clustering method and “noun-cls.” Table 5 compares the clustering method and the corpora that we used. We tried eight dimensions (50, 100, 150, 200, 250, 300, 350, and 500), and the number of dimensions in Table 5 achieved the best performance for each corpus. The “noun-cls” column"
Y15-1063,W04-3230,0,0.0293617,"that we modified the k-means clustering of the word2vec tool so that the word vectors are normalized to the length of the vector. Three distinct corpora were given to the word2vec tool: 1. All of the articles from Japanese Wikipedia (revision of 18 Jan. 2015), 2. Web pages crawled in 2007, i.e., about four years before the earthquake, 3. Twitter data posted from Feb. 14 to 28, 2015, i.e., about four years after the earthquake. We randomly sampled sentences for corpora 2 (4.5 GB) and 3 (4.3 GB) to match Wikipedia’s size (4.2 GB). We tokenized each document with a morphological analyzer MeCab (Kudo et al., 2004) and the Juman PoS tag set (Kurohashi et al., 1994) and applied the word2vec tool and k-means clustering. We needed to choose several parameters, including the 3 https://code.google.com/p/word2vec/ 547 numbers of vector dimensions and clusters, whose values were based on 5-fold cross-validation on our annotated training data, as described in Section 4.1. We tried eight dimensions (50, 100, 150, 200, 250, 300, 350, and 500) of vectors and six numbers of clusters (100, 500, 1,000, 2,000 5,000, and 10,000) for each corpus. For the optimal parameters, which worked best in our preliminary experimen"
Y15-1063,D12-1034,1,0.904234,"Missing"
Y15-1063,P14-2068,0,0.0146095,"sitive, negative, or unknown) and epistemic modality (certain, probable, possible, or unknown) and combined these two values to indicate an event’s factuality. de Marneffe et al. (2012) annotated the veridicality which roughly corresponds to the factuality for each event by ten annotators to the FactBank corpus (Saur´ı and Pustejovsky, 2009) and trained a classifier to predict the probabilistic distribution of event veridicality using a maximum entropy method (Berger et al., 1996). They compared the distributions predicted by the classifier and the distributions annotated by human annotators. Soni et al. (2014) examined the factuality of quoted statements in tweets. They used the cue words defined in Saur´ı (2008) that introduced the quoted event negation or speculation and the tweet’s author. They reported that conducting factuality analysis for quoted statements is quite difficult due to the error rate. PACLIC 29 核爆発 が 起きる なんて nuclear explosion occur caseadverbial noun marker verb particle デマ だ false rumor be noun verb Figure 1: Japanese bunsetsu example 3 Approach Our negation recognition algorithm takes an input sentence and classifies each predicate in it as “negated” or “non-negated.” We train"
Y15-1063,D09-1160,0,0.0236669,"a false rumor.” The vertical bars indicate the bunsetsu boundaries. A Japanese dependency tree is defined as a tree of the dependencies among the bunsetsus. In the above example, the first bunsetsu “核爆発 が kakubakuhatsu ga (nuclear explosion)” depends on another bunsetsu “起きる なんて okiru nante (occur),” which in turn depends on “デマ だ dema da (be false rumor)”. The final bunsetsu is an exceptional case in which both a verb and a noun are included unlike the other bunsetsus that contain either a noun or a verb. Note that in this paper, bunsetsu boundaries and a dependency tree are given by J.DepP (Yoshinaga and Kitsuregawa, 2009). 3.1 Baseline Features Basic Uni-, bi-, and tri-grams of words (surface, base form, and part of speech) in the bunsetsu include the target predicate and its head bunsetsu are used as basic features. The words in the two bunsetsus are distinguished in the feature set. If a bunsetsu includes a target predicate, n-grams are taken only from the strings following it. In the above example sentence, bi-gram “デマ だ dema da (be false rumor)” and uni-gram “デマ dema (false rumor)” are included in this feature set when the target predicate is “起きる okiru (occur).” Negation Words We manually created a short"
