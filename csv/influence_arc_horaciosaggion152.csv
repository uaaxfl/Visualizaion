2010.jeptalnrecital-long.25,W00-0408,0,0.0982815,"Missing"
2010.jeptalnrecital-long.25,W04-1013,0,0.0285786,"Missing"
2010.jeptalnrecital-long.25,N06-1059,0,0.0294203,"Missing"
2010.jeptalnrecital-long.25,D09-1032,0,0.0451431,"Missing"
2010.jeptalnrecital-long.25,N04-1019,0,0.0694417,"Missing"
2010.jeptalnrecital-long.25,W09-2806,0,0.0262145,"Missing"
2010.jeptalnrecital-long.25,P02-1040,0,0.0834545,"Missing"
2010.jeptalnrecital-long.25,W03-2805,1,0.883322,"Missing"
2010.jeptalnrecital-long.25,P03-1048,1,0.861714,"Missing"
2010.jeptalnrecital-long.25,C02-1073,1,0.838747,"Missing"
2010.jeptalnrecital-long.25,vivaldi-etal-2010-automatic,1,0.883924,"Missing"
2020.lrec-1.824,aburaed-etal-2017-sentence,1,0.798802,"Missing"
2020.lrec-1.824,W12-4303,0,0.0266531,"actors to their source documents to investigate how to produce non-extractive indicative abstracts. (Fisas et al., 2016) have created a multi-layered annotated corpus from 40 articles in the domain of Computer Graphics. Sentences are annotated with respect to their role in the argumentative structure of the discourse. It specifies the purpose of each citation in the scientific papers and it identifies special features of the scientific discourse such as advantages and disadvantages. In addition, a grade is allocated to each sentence according to its relevance for being included in a summary. (Athar and Teufel, 2012) created a citation context corpus from the ACL Anthology Network (AAN) which consists of 852 papers that are citing 20 papers. The corpus contains 1,034 paper–reference pairs and 203,803 sentences. It is manually annotated by identifying the sentences in the citation context. It also contains a sentiment annotation as well (negative, positive, objective/neutral). (Teufel, 2006) created a corpus based on 80 Argumentative Zoning-annotated conference articles in the computational linguistics domain. The corpus was created to research classifying academic citations in scientific articles accordin"
2020.lrec-1.824,L18-1298,1,0.883373,"Missing"
2020.lrec-1.824,C10-2049,0,0.4047,"be produced following cut-and-paste summarization strategies (Jaidka et al., 2013) which are typical of document abstracting (insertion, deletion, substitution, etc.) (Endres-Niggemeyer et al., 1995; Saggion, 2011). Recent studies have proposed to take advantage of the scientific paper’s citation network to approach scientific literature summarization. For that reason we introduce here our corpus which we hope will facilitate the usage of citation networks to boost scientific literature summarization research. The generation of related work sections has been studied from different viewpoints (Hoang and Kan, 2010; Vu, 2010; Hu and Wan, 2014), however no manual annotated data-set, analog to the one we will present here, has been produced until now. Our corpus expands considerably the data-set of related work sections used in (Hoang and Kan, 2010) by providing: (i) related work sections, (ii) a manually annotated layer of cited papers and sentences, (iii) citing papers referring to the cited papers in the related work section, and (iv) a layer of rich linguistic, rhetorical, and semantic annotations computed automatically. While the manually identified cited sentences are useful to support the study of"
2020.lrec-1.824,D14-1170,0,0.0212319,"aste summarization strategies (Jaidka et al., 2013) which are typical of document abstracting (insertion, deletion, substitution, etc.) (Endres-Niggemeyer et al., 1995; Saggion, 2011). Recent studies have proposed to take advantage of the scientific paper’s citation network to approach scientific literature summarization. For that reason we introduce here our corpus which we hope will facilitate the usage of citation networks to boost scientific literature summarization research. The generation of related work sections has been studied from different viewpoints (Hoang and Kan, 2010; Vu, 2010; Hu and Wan, 2014), however no manual annotated data-set, analog to the one we will present here, has been produced until now. Our corpus expands considerably the data-set of related work sections used in (Hoang and Kan, 2010) by providing: (i) related work sections, (ii) a manually annotated layer of cited papers and sentences, (iii) citing papers referring to the cited papers in the related work section, and (iv) a layer of rich linguistic, rhetorical, and semantic annotations computed automatically. While the manually identified cited sentences are useful to support the study of sequence to sequence models i"
2020.lrec-1.824,W13-2116,0,0.112642,", Literature Reviews 1. Introduction Most scientific papers include a related work section providing, in a well organized and condensed form, the key information from a carefully selected list of publications which contextualize and ground the research being presented by an author (Rowley and Slack, 2004). Related work sections are critical for quality assessment since journals pay particular attention to them where evaluation of manuscripts is of concern (Maggio et al., 2016). Past research has shown that related work sections can be produced following cut-and-paste summarization strategies (Jaidka et al., 2013) which are typical of document abstracting (insertion, deletion, substitution, etc.) (Endres-Niggemeyer et al., 1995; Saggion, 2011). Recent studies have proposed to take advantage of the scientific paper’s citation network to approach scientific literature summarization. For that reason we introduce here our corpus which we hope will facilitate the usage of citation networks to boost scientific literature summarization research. The generation of related work sections has been studied from different viewpoints (Hoang and Kan, 2010; Vu, 2010; Hu and Wan, 2014), however no manual annotated data"
2020.lrec-1.824,C18-2029,0,0.0608617,"Missing"
2020.lrec-1.824,W09-1325,0,0.0930648,"Missing"
2020.lrec-1.824,C08-1087,0,0.0598117,"nds considerably the data-set of related work sections used in (Hoang and Kan, 2010) by providing: (i) related work sections, (ii) a manually annotated layer of cited papers and sentences, (iii) citing papers referring to the cited papers in the related work section, and (iv) a layer of rich linguistic, rhetorical, and semantic annotations computed automatically. While the manually identified cited sentences are useful to support the study of sequence to sequence models in scientific summarization, the new layer of citing papers facilitates the test of citation-based summarization approaches (Qazvinian and Radev, 2008; Jaidka et al., 2014b) which rely on citation networks to assess sentence relevance. In this corpus we refer to three types of scientific papers: target papers, reference papers, and citing papers which we organize in a two-level network. Level 1 contains target papers with their related work sections we are interested in and, which cite a set of reference papers. Level 2 extends the corpus by adding a layer representing a set of scientific papers explicitly citing the reference papers in Level 1. The rest of this article is organized as follows: The next Section describes related work, then"
2020.lrec-1.824,J02-4005,1,0.522963,"e paper that best represent the citance, as well as their corresponding facets. One of the main problems with the data-set is the lack of agreed manual annotations since only one annotator was in charge of annotating each cluster. Those previously mentioned data-sets are considered the closest to our corpus however they are only equivalent to what we name Level 2 of our corpus and they provide no link between a target paper with a segmented related work section that explicitly mention a set of reference papers. There are also corpora for the study of scientific text mining and summarization. (Saggion and Lapalme, 2002) have aligned 200 abstracts produced by professional abstractors to their source documents to investigate how to produce non-extractive indicative abstracts. (Fisas et al., 2016) have created a multi-layered annotated corpus from 40 articles in the domain of Computer Graphics. Sentences are annotated with respect to their role in the argumentative structure of the discourse. It specifies the purpose of each citation in the scientific papers and it identifies special features of the scientific discourse such as advantages and disadvantages. In addition, a grade is allocated to each sentence acc"
2020.lrec-1.824,J02-4002,0,0.288681,"E system (Maynard et al., 2002; Cunningham et al., 2002), the SUMMA library (Saggion, 2008), and the freely available Dr Inventor library (DRI Framework) (Ronzano and Saggion, 2015). The tools semantically enrich the corpus by providing rhetorical annotation, causality identification, coreference, and BabelNet synsets. The SUMMA library was used to produce different normalized term vectors for each document. Vector of terms and BabelNet synsets are created using tf*idf weighting computed from a corpus of 4K ACL scientific papers. Using 58 gazetteer lists created from the lexicons proposed by (Teufel and Moens, 2002) we identified scientific concepts and actions useful for text summarization. The corpus is available for research and development purposes in two versions1 , one version contains the manual 1 6675 http://taln.upf.edu/sciencecorpus Citing Paper: C08-1031: Mining Opinions in Comparative Sentences Cited Paper: Fiszman-et-al-2007: Interpreting Comparative Constructions in Biomedical Text FISZMAN ET AL (2007) studied the problem of identifying which entity has more of certain features in Citation comparative sentences. PAGE 4 of 4 55: In our sample, expressions interpreted as empty heads include t"
2020.lrec-1.824,bird-etal-2008-acl,0,0.244636,"model analyzing a number of factors including the number of citations to a publication, and the surrounding context for each (Valenzuela et al., 6673 Figure 1: Our corpus outline presenting a target paper, a set of reference papers (Level 1) and for each reference paper a set of citing papers (Level 2) 2015). The ACL Anthology Network (AAN) (Radev et al., 2013) is a wide-range manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. AAN provides citation and collaboration networks of the articles included in the ACL Anthology (Bird et al., 2008) (excluding book reviews). The order of querying the data sources was first Semantic Scholar, then MAG and, finally, ACL. The citing papers were collected from the same source where the reference paper was found. We kept the most cited or most influential papers depending on the source from where the papers were collected. Overall, we collected up to 15 citing papers for each reference paper (with an average of 12 per reference paper). 4.2. Corpus Basic Data Processing We converted the PDF documents for the entire corpus into GATE (Maynard et al., 2002) documents using three converters; Grobid"
2020.lrec-1.824,L16-1492,1,0.794989,"tor was in charge of annotating each cluster. Those previously mentioned data-sets are considered the closest to our corpus however they are only equivalent to what we name Level 2 of our corpus and they provide no link between a target paper with a segmented related work section that explicitly mention a set of reference papers. There are also corpora for the study of scientific text mining and summarization. (Saggion and Lapalme, 2002) have aligned 200 abstracts produced by professional abstractors to their source documents to investigate how to produce non-extractive indicative abstracts. (Fisas et al., 2016) have created a multi-layered annotated corpus from 40 articles in the domain of Computer Graphics. Sentences are annotated with respect to their role in the argumentative structure of the discourse. It specifies the purpose of each citation in the scientific papers and it identifies special features of the scientific discourse such as advantages and disadvantages. In addition, a grade is allocated to each sentence according to its relevance for being included in a summary. (Athar and Teufel, 2012) created a citation context corpus from the ACL Anthology Network (AAN) which consists of 852 pap"
2020.trac-1.13,W18-4401,0,0.0216061,"izers contains two separate files prepared for training and test. The training dataset contains around 4,000 instances (Bhattacharya et al., 2020) with two given labels for each classification type for aggression and misogyny. F1 (weighted) 0.7100 0.7230 0.7246 Accuracy 0.7308 0.7392 0.7375 Table 1: Results for our 3 different submissions for Subtask A. For the agression sub-task we submitted 3 different runs. For the first run we used only the training data provided by the organizers. For the second run we used the additional dataset published with the same task of last year, TRAC-1 dataset (Kumar et al., 2018). For the last run, we used additional dataset from TRAC-1 and changed the optimizer to RmsProp from Adam. System run1 run2 run3 F1 (weighted) 0.8137 0.8199 0.8146 Accuracy 0.8242 0.8242 0.8217 Table 2: Results for our 3 different submissions for Subtask B. Likewise, for the misogyny sub-task we submitted 3 different runs. For the first run, again we used only the training data provided by the organizers. For the second run, we used only the training dataset and changed optimizer to Nadam. For the last run we used an additional misogyny dataset (Lynn et al., 2019). 5. Conclusion In this paper,"
2020.trac-1.13,W17-1101,0,0.0290338,"ith the classes and labels given below: • Overtly Aggressive (OAG), As a solution to this problem, machine learning and deep learning approaches have been utilised to classify text accordingly. Surveys reviewing previous researches indicated that instead of particular features for hate speech; generic features such as n-grams, part of speech, bag of words or embeddings are mainly used and result in reasonable performance. Moreover, character-level approaches work better than token-level approaches. In addition, lexical resources do not seem to be effective unless combined with other features (Schmidt and Wiegand, 2017), (Fortuna and Nunes, 2018). (Zampieri et al., 2019) emphasized the challenges of distinguishing profanity and threatening language which may not actually contain any swearword or profane language overtly. • Covertly Aggressive (CAG), • Non-aggressive (NAG) Sub-task B: Misogynistic Aggression Identification Shared Task with the classes and labels given below: • Gendered (GEN), • Non-gendered (NGEN) The shared task is held in three Languages: English, Hindi, Bangla. With our approach, we participated in both subtasks only for the English language and submitted three different runs for each sub-"
2020.trac-1.13,S19-2120,1,0.884085,"Missing"
2020.trac-1.13,2020.trac-1.25,0,0.0881145,"challenges of distinguishing profanity and threatening language which may not actually contain any swearword or profane language overtly. • Covertly Aggressive (CAG), • Non-aggressive (NAG) Sub-task B: Misogynistic Aggression Identification Shared Task with the classes and labels given below: • Gendered (GEN), • Non-gendered (NGEN) The shared task is held in three Languages: English, Hindi, Bangla. With our approach, we participated in both subtasks only for the English language and submitted three different runs for each sub-task. The methodology used to create this dataset is described in (Bhattacharya et al., 2020). Example instances from the dataset can be seen below: Misogyny is defined as hatred, dislike, or mistrust of women, or prejudice against women 1 . One example of online misogyny is observed in the gender-biased job ads. Although, researches claim that gender discrimination in jobs ads tend to decrease (Tang et al., 2017), with the exponential increase in social media content, the need for –”Homosexuality is against nature. Thats all!” (OAG, GEN) –”worst video” (CAG, NGEN) 1 –”That’s the truth” (NAG, NGEN) 83 https://www.dictionary.com/browse/misogyny?s=t 4. an automated identification mechan"
2021.inlg-1.38,I17-1030,0,0.0126375,"orpus with the SARI score of 43.31, while the other model T5-base+All Tokens performs best on ASSET with SARI score of 45.04 compared to the current state-of-the-art BART+ACCESS with the SARI score of 42.62 on TurkCorpus and 43.63 on ASSET. Following these results, our models out-perform all the state-ofthe-art models in the literature in all approaches: rule-based, supervised and unsupervised approach even without using any additional resources. 5 Human Evaluation In addition to automatic evaluation, we performed a human evaluation on the outputs of different systems. Following recent works (Alva-Manchego et al., 2017; Dong et al., 2019; Zhao et al., 2020), we run our evaluation on Amazon Mechanical Turk by asking five workers to rate using 5-point likert scale on three aspects: (1) Fluency (or Grammaticality): is it grammatically correct and wellformed?, (2) Simplicity: is it simpler than the original sentence?, and (3) Adequacy (or Meaning preservation): does it preserve meaning of the original sentence? More detailed instructions can be found in Appendix A. For this evaluation, we 344 Model ASSET Data TurkCorpus SARI↑ BLEU↑ FKGL↓ SARI↑ BLEU↑ FKGL↓ YATS Rule-based 34.4 72.07 7.65 37.39 74.87 7.67 PBMT-R"
2021.inlg-1.38,2020.acl-main.424,0,0.0229504,"Missing"
2021.inlg-1.38,D19-3009,0,0.0113691,"simplification task. 4 BLEU (Papineni et al., 2002) is originally designed for Machine Translation and is commonly used previously. BLEU has lost its popularity on Text Simplification due to the fact that it correlates poorly with human judgments and often penalizes simpler sentences (Sulem et al., 2018). We keep using it so that we can compare our system with previous systems. FKGL (Kincaid et al., 1975) In addition to SARI and BLEU, we use FKGL to measure readability; however, it does not take into account grammaticality and meaning preservation. We compute SARI, BLEU, and FKGL using EASSE (Alva-Manchego et al., 2019)5 , a simplification evaluation library. Experiments Our model is developed using the Huggingface Transformers library (Wolf et al., 2019)2 with PyTorch3 and Pytorch lightning4 . 2 https://huggingface.co/transformers/ model_doc/t5.html 3 https://pytorch.org 4 https://pytorchlightning.ai 4.3 Training Details We performed hyperparameters search using Optuna (Akiba et al., 2019) with T5-small and reduced 5 343 https://github.com/feralvam/easse size dataset to speed up the process. All models are trained with the same hyperparameters such as a batch size of 6 for T5-base and 12 for T5small, maximu"
2021.inlg-1.38,C96-2183,0,0.376506,"age complexity in both vocabulary and sentence structure while preserving its original information and meaning (Saggion, 2017). Its applications can be used as reading assessment tools for people with low-literacy skills such as children (Watanabe et al., 2009), and nonnative speakers (Paetzold and Specia, 2016), or people with cognitive disabilities such as autism (Barbu et al., 2015), aphasia (Carroll et al., 1999), and dyslexia (Rello et al., 2013a; Matausch and Peb¨ock, 2010). In addition, TS can also be used as a preprocessing step to improve the results of many NLP tasks, e.g., Parsing (Chandrasekar et al., 1996), Information Extraction (Evans, 2011; Jonnalagadda and Gonzalez, 2010), Question Generation (Bernhard et al., 2012), Text Summarization 1 The code and data are available at https://github. com/KimChengSHEANG/TS_T5 • We introduce a transfer learning approach combined with a controllable mechanism for sentence simplification task. • We make an improvement to the performance of the sentence simplification system. • We introduce a new control token #words to help the model generate sentences by replacing long complex words with shorter alternatives. • We conduct an evaluation and comparison betwe"
2021.inlg-1.38,P11-2117,0,0.0174182,"tween different sizes of pre-trained models and a detailed analysis on the effect of each control token. • We show that by choosing the right control token values and pre-trained model, the model achieves the state-of-the-art performance in two well-known benchmarking datasets. 341 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 341–352, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Sentence Simplification It is often regarded as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012), where the models are trained on parallel complex-simple sentences extracted from English Wikipedia and Simple English Wikipedia (SEW) (Zhu et al., 2010). There are many approaches based on statistical Machine Translation (SMT), including phraseˇ based MT (PBMT) (Stajner et al., 2015), and syntax-based MT (SBMT) (Xu et al., 2016). Nisioi et al. (2017) introduced Neural Text Simplification (NTS), a Neural-Machine-Translation-based system (NMT) which performs better than SMT. Zhang and Lapata (2017) took a similar approach adding lexical constraints combining the NMT model"
2021.inlg-1.38,N19-1423,0,0.0384395,"new sentences with a similar length as the source but shorter in word length as we believe that the number characters ratio alone is not enough for the model to generate shorter words. 3 Model In this work, we fine-tune T5 pre-trained model with the controllable mechanism on Text Simplification. T5 (A Unified Text-to-Text Transfer Transformer) (Raffel et al., 2019) is pre-trained on a number of supervised and unsupervised tasks such as machine translation, document summarization, question answering, classification tasks, and reading comprehension, as well as BERT-style token and span masking (Devlin et al., 2019). There are five different variants of T5 pre-trained models: T5small (5 attention modules, 60 million parameters), and T5-base (12 attention modules, 220 million parameters). Due to the limited resources of Colab Pro, we are able to train only T5-small and T5-base. 3.1 Control Tokens We use control tokens to control different aspects of simplification such as compression ratio (#Chars), paraphrasing (Levenshtein similarity), lexical complexity (word rank), and syntactic complexity (the depth of dependency tree) as defined in (Martin et al., 2020b). Then, we add another control token word rati"
2021.inlg-1.38,P19-1331,0,0.0151317,"43.31, while the other model T5-base+All Tokens performs best on ASSET with SARI score of 45.04 compared to the current state-of-the-art BART+ACCESS with the SARI score of 42.62 on TurkCorpus and 43.63 on ASSET. Following these results, our models out-perform all the state-ofthe-art models in the literature in all approaches: rule-based, supervised and unsupervised approach even without using any additional resources. 5 Human Evaluation In addition to automatic evaluation, we performed a human evaluation on the outputs of different systems. Following recent works (Alva-Manchego et al., 2017; Dong et al., 2019; Zhao et al., 2020), we run our evaluation on Amazon Mechanical Turk by asking five workers to rate using 5-point likert scale on three aspects: (1) Fluency (or Grammaticality): is it grammatically correct and wellformed?, (2) Simplicity: is it simpler than the original sentence?, and (3) Adequacy (or Meaning preservation): does it preserve meaning of the original sentence? More detailed instructions can be found in Appendix A. For this evaluation, we 344 Model ASSET Data TurkCorpus SARI↑ BLEU↑ FKGL↓ SARI↑ BLEU↑ FKGL↓ YATS Rule-based 34.4 72.07 7.65 37.39 74.87 7.67 PBMT-R PWKP (Wikipedia) 34"
2021.inlg-1.38,W12-2202,1,0.820259,"Missing"
2021.inlg-1.38,W17-4912,0,0.0236717,"models in sentence simplification. Our proposed model is also a sequence-tosequence Transformer-based model, but instead of using the original Transformer by Vaswani et al. (2017), we use T5 (Raffel et al., 2020). 2.2 Controllable Sentence Simplification In recent years, there has been increased interest in conditional training with sequence-to-sequence models. It has been applied to some NLP tasks such as controlling the length and content of summaries (Kikuchi et al., 2016; Fan et al., 2017), politeness in machine translation (Sennrich et al., 2016), and linguistic style in text generation (Ficler and Goldberg, 2017). Scarton and Specia (2018) introduced the controllable TS model by embedding grade level token &lt;grade> into the sequenceto-sequence model. Martin et al. (2020b) took a similar approach adding 4 tokens into source sentences to control different aspects of the output such as length, paraphrasing, lexical complexity, and syntactic complexity. Kariuk and Karamshuk (2020) took the idea of using control tokens from Martin et al. (2020b) and used it in unsupervised approach by integrating those control tokens into the back translation algorithm, which allows the model to self-supervise the process o"
2021.inlg-1.38,E99-1042,0,0.490963,"large amounts of data, can help improve Text Simplification.1 1 Introduction Text Simplification (TS) can be regarded as a natural language generation task where the generated text has a reduced language complexity in both vocabulary and sentence structure while preserving its original information and meaning (Saggion, 2017). Its applications can be used as reading assessment tools for people with low-literacy skills such as children (Watanabe et al., 2009), and nonnative speakers (Paetzold and Specia, 2016), or people with cognitive disabilities such as autism (Barbu et al., 2015), aphasia (Carroll et al., 1999), and dyslexia (Rello et al., 2013a; Matausch and Peb¨ock, 2010). In addition, TS can also be used as a preprocessing step to improve the results of many NLP tasks, e.g., Parsing (Chandrasekar et al., 1996), Information Extraction (Evans, 2011; Jonnalagadda and Gonzalez, 2010), Question Generation (Bernhard et al., 2012), Text Summarization 1 The code and data are available at https://github. com/KimChengSHEANG/TS_T5 • We introduce a transfer learning approach combined with a controllable mechanism for sentence simplification task. • We make an improvement to the performance of the sentence si"
2021.inlg-1.38,P13-1151,0,0.0181657,"htein, 1966) between the source and target. • WordRank (WR): inverse frequency order of all words in the target divided by that of the source. • DepTreeDepth (DTD): maximum depth of the dependency tree of the target divided by that of the source. Datasets We use the WikiLarge dataset (Zhang and Lapata, 2017) for training. It is the largest and most commonly used text simplification dataset containing 296,402 sentence pairs from automatically aligned complex-simple sentence pairs English Wikipedia and Simple English Wikipedia which is compiled from (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). For validation and testing, we use TurkCorpus (Xu et al., 2016), which has 2000 samples for validation and 359 samples for testing, and each complex sentence has 8 human simplifications. We also use a newly created dataset called ASSET (AlvaManchego et al., 2020) for testing, which contains 2000/359 samples (validation/test) with 10 simplifications per source sentence. 4.2 Evaluation Metrics • #Words (W): number of words ratio between source sentence and target sentence. The number of words in target divided by that of the source. Following previous research (Zhang and Lapata, 2017; Martin e"
2021.inlg-1.38,D16-1140,0,0.0228248,"rase database for simplification called Simple PPDB (Pavlick and Callison-Burch, 2016a). The model outperforms all previous state-of-the-art models in sentence simplification. Our proposed model is also a sequence-tosequence Transformer-based model, but instead of using the original Transformer by Vaswani et al. (2017), we use T5 (Raffel et al., 2020). 2.2 Controllable Sentence Simplification In recent years, there has been increased interest in conditional training with sequence-to-sequence models. It has been applied to some NLP tasks such as controlling the length and content of summaries (Kikuchi et al., 2016; Fan et al., 2017), politeness in machine translation (Sennrich et al., 2016), and linguistic style in text generation (Ficler and Goldberg, 2017). Scarton and Specia (2018) introduced the controllable TS model by embedding grade level token &lt;grade> into the sequenceto-sequence model. Martin et al. (2020b) took a similar approach adding 4 tokens into source sentences to control different aspects of the output such as length, paraphrasing, lexical complexity, and syntactic complexity. Kariuk and Karamshuk (2020) took the idea of using control tokens from Martin et al. (2020b) and used it in un"
2021.inlg-1.38,N19-1317,0,0.0244182,"Missing"
2021.inlg-1.38,2020.acl-main.703,0,0.0400813,"beam size of 8. Our models are trained and evaluated using Google Colab Pro, which has a random GPU T4 or P100. Both have 16GB of memory, up to 25GB of RAM, and a time limit of 24h maximum for the execution of cells. Training of T5-base model for 5 epochs usually takes around 20 hours. 4.4 ACCESS (Martin et al., 2020b) Seq2Seq system trained with four control tokens attached to source sentence: character length ratio, Levenshtein similarity ratio, word rank ratio, and dependency tree depth ratio between source and target sentence. BART+ACCESS (Martin et al., 2020a) The system fine-tunes BART (Lewis et al., 2020) and adds the simplification control tokens from ACCESS. 4.6 Baselines We benchmark our model against several wellknown state-of-the-art systems: YATS (Ferr´es et al., 2016)6 Rule-based system with linguistically motivated rule-based syntactic analysis and corpus-based lexical simplifier which generates sentences based on part-of-speech tags and dependency information. PBMT-R (Wubben et al., 2012) Phrase-based MT system trained on a monolingual parallel corpus with candidate re-ranking based on dissimilarity using Levenshtein distance. UNTS (Surya et al., 2019) Unsupervised Neural Text Simplif"
2021.inlg-1.38,2020.lrec-1.577,0,0.108575,"Missing"
2021.inlg-1.38,P17-2014,0,0.0177956,"–352, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Sentence Simplification It is often regarded as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012), where the models are trained on parallel complex-simple sentences extracted from English Wikipedia and Simple English Wikipedia (SEW) (Zhu et al., 2010). There are many approaches based on statistical Machine Translation (SMT), including phraseˇ based MT (PBMT) (Stajner et al., 2015), and syntax-based MT (SBMT) (Xu et al., 2016). Nisioi et al. (2017) introduced Neural Text Simplification (NTS), a Neural-Machine-Translation-based system (NMT) which performs better than SMT. Zhang and Lapata (2017) took a similar approach adding lexical constraints combining the NMT model with reinforcement learning. After the release of Transformer (Vaswani et al., 2017), Zhao et al. (2018a) introduced a Transformer-based approach and integrated it with a paraphrase database for simplification called Simple PPDB (Pavlick and Callison-Burch, 2016a). The model outperforms all previous state-of-the-art models in sentence simplification. Our proposed model is"
2021.inlg-1.38,W16-4912,0,0.0326797,"tate-of-the-art (BART+ACCESS). We argue that using a pre-trained model such as T5, trained on several tasks with large amounts of data, can help improve Text Simplification.1 1 Introduction Text Simplification (TS) can be regarded as a natural language generation task where the generated text has a reduced language complexity in both vocabulary and sentence structure while preserving its original information and meaning (Saggion, 2017). Its applications can be used as reading assessment tools for people with low-literacy skills such as children (Watanabe et al., 2009), and nonnative speakers (Paetzold and Specia, 2016), or people with cognitive disabilities such as autism (Barbu et al., 2015), aphasia (Carroll et al., 1999), and dyslexia (Rello et al., 2013a; Matausch and Peb¨ock, 2010). In addition, TS can also be used as a preprocessing step to improve the results of many NLP tasks, e.g., Parsing (Chandrasekar et al., 1996), Information Extraction (Evans, 2011; Jonnalagadda and Gonzalez, 2010), Question Generation (Bernhard et al., 2012), Text Summarization 1 The code and data are available at https://github. com/KimChengSHEANG/TS_T5 • We introduce a transfer learning approach combined with a controllable"
2021.inlg-1.38,P02-1040,0,0.110195,"y adopted metric and we use it as an overall score. Source simplify: W 0.58 C 0.52 L 0.67 WR 0.92 DTD 0.71 In architectural decoration Small pieces of colored and iridescent shell have been used to create mosaics and inlays, which have been used to decorate walls, furniture and boxes. Target Small pieces of colored and shiny shell has been used to decorate walls, furniture and boxes. Table 1: This table shows how control tokens are embedded into the source sentence for training. The keyword simplify is added at the beginning of each source sentence to mark it as a simplification task. 4 BLEU (Papineni et al., 2002) is originally designed for Machine Translation and is commonly used previously. BLEU has lost its popularity on Text Simplification due to the fact that it correlates poorly with human judgments and often penalizes simpler sentences (Sulem et al., 2018). We keep using it so that we can compare our system with previous systems. FKGL (Kincaid et al., 1975) In addition to SARI and BLEU, we use FKGL to measure readability; however, it does not take into account grammaticality and meaning preservation. We compute SARI, BLEU, and FKGL using EASSE (Alva-Manchego et al., 2019)5 , a simplification eva"
2021.inlg-1.38,P16-2024,0,0.30435,"e Translation (SMT), including phraseˇ based MT (PBMT) (Stajner et al., 2015), and syntax-based MT (SBMT) (Xu et al., 2016). Nisioi et al. (2017) introduced Neural Text Simplification (NTS), a Neural-Machine-Translation-based system (NMT) which performs better than SMT. Zhang and Lapata (2017) took a similar approach adding lexical constraints combining the NMT model with reinforcement learning. After the release of Transformer (Vaswani et al., 2017), Zhao et al. (2018a) introduced a Transformer-based approach and integrated it with a paraphrase database for simplification called Simple PPDB (Pavlick and Callison-Burch, 2016a). The model outperforms all previous state-of-the-art models in sentence simplification. Our proposed model is also a sequence-tosequence Transformer-based model, but instead of using the original Transformer by Vaswani et al. (2017), we use T5 (Raffel et al., 2020). 2.2 Controllable Sentence Simplification In recent years, there has been increased interest in conditional training with sequence-to-sequence models. It has been applied to some NLP tasks such as controlling the length and content of summaries (Kikuchi et al., 2016; Fan et al., 2017), politeness in machine translation (Sennrich"
2021.inlg-1.38,P18-2113,0,0.0126935,"ation. Our proposed model is also a sequence-tosequence Transformer-based model, but instead of using the original Transformer by Vaswani et al. (2017), we use T5 (Raffel et al., 2020). 2.2 Controllable Sentence Simplification In recent years, there has been increased interest in conditional training with sequence-to-sequence models. It has been applied to some NLP tasks such as controlling the length and content of summaries (Kikuchi et al., 2016; Fan et al., 2017), politeness in machine translation (Sennrich et al., 2016), and linguistic style in text generation (Ficler and Goldberg, 2017). Scarton and Specia (2018) introduced the controllable TS model by embedding grade level token &lt;grade> into the sequenceto-sequence model. Martin et al. (2020b) took a similar approach adding 4 tokens into source sentences to control different aspects of the output such as length, paraphrasing, lexical complexity, and syntactic complexity. Kariuk and Karamshuk (2020) took the idea of using control tokens from Martin et al. (2020b) and used it in unsupervised approach by integrating those control tokens into the back translation algorithm, which allows the model to self-supervise the process of learning inter-relations"
2021.inlg-1.38,N16-1005,0,0.0279622,"rch, 2016a). The model outperforms all previous state-of-the-art models in sentence simplification. Our proposed model is also a sequence-tosequence Transformer-based model, but instead of using the original Transformer by Vaswani et al. (2017), we use T5 (Raffel et al., 2020). 2.2 Controllable Sentence Simplification In recent years, there has been increased interest in conditional training with sequence-to-sequence models. It has been applied to some NLP tasks such as controlling the length and content of summaries (Kikuchi et al., 2016; Fan et al., 2017), politeness in machine translation (Sennrich et al., 2016), and linguistic style in text generation (Ficler and Goldberg, 2017). Scarton and Specia (2018) introduced the controllable TS model by embedding grade level token &lt;grade> into the sequenceto-sequence model. Martin et al. (2020b) took a similar approach adding 4 tokens into source sentences to control different aspects of the output such as length, paraphrasing, lexical complexity, and syntactic complexity. Kariuk and Karamshuk (2020) took the idea of using control tokens from Martin et al. (2020b) and used it in unsupervised approach by integrating those control tokens into the back translat"
2021.inlg-1.38,C04-1129,0,0.306259,"Missing"
2021.inlg-1.38,R15-1080,1,0.805463,"rnational Conference on Natural Language Generation (INLG), pages 341–352, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Sentence Simplification It is often regarded as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012), where the models are trained on parallel complex-simple sentences extracted from English Wikipedia and Simple English Wikipedia (SEW) (Zhu et al., 2010). There are many approaches based on statistical Machine Translation (SMT), including phraseˇ based MT (PBMT) (Stajner et al., 2015), and syntax-based MT (SBMT) (Xu et al., 2016). Nisioi et al. (2017) introduced Neural Text Simplification (NTS), a Neural-Machine-Translation-based system (NMT) which performs better than SMT. Zhang and Lapata (2017) took a similar approach adding lexical constraints combining the NMT model with reinforcement learning. After the release of Transformer (Vaswani et al., 2017), Zhao et al. (2018a) introduced a Transformer-based approach and integrated it with a paraphrase database for simplification called Simple PPDB (Pavlick and Callison-Burch, 2016a). The model outperforms all previous state-"
2021.inlg-1.38,W16-3411,0,0.0559083,"Missing"
2021.inlg-1.38,R19-1131,0,0.0430849,"Missing"
2021.inlg-1.38,D18-1081,0,0.0330903,"Missing"
2021.inlg-1.38,P19-1198,0,0.0265201,"20a) The system fine-tunes BART (Lewis et al., 2020) and adds the simplification control tokens from ACCESS. 4.6 Baselines We benchmark our model against several wellknown state-of-the-art systems: YATS (Ferr´es et al., 2016)6 Rule-based system with linguistically motivated rule-based syntactic analysis and corpus-based lexical simplifier which generates sentences based on part-of-speech tags and dependency information. PBMT-R (Wubben et al., 2012) Phrase-based MT system trained on a monolingual parallel corpus with candidate re-ranking based on dissimilarity using Levenshtein distance. UNTS (Surya et al., 2019) Unsupervised Neural Text Simplification is based on the encodeattend-decode style architecture (Bahdanau et al., 2014) with a shared encoder and two decoders and trained on unlabeled data extracted from English Wikipedia dump. Dress-LS (Zhang and Lapata, 2017) A Seq2Seq model trained with deep reinforcement learning 6 DMASS+DCSS (Zhao et al., 2018b) A Seq2Seq model trained with the original Transformer architecture (Vaswani et al., 2017) combined with the simple paraphrase database for simplification PPDB. (Pavlick and Callison-Burch, 2016b). Choosing Control Token Values at Inference In this"
2021.inlg-1.38,N18-2013,0,0.0341147,"Missing"
2021.inlg-1.38,2020.emnlp-demos.6,0,0.0942737,"Missing"
2021.inlg-1.38,D11-1038,0,0.0417697,"enshtein similarity (Levenshtein, 1966) between the source and target. • WordRank (WR): inverse frequency order of all words in the target divided by that of the source. • DepTreeDepth (DTD): maximum depth of the dependency tree of the target divided by that of the source. Datasets We use the WikiLarge dataset (Zhang and Lapata, 2017) for training. It is the largest and most commonly used text simplification dataset containing 296,402 sentence pairs from automatically aligned complex-simple sentence pairs English Wikipedia and Simple English Wikipedia which is compiled from (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). For validation and testing, we use TurkCorpus (Xu et al., 2016), which has 2000 samples for validation and 359 samples for testing, and each complex sentence has 8 human simplifications. We also use a newly created dataset called ASSET (AlvaManchego et al., 2020) for testing, which contains 2000/359 samples (validation/test) with 10 simplifications per source sentence. 4.2 Evaluation Metrics • #Words (W): number of words ratio between source sentence and target sentence. The number of words in target divided by that of the source. Following previous research (Zhang and Lapata"
2021.inlg-1.38,P12-1107,0,0.0791216,"Missing"
2021.inlg-1.38,Q16-1029,0,0.115409,"(INLG), pages 341–352, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Sentence Simplification It is often regarded as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012), where the models are trained on parallel complex-simple sentences extracted from English Wikipedia and Simple English Wikipedia (SEW) (Zhu et al., 2010). There are many approaches based on statistical Machine Translation (SMT), including phraseˇ based MT (PBMT) (Stajner et al., 2015), and syntax-based MT (SBMT) (Xu et al., 2016). Nisioi et al. (2017) introduced Neural Text Simplification (NTS), a Neural-Machine-Translation-based system (NMT) which performs better than SMT. Zhang and Lapata (2017) took a similar approach adding lexical constraints combining the NMT model with reinforcement learning. After the release of Transformer (Vaswani et al., 2017), Zhao et al. (2018a) introduced a Transformer-based approach and integrated it with a paraphrase database for simplification called Simple PPDB (Pavlick and Callison-Burch, 2016a). The model outperforms all previous state-of-the-art models in sentence simplification."
2021.inlg-1.38,D17-1062,0,0.23278,"is often regarded as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012), where the models are trained on parallel complex-simple sentences extracted from English Wikipedia and Simple English Wikipedia (SEW) (Zhu et al., 2010). There are many approaches based on statistical Machine Translation (SMT), including phraseˇ based MT (PBMT) (Stajner et al., 2015), and syntax-based MT (SBMT) (Xu et al., 2016). Nisioi et al. (2017) introduced Neural Text Simplification (NTS), a Neural-Machine-Translation-based system (NMT) which performs better than SMT. Zhang and Lapata (2017) took a similar approach adding lexical constraints combining the NMT model with reinforcement learning. After the release of Transformer (Vaswani et al., 2017), Zhao et al. (2018a) introduced a Transformer-based approach and integrated it with a paraphrase database for simplification called Simple PPDB (Pavlick and Callison-Burch, 2016a). The model outperforms all previous state-of-the-art models in sentence simplification. Our proposed model is also a sequence-tosequence Transformer-based model, but instead of using the original Transformer by Vaswani et al. (2017), we use T5 (Raffel et al.,"
2021.inlg-1.38,D18-1355,0,0.0740532,"Missing"
2021.inlg-1.38,C10-1152,0,0.273438,"and comparison between different sizes of pre-trained models and a detailed analysis on the effect of each control token. • We show that by choosing the right control token values and pre-trained model, the model achieves the state-of-the-art performance in two well-known benchmarking datasets. 341 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 341–352, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Sentence Simplification It is often regarded as a monolingual translation problem (Zhu et al., 2010; Coster and Kauchak, 2011; Wubben et al., 2012), where the models are trained on parallel complex-simple sentences extracted from English Wikipedia and Simple English Wikipedia (SEW) (Zhu et al., 2010). There are many approaches based on statistical Machine Translation (SMT), including phraseˇ based MT (PBMT) (Stajner et al., 2015), and syntax-based MT (SBMT) (Xu et al., 2016). Nisioi et al. (2017) introduced Neural Text Simplification (NTS), a Neural-Machine-Translation-based system (NMT) which performs better than SMT. Zhang and Lapata (2017) took a similar approach adding lexical constrain"
barbieri-saggion-2014-modelling-irony,N03-1033,0,\N,Missing
barbieri-saggion-2014-modelling-irony,W13-1605,0,\N,Missing
barbieri-saggion-2014-modelling-irony,filatova-2012-irony,0,\N,Missing
barbieri-saggion-2014-modelling-irony,P11-2102,0,\N,Missing
barbieri-saggion-2014-modelling-irony,ide-suderman-2004-american,0,\N,Missing
barbieri-saggion-2014-modelling-irony,W13-1104,0,\N,Missing
barbieri-saggion-2014-modelling-irony,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
barbieri-saggion-2014-modelling-irony,R13-1011,0,\N,Missing
bautista-saggion-2014-numerical,padro-etal-2010-freeling,0,\N,Missing
bautista-saggion-2014-numerical,C96-2183,0,\N,Missing
bautista-saggion-2014-numerical,J12-1004,0,\N,Missing
bautista-saggion-2014-numerical,C10-1152,0,\N,Missing
bautista-saggion-2014-numerical,W09-0620,0,\N,Missing
bott-etal-2012-text,W09-1210,0,\N,Missing
bott-etal-2012-text,C96-2183,0,\N,Missing
bott-etal-2012-text,C10-1152,0,\N,Missing
bott-etal-2012-text,W00-1436,0,\N,Missing
bott-etal-2012-text,W11-2802,0,\N,Missing
bott-etal-2012-text,W11-1601,0,\N,Missing
bott-etal-2012-text,W11-1603,1,\N,Missing
bott-etal-2012-text,mille-wanner-2008-making,1,\N,Missing
bott-etal-2012-text,taule-etal-2008-ancora,0,\N,Missing
bott-etal-2012-text,W05-0610,0,\N,Missing
C02-1073,J93-1004,0,\N,Missing
C02-1073,A00-2024,0,\N,Missing
C02-1073,saggion-etal-2002-developing,1,\N,Missing
C02-1073,A00-2035,0,\N,Missing
C02-1073,W00-0401,1,\N,Missing
C02-1073,W00-0408,0,\N,Missing
C02-1073,E99-1011,0,\N,Missing
C02-1073,W97-0704,0,\N,Missing
C02-1073,W00-0403,1,\N,Missing
C02-1073,grover-etal-2000-lt,0,\N,Missing
C10-2122,W00-0408,0,0.0582947,"o a question or set of questions) a Responsiveness score is also assigned to each summary which indicates how responsive the summary is to the question(s). Because manual comparison of peer summaries with model summaries is an arduous and costly 1 http://www.nist.gov/tac 1059 Coling 2010: Poster Volume, pages 1059–1067, Beijing, August 2010 process, a body of research has been produced in the last decade on automatic content-based evaluation procedures. Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries (Donaway et al., 2000), various vocabulary overlap measures such as set of n-grams overlap or longest common subsequence between peer and model have also been proposed (Saggion et al., 2002; Radev et al., 2003). The Bleu machine translation evaluation measure (Papineni et al., 2002) has also been tested in summarization (Pastra and Saggion, 2003). The DUC conferences adopted the ROUGE package for content-based evaluation (Lin, 2004). It implements a series of recall measures based on ngram co-occurrence statistics between a peer summary and a set of model summaries. ROUGE measures can be used to produce systems ran"
C10-2122,N03-1020,0,0.121627,"duced by recall and cosinebased measures. They showed that there was weak correlation between rankings produced by recall, but that content-based measures produce rankings which were strongly correlated, thus paving the way for content-based measures in text summarization evaluation. Radev et al. (2003) also compared various evaluation measures based on vocabulary overlap. Although these measures were able to separate random from non-random systems, no clear conclusion was reached on the value of each of the measures studied. Nowadays, a widespread summarization evaluation framework is ROUGE (Lin and Hovy, 2003) which, as we have mentioned before, offers a set of statistics that compare peer summaries with models. Various statistics exist depending on the used n-gram and on the type of text processing applied to the input texts (e.g., lemmatization, stopword removal). Lin et al. (2006) proposed a method of evaluation based on the use of “distances” or divergences between two probability distributions (the distribution of units in the automatic summary and the distribution of units in the model summary). They studied two different Information Theoretic measures of divergence: the Kullback-Leibler (KL)"
C10-2122,N06-1059,0,0.0395723,"on. Radev et al. (2003) also compared various evaluation measures based on vocabulary overlap. Although these measures were able to separate random from non-random systems, no clear conclusion was reached on the value of each of the measures studied. Nowadays, a widespread summarization evaluation framework is ROUGE (Lin and Hovy, 2003) which, as we have mentioned before, offers a set of statistics that compare peer summaries with models. Various statistics exist depending on the used n-gram and on the type of text processing applied to the input texts (e.g., lemmatization, stopword removal). Lin et al. (2006) proposed a method of evaluation based on the use of “distances” or divergences between two probability distributions (the distribution of units in the automatic summary and the distribution of units in the model summary). They studied two different Information Theoretic measures of divergence: the Kullback-Leibler (KL) (Kullback and Leibler, 1951) and Jensen-Shannon (J S) (Lin, 1991a) divergences. In this work we use the Jensen-Shannon (J S) divergence that is defined as follows: DJ S (P ||Q) = 2Pw 1X Pw log2 2 Pw + Qw w + Qw log2 2Qw Pw + Qw (1) This measure can be applied to the distributio"
C10-2122,W04-1013,0,0.136825,"ent-based evaluation procedures. Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries (Donaway et al., 2000), various vocabulary overlap measures such as set of n-grams overlap or longest common subsequence between peer and model have also been proposed (Saggion et al., 2002; Radev et al., 2003). The Bleu machine translation evaluation measure (Papineni et al., 2002) has also been tested in summarization (Pastra and Saggion, 2003). The DUC conferences adopted the ROUGE package for content-based evaluation (Lin, 2004). It implements a series of recall measures based on ngram co-occurrence statistics between a peer summary and a set of model summaries. ROUGE measures can be used to produce systems ranks. It has been shown that system rankings produced by some ROUGE measures (e.g., ROUGE-2 which uses bi-grams) correlate with rankings produced using coverage. In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. It is based on the distribution of “content” in a set of model summaries. Summary Content Units (SCUs) are first identified in the model summaries, then each SC"
C10-2122,D09-1032,0,0.73848,"were used and that Pyramids rankings correlate with rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE with skip bi-grams). Still this method requires the creation of models and the identification, matching, and weighting of SCUs in both models and peers. Donaway et al. (2000) put forward the idea of using directly the full document for comparison purposes, and argued that content-based measures which compare the document to the summary may be acceptable substitutes for those using model summaries. A method for evaluation of summarization systems without models has been recently proposed (Louis and Nenkova, 2009). It is based on the direct content-based comparison between summaries and their corresponding source documents. Louis and Nenkova (2009) evaluated the effectiveness of the Jensen-Shannon (Lin, 1991b) theoretic measure in predicting systems ranks in two summarization tasks query-focused and update summarization. They have shown that ranks produced by Pyramids and ranks produced by the Jensen-Shannon measure correlate. However, they did not investigate the effect of the measure in past summarization tasks such as generic multi-document summarization (DUC 2004 Task 2), biographical summarization"
C10-2122,N04-1019,0,0.175811,"ranslation evaluation measure (Papineni et al., 2002) has also been tested in summarization (Pastra and Saggion, 2003). The DUC conferences adopted the ROUGE package for content-based evaluation (Lin, 2004). It implements a series of recall measures based on ngram co-occurrence statistics between a peer summary and a set of model summaries. ROUGE measures can be used to produce systems ranks. It has been shown that system rankings produced by some ROUGE measures (e.g., ROUGE-2 which uses bi-grams) correlate with rankings produced using coverage. In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. It is based on the distribution of “content” in a set of model summaries. Summary Content Units (SCUs) are first identified in the model summaries, then each SCU receives a weight which is the number of models containing or expressing the same unit. Peer SCUs are identified in the peer, matched against model SCUs, and weighted accordingly. The Pyramids score given to the peer is the ratio of the sum of the weights of its units and the sum of the weights of the best possible ideal summary with the same number of SCUs as the peer. The Pyramids scores can be used for ranking summ"
C10-2122,vivaldi-etal-2010-automatic,1,0.835628,"Missing"
C10-2122,W09-2806,0,0.285151,"the creation of models and the identification, matching, and weighting of SCUs in both models and peers. Donaway et al. (2000) put forward the idea of using directly the full document for comparison purposes, and argued that content-based measures which compare the document to the summary may be acceptable substitutes for those using model summaries. A method for evaluation of summarization systems without models has been recently proposed (Louis and Nenkova, 2009). It is based on the direct content-based comparison between summaries and their corresponding source documents. Louis and Nenkova (2009) evaluated the effectiveness of the Jensen-Shannon (Lin, 1991b) theoretic measure in predicting systems ranks in two summarization tasks query-focused and update summarization. They have shown that ranks produced by Pyramids and ranks produced by the Jensen-Shannon measure correlate. However, they did not investigate the effect of the measure in past summarization tasks such as generic multi-document summarization (DUC 2004 Task 2), biographical summarization (DUC 2004 Task 5), opinion summarization (TAC 2008 OS), and summarization in languages other than English. We think that, in order to ha"
C10-2122,P02-1040,0,0.107698,"ist.gov/tac 1059 Coling 2010: Poster Volume, pages 1059–1067, Beijing, August 2010 process, a body of research has been produced in the last decade on automatic content-based evaluation procedures. Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries (Donaway et al., 2000), various vocabulary overlap measures such as set of n-grams overlap or longest common subsequence between peer and model have also been proposed (Saggion et al., 2002; Radev et al., 2003). The Bleu machine translation evaluation measure (Papineni et al., 2002) has also been tested in summarization (Pastra and Saggion, 2003). The DUC conferences adopted the ROUGE package for content-based evaluation (Lin, 2004). It implements a series of recall measures based on ngram co-occurrence statistics between a peer summary and a set of model summaries. ROUGE measures can be used to produce systems ranks. It has been shown that system rankings produced by some ROUGE measures (e.g., ROUGE-2 which uses bi-grams) correlate with rankings produced using coverage. In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. It is b"
C10-2122,W03-2805,1,0.782467,"Beijing, August 2010 process, a body of research has been produced in the last decade on automatic content-based evaluation procedures. Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries (Donaway et al., 2000), various vocabulary overlap measures such as set of n-grams overlap or longest common subsequence between peer and model have also been proposed (Saggion et al., 2002; Radev et al., 2003). The Bleu machine translation evaluation measure (Papineni et al., 2002) has also been tested in summarization (Pastra and Saggion, 2003). The DUC conferences adopted the ROUGE package for content-based evaluation (Lin, 2004). It implements a series of recall measures based on ngram co-occurrence statistics between a peer summary and a set of model summaries. ROUGE measures can be used to produce systems ranks. It has been shown that system rankings produced by some ROUGE measures (e.g., ROUGE-2 which uses bi-grams) correlate with rankings produced using coverage. In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. It is based on the distribution of “content” in a set of model summaries"
C10-2122,P03-1048,1,0.882792,"Missing"
C10-2122,C02-1073,1,0.805029,"Missing"
C12-1023,W10-1607,0,0.299656,"Missing"
C12-1023,P11-2087,0,0.407324,"Missing"
C12-1023,W11-1603,1,0.901786,"Missing"
C12-1023,bott-etal-2012-text,1,0.813411,"Missing"
C12-1023,N07-4002,0,0.0454736,"Missing"
C12-1023,C96-2183,0,0.4367,"Missing"
C12-1023,W11-1601,0,0.0697297,"Missing"
C12-1023,P11-2117,0,0.0501199,"Missing"
C12-1023,W12-2202,1,0.791982,"Missing"
C12-1023,padro-etal-2010-freeling,0,0.0234477,"Missing"
C12-1023,S12-1046,0,0.0839227,"Missing"
C12-1023,D10-1050,0,0.0193614,"Missing"
C12-1023,D11-1038,0,0.0982972,"Missing"
C12-1023,P12-1107,0,0.0461992,"Missing"
C12-1023,N10-1056,0,0.29868,"Missing"
C12-1023,C10-1152,0,0.107249,"Missing"
C16-1323,J75-4040,0,0.7351,"Missing"
C16-1323,S15-2151,0,0.0388232,"database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g."
C16-1323,P10-2020,0,0.0292908,"ore valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422 Proceedings of COLING 2016, the 26th International Conference on Compu"
C16-1323,D14-1110,0,0.0460532,"m a 3B-word corpus extracted from the web (Han et al., 2013),5 arguably richer in collocations than the encyclopedic style of Wikipedia. Similarly to S ENS E MBED, this model is based on a pre-disambiguation of text corpora using BabelNet as sense inventory. However, unlike S ENS E MBED, which learns vector representations for individual word senses, for this model we are interested in obtaining fine-grained information in the form of both plain text words and synsets6 in a shared vector space (see Section 3.2 for the motivation behind this choice, and its application). To this end, we follow Chen et al. (2014) and modify the objective function of Word2Vec,7 so that words and synsets can be learned jointly in a single training. The output is a vector space of word and synset embeddings that we use as collocates model. 3 Methodology In this section, we provide a detailed description of the algorithm behind the construction of CWN. The system takes as input the WordNet lexical database and a set of collocation lists pertaining to predefined semantic categories, and outputs CWN. First, we collect training data and perform automatic disambiguation (Section 3.1). Then, we use this disambiguated data for"
C16-1323,P89-1010,0,0.824988,"erally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422"
C16-1323,D15-1084,1,0.857019,"the idea of ‘intense’ and ‘perform’ than ‘begin to perform’), the number of instances per category in our training data also varies significantly (see Table 1). Our training dataset consists at this stage of pairs of plain words, with the inherent ambiguity this gives raise to. We surmount this challenge by applying a disambiguation strategy based on the notion that, from all the available senses for a collocation’s base and collocate, their correct senses are those which are most similar. This is a strategy that has been proved effective in previous concept-level disambiguation tasks (Delli Bovi et al., 2015). Formally, let us denote the S ENS E MBED vector space as S, and our original text-based training data as T. For each training collocation hb, ci ∈ T we consider all the available lexicalizations (i.e., senses) for both the base b and the collocate c in S, namely Lb = {lb1 ...lbn }, and Lc = {lc1 ...lcm }, and their corresponding set of sense embeddings Vb = {~v 1b , ..., ~v nb } and Vc = 4 We downloaded the pre-trained sense embeddings at http://lcl.uniroma1.it/sensembed/. ebiquity.umbc.edu/blogger/2013/05/01/umbc-webbase-corpus-of-3b-english-words/ 6 As explained above, a synset is a set co"
C16-1323,D16-1041,1,0.848512,".g., modeling analogies or projecting similar words nearby in the vector space), the most pertinent to this work is the linear relation that holds between semantically similar words in two analogous spaces (Mikolov et al., 2013b). Mikolov et al.’s original work learned a linear projection between two monolingual embeddings models to train a word-level machine translation system between English and Spanish. Other examples include the exploitation of this property for language normalization, i.e. finding regular English counterparts of Twitter language (Tan et al., 2015), or hypernym discovery (Espinosa-Anke et al., 2016). In our specific case, we learn a linear transformation from ~v 0b to ~v 0c , aiming at reflecting an inherent condition of collocations. Since collocations are a linguistic phenomenon that is more frequent in the narrative discourse than in formal essays, they are less likely to appear in an encyclopedic corpus (recall that S ENS E MBED vectors, which we use, are trained on a dump of the English Wikipedia). This motivates the use of S as our base space, and our S HARED E MBED X as the collocate model, as it was trained over more varied language such as blog posts or news items. Then, we cons"
C16-1323,esuli-sebastiani-2006-sentiwordnet,0,0.0248628,"var, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information ha"
C16-1323,P08-1017,0,0.0329205,") applications (Jurgens and Pilehvar, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and"
C16-1323,N15-1184,0,0.145399,"et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations ha"
C16-1323,P14-1113,0,0.0269,"eves the highest MRR score, which we claim to be the most relevant measure, as it rewards cases where the first ranked returned collocation is correct without measuring in the retrieved collocates at other positions. Moreover, let us highlight the importance of two main factors. First, the need for a well-defined semantic relation between bases and collocates. It has been shown in other tasks that exploit linear transformations between embeddings models that even for one single relation there may be clusters that require certain specificity in the domain or semantic of the data (see Fu et al. Fu et al. (2014) for a discussion of this phenomenon in the task of taxonomy learning). Second, the importance of having a reasonable amount of training pairs so that the model can learn the idiosyncrasies of the semantic relation that is being encoded (e.g., Mikolov et al. (2013b) report a major increase in performance as training data increases in several orders of magnitude). This is reinforced in our experiments, where we obtain the highest MAP score for ‘intense’, the semantic category for which we have the largest training data available. 13 See Bian et al. (2008) for an in-depth analysis of these metri"
C16-1323,Y13-2006,0,0.0206674,"resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, however, collocations have been the focus of a substantial amount of work, e.g. for automatically retrieving them from corpora (Choueka, 1988; Church and Hanks, 1989; Smadja, 1993; Kilgariff, 2006; Evert, 2007; Pecina, 2008; Bouma, 2010; Gao, 2013), and for their semantic classification according to different typologies (Wanner et al., 2006; Gelbukh and Kolesnikova., 2012; Moreno et al., 2013; Wanner et al., 2016). However, to the best of our knowledge, no previous work attempted the automatic enrichment of WordNet with collocational information. The only related attempt consisted in designing a schema for This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 3422 Proceedings of COLING 2016, the 26th International Conference on Computational Lin"
C16-1323,S13-1005,0,0.0203441,"al word senses based on Word2Vec (Mikolov et al., 2013a). Unlike other sense-based embeddings approaches, such as, e.g., Huang et al. (2012), which address the inherent polysemy of word-level representations relying solely on text corpora, S ENS E MBED exploits the structured knowledge of BabelNet along with distributional information gathered from the Wikipedia corpus. In this paper, we used S ENS E MBED for automatically disambiguating our training data, and as our bases model. S HARED E MBED. For this model we exploit distributional information from a 3B-word corpus extracted from the web (Han et al., 2013),5 arguably richer in collocations than the encyclopedic style of Wikipedia. Similarly to S ENS E MBED, this model is based on a pre-disambiguation of text corpora using BabelNet as sense inventory. However, unlike S ENS E MBED, which learns vector representations for individual word senses, for this model we are interested in obtaining fine-grained information in the form of both plain text words and synsets6 in a shared vector space (see Section 3.2 for the motivation behind this choice, and its application). To this end, we follow Chen et al. (2014) and modify the objective function of Word"
C16-1323,P12-1092,0,0.364618,"ry, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idiosyncratic binary lexical co-occurrences. As a standalone research topic, h"
C16-1323,P15-1010,0,0.0362667,"In its 3.6 release version, BabelNet is composed of 6.1M concepts and 7.7M named entities. 3 For example, the concept defined as principal activity in your life that you do to earn money is represented by the synset {occupation, business, job, line of work, line}, where occupation, business, job, line of work, and line are senses/lexicalizations of the given synset. 2 3423 its corresponding synsetnwn , provided there exists one. In what follows, we briefly describe two different vector space models that are used in this paper for the task of synset-level collocation discovery. S ENS E MBED4 (Iacobacci et al., 2015) is a knowledge-based approach for obtaining latent continuous representations of individual word senses based on Word2Vec (Mikolov et al., 2013a). Unlike other sense-based embeddings approaches, such as, e.g., Huang et al. (2012), which address the inherent polysemy of word-level representations relying solely on text corpora, S ENS E MBED exploits the structured knowledge of BabelNet along with distributional information gathered from the Wikipedia corpus. In this paper, we used S ENS E MBED for automatically disambiguating our training data, and as our bases model. S HARED E MBED. For this"
C16-1323,N15-1169,0,0.0589463,"grained collocational information, automatically introduced thanks to a method exploiting linear relations between analogous sense-level embeddings spaces. We perform both intrinsic and extrinsic evaluations, and release CWN for the use and scrutiny of the community. 1 Introduction The embedding of cues about how we perceive concepts and how these concepts relate and generalize across different domains gives knowledge resources the capacity of generalization, which lies at the core of human cognition (Yu et al., 2015) and is also central to many Natural Language Processing (NLP) applications (Jurgens and Pilehvar, 2015). It is general practice to identify and formalize conceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and"
C16-1323,N13-1090,0,0.716301,"rence on Computational Linguistics: Technical Papers, pages 3422–3432, Osaka, Japan, December 11-17 2016. the manual inclusion of lexical functions from Explanatory Combinatorial Lexicology (ECL) (Mel’ˇcuk, 1996) into the Spanish EuroWordNet (Wanner et al., 2004). Given the importance of collocations for a series of NLP applications (e.g. machine translation, text generation or paraphrasing), we propose to fill this gap by putting forward a new methodology which exploits intrinsic properties of state-of-the-art semantic vector space models and leverages the transformation matrix introduced by Mikolov et al. (2013b) in a word-level machine translation task. As a result, we release an extension of WordNet with detailed collocational information, named ColWordNet (CWN). This extension is carried out by means of the inclusion of novel edges, where each edge encodes a collocates-with relation, as well as the semantics of the collocation itself. For example, given the pair col:intense of synsets desire.n.01 and ardent.a.01, a novel relation −−−−−−−→ is introduced, where ‘inx tense’ is the semantic category denoting intensification, and x is the confidence score assigned by our algorithm. The remainder of th"
C16-1323,N16-1018,0,0.0583307,"Missing"
C16-1323,P16-2074,0,0.0357694,"ocation clusters by extracting all the synsets associated lemmas (e.g. for heavy.a.01 rain.n.01, we would extract the cluster [heavy, rain, rainfall]). These are used as input for the Retrofitting Word Vectors algorithm (Faruqui et al., 2015).15 This algorithm takes as input a vector space and a semantic lexicon which may encode any semantic relation, and puts closer in the vector space words that are related in the lexicon. Previous approaches have encoded semantic relations by introducing some kind of bias into a vector space model (Yu et al., 2015; Pham et al., 2015; Mrkˇsi´c et al., 2016; Nguyen et al., 2016). For instance, Yu et al. (2015) encode (term, hypernym) relations by grouping together terms and their hypernyms, rather than semantically related items. In this way, their biased model puts closer to jaguar terms like animal or vehicle, while an unbiased model would put nearby terms such as lion, bmw or jungle. We aim at introducing a similar bias, but in terms of collocational information. This is achieved, for each lexical function and each synset in CWN-st, by obtaining its top 3 collocate candidates and incorporate information on their collocationality into the model. 4.2.1 Collocational"
C16-1323,P15-2004,0,0.0213646,"ings model.14 To this end, we extract collocation clusters by extracting all the synsets associated lemmas (e.g. for heavy.a.01 rain.n.01, we would extract the cluster [heavy, rain, rainfall]). These are used as input for the Retrofitting Word Vectors algorithm (Faruqui et al., 2015).15 This algorithm takes as input a vector space and a semantic lexicon which may encode any semantic relation, and puts closer in the vector space words that are related in the lexicon. Previous approaches have encoded semantic relations by introducing some kind of bias into a vector space model (Yu et al., 2015; Pham et al., 2015; Mrkˇsi´c et al., 2016; Nguyen et al., 2016). For instance, Yu et al. (2015) encode (term, hypernym) relations by grouping together terms and their hypernyms, rather than semantically related items. In this way, their biased model puts closer to jaguar terms like animal or vehicle, while an unbiased model would put nearby terms such as lion, bmw or jungle. We aim at introducing a similar bias, but in terms of collocational information. This is achieved, for each lexical function and each synset in CWN-st, by obtaining its top 3 collocate candidates and incorporate information on their colloca"
C16-1323,P13-1132,0,0.0216348,"onceptual relations using a reference knowledge repository. As such a repository, WordNet (Miller et al., 1990) stands out as the de facto standard lexical database, containing over 200k English senses with 155k word forms. Over the years, WordNet has become the cornerstone of agglutinative resources such as BabelNet (Navigli and Ponzetto, 2012) and Yago (Suchanek et al., 2007). It is also used in semantically intensive tasks such as Word Sense Disambiguation (Navigli, 2009), Query Expansion and IR (Fang, 2008), Sentiment Analysis (Esuli and Sebastiani, 2006), semantic similarity measurement (Pilehvar et al., 2013), development and evaluation of word embeddings models (Huang et al., 2012; Faruqui et al., 2015), and Taxonomy Learning Evaluation (Bordea et al., 2015). While the value of WordNet for NLP is indisputable, it is generally recognized that enriching it with additional information makes it an even more valuable resource. Thus, there is a line of research aimed at extending it with novel terminology (Jurgens and Pilehvar, 2016), cross-predicate relations (Lopez de la Calle et al., 2016), and so forth. Nonetheless, one type of information has been largely neglected so far: collocations, i.e., idio"
C16-1323,L16-1367,1,0.874671,"Missing"
C16-1323,P15-2108,0,0.102421,"been explored so far in the literature (e.g., modeling analogies or projecting similar words nearby in the vector space), the most pertinent to this work is the linear relation that holds between semantically similar words in two analogous spaces (Mikolov et al., 2013b). Mikolov et al.’s original work learned a linear projection between two monolingual embeddings models to train a word-level machine translation system between English and Spanish. Other examples include the exploitation of this property for language normalization, i.e. finding regular English counterparts of Twitter language (Tan et al., 2015), or hypernym discovery (Espinosa-Anke et al., 2016). In our specific case, we learn a linear transformation from ~v 0b to ~v 0c , aiming at reflecting an inherent condition of collocations. Since collocations are a linguistic phenomenon that is more frequent in the narrative discourse than in formal essays, they are less likely to appear in an encyclopedic corpus (recall that S ENS E MBED vectors, which we use, are trained on a dump of the English Wikipedia). This motivates the use of S as our base space, and our S HARED E MBED X as the collocate model, as it was trained over more varied lang"
C16-1323,wanner-etal-2004-enriching,1,0.827897,"Missing"
C16-1323,S16-1169,0,\N,Missing
C16-3003,P11-1051,0,0.0261205,"13; Guo et al., 2011; Ronzano and Saggion, 2016b) constitute a key factor for the development of rich scientific knowledge bases which can be leveraged to support structured and semantically-enabled searches, intelligent question answering and personalized content recommendation (He et al., 2010; Huang et al., 2012). Summarization techniques can help to identify the essential contents of publications thus generating automatic state of the art reviews while paraphrase or textual entailment can contribute to identify relations across different scientific textual sources (Teufel and Moens, 2002; Abu-Jbara and Radev, 2011; Ronzano and Saggion, 2016a; Saggion and Lapalme, 2002; Saggion, 2008). The objective of this tutorial is to provide a comprehensive overview of the most relevant problems we have to face when we mine scientific literature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent"
C16-3003,N13-1067,0,0.0145043,"ns of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent information (Ramakrishnan et al., 2012; Luong et al., 2012; Councill et al., 2008), discursive structure (Liakata et al., 2010; Teufel et al., 2009; Fisas et al., 2015) and networks of citations (Teufel et al., 2006; Athar, 2011; Abu-Jbara et al., 2013). We discuss relevant scenarios where the availability of structured, semantically-annotated publications improves the way we benefit from scientific literature, including article summarization, scientific content search, selection and aggregation, and publication impact assessment. Related tools, applications, datasets and publication venues are also reviewed. 2 Outline The half-day tutorial will review the following nine of top-level topics. For each topic, some of the core themes to discuss is specified. 1. Scientific Information Overload: Challenges and Opportunities • Overwhelmed by scien"
C16-3003,P11-3015,0,0.028633,"rature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent information (Ramakrishnan et al., 2012; Luong et al., 2012; Councill et al., 2008), discursive structure (Liakata et al., 2010; Teufel et al., 2009; Fisas et al., 2015) and networks of citations (Teufel et al., 2006; Athar, 2011; Abu-Jbara et al., 2013). We discuss relevant scenarios where the availability of structured, semantically-annotated publications improves the way we benefit from scientific literature, including article summarization, scientific content search, selection and aggregation, and publication impact assessment. Related tools, applications, datasets and publication venues are also reviewed. 2 Outline The half-day tutorial will review the following nine of top-level topics. For each topic, some of the core themes to discuss is specified. 1. Scientific Information Overload: Challenges and Opportuniti"
C16-3003,councill-etal-2008-parscit,0,0.0157751,"gion, 2008). The objective of this tutorial is to provide a comprehensive overview of the most relevant problems we have to face when we mine scientific literature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent information (Ramakrishnan et al., 2012; Luong et al., 2012; Councill et al., 2008), discursive structure (Liakata et al., 2010; Teufel et al., 2009; Fisas et al., 2015) and networks of citations (Teufel et al., 2006; Athar, 2011; Abu-Jbara et al., 2013). We discuss relevant scenarios where the availability of structured, semantically-annotated publications improves the way we benefit from scientific literature, including article summarization, scientific content search, selection and aggregation, and publication impact assessment. Related tools, applications, datasets and publication venues are also reviewed. 2 Outline The half-day tutorial will review the following nine of"
C16-3003,W15-1605,1,0.843677,"most relevant problems we have to face when we mine scientific literature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent information (Ramakrishnan et al., 2012; Luong et al., 2012; Councill et al., 2008), discursive structure (Liakata et al., 2010; Teufel et al., 2009; Fisas et al., 2015) and networks of citations (Teufel et al., 2006; Athar, 2011; Abu-Jbara et al., 2013). We discuss relevant scenarios where the availability of structured, semantically-annotated publications improves the way we benefit from scientific literature, including article summarization, scientific content search, selection and aggregation, and publication impact assessment. Related tools, applications, datasets and publication venues are also reviewed. 2 Outline The half-day tutorial will review the following nine of top-level topics. For each topic, some of the core themes to discuss is specified. 1."
C16-3003,D11-1025,0,0.0165096,"bution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 9 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Tutorial Abstracts, pages 9–13, Osaka, Japan, December 11-17 2016. In this context, the Natural Language Processing community plays a central role in investigating and improving new approaches to the analysis of scientific information, thus uncovering incredible opportunities for contributions and experimentations. The extraction and integration of information from scientific papers (Lipinski et al., 2013; Guo et al., 2011; Ronzano and Saggion, 2016b) constitute a key factor for the development of rich scientific knowledge bases which can be leveraged to support structured and semantically-enabled searches, intelligent question answering and personalized content recommendation (He et al., 2010; Huang et al., 2012). Summarization techniques can help to identify the essential contents of publications thus generating automatic state of the art reviews while paraphrase or textual entailment can contribute to identify relations across different scientific textual sources (Teufel and Moens, 2002; Abu-Jbara and Radev,"
C16-3003,liakata-etal-2010-corpora,0,0.0258749,"to provide a comprehensive overview of the most relevant problems we have to face when we mine scientific literature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent information (Ramakrishnan et al., 2012; Luong et al., 2012; Councill et al., 2008), discursive structure (Liakata et al., 2010; Teufel et al., 2009; Fisas et al., 2015) and networks of citations (Teufel et al., 2006; Athar, 2011; Abu-Jbara et al., 2013). We discuss relevant scenarios where the availability of structured, semantically-annotated publications improves the way we benefit from scientific literature, including article summarization, scientific content search, selection and aggregation, and publication impact assessment. Related tools, applications, datasets and publication venues are also reviewed. 2 Outline The half-day tutorial will review the following nine of top-level topics. For each topic, some of t"
C16-3003,J02-4005,1,0.644038,"itute a key factor for the development of rich scientific knowledge bases which can be leveraged to support structured and semantically-enabled searches, intelligent question answering and personalized content recommendation (He et al., 2010; Huang et al., 2012). Summarization techniques can help to identify the essential contents of publications thus generating automatic state of the art reviews while paraphrase or textual entailment can contribute to identify relations across different scientific textual sources (Teufel and Moens, 2002; Abu-Jbara and Radev, 2011; Ronzano and Saggion, 2016a; Saggion and Lapalme, 2002; Saggion, 2008). The objective of this tutorial is to provide a comprehensive overview of the most relevant problems we have to face when we mine scientific literature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent information (Ramakrishnan et al., 2012; Luong et al., 2"
C16-3003,J02-4002,0,0.140885,"ers (Lipinski et al., 2013; Guo et al., 2011; Ronzano and Saggion, 2016b) constitute a key factor for the development of rich scientific knowledge bases which can be leveraged to support structured and semantically-enabled searches, intelligent question answering and personalized content recommendation (He et al., 2010; Huang et al., 2012). Summarization techniques can help to identify the essential contents of publications thus generating automatic state of the art reviews while paraphrase or textual entailment can contribute to identify relations across different scientific textual sources (Teufel and Moens, 2002; Abu-Jbara and Radev, 2011; Ronzano and Saggion, 2016a; Saggion and Lapalme, 2002; Saggion, 2008). The objective of this tutorial is to provide a comprehensive overview of the most relevant problems we have to face when we mine scientific literature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lope"
C16-3003,W06-1613,0,0.047852,"mine scientific literature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent information (Ramakrishnan et al., 2012; Luong et al., 2012; Councill et al., 2008), discursive structure (Liakata et al., 2010; Teufel et al., 2009; Fisas et al., 2015) and networks of citations (Teufel et al., 2006; Athar, 2011; Abu-Jbara et al., 2013). We discuss relevant scenarios where the availability of structured, semantically-annotated publications improves the way we benefit from scientific literature, including article summarization, scientific content search, selection and aggregation, and publication impact assessment. Related tools, applications, datasets and publication venues are also reviewed. 2 Outline The half-day tutorial will review the following nine of top-level topics. For each topic, some of the core themes to discuss is specified. 1. Scientific Information Overload: Challenges an"
C16-3003,D09-1155,0,0.015863,"nsive overview of the most relevant problems we have to face when we mine scientific literature by means of Natural Language Processing Technologies, thus identifying challenges, solutions, and opportunities for our community. In particular, we consider approaches and tools useful to analyze and characterize a wide range of structural and semantic peculiarities of scientific articles, including document formats (Constantin et al., 2013; Lopez, 2009), layout-dependent information (Ramakrishnan et al., 2012; Luong et al., 2012; Councill et al., 2008), discursive structure (Liakata et al., 2010; Teufel et al., 2009; Fisas et al., 2015) and networks of citations (Teufel et al., 2006; Athar, 2011; Abu-Jbara et al., 2013). We discuss relevant scenarios where the availability of structured, semantically-annotated publications improves the way we benefit from scientific literature, including article summarization, scientific content search, selection and aggregation, and publication impact assessment. Related tools, applications, datasets and publication venues are also reviewed. 2 Outline The half-day tutorial will review the following nine of top-level topics. For each topic, some of the core themes to dis"
C18-3005,W10-1001,0,0.0156229,"placing it with synonyms which are simpler to read or understand, while syntactic simplification is concerned with transforming sentences containing syntactic phenomena which may hinder readability and comprehension (e.g. complex subordination phenomena, passive voice constructions) into simpler equivalents. Several ATS projects were conducted aimed at producing simplification systems for different audiences and languages. The PSET (Practical Simplification of English Texts) project was a UK initiative to produce adapted texts for aphasic people (Carroll et al., 1998). The PorSimples project (Aluisio et al., 2010) developed an automatic system and editing assistance tool to simplify texts for people with low literacy levels in Brazil. The Simplext project (Saggion et al., 2015) developed simplification technology for Spanish speakers with intellectual disabilities. The FIRST project (Mart´ın-Valdivia et al., 2014) developed a semi-automatic text adaptation tool for English, Spanish and Bulgarian to improve accessibility of written texts to people with autism, while the Able to Include project (Saggion et al., 2017; Ferr´es et al., 2016) targeted people with intellectual disabilities on the Web. All tho"
C18-3005,C96-2183,0,0.736586,"he vocabulary, syntax, and discourse levels of the text. Over the last years research in automatic text simplification has intensified not only in the number of human languages being addressed but also in the number of techniques being proposed to deal with it from initial rule-based approaches to current data-driven techniques. The aim of this tutorial is to provide a comprehensive overview of past and current research on automatic text simplification. 1 Introduction Automatic text simplification (ATS) appeared as an area of research in natural language processing (NLP) in the late nineties (Chandrasekar et al., 1996). Its goal is to automatically transform given input (text or sentences) into a simpler variant without significantly changing the input original meaning (Saggion, 2017). What is considered a simpler variant clearly depends on who/what is the target readership/application. Initially, ATS was proposed as a pre-processing step to improve various NLP tasks, e.g. machine translation, information extraction, summarisation, and semantic role labeling. In such scenario, a simpler variant is the one that improves the performance of the targeted NLP task, when used instead of the original input text. L"
C18-3005,W11-1601,0,0.0997524,"al linguistics, and natural language processing. We will present the techniques used to transform written texts in each of those projects and make an in-depth discussion of what those projects had in common in terms of techniques and resources, and in what they differed. 19 1.1 Data-driven Paradigm in Simplification With the emergence of Simple English Wikipedia and its (comparable) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the prev"
C18-3005,N15-1022,0,0.0529265,"Missing"
C18-3005,P13-1151,0,0.0233496,"ing. We will present the techniques used to transform written texts in each of those projects and make an in-depth discussion of what those projects had in common in terms of techniques and resources, and in what they differed. 19 1.1 Data-driven Paradigm in Simplification With the emergence of Simple English Wikipedia and its (comparable) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approachin"
C18-3005,P17-2014,1,0.893151,"Missing"
C18-3005,I17-3001,0,0.0238948,", given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. Sequence-to-sequence neural models (Nisioi et al., ˇ 2017; Stajner and Nisioi, 2018), and the neural model based on reinforcement learning techniques (Zhang and Lapata, 2017) showed a dominance of neural ATS approaches over the previous data-driven approaches in terms of quality of generated output (better grammaticality and meaning preservation). The q"
C18-3005,P16-2024,0,0.0388999,"l of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. Sequence-to-sequence neural models (Nisioi et al., ˇ 2017; Stajner and Nisioi, 2018), and the neural model based on reinforcement lea"
C18-3005,E14-1076,0,0.070711,"d to transform written texts in each of those projects and make an in-depth discussion of what those projects had in common in terms of techniques and resources, and in what they differed. 19 1.1 Data-driven Paradigm in Simplification With the emergence of Simple English Wikipedia and its (comparable) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task ("
C18-3005,L18-1479,1,0.769042,"Missing"
C18-3005,P17-2016,1,0.885434,"Missing"
C18-3005,L18-1615,1,0.815163,"implifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. Sequence-to-sequence neural models (Nisioi et al., ˇ 2017; Stajner and Nisioi, 2018), and the neural model based on reinforcement learning techniques (Zhang and Lapata, 2017) showed a dominance of neural ATS approaches over the previous data-driven approaches in terms of quality of generated output (better grammaticality and meaning preservation). The question of simplicity of the generated output a"
C18-3005,Q15-1021,0,0.0193383,"ble) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignme"
C18-3005,Q16-1029,0,0.0319258,"component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed idea of approaching ATS as a monolingual machine translation (MT) task (Specia, 2010; Coster and Kauchak, 2011), Xu et al. (2016) proposed an MT-based ATS system for English built upon Newsela and the large paraphrase database (Pavlick and Callison-Burch, 2016). The manual sentence alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. S"
C18-3005,D17-1062,0,0.0305122,"nce alignment of English Newsela (Xu et al., 2015), improved automatic alignment of EW-SEW corpus (Hwang et al., ˇ 2015), and the recently released free tools for sentence alignment (Paetzold et al., 2017; Stajner et al., ˇ 2017; Stajner et al., 2018), offered new opportunities for data-driven ATS. In 2017, several ATS systems exploring various deep learning architectures appeared, using the new alignments of Wikipedia and Newsela for training. Sequence-to-sequence neural models (Nisioi et al., ˇ 2017; Stajner and Nisioi, 2018), and the neural model based on reinforcement learning techniques (Zhang and Lapata, 2017) showed a dominance of neural ATS approaches over the previous data-driven approaches in terms of quality of generated output (better grammaticality and meaning preservation). The question of simplicity of the generated output and the adaptability of those models to different text genres and languages other than English, is still present. While solving the problems of grammaticality and meaning preservation, the neural TS systems introduced a new challenge, showing problems in dealing with abundance of name entities present both in news articles and Wikipedia articles. 2 Tutorial Overview In t"
C18-3005,C10-1152,0,0.0500442,"l language processing. We will present the techniques used to transform written texts in each of those projects and make an in-depth discussion of what those projects had in common in terms of techniques and resources, and in what they differed. 19 1.1 Data-driven Paradigm in Simplification With the emergence of Simple English Wikipedia and its (comparable) alignment with English Wikipedia, which for the first time offered a large parallel dataset for training of the ATS systems, the approaches to ATS shifted from rule-based (Siddharthan, 2006) to purely data-driven (Coster and Kauchak, 2011; Zhu et al., 2010; Kauchak, 2013), and later hybrid ones (Siddharthan and Mandya, 2014). It created opportunity for stronger NLP component of the systems and new challenges in text/sentence generation, but at the cost of blurring the final goal of those ATS systems, as there was no clear target population in mind anymore. The release of Newsela dataset (Xu et al., 2015) for English and Spanish in 2015, created opportunities for better modelling of simplification operations, given its well-controlled quality of manual simplifications at five different text complexity levels. Following the previously proposed id"
D16-1041,P14-1098,0,0.0292126,"Missing"
D16-1041,W11-2501,0,0.0222365,"information is also used for supervised definition and hypernym extraction (Navigli and Velardi, 2010; Boella and Di Caro, 2013), or together with Wikipedia-specific heuristics (Flati et al., 2014). One of the main drawbacks of these methods is that they require both term and hypernym to co-occur in text within a certain window, which strongly hinders their recall. Higher recall can be achieved thanks to distributional methods, as they do not have co-occurrence requirements. In addition, they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy (Baroni and Lenci, 2011), but also cause-effect or entity-origin (Hendrickx et al., 2009). However, they are often more imprecise and seem to perform best in discovering broader semantic relations (Shwartz et al., 2016). One way to surmount the issue of generality was proposed by Fu et al. (2014), who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space. As shown empirically in Fu et al.’s original work, the hypernymic relation that holds for the pair (dragonfly, insect) differs from the one of e.g. (carpenter, man). Prior to training, their system addresses this discrepan"
D16-1041,E12-1004,0,0.0546851,"d Di Caro, 2013; Espinosa-Anke et al., 2016), clustering (Yang and Callan, 2009) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013). Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings (Mikolov et al., 2013d). In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relations. These pairs may be represented either as a concatenation of both vectors (Baroni et al., 2012), difference (Roller et al., 2014), dot-product (Mikolov et al., 2013c), or including additional linguistic information for LSTMbased learning (Shwartz et al., 2016). In this paper we propose TAXO E MBED2 , a hypernym detection algorithm based on sense embeddings, which can be easily applied to the construction of lexical taxonomies. It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces (Mikolov et al., 2013b) and, unlike previous approaches, leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain o"
D16-1041,S15-2151,0,0.0812627,"dings space. As shown empirically in Fu et al.’s original work, the hypernymic relation that holds for the pair (dragonfly, insect) differs from the one of e.g. (carpenter, man). Prior to training, their system addresses this discrepancy via k-means clustering using a held-out development set for tuning. The previously described methods for hypernym and taxonomy learning operate inherently at the surface level. This is partly due to the way evaluation is conducted, which is often limited to very specific domains with no integrative potential (e.g. taxonomies in food, science or equipment from Bordea et al. (2015)), or restricted to lists of word pairs. Hence, a drawback of surface-level taxonomy learning, apart from ambiguity issues, is that they require additional and error-prone steps to identify semantic clusters (Fu et al., 2014). Alternatively, recent advances in OIE based on disambiguation and deeper semantic analysis (Nakashole et al., 2012; Grycner and Weikum, 2014; Delli Bovi et al., 2015b) have shown their potential to construct taxonomized disambiguated resources both at node and at relation level. However, in addition to their inherently broader scope, OIE approaches are designed to achiev"
D16-1041,P15-1072,1,0.827607,"a predefined knowledge domain, under the assumption that vectors clustered with this criterion are likely to exhibit similar semantic properties (e.g. similarity). First, we allocate each synset into its most representative domain, which is achieved by exploiting the set of thirty four domains available in the Wikipedia featured articles page9 . Warfare, transport, or music are some of these domains. In the Wikipedia featured articles page each domain is composed of 128 Wikipedia pages on average. Then, in order to expand the set of concepts associated with each domain, we leverage NASARI10 (Camacho-Collados et al., 2015), a distributional approach that has been used to construct explicit vector representations of BabelNet synsets. Our goal is to associate BabelNet synsets with domains. To this end, we follow Camacho-Collados et al. (2016) and build a lexical vector for each Wikipedia domain by concatenating all Wikipedia pages representing the given domain into a single text. Finally, given a BabelNet synset b, we calculate the similarity between its corresponding NASARI lexical vector and all the domain vectors, selecting the domain leading to the highest similarity score: ˆ = max W O(d, ~ ~b) d(b) d∈D (1) w"
D16-1041,D15-1084,1,0.712095,"f lexical taxonomies. It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces (Mikolov et al., 2013b) and, unlike previous approaches, leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge. Our best configuration (ranking first in two thirds of the experiments conducted) considers two training sources: (1) Manually curated pairs from Wikidata (Vrandeˇci´c and Krötzsch, 2014); and (2) Hypernymy relations from a KB which integrates several Open Information Extraction (OIE) systems (Delli Bovi et al., 2015a). Since our method uses a very large semantic network as reference sense inventory, we are able to perform jointly hypernym extraction and disambiguation, from which 1 The terminology is not entirely unified in this respect. In addition to pattern-based (Fountain and Lapata, 2012; Bansal et al., 2014; Yu et al., 2015), other terms like path-based (Shwartz et al., 2016) or rule-based (Navigli and Velardi, 2010) are also used. 2 Data and source code available from the following link: www.taln.upf.edu/taxoembed. 425 expanding existing ontologies becomes a trivial task. Compared to word-level ta"
D16-1041,Q15-1038,1,0.689134,"f lexical taxonomies. It is designed to discover hypernymic relations by exploiting linear transformations in embedding spaces (Mikolov et al., 2013b) and, unlike previous approaches, leverages this intuition to learn a specific semanticallyaware transformation matrix for each domain of knowledge. Our best configuration (ranking first in two thirds of the experiments conducted) considers two training sources: (1) Manually curated pairs from Wikidata (Vrandeˇci´c and Krötzsch, 2014); and (2) Hypernymy relations from a KB which integrates several Open Information Extraction (OIE) systems (Delli Bovi et al., 2015a). Since our method uses a very large semantic network as reference sense inventory, we are able to perform jointly hypernym extraction and disambiguation, from which 1 The terminology is not entirely unified in this respect. In addition to pattern-based (Fountain and Lapata, 2012; Bansal et al., 2014; Yu et al., 2015), other terms like path-based (Shwartz et al., 2016) or rule-based (Navigli and Velardi, 2010) are also used. 2 Data and source code available from the following link: www.taln.upf.edu/taxoembed. 425 expanding existing ontologies becomes a trivial task. Compared to word-level ta"
D16-1041,P14-1089,0,0.076031,"minent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson et al., 2010) rely crucially on taxonomized concepts and their relations within their learning process. Taxonomy learning is roughly based on a twostep process, namely is-a (hypernymic) relation de424 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 424–435, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tection, and graph induction. The hypernym detection phase has gathered m"
D16-1041,N12-1051,0,0.0843009,"criterion (Hwang et al., 2012). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson et al., 2010) rely crucially on taxonomized concepts"
D16-1041,P14-1113,0,0.221824,"pernym to co-occur in text within a certain window, which strongly hinders their recall. Higher recall can be achieved thanks to distributional methods, as they do not have co-occurrence requirements. In addition, they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy (Baroni and Lenci, 2011), but also cause-effect or entity-origin (Hendrickx et al., 2009). However, they are often more imprecise and seem to perform best in discovering broader semantic relations (Shwartz et al., 2016). One way to surmount the issue of generality was proposed by Fu et al. (2014), who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space. As shown empirically in Fu et al.’s original work, the hypernymic relation that holds for the pair (dragonfly, insect) differs from the one of e.g. (carpenter, man). Prior to training, their system addresses this discrepancy via k-means clustering using a held-out development set for tuning. The previously described methods for hypernym and taxonomy learning operate inherently at the surface level. This is partly due to the way evaluation is conducted, which is often limited to very specifi"
D16-1041,C14-1207,0,0.0192907,"y learning operate inherently at the surface level. This is partly due to the way evaluation is conducted, which is often limited to very specific domains with no integrative potential (e.g. taxonomies in food, science or equipment from Bordea et al. (2015)), or restricted to lists of word pairs. Hence, a drawback of surface-level taxonomy learning, apart from ambiguity issues, is that they require additional and error-prone steps to identify semantic clusters (Fu et al., 2014). Alternatively, recent advances in OIE based on disambiguation and deeper semantic analysis (Nakashole et al., 2012; Grycner and Weikum, 2014; Delli Bovi et al., 2015b) have shown their potential to construct taxonomized disambiguated resources both at node and at relation level. However, in addition to their inherently broader scope, OIE approaches are designed to achieve high coverage, and hence they tend to produce noisier data compared to taxonomy learning systems. In our sense-based approach, instead, not only do we leverage an unambiguous vector representation for hypernym discovery, but we also take advantage of a domain-wise clustering strategy to directly obtain specific term-hypernym training pairs, thereby substantially"
D16-1041,C92-2082,0,0.616227,"lson et al., 2010) rely crucially on taxonomized concepts and their relations within their learning process. Taxonomy learning is roughly based on a twostep process, namely is-a (hypernymic) relation de424 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 424–435, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tection, and graph induction. The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics. It has been addressed by means of pattern-based methods1 (Hearst, 1992; Snow et al., 2004; Kozareva and Hovy, 2010; Carlson et al., 2010; Boella and Di Caro, 2013; Espinosa-Anke et al., 2016), clustering (Yang and Callan, 2009) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013). Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings (Mikolov et al., 2013d). In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relation"
D16-1041,W09-2415,0,0.00699351,"extraction (Navigli and Velardi, 2010; Boella and Di Caro, 2013), or together with Wikipedia-specific heuristics (Flati et al., 2014). One of the main drawbacks of these methods is that they require both term and hypernym to co-occur in text within a certain window, which strongly hinders their recall. Higher recall can be achieved thanks to distributional methods, as they do not have co-occurrence requirements. In addition, they can be tailored to cover any number of predefined semantic relations such as cohyponymy or meronymy (Baroni and Lenci, 2011), but also cause-effect or entity-origin (Hendrickx et al., 2009). However, they are often more imprecise and seem to perform best in discovering broader semantic relations (Shwartz et al., 2016). One way to surmount the issue of generality was proposed by Fu et al. (2014), who explored the possibility to learn a hypernymic transformation matrix over a word embeddings space. As shown empirically in Fu et al.’s original work, the hypernymic relation that holds for the pair (dragonfly, insect) differs from the one of e.g. (carpenter, man). Prior to training, their system addresses this discrepancy via k-means clustering using a held-out development set for tu"
D16-1041,P12-1092,0,0.0162966,"er. Initially, |W |= 5,301,867 and |K |= 1,358,949. 3.2 Sense vectors S ENS E MBED (Iacobacci et al., 2015)8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm. Vectors in the S ENS E MBED space, denoted as S, are latent continuous representations of word senses based on the Word2Vec architecture (Mikolov et al., 2013a), which was applied on a disambiguated Wikipedia corpus. Each vector ~v ∈ S represents a BabelNet sense, i.e. a synset along with one of its lexicalizations (e.g. album_chart_bn:00002488n). This differs from unsupervised approaches (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) that learn sense representations from text corpora only and are not mapped to any lexical resource, limiting their application in our task. 4 Methodology Our approach can be summarized as follows. First, we take advantage of a clustering algorithm for allocating each BabelNet synset of the training set into a domain cluster C (Section 4.1). Then, we expand the training set by exploiting the different lexicalizations available for each BabelNet synset (Section 4.2). Finally, we learn a cluster-wise linear projection (a hypernym transformation matri"
D16-1041,P15-1010,0,0.0659627,"ures its own manually-built taxonomic structure and relation type inventory (hence its own is-a relation type), we identified the relation synset containing N ELL’s is-a7 and then drew from the unified KB all the corresponding triples, which we denote as K. These triples constitute, similarly as in the previous case, a set of term-hypernym pairs automatically extracted from OIE-derived resources, with a disambiguation confidence of above 0.9 according to the disambiguation strategy described in the original paper. Initially, |W |= 5,301,867 and |K |= 1,358,949. 3.2 Sense vectors S ENS E MBED (Iacobacci et al., 2015)8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm. Vectors in the S ENS E MBED space, denoted as S, are latent continuous representations of word senses based on the Word2Vec architecture (Mikolov et al., 2013a), which was applied on a disambiguated Wikipedia corpus. Each vector ~v ∈ S represents a BabelNet sense, i.e. a synset along with one of its lexicalizations (e.g. album_chart_bn:00002488n). This differs from unsupervised approaches (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) that learn sense representations from text"
D16-1041,D10-1108,0,0.221822,"nomies henceforth) are graph-like hierarchical structures where terms are nodes, and are typically organized over a predefined merging or splitting criterion (Hwang et al., 2012). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy le"
D16-1041,N15-1098,0,0.0797191,"Missing"
D16-1041,D14-1088,0,0.0134584,"zed over a predefined merging or splitting criterion (Hwang et al., 2012). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson e"
D16-1041,D15-1117,0,0.010831,"generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson et al., 2010) rely crucially on taxonomized concepts and their relations within their learning process. Taxonomy learning is roughly based on a twos"
D16-1041,N13-1090,0,0.635083,"graph induction. The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics. It has been addressed by means of pattern-based methods1 (Hearst, 1992; Snow et al., 2004; Kozareva and Hovy, 2010; Carlson et al., 2010; Boella and Di Caro, 2013; Espinosa-Anke et al., 2016), clustering (Yang and Callan, 2009) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013). Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings (Mikolov et al., 2013d). In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relations. These pairs may be represented either as a concatenation of both vectors (Baroni et al., 2012), difference (Roller et al., 2014), dot-product (Mikolov et al., 2013c), or including additional linguistic information for LSTMbased learning (Shwartz et al., 2016). In this paper we propose TAXO E MBED2 , a hypernym detection algorithm based on sense embeddings, which can be easily applied to the construction of"
D16-1041,D12-1104,0,0.0559942,"Missing"
D16-1041,P10-1134,0,0.428627,"ures where terms are nodes, and are typically organized over a predefined merging or splitting criterion (Hwang et al., 2012). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Info"
D16-1041,D14-1113,0,0.0139555,"= 1,358,949. 3.2 Sense vectors S ENS E MBED (Iacobacci et al., 2015)8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm. Vectors in the S ENS E MBED space, denoted as S, are latent continuous representations of word senses based on the Word2Vec architecture (Mikolov et al., 2013a), which was applied on a disambiguated Wikipedia corpus. Each vector ~v ∈ S represents a BabelNet sense, i.e. a synset along with one of its lexicalizations (e.g. album_chart_bn:00002488n). This differs from unsupervised approaches (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) that learn sense representations from text corpora only and are not mapped to any lexical resource, limiting their application in our task. 4 Methodology Our approach can be summarized as follows. First, we take advantage of a clustering algorithm for allocating each BabelNet synset of the training set into a domain cluster C (Section 4.1). Then, we expand the training set by exploiting the different lexicalizations available for each BabelNet synset (Section 4.2). Finally, we learn a cluster-wise linear projection (a hypernym transformation matrix) over all pairs (term-hypernym) of the expan"
D16-1041,P13-1132,0,0.0162698,"ollow Camacho-Collados et al. (2016) and build a lexical vector for each Wikipedia domain by concatenating all Wikipedia pages representing the given domain into a single text. Finally, given a BabelNet synset b, we calculate the similarity between its corresponding NASARI lexical vector and all the domain vectors, selecting the domain leading to the highest similarity score: ˆ = max W O(d, ~ ~b) d(b) d∈D (1) where D is the set of all thirty-three domains, d~ is the vector of the domain d ∈ D, ~b is the vector of the BabelNet synset b, and WO refers to the Weighted Overlap comparison measure (Pilehvar et al., 2013), which is defined as follows: v uP −1 u w∈O rankw,v~1 + rankw,v~2 t W O(v~1 , v~2 ) = P|O| −1 i=1 (2i) (2) where rankw,v~i is the rank of the word w in the vector v~i according to its weight, and O is the set of overlapping words between the two vectors. In order to have a highly reliable set of domain labels, those 9 https://en.wikipedia.org/wiki/ Wikipedia:Featured_articles 10 http://lcl.uniroma1.it/nasari synsets whose maximum similarity score is below a certain threshold are not annotated with any domain. We fixed the threshold to 0.35, which provided a fine balance between precision (es"
D16-1041,C14-1097,0,0.121493,"Missing"
D16-1041,P16-1226,0,0.119326,"Missing"
D16-1041,P06-1101,0,0.0214229,"Missing"
D16-1041,P15-2108,0,0.0629219,"Missing"
D16-1041,C14-1016,0,0.0205832,"5,301,867 and |K |= 1,358,949. 3.2 Sense vectors S ENS E MBED (Iacobacci et al., 2015)8 constitutes the sense embeddings space that we use for training our hypernym detection algorithm. Vectors in the S ENS E MBED space, denoted as S, are latent continuous representations of word senses based on the Word2Vec architecture (Mikolov et al., 2013a), which was applied on a disambiguated Wikipedia corpus. Each vector ~v ∈ S represents a BabelNet sense, i.e. a synset along with one of its lexicalizations (e.g. album_chart_bn:00002488n). This differs from unsupervised approaches (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014) that learn sense representations from text corpora only and are not mapped to any lexical resource, limiting their application in our task. 4 Methodology Our approach can be summarized as follows. First, we take advantage of a clustering algorithm for allocating each BabelNet synset of the training set into a domain cluster C (Section 4.1). Then, we expand the training set by exploiting the different lexicalizations available for each BabelNet synset (Section 4.2). Finally, we learn a cluster-wise linear projection (a hypernym transformation matrix) over all pairs ("
D16-1041,J13-3007,0,0.223644,"12). By embedding cues about how we perceive concepts, and how these concepts generalize in a domain of knowledge, these resources bear a capacity for generalization that lies at the core of human cognition (Yu et al., 2015) and have become key in Natural Language Processing (NLP) tasks where inference and reasoning have proved to be essential. In In domain knowledge formalization, prominent work has made use of the web (Kozareva and Hovy, 2010), lexico-syntactic patterns (Navigli and Velardi, 2010), syntactic evidence (Luu Anh et al., 2014), graph-based algorithms (Fountain and Lapata, 2012; Velardi et al., 2013; Bansal et al., 2014) or popularity of web sources (Luu Anh et al., 2015). As for enabling large-scale knowledge repositories, this task often tackles the additional problem of disambiguating word senses and entity mentions. Notable approaches of this kind include Yago (Suchanek et al., 2007), WikiTaxonomy (Ponzetto and Strube, 2008), and the Wikipedia Bitaxonomy (Flati et al., 2014). In addition, while not being taxonomy learning systems per se, semi-supervised systems for Information Extraction such as N ELL (Carlson et al., 2010) rely crucially on taxonomized concepts and their relations w"
D16-1041,P09-1031,0,0.0118506,"a twostep process, namely is-a (hypernymic) relation de424 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 424–435, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tection, and graph induction. The hypernym detection phase has gathered much interest not only for taxonomy learning but also for lexical semantics. It has been addressed by means of pattern-based methods1 (Hearst, 1992; Snow et al., 2004; Kozareva and Hovy, 2010; Carlson et al., 2010; Boella and Di Caro, 2013; Espinosa-Anke et al., 2016), clustering (Yang and Callan, 2009) and graph-based approaches (Fountain and Lapata, 2012; Velardi et al., 2013). Moreover, work stemming from distributional semantics introduced notions of linguistic regularities found in vector representations such as word embeddings (Mikolov et al., 2013d). In this area, supervised approaches, arguably the most popular nowadays, learn a feature vector between term-hypernym vector pairs and train classifiers to predict hypernymic relations. These pairs may be represented either as a concatenation of both vectors (Baroni et al., 2012), difference (Roller et al., 2014), dot-product (Mikolov et"
D16-1041,P14-2089,0,0.0250439,"Missing"
D18-1508,E17-2017,1,0.714713,"ended meaning of a social media message. In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). The problem of emoji prediction, albeit recent, has already seen important developments. For example, Barbieri et al. (2017) describe an LSTM model which outperforms a logistic regression baseline based on word vector averaging, and even human judgement in some scenarios. The above contributions, in addition to emoji similarity datasets (Barbieri et al., 2016; Wijeratne et al., 2017) or emoji sentiment lexicons (Novak et al., 2015; Wijeratne et al., 2016; Kimura and Katsurai, 2017; Rodrigues et al., 2018), have paved the way for better understanding the semantics of emojis. However, our understanding of what exactly the neural models for emoji prediction are capturing is currently very limited. What is a model prio"
D18-1508,S18-2011,1,0.881576,"Missing"
D18-1508,S18-1003,1,0.85637,"tention modifications to the “vanilla” 2-BiLSTM model. αj,l hj j=1 stacked Bi-LSTMs (2-BiLSTMs) without attention; and (3) 2 stacked Bi-LSTMs with standard attention (2-BiLSTMsa ) (Felbo et al., 2017). Finally, we denote as 2-BiLSTMsl our proposed label-wise attentive Bi-LSTM architecture. βl = wf,l sl + bf,l eβl pl = PL r=1 e 3 βr Evaluation This section describes the main experiment w.r.t the performance of our proposed attention mechanism, in comparison with existing emoji prediction systems. We use the data made available in the context of the SemEval 2018 Shared Task on Emoji Prediction (Barbieri et al., 2018). Given a tweet, the task consists of predicting an associated emoji from a predefined set of 20 emoji labels. We evaluate our model on the English split of the official task dataset. We also show results from additional experiments in which the label space ranged from 20 to 200 emojis. These extended experiments are performed on a corpus of around 100M tweets geolocalized in the United States and posted between October 2015 and May 2018. Models. In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1) FastText (Joulin et al"
D18-1508,L16-1626,1,0.748553,"ieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). The problem of emoji prediction, albeit recent, has already seen important developments. For example, Barbieri et al. (2017) describe an LSTM model which outperforms a logistic regression baseline based on word vector averaging, and even human judgement in some scenarios. The above contributions, in addition to emoji similarity datasets (Barbieri et al., 2016; Wijeratne et al., 2017) or emoji sentiment lexicons (Novak et al., 2015; Wijeratne et al., 2016; Kimura and Katsurai, 2017; Rodrigues et al., 2018), have paved the way for better understanding the semantics of emojis. However, our understanding of what exactly the neural models for emoji prediction are capturing is currently very limited. What is a model prioritizing when associating a message with, for example, positive ( ), negative ( ) or patriotic ( ) intents? A natural way of assessing this would be to implement an attention mechanism over the hidden states of LSTM layers. Attentive arc"
D18-1508,S18-1004,0,0.0966617,"ent features are used for predictions, a topic of increasing interest in NLP (Linzen et al., 2016; Palangi et al., 2017). As we experimented with sets of emoji labels of different sizes, our proposed label-wise attention architecture proved especially well-suited for emojis which were infrequent in the training data, making the system less biased towards the most frequent. We see this as a first step to improve the robustness of recurrent neural networks in datasets with unbalanced distributions, as they were shown not to perform better than well-tuned SVMs on the emoji predicion task (C ¸ o¨ ltekin and Rama, 2018). As for future work, we plan to apply our labelwise attention mechanism to understand other interesting linguistic properties of human-generated text in social media, and other multi-class or multilabel classification problems. Finally, code to reproduce our experiments and additional examples of label-wise attention weights from input tweets can be downloaded at https://fvancesco.github. io/label_wise_attention/. Acknowledgments F. Barbieri and H. Saggion acknowledge support from the TUNER project (TIN2015-65308-C5-5R, MINECO/FEDER, UE). Luis Espinosa-Anke, Jose Camacho-Collados and Steven S"
D18-1508,D17-1169,0,0.511515,"y ( ). Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a social media message. In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online. It has furthermore proven to be useful for sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). The problem of emoji prediction, albeit recent, has already seen important developments. For example, Barbieri et al. (2017) describe an LSTM model which outperforms a logistic regression baseline based on word vector averaging, and even human judgement in some scenarios. The above contributions, in addition to emoji similarity datasets (Barbieri et al., 2016; Wijeratne et al., 2017) or emoji sentiment lexicons (Novak et al., 2015; Wijeratne et al., 2016; Kimura and Katsurai, 2017; Rodrigues et al., 2018), have paved the way for better understanding the semantics of emojis. However, our unde"
D18-1508,E17-2068,0,0.289792,"ake decisions over one single emoji at a time, but without the computational burden and risk of overfitting associated with learning separate LSTMbased classifiers for each emoji. Our contribution in this paper is twofold. First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages. Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier. We observed a performance improvement over competitive baselines such as FastText (FT) (Joulin et al., 2017) and Deepmoji (Felbo et al., 2017), which is most noticeable in the case of infrequent emojis. This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes. 2 Methodology Our base architecture is the Deepmoji model (Felbo et al., 2017), which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention"
D18-1508,Q16-1037,0,0.0308213,"experience ! ( ) second day snowboarding ever and i decided to try night boarding ... what an experience ! ( ) second day snowboarding ever and i decided to try night boarding ... what an experience ! ( ) Figure 4: Attention weights α and αl of single and label-wise attentive models. Gold: 5 Conclusion In this paper we have presented a neural architecture for emoji prediction based on a label-wise attention mechanism, which, in addition to improving performance, provides a degree of interpretability about how different features are used for predictions, a topic of increasing interest in NLP (Linzen et al., 2016; Palangi et al., 2017). As we experimented with sets of emoji labels of different sizes, our proposed label-wise attention architecture proved especially well-suited for emojis which were infrequent in the training data, making the system less biased towards the most frequent. We see this as a first step to improve the robustness of recurrent neural networks in datasets with unbalanced distributions, as they were shown not to perform better than well-tuned SVMs on the emoji predicion task (C ¸ o¨ ltekin and Rama, 2018). As for future work, we plan to apply our labelwise attention mechanism to"
D18-1508,D15-1166,0,0.0356094,"are capturing is currently very limited. What is a model prioritizing when associating a message with, for example, positive ( ), negative ( ) or patriotic ( ) intents? A natural way of assessing this would be to implement an attention mechanism over the hidden states of LSTM layers. Attentive architectures in NLP, in fact, have recently received substantial interest, mostly for sequenceto-sequence models (which are useful for machine translation, summarization or language modeling), and a myriad of modifications have been proposed, including additive (Bahdanau et al., 2015), multiplicative (Luong et al., 2015) or self (Lin et al., 2017) attention mechanisms. However, standard attention mechanisms only tell us which text fragments are considered impor4766 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4766–4771 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Standard attention BiLSTM Labelwise attention BiLSTM LookUp LookUp BiLSTMs BiLSTMs h1, ..., hN h1, ..., hN Attention (α) s Linear |s|→ L β1, ..., βL Attention (α1) s1 Linear |s |→ 1 β1 Attention (α2) s2 Linear |s |→ 1 β2 Attention (αi) si Linea"
D18-1508,N16-1174,0,0.0658504,"ase of infrequent emojis. This suggests that an attentive mechanism can be leveraged to make neural architectures more sensitive to instances of underrepresented classes. 2 Methodology Our base architecture is the Deepmoji model (Felbo et al., 2017), which is based on two stacked word-based bi-directional LSTM recurrent neural networks with skip connections between the first and the second LSTM. The model also includes an attention module to increase its sensitivity to individual words during prediction. In general, attention mechanisms allow the model to focus on specific words of the input (Yang et al., 2016), instead of having to memorize all the important features in a fixed-length vector. The main architectural difference with respect to the typical attention is illustrated in Figure 1. In Felbo et al. (2017), attention is computed as follows: zi = wa hi + ba e zi αi = PN zj j=1 e s= N X αj hj j=1 Here hi ∈ Rd is the hidden representation of the LSTM corresponding to the ith word, with N the total number of words in the sentence. The weight vector wa ∈ Rd and bias term ba ∈ R map this hidden representation to a value that reflects the importance of this state for the considered classification p"
E03-2013,W03-2805,1,0.791342,"entences to a set of &quot;correct&quot; extracted sentences, then co-selection is measured by precision, recall and F-score. Gate&apos;s AnnotationDiff tool enables two sets of annotations on a document to be quantitative compared (i,e. two summaries produced by two summarisation configurations). We are making use of human annotated corpus (source documents and sets of extracts) (Saggion et al., 2002b) in order to evaluate different system configurations and to identify experimentally the best feature combination. Processing resources for content-based evaluation have already been integrated in the system (Pastra and Saggion, 2003). Future work will include the use of document-summary (non extractive) pairs (from the Document Understanding Conferences Corpus as well as from the HKNews Corpus (Saggion et al., 2002a)) and machine learning algorithms to obtain the best combination of the summarisation features, where &apos;extracts&apos; will be learn based on the automatic alignment between the non-extractive summaries and their source documents. The summarisation 238 system presented here provides a framework for experimentation in text summarisation research. The summariser combines two orthogonal approaches in a simple way takin"
E03-2013,J02-4005,1,0.748078,"summarisation addressing the need for user adaptation. 1 Introduction Two approaches are generally considered in automatic text summarisation research: the shallow sentence extraction approach and the deep, understand and generate approach (Mani, 2000). Sentence extraction methods are quite robust, but sentence extracts suffer from lack of cohesion and coherence. Methods that identify the essential information of the document by either information extraction or text understanding and that use the key information to produce a new text, lead to high-quality summarisation (Paice and Jones, 1993; Saggion and Lapalme, 2002) but suffer from the knowledge-bottleneck problem: adapting information extraction rules, templates, and generation grammars to new tasks or domains is time consuming. An alternative to these approaches is to use combination of robust techniques for semantic tagging together with statistical methods (Saggion, 2002). Here, we present a summarisation system that makes use of robust components for semantic tagging and coreference resolution provided by GATE (Cunningham et al., 2002). Our system combines GATE components with well established statistical techniques developed for the purpose of text"
E03-2013,C02-1073,1,0.68697,"Missing"
E03-2013,saggion-etal-2002-developing,1,0.791192,"rch projects make use of in-house evaluation, making it difficult to replicate experiments, to compare results, or to use evaluation data for training purposes. When text summarisation systems are evaluated by comparing extracted sentences to a set of &quot;correct&quot; extracted sentences, then co-selection is measured by precision, recall and F-score. Gate&apos;s AnnotationDiff tool enables two sets of annotations on a document to be quantitative compared (i,e. two summaries produced by two summarisation configurations). We are making use of human annotated corpus (source documents and sets of extracts) (Saggion et al., 2002b) in order to evaluate different system configurations and to identify experimentally the best feature combination. Processing resources for content-based evaluation have already been integrated in the system (Pastra and Saggion, 2003). Future work will include the use of document-summary (non extractive) pairs (from the Document Understanding Conferences Corpus as well as from the HKNews Corpus (Saggion et al., 2002a)) and machine learning algorithms to obtain the best combination of the summarisation features, where &apos;extracts&apos; will be learn based on the automatic alignment between the non-e"
E03-2014,J98-3005,0,0.0411612,"e definitions; and template generation and filling component that uses the domain lexicon and linguistic output of the first step as a guidance to fill-in the templates. The systems takes advantage of the information extracted from formal texts (e.g., lists of players) in order to carry out the analysis of tickers. 3 Merging or Cross-document Event Coreference The merging component in MUMIS combines the partial information as extracted from various sources, such that more complete annotations can be obtained. Information extraction and merging from multiple sources has been tried in the past (Radev and McKeown, 1998) but only for single events, the novelty of our approach consists on applying merging to multiple-events extracted from multiple sources. As an example consider the following situation (Netherlands-Yugoslavia match): One of the IE components extracted from document A that in the 30th minute of the match a free-kick was taken, but did not discover who took it. It did find the names of two players, though: Mihajlovic (a Yugoslavian player) and Van der Sar (the Dutch keeper). From document B a save in the 31st minute was extracted by the IE component, and the names of the same two players were re"
E14-3007,R13-1011,0,0.0113776,"that associates to a word with its most frequently used part of speech. We also adopted the Java API for WordNet Searching 2 The American National Corpus (http://www.anc.org/) is, as we read in the web site, a massive electronic collection of American English words (15 million) 3 https://github.com/vinhkhuc/TwitterTokenizer/blob/master/src/Twokenizer.java 57 4.1 (Spell, 2009) to perform some operation on WordNet synsets. It is worth noting that although our approach to text processing is rather superficial for the moment, other tools are available to perform deeper tweet linguistic analysis (Bontcheva et al., 2013; Derczynski et al., 2013). 4 Frequency As said previously unexpectedness can be a signal of irony and in this first group of features we try to detect it. We explore the frequency imbalance between words, i.e. register inconsistencies between terms of the same tweet. The idea is that the use of many words commonly used in English (i.e. high frequency in ANC) and only a few terms rarely used in English (i.e. low frequency in ANC) in the same sentence creates imbalance that may cause unexpectedness, since within a single tweet only one kind of register is expected. We are able to explore this a"
E14-3007,W02-1011,0,0.0256521,"s features will not be able to detect it, thus we needed to add two more. For this purpose the model includes positive single gap defined as the difference between most positive word and the mean of all the sentiment scores of all the words of the tweet and negative single gap defined in the same way, but with the most negative one. 4.8 Figure 1: Information gain value of each group (mean of the features belonged to each group) over the three balanced corpus. Bag of Words Baseline We perform two types of experiments: Based on previous work on sentiment analysis and opinon classification (see (Pang et al., 2002; Dave et al., 2003) for example) we also investigate the value of using bag of words representations for irony classification. In this case, each tweet is represented as a set of word features. Because of the brevity of tweets, we are only considering presence/absence of terms instead of frequency-based representations based on tf ∗ idf . 5 • we run in each of the datasets a 10-fold crossvalidation classification; • across datasets, we train the classifier in one dataset and apply it to the other two datasets. To perform these experiments, we create three balanced datasets containing each one"
E14-3007,W10-2914,0,0.235418,"nic similes, detecting common terms used in this ironic comparison. Reyes et. al (2013) have recently proposed a model to detect irony in Twitter, which is based on four groups of features: signatures, unexpectedness, style, and emotional scenarios. Their classification results support the idea that textual features can capture patterns used by people to convey irony. Among the proposed features, skip-grams (part of the style group) which captures word sequences that contain (or skip over) arbitrary gaps, seems to be the best one. There are also a few computational model that detect sarcasm ((Davidov et al., 2010); (Gonz´alezIb´an˜ ez et al., 2011); (Liebrecht et al., 2013)) on Twitter and Amazon, but even if one may argue that sarcasm and irony are the same linguistic phenomena, the latter is more similar to mocking or 3 Data and Text Processing The dataset used for the experiments reported in this paper has been prepared by Reyes et al. (2013). It is a corpus of 40.000 tweets equally divided into four different topics: Irony, Education, Humour, and Politics where the last three topics are considered non-ironic. The tweets were automatically selected by looking at Twitter hashtags (#irony, #education,"
E14-3007,R13-1026,0,0.00963679,"rd with its most frequently used part of speech. We also adopted the Java API for WordNet Searching 2 The American National Corpus (http://www.anc.org/) is, as we read in the web site, a massive electronic collection of American English words (15 million) 3 https://github.com/vinhkhuc/TwitterTokenizer/blob/master/src/Twokenizer.java 57 4.1 (Spell, 2009) to perform some operation on WordNet synsets. It is worth noting that although our approach to text processing is rather superficial for the moment, other tools are available to perform deeper tweet linguistic analysis (Bontcheva et al., 2013; Derczynski et al., 2013). 4 Frequency As said previously unexpectedness can be a signal of irony and in this first group of features we try to detect it. We explore the frequency imbalance between words, i.e. register inconsistencies between terms of the same tweet. The idea is that the use of many words commonly used in English (i.e. high frequency in ANC) and only a few terms rarely used in English (i.e. low frequency in ANC) in the same sentence creates imbalance that may cause unexpectedness, since within a single tweet only one kind of register is expected. We are able to explore this aspect using the ANC Freque"
E14-3007,esuli-sebastiani-2006-sentiwordnet,0,0.00576052,"defined syno lower mean as mean of slwi (i.e. the arithmetic average of slwi over all the words of a tweet). We also designed two more features: syno lower gap and syno greater gap, but to define them we need two more parameters. The first one is word lowest syno that is the maximum slwi in a tweet. It is formally defined as: wlst = max{|syni,k : f (syni,k ) &lt; f (wi )|} wi (2) The second one is word greatest syno defined as: 4.7 wgst = max{|syni,k : f (syni,k ) > f (wi )|} Sentiments We think that sign of irony could also be found using sentiment analysis. The SentiWordNet sentiment lexicon (Esuli and Sebastiani, 2006) assigns to each synset of WordNet sentiment scores of positivity and negativity. We used these scores to examine what kind of sentiments characterises irony. We explore ironic sentiments with two different views: the first one is the simple analysis of sentiments (to identify the main sentiment that arises from ironic tweets) and the second one concerns sentiment imbalances between words, dewi (3) We are now able to describe syno lower gap which detects the imbalance that creates a common synonym in a context of rare synonyms. It is the difference between word lowest syno and syno lower mean."
E14-3007,filatova-2012-irony,0,0.0167142,"andom Forest (RF) and Decision Trees (DT)) we observe that (see Table 2) RF is better in cross-validation but acrossdomains both algorithms are comparable. Turning now to the state of the art we compare our approach to (Reyes et al., 2013), the numbers presented in Table 3 seem to indicate that (i) our approach is more balanced in terms of precision and recall and that (ii) our approach performs slightly better in terms of F-Measure in two out of three domains. ments. The ambiguity aspect is still weak in this research, and it needs to be improved. Also experiments adopting different corpora (Filatova, 2012) and different negative topics may be useful in order to explore the system behaviour in a real situation. Finally, we have relied on very basic tools for linguistic analysis of the tweets, so in the near future we intend to incorporate better linguistic processors. A final aspect we want to investigate is the use of n-grams from huge collections to model “unexpected” word usage. 7 We are greatful to three anonymous reviewers for their comments and suggestions that help improve our paper. The research described in this paper is partially funded by fellowship RYC-2009-04291 from Programa Ram´on"
E14-3007,P11-2102,0,0.155589,"in this ironic comparison. Reyes et. al (2013) have recently proposed a model to detect irony in Twitter, which is based on four groups of features: signatures, unexpectedness, style, and emotional scenarios. Their classification results support the idea that textual features can capture patterns used by people to convey irony. Among the proposed features, skip-grams (part of the style group) which captures word sequences that contain (or skip over) arbitrary gaps, seems to be the best one. There are also a few computational model that detect sarcasm ((Davidov et al., 2010); (Gonz´alezIb´an˜ ez et al., 2011); (Liebrecht et al., 2013)) on Twitter and Amazon, but even if one may argue that sarcasm and irony are the same linguistic phenomena, the latter is more similar to mocking or 3 Data and Text Processing The dataset used for the experiments reported in this paper has been prepared by Reyes et al. (2013). It is a corpus of 40.000 tweets equally divided into four different topics: Irony, Education, Humour, and Politics where the last three topics are considered non-ironic. The tweets were automatically selected by looking at Twitter hashtags (#irony, #education, #humour, and #politics) added by u"
E14-3007,ide-suderman-2004-american,0,0.0165736,"and #politics) added by users in order to link their contribution to a particular subject and community. The hashtags are removed from the tweets for the experiments. According to Reyes et. al (2013), these hashtags were selected for three main reasons: (i) to avoid manual selection of tweets, (ii) to allow irony analysis beyond literary uses, and because (iii) irony hashtag may “reflect a tacit belief about what constitutes irony.” Another corpora is employed in our approach to measure the frequency of word usage. We adopted the Second Release of the American National Corpus Frequency Data2 (Ide and Suderman, 2004), which provides the number of occurrences of a word in the written and spoken ANC. From now on, we will mean with “frequency of a term” the absolute frequency the term has in the ANC. 3.1 Text Processing In order to process the tweets we use the freely available vinhkhuc Twitter Tokenizer3 which allows us to recognise words in each tweet. To partof-speech tag the words, we rely on the Rita WordNet API (Howe, 2009) that associates to a word with its most frequently used part of speech. We also adopted the Java API for WordNet Searching 2 The American National Corpus (http://www.anc.org/) is, a"
E14-3007,W13-1605,0,0.097037,"Missing"
E17-2017,D15-1041,1,0.823519,"esentations: We generate word embeddings which are learned together with the updates to the model. We stochastically replace (with p = 0.5) each word that occurs only once in the training data with a fixed represenation (outof-vocabulary words vector). When we use pretrained word embeddings, these are concatenated with the learned vector representations obtaining a final representation for each word type. This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs. The character-based approach learns representations for words that are orthographically similar, thus, they should be able to handle different alternatives of the same word type occurring in social media. Models In this Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the RNNs and the baselines, is th"
E17-2017,W16-6208,0,0.0726224,"Missing"
E17-2017,P82-1020,0,0.842587,"Missing"
E17-2017,N16-1030,1,0.449927,"s Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the RNNs and the baselines, is that the RNNs take into account sequences of words and thus, the entire context. 3.1 Bi-Directional LSTMs Given the proven effectiveness and the impact of recurrent neural networks in different tasks (Chung et al., 2014; Vinyals et al., 2015; Dzmitry et al., 2014; Dyer et al., 2015; Lample et al., 2016; Wang et al., 2016, inter-alia), which also includes modeling of tweets (Dhingra et al., 2016), our emoji prediction model is based on bi-directional 5 The output embeddings of the emojis have 100 dimensions. 6 100 dimensions. 3 https://dev.twitter.com 4 Available at http://sempub.taln.upf.edu/tw/eacl17 106 3.2 Baselines In this Section we describe the two baselines. Unlike the previous model, the baselines do not take into account the word order. However, in the second baseline (Section 3.2.2) we abstract on the plain word representation using semantic vectors, previously trained on Twitter"
E17-2017,L16-1626,1,0.335627,"advent of social media has brought along a novel way of communication where meaning is composed by combining short text messages and visual enhancements, the so-called emojis. This visual language is as of now a de-facto standard for online communication, available not only in Twitter, but also in other large online platforms such as Facebook, Whatsapp, or Instagram. Despite its status as language form, emojis have been so far scarcely studied from a Natural Language Processing (NLP) standpoint. Notable exceptions include studies focused on emojis’ semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a; Barbieri et al., 2016b; Barbieri et al., 2016c; Eisner et al., 2016; Ljubeˇsic and Fiˇser, 2016), or sentiment (Novak et al., 2015). However, the interplay between text-based messages 1 https://www.washingtonpost.com/news/theintersect/wp/2016/02/19/the-secret-meanings-of-emoji/ 2 http://www.dailydot.com/debug/emojimiscommunicate/ 105 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 105–111, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Long Short-term Memory Network"
E17-2017,N15-1142,0,0.0599508,"his paper. Word Representations: We generate word embeddings which are learned together with the updates to the model. We stochastically replace (with p = 0.5) each word that occurs only once in the training data with a fixed represenation (outof-vocabulary words vector). When we use pretrained word embeddings, these are concatenated with the learned vector representations obtaining a final representation for each word type. This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs. The character-based approach learns representations for words that are orthographically similar, thus, they should be able to handle different alternatives of the same word type occurring in social media. Models In this Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the R"
E17-2017,W10-2914,0,0.0154471,"h token of the tweet. Formally, each message m is represented with the vector Vm : P St V m = t∈Tm |Tm | 4 5 R .60 .60 .59 .61 .61 .63 Table 2: Results of 5, 10 and 20 emojis. Precision, Recall, F-measure. BOW is bag of words, AVG is the Skipgram Average model, C refers to charBLSTM and W refers to word-BLSTM. +P refers to pretrained embeddings. Bag of Words We applied a bag of words classifier as baseline, since it has been successfully employed in several classification tasks, like sentiment analysis and topic modeling (Wallach, 2006; Blei, 2012; Titov and McDonald, 2008; Maas et al., 2011; Davidov et al., 2010). We represent each message with a vector of the most informative tokens (punctuation marks included) selected using term frequency−inverse document frequency (TFIDF). We employ a L2-regularized logistic regression classifier to make the predictions. 3.2.2 P .59 .60 .59 .61 .61 .63 First Experiment This experiment is a classification task, where in each tweet the unique emoji is removed and 7 107 https://catalog.ldc.upenn.edu/LDC2003T05 vised pre-trained semantic knowledge (Ling et al., 2015a), which helps to achieve better results. Emoji P R F1 Rank Num 0.48 0.32 0.35 0.31 0.24 0.46 1 0.44 0."
E17-2017,D15-1176,0,0.0383434,"his paper. Word Representations: We generate word embeddings which are learned together with the updates to the model. We stochastically replace (with p = 0.5) each word that occurs only once in the training data with a fixed represenation (outof-vocabulary words vector). When we use pretrained word embeddings, these are concatenated with the learned vector representations obtaining a final representation for each word type. This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs. The character-based approach learns representations for words that are orthographically similar, thus, they should be able to handle different alternatives of the same word type occurring in social media. Models In this Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the R"
E17-2017,P16-2044,0,0.0123791,"The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the RNNs and the baselines, is that the RNNs take into account sequences of words and thus, the entire context. 3.1 Bi-Directional LSTMs Given the proven effectiveness and the impact of recurrent neural networks in different tasks (Chung et al., 2014; Vinyals et al., 2015; Dzmitry et al., 2014; Dyer et al., 2015; Lample et al., 2016; Wang et al., 2016, inter-alia), which also includes modeling of tweets (Dhingra et al., 2016), our emoji prediction model is based on bi-directional 5 The output embeddings of the emojis have 100 dimensions. 6 100 dimensions. 3 https://dev.twitter.com 4 Available at http://sempub.taln.upf.edu/tw/eacl17 106 3.2 Baselines In this Section we describe the two baselines. Unlike the previous model, the baselines do not take into account the word order. However, in the second baseline (Section 3.2.2) we abstract on the plain word representation using semantic vectors, previously trained on Twitter data. 3.2.1 BOW AVG W C W+P C+P Skip-Gram Vector Average Where Tm are the set of tokens include"
E17-2017,W16-2610,0,0.116772,"Missing"
E17-2017,P15-1033,1,0.46148,"f the LSTMs are word embeddings6 . Following, we present two alternatives explored in the experiments presented in this paper. Word Representations: We generate word embeddings which are learned together with the updates to the model. We stochastically replace (with p = 0.5) each word that occurs only once in the training data with a fixed represenation (outof-vocabulary words vector). When we use pretrained word embeddings, these are concatenated with the learned vector representations obtaining a final representation for each word type. This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs. The character-based approach learns representations for words that are orthographically similar, thus, they should be able to handle different alternatives of the same word type occurring in social media. Models In this Section, we present and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1"
E17-2017,N16-1064,0,0.0376523,"and motivate the models that we use to predict an emoji given a tweet. The first model is an architecture based on Recurrent Neural Networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two major differences between the RNNs and the baselines, is that the RNNs take into account sequences of words and thus, the entire context. 3.1 Bi-Directional LSTMs Given the proven effectiveness and the impact of recurrent neural networks in different tasks (Chung et al., 2014; Vinyals et al., 2015; Dzmitry et al., 2014; Dyer et al., 2015; Lample et al., 2016; Wang et al., 2016, inter-alia), which also includes modeling of tweets (Dhingra et al., 2016), our emoji prediction model is based on bi-directional 5 The output embeddings of the emojis have 100 dimensions. 6 100 dimensions. 3 https://dev.twitter.com 4 Available at http://sempub.taln.upf.edu/tw/eacl17 106 3.2 Baselines In this Section we describe the two baselines. Unlike the previous model, the baselines do not take into account the word order. However, in the second baseline (Section 3.2.2) we abstract on the plain word representation using semantic vectors, previously trained on Twitter data. 3.2.1 BOW AVG"
E17-2017,P11-1015,0,\N,Missing
I08-1020,S07-1012,0,0.112206,"Missing"
I08-1020,P98-1012,0,0.447942,"r documents/pages referring to a person name may return thousand of pages which although containing the name, do not refer to the same individual. Crossdocument coreference is the task of deciding if two entity mentions in two sources refer to the same individual. Because person names are highly ambiguous (i.e., names are shared by many individuals), deciding if two documents returned by a search engine such as Google or Yahoo! refer to the same individual is a difficult problem. Automatic techniques for solving this problem are required not only for better access to information Similarly to (Bagga and Baldwin, 1998; Phan et al., 2006), we have addressed the task as a document clustering problem. We have implemented our own clustering algorithms but rely on available extraction and summarization technology to produce document representations used as input for the clustering procedure. We will shown that our techniques produce not only very good results but are also very competitive when compared with SemEval 2007 systems. We will also show that carefully selection of document representation is of paramount importance to achieve good performance. Our system has a similar level of performance as the best s"
I08-1020,S07-1024,0,0.335685,"it might be - to the target person. Phan el al. (Phan et al., 2006) follow Mann and Yarowsky in their use of a kind of biographical information about a person. They use a machine learning algorithm to classify sentences according to particular information types in order to automatically construct a person profile. Instead of comparing biographical information in the person profile altogether as in (Mann and Yarowsky, 2003), they compare each type of information independently of each other, combining them only to make the final decision. Finally, the best SemEval 2007 Web People Search system (Chen and Martin, 2007) used techniques similar to ours: named entity recognition using off-the-shelf systems. However in addition to semantic information and full document condition they also explore the use of contextual information such as the url where the document comes from. They show that this information is of little help. Our improved system obtained a slightly higher macroaveraged f-score over their system. 9 Conclusions and Future Work We have presented experiments on cross-document coreference of person names in the context of the first SemEval 2007 Web People Search task. We have designed and implemente"
I08-1020,W03-0405,0,0.378522,"and Baldwin, 1998) used the vector space model together with summarization techniques to tackle the cross-document coreference problem. Their approach uses vector representations following a bag-of-words approach. Terms for vector representation are obtained from sentences where the target person appears. They have not presented an analysis of the impact of full document versus summary condition and their clustering algorithm is rather under-specified. Here we have presented a clearer picture of the influence of summary vs full document condition in the clustering process. Mann and Yarowsky (Mann and Yarowsky, 2003) used semantic information extracted from documents referring to the target person in an hierarchical agglomerative clustering algorithm. Semantic information here refers to factual information about a person such as the date of birth, professional career or education. Information is extracted using patterns some of them manually developed and others induced from examples. We differ from this approach in that our semantic information is more general and is not particularly related - although it might be - to the target person. Phan el al. (Phan et al., 2006) follow Mann and Yarowsky in their u"
I08-1020,S07-1041,0,0.278586,"c information with summary text condition (only personal summaries were tried, experiments using descriptive phrases are underway) and it is therefore compared to our configuration PS+M which also uses summary condition together with semantic information. The last column in the table shows improvements over that configuration. Here all semantic types of information taken individually outperform a system which uses the combination of all types. This is probably because all types of information in a personal summary are somehow related to the target person. 7.2 Results per Person Set Following (Popescu and Magnini, 2007), we present purity, inverse purity, and F-score results for all our configurations per category (ACL, US Census, Wikipedia) in the test set. In Tables 7, 8, and 9, results are reported for full Set ACL US C. Wikip. ACL US C. Wikip. ACL US C. Wikip. ACL US C. Wikip. ACL US C. Wikip. ACL US C. Wikip. Purity 0.86 0.81 0.78 0.96 0.94 0.88 0.63 0.52 0.59 0.88 0.88 0.84 0.63 0.52 0.49 0.87 0.85 0.74 I.Purity 0.48 0.71 0.70 0.38 0.61 0.62 0.82 0.87 0.85 0.49 0.64 0.67 0.78 0.86 0.91 0.47 0.66 0.75 F-Score 0.57 0.75 0.73 0.50 0.72 0.71 0.69 0.64 0.68 0.59 0.72 0.72 0.65 0.64 0.62 0.54 0.73 0.72 Table"
I08-1020,S07-1063,1,0.677324,"own clustering algorithms but rely on available extraction and summarization technology to produce document representations used as input for the clustering procedure. We will shown that our techniques produce not only very good results but are also very competitive when compared with SemEval 2007 systems. We will also show that carefully selection of document representation is of paramount importance to achieve good performance. Our system has a similar level of performance as the best system in the recent SemEval 2007 evaluation framework. This paper extends our previous work on this task (Saggion, 2007). 149 2 Evaluation Framework The SemEval evaluation has prepared two sets of data to investigate the cross-document coreference problem: one for development and one for testing. The data consists of around 100 Web files per person name, which have been frozen and so, can be used as an static corpus. Each file in the corpus is associated with an integer number which indicates the rank at which the particular page was retrieved by the search engine. In addition to the files themselves, the following information was available: the page title, the url, and the snippet. In addition to the data itse"
I08-1020,C98-1012,0,\N,Missing
I13-1043,W03-1004,0,0.0117411,"vances of natural language processing (NLP) tools and techniques, new approaches to readability assessment have emerged. Schwarm and Ostendorf (2005), and Petersen and Ostendorf (2009), used statistical language modeling and support vector machines to show that more complex features (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to auto"
I13-1043,E99-1042,0,0.089991,"es indicate that lexically and syntactically complex texts can be very difficult for non-native speakers and people with various reading impairments (e.g. autistic, aphasic, dyslexic or deaf people). Aphasic people, for instance, may encounter problems with less frequent words and some particular sentence constructions (Devlin, 1999). They also have problems in understanding syntactic constructions which do not follow the canonical subject-verbobject structure (e.g. passive constructions), and especially those sentences which are semantically reversible, e.g. “The boy was kissed by the girl” (Carroll et al., 1999). Additionally, aphasic readers may have additional problems with 1 http://www.plainlanguage.gov/ http://november5th.net/resources/Mencap/MakingMyself-Clear.pdf 3 http://www.w3.org/TR/WCAG20/ 2 374 International Joint Conference on Natural Language Processing, pages 374–382, Nagoya, Japan, 14-18 October 2013. (Automated Readability Index (Smith and Senter, 1967)), or US healthcare documents intended for the general public (the SMOG grading (McLaughlin, 1969)). Some of these first readability formulae are still widely in use, given their simplicity (they require only the average sentence and wo"
I13-1043,W11-1601,0,0.0181138,"tures (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the target users might be difficult. This study is an attempt to address this issue. We focus on text simplification systems for Spanish and investigate whether some of the already existing readability indices cou"
I13-1043,E09-1027,0,0.0758898,"to readability assessment have emerged. Schwarm and Ostendorf (2005), and Petersen and Ostendorf (2009), used statistical language modeling and support vector machines to show that more complex features (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the targe"
I13-1043,van-oosten-etal-2010-towards,0,0.0704143,"Missing"
I13-1043,D11-1038,0,0.0180526,"of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the target users might be difficult. This study is an attempt to address this issue. We focus on text simplification systems for Spanish and investigate whether some of the already existing readability indices could be used for the automati"
I13-1043,P12-1107,0,0.0282904,"Missing"
I13-1043,C10-1152,0,0.0149525,"t more complex features (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the target users might be difficult. This study is an attempt to address this issue. We focus on text simplification systems for Spanish and investigate whether some of the already existi"
I13-1043,ruiter-etal-2010-human,0,0.0243733,"and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features which should improve automatic readability assessment of texts for people with cognitive disoped for English (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012), Spanish (Saggion et al., 2011), and Portuguese (Alu´ısio et al., 2008), with recent attempts at Basque (Aranzabe et al., 2012), Swedish (Rybing et al., 2010), and Dutch (Ruiter et al., 2010). With the emergence of these systems, the question we are faced with is how to automatically evaluate their performance given that the access to the target users might be difficult. This study is an attempt to address this issue. We focus on text simplification systems for Spanish and investigate whether some of the already existing readability indices could be used for the automatic evaluation of these systems. Using a corpus of original news texts and their manual simplifications which followed specific guidelines for writing for people with cognitive disabilities, we show that two lexical"
I13-1043,P05-1065,0,0.206827,"ae for Dutch represent the adaptations of the Flesch Reading Ease score, while Spaulding’s Spanish readability formula (Spaulding, 1956) could be seen as an adaptation of the Dale-Chall formula (Dale and Chall, 1948)). Oosten et al. (2010) showed that readability formulae which are solely based on superficial text characteristics (average sentence and word length) seem to be strongly correlated even across different languages (English, Dutch, and Swedish). With the recent advances of natural language processing (NLP) tools and techniques, new approaches to readability assessment have emerged. Schwarm and Ostendorf (2005), and Petersen and Ostendorf (2009), used statistical language modeling and support vector machines to show that more complex features (e.g. average height of the parse tree, average number of noun and verb phrases, etc.) give better readability prediction than the traditional Flesch-Kincaid readability formula. They based their approach on the texts from Weekly Reader4 , and two smaller corpora: Encyclopedia Britannica and Britannica Elementary (Barzilay and Elhadad, 2003), and CNN news stories and their abridged vesions5 . Feng et al. (2009) introduced some new cognitively motivated features"
J02-4005,W97-0703,0,0.308081,"l work of Luhn (1958). Since then several methods and theories have been applied, including the use of term frequency ∗ inverse document frequency (TF ∗ IDF) measures, sentence position, and cue and title words (Luhn 1958; Edmundson 1969; Kupiec, Pedersen, and Chen 1995; Brandow, Mitze, and Rau 1995); partial understanding using conceptual structures (DeJong 1982; Tait 1982); bottom-up understanding, top-down parsing, and automatic linguistic acquisition (Rau, Jacobs, and Zernik 1989); recognition of thematic text structures (Hahn 1990); cohesive properties of texts (Benbrahim and Ahmad 1995; Barzilay and Elhadad 1997); and rhetorical structure theory (Ono, Sumita, and Miike 1994; Marcu 1997). In the context of the scientific article, Rino and Scott (1996) have addressed the problem of coherent selection for text summarization, but they depend on the availability of a complex meaning representation, which in practice is difficult to obtain from the raw text. Instead, superficial analysis in scientific text summarization using lexical information was applied by Lehmam (1997) for the French language. Liddy (1991) produced one of the most complete descriptions of conceptual information for abstracts of empiric"
J02-4005,A00-2024,0,0.104599,"Missing"
J02-4005,P99-1072,0,0.037478,"Missing"
J02-4005,W97-0713,0,0.620733,"een the two automatic systems (SumUM and Autosummarize) and differences with the author abstract at 0.05. In the third experiment, the ANOVA for text quality did not allow us to draw any conclusions about differences in text quality (F(2, 42) = 0.83). 5.2 Evaluation of Content in a Coselection Experiment Our objective in the evaluation of content in a coselection experiment is to measure coselection between sentences selected by our system and a set of “correct” extracted sentences. This method of evaluation has already been used in other summarization evaluations such as Edmundson (1969) and Marcu (1997). The idea is that if we find a high degree of overlap between the sentences selected by an automatic method and the sentences selected by a human, the method can be regarded as effective. Nevertheless, this method of evaluation has been criticized not only because of the low rate of agreement between human subjects in this task (Jing et al. 1998), but also because there is no unique ideal or target abstract for a given document. Instead, there is a set of main ideas that a good abstract should contain (Johnson 1995). In our coselection experiment, we were also interested in comparing our syst"
J02-4005,C94-1056,0,0.0240794,"Missing"
J02-4005,J98-3005,0,0.0727489,"nd generates coordinate structures, avoiding verb repetition. Whereas our algorithm is genre dependent, requiring only shallow parsing, Jing’s algorithm is genre and domain independent and requires full syntactic parsing and disambiguation and extensive linguistic resources. Regarding the fusion of information, we have concentrated only on the fusion of explicit topical information (document topic, section topic, and signaling structural and conceptual elements). Jing and McKeown (2000) have proposed a rule-based algorithm for sentence combination, but no results have been reported. Radev and McKeown (1998) have already addressed the issue of information fusion in the context of multidocument summarization in one specific domain (i.e., terrorism): The fusion of information is achieved through the implementation of summary operators that integrate the information of different templates from different documents referring to the same event. Although those operators are dependent on the specific task of multidocument summarization, and to some extent on the particular domain they deal with, it is interesting to observe that some of Radev and McKeown’s ideas could be applied in order to improve our t"
J02-4005,W00-0401,1,0.709135,"the reader to determine the topics to expand. 3.1 Implementing SumUM The architecture of SumUM is depicted in Figure 3. Our approach to text summarization is based on a superficial analysis of the source document to extract appropriate types of information and on the implementation of some text regeneration techniques. SumUM has been implemented in SICStus Prolog (release 3.7.1) (SICStus 1998) and Perl (Wall, Christiansen, and Schwartz 1996) running on Sun workstations (5.6) and Linux machines (RH 6.0). For a complete description of the system and its implementation, the reader is referred to Saggion (2000). The sources of information we use for implementing our system are a POS tagger (Foster 1991); linguistic and conceptual patterns specified by regular expressions combining POS tags, our syntactic categories, domain concepts, and words; and a conceptual dictionary that implements our conceptual model (241 domain verbs, 163 domain nouns, and 129 adjectives); see Table 5. 3.1.1 Preprocessing and Interpretation. The input article (plain ASCII text in English without markup) is segmented in main units (title, author information, main sections and references) using typographic information (i.e., n"
J02-4005,W98-0307,0,0.0596896,"Missing"
J02-4005,A00-1043,0,\N,Missing
J02-4005,J02-4001,0,\N,Missing
J02-4005,E99-1011,0,\N,Missing
L16-1492,N12-1009,0,0.0599492,"Missing"
L16-1492,W15-1605,1,0.645087,"lties, an early check was scheduled once all annotators had tagged 4 documents each. Table 2: Grading Scale The annotatorion includes a double task: grading the sentences in each document according to their relevance for being included in a summary and providing a handwritten summary no longer than 250 words. We adopted a shorter sentence relevance grading scale than Radev et al. (2003) and asked the annotators to mark the sentences with a value from 1 to 5, 1 being the lowest relevance value and 5 the highest relevance value (Table 2). 4. Corpus Dataset and Annotation Process As described in Fisas et al. (2015), the corpus is a set of 40 randomly selected articles among a representative sample of research papers previously chosen by experts in the domain of Computer Graphics. Articles were classified into four important subjects in this area: Skinning, Motion Capture, Fluid Simulation and Cloth Simulation. The annotation is sentence based as we have considered sentences to be the most meaningful minimal unit for the analysis of scientific discourse. The annotation process is characterized by its collaborative approach between the developers of the methodology, experts in annotation and text mining,"
L16-1492,N13-1132,0,0.0127059,"values of the inter-annotator agreement intra-group (considering only the 3 annotators of the same team) were very low for some annotators, especially in the Skinning annotation team. Further analysis must be done 3086 in order to detect those annotators that do not meet the standard quality in the Citations and Crosswise Features annotation tasks. In this respect, two different strategies are possible: benchmarking the quality across the 4 teams against a reference annotation, or, alternatively, the most reliable annotators could be determined with MACE Multi-Annotator Competence Estimation (Hovy et al., 2013). The grading annotation results can be evaluated considering groups of grades, in order to detect if human annotators have difficulties in distinguishing relevant (grade 4) and very relevant (grade 5) sentences for a summary, or useful (grade 3) from unnecessary information (grade 2). Finally, a second round in the annotation in order to re-annotate the sub-purposes which were left out after the early annotation check would now be an easier task, as the annotators are already trained, and the best ones could be selected. The annotation of the sub-purposes of citations would provide a richer r"
L16-1492,liakata-etal-2010-corpora,0,0.352967,"etorical structure to study the interplay of discourse structures of scientific arguments with formal citations. COMPARISON USE Following Spiegel-R¨osing (1977) and Teufel et al. (2006), Abu-Jbara and Radev (2012) stay with 6 categories (Criticism, Comparison, Use, Substantiation, Basis and Neutral) to determine the purpose and polarity of citations. Research papers include the description of concepts such as advantages, disadvantages or novelties that do not belong exclusively to any of the structural sections of the discourse. They are useful for comparison between scientific articles. Both Liakata et al. (2010) and Teufel et al. (2009) have incorporated some crosswise features in their annotation schemes. Liakata’s 3-layered annotation scheme devotes the 2nd layer to the annotation of properties of some of the concepts previously identified in the first layer. AZ-II, Teufel’s annotation scheme, defines a category to characterize a novelty or an advantage of the approach mentioned in the paper. In reference to grading sentences for summarization, Saggion et al. (2002) compiled human-generated ”ideal” summaries at different compression rates, and obtained a goldstandard of sentence-based agreement, bo"
L16-1492,P03-1048,1,0.669945,") and comments on Common practices in the field, so these concepts were included in the annotation scheme. Finally, Limitations (only referred to the author’s work) are also tagged, as they are of paramount importance in the comparison of different investigations. For example, the sentence: Skeleton Subspace Deformation (SSD) is the predominant approach to character skinning at present. is classified as containing a Common Practice. 3.3. Grading for summarization The third annotation task is related to the summarization of scientific documents, following the works of Saggion et al. (2002) and Radev et al. (2003). 3082 GRADE 1 2 3 4 5 DEFINITION The leader annotators were then assigned a demo environment for training new annotators, together with guidelines and recommendations. Similarly, once selected, the new annotators had also a set of documents just for testing and practicing with the Annote Web annotation platform. TOTALLY IRRELEVANT FOR A SUMMARY SHOULD NOT APPEAR IN A SUMMARY MAY APPEAR IN A SUMMARY RELEVANT FOR A SUMMARY VERY RELEVANT FOR A SUMMARY In order to monitor the progress of the annotation and detect possible deviations or difficulties, an early check was scheduled once all annotator"
L16-1492,saggion-etal-2002-developing,1,0.800451,"elong exclusively to any of the structural sections of the discourse. They are useful for comparison between scientific articles. Both Liakata et al. (2010) and Teufel et al. (2009) have incorporated some crosswise features in their annotation schemes. Liakata’s 3-layered annotation scheme devotes the 2nd layer to the annotation of properties of some of the concepts previously identified in the first layer. AZ-II, Teufel’s annotation scheme, defines a category to characterize a novelty or an advantage of the approach mentioned in the paper. In reference to grading sentences for summarization, Saggion et al. (2002) compiled human-generated ”ideal” summaries at different compression rates, and obtained a goldstandard of sentence-based agreement, both between the annotators, and between the summarizer and the human annotators. Sentences were assigned a score from 0 (irrelevant) to 10 (essential) expressing the annotators subjective opinion about how relevant each sentence is for a summary. 3. 3.1. Multi-Layered Annotation Schema Citations: Purposes and Subpurposes Our annotation scheme for citation purposes is an extension of the proposal of Abu-Jbara and Radev (2012), a well-balanced selection of 6 top-c"
L16-1492,W06-1613,0,0.598525,"Missing"
L16-1492,D09-1155,0,0.530387,"y the interplay of discourse structures of scientific arguments with formal citations. COMPARISON USE Following Spiegel-R¨osing (1977) and Teufel et al. (2006), Abu-Jbara and Radev (2012) stay with 6 categories (Criticism, Comparison, Use, Substantiation, Basis and Neutral) to determine the purpose and polarity of citations. Research papers include the description of concepts such as advantages, disadvantages or novelties that do not belong exclusively to any of the structural sections of the discourse. They are useful for comparison between scientific articles. Both Liakata et al. (2010) and Teufel et al. (2009) have incorporated some crosswise features in their annotation schemes. Liakata’s 3-layered annotation scheme devotes the 2nd layer to the annotation of properties of some of the concepts previously identified in the first layer. AZ-II, Teufel’s annotation scheme, defines a category to characterize a novelty or an advantage of the approach mentioned in the paper. In reference to grading sentences for summarization, Saggion et al. (2002) compiled human-generated ”ideal” summaries at different compression rates, and obtained a goldstandard of sentence-based agreement, both between the annotators"
L16-1528,Q14-1019,0,0.24586,"as a Web service freely available for public use2 . DBpedia Spotlight gives as a result the DBpedia URI, start and end char positions, the value of the rdf:type property, and a confidence score for for each prediction. • TagMe (Ferragina and Scaiella, 2012) is an EL system that matches terms with Wikipedia link texts and disambiguates them using the in-link graph and the Wikipedia page dataset. Then, it performs a pruning process by looking at the entity context. TagMe is available as a web service3 , and provides the Wikipedia page id, Wikipedia categories, and a confidence score. • Babelfy (Moro et al., 2014) is an EL and Word Sense Disambiguation based on non-strict identification of candidate meanings (i.e. not necessarily exact string matching), together with a graph based algorithm that traverses the BabelNet graph and selects the most appropriate semantic interpretation for each candidate. Babelfy is available as a web service4 . Its output is based on the corresponding BabelNet synset of the disambiguated mention. If the synset references to a Wikipedia page, it returns the Wikipedia URL, the DBpedia URI, as well as Wikipedia categories. ELVIS In this section we describe ELVIS, the generic i"
L16-1528,P10-1140,0,0.0261995,"Missing"
L16-1528,rizzo-etal-2014-benchmarking,0,\N,Missing
L16-1626,W14-2609,1,0.362401,"t any Social Media service and instant messaging platform (Jibril and Abdullah, 2013; Park et al., 2013; Park et al., 2014). Emojis (like the older emoticons) support the possibility to express diverse types of contents in a visual, concise and appealing way that is perfectly suited to the informal style of social media communications. The meaning expressed by emoticons has been exploited to enable or improve several tasks related to the automated analysis of Social Media contents, like sentiment analysis (Hogenboom et al., 2015; Hogenboom et al., 2013) or irony detection (Reyes et al., 2013; Barbieri et al., 2014). In this context, emoticons have also been often exploited to label and thus characterize the textual excerpts where they occur. As a consequence, by analyzing all the textual contents where a specific emoticon appears several language resources have been built. In this context, Yang et al. (2007) propose a method to create an emotional lexicons by relying on the textual contents occurring together with emoticons in messages published in the Yahoo! Kimo Blog Service. Tang et al. (2014) build a sentiment lexicon customized to Twitter by training a neural network thanks to tweets labeled with p"
L16-1626,P14-1023,0,0.0205756,"n a dataset of vocabulary 600,141 tokens. The vocabulary of the dataset filtered with the clean filter includes 187,308 tokens, while if filtered with only emojis 856 tokens (the emojis that our model includes). 5. Experiments and Evaluation We performed two different experiments to evaluate our system. In the first experiment we carried out a quantitative evaluation and in the second experiment a qualitative evaluation. 5.1. Quantitative Evaluation We performed a pair similarity and relatedness test, a common practice in embeddings evaluations (Mikolov et al., 2013a; Levy and Goldberg, 2014; Baroni et al., 2014). While there are currently many shared and widely used dataset that model word similarity (Rubenstein and Goodenough, 1965; Miller and Charles, 1991; Finkelstein et al., 2001), in our knowledge, there are not previous similarity tests of Emojis. We compiled EmoTwi50, a dataset that 3968 Filter raw raw raw raw clean clean clean clean onlyemo onlyemo onlyemo onlyemo Window 3 6 9 12 3 6 9 12 3 6 9 12 Sim 0.733 0.749 0.757 0.738 0.777 0.772 0.763 0.766 0.618 0.628 0.616 0.630 Rel 0.788 0.787 0.787 0.775 0.780 0.785 0.770 0.772 0.666 0.678 0.663 0.672 Avg 0.796 0.804 0.808 0.792 0.815 0.815 0.803"
L16-1626,P11-2008,0,0.0474682,"Missing"
L16-1626,C14-1018,0,0.018045,"timent analysis (Hogenboom et al., 2015; Hogenboom et al., 2013) or irony detection (Reyes et al., 2013; Barbieri et al., 2014). In this context, emoticons have also been often exploited to label and thus characterize the textual excerpts where they occur. As a consequence, by analyzing all the textual contents where a specific emoticon appears several language resources have been built. In this context, Yang et al. (2007) propose a method to create an emotional lexicons by relying on the textual contents occurring together with emoticons in messages published in the Yahoo! Kimo Blog Service. Tang et al. (2014) build a sentiment lexicon customized to Twitter by training a neural network thanks to tweets labeled with positive and negative emoticons. Boia et al. (2013) analyze sentiment lexicons generated by considering emoticons showing that in many cases they do not outperform lexicons created only with textual features. Go et al. (2009) and Castellucci et al. (2015) use distant supervision over emotion-labeled textual contents in order to respectively train a sentiment classifier and build a polarity lexicon. In order to better compare the meaning of emoticons, recently new approaches to model the"
L16-1626,P07-2034,0,0.0371449,"le of social media communications. The meaning expressed by emoticons has been exploited to enable or improve several tasks related to the automated analysis of Social Media contents, like sentiment analysis (Hogenboom et al., 2015; Hogenboom et al., 2013) or irony detection (Reyes et al., 2013; Barbieri et al., 2014). In this context, emoticons have also been often exploited to label and thus characterize the textual excerpts where they occur. As a consequence, by analyzing all the textual contents where a specific emoticon appears several language resources have been built. In this context, Yang et al. (2007) propose a method to create an emotional lexicons by relying on the textual contents occurring together with emoticons in messages published in the Yahoo! Kimo Blog Service. Tang et al. (2014) build a sentiment lexicon customized to Twitter by training a neural network thanks to tweets labeled with positive and negative emoticons. Boia et al. (2013) analyze sentiment lexicons generated by considering emoticons showing that in many cases they do not outperform lexicons created only with textual features. Go et al. (2009) and Castellucci et al. (2015) use distant supervision over emotion-labeled"
L18-1298,C16-2029,0,0.0694068,"Missing"
L18-1298,padro-stanilovsky-2012-freeling,0,0.0955869,"Missing"
N13-1027,P08-1004,0,0.0326499,"oach that needs both full parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36"
N13-1027,N03-1003,0,0.0409199,"plate induction from text. Then, in Section 5, we describe the experiments on template induction indicating how we have instantiated the algorithms and in Section 6 we explain how we have extrinsically evaluated the induction process. In Section 7 we discuss the obtained results and in Section 8 we summarize our findings and close the paper. 2 Related Work A long standing issue in natural language processing is how to learn conceptualizations from text in automatic or semi-automatic ways. The availability of redundant data has been used, for example, to discover template-like representations (Barzilay and Lee, 2003) or sentence-level paraphrases which could be used for extraction or generation. Various approaches to concept learning use clustering techniques. (Leung et al., 2011) apply various clustering procedures to learn a small number of slots in three typical information extraction domains, using manually annotated data and fixing the number of concepts to be learnt. (Li et al., 2010) generate templates and extraction patterns for specific entity types (actors, companies, etc.). (Chambers and Jurafsky, 2011) learn the structure of MUC tem271 plates from raw data in English, an approach that needs bo"
N13-1027,P11-1098,0,0.116652,"mmary in Spanish An example of the summaries we want to learn from is presented in Figure 1. It is a summary in the terrorist attack domain in Spanish. It has been 270 Proceedings of NAACL-HLT 2013, pages 270–279, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics manually annotated with concepts such as DateOfAttack, Target, PlaceOfAttack, and NumberOfVictims, which are key in the domain. Our task is to discover from this kind of summary what the concepts are and how to recognise them automatically. As will be shown in this paper and unlike current approaches (Chambers and Jurafsky, 2011; Leung et al., 2011), the methods to be presented here do not require parsing or semantic dictionaries to work or specification of the underlying number of concepts in the domain to be learn. The approach we take learns concepts in the set of domain summaries, relying on noun phrase contextual information. They are able to generate reasonable domain conceptualizations from relatively small datasets and in different languages. The rest of the paper is structured as follows: In Section 2 we overview related work in the area of concept induction from text. Next, in Section 3 we describe the data"
N13-1027,P12-1039,0,0.0239687,"scover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into English, automatic translation of English summaries into"
N13-1027,D11-1075,0,0.383332,"f the summaries we want to learn from is presented in Figure 1. It is a summary in the terrorist attack domain in Spanish. It has been 270 Proceedings of NAACL-HLT 2013, pages 270–279, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics manually annotated with concepts such as DateOfAttack, Target, PlaceOfAttack, and NumberOfVictims, which are key in the domain. Our task is to discover from this kind of summary what the concepts are and how to recognise them automatically. As will be shown in this paper and unlike current approaches (Chambers and Jurafsky, 2011; Leung et al., 2011), the methods to be presented here do not require parsing or semantic dictionaries to work or specification of the underlying number of concepts in the domain to be learn. The approach we take learns concepts in the set of domain summaries, relying on noun phrase contextual information. They are able to generate reasonable domain conceptualizations from relatively small datasets and in different languages. The rest of the paper is structured as follows: In Section 2 we overview related work in the area of concept induction from text. Next, in Section 3 we describe the dataset used and how we h"
N13-1027,P10-1066,0,0.0315865,"ral language processing is how to learn conceptualizations from text in automatic or semi-automatic ways. The availability of redundant data has been used, for example, to discover template-like representations (Barzilay and Lee, 2003) or sentence-level paraphrases which could be used for extraction or generation. Various approaches to concept learning use clustering techniques. (Leung et al., 2011) apply various clustering procedures to learn a small number of slots in three typical information extraction domains, using manually annotated data and fixing the number of concepts to be learnt. (Li et al., 2010) generate templates and extraction patterns for specific entity types (actors, companies, etc.). (Chambers and Jurafsky, 2011) learn the structure of MUC tem271 plates from raw data in English, an approach that needs both full parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction sc"
N13-1027,P10-1031,0,0.171192,"dictionaries. If high quality human annotated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data (Ciravegna and Wilks, 2003; Yangarber, 2003). Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications. However, it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (Poon and Domingos, 2010; Biemann, 2005; Buitelaar and Magnini, 2005). We therefore here concentrate on learning template-like representations from concise event summaries which should contain the key information of an event. 18 de julio de 1994DateOfAttack . Un atentado contra la sede de la Asociaci´on Mutual Israelita ArgentinaTarget de Buenos AiresPlaceOfAttack causa la muerte de 86NumberOfVictims personas. (18th July 1994. An attack against the headquarters of the Jewish Mutual Association in Buenos Aires, Argentina, kills 86 people.) Figure 1: Sample of Human Annotated Summary in Spanish An example of the summar"
N13-1027,A00-2026,0,0.0750146,"an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into Eng"
N13-1027,C02-1070,0,0.10789,"Missing"
N13-1027,J02-4005,1,0.652626,"ing procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into English, automatic translation"
N13-1027,W11-4502,1,0.880498,"Missing"
N13-1027,saggion-szasz-2012-concisus,1,0.714898,"n a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into English, automatic translation of English summaries into Spanish, and associated original full documents in Spanish and English for two of the domains (Aviation Accidents and Earthquakes"
N13-1027,W12-3003,1,0.837752,"al., 2011) apply various clustering procedures to learn a small number of slots in three typical information extraction domains, using manually annotated data and fixing the number of concepts to be learnt. (Li et al., 2010) generate templates and extraction patterns for specific entity types (actors, companies, etc.). (Chambers and Jurafsky, 2011) learn the structure of MUC tem271 plates from raw data in English, an approach that needs both full parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993;"
N13-1027,P03-1044,0,0.0327687,"titividad, Secretar´ıa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Spain under project number TIN201238584-C06-03 and Advanced Research Fellowship RYC-200904291. We thank Biljana Drndarevi´c for proofreading the paper. quality human annotated data is available, then rulebased or statistical systems can be trained on this data (Brill, 1994), reducing the efforts of writing rules and handcrafting dictionaries. If high quality human annotated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data (Ciravegna and Wilks, 2003; Yangarber, 2003). Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications. However, it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (Poon and Domingos, 2010; Biemann, 2005; Buitelaar and Magnini, 2005). We therefore here concentrate on learning template-like representations from concise event summaries which should contain the key informa"
N18-2107,D15-1041,1,0.848507,"101), initialized with pretrained parameters learned on ImageNet (Deng et al., 2009). We use this model as a starting point to later finetune it on our emoji classification task. Learning rate was set to 0.0001 and we early stopped the training when there was not improving in the validation set. 3.3 B-LSTM Baseline Barbieri et al. (2017) propose a recurrent neural network approach for the emoji prediction task. We use this model as baseline, to verify whether FastText achieves comparable performance. They used a Bidirectional LSTM with character representation of the words (Ling et al., 2015; Ballesteros et al., 2015) to handle orthographic variants (or even spelling errors) of the same word that occur in social media (e.g. cooooool vs cool). 4 Experiments and Evaluation In order to study the relation between Instagram posts and emojis, we performed two different experiments. In the first experiment (Section 4.2) we compare the FastText model with the state of the art on emoji classification (B-LSTM) by Barbieri et al. (2017). Our second experiment (Section 4.3) evaluates the visual (ResNet) and textual (FastText) models on the emoji prediction task. Moreover, we evaluate a multimodal combination of both m"
N18-2107,E17-2017,1,0.94921,"is that incorporating the two synergistic modalities, in a combined model, improves accuracy in an emoji prediction task. This result demonstrates that these two modalities (text and images) encode different information on the use of emojis and therefore can complement each other. 1 Introduction In the past few years the use of emojis in social media has increased exponentially, changing the way we communicate. The combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images. Recent work (Barbieri et al., 2017) has shown that textual information can be used to predict emojis associated to text. In this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models. We explore the use of emojis in the social media platform Instagram. We put forward a multimodal approach to predict the emojis associated to an In2 Dataset and Task Dataset: We gathered Instagram posts published between July 2016 and October 2016, and geolocalized in the U"
N18-2107,W17-5216,0,0.261348,"orrectly predicts both of them: the blue color in the picture associated to (2) helps to change the color of the heart, and the sunny/bright picture of the garden in (1) helps to correctly predict . 5 Related Work Modeling the semantics of emojis, and their applications, is a relatively novel research problem with direct applications in any social media task. Since emojis do not have a clear grammar, it is not clear their role in text messages. Emojis are considered function words or even affective markers (Na’aman et al., 2017), that can potentially affect the overall semantics of a message (Donato and Paggio, 2017). Emojis can encode different meanings, and they can be interpreted differently. Emoji interpretation has been explored user-wise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), and gender-wise (Chen et al., 2017) and time-wise (Barbieri et al., 2018). Emoji sematics and usage have been studied with distributional semantics, with models trained on Twitter data (Barbieri et al., 2016c), Twitter data together with the official unicode description (Eisner et al., 2016), or using text from a popular keyboard app Ai et al"
N18-2107,S18-2011,1,0.823962,"ncy Maps In order to show the parts of the image most relevant for each class we analyze the global average pooling (Lin et al., 2013) on the convolutional 682 context, Wijeratne et al. (2017a) propose a platform for exploring emoji semantics. In order to further study emoji semantics, two datasets with pairwise emoji similarity, with human annotations, have been proposed: EmoTwi50 (Barbieri et al., 2016c) and EmoSim508 (Wijeratne et al., 2017b). Emoji similarity has been also used for proposing efficient keyboard emoji organization (Pohl et al., 2017). Recently, Barbieri and Camacho-Collados (2018) show that emoji modifiers (skin tones and gender) can affect the semantics vector representation of emojis. Emoji play an important role in the emotional content of a message. Several sentiment lexicons for emojis have been proposed (Novak et al., 2015; Kimura and Katsurai, 2017; Rodrigues et al., 2018) and also studies in the context of emotion and emojis have been published recently (Wood and Ruder, 2016; Hu et al., 2017). During the last decade several studies have shown how sentiment analysis improves when we jointly leverage information coming from different modalities (e.g. text, images"
N18-2107,W16-6208,0,0.104099,"Missing"
N18-2107,D17-1169,0,0.0624218,"edict the emoji to be associated to a piece of content may help to improve natural language processing tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online. Given that emojis may also mislead humans (Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding. As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). Emojis are small images that are commonly included in social media text messages. The combination of visual and textual content in the same message builds up a modern way of communication, that automatic systems are not used to deal with. In this paper we extend recent advances in emoji prediction by putting forward a multimodal approach that is able to predict emojis in Instagram posts. Instagram posts are composed of pictures together with texts which sometimes include emojis. We show that these emojis can be predicted by using the text, but also using the picture. Our main finding is that"
N18-2107,E17-2068,0,0.0438927,"emojis (the 1 In this paper we only utilize the first comment issued by the user who posted the picture. 679 Proceedings of NAACL-HLT 2018, pages 679–686 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 3.2 most frequent emojis are shown in Table 3). Our dataset is composed of 299,809 posts, each containing a picture, the text associated to it and only one emoji. In the experiments we also considered the subsets of the 10 (238,646 posts) and 5 most frequent emojis (184,044 posts) (similarly to the approach followed by Barbieri et al. (2017)). Fastext (Joulin et al., 2017) is a linear model for text classification. We decided to employ FastText as it has been shown that on specific classification tasks, it can achieve competitive results, comparable to complex neural classifiers (RNNs and CNNs), while being much faster. FastText represents a valid approach when dealing with social media content classification, where huge amounts of data needs to be processed and new and relevant information is continuously generated. The FastText algorithm is similar to the CBOW algorithm (Mikolov et al., 2013), where the middle word is replaced by the label, in our case the em"
N18-2107,L16-1626,1,0.900084,"an give context to textual messages like in the following Instagram posts: (1)“Love my new home ” (associated to a picture of a bright garden, outside) and (2) “I can’t believe it’s the first day of school!!! Saliency Maps In order to show the parts of the image most relevant for each class we analyze the global average pooling (Lin et al., 2013) on the convolutional 682 context, Wijeratne et al. (2017a) propose a platform for exploring emoji semantics. In order to further study emoji semantics, two datasets with pairwise emoji similarity, with human annotations, have been proposed: EmoTwi50 (Barbieri et al., 2016c) and EmoSim508 (Wijeratne et al., 2017b). Emoji similarity has been also used for proposing efficient keyboard emoji organization (Pohl et al., 2017). Recently, Barbieri and Camacho-Collados (2018) show that emoji modifiers (skin tones and gender) can affect the semantics vector representation of emojis. Emoji play an important role in the emotional content of a message. Several sentiment lexicons for emojis have been proposed (Novak et al., 2015; Kimura and Katsurai, 2017; Rodrigues et al., 2018) and also studies in the context of emotion and emojis have been published recently (Wood and Ru"
N18-2107,D15-1293,0,0.0722539,"Missing"
N18-2107,D15-1176,0,0.0230806,"101 layers (ResNet-101), initialized with pretrained parameters learned on ImageNet (Deng et al., 2009). We use this model as a starting point to later finetune it on our emoji classification task. Learning rate was set to 0.0001 and we early stopped the training when there was not improving in the validation set. 3.3 B-LSTM Baseline Barbieri et al. (2017) propose a recurrent neural network approach for the emoji prediction task. We use this model as baseline, to verify whether FastText achieves comparable performance. They used a Bidirectional LSTM with character representation of the words (Ling et al., 2015; Ballesteros et al., 2015) to handle orthographic variants (or even spelling errors) of the same word that occur in social media (e.g. cooooool vs cool). 4 Experiments and Evaluation In order to study the relation between Instagram posts and emojis, we performed two different experiments. In the first experiment (Section 4.2) we compare the FastText model with the state of the art on emoji classification (B-LSTM) by Barbieri et al. (2017). Our second experiment (Section 4.3) evaluates the visual (ResNet) and textual (FastText) models on the emoji prediction task. Moreover, we evaluate a multi"
N18-2107,P17-3022,0,0.316941,"Missing"
N18-2107,D15-1303,0,0.0352395,"and gender) can affect the semantics vector representation of emojis. Emoji play an important role in the emotional content of a message. Several sentiment lexicons for emojis have been proposed (Novak et al., 2015; Kimura and Katsurai, 2017; Rodrigues et al., 2018) and also studies in the context of emotion and emojis have been published recently (Wood and Ruder, 2016; Hu et al., 2017). During the last decade several studies have shown how sentiment analysis improves when we jointly leverage information coming from different modalities (e.g. text, images, audio, video) (Morency et al., 2011; Poria et al., 2015; Tran and Cambria, 2018). In particular, when we deal with Social Media posts, the presence of both textual and visual content has promoted a number of investigations on sentiment or emotions (Baecchi et al., 2016; You et al., 2016b,a; Yu et al., 2016; Chen et al., 2015) or emojis (Cappallo et al., 2015, 2018). Figure 2: Three test pictures. From left to right, we show the four most likely predicted emojis and their correspondent class activation mapping heatmap. I love being these boys’ mommy!!!! #myboys #mommy ” (associated to picture of two boys wearing two blue shirts). In both examples t"
P03-1048,J93-1004,0,0.0343641,"Missing"
P03-1048,W00-0403,1,0.724039,". The Kappa coefficient controls agreement P (A) by taking into account agreement by chance P (E) : K= P (A) − P (E) 1 − P (E) No matter how many items or annotators, or how the categories are distributed, K = 0 when there is no agreement other than what would be expected by chance, and K = 1 when agreement is perfect. If two annotators agree less than expected by chance, Kappa can also be negative. We report Kappa between three annotators in the case of human agreement, and between three humans and a system (i.e. four judges) in the next section. 3.1.3 Relative Utility Relative Utility (RU) (Radev et al., 2000) is tested on a large corpus for the first time in this project. RU takes into account chance agreement as a lower bound and interjudge agreement as an upper bound of performance. RU allows judges and summarizers to pick different sentences with similar content in their summaries without penalizing them for doing so. Each judge is asked to indicate the importance of each sentence in a cluster on a scale from 0 to 10. Judges also specify which sentences subsume or paraphrase each other. In relative utility, the score of an automatic summary increases with the importance of the sentences that it"
P03-1048,W97-0710,1,0.831436,"Missing"
P03-1048,E99-1011,0,\N,Missing
P03-1048,W97-0704,0,\N,Missing
P03-1048,J96-2004,0,\N,Missing
P03-1048,I05-2047,0,\N,Missing
P15-2135,P02-1040,0,0.0943817,"Missing"
P15-2135,W03-1004,0,0.0433291,"a greater impact than the size of the datasets on the system’s performance. 3.1 Corpora Wikipedia is a comparable TS corpus of 137,000 automatically aligned sentence pairs from English Wikipedia and Simple English Wikipedia1 , previously used by Coster and Kauchak (2011a). We use a small portion of this corpus (240 sentence pairs) to build the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and bu"
P15-2135,W11-1601,0,0.829994,"e. Our results indicate that only the former can influence the MT output significantly. In order to explore the last issue, we test our models on two different test sets and perform human evaluation of the output of several systems. Introduction In the last few years, a growing number of studies have addressed the text simplification (TS) task as a monolingual machine translation (MT) problem of translating sentences from ‘original’ to ‘simple’ language. Several studies reported promising results using standard phrase-based statistical machine translation (PB-SMT) for this task (Specia, 2010; Coster and Kauchak, 2011a; Wubben et al., 2012), but made no attempt to explain the reasons behind the success of their systems. Specia (2010) obtained reasonably good results (BLEU = 60.75) 823 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 823–828, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Methodology We focus on the two TS corpora available for English (Wikipedia and EncBrit) and train a series of translation models on tra"
P15-2135,P11-2117,0,0.7907,"e. Our results indicate that only the former can influence the MT output significantly. In order to explore the last issue, we test our models on two different test sets and perform human evaluation of the output of several systems. Introduction In the last few years, a growing number of studies have addressed the text simplification (TS) task as a monolingual machine translation (MT) problem of translating sentences from ‘original’ to ‘simple’ language. Several studies reported promising results using standard phrase-based statistical machine translation (PB-SMT) for this task (Specia, 2010; Coster and Kauchak, 2011a; Wubben et al., 2012), but made no attempt to explain the reasons behind the success of their systems. Specia (2010) obtained reasonably good results (BLEU = 60.75) 823 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 823–828, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Methodology We focus on the two TS corpora available for English (Wikipedia and EncBrit) and train a series of translation models on tra"
P15-2135,P12-1107,0,0.362968,"Missing"
P15-2135,N03-1017,0,0.0275149,"build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references and the original sentences as hypotheses, 1 http://www.cs.middlebury.edu/ ˜dkauchak/simplific"
P15-2135,J03-1002,0,0.0084212,"the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references"
P15-2135,P03-1021,0,0.0829358,"from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references and the original sentences as hypotheses, 1 http://www.cs.middlebury.edu/ ˜dkauchak/simplification/ 2 http://www.cs.columbia.edu/˜noemie/ alignment/ 3 Version 2.0 docu"
P15-2135,P07-2045,0,\N,Missing
P99-1078,W97-0713,0,0.0302837,"e the informative part of the abstract elaborate some topics according to the reader's interest by motivating the topics, describing entities and defining concepts. We have defined our method of automatic abstracting by studying a corpus professional abstracts. The method also considers the reader's interest as essential in the process of abstracting. 1 Introduction The idea of producing abstracts or summaries by automatic means is not new, several methodologies have been proposed and tested for automatic abstracting including among others: word distribution (Luhn, 1958); rhetorical analysis (Marcu, 1997); and probabilistic models (Kupiec et al., 1995). Even though some approaches produce acceptable abstracts for specific tasks, it is generally agreed that the problem of coherent selection and expression of information in automatic abstracting remains (Johnson, 1995). One of the main problems is how to ensure the preservation of the message of the original text if sentences picked up from distant parts of the source text are juxtaposed and presented to the reader. Rino and Scott (1996) address the problem of coherent selection for gist preservation, however they depend on the availability of a"
P99-1078,J98-3005,0,0.027513,"he generation step. 5 Discussion In this paper, we have presented a new method of automatic abstracting based on the results obtained from the study of a corpus of professional abstracts and parent documents. In order to implement the model, we rely on techniques in finite state processing, instantiation of templates and re-generation techniques. Paice and Jones (1993) have already used templates representing specific information in a restricted domain in order to generate indicative abstracts. Instead, we aim at the generation of indicative-informative abstracts for domain independent texts. Radev and McKeown (1998) also used instantiated templates, but in order to produce summaries of multiple documents. They focus on the generation of the text while we are addressing the overall process of automatic abstracting. We are testing our method using long technical articles found on the ""Web."" Some outstanding issues axe: the problem of co-reference, the problem of polysemy of the lexical items, the re-generation techniques and the evaluation of the methodology which will be based on the judgment of readers. Acknowledgments I would like to thank my adviser, Prof. Guy Lapalme for encouraging me to present this"
R15-1006,E14-3007,1,0.832392,"created by modelling the contents of the Tweets of each class and exploited to support the classification of new Tweets as belonging to one of these classes. Since 2010 researchers have been proposing several models to detect irony automatically. Veale and Hao (2010) suggested an algorithm for separating ironic from non-ironic similes in English, detecting common terms used in this ironic comparison, Reyes et. al (Reyes et al., 2013) proposed a model to detect irony in English Tweets, pointing out the relevance of skip-grams (word sequences that contain arbitrary gap) to carry out this task. Barbieri and Saggion (2014) designed an irony detection system that avoided the use of word-based features, employing features like frequency imbalance (rare words in a context of common words) and ambiguity (number of senses of a word). However, irony has not been studied intensively in languages other than English. A few researches have been carried out on irony detection on other languages like Portuguese (Carvalho et al., 2009; de Freitas et al., 2014), Dutch (Liebrecht et al., 2013), Spanish (Barbieri et al., 2015), and Italian (Barbieri et al., 2014). Bosco et. al (2013) collected and annotated tweets in Italian f"
R15-1006,W13-1605,0,0.0782338,"Missing"
R15-1006,S13-2053,0,0.0149995,"utomatically mine opinions and sentiments of the people. Sentiment Analysis became recently the subject of several works, many of them focused on short text (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Tumasjan et al., 2010). Some of the best systems for Sentiment Analysis in English also participated to the SemEval shared task (Nakov et al., 2013; Rosenthal et al., 2014). The system that obtained the best performance in the Sentiment Analysis at message level task of Semeval 2013 (Nakov et al., 2013) and 2014 (Rosenthal et al., 2014) mined Twitter to build big sentiment (Mohammad et al., 2013) and emotion lexicons (Mohammad, 2012). Regarding Sentiment Analysis in Italian, the best system (Basile and Novielli, 2014) presented at the 2014 SENTIPOLC shared task used Distributional Semantics. This system took advantage of ten million Tweets split into four classes: Text Analysis and Tools In order to process the text of Tweets so as to enable the feature extraction process, we used the same methodology and tools as Barbieri et al. (2014), the reader can find all the details on the tools used in the said paper. In our experiments we used the dataset employed in SENTIPOLC – the combinati"
R15-1006,C10-2005,0,0.335034,"ne primo tempo: #FiorentinaJuve 0-2 (Tevez, Pogba). Quali sono i vostri commenti sui primi 45 minuti?#ForzaJuve (RT @user: First half: #FiorentinaJuve 0-2 (Tevez, Pogba). What are your comments on the first 45 minutes? #GOJUVE) Introduction The automatic identification of sentiments and opinions expressed by users online is a significant and challenging research trend. The task becomes even more difficult when dealing with short and informal texts like Tweets and other microblog texts. Sentiment Analysis of Tweets has been already investigated by several research studies (Jansen et al., 2009; Barbosa and Feng, 2010). Moreover, during the last few years, many evaluation campaigns have been organised to discuss and compare Sentiment Analysis systems tailored to Tweets. Among these campaigns, since 2013, in the context of SemEval (Nakov et al., 2013), several tasks targeting Sentiment Analysis of English Short Texts took place. In 2014, SENTIPOLC (Basile et al., 2014), the SENTIment POLarity Classification Task of Italian Tweets, was organized in the context of EVALITA 2014, the fourth evaluation campaign of Natural Language Processing and Speech tools for Italian. SENTIPOLC distributed a dataset of Italian"
R15-1006,S12-1033,0,0.0373827,"the people. Sentiment Analysis became recently the subject of several works, many of them focused on short text (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Tumasjan et al., 2010). Some of the best systems for Sentiment Analysis in English also participated to the SemEval shared task (Nakov et al., 2013; Rosenthal et al., 2014). The system that obtained the best performance in the Sentiment Analysis at message level task of Semeval 2013 (Nakov et al., 2013) and 2014 (Rosenthal et al., 2014) mined Twitter to build big sentiment (Mohammad et al., 2013) and emotion lexicons (Mohammad, 2012). Regarding Sentiment Analysis in Italian, the best system (Basile and Novielli, 2014) presented at the 2014 SENTIPOLC shared task used Distributional Semantics. This system took advantage of ten million Tweets split into four classes: Text Analysis and Tools In order to process the text of Tweets so as to enable the feature extraction process, we used the same methodology and tools as Barbieri et al. (2014), the reader can find all the details on the tools used in the said paper. In our experiments we used the dataset employed in SENTIPOLC – the combination SENTITUT (Bosco et al., 2013) and T"
R15-1006,W13-1614,0,0.0194435,"rding Sentiment Analysis in Italian, the best system (Basile and Novielli, 2014) presented at the 2014 SENTIPOLC shared task used Distributional Semantics. This system took advantage of ten million Tweets split into four classes: Text Analysis and Tools In order to process the text of Tweets so as to enable the feature extraction process, we used the same methodology and tools as Barbieri et al. (2014), the reader can find all the details on the tools used in the said paper. In our experiments we used the dataset employed in SENTIPOLC – the combination SENTITUT (Bosco et al., 2013) and TWITA (Basile and Nissim, 2013)). Each Tweet was annotated over four dimensions: subjectivity/objectivity, positivity/negativity, irony/non-irony, and political/nonpolitical topic. SENTIPOLC dataset is made of a collection of Tweet IDs, since the privacy policy of Twitter does not allow to share the text of Tweets. As a consequence we were able to retrieve by the Twitter API the text of only a subset of the Tweets included in the original SENTIPOLC dataset. In particular, our training set included 3998 Tweets (while the original dataset included 4513). 42 Subjectivity Polarity (POS) Polarity (NEG) Irony subjective objective"
R15-1006,S14-2009,0,\N,Missing
R15-1006,S13-2052,0,\N,Missing
R15-1025,bird-etal-2008-acl,0,0.0656254,"iments, we use as T S the WCL Corpus (Navigli et al., 2010), a subset of Wikipedia 177 manually annotated with definitions and hypernyms. This dataset is constructed under the intuition that the first sentence of a Wikipedia article constitutes its textual definition. It is important to highlight that, while this dataset includes semantic information manually annotated such as definiendum or hypernym, we do not exploit any of it, which makes the seed-construction step highly flexible as it only requires the sentence definition/non-definition class. We use as DS a subset of the ACL ARC corpus (Bird et al., 2008), processed with ParsCit (Councill et al., 2008). In this dataset, a well-formedness confidence score is given to each sentence (as these come from pdf parsing and noise is introduced in the process). We exploit this information and keep 500k sentences with a score of over .95. For evaluation, we use two datasets: The MSRNLP 2 and the W00 corpus. The MSR-NLP is a manually constructed small list of 50 abstracts in the NLP field, amounting to 304 sentences: 49 definitions and 255 non-definitions. They are extracted from the Microsoft Academic Research website3 , where abstracts including a defin"
R15-1025,W09-4405,0,0.521941,"used for evaluating DE systems are varied, although in general efforts have been greatly focused on academic and encyclopedic genres. Some prominent examples include German technical texts (Storrer and Wellinghoff, 2006), the IULA Technical Corpus (in Spanish) (Alarc´on et al., 2009), the ACL Anthology (Jin et al., 2013; Reiplinger et al., 2012), the BNC corpus (Rodr´ıguez, 2004), Wikipedia (Navigli and Velardi, 2010), ensembles of domain glossaries and Web documents (Velardi et al., 2008), or technical texts in various languages (Westerhout and Monachesi, 2007; Przepi´orkowski et al., 2007; Borg et al., 2009; Deg´orski et al., 2008; Del Gaudio et al., 2013). We propose a DE approach which, from a starting set of encyclopedic definition seeds, self-trains iteratively and gradually fits its classification capability to a target domain-specific test set. Evaluation is carried out on two corpora: First, a set of 50 abstracts of papers in the field of NLP1 . Here, the target term is defined in the first sentence, and additional information may appear in the form of “syntactically plausible false definitions”, i.e. sentences where the target term is also present, relevant information is provided, but d"
R15-1025,councill-etal-2008-parscit,0,0.0272,"i et al., 2010), a subset of Wikipedia 177 manually annotated with definitions and hypernyms. This dataset is constructed under the intuition that the first sentence of a Wikipedia article constitutes its textual definition. It is important to highlight that, while this dataset includes semantic information manually annotated such as definiendum or hypernym, we do not exploit any of it, which makes the seed-construction step highly flexible as it only requires the sentence definition/non-definition class. We use as DS a subset of the ACL ARC corpus (Bird et al., 2008), processed with ParsCit (Councill et al., 2008). In this dataset, a well-formedness confidence score is given to each sentence (as these come from pdf parsing and noise is introduced in the process). We exploit this information and keep 500k sentences with a score of over .95. For evaluation, we use two datasets: The MSRNLP 2 and the W00 corpus. The MSR-NLP is a manually constructed small list of 50 abstracts in the NLP field, amounting to 304 sentences: 49 definitions and 255 non-definitions. They are extracted from the Microsoft Academic Research website3 , where abstracts including a definition provide a “Definition Context” section. Th"
R15-1025,P13-1052,0,0.0328878,"Missing"
R15-1025,degorski-etal-2008-definition,0,0.0655926,"Missing"
R15-1025,D13-1018,0,0.172677,"t fragments. Finally, more complex morphosyntactic patterns were used by (Boella et al., 2014), who model single tokens as relations over the sentence syntactic dependencies. We refer now to unsupervised approaches to DE. (Reiplinger et al., 2012) benefit from hand crafted definitional patterns. Starting from a set of seed terms and patterns, term/definition pairs are iteratively acquired, together with bootstrapped new patterns. These are obtained via a generalization approach over part-of-speech and term wildcards. Additionally, two interconnected works are (De Benedictis et al., 2013) and (Faralli and Navigli, 2013), in that both bootstrap the web for acquiring large multilingual domain glossaries starting with a few seeds for term and gloss. While both systems behave similarly in extracting glosses and learning new patterns by exploiting html tags, they are substantially different in how acquired glosses are ranked. Specifically, the former exploits the bag-of-words representation of each extracted gloss and its intersection with the domain terminology, while the latter leverages Probabilistic Topic Models (PTM) by estimating the probability of words and term/gloss pairs to be pertinent to the domain. 3"
R15-1025,P14-1089,0,0.0159154,"and Nagao, 1988), question answering systems (Saggion ∗ This work is partially funded by the SKATER project, TIN2012-38584-C06-03, Ministerio de Econom´ıa y Competitividad, Secretar´ıa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Espa˜na; and Dr. Inventor (FP7-ICT-2013.8.1 611383). and Gaizauskas, 2004; Cui et al., 2005), for supporting terminological applications (Meyer, 2001; Sierra et al., 2006), e-learning (Westerhout and Monachesi, 2007), and more recently for multilingual paraphrase extraction (Yan et al., 2013), ontology learning (Velardi et al., 2013) or hypernym discovery (Flati et al., 2014). The corpora that have been used for evaluating DE systems are varied, although in general efforts have been greatly focused on academic and encyclopedic genres. Some prominent examples include German technical texts (Storrer and Wellinghoff, 2006), the IULA Technical Corpus (in Spanish) (Alarc´on et al., 2009), the ACL Anthology (Jin et al., 2013; Reiplinger et al., 2012), the BNC corpus (Rodr´ıguez, 2004), Wikipedia (Navigli and Velardi, 2010), ensembles of domain glossaries and Web documents (Velardi et al., 2008), or technical texts in various languages (Westerhout and Monachesi, 2007; Pr"
R15-1025,D13-1073,0,0.804536,"minological applications (Meyer, 2001; Sierra et al., 2006), e-learning (Westerhout and Monachesi, 2007), and more recently for multilingual paraphrase extraction (Yan et al., 2013), ontology learning (Velardi et al., 2013) or hypernym discovery (Flati et al., 2014). The corpora that have been used for evaluating DE systems are varied, although in general efforts have been greatly focused on academic and encyclopedic genres. Some prominent examples include German technical texts (Storrer and Wellinghoff, 2006), the IULA Technical Corpus (in Spanish) (Alarc´on et al., 2009), the ACL Anthology (Jin et al., 2013; Reiplinger et al., 2012), the BNC corpus (Rodr´ıguez, 2004), Wikipedia (Navigli and Velardi, 2010), ensembles of domain glossaries and Web documents (Velardi et al., 2008), or technical texts in various languages (Westerhout and Monachesi, 2007; Przepi´orkowski et al., 2007; Borg et al., 2009; Deg´orski et al., 2008; Del Gaudio et al., 2013). We propose a DE approach which, from a starting set of encyclopedic definition seeds, self-trains iteratively and gradually fits its classification capability to a target domain-specific test set. Evaluation is carried out on two corpora: First, a set o"
R15-1025,W04-1807,0,0.0991247,"Missing"
R15-1025,muresan-klavans-2002-method,0,0.577323,"ing textual definitions with higher linguistic variability than the classic encyclopedic genus-et-differentia definition, and take the domain of Natural Language Processing as a use case. We also introduce a novel set of features for DE and explore their relevance. Evaluation is carried out on two datasets that reflect opposed ways of expressing definitional knowledge. 1 Introduction Definition Extraction (DE) is the task to automatically extract textual definitions from text (Navigli and Velardi, 2010). It has received notorious attention for its potential application to glossary generation (Muresan and Klavans, 2002; Park et al., 2002), terminological databases (Nakamura and Nagao, 1988), question answering systems (Saggion ∗ This work is partially funded by the SKATER project, TIN2012-38584-C06-03, Ministerio de Econom´ıa y Competitividad, Secretar´ıa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Espa˜na; and Dr. Inventor (FP7-ICT-2013.8.1 611383). and Gaizauskas, 2004; Cui et al., 2005), for supporting terminological applications (Meyer, 2001; Sierra et al., 2006), e-learning (Westerhout and Monachesi, 2007), and more recently for multilingual paraphrase extraction (Yan et al., 2013), ontology"
R15-1025,C88-2098,0,0.83573,"ic encyclopedic genus-et-differentia definition, and take the domain of Natural Language Processing as a use case. We also introduce a novel set of features for DE and explore their relevance. Evaluation is carried out on two datasets that reflect opposed ways of expressing definitional knowledge. 1 Introduction Definition Extraction (DE) is the task to automatically extract textual definitions from text (Navigli and Velardi, 2010). It has received notorious attention for its potential application to glossary generation (Muresan and Klavans, 2002; Park et al., 2002), terminological databases (Nakamura and Nagao, 1988), question answering systems (Saggion ∗ This work is partially funded by the SKATER project, TIN2012-38584-C06-03, Ministerio de Econom´ıa y Competitividad, Secretar´ıa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Espa˜na; and Dr. Inventor (FP7-ICT-2013.8.1 611383). and Gaizauskas, 2004; Cui et al., 2005), for supporting terminological applications (Meyer, 2001; Sierra et al., 2006), e-learning (Westerhout and Monachesi, 2007), and more recently for multilingual paraphrase extraction (Yan et al., 2013), ontology learning (Velardi et al., 2013) or hypernym discovery (Flati et al., 201"
R15-1025,P10-1134,0,0.267095,"iscourse has received less attention. In this paper we propose a weakly supervised bootstrapping approach for identifying textual definitions with higher linguistic variability than the classic encyclopedic genus-et-differentia definition, and take the domain of Natural Language Processing as a use case. We also introduce a novel set of features for DE and explore their relevance. Evaluation is carried out on two datasets that reflect opposed ways of expressing definitional knowledge. 1 Introduction Definition Extraction (DE) is the task to automatically extract textual definitions from text (Navigli and Velardi, 2010). It has received notorious attention for its potential application to glossary generation (Muresan and Klavans, 2002; Park et al., 2002), terminological databases (Nakamura and Nagao, 1988), question answering systems (Saggion ∗ This work is partially funded by the SKATER project, TIN2012-38584-C06-03, Ministerio de Econom´ıa y Competitividad, Secretar´ıa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Espa˜na; and Dr. Inventor (FP7-ICT-2013.8.1 611383). and Gaizauskas, 2004; Cui et al., 2005), for supporting terminological applications (Meyer, 2001; Sierra et al., 2006), e-learning (W"
R15-1025,navigli-etal-2010-annotated,0,0.673192,"Missing"
R15-1025,C02-1142,0,0.547882,"h higher linguistic variability than the classic encyclopedic genus-et-differentia definition, and take the domain of Natural Language Processing as a use case. We also introduce a novel set of features for DE and explore their relevance. Evaluation is carried out on two datasets that reflect opposed ways of expressing definitional knowledge. 1 Introduction Definition Extraction (DE) is the task to automatically extract textual definitions from text (Navigli and Velardi, 2010). It has received notorious attention for its potential application to glossary generation (Muresan and Klavans, 2002; Park et al., 2002), terminological databases (Nakamura and Nagao, 1988), question answering systems (Saggion ∗ This work is partially funded by the SKATER project, TIN2012-38584-C06-03, Ministerio de Econom´ıa y Competitividad, Secretar´ıa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Espa˜na; and Dr. Inventor (FP7-ICT-2013.8.1 611383). and Gaizauskas, 2004; Cui et al., 2005), for supporting terminological applications (Meyer, 2001; Sierra et al., 2006), e-learning (Westerhout and Monachesi, 2007), and more recently for multilingual paraphrase extraction (Yan et al., 2013), ontology learning (Velardi e"
R15-1025,W12-3206,0,0.298645,"Missing"
R15-1025,W09-4408,0,0.0664119,"Missing"
R15-1025,storrer-wellinghoff-2006-automated,0,0.641265,"n, Espa˜na; and Dr. Inventor (FP7-ICT-2013.8.1 611383). and Gaizauskas, 2004; Cui et al., 2005), for supporting terminological applications (Meyer, 2001; Sierra et al., 2006), e-learning (Westerhout and Monachesi, 2007), and more recently for multilingual paraphrase extraction (Yan et al., 2013), ontology learning (Velardi et al., 2013) or hypernym discovery (Flati et al., 2014). The corpora that have been used for evaluating DE systems are varied, although in general efforts have been greatly focused on academic and encyclopedic genres. Some prominent examples include German technical texts (Storrer and Wellinghoff, 2006), the IULA Technical Corpus (in Spanish) (Alarc´on et al., 2009), the ACL Anthology (Jin et al., 2013; Reiplinger et al., 2012), the BNC corpus (Rodr´ıguez, 2004), Wikipedia (Navigli and Velardi, 2010), ensembles of domain glossaries and Web documents (Velardi et al., 2008), or technical texts in various languages (Westerhout and Monachesi, 2007; Przepi´orkowski et al., 2007; Borg et al., 2009; Deg´orski et al., 2008; Del Gaudio et al., 2013). We propose a DE approach which, from a starting set of encyclopedic definition seeds, self-trains iteratively and gradually fits its classification capa"
R15-1025,J13-3007,0,0.0167486,"l., 2002), terminological databases (Nakamura and Nagao, 1988), question answering systems (Saggion ∗ This work is partially funded by the SKATER project, TIN2012-38584-C06-03, Ministerio de Econom´ıa y Competitividad, Secretar´ıa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Espa˜na; and Dr. Inventor (FP7-ICT-2013.8.1 611383). and Gaizauskas, 2004; Cui et al., 2005), for supporting terminological applications (Meyer, 2001; Sierra et al., 2006), e-learning (Westerhout and Monachesi, 2007), and more recently for multilingual paraphrase extraction (Yan et al., 2013), ontology learning (Velardi et al., 2013) or hypernym discovery (Flati et al., 2014). The corpora that have been used for evaluating DE systems are varied, although in general efforts have been greatly focused on academic and encyclopedic genres. Some prominent examples include German technical texts (Storrer and Wellinghoff, 2006), the IULA Technical Corpus (in Spanish) (Alarc´on et al., 2009), the ACL Anthology (Jin et al., 2013; Reiplinger et al., 2012), the BNC corpus (Rodr´ıguez, 2004), Wikipedia (Navigli and Velardi, 2010), ensembles of domain glossaries and Web documents (Velardi et al., 2008), or technical texts in various la"
R15-1025,W07-1706,0,\N,Missing
R15-1025,N13-1007,0,\N,Missing
R15-1025,sarmento-etal-2006-corpografo,0,\N,Missing
R15-1079,P07-2045,0,0.010973,"Missing"
R15-1079,J03-1002,0,0.00467958,"ngs of Recent Advances in Natural Language Processing, pages 611–617, Hissar, Bulgaria, Sep 7–9 2015. iments on three different datasets and languages following the methods proposed in previous studies (Specia, 2010; Coster and Kauchak, 2011a). Training Dev. Test Total Selection • We perform automatic evaluation in terms of the document-wise (BLEU) and the sentencewise BLEU score (S-BLEU). 3.2 PorSim 800 200 100 1100 random Translation Experiments We run three MT experiments using the standard PB-SMT models (Koehn et al., 2003) implemented in the Moses toolkit (Koehn et al., 2007) and GIZA++ (Och and Ney, 2003) to obtain the word alignment. The English experiment uses the Wiki aligned corpus for translation model (TM) and the English part of the Europarl corpora2 for building the language model (LM). The Spanish experiment uses the EsSim dataset to build the TM and the Spanish Europarl for the LM. The Brazilian Portuguese experiment uses the PorSim dataset for the TM and the L´acio-Web corpus3 in Brazilian Portuguese for the LM4 . The sentence pairs for training, development and test sets are selected randomly from the initial dataset. • We calculate sentence-wise BLEU score on the training and deve"
R15-1079,W10-1607,0,0.068022,"Missing"
R15-1079,E14-1076,0,0.0130431,"nature of the sentence pairs used for training and tuning of the PB-SMT models. Introduction Text Simplification (TS) aims to convert complex texts into simpler variants which are more accessible to a wider audiences, e.g. non-native speakers, children, and people diagnosed with intellectual disability, autism, aphasia, dyslexia or congenital deafness. In the last twenty years, many automatic text simplification systems have been proposed, varying from rule-based, e.g. (Brouwers et al., 2014; Saggion et al., 2015) to datadriven, e.g. (Zhu et al., 2010; Woodsend and Lapata, 2011), and hybrid (Siddharthan and Angrosh, 2014). Since 2010, there have been several attempts to approach TS as a machine translation (MT) problem (Specia, 2010; Coster and ˇ Kauchak, 2011a; Stajner, 2014). Instead of translating sentences from one language to another, the goal of text simplification is to translate sentences from ‘original’ to ‘simplified’ language. In this paper, we seek to explore the main reasons for the success or failure of the phrasebased statistical machine translation (PB-SMT) 3 Methodology We apply the following methodology: • We run MT-based text simplification exper611 Proceedings of Recent Advances in Natural"
R15-1079,W14-1206,0,0.0315214,"Missing"
R15-1079,W11-1601,0,0.360071,"dings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations. 1 2 Related Work Specia (2010) achieves BLEU score of 60.75 on a small (only 4,483 sentence pairs) dataset in Brazilian Portuguese, using the standard phrasebased translation model (Koehn et al., 2003) in the Moses toolkit (Koehn et al., 2007). The dataset consists of original sentences and their corresponding manually simplified versions obtained under the PorSimples project (Alu´ısio and Gasperin, 2010) following specific guidelines. Coster and Kauchak (2011a) exploit the same translation model to learn how to simplify English sentences using 137,000 sentence pairs from Wikipedia and Simple English Wikipedia. They show that those results (BLEU = 59.87) can be improved by adding phrasal deletion to the probabilistic translation model, reaching the BLEU score of 60.46. Both those approaches seem to outperform all previous non-MT approaches to TS for English. The fact that Specia (2010) and Coster and Kauchak (2011a) achieve similar performances of the PB-SMT system in spite of large differences in size of their datasets motivates our hypothesis tha"
R15-1079,P15-2135,1,0.876423,"Missing"
R15-1079,P11-2117,0,0.399424,"dings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations. 1 2 Related Work Specia (2010) achieves BLEU score of 60.75 on a small (only 4,483 sentence pairs) dataset in Brazilian Portuguese, using the standard phrasebased translation model (Koehn et al., 2003) in the Moses toolkit (Koehn et al., 2007). The dataset consists of original sentences and their corresponding manually simplified versions obtained under the PorSimples project (Alu´ısio and Gasperin, 2010) following specific guidelines. Coster and Kauchak (2011a) exploit the same translation model to learn how to simplify English sentences using 137,000 sentence pairs from Wikipedia and Simple English Wikipedia. They show that those results (BLEU = 59.87) can be improved by adding phrasal deletion to the probabilistic translation model, reaching the BLEU score of 60.46. Both those approaches seem to outperform all previous non-MT approaches to TS for English. The fact that Specia (2010) and Coster and Kauchak (2011a) achieve similar performances of the PB-SMT system in spite of large differences in size of their datasets motivates our hypothesis tha"
R15-1079,N03-1017,0,0.112247,"g the standard phrasebased translation model. However, our results indicate that this approach works well only when the training dataset consists mostly of those sentence pairs in which the simplified sentence is already very similar to its original. Our findings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations. 1 2 Related Work Specia (2010) achieves BLEU score of 60.75 on a small (only 4,483 sentence pairs) dataset in Brazilian Portuguese, using the standard phrasebased translation model (Koehn et al., 2003) in the Moses toolkit (Koehn et al., 2007). The dataset consists of original sentences and their corresponding manually simplified versions obtained under the PorSimples project (Alu´ısio and Gasperin, 2010) following specific guidelines. Coster and Kauchak (2011a) exploit the same translation model to learn how to simplify English sentences using 137,000 sentence pairs from Wikipedia and Simple English Wikipedia. They show that those results (BLEU = 59.87) can be improved by adding phrasal deletion to the probabilistic translation model, reaching the BLEU score of 60.46. Both those approaches"
R15-1079,D11-1038,0,0.0296337,"e size of the datasets but rather in the nature of the sentence pairs used for training and tuning of the PB-SMT models. Introduction Text Simplification (TS) aims to convert complex texts into simpler variants which are more accessible to a wider audiences, e.g. non-native speakers, children, and people diagnosed with intellectual disability, autism, aphasia, dyslexia or congenital deafness. In the last twenty years, many automatic text simplification systems have been proposed, varying from rule-based, e.g. (Brouwers et al., 2014; Saggion et al., 2015) to datadriven, e.g. (Zhu et al., 2010; Woodsend and Lapata, 2011), and hybrid (Siddharthan and Angrosh, 2014). Since 2010, there have been several attempts to approach TS as a machine translation (MT) problem (Specia, 2010; Coster and ˇ Kauchak, 2011a; Stajner, 2014). Instead of translating sentences from one language to another, the goal of text simplification is to translate sentences from ‘original’ to ‘simplified’ language. In this paper, we seek to explore the main reasons for the success or failure of the phrasebased statistical machine translation (PB-SMT) 3 Methodology We apply the following methodology: • We run MT-based text simplification exper61"
R15-1079,C10-1152,0,0.414816,"tion (TS) task as a machine translation (MT) problem. They report promising results in learning how to translate from ‘original’ to ‘simplified’ language using the standard phrasebased translation model. However, our results indicate that this approach works well only when the training dataset consists mostly of those sentence pairs in which the simplified sentence is already very similar to its original. Our findings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations. 1 2 Related Work Specia (2010) achieves BLEU score of 60.75 on a small (only 4,483 sentence pairs) dataset in Brazilian Portuguese, using the standard phrasebased translation model (Koehn et al., 2003) in the Moses toolkit (Koehn et al., 2007). The dataset consists of original sentences and their corresponding manually simplified versions obtained under the PorSimples project (Alu´ısio and Gasperin, 2010) following specific guidelines. Coster and Kauchak (2011a) exploit the same translation model to learn how to simplify English sentences using 137,000 sentence pairs from Wikipedia and Simple English Wikipedia. They show t"
R15-1080,W10-1607,0,0.0760938,"Missing"
R15-1080,J03-1002,0,0.0260985,"V word was complex), but it will not necessarily deteriorate the grammaticality and meaning preservation of the output sentence (as would be the case in cross-lingual SMT). The problem of accuracy is still present even in monolingual SMT. A small model will not have high enough probability mass to be able to generalise well all the linguistic phenomena a good translation should encompass. The translation model will suffer from a low number of examples and thus might not be able to estimate the probabilities correctly. The unsupervised alignment model implemented in Moses using GIZA++ aligner (Och and Ney, 2003) will have rough statistics for the alignment estimation if computed from a small number of parallel sentences. Related Work With the emergence of the Simple English Wikipedia1 , which together with the “original” English Wikipedia offered a large comparable text simplification (TS) corpus (137,000 sentence pairs), the focus of the ATS for English was shifted towards data-driven approaches. Most of them applied various SMT techniques, either phrasebased (Coster and Kauchak, 2011; Wubben et al., 2012), or syntax-based (Zhu et al., 2010; Woodsend and Lapata, 2011). In other languages, TS corpora"
R15-1080,C12-1023,1,0.848194,"(“heavy” or “light”) significantly influences the results, and (2) the model built using the “light” corpora can still learn some useful simplifications deˇ spite the very small size of the dataset (Stajner, 2014). 2.1 2.3 State-of-the-Art ATS System for Spanish The current state-of-the-art text simplification system for Spanish (Saggion et al., 2015) was built under the Simplext project.2 It employs a modular approach to TS, consisting of three main modules: a rule-based syntactic and lexical simplification modules (Drndarevi´c et al., 2013); and a synonymbased lexical simplification module (Bott et al., 2012). According to the recent evaluation of the full Simplext system (Saggion et al., 2015), the system achieved human scores for grammaticality, meaning preservation, and simplicity comparable to those of the current state-of-the-art data-driven text simplification systems for English (Wubben et al., 2012; Angrosh and Siddharthan, 2014). SMT for Low-Resourced Languages The main problems in SMT applied to lowresourced languages (“simplified” Spanish can be seen as such) are the accuracy and coverage (Irvine and Callison-Burch, 2013). The first problem is the result of the fact that the model does"
R15-1080,P02-1040,0,0.0924012,"g the Light corpus was rated better than the output of the systems built using the Heavy corpus on the same test sentence (Table 5), and (2) the output of the HIERO models was rated better than the output of Results The results of the automatic and human evaluations are presented in the next two subsections. 4.1 Light Manual PB HIERO 4.03 3.91 4.61 5 4 5 5 5 5 4.57 4.40 3.62 5 5 4 5 5 4 2.99 2.93 4.40 3 3 5 2 2 5 Automatic Evaluation We compared the performances of the systems using two automatic MT evaluation metrics, the sentence-level BLEU score (S-BLEU)3 and the document-level BLEU score (Papineni et al., 2002). As the baseline, we used the system which makes no changes to the input (i.e. output of the system is the original sentence). This seems as a natural baseline for this specific task (ATS), as all previous studies (Specia, 2010; Coster and ˇ Kauchak, 2011; Stajner, 2014) reported that their systems are overcautious, usually making only a few or no changes to the input sentence, and only slightly outperform this baseline. For calculating the S-BLEU and BLEU scores, we used the manual simplification (‘gold standard’) as the reference, and the original sentences and the outputs of the four syste"
R15-1080,W14-1206,0,0.0369078,"Missing"
R15-1080,J07-2003,0,0.0290448,"r´oxima reuni´on del Comit´e del Patrimonio Mundial, que ser´a en Bahrein en junio de 2011. Los expertos presentar´an un informe sobre Pompeya en la pr´oxima reuni´on sobre la cultura del mundo. Esta reuni´on ser´a en junio de 2011. Table 1: Different levels of simplification (deviations from the original sentence are shown in italics) Corpus Light Light Heavy Heavy in the next three subsections. 3.1 Corpora In order to test the influence of the level of simplification in TS datasets (“heavy” or “light”) on the system performance, we trained the standard PB-SMT (Koehn et al., 2003) and HIERO (Chiang, 2007) models in the Moses toolkit (Koehn et al., 2007) on two TS corpora: Training 659 659 725 725 Dev. 100 100 100 100 Test 94 94 94 94 Table 2: SMT experiments tokens. The sizes of the datasets used in the four experiments are given in Table 2. 1. Heavy – The TS corpus built under the Simplext project (Saggion et al., 2011), aimed at simplifying texts for people with intellectual disabilities. The original news stories were simplified manually by trained human editors, following detailed guidelines (Anula, 2007). 3.3 Evaluation In order to obtain better insights into the potential problems in the"
R15-1080,W11-1601,0,0.234658,"able to estimate the probabilities correctly. The unsupervised alignment model implemented in Moses using GIZA++ aligner (Och and Ney, 2003) will have rough statistics for the alignment estimation if computed from a small number of parallel sentences. Related Work With the emergence of the Simple English Wikipedia1 , which together with the “original” English Wikipedia offered a large comparable text simplification (TS) corpus (137,000 sentence pairs), the focus of the ATS for English was shifted towards data-driven approaches. Most of them applied various SMT techniques, either phrasebased (Coster and Kauchak, 2011; Wubben et al., 2012), or syntax-based (Zhu et al., 2010; Woodsend and Lapata, 2011). In other languages, TS corpora either do not exist or they are very limited in size (only up to 1,000 sentence pairs). The only known exception to this is the case of Brazilian Portuguese for which there is a parallel TS corpus with 4,483 sentence pairs, built under the PorSimples project (Alu´ısio and Gasperin, 2010), aimed at simplifying texts for low literacy readers. This corpus has been used to train the standard PB-SMT model for ATS (Specia, 2010), and the reported results were promising (BLEU = 60.75)"
R15-1080,W13-2233,0,0.0218352,"s (Drndarevi´c et al., 2013); and a synonymbased lexical simplification module (Bott et al., 2012). According to the recent evaluation of the full Simplext system (Saggion et al., 2015), the system achieved human scores for grammaticality, meaning preservation, and simplicity comparable to those of the current state-of-the-art data-driven text simplification systems for English (Wubben et al., 2012; Angrosh and Siddharthan, 2014). SMT for Low-Resourced Languages The main problems in SMT applied to lowresourced languages (“simplified” Spanish can be seen as such) are the accuracy and coverage (Irvine and Callison-Burch, 2013). The first problem is the result of the fact that the model does not have enough data to estimate good probabilities over the possible translations and therefore ensure correctness of the translation pairs. The second problem occurs when the model and its word coverage are small, which leads to a high number of out-of-vocabulary (OOV) words. Words which 1 Monolingual SMT 3 Methodology The corpora, translation/simplification experiments, and the evaluation procedure are presented 2 https://simple.wikipedia.org/wiki/Main Page 619 www.simplext.es Version Original Light Heavy Example Los expertos"
R15-1080,D11-1038,0,0.0471341,"lemented in Moses using GIZA++ aligner (Och and Ney, 2003) will have rough statistics for the alignment estimation if computed from a small number of parallel sentences. Related Work With the emergence of the Simple English Wikipedia1 , which together with the “original” English Wikipedia offered a large comparable text simplification (TS) corpus (137,000 sentence pairs), the focus of the ATS for English was shifted towards data-driven approaches. Most of them applied various SMT techniques, either phrasebased (Coster and Kauchak, 2011; Wubben et al., 2012), or syntax-based (Zhu et al., 2010; Woodsend and Lapata, 2011). In other languages, TS corpora either do not exist or they are very limited in size (only up to 1,000 sentence pairs). The only known exception to this is the case of Brazilian Portuguese for which there is a parallel TS corpus with 4,483 sentence pairs, built under the PorSimples project (Alu´ısio and Gasperin, 2010), aimed at simplifying texts for low literacy readers. This corpus has been used to train the standard PB-SMT model for ATS (Specia, 2010), and the reported results were promising (BLEU = 60.75) despite the small size of the dataset. The recent attempt at using the standard PB-S"
R15-1080,N03-1017,0,0.0361006,"onservaci´on de Pompeya en la pr´oxima reuni´on del Comit´e del Patrimonio Mundial, que ser´a en Bahrein en junio de 2011. Los expertos presentar´an un informe sobre Pompeya en la pr´oxima reuni´on sobre la cultura del mundo. Esta reuni´on ser´a en junio de 2011. Table 1: Different levels of simplification (deviations from the original sentence are shown in italics) Corpus Light Light Heavy Heavy in the next three subsections. 3.1 Corpora In order to test the influence of the level of simplification in TS datasets (“heavy” or “light”) on the system performance, we trained the standard PB-SMT (Koehn et al., 2003) and HIERO (Chiang, 2007) models in the Moses toolkit (Koehn et al., 2007) on two TS corpora: Training 659 659 725 725 Dev. 100 100 100 100 Test 94 94 94 94 Table 2: SMT experiments tokens. The sizes of the datasets used in the four experiments are given in Table 2. 1. Heavy – The TS corpus built under the Simplext project (Saggion et al., 2011), aimed at simplifying texts for people with intellectual disabilities. The original news stories were simplified manually by trained human editors, following detailed guidelines (Anula, 2007). 3.3 Evaluation In order to obtain better insights into the"
R15-1080,P12-1107,0,0.294938,"Missing"
R15-1080,C10-1152,0,0.264479,"Missing"
R15-1080,2005.mtsummit-papers.11,0,0.0101995,"An example of an original sentence and its corresponding manual simplifications in the two corpora is given in Table 1. 3.2 Model PB-SMT HIERO PB-SMT HIERO SMT Models In order to compare the impact of different SMT models (PB vs. HIERO) on the system performance, the language model (LM) and the test set (consisting of 47 sentence pairs from each of the two corpora) were kept the same for all systems. Ideally, the LM should be trained on a large corpus of “simplified” Spanish. However, as such a corpus has not been compiled yet, we trained the LM on a subset of the Europarl v7 Spanish corpus (Koehn, 2005) using the SRILM toolkit (Stolcke, 2002). In order to reduce the complexity of the sentences used for the training of the LM, we filtered out all sentences that contain more than 15 620 System Corpus Light PB Heavy Light HIERO Heavy Baseline S-BLEU 0.3742 0.3662 0.3718 0.2959 0.3645 BLEU 0.3374 0.3313 0.3336 0.2718 0.3260 Heavy PB HIERO Mean 1.74 1.77 G Median 1 1 Mode 1 1 Mean 1.98 1.93 M Median 1 1 Mode 1 1 Mean 2.31 2.29 S Median 2 2 Mode 1 1 Aspect Table 3: Automatic evaluation sponding simplified variants) were selected randomly from the test set under the criterion that they have been mo"
R15-1080,W14-5604,1,0.736955,"produced by the two best SMT systems and the third by the Simplext system) in order to directly compare the performances of the SMT systems with the state-ofthe-art (rule-based) text simplification system for Spanish (Section 2.3). We obtained a total of 260 human scores for each aspect-system-corpora combination in each of the two evaluation phases. The 40 original sentences for human evaluation (and their corre2. Light – The TS corpus consisting of various texts (some of which present in the Heavy corpus) and their manual simplifications obtained using only six main simplifiˇ cation rules (Mitkov and Stajner, 2014). In both corpora, the sentence-alignment was manually checked and corrected where necessary. An example of an original sentence and its corresponding manual simplifications in the two corpora is given in Table 1. 3.2 Model PB-SMT HIERO PB-SMT HIERO SMT Models In order to compare the impact of different SMT models (PB vs. HIERO) on the system performance, the language model (LM) and the test set (consisting of 47 sentence pairs from each of the two corpora) were kept the same for all systems. Ideally, the LM should be trained on a large corpus of “simplified” Spanish. However, as such a corpus"
R15-1080,P07-2045,0,\N,Missing
R15-1080,W14-4404,0,\N,Missing
radev-etal-2004-mead,W02-0404,1,\N,Missing
radev-etal-2004-mead,H01-1056,1,\N,Missing
radev-etal-2004-mead,W00-1009,1,\N,Missing
radev-etal-2004-mead,P02-1040,0,\N,Missing
S07-1063,S07-1012,0,0.186137,"ering Algorithm 1 Introduction Cross-document coreference resolution is the task of identifying if two mentions of the same (or similar) name in different sources refer to the same individual. Deciding if two documents refer to the same individual is a difficult problem because names are highly ambiguous. Automatic techniques for solving this problem are required not only for better access to information but also in natural language processing applications such as multidocument summarization and information extraction. Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al., 2007): a search engine user types in a person name as a query. Instead of ranking web pages, an ideal system should organize search results in as many clusters as there are different people sharing the same name in the documents returned by the search engine. The input is, therefore, the results given by a web search engine using a person name as query. The output is a numWe have implemented an agglomerative clustering algorithm. The input to the algorithm is a set of document representations implemented as vectors of terms and weights. Initially, there are as many clusters as input documents; as t"
S07-1063,P98-1012,0,0.29798,"iven with the input document. The processing elements include:  Document tokenisation  Sentence splitting  Parts-of-speech tagging  created for each document set (or person): (i) an inverted document frequency table for words (no normalisation is applied); and (ii) an inverted frequency table for Mentions (the full entity string is used, no normalisation is applied). Statistics (term frequencies and tf*idf) are computed over tokens and Mentions using the appropriate tables (these tools are part of the summarization toolkit) and vector representations created for each document (same as in (Bagga and Baldwin, 1998)). Two types of representations were considered for these experiments: (i) full document or summary (terms in the summary are considered for vector creation); and (ii) words or Mentions. 4 System Configurations Four system configurations were prepared for SemEval:   System I: vector representations were created for full documents. Words were used as terms and local inverted document frequencies used (word frequencies) for weighting.  System II: vector representations were created for full documents. Mentions were used as terms and local inverted document frequencies used (Mentions frequenci"
S07-1063,C98-1012,0,\N,Missing
S15-2119,baccianella-etal-2010-sentiwordnet,0,0.00973133,"nt scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools • Sentiments and Emotional Lexicons • Frequency In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Random-Sub-Space with M5P as base algorithm. We also tried different mixed techniqu"
S15-2119,E14-3007,1,0.848867,"text of SemEval. These tasks have been mainly focused on the analysis of short texts like SMS or tweets. In this paper we describe the approach adopted by UPF-taln team for tasks 10 and 11 of SemEval 2015, both dealing with the analysis of English tweets. Task 10 concerned “Sentiment Analysis in Twitter” ∗ The research described in this paper is partially funded by the Spanish fellowship RYC-2009-04291, the SKATERTALN UPF project (TIN2012-38584-C06-03), and the EU project Dr. Inventor (n. 611383). We exploited an extended version of the tweet classification features and approach described in (Barbieri and Saggion, 2014). In particular, we experimented the use of intrinsic word features, characterising each word in a tweet to try to model and thus automatically determine its polarity. Thanks to intrinsic word features, we aimed to detect two aspects of tweets: the style used (e.g. register used, frequent or rare words, positive or negative words, etc.) and the unexpectedness in the use of words, particularly important for figurative language. We also exploited textual features (like word occurrences, bigrams, skipgrams or other word patterns) in order to capture the way words are used in positive and negative"
S15-2119,W14-2609,1,0.879405,"Missing"
S15-2119,R13-1011,0,0.0210444,"cs conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our systems we used in each task only the dataset provided by the organisers. For task 10 we were able to retrieve 9689 tweets, tagged as positive, negative and neutral (Rosenthal et al., 2015). For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools • Sentiments and Emotional Lexicons • Frequency In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determ"
S15-2119,ide-suderman-2004-american,0,0.0141975,"URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Random-Sub-Space with M5P as base algorithm. We also tried different mixed techniques, like using a supervised method to classify positive (0 to 5) and negative (-5 to 0), then a regression method (over the two subsets) but with no luck: pure regression methods fitted better task 11. In both tasks we characterised each tweet using nine groups of related features all describing both intrinsic aspe"
S15-2119,S14-2076,0,0.0183714,"method, and from our experiments with several classifiers, Support Vector Machines resulted to be the best one. On the other hand, in task 11 tweets were classified as belonging to one of 11 polarity classes associated with values ranging from -5 to 5, hence a regression approach was more suitable. The regression method employed was 705 • Lemma-Based • Ambiguity • Synonyms • Adjective / Adverb Intensity • Characters • Part of Speech • Bad Words 4.1 Sentiments and Emotional Lexicons Using sentiment lexicons in Sentiment Analysis has been a common and rewarding practice (Mohammad et al., 2013; Kiritchenko et al., 2014). The characterisation of the sentiment associated to words in tweets is important for two reasons: to detect the global sentiment (e.g. if tweets contain mainly positive or negative terms) and, in the case of figurative language, to capture unexpectedness created by a negative word in a positive context and viceversa. Using the two sentiment lexicons and two emotional lexicons mentioned in Section 3, we computed the number of positive / negative words, the sum of the intensities of the positive / negative scores of words, the mean of positive / negative score of words, the greatest positive /"
S15-2119,S15-2080,0,0.351108,"704 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 704–708, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our systems we used in each task only the dataset provided by the organisers. For task 10 we were able to retrieve 9689 tweets, tagged as positive, negative and neutral (Rosenthal et al., 2015). For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools • Sentiments and Emotional Lexicons • Frequency In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Has"
S15-2119,S13-2053,0,0.154825,"ols • Sentiments and Emotional Lexicons • Frequency In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Random-Sub-Space with M5P as base algorithm. We also tried different mixed techniques, like using a supervised method to classify positive (0 to"
S15-2119,S12-1033,0,0.0314942,"noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Random-Sub-Space with M5P as base algorithm. We also tried different mixed techniques, like using a supervised method to classify positive (0 to 5) and negative (-5 to 0), then a regression method (over the two subs"
S15-2119,S15-2078,0,0.0521385,"we introduce the features we built our model on. In Section 5 we discuss the performance of our model in SemEval 2015 and in Section 6 we 704 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 704–708, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics conclude with a recap of our approach and suggestions for further research. 2 Dataset In order to train our systems we used in each task only the dataset provided by the organisers. For task 10 we were able to retrieve 9689 tweets, tagged as positive, negative and neutral (Rosenthal et al., 2015). For task 11 the dataset was a collection of 8000 figurative tweets annotated with sentiment scores from -5 to +5 (Li et al., 2015). 3 Text Analysis and Tools • Sentiments and Emotional Lexicons • Frequency In order to deal with the noisy text of Twitter we made use of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1"
S15-2119,P14-2070,0,0.013418,"of the GATE application TwitIE (Bontcheva et al., 2013) where we modified the normaliser, adding new abbreviations, new slang words, removing URLs and changing the normalisation rules. Besides the tweet normalisation we also employed TwitIE for tokenisation, Part of Speech tagging and lemmatisation. We also used WordNet (Miller, 1995) to extract synonyms and synsets. We employed two sentiment lexicons, SentiWordNet3.0 (Baccianella et al., 2010) and the NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013) and two emotion lexicons NRC Hashtag Emotion Lexicon (Mohammad, 2012) and Depeche Mood (Staiano and Guerini, 2014). As frequency data for determining how often a word is used in English, we relied on the American National Corpus (Ide and Suderman, 2004); we also exploited the VU Amsterdam Metaphors Corpus (Steen et al., 2010) to find out how often a word is used in metaphors. Finally, the machine learning tool we used was Weka (Hall et al., 2009). 4 Random-Sub-Space with M5P as base algorithm. We also tried different mixed techniques, like using a supervised method to classify positive (0 to 5) and negative (-5 to 0), then a regression method (over the two subsets) but with no luck: pure regression method"
S15-2158,S15-2151,0,0.0793355,"avigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given this rationale, Task 17 (Bordea et al., 2015) in the SEMEVAL 2015 set of shared tasks focuses on Taxonomy Extraction Evaluation, i.e. the construction of a taxonomy out of a flat set of terms belonging to one of the four domains of choice (food, chemical, equipment and science). These terms have to be hierarchically organized, and new terms are allowed to be included in the taxonomy. As for evaluation, for each domain, two taxonomies were used as gold standard: One created by domain experts; and one derived from the WordNet taxonomy rooted at the domain node, e.g. food1 . Finally, evaluation is carried out from two standpoints: (1) The t"
S15-2158,P14-1113,0,0.293959,"(Muresan and Klavans, 2002), Ontology Learning (Navigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given this rationale, Task 17 (Bordea et al., 2015) in the SEMEVAL 2015 set of shared tasks focuses on Taxonomy Extraction Evaluation, i.e. the construction of a taxonomy out of a flat set of terms belonging to one of the four domains of choice (food, chemical, equipment and science). These terms have to be hierarchically organized, and new terms are allowed to be included in the taxonomy. As for evaluation, for each domain, two taxonomies were used as gold standard: One created by domain experts; and one derived from the WordNet taxonomy rooted at the domain node, e.g. food1 . Finally, eval"
S15-2158,N03-1011,0,0.0113171,"features. surface: A word’s surface form. Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Relevant: Botifarra is a type of sausage and one of the most important dishes of the Catalan cuisine. Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain sentences. 3 pus. For e"
S15-2158,C92-2082,0,0.0813053,"nd hypernyms manually annotated. We preprocess and parse the WCL dataset with a dependency parser (Bohnet, 2010), and then train our classifier with the following set of features. surface: A word’s surface form. Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Relevant: Botifarra is a type of sausage and one of the most important dishes of the Catalan cuisine. Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). I"
S15-2158,P08-1119,0,0.23104,"n train our classifier with the following set of features. surface: A word’s surface form. Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Relevant: Botifarra is a type of sausage and one of the most important dishes of the Catalan cuisine. Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract"
S15-2158,muresan-klavans-2002-method,0,0.0659289,"rying the BabelNet semantic network. Then, we combine a machine-learning algorithm for term-hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges. 1 Introduction Learning semantic relations out of flat terminologies is an appealing task due to its potential application in tasks like Question Answering (Cui et al., 2005; Boella et al., 2014), automatic glossary construction (Muresan and Klavans, 2002), Ontology Learning (Navigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given thi"
S15-2158,P10-1023,0,0.0365915,"), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain sentences. 3 pus. For example, given the term botifarra (a Catalan type of sausage), we add two definitions to our corpus: Definition corpus compilation We benefit from BabelNet, a very large multilingual semantic network that combines, among other resources, Wikipedia and WordNet (Navigli and Ponzetto, 2010). We get a set of BabelNet synsets associated to each term and for each synset, we extract its definition. In this step we assume that a term’s definition appears in the first sentence of its Wikipedia article, which is a regular practice in the literature (see (Navigli and Velardi, 2010) or (Boella et al., 2014)). This step allowed us to compile a domain corpus of definitional knowledge, and thus maximizing the number of relevant terms definitions. However, noise is also introduced in our cor950 lemma: The lemma of the word. pos: The word’s part-of-speech. head id: The id of the word to which"
S15-2158,P10-1134,0,0.319127,"alyzes the performance of our system as well as the difficulties encountered, and suggests potential avenues for future work. 2 Noisy: Botifarra is a point trick-taking card game for four players in fixed partnerships played in Catalonia. 3.2 Hypernym Extraction Given a set of definitional text fragments where the definiendum2 term is known, i.e. can be extracted from the url of the Wikipedia page, our goal is to tag the tokens of the definition that correspond to one or more hypernyms. To this end, we train a Conditional Random Fields (Lafferty et al., 2001) classifier3 with the WCL Dataset (Navigli and Velardi, 2010). We argue that CRFs are a valid approach for sequential classification, and particularly for this task, due to their potential to capture prior and posterior token features on the current iteration. The WCL dataset includes near 2000 definitional sentences with terms and hypernyms manually annotated. We preprocess and parse the WCL dataset with a dependency parser (Bohnet, 2010), and then train our classifier with the following set of features. surface: A word’s surface form. Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Relevant: Botifarra i"
S15-2158,C14-1097,0,0.0422391,"hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges. 1 Introduction Learning semantic relations out of flat terminologies is an appealing task due to its potential application in tasks like Question Answering (Cui et al., 2005; Boella et al., 2014), automatic glossary construction (Muresan and Klavans, 2002), Ontology Learning (Navigli et al., 2011) or Textual Entailment (Roller et al., 2014). Today, in the context of massive webenabled data, hypernym (is-a) relations are the focus of much research, as they constitute the backbone of ontologies (Navigli et al., 2011). However, one challenge remains open in the automatic construction of knowledge bases that exploit this type of relation. It is unfeasible to have up-to-date semantic resources for each domain, as they are limited in scope and domain, and their manual construction is knowledge intensive and time consuming (Fu et al., 2014). Given this rationale, Task 17 (Bordea et al., 2015) in the SEMEVAL 2015 set of shared tasks foc"
S15-2158,C02-1114,0,0.023502,"with the following set of features. surface: A word’s surface form. Method This section describes the main modules that constitute our taxonomy learning system. 3.1 Relevant: Botifarra is a type of sausage and one of the most important dishes of the Catalan cuisine. Background Generally, taxonomy learning from text has been carried out either following rule-based or distributional approaches. In terms of rule-based methods reported in the literature, (Hearst, 1992) introduced lexico-syntactic patterns, which were exploited in subsequent work (Berland and Charniak, 1999; Kozareva et al., 2008; Widdows and Dorow, 2002; Girju et al., 2003). Distributional approaches, on the other hand, have become increasingly popular due to the availability of large corpora. Systems aimed at extracting hypernym relations from text have exploited hybrid patterns as word-class lattices (Navigli and Velardi, 2010), syntactic relations as features for an SVM classifier (Boella et al., 2014) or word-embedding-based semantic projections (Fu et al., 2014; Roller et al., 2014). Inspired by the reported success in the latter methods, we opted for combining syntactic patterns with machine learning to extract hypernyms from domain se"
S15-2158,P99-1008,0,\N,Missing
S16-1157,E09-1005,0,0.0443146,"rporate the text readability dimension, as this list contains words which 4th grade students considered understandable. 4.2 Text Analysis Tools We analyze the sentences in which a word to evaluate occurs by means of the Mate dependency parser (Bohnet, 2010). As a result, we obtain a lemmatized and Part-Of-Speech (POS) tagged version of the sentence, along with its syntactic dependencies. Both POS tags and dependency information are used to compute several features as described in the following Section. We also processed each sentence by the UKB graph-based Word Sense Disambiguation algorithm (Agirre and Soroa, 2009). Specifically, we benefited from the UKB implementation integrated in the Freeling workbench (Padr´o and Stanilovsky, 2012). In this way, we may disambiguate single or multiword expressions against WordNet 3.0. 5 Method In order to evaluate the complexity of a word, we modelled each word as a feature vector. Then, we used such word representation to enable the training and evaluation of distinct binary classification algorithms tailored to determine whether a word is complex or not. To this end, we relied on the Weka machine learning framework (Witten and Frank, 2000). We evaluated the perfor"
S16-1157,C12-1023,1,0.956014,"pectively considered unweighted and weighted instances of complex words to train our classifier, where the weight of each instance is proportional to the number of evaluators that judged the word as complex. Our system scored as the third best performing one. 1 If the growth rate is known , the maximum lichen size will give a minimum age for when this rock was deposited. Introduction Approaches to automatically identify if a target audience will perceive a certain word as complex or not constitute a core component in several languagerelated areas of research, including Lexical Simplification (Bott et al., 2012) and Readability Assessment (Collins-Thompson, 2014). The Complex Word Identification Task of SemEval-2016 proposes a shared framework for In this sentence, the words ’lichen’ and ’deposited’ were classified as complex by at least one out of the 20 evaluators, unlike e.g. ’growth’, which did not received this label by any of them. In our participation in Task 11, we cast the identification of complex words as a binary classification problem in which each word is evaluated as complex or not, given the sentence in which it occurs. We modelled each word by a set of lexical, semantic and contextua"
S16-1157,padro-stanilovsky-2012-freeling,0,0.0470154,"Missing"
S16-1157,P13-3015,0,0.515874,"omplex words. In the context of the PSET Project (Devlin and Tait, 1998), the first lexical simplification system for English was developed, aimed at people with aphasia. It relies on a word difficulty assessment based on psycholinguistic evidence (Quinlan, 1992) in order to decide whether to simplify a word. Recent work exploited the availability of comparable corpora of original documents (e.g. English Wikipedia) and their ’simplified’ versions (e.g. Simple English Wikipedia pages) to induce measures which can be used to compare and rank ’quasi-synonymic’ word pairs (Yatskar et al., 2010). (Shardlow, 2013) compares three techniques to identify complex words in English: a psycholinguistic approach (Devlin and Tait, 1998), frequency thresholding (i.e. words with low frequency are considered complex), and a machine learning algorithm trained only on word features (frequency, syllable count, ambiguity, etc.). In this work, a corpus of complex words is created based on edit histories from the Simple Wikipedia. The authors conclude that the three tested methods perform similarly in terms of F-measure. (Saggion et al., 2016) use the combined evidence of word frequency and word length to assess the wor"
S16-1157,N10-1056,0,0.0327292,"ied so far to identify complex words. In the context of the PSET Project (Devlin and Tait, 1998), the first lexical simplification system for English was developed, aimed at people with aphasia. It relies on a word difficulty assessment based on psycholinguistic evidence (Quinlan, 1992) in order to decide whether to simplify a word. Recent work exploited the availability of comparable corpora of original documents (e.g. English Wikipedia) and their ’simplified’ versions (e.g. Simple English Wikipedia pages) to induce measures which can be used to compare and rank ’quasi-synonymic’ word pairs (Yatskar et al., 2010). (Shardlow, 2013) compares three techniques to identify complex words in English: a psycholinguistic approach (Devlin and Tait, 1998), frequency thresholding (i.e. words with low frequency are considered complex), and a machine learning algorithm trained only on word features (frequency, syllable count, ambiguity, etc.). In this work, a corpus of complex words is created based on edit histories from the Simple Wikipedia. The authors conclude that the three tested methods perform similarly in terms of F-measure. (Saggion et al., 2016) use the combined evidence of word frequency and word length"
S16-1208,N15-1169,0,0.0242942,"Missing"
S16-1208,S16-1169,0,0.0429144,"Missing"
S16-1208,D14-1088,0,0.0125912,"in the former group, we find projects like BABEL N ET (Navigli and Ponzetto, 2012), a very large multilingual semantic network, or pairwise lexical resources alignments (Miller and Gurevych, 2014; Pilehvar and Navigli, 2014). These approaches, however, are not designed to capture novel terminology due to the fact that, for an alignment to occur, there must exist a direct correspondence between lemmas. This is addressed in projects belonging to the latter group, starting from (Snow et al., 2006), and following more recently with extraction of information from the web (Velardi et al., 2013; Luu Anh et al., 2014) or from BABEL N ET’s 1333 glosses (Espinosa-Anke et al., 2016). These approaches, however, do not perform a direct mapping against WordNet, but rather separate taxonomies with their own hierarchical structure. 3 Method Our approach to finding the best match in WordNet for an OOV term is based on transforming the associated textual definition into a representative vector. However, performing this action over plain text would introduce an ambiguity problem due to the occurrence of polysemic entities and specific lexical and syntactic formulations within the definition. Therefore, we leveraged S"
S16-1208,miller-gurevych-2014-wordnet,0,0.104021,"inally, we outline directions of future work in a novel and exciting task, which opens a very interesting research problem to be tackled in the future. 2 Related Work Expanding current semantic knowledge is a task that may be approached either by expanding WordNet with knowledge derived from structured or semistructured repositories, or by learning hyponymhypernym hierarchies separately in order to obtain novel knowledge. Within the former group, we find projects like BABEL N ET (Navigli and Ponzetto, 2012), a very large multilingual semantic network, or pairwise lexical resources alignments (Miller and Gurevych, 2014; Pilehvar and Navigli, 2014). These approaches, however, are not designed to capture novel terminology due to the fact that, for an alignment to occur, there must exist a direct correspondence between lemmas. This is addressed in projects belonging to the latter group, starting from (Snow et al., 2006), and following more recently with extraction of information from the web (Velardi et al., 2013; Luu Anh et al., 2014) or from BABEL N ET’s 1333 glosses (Espinosa-Anke et al., 2016). These approaches, however, do not perform a direct mapping against WordNet, but rather separate taxonomies with t"
S16-1208,P14-1044,0,0.0643466,"Missing"
S16-1208,P06-1101,0,0.162695,"Missing"
S16-1208,J13-3007,0,0.15159,"tain novel knowledge. Within the former group, we find projects like BABEL N ET (Navigli and Ponzetto, 2012), a very large multilingual semantic network, or pairwise lexical resources alignments (Miller and Gurevych, 2014; Pilehvar and Navigli, 2014). These approaches, however, are not designed to capture novel terminology due to the fact that, for an alignment to occur, there must exist a direct correspondence between lemmas. This is addressed in projects belonging to the latter group, starting from (Snow et al., 2006), and following more recently with extraction of information from the web (Velardi et al., 2013; Luu Anh et al., 2014) or from BABEL N ET’s 1333 glosses (Espinosa-Anke et al., 2016). These approaches, however, do not perform a direct mapping against WordNet, but rather separate taxonomies with their own hierarchical structure. 3 Method Our approach to finding the best match in WordNet for an OOV term is based on transforming the associated textual definition into a representative vector. However, performing this action over plain text would introduce an ambiguity problem due to the occurrence of polysemic entities and specific lexical and syntactic formulations within the definition. Th"
S18-1003,N18-2107,1,0.891609,", the goal is to alleviate the lack of an associated grammar. This context, which makes it difficult to encode a clear and univocous single meaning for each emoji, has given rise to work considering emojis as function words or even affective markers (Na’aman et al., 2017), potentially affecting the overall semantics of longer utterances like sentences (Monti et al., 2016; Donato and Paggio, 2017). The polysemy of emoji has been explored userwise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), gender-wise, time-wise (Barbieri et al., 2018b; Chen et al., 2017), and even device-wise, due to the fact that emojis may have different pictorial characteristics (and therefore, different interpretations), depending on the device (e.g., Iphone, Android, Samsung, etc.) or app (Whatsapp, Twitter, Facebook, and so forth)3 (Tigwell and Flatla, 2016; Miller et al., 2016). An aspect related with emoji semantic modeling in which awareness is increasing dramatically is the inherent bias existing in these representations. For example, Barbieri and CamachoCollados (2018) show that emoji modifiers can affect the semantics of emojis (they looked sp"
S18-1003,E17-2017,1,0.505123,"iently rich that oversimplifying them to sentiment carriers or boosters would be to neglect the semantic richness of these ideograms, which in addition to mood ( ) include in their vocabulary references to food ( ), sports ( ), scenery ( ), etc2 . In general, however, effectively predicting the emoji associated with a piece of content may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online. Given that emojis may also mislead humans (Barbieri et al., 2017; Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding. As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). In this context, Barbieri et al. (2017) introduced the task of emoji prediction in Twitter by training several models based on bidirectional Long ShortTerm Memory networks (LSTMs) (Graves, 2012), and showing they can outperform humans in solvThis paper describes the results of the first shared task on Multil"
S18-1003,S18-2011,1,0.899882,"Missing"
S18-1003,L16-1626,1,0.837949,"elements of this shared task (Section 2 and 3). Then, we cover the dataset compilation, curation and release process (Section 4). In Section 5 we detail the evaluation metrics and describe the overall results obtained by participating systems. Finally, we wrap this task description paper up with the main conclusions drawn from the organization of this challenge, as well as outlining potential avenues for future work, in Section 6. 2 Today, modeling emoji semantics via vector representations is a well defined avenue of work. Contributions in this respect include models trained on Twitter data (Barbieri et al., 2016c), Twitter data together with the official unicode description (Eisner et al., 2016), or using text from a popular keyboard app Ai et al. (2017). In the latter contribution it is argued that emojis used in an affective context are more likely to become popular, and in general, the most important factor for an emoji to become popular is to have a clear meaning. In fact, the area of emoji vector evaluation has also experienced a significant growth as of recent. For instance, Wijeratne et al. (2017a) propose a platform for exploring emoji semantics. Further studies on evaluating emoji semantics"
S18-1003,S18-1075,0,0.0205523,"do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble ap"
S18-1003,S18-1062,0,0.0222283,"arch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine"
S18-1003,D17-1169,0,0.303851,"the emoji associated with a piece of content may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online. Given that emojis may also mislead humans (Barbieri et al., 2017; Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding. As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017). In this context, Barbieri et al. (2017) introduced the task of emoji prediction in Twitter by training several models based on bidirectional Long ShortTerm Memory networks (LSTMs) (Graves, 2012), and showing they can outperform humans in solvThis paper describes the results of the first shared task on Multilingual Emoji Prediction, organized as part of SemEval 2018. Given the text of a tweet, the task consists of predicting the most likely emoji to be used along such tweet. Two subtasks were proposed, one for English and one for Spanish, and participants were allowed to submit a system run t"
S18-1003,S18-1079,0,0.0244507,", but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competiti"
S18-1003,S18-1067,0,0.0177432,"rse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperfor"
S18-1003,S18-1072,0,0.0393464,"Missing"
S18-1003,S18-1081,0,0.0367034,"Missing"
S18-1003,S18-1004,0,0.0800282,"Missing"
S18-1003,S18-1078,0,0.0516896,"Missing"
S18-1003,S18-1070,0,0.037717,"Missing"
S18-1003,W17-5216,0,0.0205954,"d Work Modeling the semantics of emojis, and their applications thereof, is a relatively novel research problem with direct applications in any social media task. By explicitly modeling emojis as selfcontaining semantic units, the goal is to alleviate the lack of an associated grammar. This context, which makes it difficult to encode a clear and univocous single meaning for each emoji, has given rise to work considering emojis as function words or even affective markers (Na’aman et al., 2017), potentially affecting the overall semantics of longer utterances like sentences (Monti et al., 2016; Donato and Paggio, 2017). The polysemy of emoji has been explored userwise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), gender-wise, time-wise (Barbieri et al., 2018b; Chen et al., 2017), and even device-wise, due to the fact that emojis may have different pictorial characteristics (and therefore, different interpretations), depending on the device (e.g., Iphone, Android, Samsung, etc.) or app (Whatsapp, Twitter, Facebook, and so forth)3 (Tigwell and Flatla, 2016; Miller et al., 2016). An aspect related with emoji semantic modeling in wh"
S18-1003,S18-1077,0,0.021307,"018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infrequent classes are oversampled using the SMOTE algorithm. As for features, they use both unigrams and bigrams. English Emo F1 87.8 37.8 47.1 26.9 55.5 16.2 22.6 36.2 24 22.2 40 64.7 63.7 17.1 13 29.2 14.3 73.6 38.4 9 particularities of each individual language should be taken into consideration for best performance. The most precise systems were EmoNLP and T¨ubingen-Oslo, whereas the highest Recall was obtained by NTUA-SLP a"
S18-1003,E17-2068,0,0.0346041,"is was a single label classification problem, the classic precision (Prec.), recall (Recall), fscore (F1) and accuracy (Acc.) were used as official evaluation metrics. Note that because of the skewed distribution of the label set we opted for macro average over all labels. 5.2 • EmoNLP (Liu, 2018). This system is based on a Gradient Boosting Regression Tree Approach combined with a Bi-LSTM on character and word ngrams. It is complemented with several lexicons as well as learning sentiment specific word embeddings. Baseline The baseline system for this task was a classifier based on FastText6 (Joulin et al., 2017). Given a set of N documents, the loss that the model attempts to minimize is the negative log-likelihood over the labels (in our case, the emojis): loss = − • UMDuluth-CS8761 (Beaulieu and Asamoah Owusu, 2018) This supervised system combines an SVM with a bag-of-words approach for extracting salient features. This is one of the most competitive systems with the highest precision in English and the third best result in Spanish. n=1 1 X en log(softmax (BAxn )) N N where en is the emoji included in the n-th Twitter post, represented as hot vector, and used as label. Hyperparameters were set as d"
S18-1003,W16-6208,0,0.0910628,"Missing"
S18-1003,S18-1064,0,0.0208509,"default7 . 5.3 • Hatching Chick (Coster et al., 2018). This system builds an SVM classifier (with gradient descent optimization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but"
S18-1003,S18-1059,0,0.0252405,"on, and pretrained word2vec vectors. They used external resources for associating each tweet with information on emotions, concreteness, familiarity, and others. They only participated in the English subtask but they classified second (according to the F1 score) with the highest recall. Evaluation Metrics As this was a single label classification problem, the classic precision (Prec.), recall (Recall), fscore (F1) and accuracy (Acc.) were used as official evaluation metrics. Note that because of the skewed distribution of the label set we opted for macro average over all labels. 5.2 • EmoNLP (Liu, 2018). This system is based on a Gradient Boosting Regression Tree Approach combined with a Bi-LSTM on character and word ngrams. It is complemented with several lexicons as well as learning sentiment specific word embeddings. Baseline The baseline system for this task was a classifier based on FastText6 (Joulin et al., 2017). Given a set of N documents, the loss that the model attempts to minimize is the negative log-likelihood over the labels (in our case, the emojis): loss = − • UMDuluth-CS8761 (Beaulieu and Asamoah Owusu, 2018) This supervised system combines an SVM with a bag-of-words approach"
S18-1003,S18-1068,0,0.0230149,"with gradient descent optimization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This sys"
S18-1003,S18-1066,0,0.053806,"Missing"
S18-1003,S18-1073,0,0.0252344,"below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infrequent classes are oversampled usi"
S18-1003,S18-1060,0,0.0230182,"is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve Bayes, Lo27 gistic Regression, Random Forests, etc.). Infreque"
S18-1003,P17-3022,0,0.0972214,"Missing"
S18-1003,S18-1063,0,0.0211944,"om/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of features such as tf-idf, part-ofspeech tags and bigrams. The system was competitive on both languages, outperforming the baseline on the Spanish dataset. • Duluth UROP (Jin and Pedersen, 2018). This system consists of a soft voting ensemble approach combining different machine learning algorithms (Na¨ıve"
S18-1003,S18-1065,0,0.0231871,"timization) on words and character ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM clas"
S18-1003,S18-1076,0,0.0529383,"Missing"
S18-1003,S18-1071,0,0.0142065,"cter ngrams. They obtained the second best result in the Spanish subtask, but their English system performed worse than the baseline. Participant Systems Due to the overwhelming number of participants, we cannot describe all systems.8 We do, however, 6 github.com/facebookresearch/fastText https://github.com/facebookresearch/ fastText#full-documentation 8 This is the list of systems that ranked below the baseline in either of the subtasks: #TeamINF (Ribeiro and Silva, 2018), CENNLP (J R et al., 2018), DUTH (Effrosynidis et al., 2018), ECNU (Lu et al., 2018), EICA (Xie and Song, 2018), EPUTION (Zhou et al., 2018), LIS (Guibon et al., 2018), Manchester Metropolitan (Gerber and Shardlow, 2018), Peperomia (Chen et al., 2018), PickleTeam! (Groot et al., 2018), Shi (Shiyun et al., 2018), SyntNN (Zanzotto and Santilli, 2018), TAJJEB (Basile and Lino, 2018), The Dabblers (Alexa et al., 2018), THU NGN (Wu et al., 2018), Tweety (Kopev et al., 2018), UMDSub (Wang and Pedersen, 2018), YNU-HPCC (Wang et al., 2018). Note that some participants did not submit a final paper but they are included in the results table. 7 • TAJJEB (Basile and Lino, 2018). This system made use of an SVM classifier over wide variety of f"
S18-1115,S13-1005,0,0.16065,"Missing"
S18-1115,S18-1116,0,0.0792436,"Missing"
S18-1115,S18-1149,0,0.0363408,"Missing"
S18-1115,E17-2013,0,0.185703,"Missing"
S18-1115,C92-2082,0,0.323886,"systems for any individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification"
S18-1115,S15-2151,1,0.928124,"i♥ Luis Espinosa-Anke♣ Sergio Oramas♦ Tommaso Pasini♥ Enrico Santus♥ Vered Shwartz♠ Roberto Navigli♥ Horacio Saggion♦ ♣ School of Computer Science and Informatics, Cardiff University, United Kingdom ♥ Computer Science Department, Sapienza University of Rome, Italy ♦ Pompeu Fabra University, Barcelona, Spain ♥ MIT, United States ♠ Bar-Ilan University, Ramat Gan, Israel ♣ {camachocolladosj,espinosa-ankel}@cardiff.ac.uk, ♥ {dellibovi,pasini,navigli}@di.uniroma1.it, ♦ {name.surname}@upf.edu, ♥ esantus@mit.edu, ♠ vered1986@gmail.com Abstract web retrieval, website navigation or records management (Bordea et al., 2015). This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were al"
S18-1115,S16-1168,0,0.263061,"s either a concept or a 4 As an example, the term apple could either refer to a fruit (if labeled as concept) or to a company (if labeled as named entity). 5 http://ebiquity.umbc. edu/blogger/2013/05/01/ umbc-webbase-corpus-of-3b-english-words/ 6 http://dbpubs.stanford.edu:8091/ testbed/doc2/WebBase/ ˜ 3 In fact, WordNet encodes hypernym and instance as two separate semantic relations. Instances are always leaf (terminal) nodes in their hierarchies. 714 sources of information with respect to the corpora used in previous tasks, such as Wikipedia in the SemEval 2016 task on taxonomy extraction (Bordea et al., 2016). In fact, the encyclopedic nature of Wikipedia has been exploited in a wide variety of works (Ponzetto and Strube, 2007; Flati et al., 2016; Gupta et al., 2016), and differs substantially from the web-based corpus we put forward here. As source corpus for the Italian subtask (1B) we instead used the 1.3-billion-word itWac corpus7 (Baroni et al., 2009), extracted from different sources of the web within the .it domain. Finally, as source corpus for the Spanish subtask (1C) we considered the 1.8-billion-word Spanish corpus8 (Cardellino, 2016), which also contains heterogeneous documents from di"
S18-1115,S18-1150,0,0.0250373,"Missing"
S18-1115,E17-2036,1,0.898611,"Missing"
S18-1115,N15-1098,0,0.0369984,"ttps://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within do"
S18-1115,D16-1041,1,0.888795,"Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Collados, 2017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The ma"
S18-1115,S18-1151,0,0.0726421,"14 We used the open-source code available at https:// bitbucket.org/luisespinosa/taxoembed 15 https://github.com/vered1986/ UnsupervisedHypernymy 16 Following the conclusions from Shwartz et al. (2017), we set the hyper-parameters to: SLQS: median, PLMI, N = 100 and APSyn: N = 500. 13 Although only P@5 is displayed in the tables due to lack of space, the other thresholds were used in the official evaluation as well. 717 5.2 Participant Systems in general they were outperformed by supervised systems, in some cases their performance came close, especially for concepts. For instance, the ADAPT (Maldonado and Klubika, 2018) system, which is based on a simple similarity measure applied to word embeddings, achieved a very decent 8.13 MAP percentage performance on the medical dataset, using neither supervision nor external resources. Supervised systems produced a larger gap for entities, probably due, as mentioned above, to the lower diversity of possible hypernyms. Table 3 shows a summary of all participant systems, displaying their main features with respect to supervison and external resources used, if any. 5.3 Results A summary of the results is provided in tables 3 to 7, respectively describing results for Eng"
S18-1115,P14-1113,0,0.377885,"t al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathe"
S18-1115,P16-2081,1,0.844897,"Missing"
S18-1115,P10-1134,1,0.838694,"y individual subtask. Along with a specific source corpus and vocabulary, each subtask features its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni"
S18-1115,D16-1234,0,0.0290792,"rrent research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 1A: English 1B: Italian 1C: Spanish Term sorrow Nina Simone guacamole 2A: Medical pulmonary embolism 2B: Music Green Day Hypernym(s) sadness, unhappiness musicista, pianista, persona salsa para mojar, salsa, alimento"
S18-1115,D17-1022,0,0.0580762,"Missing"
S18-1115,C14-1097,0,0.0705885,"Missing"
S18-1115,S18-1146,0,0.019286,"Missing"
S18-1115,E17-2064,0,0.0114909,"stributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different and heterogeneous sources. A system operating in this setting r"
S18-1115,L16-1722,1,0.929363,". codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld ap"
S18-1115,L16-1528,1,0.817501,"t appeared too vague or general, as well as terms with mis-attributed domains. Domain-specific corpora. As source corpus for the medical domain (subtask 2A) we provided a combination of texts drawn from the MEDLINE9 (Medical Literature Analysis and Retrieval System) repository, which contains academic documents such as scientific publications and paper abstracts. This corpus contains 130 million words. As regards the music domain (subtask 2B), instead, the source corpus we compiled is a concatenation of several music-specific corpora, i.e. music biographies from Last.fm contained in ELMD 2.0 (Oramas et al., 2016), articles from the music branch of Wikipedia, and a corpus of album customer reviews from Amazon (Oramas et al., 2017). The resulting corpus reaches 100 million words in total. 4.1.2 Term Collection Vocabulary Creation With the aim of simplifying the task for participants by providing a unified hypernym search space, we built a series of vocabulary files including all the possible hypernyms on each dataset. Each vocabulary was constructed by considering all the words occurring at least N times across the source corpus of the corresponding subtask. We set N to five and three in the general-pur"
S18-1115,E14-4008,1,0.929727,"i Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose te"
S18-1115,P17-1192,0,0.0129572,"es its specific training and testing data, consisting of input terms and corresponding gold hypernym lists, obtained as described throughout Section 4. Traditionally, identifying hypernymic relations from text corpora has been addressed with two main approaches: pattern-based and distributional (Wang et al., 2017). Pattern-based (path-based) methods, which provide higher precision at the price of lower coverage, exploit the co-occurrence of a hyponym and its hypernym in a textual corpus (Hearst, 1992; Navigli and Velardi, 2010; Boella and Di Caro, 2013; Flati et al., 2016; Gupta et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al."
S18-1115,P16-1226,1,0.918452,"017). In fact, lessons learned from these studies have motivated the construction of a full-fledged benchmarking dataset for the shared task we present here, which covers multiple languages and knowledge domains. The main goal of this task is that of complementing current research in hypernymy modeling with this novel discovery setting. Hypernymy, i.e. the capability to relate generic terms or classes to their specific instances, lies at the core of human cognition. It is not surprising, therefore, that identifying hypernymic (is-a) relations has been pursued in NLP for more than two decades (Shwartz et al., 2016): indeed, successfully identifying this lexical relation substantially improves Question Answering applications (Prager et al., 2008; Yahya et al., 2013), Textual Entailment and Semantic Search systems (Hoffart et al., 2014; Roller et al., 2014; Roller and Erk, 2016). In addition, hypernymic relations are the backbone of almost every ontology, semantic network and taxonomy (Yu et al., 2015), which are in turn useful resources for downstream tasks such as 712 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 712–724 New Orleans, Louisiana, June 5–6, 201"
S18-1115,E17-1007,1,0.68931,"ions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2016) proposed to frame the problem as Hypernym Discovery, i.e. given the search space of a domain’s vocabulary, and given an input term, discover its best (list of) candidate hypernyms. This formulation addresses one of the main drawbacks of the evaluation criterion described above, and better frames the evaluated systems within downstream realworld applications (Camacho-Coll"
S18-1115,S18-1148,0,0.0202622,"Missing"
S18-1115,J17-4004,0,0.0330605,"Missing"
S18-1115,S17-1004,0,0.0128295,"ish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the subtasks. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the task can be found at https://competitions. codalab.org/competitions/17119. 1 Generally, evaluation benchmarks for modeling hypernymy have been designed such that in most cases they are reduced to binary classification (Baroni and Lenci, 2011; Snow et al., 2004; Boleda et al., 2017; Vyas and Carpuat, 2017), where a system has to decide whether a hypernymic relation holds between a given candidate pair of terms. Criticisms to this experimental setting point out that supervised systems tend to benefit from the inherent modeling of the datasets in the hypernym detection task, leading to lexical memorization phenomena (Levy et al., 2015; Santus et al., 2016a; Shwartz et al., 2017). In this respect, recent work has attempted to alleviate this issue by including a graded scale for evaluating the degree of hypernymy on a given pair (Vuli´c et al., 2017). Introduction Crucially, Espinosa-Anke et al. (2"
S18-1115,D17-1123,0,0.0255651,"Missing"
S18-1115,C14-1212,0,0.0535272,"a et al., 2016; Pavlick and Pasca, 2017). Conversely, distributional models rely on a distributional representation for each observed word, and are capable of identifying hypernymic relations between concepts even when they do not co-occur explicitly in text. Earlier work on hypernym modeling was unsupervised, and leveraged various interpretations of the distributional hypothesis.1 Most of the recent work on the subject is however supervised, and in the main based on using word embeddings as input for classification or prediction (e.g Baroni et al., 2012; Santus et al., 2014; Fu et al., 2014; Weeds et al., 2014; Espinosa-Anke et al., 2016; Sanchez Carmona and Riedel, 2017; Nguyen et al., 2017). As shown by Shwartz et al. (2016), pattern-based and distributional evidences can be effectively combined within a neural architecture. In this shared task we have actually received systems of both natures, including a combination of pattern-based and distributional cues, similar to the one mentioned above, which also proved to be highly effective (see Section 5). 3 General-Purpose Hypernym Discovery consists in discovering hypernyms in a large corpus of general-purpose textual data, gathered from different a"
S18-1115,P10-2021,0,0.026139,"aining set, separately for each subtask and measure. where Q is a sample of experiment runs, AP(·) refers to average precision, i.e. an average of the correctness of each individual obtained hypernym from the search space. Mean Reciprocal Rank (MRR). MRR rewards the position of the first correct result in a ranked list of outcomes, and is defined as: |Q| 1 X 1 MRR = |Q| ranki i=1 where ranki refers to the rank position of the first relevant outcome for the ith run. While its main field of application is Information Retrieval, it has also been used in NLP tasks such as collocation recognition (Wu et al., 2010; Rodr´ıguezFern´andez et al., 2016). In addition to the above, we also provide results according to P@k, i.e. the number of correctly retrieved hypernyms at different cut-off thresholds, specifically k ∈ {1, 3, 5, 15}.13 5.1 Baselines We compared the participating systems with both supervised and unsupervised baselines for each subtask, inspired by recent work on hypernym detection and discovery. In this section we briefly describe each of them. 5.1.1 Unsupervised Baselines Supervised Baselines We first used a na¨ıve most frequent hypernym (MFH) baseline, which simply returns, for each input"
S18-1115,S18-1147,0,0.0304331,"Missing"
S19-2120,W17-3013,0,0.0486775,"Missing"
S19-2120,D14-1181,0,0.00889265,"Missing"
S19-2120,W18-4401,0,0.114572,"Missing"
S19-2120,W17-1101,0,0.0272053,"672–677 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics tional Long Short-Term Memory Networks (biLSTM) model with an Attention layer on top. The model capture the most important semantic information in a tweet, including emojis and hashtags, to face the three sub-tasks. In Figure 1 a simplified schema of our model can be seen. In the following we explain how the model works. issue has been carried out with approaches from different perspectives such as abusive language (Waseem et al., 2017) (Chu et al., 2017), hate speech (Davidson et al., 2017) (Schmidt and Wiegand, 2017) (Fortuna and Nunes, 2018) and cyberbullying (Hee et al., 2018). It has been referred by (Kumar et al., 2018) that for identification of aggression in a more general manner, classifiers such as SVM and logistic regression can equalize the results of neural networksbased systems if the right features are selected. On the other hand, (Zhang et al., 2018) pointed out that a deep neural network model combining convolutional neural network and long short term memory network, performed better than state of the art, including SVM. Furthermore, indicated that automatically selected features performed"
S19-2120,W17-3012,0,0.0364574,"f the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 672–677 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics tional Long Short-Term Memory Networks (biLSTM) model with an Attention layer on top. The model capture the most important semantic information in a tweet, including emojis and hashtags, to face the three sub-tasks. In Figure 1 a simplified schema of our model can be seen. In the following we explain how the model works. issue has been carried out with approaches from different perspectives such as abusive language (Waseem et al., 2017) (Chu et al., 2017), hate speech (Davidson et al., 2017) (Schmidt and Wiegand, 2017) (Fortuna and Nunes, 2018) and cyberbullying (Hee et al., 2018). It has been referred by (Kumar et al., 2018) that for identification of aggression in a more general manner, classifiers such as SVM and logistic regression can equalize the results of neural networksbased systems if the right features are selected. On the other hand, (Zhang et al., 2018) pointed out that a deep neural network model combining convolutional neural network and long short term memory network, performed better than state of the art, i"
S19-2120,N19-1144,0,0.0844258,"), Spain lutfiyeseda.mut01@estudiant.upf.edu, {alex.bravo, horacio.saggion}@upf.edu Abstract target type of targeted offensive posts. The target type is supposed to be classified as individual, group or other for the rest (annotated as IND, GRP or OTH). We submitted three different runs for each sub-task. The training dataset released by the shared task organizers, consists of 14,100 English tweets with one annotation layer per task with a hierarchical annotation scheme where each annotation level is related to an independent sub-task. The methods used to collect this dataset is described in (Zampieri et al. (2019a)). Examples from the dataset with annotations at the end are given below: This paper describes a bidirectional LongShort Term Memory network for identifying offensive language in Twitter. Our system has been developed in the context of the SemEval 2019 Task 6 which comprises three different sub-tasks, namely A: Offensive Language Detection, B: Categorization of Offensive Language, C: Offensive Language Target Identification. We used a pre-trained Word Embeddings in tweet data, including information about emojis and hashtags. Our approach achieves good performance in the three subtasks. 1 Int"
S19-2120,S19-2010,0,0.0942423,"), Spain lutfiyeseda.mut01@estudiant.upf.edu, {alex.bravo, horacio.saggion}@upf.edu Abstract target type of targeted offensive posts. The target type is supposed to be classified as individual, group or other for the rest (annotated as IND, GRP or OTH). We submitted three different runs for each sub-task. The training dataset released by the shared task organizers, consists of 14,100 English tweets with one annotation layer per task with a hierarchical annotation scheme where each annotation level is related to an independent sub-task. The methods used to collect this dataset is described in (Zampieri et al. (2019a)). Examples from the dataset with annotations at the end are given below: This paper describes a bidirectional LongShort Term Memory network for identifying offensive language in Twitter. Our system has been developed in the context of the SemEval 2019 Task 6 which comprises three different sub-tasks, namely A: Offensive Language Detection, B: Categorization of Offensive Language, C: Offensive Language Target Identification. We used a pre-trained Word Embeddings in tweet data, including information about emojis and hashtags. Our approach achieves good performance in the three subtasks. 1 Int"
S19-2120,P16-2034,0,0.182475,"16), which were updated during the training. Then, a biLSTM layer gets high-level features from previous embeddings. The LSTM were introduced by Hochreiter and Schmidhuber (1997) and were explicitly designed to avoid the longterm dependency problem. LSTM systems keep relevant information of inputs by incorporating a loop enabling data to flow from one step to the following. LSTM gets a word embedding sequentially, left to right, at each time step, produces a hidden step and keeps its hidden state through Methodology and Data This paper describes a neural network based on the model proposed by Zhou et al. (2016) for relation extraction. The model consist of a bidirec673 optimizer in the Run3A). The third run (Run2A) was obtained using only the dataset provided by the organizers and using Adam as optimizer. For sub-tasks B and C, we did not use additional data for training. Instead, we weighted the classes in the training giving major relevance to unbalanced classes. For the rest of the runs, some parameters were changed in order to obtain different results. Specifically, the Run1B and Run1C the Adam optimizer was used with 50 units in the LSTM. The RMSProp optimizer was used in the Run2B and RUN2C wi"
saggion-2004-identifying,C92-2082,0,\N,Missing
saggion-2004-identifying,J02-4005,1,\N,Missing
saggion-2006-multilingual,saggion-etal-2002-developing,1,\N,Missing
saggion-2006-multilingual,W00-0408,0,\N,Missing
saggion-2006-multilingual,N04-1019,0,\N,Missing
saggion-2006-multilingual,C02-1073,1,\N,Missing
saggion-2006-multilingual,W97-0704,0,\N,Missing
saggion-2006-multilingual,N04-1015,0,\N,Missing
saggion-2006-multilingual,W04-1013,0,\N,Missing
saggion-2006-multilingual,I05-2047,0,\N,Missing
saggion-2006-multilingual,lenci-etal-2002-multilingual,0,\N,Missing
saggion-2006-multilingual,N03-4008,0,\N,Missing
saggion-2006-multilingual,N04-3001,0,\N,Missing
saggion-2014-creating,C10-2122,1,\N,Missing
saggion-2014-creating,W04-3252,0,\N,Missing
saggion-2014-creating,A97-1042,0,\N,Missing
saggion-2014-creating,W08-2008,0,\N,Missing
saggion-2014-creating,W08-1404,0,\N,Missing
saggion-2014-creating,W04-1013,0,\N,Missing
saggion-2014-creating,radev-etal-2004-mead,1,\N,Missing
saggion-2014-creating,E03-2013,1,\N,Missing
saggion-etal-2002-developing,J93-1004,0,\N,Missing
saggion-etal-2002-developing,W97-0703,0,\N,Missing
saggion-etal-2002-developing,A00-2035,0,\N,Missing
saggion-etal-2002-developing,W00-0408,0,\N,Missing
saggion-etal-2002-developing,E99-1011,0,\N,Missing
saggion-etal-2002-developing,W97-0704,0,\N,Missing
saggion-etal-2002-developing,W00-0403,1,\N,Missing
saggion-etal-2002-developing,grover-etal-2000-lt,0,\N,Missing
saggion-etal-2002-developing,J96-2004,0,\N,Missing
saggion-etal-2002-developing,I05-2047,0,\N,Missing
saggion-etal-2002-developing,W01-0100,0,\N,Missing
saggion-etal-2002-extracting,X98-1004,0,\N,Missing
saggion-etal-2002-extracting,P00-1036,0,\N,Missing
saggion-etal-2002-extracting,J95-4004,0,\N,Missing
saggion-etal-2002-extracting,W01-1017,1,\N,Missing
saggion-etal-2002-extracting,O98-4002,1,\N,Missing
saggion-etal-2010-nlp,P07-1124,0,\N,Missing
saggion-etal-2010-nlp,W10-1605,1,\N,Missing
saggion-etal-2010-nlp,saggion-funk-2010-interpreting,1,\N,Missing
saggion-etal-2010-nlp,P07-1053,0,\N,Missing
saggion-funk-2010-interpreting,I08-1040,0,\N,Missing
saggion-funk-2010-interpreting,J90-1003,0,\N,Missing
saggion-funk-2010-interpreting,P07-1124,0,\N,Missing
saggion-funk-2010-interpreting,P02-1053,0,\N,Missing
saggion-funk-2010-interpreting,I08-1020,1,\N,Missing
saggion-funk-2010-interpreting,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
saggion-funk-2010-interpreting,P07-1053,0,\N,Missing
saggion-gaizauskas-2006-language,W05-1527,1,\N,Missing
saggion-szasz-2012-concisus,eisele-chen-2010-multiun,0,\N,Missing
saggion-szasz-2012-concisus,saggion-etal-2002-developing,1,\N,Missing
saggion-szasz-2012-concisus,steinberger-etal-2006-jrc,0,\N,Missing
saggion-szasz-2012-concisus,E03-2009,0,\N,Missing
saggion-szasz-2012-concisus,W03-0419,0,\N,Missing
saggion-szasz-2012-concisus,P03-1048,1,\N,Missing
saggion-szasz-2012-concisus,2005.mtsummit-papers.11,0,\N,Missing
saggion-szasz-2012-concisus,saggion-2006-multilingual,1,\N,Missing
saggion-szasz-2012-concisus,W05-0610,0,\N,Missing
saggion-szasz-2012-concisus,C96-1079,0,\N,Missing
W00-0401,W97-0713,0,0.439339,"to de ComputaciSn, Facultad de CienciasExactas y Naturales, UBA, Argentina. Text Summarization The process of producing a summary from a source text consists of the following steps: (i) the interpretation of the text; (ii) the extraction of the relevant information which ideally includes the ""topics"" of the source; (iii) the condensation of the extracted information and construction of a summary representation; and (iv) the presentation of the summary representation to the reader in natural language. While some techniques exist for producing summaries for domain independent texts (Luhn, 1958; Marcu, 1997) it seems that domain specific texts require domain specific techniques (DeJong, 1982; Paice and Jones, 1993). In our case, we are dealing with technical articles which are the result of the complex process of scientific inquiry that starts with the. identification of a knowledge problem and eventually culminates with the discovery of an answer to it. Even if authors of technical articles write about several concepts in their articles, not all of them are topics. In order to address the issue of topic identification, content selection and presentation, we have studied alignments (manually prod"
W00-0401,W97-0703,0,0.133721,"the following criteria taken from (Rowley, 1982) (they were only suggestions to the evaluators and were not enforced): good spelling and grammar; clear indication of the topic of Experiment We compared abstrac£s produced by o u r m e t h o d with abstracts produced by Microsoft&apos;97 Summarizer and with others published with source documents (usually author abstracts). We have chosen Microsoft&apos;97 Summarizer because, even if it only produces extracts, it was the only summarizer available in order to carry out this evaluation and because it has already been used in other evaluations (Marcu, 1997; Barzilay and Elhadad, 1997). 7 I different page. It included 5 lists of keywords, a field to be completed with the quality score associated to the abstract and a field to be f i l l e d with comments about the abstract. One of the lists of keywords was the one published with the source document, the other four were randomly selected from the set of 11 remaining keyword lists, they were printed in the form in random order. One page was also available to be completed with comments about the task, in particular with the time it took to the judges to complete the evaluation. We produced three copies of each form for a total"
W00-0401,J98-3005,0,\N,Missing
W00-0401,E99-1011,0,\N,Missing
W02-0403,tablan-etal-2002-unicode,1,\N,Missing
W02-0403,X98-1004,0,\N,Missing
W02-0403,P00-1036,0,\N,Missing
W02-0403,W00-0401,1,\N,Missing
W02-0403,A97-1051,0,\N,Missing
W03-2805,W00-0408,0,0.225591,"t adequate for getting reliable results with high correlation with the human evaluators. It is this conclusion that Lin and Hovy have drawn, that contradicts findings by the IBM and NIST people for the importance of using multiple references when using BLEU in Machine Translation. The use of either multiple references or just a single reference has been proved not to affect the reliability of the results provided by BLEU (Papineni et al., 2001; Doddington, 2002), which seems not to be the case in summarization. This is not a surprise; comparisons of content-based metrics for summarization in (Donaway et al., 2000) have led the authors to the conclusion that such metrics correlate highly with human judgement when the humans do not disagree substantially. The fact that more than one reference summaries are needed because of the low agreement between human evaluators has been repeatedly indicated in automatic summarization evaluation (Mani, 2001). We attempt to test BLEU’s reliability when changing various evaluation parameters such as the source documents, the reference summaries used and even parameters unique to the evaluation of summaries, such as the compression rate of the extract. In doing so, we e"
W03-2805,W02-0406,0,0.0249811,"ted by the same people, which still needs to be put into comparative testing with BLEU before any claims for its performance are made. BLEU has been used for evaluating different types of NLP output to a small extent. In (Zajic et al., 2002), the algorithm has been used in a specific Natural Language Generation application: headline generation. The purpose of this work was to use an automated metric for evaluating a system generated headline against a human generated one, in order to draw conclusions on the parameters that affect the performance of a system and improve scoring similarity. In (Lin and Hovy, 2002) BLEU has been applied on summarization. The authors argue on the unstable and unreliable nature of manual evaluation and the low agreement among humans on the contents of a reference summary. Lin and Hovy make the case that automated metrics are necessary and test their own modified recall metric, along with BLEU itself, on single and multi-document summaries and compare the results with human judgement. Modified recall seems to reach very high correlation scores, though direct comparative experimentation is needed for drawing conclusions on its performance in relation to BLEU. The latter, ha"
W03-2805,2001.mtsummit-papers.68,0,0.159238,"ion; this is because it refers to the adequacy of the contents chosen to form the extract, rather than what constitutes an adequate way of expressing all the contents of the source document in a target language. The difference on the parameters to be taken into consideration when performing evaluation within these two NLP tasks presents a challenge for porting evaluation metrics from the one research area to the other. Given the relatively recent success in achieving high correlations with human judgement for Machine Translation evaluation, using the IBM content-based evaluation metric, BLEU (Papineni et al., 2001), we attempt to run this same metric on system generated extracts; this way we explore whether BLEU can be used reliably in this research area and if so, which testing parameters need to be taken into consideration. First, we refer briefly to BLEU and its use across different NLP areas, then we locate our experiments relatively to this related work and we describe the resources we used, the tools we developed and the parameters we set for running the experiments. The description of these experiments and the interpretation of the results follows. The paper concludes with some preliminary observ"
W03-2805,W00-0403,0,0.0925501,"Missing"
W03-2805,2001.mtsummit-eval.6,0,0.0626643,"Missing"
W03-2805,saggion-etal-2002-developing,1,0.941741,"d whether any other testing parameter could compensate for lack of multiple references, if used appropriately. 3 Evaluation Experiment In this section, we will present a description of the experiments themselves, along with the results obtained and their analysis, preceded by information on the corpus we used for our experiments and the tools we developed for setting their parameters and running them automatically. 3.1 Testing corpus We make use of part of the language resources (HKNews Corpus) developed during the 2001 Workshop on Automatic Summarization of Multiple (Multilingual) Documents (Saggion et al., 2002). The documents of each cluster are all relevant to a specific topic-query, so that they form, in fact, thematic clusters. The texts are marked up on the paragraph, sentence and word level. Annotations with linguistic information (Part of speech tags and morphological information), though marked up on the documents have not been used in our experiments at all. Three judges have assessed the sentences in each cluster and have provided a score on a scale from 0 to 10 (i.e. utility judgement), expressing how important the sentence is for the topic of the cluster (Radev et al., 2000). In our exper"
W03-2805,P02-1040,0,\N,Missing
W05-1527,J93-2004,0,\N,Missing
W09-2807,P00-1041,0,0.274714,"ition of the predicate to be generated in the structure), length features (number of words in the sentence), title features (e.g., presence of title words in sentence), content features computed as the syntactic head of noun and verb phrases, semantic features computed as the at each iteration the algorithm selects the “best” available phrase or predicate to prepend to the current fragment from a finite vocabulary (induced from the analysed corpus) based on local and contextual information. One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi ’s from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). This is the reason why we only address here the discourse structure generation problem. 3.1 Predicting Discourse Structure as Classification There are various possible ways of predicting what expression to insert at each point in the genera33 to add; to conclude; to contain; to describe; to discuss; to explain; to feature; to include; to indicate; to mention; to note; to point out; to present; to provide; to report; to say Table"
W09-2807,P03-1069,0,0.184753,"own particularly suitable for text categorization (Joachims, 1998). We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy (a comparison with Naive Bayes will be presented in Section 4). The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). The features are extracted from the analyzed abstracts with specialized programs. In particular we use positional features (position of the predicate to be generated in the structure), length features (number of words in the sentence), title features (e.g., presence of title words in sentence), content features computed as the syntactic head of noun and verb phrases, semantic features computed as the at each iteration the algorithm selects the “best” available phrase or predicate to prepend to the current fragment from a finite vocabulary (induced from the analysed corpus) based on local and"
W09-2807,W97-0703,0,0.252828,"Missing"
W09-2807,A97-1042,0,0.117111,"Missing"
W09-2807,N04-1015,0,0.0193411,"-and-paste summarization (Jing and McKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. Previous studies on text-to-text abstracting (Banko et al., 2000; Knight and Marcu, 2000) have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here. The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation (Barzilay and Lee, 2004) and canned-based sum• Second, we have proposed – based on previous summarization research – a number of features to be used for solving this problem; and • Finally, we have propose several instantiations of the algorithm to solve the problem and achieved a reasonable accuracy using the designed features; There is however much space for improvement even though the algorithm recalls some “partial structures”, many “full structures” can not be generated. We are currently investigating the use of induced rules to address the problem and will compare a rule-based approach with our classifier. Less"
W09-2807,C08-1018,0,0.0137203,"ntent of the abstracted document by explicitly marking what the author says or mentions, presents or introduces, concludes, or includes, in her paper. Montesi and Owen (2007) observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract objective. We investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended. Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on EndresNiggemeyer’s observations. The cut-and-paste 31 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31–38, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP pressions learned from a corpus a subset of which is shown in Table 1. We have collected abstracts from various databases including LISA, ERIC, and Internet & Personal Com"
W09-2807,W05-1527,1,0.713514,"entify meta-data such as title, author, source document, the text of the abstract, etc. Each sentence in the abstract was stripped from the predicate or phrase inserted by the abstractor (e.g., “Mentions that”, “Concludes with”) and a normalised version of the expression was used to annotate the sentence, in a way similar to the abstracts in Figures 1 and 2. After this each abstract and document title was tokenised, sentence splitted, part-of-speech tagged, and morphologically analyzed. A rule-based system was used to carry out partial, robust syntactic and semantic analysis of the abstracts (Gaizauskas et al., 2005) producing predicate-argument representations where predicates which are used to represent entities are created from the morphological roots of nouns or verbs in the text (unary predicates) and predicates with are used to represent binary relations are a closed set of names representing grammatical relations such as the verb logical object, or the verb logical subject or a prepositional attachment, etc. This predicate-argument structure representation was further analysed in order to extract “semantic” triples which are used in the experiments reported here. Output of this analysis is shown in"
W09-2807,A00-2024,0,0.254209,"e that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract objective. We investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended. Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on EndresNiggemeyer’s observations. The cut-and-paste 31 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31–38, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP pressions learned from a corpus a subset of which is shown in Table 1. We have collected abstracts from various databases including LISA, ERIC, and Internet & Personal Computing Abstracts, using our institutional library’s facilities and the abstracts’ providers’ keyword search facilities. Electronic copies of the abstracted documents can also be acce"
W09-2807,J02-4002,0,0.208431,"s in natural language processing, they have been shown particularly suitable for text categorization (Joachims, 1998). We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy (a comparison with Naive Bayes will be presented in Section 4). The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). The features are extracted from the analyzed abstracts with specialized programs. In particular we use positional features (position of the predicate to be generated in the structure), length features (number of words in the sentence), title features (e.g., presence of title words in sentence), content features computed as the syntactic head of noun and verb phrases, semantic features computed as the at each iteration the algorithm selects the “best” available phrase or predicate to prepend to the current fragment from a finite vocabulary (induced fro"
W09-2807,W02-2101,0,0.0430336,"Missing"
W09-2807,J02-4005,1,\N,Missing
W10-0213,P98-1012,0,0.0544264,"elp in the rating-inference task. In re1 2 http:trec.nist.gov/ http://www.nist.gov/tac/ 107 Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107–115, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics cent years, text summarisation has been used to support both manual and automatic tasks; in the SUMMAC evaluation (Mani et al., 1998), text summaries were tested in document classification and question answering tasks where summaries were considered suitable surrogates for full documents; Bagga and Baldwin (1998) studied summarisation in the context of a cross-document coreference task and found that summaries improved the performance of a clustering-based coreference mechanism; more recently Latif and McGee (2009) have proposed text summarisation as a preprocessing step for student essay assessment finding that summaries could be used instead of full essays to group “similar” quality essays. Summarisation has been studied in the field of sentiment analysis with the objective of producing opinion summaries, however, to the best of our knowlegde there has been little research on the study of document s"
W10-0213,Y09-2019,0,0.0146838,"the cosine similarity between a sentence vector (terms and weights) and a query vector (terms and weigths) and where the query is the name of the entity being reviewed (e.g. National Westminster). 5.3 Opinion-oriented Summarisation Since reviews are written by people who want to express their opinion and experience with regard to a bank, in this particular case, either generic or query-focused summaries can miss including some important information concerning their sentiments and feelings towards this particular entity. Therefore, a sentiment classification system similar to the one used in (Balahur-Dobrescu et al., 2009) is used together with the summarisation approach, in order to generate opinion-oriented summaries. First of all, the sentences containing opinions are identified, assigning each of them a polarity (positive and negative) and a numerical value corresponding to the polarity strength (the higher the negative score, the more negative the sentence and similarly, the higher the positive score, the more positive the sentence). Sentences containing a polarity value of 0 are considered neutral and are not taken into account. Once the sentences are classified into positives, negatives 111 and neutrals,"
W10-0213,P07-1124,0,0.014174,"ed from a different point of view in (Shimada and Endo, 2008). Here it is approached from the perspective of rating different details of a product under the same review. Consequently, they rename the problem as “seeing several stars” instead of only one, corresponding to the overall sentiment of the review. Also, in (Baccianella et al., 2009) the rating of different features regarding hotel reviews (cleanliness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product review’s representations, such as part-of-speech and lexicons. Other approaches (Devitt and Ahmad, 2007), (Turney, 2002) face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summarisation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combining"
W10-0213,N09-1055,0,0.0177574,"existing work with respect to the inference-rating problem; Section 3 and Section 4 will describe the corpus and the NLP tools used for all the experimental set-up. Next, the text summarisation approaches will be described in Section 5, and then Section 6 will show the experiments conducted and the results obtained together with a discussion. Finally, we will draw some conclusions and address further work in Section 7. 2 Related Work Most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at a sentence level (Wilson et al., 2005; Du and Tan, 2009), or by attempting to capture the overall sentiment of a document (McDonald et al., 2007; Hu et al., 2008). Traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. ob108 jective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical rating for a review."
W10-0213,esuli-sebastiani-2006-sentiwordnet,0,0.0179147,"of features for document representation. Features produced from the annotations are: string – the original, unmodified text of each token; root – the lemmatised, lower-case form of the token; category – the part-of-speech (POS) tag, a symbol that represents a grammatical category such as determiner, present-tense verb, past-tense verb, singular noun, etc.; orth – a code representing the token’s combination of upper- and lower-case letters. In addition to these basic features, “sentiment” features based on a lexical resource are computed as explained below. 4.1 Sentiment Features SentiWordNet (Esuli and Sebastiani, 2006) is a lexical resource in which each synset (set of synonyms) of WordNet (Fellbaum, 1998) is associated with three numerical scores obj (how objective the word is), pos (how positive the word is), and neg (how negative the word is). Each of the scores ranges from 0 to 1, and their sum equals 1. SentiWordNet word values have been semi-automatically computed based on the use of weakly supervised classification algorithms. In this work we compute the “general sentiment” of a word in the following way: given a word w we compute the number of times the word w is more positive than negative (positiv"
W10-0213,W07-1411,1,0.770821,"Missing"
W10-0213,C02-1054,0,0.0442619,"ariation between positive and negative). For example a word such as “good” has many more entries where the positive score is greater than the negativity score while a word such as “unhelpful” has more negative occurrences than positive. We use this aggregated scores in our classification experiments. Note that we do not apply any word sense disambiguation procedure here. 4.2 Machine Learning Tool For the experiments reported here, we adopt a Support Vector Machine (SVM) learning paradigm not only because it has recently been used with success in different tasks in natural language processing (Isozaki and Kazawa, 2002), but it has been shown particularly suitable for text categorization (Kumar and Gopal, 2009) where the feature space is huge, as it is in our case. We rely on the support vector machines implementation distributed with the GATE system (Li et al., 2009) which hides from the user the complexities of feature extraction and conversion from documents to the machine learning implementation. The tool has been applied with success to a number of datasets for opinion classification and rating-inference (Saggion and Funk, 2009). 5 Text Summarisation Approach In this Section, three approaches for carryi"
W10-0213,N09-3013,1,0.836854,"ing the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summarisation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (Lloret et al., 2009), or reviews (Zhuang et al., 2006), the use of summaries as substitutes of full documents in tasks such as rating-inference has been not yet explored to the best of our knowledge. In contrast to the existing literature, this paper uses summaries instead of full reviews to tackle the rating-inference task in the financial domain, and we carry out a preliminary analysis concerning the potential benefits of text summaries for this task. 3 Dataset for the Rating-inference Task Since there is no standard dataset for carrying out the rating-inference task, the corpus used for our experiments was one"
W10-0213,P07-1055,0,0.0306039,"will describe the corpus and the NLP tools used for all the experimental set-up. Next, the text summarisation approaches will be described in Section 5, and then Section 6 will show the experiments conducted and the results obtained together with a discussion. Finally, we will draw some conclusions and address further work in Section 7. 2 Related Work Most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at a sentence level (Wilson et al., 2005; Du and Tan, 2009), or by attempting to capture the overall sentiment of a document (McDonald et al., 2007; Hu et al., 2008). Traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. ob108 jective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical rating for a review. For instance, given a review and 5-star-rating scale (ranging from 1 -the worst- to 5 -"
W10-0213,P05-1015,0,0.0645997,"ts) are classified into positive vs. negative, or subjective vs. ob108 jective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical rating for a review. For instance, given a review and 5-star-rating scale (ranging from 1 -the worst- to 5 -the best), this task should correctly predict the review’s rating, based on the language and sentiment expressed in its content. In (Pang and Lee, 2005), the rating-inference problem is analysed for the movies domain. In particular, the utility of employing label and item similarity is shown by analysing the performance of three different methods based on SVM (one vs. all, regression and metric labeling), in order to infer the author’s implied numerical rating, which ranges from 1 up to 4 stars, depending on the degree the author of the review liked or not the film. The approach described in (Leung et al., 2006) suggests the use of collaborative filtering algorithms together with sentiment analysis techniques to obtain user preferences expres"
W10-0213,W06-0302,0,0.0149313,"part-of-speech and lexicons. Other approaches (Devitt and Ahmad, 2007), (Turney, 2002) face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summarisation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combining sentiment analysis and text summarisation focusing on generating opinion-oriented summaries for the new textual genres, such as blogs (Lloret et al., 2009), or reviews (Zhuang et al., 2006), the use of summaries as substitutes of full documents in tasks such as rating-inference has been not yet explored to the best of our knowledge. In contrast to the existing literature, this paper uses summaries instead of full reviews to tackle the rating-inference task in the financial domain, and we carry out a preliminary analysis concerning the potential benef"
W10-0213,P02-1053,0,0.00450696,"of view in (Shimada and Endo, 2008). Here it is approached from the perspective of rating different details of a product under the same review. Consequently, they rename the problem as “seeing several stars” instead of only one, corresponding to the overall sentiment of the review. Also, in (Baccianella et al., 2009) the rating of different features regarding hotel reviews (cleanliness, location, staff, etc.) is addressed by analysing several aspects involved in the generation of product review’s representations, such as part-of-speech and lexicons. Other approaches (Devitt and Ahmad, 2007), (Turney, 2002) face this problem by grouping documents with closer stars under the same category, i.e. positive or negative, simplifying the task into a binary classification problem. Recently, due to the vast amount of on-line information and the subjectivity appearing in documents, the combination of sentiment analysis and summarisation task in tandem can result in great benefits for stand-alone applications of sentiment analysis, as well as for the potential uses of sentiment analysis as part of other NLP applications (Stoyanov and Cardie, 2006). Whilst there is much literature combining sentiment analys"
W10-0213,H05-1044,0,0.00549763,"on 2 will compile the existing work with respect to the inference-rating problem; Section 3 and Section 4 will describe the corpus and the NLP tools used for all the experimental set-up. Next, the text summarisation approaches will be described in Section 5, and then Section 6 will show the experiments conducted and the results obtained together with a discussion. Finally, we will draw some conclusions and address further work in Section 7. 2 Related Work Most of the literature regarding sentiment analysis addresses the problem either by detecting and classifying opinions at a sentence level (Wilson et al., 2005; Du and Tan, 2009), or by attempting to capture the overall sentiment of a document (McDonald et al., 2007; Hu et al., 2008). Traditional approaches tackle the task as binary classification, where text units (e.g. words, sentences, fragments) are classified into positive vs. negative, or subjective vs. ob108 jective, according to their polarity and subjectivity degree, respectively. However, sentiment classification taking into account a finer granularity has been less considered. Rating-inference is a particular task within sentiment analysis, which aims at inferring the author’s numerical r"
W10-0213,E99-1011,0,\N,Missing
W10-0213,C98-1012,0,\N,Missing
W10-1605,P96-1009,0,0.0119849,"Missing"
W10-1605,P07-1124,0,0.387846,"O’Connell, 2005) have developed a dictionary in the area of psychotherapy Drive IL to measure referential activity. Early work on dictionaries in the area of psychology include the General Inquirer psychosociological dictionary (Stone and Hunt, 1963) which can be used in various applications; current work on lexical resources for identifying particular text variables – such as measuring strong/weak opinions, sentiments, subjective/objective language, etc. – include the SentiWordnet resource (Esuli and Sebastiani, 2006) derived from WordNet which has been used in various opinion mining works (Devitt and Ahmad, 2007); other lines of research include the derivation of word-lists (semi) automatically for opinion classification (Turney, 2002). To the best of our knowledge, little research has been carried out on natural language processing for discourse interpretation in psychology. 3 Theoretical Framework Overview O2 A1 A2 UPH GPH Liberman’s theory identifies 7 drives (i.e., a subset of Freud’s drives) which are introduced in Table 1 we may associate these drives with emotional or affective states such as: strong emotions associated with IL; ecstasy or trance with O1; sadness with O2; anger with A1; concret"
W10-1605,esuli-sebastiani-2006-sentiwordnet,0,0.0218393,"emotion words, word analysis in psychotherapy, references to self and others. For Spanish (Roussos and O’Connell, 2005) have developed a dictionary in the area of psychotherapy Drive IL to measure referential activity. Early work on dictionaries in the area of psychology include the General Inquirer psychosociological dictionary (Stone and Hunt, 1963) which can be used in various applications; current work on lexical resources for identifying particular text variables – such as measuring strong/weak opinions, sentiments, subjective/objective language, etc. – include the SentiWordnet resource (Esuli and Sebastiani, 2006) derived from WordNet which has been used in various opinion mining works (Devitt and Ahmad, 2007); other lines of research include the derivation of word-lists (semi) automatically for opinion classification (Turney, 2002). To the best of our knowledge, little research has been carried out on natural language processing for discourse interpretation in psychology. 3 Theoretical Framework Overview O2 A1 A2 UPH GPH Liberman’s theory identifies 7 drives (i.e., a subset of Freud’s drives) which are introduced in Table 1 we may associate these drives with emotional or affective states such as: stro"
W10-1605,P02-1053,0,0.0034482,"ionaries in the area of psychology include the General Inquirer psychosociological dictionary (Stone and Hunt, 1963) which can be used in various applications; current work on lexical resources for identifying particular text variables – such as measuring strong/weak opinions, sentiments, subjective/objective language, etc. – include the SentiWordnet resource (Esuli and Sebastiani, 2006) derived from WordNet which has been used in various opinion mining works (Devitt and Ahmad, 2007); other lines of research include the derivation of word-lists (semi) automatically for opinion classification (Turney, 2002). To the best of our knowledge, little research has been carried out on natural language processing for discourse interpretation in psychology. 3 Theoretical Framework Overview O2 A1 A2 UPH GPH Liberman’s theory identifies 7 drives (i.e., a subset of Freud’s drives) which are introduced in Table 1 we may associate these drives with emotional or affective states such as: strong emotions associated with IL; ecstasy or trance with O1; sadness with O2; anger with A1; concrete language with A2; warnings, suspense, and premonition with UPH ; and congratulation, adulation, and promises with GPH. In d"
W11-1603,W03-1004,0,0.921162,"anguage. The methodology consisted in the creation of a corpus of simplification at two different levels and on the use of the corpus to train a decision procedure for simplification based on linguistic features. Simplification decisions about whether to simplify a text or sentence have been studied following rule-based paradigms (Chandrasekar et al., 1996) or trainable systems (Petersen and Ostendorf, 2007) where a corpus of texts and their simplifications becomes necessary. Some resources are available for the English language such as parallel corpora created or studied in various projects (Barzilay and Elhadad, 2003; Feng et al., 2009; Petersen and Ostendorf, 2007; Quirk et al., 2004); however there is no parallel Spanish corpus available for research into text simplification. The algorithms to be presented here will be used to create such resource. 3 Related Work The problem of sentence alignment was first tackled in the context of statistical machine translation. Gale and Church (1993) proposed a dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly corresponds to the length of the original sentences and the s"
W11-1603,C96-2183,0,0.52507,"in vocabulary and form, aims at reducing the efforts and costs associated with human simplification. In addition to transforming texts into their simplification for human consumption, text simplification has other advantages since simpler texts can be processed more efficiently by different natural language processing processors such as parsers and used in applications such as machine translation, information extraction, question answering, and text summarization. Early attempts to text simplification were based on rule-based methods where rules were designed following linguistic intuitions (Chandrasekar et al., 1996). Steps in the process included linguistic text analysis (including parsing) and pattern matching and transformation steps. Other computational models of text simplification included processes of analysis, transformation, and phrase re-generation (Siddharthan, 2002) also using rule-based techniques. In the PSET project (Carroll et al., 1998) the proposal is for a news simplification system for aphasic readers and particular attention is paid to linguistic phenomena such as passive constructions and coreference which are difficult to deal with by people with disabilities. The PorSimples project"
W11-1603,E09-1027,0,0.0124281,"sisted in the creation of a corpus of simplification at two different levels and on the use of the corpus to train a decision procedure for simplification based on linguistic features. Simplification decisions about whether to simplify a text or sentence have been studied following rule-based paradigms (Chandrasekar et al., 1996) or trainable systems (Petersen and Ostendorf, 2007) where a corpus of texts and their simplifications becomes necessary. Some resources are available for the English language such as parallel corpora created or studied in various projects (Barzilay and Elhadad, 2003; Feng et al., 2009; Petersen and Ostendorf, 2007; Quirk et al., 2004); however there is no parallel Spanish corpus available for research into text simplification. The algorithms to be presented here will be used to create such resource. 3 Related Work The problem of sentence alignment was first tackled in the context of statistical machine translation. Gale and Church (1993) proposed a dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly corresponds to the length of the original sentences and the sequence of sentence"
W11-1603,J93-1004,0,0.504437,"Missing"
W11-1603,J02-4006,0,0.130468,"Missing"
W11-1603,E06-1021,0,0.579249,"e of the angle between the two vectors given in the following formula: 1 The relevant unit for the calculation of IDF (the D in IDF) here is the sentence, not the document as in information retrieval. Pn wi,s1 ∗ wi,s2 pPn 2 2 i=1 (wi,s1 ) ∗ i=1 (wi,s2 ) cosine(s1 , s2 ) = pPn i=1 Here s1 and s2 are the sentence vectors and wi,sk is the weight of term i in sentence sk . We align all simplified sentences (i.e. for the time being no cutoff has been used to identify new material in the simplified text). For the calculation of the first baseline we calculate IDF over the sentences in whole corpus. Nelken and Shieber (2006) argue that that the relevant unit for this calculation should be each document for the following reason: Some words are much more frequent in some texts than they are in others. For example the word unicorn is relatively infrequent in English and it it may also be infrequent in a given colletion of texts. So this word is highly discriminative and it’s IDF will be relatively high. In a specific text about imagenary creatures, however, the same word unicorn may be much more frequent and hence it’s discrimiative power is much lower. For this reason we calcuated a second baseline, where we calcul"
W11-1603,W04-3219,0,0.111297,"Missing"
W11-1603,C10-1152,0,0.241228,"Missing"
W11-4502,C96-1079,0,0.0650686,"es the paper. 2. Related Work Information extraction is the mapping of natural language texts (e.g. news articles, web pages, e-mails) into predefined structured representations or templates [Grishman 1997]. Information extraction is a complex task carried out by human analysts on a daily basis. Because it is very time-consuming and labour-intensive, there has been much research over the last 20 years to automate the process. The field of information extraction has been fuelled by two major US international evaluations efforts. From 1987 until 1997 the Message Understanding Conferences (MUC) [Grishman and Sundheim 1996, Cowie and Lehnert 1996] concentrated on template-based information extraction. After MUC, the interest was changed to content extraction in the ACE evaluations [ACE 2004] where semantics more than linguistic analysis was the focus. There was also interest on systems able to easily adapt to new languages and tasks. In recent years there has been an increasing interest in multilingual as well as cross-lingual information extraction with a number of events organized on the subject [Poibeau and Saggion 2007, Poibeau et al. 2008]. Using rule-based information extraction in three different languag"
W11-4502,J02-4007,0,0.0103367,"tity recognition is also relevant. [Steinberger et al. 2007] use extensive resources and rule-based systems developed through bootstrapping processes to identify and match names in over 8 languages. Related to the work presented here is also research related to the creation of corpora of summaries for natural language processing applications. We have identified the SummBank corpus [Saggion et al. 2002] created for the study of multi-lingual summarization in Chinese and English which is suitable for cross-lingual summarization but not for information extraction tasks. The SumTime-Meteo Corpus [Reiter and Sripada 2002] provides weather summaries in English from numerical data and are potentially useful in data to text generation applicationsn and might be suitable for summary-to-template applications. 3. Data Set Creation and Annotation In its current state, the dataset we work with is a corpus of equivalent summaries in Spanish and English in three different domains: aviation accidents, rail accidents, and earthquakes. Further domains will be incorporated in the future for researchers interested in evaluating the robustness and adaptation capabilities of different natural language processing techniques. I"
W11-4502,saggion-etal-2002-developing,1,0.752657,"07] use an information extraction system in English as a filtering step to improve retrieval of Chinese documents. As a key technology for information extraction is named entity recogni12 tion, multilingual named entity recognition is also relevant. [Steinberger et al. 2007] use extensive resources and rule-based systems developed through bootstrapping processes to identify and match names in over 8 languages. Related to the work presented here is also research related to the creation of corpora of summaries for natural language processing applications. We have identified the SummBank corpus [Saggion et al. 2002] created for the study of multi-lingual summarization in Chinese and English which is suitable for cross-lingual summarization but not for information extraction tasks. The SumTime-Meteo Corpus [Reiter and Sripada 2002] provides weather summaries in English from numerical data and are potentially useful in data to text generation applicationsn and might be suitable for summary-to-template applications. 3. Data Set Creation and Annotation In its current state, the dataset we work with is a corpus of equivalent summaries in Spanish and English in three different domains: aviation accidents, rai"
W11-4502,W02-2024,0,0.107602,"Missing"
W12-2202,P11-2087,0,0.0362472,"nually simplified texts in Portuguese, using lists of simple words and discourse markers as resources. Bautista et al. (2011) focused on numerical expressions as one particular problem of lexical simplification and suggested the use of hedges as a means of dealing with complex numerical content. Given the fact that many words tend to be polysemic, attempts have been made to address this issue so as to provide more accurate, context-aware lexical substitution. De Belder et al. (2010) were the first to employ word sense disambiguation techniques in order to capture contextual information, while Biran et al. (2011) apply an unsupervised method for learning pairs of complex and simple synonyms based on an unaligned corpus of texts from the original Wikipedia and Simple English Wikipedia. 3 Experimental Setting We have gathered a corpus consisting of 200 informative texts in Spanish, obtained from the news agency Servimedia. The articles have been classified into four categories: national news, international news, society and culture. We then obtained simplified versions of the said texts, courtesy of the DILES (Discurso y Lengua Espa˜nola) group of the Autonomous University of Madrid. Simplifications hav"
W12-2202,W11-1603,1,0.831512,"ES (Discurso y Lengua Espa˜nola) group of the Autonomous University of Madrid. Simplifications have been applied manually, by trained human editors, following easy-to-read guidelines suggested by Anula (2009), (2008). We are interested to see how these guidelines are applied in practice, as well as how human editors naturally deal with cases not treated by the guidelines in sufficient detail. The corpus has been automatically annotated using part-of-speech tagging, named entity recognition and parsing (Padr´o et al., 2010). Furthermore, a text aligning algorithm based on Hidden Markov Models (Bott and Saggion, 2011) has been applied to obtain sentence-level alignments. The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al., 2002). A total of 570 sentences have been aligned (246 in original and 324 in simple texts), with the following correlations between them: one to one, one to many or many to one, as well as cases where there is no correlation (cases of content reduction through summarisation or information expansion through the introduction of definitions). The alignments facilitate the observation of the corpus, particu"
W12-2202,bott-etal-2012-text,1,0.82586,"Missing"
W12-2202,W12-2910,1,0.845631,"Missing"
W12-2202,N07-4002,0,0.0873953,"dealt with simplification of news articles in English for aphasic readers. Together with syntactic analysis and transformations similar to those of Chandrasekar et al. (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency 9 from the Oxford Psycholinguistic Database (Quinlan, 1992). Therefore, the most frequent of a set of synonyms for every content word of the input text was chosen to appear in its simplified version. The above approach to lexical simplification has been repeated in a number of works (Lal and Ruger, 2002; Burstein et al., 2007). Bautista et al. (2009) also rely on a dictionary of synonyms, but their criterion for choosing the most appropriate one is wordlength rather than frequency. Caseli et al. (2009) analyse lexical operations on a parallel corpus of original and manually simplified texts in Portuguese, using lists of simple words and discourse markers as resources. Bautista et al. (2011) focused on numerical expressions as one particular problem of lexical simplification and suggested the use of hedges as a means of dealing with complex numerical content. Given the fact that many words tend to be polysemic, atte"
W12-2202,C96-2183,0,0.815315,"two different aims. One is to offer simplified versions of original text to human readers, such as foreign language learners (Petersen and Ostendorf, 2007; Medero and Ostendorf, 2011); aphasic people (Devlin and Unthank, 2006); low literacy individuals (Specia, 2010) and others. On the other hand, simplified text is seen as input for further natural language processing to enhance its proficiency, e.g. in machine translation or information retrieval tools (Klebanov et al., 2004). The earliest simplification systems employed a rule-based approach and focused on syntactic structure of the text (Chandrasekar et al., 1996). The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers. Together with syntactic analysis and transformations similar to those of Chandrasekar et al. (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency 9 from the Oxford Psycholinguistic Database (Quinlan, 1992). Therefore, the most frequent of a set of synonyms for every content word of the input text was chosen to appear in its simplified version. The above approach to lexical simplification has been repeated in"
W12-2202,padro-etal-2010-freeling,0,0.0414438,"Missing"
W12-2204,bott-etal-2012-text,1,0.867013,"Missing"
W12-2204,W03-1602,0,0.0294058,"ive data from questionnaires and quantitative data from tests carried out using eye tracking. The findings suggest that graphical schemes may help to improve readability for dyslexics but are, unexpectedly, counterproductive for understandability. 1 Introduction Readability refers to the legibility of a text, that is, the ease with which text can be read. On the other hand, understandability refers to comprehensibility, the ease with which text can be understood. Since readability strongly affects text comprehension (Barzilay et al., 2002), sometimes both terms have been used interchangeably (Inui et al., 2003). However, previous research with dyslexic people have shown that both concepts need to be taken into consideration separately. For instance, while in dyslexic population reading, comprehension has been found to be independent of the spelling errors of the text; lexical quality can be used as an indicator of understandability for the non-dyslexic population (Rello and Baeza-Yates, 2012). Dyslexia has been defined both as a specific reading disability (Vellutino et al., 2004) and as a learning disability (International Dyslexia Association, 2011). It is neurological in origin and it is characte"
W12-2204,W03-2317,0,0.163218,"Missing"
W12-2910,W09-1210,0,0.020733,"ing a suitable definition for the target word. 4 Syntactic Simplification We are developing a text simplification system which will integrate different simplification modules, such as syntactic simplification, lexical simplification (Drndarevic and Saggion, 2012) and content reduction. At the moment the most advanced module of this system is the one for syntactic simplification. In (Bott et al., 2012) we describe the functioning of the simplification grammar in more detail. For the representation of syntactic structures we use dependency trees. The trees are produced by the Mate-tools parser (Bohnet, 2009) and the syntactic simplification rules are developed within the MATE framework (Bohnet et al., 2000). MATE is a graph transducer which uses hand written grammars. For grammar development we used a development corpus of 282 sentences. The grammar mainly focuses on syntactic simplification and, in particular, sentence splitting. The types of sentence splitting operations we treat at the moment are the following ones: • Relative clauses: we distinguish between simple relative clauses which are only introduced by a bare relative pronoun (e.g. a question which is hard to answer) and complex relati"
W12-2910,bott-etal-2012-text,1,0.742833,"Missing"
W12-2910,C96-2183,0,0.188686,"the grammar from manipulating wrong target structures. Section 2 describes related work, in the context of which our research has been carried out. Section 3 justifies the hybrid approach we have taken and section 4 describes our syntactic simplification module, including an evaluation of the grammar and the statistical component. Finally, in section 5 we show how our simplification system is integrated in a larger architecture of applications and services. 2 Related Work As it has happened with other NLP tasks, the first attempts to tackle the problem of text simplification were rule-based (Chandrasekar et al., 1996; Siddharthan, 2002). In the last decade the focus has been gradually shifting to more data driven approaches (Petersen and Ostendorf, 2007) and hybrid solutions. The PorSimples (Aluísio et al., 2008; Gasperin et al., 2010) project used a methodology where a parallel corpus was created and this corpus was used to train a decision process for simplification based on linguistic features. Siddharthan (2011) compares a rule-based simplification system with a simplification system based on a general purpose generator. Some approaches have concentrated on specific constructions which are especially"
W12-2910,W11-1601,0,0.019756,"as a help for other linguistic tasks such as the simplification of patent texts (Mille and Wanner, 2008; Bouayad-Agha et al., 2009). Recently the availability of larger parallel or quasiparallel corpora, most notably the combination of the English and the Simple English Wikipedia, has opened up new possibilities for the use of more purely data-driven approaches. Zhu et al. (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. A recent work, which is interesting because of its purely data-driven setup, is Coster and Kauchak (2011). They use standard software from the field of statistical machine translation (SMT) and apply these to the problem of text simplification. They complement these with a deletion component which was created for the task. They concentrate on four text simplification operations: deletion, rewording (lexical simplification), reordering and insertions. Text simplification is explicitly treated in a similar way to sentence compression. They use standard SMT software, Moses (Koehn et al., 2007) and GIZA++ (Och and Ney, 2000), and define the problem as translating from English (represented by the Engl"
W12-2910,W12-2202,1,0.791279,"riven methods. Lexical simplification requires the measurement of lexical similarity, combined with word sense disambiguation. Content reduction is very similar to extractive summarization or sentence compression and the insertion of clarifications can be broken down into three learnable steps: identification of difficult words, finding an insertion site and choosing a suitable definition for the target word. 4 Syntactic Simplification We are developing a text simplification system which will integrate different simplification modules, such as syntactic simplification, lexical simplification (Drndarevic and Saggion, 2012) and content reduction. At the moment the most advanced module of this system is the one for syntactic simplification. In (Bott et al., 2012) we describe the functioning of the simplification grammar in more detail. For the representation of syntactic structures we use dependency trees. The trees are produced by the Mate-tools parser (Bohnet, 2009) and the syntactic simplification rules are developed within the MATE framework (Bohnet et al., 2000). MATE is a graph transducer which uses hand written grammars. For grammar development we used a development corpus of 282 sentences. The grammar mai"
W12-2910,P07-2045,0,0.00441376,") with this data set. A recent work, which is interesting because of its purely data-driven setup, is Coster and Kauchak (2011). They use standard software from the field of statistical machine translation (SMT) and apply these to the problem of text simplification. They complement these with a deletion component which was created for the task. They concentrate on four text simplification operations: deletion, rewording (lexical simplification), reordering and insertions. Text simplification is explicitly treated in a similar way to sentence compression. They use standard SMT software, Moses (Koehn et al., 2007) and GIZA++ (Och and Ney, 2000), and define the problem as translating from English (represented by the English Wikipedia) to Simple English (represented by the Simple English Wikipedia). The translation process can then imply any of the four mentioned operations. They compared their approach to various other systems, including a dedicated sentence compression system (Knight and Marcu, 2002) and show that their system outperforms the others when evaluated on automatic metrics which use human created reference text, including BLEU (Papineni et al., 2002). Their problem setting does, however, no"
W12-2910,Y03-1024,0,0.0572038,"re on finite state approximations implementing all features in regular grammars using the GATE JAPE language (Cunningham et al., 2000; Maynard et al., 2002). For other learning tasks such as deciding for the splitting of coordinations or the separation of participle clauses we design and implement specific features based on intuitions; contextual features remain the same for all problems. The classification framework is implemented in the GATE system, using the machine learning libraries it provides (Li et al., 2005). In particular, we have used the Support Vector Machines learning libraries (Li and Shawe-Taylor, 2003) which have given acceptable classification results in other NLP tasks. The framework allows us to run crossvalidation experiments as well as training and testing. Table 2 shows the performance of the statistical filter in isolation, i.e. the capacity of the filter alone to distinguish between good and bad target structures for simplification operations. The in-domain performance was obtained by a ten-fold cross classification of the training data. The out-of-domain evaluation was carried out over news texts from our own corpus, the same collection we used for the Este miércoles las personas c"
W12-2910,W05-0610,0,0.0135412,"ld be implemented relying on syntactic analysis we have relied for the experiments reported here on finite state approximations implementing all features in regular grammars using the GATE JAPE language (Cunningham et al., 2000; Maynard et al., 2002). For other learning tasks such as deciding for the splitting of coordinations or the separation of participle clauses we design and implement specific features based on intuitions; contextual features remain the same for all problems. The classification framework is implemented in the GATE system, using the machine learning libraries it provides (Li et al., 2005). In particular, we have used the Support Vector Machines learning libraries (Li and Shawe-Taylor, 2003) which have given acceptable classification results in other NLP tasks. The framework allows us to run crossvalidation experiments as well as training and testing. Table 2 shows the performance of the statistical filter in isolation, i.e. the capacity of the filter alone to distinguish between good and bad target structures for simplification operations. The in-domain performance was obtained by a ten-fold cross classification of the training data. The out-of-domain evaluation was carried ou"
W12-2910,J10-3003,0,0.0135878,"use quasisynchronous grammars as a more sophisticated formalism and integer programming to learn to translate from English to Simple English. This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al. (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. Text simplification can also be seen as a type of paraphrasing problem. There are various data-driven approaches to this NLP-task (Madnani and Dorr, 2010), but they usually focus on lexical paraphrases and do not address the problem of sentence splitting, either. Such data-driven methods are very attractive, especially because they are in principle language independent, but they do depend on a large amount of data, which are not available for the majority of languages. 77 3 A Hybrid Approach to Text Simplification There are several considerations which lead us to take a hybrid approach to text simplification. First of all there is a lack of parallel data in the case of Spanish. Within our project we are preparing a corpus of Spanish news texts"
W12-2910,mille-wanner-2008-making,0,0.0174667,"l., 2010) project used a methodology where a parallel corpus was created and this corpus was used to train a decision process for simplification based on linguistic features. Siddharthan (2011) compares a rule-based simplification system with a simplification system based on a general purpose generator. Some approaches have concentrated on specific constructions which are especially hard to understand for readers with disabilities (Carroll et al., 1998; Canning et al., 2000), others focused on text simplification as a help for other linguistic tasks such as the simplification of patent texts (Mille and Wanner, 2008; Bouayad-Agha et al., 2009). Recently the availability of larger parallel or quasiparallel corpora, most notably the combination of the English and the Simple English Wikipedia, has opened up new possibilities for the use of more purely data-driven approaches. Zhu et al. (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. A recent work, which is interesting because of its purely data-driven setup, is Coster and Kauchak (2011). They use standard software from the field of statistical machine translation"
W12-2910,P00-1056,0,0.0212975,"ork, which is interesting because of its purely data-driven setup, is Coster and Kauchak (2011). They use standard software from the field of statistical machine translation (SMT) and apply these to the problem of text simplification. They complement these with a deletion component which was created for the task. They concentrate on four text simplification operations: deletion, rewording (lexical simplification), reordering and insertions. Text simplification is explicitly treated in a similar way to sentence compression. They use standard SMT software, Moses (Koehn et al., 2007) and GIZA++ (Och and Ney, 2000), and define the problem as translating from English (represented by the English Wikipedia) to Simple English (represented by the Simple English Wikipedia). The translation process can then imply any of the four mentioned operations. They compared their approach to various other systems, including a dedicated sentence compression system (Knight and Marcu, 2002) and show that their system outperforms the others when evaluated on automatic metrics which use human created reference text, including BLEU (Papineni et al., 2002). Their problem setting does, however, not include sentence splitting (a"
W12-2910,P02-1040,0,0.0918556,"ion. They use standard SMT software, Moses (Koehn et al., 2007) and GIZA++ (Och and Ney, 2000), and define the problem as translating from English (represented by the English Wikipedia) to Simple English (represented by the Simple English Wikipedia). The translation process can then imply any of the four mentioned operations. They compared their approach to various other systems, including a dedicated sentence compression system (Knight and Marcu, 2002) and show that their system outperforms the others when evaluated on automatic metrics which use human created reference text, including BLEU (Papineni et al., 2002). Their problem setting does, however, not include sentence splitting (as we will describe below). Another potential problem is that the metrics they use for evaluation compare to human references, but they do not necessarily reflect human acceptability or grammaticality. Woodsend and Lapata (2011) use quasisynchronous grammars as a more sophisticated formalism and integer programming to learn to translate from English to Simple English. This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu e"
W12-2910,W11-2802,0,0.0611209,"arger architecture of applications and services. 2 Related Work As it has happened with other NLP tasks, the first attempts to tackle the problem of text simplification were rule-based (Chandrasekar et al., 1996; Siddharthan, 2002). In the last decade the focus has been gradually shifting to more data driven approaches (Petersen and Ostendorf, 2007) and hybrid solutions. The PorSimples (Aluísio et al., 2008; Gasperin et al., 2010) project used a methodology where a parallel corpus was created and this corpus was used to train a decision process for simplification based on linguistic features. Siddharthan (2011) compares a rule-based simplification system with a simplification system based on a general purpose generator. Some approaches have concentrated on specific constructions which are especially hard to understand for readers with disabilities (Carroll et al., 1998; Canning et al., 2000), others focused on text simplification as a help for other linguistic tasks such as the simplification of patent texts (Mille and Wanner, 2008; Bouayad-Agha et al., 2009). Recently the availability of larger parallel or quasiparallel corpora, most notably the combination of the English and the Simple English Wik"
W12-2910,D11-1038,0,0.0357329,"f the four mentioned operations. They compared their approach to various other systems, including a dedicated sentence compression system (Knight and Marcu, 2002) and show that their system outperforms the others when evaluated on automatic metrics which use human created reference text, including BLEU (Papineni et al., 2002). Their problem setting does, however, not include sentence splitting (as we will describe below). Another potential problem is that the metrics they use for evaluation compare to human references, but they do not necessarily reflect human acceptability or grammaticality. Woodsend and Lapata (2011) use quasisynchronous grammars as a more sophisticated formalism and integer programming to learn to translate from English to Simple English. This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al. (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. Text simplification can also be seen as a type of paraphrasing problem. There are various data-driven approaches to this NLP-task"
W12-2910,C10-1152,0,0.488316,"l purpose generator. Some approaches have concentrated on specific constructions which are especially hard to understand for readers with disabilities (Carroll et al., 1998; Canning et al., 2000), others focused on text simplification as a help for other linguistic tasks such as the simplification of patent texts (Mille and Wanner, 2008; Bouayad-Agha et al., 2009). Recently the availability of larger parallel or quasiparallel corpora, most notably the combination of the English and the Simple English Wikipedia, has opened up new possibilities for the use of more purely data-driven approaches. Zhu et al. (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. A recent work, which is interesting because of its purely data-driven setup, is Coster and Kauchak (2011). They use standard software from the field of statistical machine translation (SMT) and apply these to the problem of text simplification. They complement these with a deletion component which was created for the task. They concentrate on four text simplification operations: deletion, rewording (lexical simplification), reordering and insertions. Text si"
W12-2910,W00-1436,0,\N,Missing
W12-3003,N03-1003,0,0.495553,"ated with chunks For example, in (Li et al., 2010) clustering is applied to generate templates for specific entity types (actors, companies, etc.) and patterns are automatically produced that describe the information in the templates. In (Chambers and Jurafsky, 2009) narrative schemas are induced from corpora using coreference relations between participants in texts. Transformation-based learning is used in (Saggion, 2011) to induce templates and rules for non-extractive summary generation. Paraphrase templates containing concepts and typical strings were induced from comparable sentences in (Barzilay and Lee, 2003) using multisentence alignment to discover “variable” and fixed 13 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 13–18, c NAACL-HLT, Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics structures. Linguistic patterns were applied to huge amounts of non-annotated pre-classified texts in (Riloff, 1996) to bootstrap information extraction patterns. Similarly, semi-supervised or unsupervised methods have been used to learn question/answering patterns (Ravichandran and Hovy, 2002) or text schem"
W12-3003,P07-1073,0,0.0155009,"ng multisentence alignment to discover “variable” and fixed 13 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 13–18, c NAACL-HLT, Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics structures. Linguistic patterns were applied to huge amounts of non-annotated pre-classified texts in (Riloff, 1996) to bootstrap information extraction patterns. Similarly, semi-supervised or unsupervised methods have been used to learn question/answering patterns (Ravichandran and Hovy, 2002) or text schemas (Bunescu and Mooney, 2007). One current paradigm to learn from raw data is open information extraction (Downey et al., 2004; Banko, 2009), which without any prior knowledge aims at discovering all possible relations between pairs of entities occurring in text. Our work tries to learn the main concepts making up the template structure in domain summaries, similar to (Chambers and Jurafsky, 2011). However, we do not rely on any source of external knowledge (i.e. WordNet) to do so. This paper presents an iterative-learning algorithm which is able to identify the key components of event summaries. We will show that the alg"
W12-3003,P09-1068,0,0.0371525,"01 August 24: Air Transat Flight 236 runs out of fuel over the Atlantic Ocean and makes an emergency landing in the Azores. Upon landing some of the tires blow out, causing a fire that is extinguished by emergency personnel on the ground. None of the 304 people on board the Airbus A330-200 were seriously injured. Figure 1: Summary in the aviation domain annotated with chunks For example, in (Li et al., 2010) clustering is applied to generate templates for specific entity types (actors, companies, etc.) and patterns are automatically produced that describe the information in the templates. In (Chambers and Jurafsky, 2009) narrative schemas are induced from corpora using coreference relations between participants in texts. Transformation-based learning is used in (Saggion, 2011) to induce templates and rules for non-extractive summary generation. Paraphrase templates containing concepts and typical strings were induced from comparable sentences in (Barzilay and Lee, 2003) using multisentence alignment to discover “variable” and fixed 13 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 13–18, c NAACL-HLT, Montr´eal, Canada, June 7-8, 2012."
W12-3003,P11-1098,0,0.309578,"-classified texts in (Riloff, 1996) to bootstrap information extraction patterns. Similarly, semi-supervised or unsupervised methods have been used to learn question/answering patterns (Ravichandran and Hovy, 2002) or text schemas (Bunescu and Mooney, 2007). One current paradigm to learn from raw data is open information extraction (Downey et al., 2004; Banko, 2009), which without any prior knowledge aims at discovering all possible relations between pairs of entities occurring in text. Our work tries to learn the main concepts making up the template structure in domain summaries, similar to (Chambers and Jurafsky, 2011). However, we do not rely on any source of external knowledge (i.e. WordNet) to do so. This paper presents an iterative-learning algorithm which is able to identify the key components of event summaries. We will show that the algorithm can induce template-like representations in various domains and languages. The rest of the paper is organized in the following way: In Section 2 we introduce the dataset we are using for our experiments and describe how we have prepated it for experimentation. Then, in Section 3 we provide an overview of our concept induction learnig algorithm while in Section 4"
W12-3003,P10-1066,0,0.134436,"to new domains and languages because of the efforts needed for modelling the underlying event template structure. In this paper we propose a method for learning the main concepts in domain summaries in 2001 August 24: Air Transat Flight 236 runs out of fuel over the Atlantic Ocean and makes an emergency landing in the Azores. Upon landing some of the tires blow out, causing a fire that is extinguished by emergency personnel on the ground. None of the 304 people on board the Airbus A330-200 were seriously injured. Figure 1: Summary in the aviation domain annotated with chunks For example, in (Li et al., 2010) clustering is applied to generate templates for specific entity types (actors, companies, etc.) and patterns are automatically produced that describe the information in the templates. In (Chambers and Jurafsky, 2009) narrative schemas are induced from corpora using coreference relations between participants in texts. Transformation-based learning is used in (Saggion, 2011) to induce templates and rules for non-extractive summary generation. Paraphrase templates containing concepts and typical strings were induced from comparable sentences in (Barzilay and Lee, 2003) using multisentence alignm"
W12-3003,P02-1006,0,0.0152221,"able sentences in (Barzilay and Lee, 2003) using multisentence alignment to discover “variable” and fixed 13 Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 13–18, c NAACL-HLT, Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics structures. Linguistic patterns were applied to huge amounts of non-annotated pre-classified texts in (Riloff, 1996) to bootstrap information extraction patterns. Similarly, semi-supervised or unsupervised methods have been used to learn question/answering patterns (Ravichandran and Hovy, 2002) or text schemas (Bunescu and Mooney, 2007). One current paradigm to learn from raw data is open information extraction (Downey et al., 2004; Banko, 2009), which without any prior knowledge aims at discovering all possible relations between pairs of entities occurring in text. Our work tries to learn the main concepts making up the template structure in domain summaries, similar to (Chambers and Jurafsky, 2011). However, we do not rely on any source of external knowledge (i.e. WordNet) to do so. This paper presents an iterative-learning algorithm which is able to identify the key components of"
W12-3003,W11-4502,1,0.367537,"Missing"
W12-3003,saggion-szasz-2012-concisus,1,0.335368,"oduce the dataset we are using for our experiments and describe how we have prepated it for experimentation. Then, in Section 3 we provide an overview of our concept induction learnig algorithm while in Section 4 we explain how we have instantiated the algorithm for the experiments presented in this paper. Section 5 describe the experiments and results obtained and Section 6 discusses our approach comparing it with past research. Finally, in Section 7 we close the paper with conclusions and future work. 2 Data and Data Preparation The dataset used for this study – part of the CONCISUS corpus (Saggion and Szasz, 2012) – consists of a set of 250 summaries in Spanish and English for three different domains: aviation accidents, rail accidents, and earthquakes. This dataset makes it possible to compare the performance of learning procedures across languages and domains. Based on commonsense, a human annotator developed an annotation schema per domain to describe in a templatelike representation the essential elements (i.e., slots) of each event. For example, for the aviation accident domain these essential elements were: the date of the accident, the number of victims, the airline, the 14 aircraft, the locatio"
W12-3003,J02-4005,1,\N,Missing
W14-1201,R13-2011,1,0.875676,"Missing"
W14-1201,E99-1042,0,0.198958,"ality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the origin"
W14-1201,W03-1602,0,0.0740564,"sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising. 1 Introduction Lexically and syntactically complex sentences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibil"
W14-1201,C96-2183,0,0.779222,"tences can be difficult to understand for non-native speakers (Petersen and Ostendorf, 2007; Alu´ısio et al., 2008b), and for people with language impairments, e.g. people diagnosed with aphasia (Carroll et al., 1999; Devlin, 1999), autism spectrum ˇ disorder (Stajner et al., 2012; Martos et al., 2012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibility of replacing human assessment of the quality of TS system output with automatic evaluation. • It is the first study to investigate the correlation of human assessment of TS system output with MT evaluation metrics. • It proposes a decision-making procedu"
W14-1201,W11-1601,0,0.139643,"Missing"
W14-1201,P03-1054,0,0.00867588,"hu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each factual event mention into a separate sentence of the output. The last simplification scheme (pronominal anaphora) additionally employs pronominal anaphora resolution on top of the event-wise simplification scheme.3 Methodology 3.2 All experiments were conducted on a freely available sentence-level dataset1 , fully described in ˇ (Glavaˇs and Stajner, 2013), and the two datasets we derived from"
W14-1201,P02-1040,0,0.106571,"ree of simplification. Woodsend and Lapata (2011b), and Glavaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event menti"
W14-1201,D11-1038,0,0.180995,"n for Computational Linguistics 2 Category weighted κ Pearson MAE Grammaticality 0.68 0.77 0.18 Meaning 0.53 0.67 0.37 Simplicity 0.54 0.60 0.28 Related Work The output of the TS system proposed by Siddharthan (2006) was rated for grammaticality and meaning preservation by three human evaluators. Similarly, Drndarevic et al. (2013) evaluated the grammaticality and the meaning preservation of automatically simplified Spanish sentences on a Likert scale with the help of twenty-five human annotators. Additionally, the authors used seven readability metrics to assess the degree of simplification. Woodsend and Lapata (2011b), and Glavaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002),"
W14-1201,P12-1107,0,0.436511,"Missing"
W14-1201,W11-2112,0,0.0548789,"fully automatic manner – using some readability measures or average sentence length as features ˇ (as in (Drndarevi´c et al., 2013; Glavaˇs and Stajner, 4 2013) for example). 3.4 3.5 Goal After we obtained the six automatic metrics (cosine, METEOR, TERp, TINE, T-BLEU, and SRL), we performed two sets of experiments, trying to answer two main questions: Features: MT Evaluation Metrics In all experiments, we focused on six commonly used MT evaluation metrics. These are cosine similarity (using the bag-of-words representation), METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011), and two components of TINE: T-BLEU (which differs from the standard BLEU (Papineni et al., 2002) by using 3-grams, 2-grams, and 1-grams when there are no 4-grams found, where the “original” BLEU would give score ‘0’) and SRL (which is the component of TINE based on semantic role labeling using SENNA4 ). Although these two components contribute equally to TINE (thus being linearly correlated with TINE), we wanted to investigate which one of them contributes more to the correlation of TINE with human judgements. Given their different natures, we expect T-BLEU to contribute more to the correlat"
W14-1201,C10-1152,0,0.652024,"012), dyslexia (Rello, 2012), congenital deafness (Inui et al., 2003), and intellectual disability (Feng, 2009). At the same time, long and complex sentences are also a stumbling block for many NLP tasks and applications such as parsing, machine translation, information retrieval, and summarisation (Chandrasekar et al., 1996). This justifies the need for Text Simplification (TS) systems which would convert such sentences into their simpler and easier-to-read variants, while at the same time preserving the original meaning. So far, TS systems have been developed for English (Siddharthan, 2006; Zhu et al., 2010; Wood• It is the first study to explore the possibility of replacing human assessment of the quality of TS system output with automatic evaluation. • It is the first study to investigate the correlation of human assessment of TS system output with MT evaluation metrics. • It proposes a decision-making procedure for the classification of simplified sentences into: (1) those which are acceptable; (2) those which need further post-editing; and (3) those which should be discarded. 1 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @"
W14-1201,ruiter-etal-2010-human,0,0.038341,"Missing"
W14-1201,W09-0441,0,0.384081,"vaˇs ˇ and Stajner (2013) used human annotators’ ratings for evaluating simplification, meaning preservation, and grammaticality, while additionally applying several readability metrics for evaluating complexity reduction in entire texts. Another set of studies approached TS as an MT task translating from “original” to “simplified” language, e.g. (Specia, 2010; Woodsend and Lapata, 2011a; Zhu et al., 2010). In this case, the quality of the output generated by the system was evaluated using several standard MT evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TERp (Snover et al., 2009). 3 ˇ Table 1: IAA from (Glavaˇs and Stajner, 2013) The simplified versions of original sentences were obtained by using four different simplification methods: baseline, sentence-wise, eventwise, and pronominal anaphora. The baseline retains only the main clause of a sentence, and discards all subordinate clauses, based on the output of the Stanford constituency parser (Klein and Manning, 2003). Sentence-wise simplification eliminates all those tokens in the original sentence that do not belong to any of the extracted factual event mentions, while the event-wise simplification transforms each"
W14-1201,W11-2107,0,\N,Missing
W14-1204,W12-2202,1,0.843165,"an objective approximation of readability. 3.3 Materials In this section we describe how we designed the texts and keywords that were used as study material, as well as the comprehension and subjective ratings questionnaires. Base Texts. We picked two similar texts from the Spanish corpus Simplext (Bott and Saggion, 2012). To meet the comparability requirements among the texts belonging to the same experiment, we adapted the texts maintaining as much as possible the original text. We matched the readability of the texts by making sure that the parameters commonly used to compute readability (Drndarevic and Saggion, 2012), had the same or similar values. Both texts: • Reading Time. The total time it takes a participant to completely read one text. Shorter reading durations are preferred to longer ones, since faster reading is related to more readable texts (Williams et al., 2003). Therefore, we use Reading Time, that is, the time it takes a participant to completely read one text, as a measure of readability, in addition to Fixation Duration. (a) are written in the same genre (news); (b) are about similar topics (culture); (c) have the same number of words (158 words): • Comprehension Score. To measure text co"
W14-1204,S10-1004,0,0.0175756,"objectively the impact of highlighting keywords in a text on the readability and comprehensibility for people with dyslexia. Second, to the best of our knowledge, there are no studies in assistive technology that uses an NLP based engine to highlight keywords for people with dyslexia. In this work we address the first issue, taking the second one into consideration. Hence, we emulated in the experiment the output that a potential NLP tool would give for highlighting the main ideas in the text. Key-phrase and Keyword Extraction There is a vast amount of NLP literature on keyphrase extraction (Kim et al., 2010; Witten et al., 1999; Frank et al., 1999). The semantic data provided by key-phrase extraction can be used as metadata for refining NLP applications, such as summarization (D’Avanzo and Magnini, 2005; Lawrie et al., 2001), text ranking (Mihalcea and Tarau, 2004), indexing (Medelyan and Witten, 2006), query expansion (Song et al., 2006), or document management and topic search (Gutwin et al., 1999). The closest work to ours is (Turney, 1999) because they highlight key-phrases in the text to facilitate its skimming. They compare the highlighting outputs of two different systems, Search 97 and G"
W14-1204,A97-1011,0,0.143575,"Missing"
W14-1204,I13-1123,0,0.0255229,"Missing"
W14-1204,W03-2317,0,0.0263931,"m the Spanish corpus Simplext (Bott and Saggion, 2012). To meet the comparability requirements among the texts belonging to the same experiment, we adapted the texts maintaining as much as possible the original text. We matched the readability of the texts by making sure that the parameters commonly used to compute readability (Drndarevic and Saggion, 2012), had the same or similar values. Both texts: • Reading Time. The total time it takes a participant to completely read one text. Shorter reading durations are preferred to longer ones, since faster reading is related to more readable texts (Williams et al., 2003). Therefore, we use Reading Time, that is, the time it takes a participant to completely read one text, as a measure of readability, in addition to Fixation Duration. (a) are written in the same genre (news); (b) are about similar topics (culture); (c) have the same number of words (158 words): • Comprehension Score. To measure text comprehensibility we used inferential items, that is, questions that require a deep understanding of the content of the text. We used multiple-choice questions with three possible choices, one correct, and two wrong. We compute the text comprehension score as the n"
W14-1204,W04-3252,0,\N,Missing
W14-1204,bott-etal-2012-text,1,\N,Missing
W14-2609,E14-3007,1,\N,Missing
W14-2609,W13-1605,0,\N,Missing
W14-2609,filatova-2012-irony,0,\N,Missing
W14-2609,P11-2102,0,\N,Missing
W14-2609,D13-1066,0,\N,Missing
W14-2609,ide-suderman-2004-american,0,\N,Missing
W14-2609,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
W14-2609,R13-1011,0,\N,Missing
W15-1605,W04-1907,0,0.0307788,"resource for scientific text mining. In this respect, we also present the results of our initial experiments of automatic classification of sentences into the 5 main categories in our corpus. 1 Introduction Understanding the internal organization of text documents is important for many content assessment tasks such as summarization or information extraction. Several studies have investigated the structure and peculiarities of scientific discourse across distinct domains, such as biology (Mizuta and Collier, 2004), chemistry and computational linguistics (Teufel et al., 2009), or astrophysics (Grover et al., 2004). The coherence of the argumentative flow that authors adopt to expose scientific contents is essential to properly contextualize these contents, to characterize their connections with related pieces of research as well as to discover relevant aspects, novelties and future directions. Because of both the huge, growing amount of scientific literature that is accessible online and the complexity that often characterizes scientific discourse, currently researchers and professionals are experimenting more and more difficulties when trying to keep themselves up to date. The analysis of the internal"
W15-1605,W10-1913,0,0.41752,"Missing"
W15-1605,liakata-etal-2010-corpora,0,0.39219,"of scope, abstracts, considered to be a brief summary of the whole article, have been the object of research in the works of Guo (2010), Lin (2006), Ruch (2007), Hirohata (2008) and Thompson (2009). Among researchers who explore full articles, Lin (2006) and Hirohata (2008) have based their analysis on section names, offering a coarse-grained annotation, while Liakata (2010; 2012), Teufel (2009) and Shatkay (2008) adopt a finer-grained approach. The annotation unit is also a controversial matter. While most researchers agree to classify sentences into categories (Liakata and Soldatova, 2008; Liakata et al., 2010; Teufel and Moens, 2002; Teufel et al., 2009; Lin et al., 2006; Hirohata et al., 2008), others segment sentences into smaller discourse units (Shatkay et al., 2008; DeWaard, 2009). Bioscience is by far the most studied domain and acts as a motor for research in information extraction from scientific publications (Mizuta et al., 2006; Wilbur et al., 2006; Liakata et al., 2010). Nevertheless, some work has also been done in the Computational Linguistics and Chemistry domains, where Teufel (2009) has implemented her AZ-II extended annotation scheme. 2.1 Argumentative Zoning - AZ Teufel’s main as"
W15-1605,W06-3309,0,0.0163149,"article, have been the object of research in the works of Guo (2010), Lin (2006), Ruch (2007), Hirohata (2008) and Thompson (2009). Among researchers who explore full articles, Lin (2006) and Hirohata (2008) have based their analysis on section names, offering a coarse-grained annotation, while Liakata (2010; 2012), Teufel (2009) and Shatkay (2008) adopt a finer-grained approach. The annotation unit is also a controversial matter. While most researchers agree to classify sentences into categories (Liakata and Soldatova, 2008; Liakata et al., 2010; Teufel and Moens, 2002; Teufel et al., 2009; Lin et al., 2006; Hirohata et al., 2008), others segment sentences into smaller discourse units (Shatkay et al., 2008; DeWaard, 2009). Bioscience is by far the most studied domain and acts as a motor for research in information extraction from scientific publications (Mizuta et al., 2006; Wilbur et al., 2006; Liakata et al., 2010). Nevertheless, some work has also been done in the Computational Linguistics and Chemistry domains, where Teufel (2009) has implemented her AZ-II extended annotation scheme. 2.1 Argumentative Zoning - AZ Teufel’s main assumptions are that scientific discourse contains descriptions o"
W15-1605,W09-3603,0,0.0368674,"Missing"
W15-1605,nawaz-etal-2010-meta,0,0.0592183,"Missing"
W15-1605,J02-4002,0,0.211424,"n our first experiments in automatic sentence classification in section 7. 2 Scientific discourse characterization: related work The analysis and annotation of scientific discourse has been approached from different points of view in previous works. Although the focus of the analysis is manifold and spans along different linguistic concepts, the scientific discourse annotation schema we propose in this paper builds upon the proposals of Teufel (1999; 2009; 2010) and Liakata (2010) hence the following subsections describe in more detail their contributions. Simone Teufel’s model (Teufel, 1999; Teufel and Moens, 2002; Teufel et al., 2009), which was named Argumentative Zoning, focuses on knowledge claims and is based on previous schemes for 42 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 42–51, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics classifying the citation functions (Garfield, 1965; Melvin Weinstock, 1971; Spiegel-R¨osing, 1977). Liakata (2010) analyses the content and conceptual structure of scientific articles with an ontologybased annotation scheme, the Core Scientific Concepts scheme (CoreSc). Closely related to this approach is the m"
W15-1605,D09-1155,0,0.388152,"corpus constitutes a semantically rich resource for scientific text mining. In this respect, we also present the results of our initial experiments of automatic classification of sentences into the 5 main categories in our corpus. 1 Introduction Understanding the internal organization of text documents is important for many content assessment tasks such as summarization or information extraction. Several studies have investigated the structure and peculiarities of scientific discourse across distinct domains, such as biology (Mizuta and Collier, 2004), chemistry and computational linguistics (Teufel et al., 2009), or astrophysics (Grover et al., 2004). The coherence of the argumentative flow that authors adopt to expose scientific contents is essential to properly contextualize these contents, to characterize their connections with related pieces of research as well as to discover relevant aspects, novelties and future directions. Because of both the huge, growing amount of scientific literature that is accessible online and the complexity that often characterizes scientific discourse, currently researchers and professionals are experimenting more and more difficulties when trying to keep themselves u"
W15-1605,W11-0217,0,0.0606878,"Missing"
W15-1605,mizuta-collier-2004-annotation,0,\N,Missing
W15-1605,W09-3742,0,\N,Missing
W15-1605,I08-1050,0,\N,Missing
W16-1505,N13-1067,0,0.0786417,"and timeconsuming task. The availability of text mining tools able to extract, aggregate and turn scientific unstructured textual contents into well organized and interconnected knowledge is fundamental. However, scientific publications are characterized by several structural (title, abstract, figures, citations...), linguistic and semantic peculiarities that make them difficult to analyze by relying on general purpose text mining tools. One of the special features of scientific papers is their network of citations, that are starting to be exploited in several context including opinion mining [2, 7] and scientific text summarization [3, 8]. Besides citations, the interpretation of the semantics of the actual textual contents of 1 2 3 http://www.ncbi.nlm.nih.gov/pubmed http://www.scopus.com http://www.webofknowledge.com 36 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries scientific papers usually needs the availability of knowledge repositories with an adequate coverage of scientific concepts and relations that could not be found on global domain knowledge resources like WordNet, DBPedia, FreeBase or BabelNet. Considering both the pec"
W16-1505,P11-1051,0,0.043083,"of text mining tools able to extract, aggregate and turn scientific unstructured textual contents into well organized and interconnected knowledge is fundamental. However, scientific publications are characterized by several structural (title, abstract, figures, citations...), linguistic and semantic peculiarities that make them difficult to analyze by relying on general purpose text mining tools. One of the special features of scientific papers is their network of citations, that are starting to be exploited in several context including opinion mining [2, 7] and scientific text summarization [3, 8]. Besides citations, the interpretation of the semantics of the actual textual contents of 1 2 3 http://www.ncbi.nlm.nih.gov/pubmed http://www.scopus.com http://www.webofknowledge.com 36 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries scientific papers usually needs the availability of knowledge repositories with an adequate coverage of scientific concepts and relations that could not be found on global domain knowledge resources like WordNet, DBPedia, FreeBase or BabelNet. Considering both the peculiar structural and semantic features of"
W16-1505,L16-1492,1,0.803066,"aluate the scientific production of papers, journals and researchers (i.e. h-index). The effectiveness of citation-based research evaluation metrics would benefit from the possibility to take into account not only the number of citations a paper receives but also the purpose and the polarity of each one of them. Several classification schemata and approaches have been proposed to characterize aspects related to the purpose and polarity of citations [2,12]. By relying on and extending the set of annotated citation included in the Dr. Inventor Multi-Layered Annotated Corpus of Scientific Papers [5]8 , we aim at exploring new approaches to citation purpose and polarity classification, by placing special attention on their robustness across domains and on the limited availability of manually annotated data. 4.2 Scientific document summarization Nowadays, the possibility to automatically identify the most relevant contents across a set of scientific publications is essential to deal with and perform screenings of the huge amount of articles currently available on-line. Several approaches to scientific papers summarization have been proposed [3, 8, 11]. Most of them extend general purpose d"
W16-1505,J02-4002,0,0.00730701,"Layered Annotated Corpus of Scientific Papers [5]8 , we aim at exploring new approaches to citation purpose and polarity classification, by placing special attention on their robustness across domains and on the limited availability of manually annotated data. 4.2 Scientific document summarization Nowadays, the possibility to automatically identify the most relevant contents across a set of scientific publications is essential to deal with and perform screenings of the huge amount of articles currently available on-line. Several approaches to scientific papers summarization have been proposed [3, 8, 11]. Most of them extend general purpose document summarization methodologies by considering information facets that are characteristic of scientific publications. In particular, the sentences of the papers in which the article to summarize is cited provide valuable material to improve the quality of scientific summarization. Also the possibility to consider the rhetorical structure (background, approach, future work, etc.) of the different excerpts of the contents of a paper to summarize provides valuable information to generate summaries that include contents better balanced across the sections"
W16-1505,W06-1613,0,0.0142754,"levant connection among both works. The count of the citation that a paper receives constitute the basis of the most common metrics exploited to evaluate the scientific production of papers, journals and researchers (i.e. h-index). The effectiveness of citation-based research evaluation metrics would benefit from the possibility to take into account not only the number of citations a paper receives but also the purpose and the polarity of each one of them. Several classification schemata and approaches have been proposed to characterize aspects related to the purpose and polarity of citations [2,12]. By relying on and extending the set of annotated citation included in the Dr. Inventor Multi-Layered Annotated Corpus of Scientific Papers [5]8 , we aim at exploring new approaches to citation purpose and polarity classification, by placing special attention on their robustness across domains and on the limited availability of manually annotated data. 4.2 Scientific document summarization Nowadays, the possibility to automatically identify the most relevant contents across a set of scientific publications is essential to deal with and perform screenings of the huge amount of articles current"
W16-1520,N13-1067,0,0.0625288,"ral elements (title, authors, abstract, sections, figures, tables, citations, bibliography) that often require specific text processing tools. Additionally, scientific documents have specific discourse structure [27, 12]. Another important aspect of scientific papers is their network of citations that identifies links among research works, making them also particularly interesting from the social viewpoint. Although citation counts had been used to assess some aspects of research output for a long time, citation semantics has started to be exploited in several context including opinion mining [28, 1] and scientific text summarization [18, 2]. 175 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Considering the urgent need for new, automated approaches to browse and aggregate scientific information, in recent years a number of natural language processing challenges have been proposed: the Biomedical Summarization Task (BioSumm2014) carried out in the context of the Text Analysis Conferences1 provided a forum for researchers interested in exploring the summarization of clusters of documents where one of the documents is a reference pape"
W16-1520,P11-1051,0,0.018843,"ctions, figures, tables, citations, bibliography) that often require specific text processing tools. Additionally, scientific documents have specific discourse structure [27, 12]. Another important aspect of scientific papers is their network of citations that identifies links among research works, making them also particularly interesting from the social viewpoint. Although citation counts had been used to assess some aspects of research output for a long time, citation semantics has started to be exploited in several context including opinion mining [28, 1] and scientific text summarization [18, 2]. 175 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Considering the urgent need for new, automated approaches to browse and aggregate scientific information, in recent years a number of natural language processing challenges have been proposed: the Biomedical Summarization Task (BioSumm2014) carried out in the context of the Text Analysis Conferences1 provided a forum for researchers interested in exploring the summarization of clusters of documents where one of the documents is a reference paper and the rest of the documents in the clu"
W16-1520,L16-1492,1,0.79867,"matize each document. The SUMMA library was used to produce normalized term vectors for each document (see Section 5). Although the Dr Inventor’s library produces very rich information, for the experiments we present here we rely only on its rhetorical sentence classification capabilities. The DRI Framework classify each sentence of a paper as belonging to a rethorical category of scientific discourse among: Approach, Background, Challenge, Outcome and FutureWork. In particular, the framework computes for each sentence the probability the sentece has to belong to each rhetorical category. See [6] for details about the corpus used for training the classifier. For each sentence in the reference paper, we computed the cosine similarity between its sentence vector (see Section 5) and the vectors corresponding to the sentences citing the reference paper in the citing articles. These values were stored in the reference paper for further processing. 4 Method In order to identify reference paper text spans for each citance (Task 1A), we modeled pairs of reference and citance sentences as a feature vector. Then, we used such pair representation to enable the training of distinct binary classif"
W16-1520,O97-1002,0,0.0830254,"ing all the pairs of synsets belonging to different sentences, we compute similarity values between citance sentence and reference sentence as follows3 : – Path similarity [7] (path similarity): The shorter the path between two words/senses in WordNet, the more similar they are. 3 We calculated similarity values between each token in the citance sentence and each and every token in the reference 179 sentence. Finally averaging all the similaries for the given sentence pair. BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries – JCN similarity [10] (jiangconrath similarity): the conditional probability of encountering an instance of a child-synset given an instance of a parent synset. – LCH similarity [11] (lch similarity): the length of the shortest path between two synsets for their measure of similarity. – LESK similarity [3] (lesk similarity): Similarity of two concepts is defined as a function of the overlap between the corresponding definitions (i.e., their WordNet glosses). – LIN similarity [13] (lin similarity): The Similarity between A and B is measured by the ratio between the amount of information needed to state the commonal"
W16-1520,liakata-etal-2010-corpora,0,0.0206548,"onsider at a given time, automatic text summarization is a useful technique [23]. However, generic text summarization techniques may not work well in specialized genres such as the scientific genre and domain specific techniques may be needed [25, 26]. Scientific publications are characterized by several structural, linguistic and semantic peculiarities. Articles include common structural elements (title, authors, abstract, sections, figures, tables, citations, bibliography) that often require specific text processing tools. Additionally, scientific documents have specific discourse structure [27, 12]. Another important aspect of scientific papers is their network of citations that identifies links among research works, making them also particularly interesting from the social viewpoint. Although citation counts had been used to assess some aspects of research output for a long time, citation semantics has started to be exploited in several context including opinion mining [28, 1] and scientific text summarization [18, 2]. 175 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Considering the urgent need for new, automated approaches to"
W16-1520,P10-1057,0,0.12503,"ctions, figures, tables, citations, bibliography) that often require specific text processing tools. Additionally, scientific documents have specific discourse structure [27, 12]. Another important aspect of scientific papers is their network of citations that identifies links among research works, making them also particularly interesting from the social viewpoint. Although citation counts had been used to assess some aspects of research output for a long time, citation semantics has started to be exploited in several context including opinion mining [28, 1] and scientific text summarization [18, 2]. 175 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Considering the urgent need for new, automated approaches to browse and aggregate scientific information, in recent years a number of natural language processing challenges have been proposed: the Biomedical Summarization Task (BioSumm2014) carried out in the context of the Text Analysis Conferences1 provided a forum for researchers interested in exploring the summarization of clusters of documents where one of the documents is a reference paper and the rest of the documents in the clu"
W16-1520,saggion-2014-creating,1,0.761063,"0.924 0.926 0.924 Table 2. SMO performance on testing data (10-fold cross validation) for the facet identification problem (Task 1B). Last row of the table contains weighted average values. 5 Summarizing Scientific Articles In order to summarize the reference paper by taking into account how it is mentioned in the citing papers, we combined information from the reference and 181 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries citing papers. We have implemented, using the resources of the freely available text summarization library SUMMA [22, 24], a series of sentence relevance features, all numeric, which are used to train a linear regression model following the methodology that was already used in [4]. In addition to rich set of features provided by the DRI Framework, document processing for summarization is carried out with SUMMA on reference and citing papers. More specifically, the following computations with the library are carried out to enable the summarization of scientific documents: – Each token (i.e., lemma) is weighted by its term frequency* inverted document frequency, where inverted document values are computed from tra"
W16-1520,J02-4005,1,0.796088,"seconds [17]. In this scenario of scientific information overload, researchers are overwhelmed by an enormous and continuously growing number of articles to consider in their research work: from the exploration of advances in specific topics, to peer reviewing, writing and evaluation. In order to cope with the growing number of relevant publications to consider at a given time, automatic text summarization is a useful technique [23]. However, generic text summarization techniques may not work well in specialized genres such as the scientific genre and domain specific techniques may be needed [25, 26]. Scientific publications are characterized by several structural, linguistic and semantic peculiarities. Articles include common structural elements (title, authors, abstract, sections, figures, tables, citations, bibliography) that often require specific text processing tools. Additionally, scientific documents have specific discourse structure [27, 12]. Another important aspect of scientific papers is their network of citations that identifies links among research works, making them also particularly interesting from the social viewpoint. Although citation counts had been used to assess som"
W16-1520,J02-4002,0,0.825791,"seconds [17]. In this scenario of scientific information overload, researchers are overwhelmed by an enormous and continuously growing number of articles to consider in their research work: from the exploration of advances in specific topics, to peer reviewing, writing and evaluation. In order to cope with the growing number of relevant publications to consider at a given time, automatic text summarization is a useful technique [23]. However, generic text summarization techniques may not work well in specialized genres such as the scientific genre and domain specific techniques may be needed [25, 26]. Scientific publications are characterized by several structural, linguistic and semantic peculiarities. Articles include common structural elements (title, authors, abstract, sections, figures, tables, citations, bibliography) that often require specific text processing tools. Additionally, scientific documents have specific discourse structure [27, 12]. Another important aspect of scientific papers is their network of citations that identifies links among research works, making them also particularly interesting from the social viewpoint. Although citation counts had been used to assess som"
W16-1520,D09-1155,0,0.0144452,"onsider at a given time, automatic text summarization is a useful technique [23]. However, generic text summarization techniques may not work well in specialized genres such as the scientific genre and domain specific techniques may be needed [25, 26]. Scientific publications are characterized by several structural, linguistic and semantic peculiarities. Articles include common structural elements (title, authors, abstract, sections, figures, tables, citations, bibliography) that often require specific text processing tools. Additionally, scientific documents have specific discourse structure [27, 12]. Another important aspect of scientific papers is their network of citations that identifies links among research works, making them also particularly interesting from the social viewpoint. Although citation counts had been used to assess some aspects of research output for a long time, citation semantics has started to be exploited in several context including opinion mining [28, 1] and scientific text summarization [18, 2]. 175 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Considering the urgent need for new, automated approaches to"
W16-1520,W06-1613,0,0.0501893,"ral elements (title, authors, abstract, sections, figures, tables, citations, bibliography) that often require specific text processing tools. Additionally, scientific documents have specific discourse structure [27, 12]. Another important aspect of scientific papers is their network of citations that identifies links among research works, making them also particularly interesting from the social viewpoint. Although citation counts had been used to assess some aspects of research output for a long time, citation semantics has started to be exploited in several context including opinion mining [28, 1] and scientific text summarization [18, 2]. 175 BIRNDL 2016 Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries Considering the urgent need for new, automated approaches to browse and aggregate scientific information, in recent years a number of natural language processing challenges have been proposed: the Biomedical Summarization Task (BioSumm2014) carried out in the context of the Text Analysis Conferences1 provided a forum for researchers interested in exploring the summarization of clusters of documents where one of the documents is a reference pape"
W17-4402,K16-1017,0,0.0322971,"Missing"
W17-4402,L16-1626,1,0.847409,"’, and thus we can conclude that both emotes are used interchangeably. 7 Related Work The most similar communicative phenomena to emotes are emojis. Emojis are used by the vast majority of Social Media services and instant messaging platforms (Jibril and Abdullah, 2013; Park et al., 2013, 2014). Emojis (like the older emoticons) give the possibility to express a variety of ideas and feelings in a visual, concise and appealing way that is perfectly suited for the informal style of Social Media. Several recent works studied Emojis, focusing on emojis’ semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a,b,c; Eisner et al., 2016; Ljubesic and Fiser, 2016; Ai et al., 2017; Miller et al., 2017), and sentiment (Novak et al., 2015; Hu et al., 2017). Finally, (Barbieri et al., 2017) presented an emoji prediction model for Twitter, where they use a char based B-LSTM to detect the 20 most frequent emojis. Most work on irony and sarcasm detection in 17 Acknowledgments Twitter has employed hashtags as labels for detecting irony. This approach was introduced by Tsur et al. (Tsur et al., 2010) and (GonzalezIbanez et al., 2011), who used the #sarcasm hashtag to retrieve sarcastic tweets. This technique"
W17-4402,E14-3007,1,0.858781,"t on the game that is being played, make an out-of-context joke, or discuss an unrelated event like a football game. 2.1 The kappa emote as a trolling indicator 3.2 Predicting Twitch Emotes Trolling Detection The availability and general usage of the ‘kappa’ emote enables a potential test bed for performing experiments on detecting troll messages in Twitch chatrooms. We approach this task under the assumption that adding ‘kappa’ at the end of a message has a similar effect as it would be to add #irony or #sarcasm at the end of a Twitter message (see (Davidov et al., 2010; Reyes et al., 2013b; Barbieri and Saggion, 2014) for extensive research on irony and sarcasm detection in Twitter under this assumption). Thus, for the trolling prediction experiments, we benefit from this particularity and construct an evaluation dataset where messages are split by considering presence or absence of this emote. In an additional experiment, we further investigate the properties of derivations Twitch Emotes Twitch messages can be enhanced with Twitch emotes, “small pictorial glyphs that fans pepper into text”1 . These emotes range from the more regular smiley faces, to others such as game-specific, channel-specific, or even"
W17-4402,P11-2102,0,0.0201227,"of the B-LSTM model. We chose two common algorithms for text classification, which unlike 4 These games are: Counter Strike: Global Offensive, Dota 2, Hearthstone: Heroes of Warcraft, League of Legends and World of Warcraft. 5 LSTM hidden states are of size 100, and each LSTM has two layers. 13 Model Majority BOW Vec-AVG B-LSTM LSTMs, do not take into account the entire sequence of words. 5.2.1 Bag of Words We designed a Bag-of-Words (Bow) classifier as such model has been successfully employed in several classification tasks, like sentiment analysis and irony detection (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Reyes et al., 2013a). We represent each message with a vector of the most informative tokens (punctuation marks are included as well). Words are selected using term frequency-inverse document frequency (TF-IDF), which is intended to reflect how important a word is to a document (message) in the corpus. After obtaining a vector for each message we classify with a L2-regularized logistic regression classifier to make the predictions6 with ε equal to 0.001. F1 0.10 0.29 0.32 0.39 each emote, along with their Ranking and occurrences in the test set. The Ranking is the average number of emotes wi"
W17-4402,W10-2914,0,0.0640622,"In a very short time span, users may comment on the game that is being played, make an out-of-context joke, or discuss an unrelated event like a football game. 2.1 The kappa emote as a trolling indicator 3.2 Predicting Twitch Emotes Trolling Detection The availability and general usage of the ‘kappa’ emote enables a potential test bed for performing experiments on detecting troll messages in Twitch chatrooms. We approach this task under the assumption that adding ‘kappa’ at the end of a message has a similar effect as it would be to add #irony or #sarcasm at the end of a Twitter message (see (Davidov et al., 2010; Reyes et al., 2013b; Barbieri and Saggion, 2014) for extensive research on irony and sarcasm detection in Twitter under this assumption). Thus, for the trolling prediction experiments, we benefit from this particularity and construct an evaluation dataset where messages are split by considering presence or absence of this emote. In an additional experiment, we further investigate the properties of derivations Twitch Emotes Twitch messages can be enhanced with Twitch emotes, “small pictorial glyphs that fans pepper into text”1 . These emotes range from the more regular smiley faces, to others"
W17-4402,W16-5001,0,0.0303422,"Missing"
W17-4402,P16-2044,0,0.0262326,"ges in total. Messages were randomly selected to avoid topic bias. The second dataset (Multi Kappa dataset) is composed of 100,000 messages per game that contain ‘kappa’ emotes, hence a total of 500,000 messages. Due to the similarity of some emotes to ‘kappa’ we considered five different emotes as ‘kappa’, namely ‘kappa’, ‘kappapride’, ‘keepo’, ‘kappaross’ and ‘kappaclaus’. 5.1 Bi-Directional LSTMs Given the proven effectiveness of recurrent neural networks in different tasks (Chung et al., 2014; Vinyals et al., 2015; Bahdanau et al., 2014, interalia), which also includes modeling of tweets (Dhingra et al., 2016; Barbieri et al., 2017), our Emote prediction model is based on RNNs, which are modeled to learn sequential data. We use the word based B-LSTM architecture by Barbieri et al. (2017), designed to model emojis in Twitter. The forward LSTM reads the message from left to right and the backward one reads the message in the reverse direction.5 The learned vector of each LSTM, is passed through a component-wise rectified linear unit (ReLU) nonlinearity (Glorot et al., 2011); finally, an affine transformation of these learned vectors is passed to a softmax layer to give a distribution over the list o"
W17-4402,W16-6208,0,0.0222945,"that both emotes are used interchangeably. 7 Related Work The most similar communicative phenomena to emotes are emojis. Emojis are used by the vast majority of Social Media services and instant messaging platforms (Jibril and Abdullah, 2013; Park et al., 2013, 2014). Emojis (like the older emoticons) give the possibility to express a variety of ideas and feelings in a visual, concise and appealing way that is perfectly suited for the informal style of Social Media. Several recent works studied Emojis, focusing on emojis’ semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a,b,c; Eisner et al., 2016; Ljubesic and Fiser, 2016; Ai et al., 2017; Miller et al., 2017), and sentiment (Novak et al., 2015; Hu et al., 2017). Finally, (Barbieri et al., 2017) presented an emoji prediction model for Twitter, where they use a char based B-LSTM to detect the 20 most frequent emojis. Most work on irony and sarcasm detection in 17 Acknowledgments Twitter has employed hashtags as labels for detecting irony. This approach was introduced by Tsur et al. (Tsur et al., 2010) and (GonzalezIbanez et al., 2011), who used the #sarcasm hashtag to retrieve sarcastic tweets. This technique was later validated by var"
W17-4402,P15-2124,0,0.0466985,"Missing"
W17-4402,W16-0425,0,0.0252767,"Missing"
W17-4402,P11-2008,0,0.11067,"Missing"
W17-4402,N15-1142,0,0.0359483,"Missing"
W17-4402,W16-2610,0,0.0311034,"used interchangeably. 7 Related Work The most similar communicative phenomena to emotes are emojis. Emojis are used by the vast majority of Social Media services and instant messaging platforms (Jibril and Abdullah, 2013; Park et al., 2013, 2014). Emojis (like the older emoticons) give the possibility to express a variety of ideas and feelings in a visual, concise and appealing way that is perfectly suited for the informal style of Social Media. Several recent works studied Emojis, focusing on emojis’ semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a,b,c; Eisner et al., 2016; Ljubesic and Fiser, 2016; Ai et al., 2017; Miller et al., 2017), and sentiment (Novak et al., 2015; Hu et al., 2017). Finally, (Barbieri et al., 2017) presented an emoji prediction model for Twitter, where they use a char based B-LSTM to detect the 20 most frequent emojis. Most work on irony and sarcasm detection in 17 Acknowledgments Twitter has employed hashtags as labels for detecting irony. This approach was introduced by Tsur et al. (Tsur et al., 2010) and (GonzalezIbanez et al., 2011), who used the #sarcasm hashtag to retrieve sarcastic tweets. This technique was later validated by various studies (Wang, 2013;"
W17-4402,S16-1003,0,0.0570629,"Missing"
W17-4402,Y13-1035,0,0.0338972,"Fiser, 2016; Ai et al., 2017; Miller et al., 2017), and sentiment (Novak et al., 2015; Hu et al., 2017). Finally, (Barbieri et al., 2017) presented an emoji prediction model for Twitter, where they use a char based B-LSTM to detect the 20 most frequent emojis. Most work on irony and sarcasm detection in 17 Acknowledgments Twitter has employed hashtags as labels for detecting irony. This approach was introduced by Tsur et al. (Tsur et al., 2010) and (GonzalezIbanez et al., 2011), who used the #sarcasm hashtag to retrieve sarcastic tweets. This technique was later validated by various studies (Wang, 2013; Sulis et al., 2016), which analyze the language associated to the use of irony-related hashtags (such as #irony, and #not). Recent years have seen an increase in models for detecting #irony and #sarcasm. Many of these models adopted hand crafted features (amoung others (Reyes et al., 2013a; Barbieri and Saggion, 2014; Liu et al., 2014; Joshi et al., 2015)), and others employed pretrained word embeddings or deep learning systems such as CNN or LSTMs (Joshi et al., 2016; Ghosh and Veale, 2016; Poria et al., 2016; Amir et al., 2016). 8 We thank the three anonymous reviewers for their time and t"
W17-4402,C16-1151,0,0.0341361,"Missing"
W17-5406,P15-2011,0,0.0302894,"Missing"
W17-5406,padro-stanilovsky-2012-freeling,0,0.0987228,"Missing"
W17-5406,C16-1069,0,0.011637,"f word sequences and the contextual meaning of words. Wikipedia has also been used in lexical simplification studies. Biran et al. (Biran et al., 2011) used word frequencies in English Wikipedia and Simple English Wikipedia (SEW) to calculate their difficulty while Yatskar et al. (Yatskar et al., 2010) used SEW edit histories to identify the simplify operations. More recently, ˇ (Glavaˇs and Stajner, 2015) proposed a simplification method based on current distributional lexical semantics approaches for languages for which lexical resources are scarce. The same line of research is followed by (Paetzold, 2016) who additionally includes a retrofitting mechanism to better distinguish between synonyms and antonyms (Faruqui et al., 2015). 2 • Specia (2010) used the Moses toolkit for phrase-based Statistical Machine Translation (SMT) and a corpus of about 4,483 sentences (3,383 for training, 500 for tuning, and 500 for test) in order to learn how to simplify sentences in Brazilian Portuguese. • Stajner (2014) also used phrase-based SMT for lexical simplification in Spanish. She built language models derived from the Spanish Europarl corpus and used 700 sentence pairs for training, 100 sentence pairs for"
W17-5406,W10-1607,0,0.0627621,"Missing"
W17-5406,N15-1156,0,0.0155856,"n synonyms and antonyms (Faruqui et al., 2015). 2 • Specia (2010) used the Moses toolkit for phrase-based Statistical Machine Translation (SMT) and a corpus of about 4,483 sentences (3,383 for training, 500 for tuning, and 500 for test) in order to learn how to simplify sentences in Brazilian Portuguese. • Stajner (2014) also used phrase-based SMT for lexical simplification in Spanish. She built language models derived from the Spanish Europarl corpus and used 700 sentence pairs for training, 100 sentence pairs for development, and three test sets for testing (of 50, 50, and 100 sentences). • Baeza-Yates et al. (2015) presented CASSA a lexical simplifier for Spanish. CASSA uses the Google Books Ngram Corpus to find the frequency of target words and its contexts and uses this information for disambiguation. The Spanish OpenThesaurus (version 2) is used to obtain synonyms and web frequencies are used for disambiguation and lexical simplicity. No morphological realization is performed in this system. Not based on parallel or comparable corpora. 41 3 Lexical Simplifier racia” (es,gl,pt)) and gastronomy (“gastronomia” (ca), ”gastronom´ıa” (es,gl,pt)). The Lexical Simplification architecture allows to simplify w"
W17-5406,S12-1046,0,0.0472599,"Missing"
W17-5406,P11-2087,0,0.0487965,"Missing"
W17-5406,C12-1023,1,0.903291,"em in Figure 1). The Document Analysis phase uses the FreeLing 4.03 system (Padr´o and Stanilovsky, 2012) to perform tokenization, sentence splitting, part-of-speech (PoS) tagging, lemmatization, and Named Entity Recognition. 3.1 lang ca es gl pt ca es gl pt #counts democracy 3,055 11,485 1,084 4,443 gastronomy 1,163 6,850 457 1,172 In order to obtain a threshold for each language for the Complex Word Detection phase the following procedure has been applied: 1) A set of pairs &lt;complex word, simpler synonym&gt; (such as &lt;novelist,writer&gt; or &lt;tenor,singer&gt;) has been extracted from the LexSiS Gold (Bott et al., 2012) (Spanish) and the PorSimples FSP (Alu´ısio et al., 2008) (Portuguese) corpora: 102 pairs have been extracted from the LexSiS Gold corpora and 279 from the PorSimples FSP. 2) The 102 pairs in Spanish from LexSiS Gold have been automatically translated to Catalan and manually revised. In order to create a set of 100 pairs from Galician some pairs have been extracted from the 279 pairs in Portuguese and some new pairs have been manually added. 3) A measure of complex word detection accuracy that involves the use of both the complex word and the simpler synonym for each pair has been created. Thi"
W17-5406,S16-1146,0,0.0299633,"Missing"
W17-5406,N15-1184,0,0.0229906,"nyms (Faruqui et al., 2015). 2 • Specia (2010) used the Moses toolkit for phrase-based Statistical Machine Translation (SMT) and a corpus of about 4,483 sentences (3,383 for training, 500 for tuning, and 500 for test) in order to learn how to simplify sentences in Brazilian Portuguese. • Stajner (2014) also used phrase-based SMT for lexical simplification in Spanish. She built language models derived from the Spanish Europarl corpus and used 700 sentence pairs for training, 100 sentence pairs for development, and three test sets for testing (of 50, 50, and 100 sentences). • Baeza-Yates et al. (2015) presented CASSA a lexical simplifier for Spanish. CASSA uses the Google Books Ngram Corpus to find the frequency of target words and its contexts and uses this information for disambiguation. The Spanish OpenThesaurus (version 2) is used to obtain synonyms and web frequencies are used for disambiguation and lexical simplicity. No morphological realization is performed in this system. Not based on parallel or comparable corpora. 41 3 Lexical Simplifier racia” (es,gl,pt)) and gastronomy (“gastronomia” (ca), ”gastronom´ıa” (es,gl,pt)). The Lexical Simplification architecture allows to simplify w"
W17-5406,N10-1056,0,0.0364205,"it, 1998). The authors used WordNet to identify synonyms and calculated their relative difficulty using KuceraFrancis frequencies in the Oxford Psycholinguistic Database. De Belder and Moens (De Belder and Moens, 2010) combined this methodology with a latent words language model which modeled both language in terms of word sequences and the contextual meaning of words. Wikipedia has also been used in lexical simplification studies. Biran et al. (Biran et al., 2011) used word frequencies in English Wikipedia and Simple English Wikipedia (SEW) to calculate their difficulty while Yatskar et al. (Yatskar et al., 2010) used SEW edit histories to identify the simplify operations. More recently, ˇ (Glavaˇs and Stajner, 2015) proposed a simplification method based on current distributional lexical semantics approaches for languages for which lexical resources are scarce. The same line of research is followed by (Paetzold, 2016) who additionally includes a retrofitting mechanism to better distinguish between synonyms and antonyms (Faruqui et al., 2015). 2 • Specia (2010) used the Moses toolkit for phrase-based Statistical Machine Translation (SMT) and a corpus of about 4,483 sentences (3,383 for training, 500 f"
W18-0517,S16-1149,0,0.0593997,"Missing"
W18-0517,E09-1005,0,0.0542962,"it respectively. The cosine similarity was used to calculate the distance between each pair of vectors in the vector space. Text Analysis Tools We analyze the sentences in which a word to evaluate occurs by means of the Mate dependency parser (Bohnet, 2010). As a result, we obtain a lemmatized and Part-Of-Speech (POS) tagged version of the sentence, along with its syntactic dependencies. Both POS tags and dependency information are used to compute several features as described in the following Section. We also processed each sentence by the UKB graph-based Word Sense Disambiguation algorithm (Agirre and Soroa, 2009). Specifically, we benefited from the UKB implementation integrated in the Freeling workbench (Padr´o and Stanilovsky, 2012). In this way, we may disambiguate single or multiword expressions against WordNet 3.0. 5 5.2 In order to evaluate the complexity of a word, we designed two systems, each system had different word and sentence representations. 5.2.1 Shallow Features We exploited the following set of shallow word features: Word Embedding (WE) System We utilized word embeddings and modeled each sentence as a Word2Vec representation from a pretrained model of Google News with 300 dimensions"
W18-0517,S16-1157,1,0.809487,"Missing"
W18-0517,C12-1023,1,0.882284,"evaluation details, the reader is referred to (Yimam et al., 2018). In Section 2 we provide an overview of relevant research related to Complex Word Identification. Section 3 and 4 respectively introduce the CWI Shared Task 2018 dataset and present the text analysis tools and resources we exploited to characterize complex words. In Section 5 we describe the features we used to build our complex word clasIntroduction Automatic identification of complex words is a core component in several language-related areas of research, including Text Simplification (Saggion, 2017), Lexical Simplification (Bott et al., 2012), and Readability Assessment (CollinsThompson, 2014). The Complex Word Identification (CWI) Shared Task 2018 proposes a shared platform for evaluating complex word identification systems under four different tracks: English, Spanish and German monolingual CWI in addition to a multilingual French CWI track with only a test set; the three previously mentioned languages can be used as training for this specific track. The task has two subtasks: binary classification task; to determine if a word is complex or not, and a 159 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Buildi"
W18-0517,N10-1056,0,0.0339088,"l techniques have been applied so far to identify complex words. In the context of the PSET Project (Devlin and Tait, 1998), people with aphasia were the target of the first lexical simplification system for English. The system relies on a word difficulty assessment based on psycholinguistic evidence (Quinlan, 1992) in order to decide whether to simplify a word. Recent work compared a corpora of original documents (e.g. English Wikipedia) and their ’simplified’ versions (e.g. Simple English Wikipedia pages) to prompt measures which can be used to compare and rank ’quasi-synonymic’ word pairs (Yatskar et al., 2010). Besides lexical simplification, the identification of complex words constitutes a core component of readability assessment (Collins-Thompson, 2014), the problem of quantifying the readability of a given text. The more complex words a text has, the harder it becomes to read it. Lists of easy words (Dale and Chall, 1948), word characteristics (Kincaid et al., 1975; Gunning, 1952; Mc Laughlin, 1969), or word use in context (e.g. language models) (Si and Callan, 2001) are all techniques or resources which have been used to support the assessment of text readability: these approaches could also b"
W18-0517,W18-0507,0,0.0252278,"t, given the sentence in which it occurs. We designed two systems, the first system modeled each word by a set of lexical, semantic and contextual features and evaluated distinct binary classification algorithms. This system participated from the (CWI) Shared Task 2016 at SemEval (Ronzano et al., 2016) achieving very good performance. The second system modeled each word with its context through a word embedding representation. Our approaches obtained reasonable performance in general but not in comparison with the other participating systems. For evaluation details, the reader is referred to (Yimam et al., 2018). In Section 2 we provide an overview of relevant research related to Complex Word Identification. Section 3 and 4 respectively introduce the CWI Shared Task 2018 dataset and present the text analysis tools and resources we exploited to characterize complex words. In Section 5 we describe the features we used to build our complex word clasIntroduction Automatic identification of complex words is a core component in several language-related areas of research, including Text Simplification (Saggion, 2017), Lexical Simplification (Bott et al., 2012), and Readability Assessment (CollinsThompson, 2"
W18-0517,I17-2068,0,0.0200126,"word frequencies are the most reliable predictor of word complexity, also high1 3 Dataset The organizers of CWI Shared Task 2018 released a training set and a development set of 27,299 and 3,328 words respectively, together with the sentence in which each word occurs. For each word, the binary complexity judgments of 20 human evaluators were provided (complex word or not complex word); ten of whom where native speakers. Similarly, CWI 2018 task testing dataset consisted of 4,252 words together with the sentence in which each word occurs. The datasets used in the shared task are described in (Yimam et al., 2017b) and (Yimam et al., 2017a) including the ones for the other tracks in this task. 4 Resources and Tools In order to identify complex words, we characterize each word by means of a set of lexical, semantic and contextual features, in addition to Word2Vec representations. To this purpose, we analyze both the word and the sentence in which it occurs by means of the language resources and text analysis tools described in what follows. 4.1 Language Resources To put the word embedding system in use we utilized a pre-trained word2vec model with 300 dimensions representing each vector in the vector s"
W18-0517,padro-stanilovsky-2012-freeling,0,0.0256445,"Missing"
W18-0517,yimam-etal-2017-multilingual,0,0.0222974,"word frequencies are the most reliable predictor of word complexity, also high1 3 Dataset The organizers of CWI Shared Task 2018 released a training set and a development set of 27,299 and 3,328 words respectively, together with the sentence in which each word occurs. For each word, the binary complexity judgments of 20 human evaluators were provided (complex word or not complex word); ten of whom where native speakers. Similarly, CWI 2018 task testing dataset consisted of 4,252 words together with the sentence in which each word occurs. The datasets used in the shared task are described in (Yimam et al., 2017b) and (Yimam et al., 2017a) including the ones for the other tracks in this task. 4 Resources and Tools In order to identify complex words, we characterize each word by means of a set of lexical, semantic and contextual features, in addition to Word2Vec representations. To this purpose, we analyze both the word and the sentence in which it occurs by means of the language resources and text analysis tools described in what follows. 4.1 Language Resources To put the word embedding system in use we utilized a pre-trained word2vec model with 300 dimensions representing each vector in the vector s"
W18-0517,W17-5910,0,0.0175886,"he more complex words a text has, the harder it becomes to read it. Lists of easy words (Dale and Chall, 1948), word characteristics (Kincaid et al., 1975; Gunning, 1952; Mc Laughlin, 1969), or word use in context (e.g. language models) (Si and Callan, 2001) are all techniques or resources which have been used to support the assessment of text readability: these approaches could also be used to evaluate word complexity. The CWI Shared Task 2018 is a follow up of the CWI shared task at SemEval 2016 - Task 11 1 reported by (Paetzold and Specia, 2016a) with the complementary evaluation paper by (Zampieri et al., 2017). 21 teams participated in the task submitting the total of 42 systems. The results concluded that word frequencies are the most reliable predictor of word complexity, also high1 3 Dataset The organizers of CWI Shared Task 2018 released a training set and a development set of 27,299 and 3,328 words respectively, together with the sentence in which each word occurs. For each word, the binary complexity judgments of 20 human evaluators were provided (complex word or not complex word); ten of whom where native speakers. Similarly, CWI 2018 task testing dataset consisted of 4,252 words together wi"
W19-4505,P17-1002,0,0.0197169,"with the 738 abstracts left in the SciDTB corpus when excluding the 60 abstracts annotated with arguments. This is done in order to avoid introducing a bias that would not reflect the results obtained when no discourse annotations are available. All the argument mining models (AFu, ATy, APa) are trained and evaluated in a 10-fold crossvalidation setting. In all cases the models are generated by means of bi-directional long short-term memory (BiLSTM) networks, as this type of architecture has proven to perform reasonably well in argument mining tasks across different classification scenarios (Eger et al., 2017). In order to simplify the experiments and the interpretation of their results we use the same architecture for all tasks: two layers of 100 recurrent units, Adam optimizer, naive dropout probability of 0.25 and a conditional random fields (CRF) classifier as the last layer of the network. We use, for the BiLSTMs, the implementation made available by the Ubiquitous Knowledge Processing Lab of the Technische Universit¨at Darmstadt (Reimers and Gurevych, 2017).9 As our intention is to compare the different approaches and not necessarily obtain the best possible models for these tasks, no hyperpa"
W19-4505,L16-1492,1,0.863466,"hich can identify different annotations with similar meaning, thus obtaining higher agreement than with standard measures. The evaluation of argument annotations is still an open issue. (Stab et al., 2014) suggest that it might be interesting to explore, for this task, evaluation schemes that are able to deal with multiple correct annotations such as those used in text summarization. (Lauscher et al., 2018b) analyze the information shared by rhetorical and argumentative structure of scientific documents. In order to do this, they add an argumentation layer to the DrInventor Scientific Corpus (Fisas et al., 2016), which includes 40 computer graphics papers annotated with four layers including citation contexts, rhetorical role of sentences, subjective information and summarization relevance. The enriched corpus is used to trained new models for the automatic identification of claims and evidence, which are made available as a web service (Lauscher et al., 2018a). Some of the first initiatives aimed at the automatic identification of rhetorical and argumentative components in scientific texts include the Argumentative Zoning (AZ) model (Teufel et al., 1999, 2009) and the CoreSC scheme (Liakata et al.,"
W19-4505,W15-0502,0,0.140862,"ettings and results and, in Section 5, we do the same with the experiments aimed at predicting the acceptance or rejection of papers in conferences. Finally, in Section 6, we summarize our main contributions and propose additional research avenues as follow-up to the current work. in texts, which is reflected in the low levels of inter-annotator agreement reached in general for this task (Habernal et al., 2014). If this is true in several knowledge domains, it poses a more difficult problem in the case of scientific texts due to their inherent argumentative complexity (Kirschner et al., 2015; Green, 2015). We propose to address this challenge by leveraging data annotated with discourse relations, as previous works suggest potential benefits in linking discourse analysis and argument mining tasks (Peldszus and Stede, 2016; Stab et al., 2014; Cabrio et al., 2013; Biran and Rambow, 2011; Green, 2015). 1.1 2 Related work This work is informed by previous research in the areas of argument mining, argumentation quality assessment and the relationship between discourse and argumentative structures and, from the methodological perspective, to transfer learning approaches. Due to space restrictions, we"
W19-4505,N18-1149,0,0.051432,"Missing"
W19-4505,W15-0501,0,0.120232,"ts, their experimental settings and results and, in Section 5, we do the same with the experiments aimed at predicting the acceptance or rejection of papers in conferences. Finally, in Section 6, we summarize our main contributions and propose additional research avenues as follow-up to the current work. in texts, which is reflected in the low levels of inter-annotator agreement reached in general for this task (Habernal et al., 2014). If this is true in several knowledge domains, it poses a more difficult problem in the case of scientific texts due to their inherent argumentative complexity (Kirschner et al., 2015; Green, 2015). We propose to address this challenge by leveraging data annotated with discourse relations, as previous works suggest potential benefits in linking discourse analysis and argument mining tasks (Peldszus and Stede, 2016; Stab et al., 2014; Cabrio et al., 2013; Biran and Rambow, 2011; Green, 2015). 1.1 2 Related work This work is informed by previous research in the areas of argument mining, argumentation quality assessment and the relationship between discourse and argumentative structures and, from the methodological perspective, to transfer learning approaches. Due to space re"
W19-4505,N16-1175,0,0.0720349,"Missing"
W19-4505,D15-1110,0,0.118269,"ed 112 argumentatively rich texts using RST and argumentation schemes in order to study the relationship between discourse and argumentation structures. The texts were generated in an experiment in which several participants wrote short texts of controlled linguistic and rhetoric complexity discussing a controversial issue from a pre-defined list. Based on this corpus, the authors conducted experiments in order to derive argumentative components and relations from RST trees, comparing three approaches: a transformation model, an aligner based on sub-graph matching and an evidence graph model (Peldszus and Stede, 2015b). Contributions • We propose to tackle the limitations posed by the lack of annotated data for argument mining in the scientific domain by leveraging existing Rhetorical Structure Theory (RST) (Mann et al., 1992) annotations in a corpus of computational linguistics abstracts (SciDTB) (Yang and Li, 2018). In order to do so: 1. We propose and test an annotation scheme that we use to conduct a pilot annotation experiment in which we enrich a subset of the SciDTB corpus with an additional layer of argumentative structures. 2. We explore the potential of a transfer learning approach to improve th"
W19-4505,W16-2812,0,0.21362,"and propose additional research avenues as follow-up to the current work. in texts, which is reflected in the low levels of inter-annotator agreement reached in general for this task (Habernal et al., 2014). If this is true in several knowledge domains, it poses a more difficult problem in the case of scientific texts due to their inherent argumentative complexity (Kirschner et al., 2015; Green, 2015). We propose to address this challenge by leveraging data annotated with discourse relations, as previous works suggest potential benefits in linking discourse analysis and argument mining tasks (Peldszus and Stede, 2016; Stab et al., 2014; Cabrio et al., 2013; Biran and Rambow, 2011; Green, 2015). 1.1 2 Related work This work is informed by previous research in the areas of argument mining, argumentation quality assessment and the relationship between discourse and argumentative structures and, from the methodological perspective, to transfer learning approaches. Due to space restrictions, we cannot cover in detail all the relevant background work. We refer the reader to (Lippi and Torroni, 2016) for a thorough summary of argument mining initiatives in various domains and with different approaches. (Wachsmut"
W19-4505,W18-5203,0,0.0461577,"Missing"
W19-4505,D14-1162,0,0.0822045,"nc DEmb+ELMo DEmb+ELMo+GloVe 10 20 30 40 50 60 70 80 90 100 Epoch 11 https://allennlp.org/elmo A true positive is considered when both the boundary and the type of the entity match. 13 The epochs before the 10th are not significant as the models have not had enough time to learn anything. 12 Figure 5: Trend lines for F1-measures in epochs 10100 for APa 46 In order to determine whether the better performance of the RST encoders is due to the knowledge conveyed by the task-specific representations we conducted an additional experiment in which we concatenated 200-dimensional GloVe embeddings14 (Pennington et al., 2014) (~g ) obtaining 1524-dimension embeddings [~k, ~e, ~g ] used as input of each of the argument mining models. In this case, the results obtained are mixed, with an increase in performance of 0.02 F1 points in average–for the epochs 10 to 100–for ATy, a worse performance of 0.01 F1 points for AFu and no difference in performance for APa. The models with the GloVe embeddings (DEmb+ELMo+GloVe) have, therefore, worse performances in average of 0.04, 0.02 and 0.02 F1 points for AFu, ATy and APa with respect to the models that include the embeddings obtained by means of the RST encoders. Figures 3,"
W19-4505,W18-5206,0,0.0638307,"Missing"
W19-4505,N18-1202,0,0.0219257,"r intention is to compare the different approaches and not necessarily obtain the best possible models for these tasks, no hyperparameter optimization is done in these experiments and, in all of the cases, the networks are trained for 100 epochs. All of the tasks are modeled as sequence labeling problems in which the tokens are tagged using the beginning-inside-outside (BIO) tagging scheme. The tokens are encoded as the concatenation of 300-dimensional dependency-based word embeddings (DEmb)10 (~k) (Komninos and Manandhar, 2016) and 1024-dimensional contextualized word embeddings (ELMo) (~e) (Peters et al., 2018). In these experiments we use the 5.5 billiontoken version of ELMo trained with Wikipedia and monolingual news from the WMT 2008-2012 Tasks We define the following set of argument mining tasks: • AFu (argumentative function): Identify the boundaries and argumentative functions of the components. In the example in Fig. 1, it would imply to identify the boundaries of the three nodes and the two support relations that link them. • ATy (argumentative unit): Identify the boundaries and types of the components. In the example, the proposal, assertion and a results units. • APa (argumentative attachm"
W19-4505,P14-1003,0,0.0207969,"nformation for the identification of argumentative components and relations we add a new annotation layer to the Discourse Dependency TreeBank for Scientific Abstracts (SciDTB) (Yang and Li, 2018). SciDTB contains 798 abstracts from the ACL Anthology (Radev et al., 2013) annotated with elementary discourse units (EDUs) and relations from the RST Framework. Polynary discourse relations in RST are binarized in SciDTB following a criteria similar to the “rightheavy” transformation used in other works that represent discourse structures as dependency trees (Morey et al., 2017; Stede et al., 2016; Li et al., 2014). We consider a subset of the SciDTB corpus consisting of 60 abstracts from the Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) and transformed them into a format suitable for the GraPAT graph annotation tool (Sonntag and Stede, 2014)3 , which had been previously tailored to the specificities of our proposed annotation scheme, described in Section 3.1. The corpus enriched with the argumentation4 level contains a total of 327 sentences, 8012 tokens, 862 discourse units and 352 argumentative units linked by 292 argumentative relations. 3.1 Annotatio"
W19-4505,liakata-etal-2010-corpora,0,0.0291238,"em at stake and/or the adequacy of the proposed approach. We introduce a fine-grained annotation scheme aimed at capturing information that accounts for the specificities of the scientific discourse, including the type of evidence that is offered to support a statement (e.g., background information, experimental data or interpretation of results). This can provide relevant information, for instance, to assess the argumentative strength of a text. The types of proposed units in our scheme were considered so they can be mapped–even if with a different level of granularity–to concepts in CoreSC (Liakata et al., 2010) and AZ categories, which would enable additional research on the potential of using existing annotated corpora for argument mining tasks. Like (Peldszus and Stede, 2016)–and in contrast with CoreSC and AZ–we consider EDUs as the minimal spans that can be annotated. Argumentative units can, in turn, cover multiple sentences. The proposed units include: • • • • • • Figure 1: Partial argumentative structure proposal (problem or approach) assertion (conclusion or known fact) result (interpretation of data) observation (data) means (implementation) description (definitions/other information) In li"
W19-4505,D17-1035,0,0.0229318,"Missing"
W19-4505,sonntag-stede-2014-grapat,0,0.0281268,"nnotated with elementary discourse units (EDUs) and relations from the RST Framework. Polynary discourse relations in RST are binarized in SciDTB following a criteria similar to the “rightheavy” transformation used in other works that represent discourse structures as dependency trees (Morey et al., 2017; Stede et al., 2016; Li et al., 2014). We consider a subset of the SciDTB corpus consisting of 60 abstracts from the Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) and transformed them into a format suitable for the GraPAT graph annotation tool (Sonntag and Stede, 2014)3 , which had been previously tailored to the specificities of our proposed annotation scheme, described in Section 3.1. The corpus enriched with the argumentation4 level contains a total of 327 sentences, 8012 tokens, 862 discourse units and 352 argumentative units linked by 292 argumentative relations. 3.1 Annotation scheme Several argumentation mining works (Lippi and Torroni, 2016) use claims and premises as basic argumentative units. In the case of scientific discourse, however, it is frequent to find that claims 3 http://angcl.ling.uni-potsdam.de/ resources/grapat.html 4 The annotations"
W19-4505,J17-3005,0,0.0557774,"m texts. The tasks involved in the extraction of arguments from text–including the identification of argumentative sentences, the detection of argument component boundaries and the prediction of argument structures–are related to other text mining tasks–including sequence labeling, text segmentation, entity recognition and relation extraction– which are in general tackled by means of supervised learning methods (Lippi and Torroni, 2016). The lack of annotated data with argumentative information, however, presents a challenge when trying to apply these well-known approaches to argument mining (Stab and Gurevych, 2017). This is so, in part, due to the inherent difficulty of unambiguously identifying argumentative elements In this work we propose to leverage resources available with discourse-level annotations to facilitate the identification of argumentative components and relations in scientific texts, which has been recognized as a particularly challenging task. In particular, we implement and evaluate a transfer learning approach in which contextualized representations learned from discourse parsing tasks are used as input of argument mining models. As a pilot application, we explore the feasibility of u"
W19-4505,D14-1033,0,0.0319136,"Missing"
W19-4505,D18-1402,0,0.114446,"rection, we plan to extend the coverage of the argumentative layer of annotations to the full SciDTB corpus. We expect this to become a valuable resource in argument mining research in scientific texts which, as mentioned, has been identified as a particularly challenging domain. The obtained results open several paths up for additional research, including the implementation of other transfer learning approaches–e.g., multitask learning settings20 –as well as other neural architectures–including attention-based architectures, which have proven to achieve good results in argument mining tasks (Stab et al., 2018). As mentioned in Section 3.1, we are also interested in exploring the possibility of leveraging other existing tools and resources to facilitate the automatic identification of argumentative structures and relations, such as corpora annotated with different schema–including variants of CoreSC and AZ. We also intend to expand our acceptance prediction experiments using the PeerRead dataset (Kang et al., 2018),21 which has a greater coverage than the NIPS and ICLR subsets that we used in our experiments. This dataset contains, in addition to the acceptance/rejection decisions, scores for differ"
W19-4505,L16-1167,0,0.12447,"t approaches in the context of computational argumentation and categorize them in relation to how they address logical, rhetorical and dialectical dimensions of argumentation. (Pan and Yang, 2010) provide an in-depth review of current trends in transfer learning, including inductive, transductive and unsupervised approaches. Furthermore, they classify the different approaches based on what is transferred: instances, feature representations, parameters or relational knowledge. A more direct antecedent to our work is the research conducted by Peldszus and Stede (Peldszus and Stede, 2016, 2015a; Stede et al., 2016), who annotated 112 argumentatively rich texts using RST and argumentation schemes in order to study the relationship between discourse and argumentation structures. The texts were generated in an experiment in which several participants wrote short texts of controlled linguistic and rhetoric complexity discussing a controversial issue from a pre-defined list. Based on this corpus, the authors conducted experiments in order to derive argumentative components and relations from RST trees, comparing three approaches: a transformation model, an aligner based on sub-graph matching and an evidence"
W19-4505,D09-1155,0,0.0707678,"Missing"
W19-4505,E17-1017,0,0.0275982,"de, 2016; Stab et al., 2014; Cabrio et al., 2013; Biran and Rambow, 2011; Green, 2015). 1.1 2 Related work This work is informed by previous research in the areas of argument mining, argumentation quality assessment and the relationship between discourse and argumentative structures and, from the methodological perspective, to transfer learning approaches. Due to space restrictions, we cannot cover in detail all the relevant background work. We refer the reader to (Lippi and Torroni, 2016) for a thorough summary of argument mining initiatives in various domains and with different approaches. (Wachsmuth et al., 2017) provide a comprehensive survey of quality assessment approaches in the context of computational argumentation and categorize them in relation to how they address logical, rhetorical and dialectical dimensions of argumentation. (Pan and Yang, 2010) provide an in-depth review of current trends in transfer learning, including inductive, transductive and unsupervised approaches. Furthermore, they classify the different approaches based on what is transferred: instances, feature representations, parameters or relational knowledge. A more direct antecedent to our work is the research conducted by P"
W19-4505,P18-2071,0,0.232285,"ersial issue from a pre-defined list. Based on this corpus, the authors conducted experiments in order to derive argumentative components and relations from RST trees, comparing three approaches: a transformation model, an aligner based on sub-graph matching and an evidence graph model (Peldszus and Stede, 2015b). Contributions • We propose to tackle the limitations posed by the lack of annotated data for argument mining in the scientific domain by leveraging existing Rhetorical Structure Theory (RST) (Mann et al., 1992) annotations in a corpus of computational linguistics abstracts (SciDTB) (Yang and Li, 2018). In order to do so: 1. We propose and test an annotation scheme that we use to conduct a pilot annotation experiment in which we enrich a subset of the SciDTB corpus with an additional layer of argumentative structures. 2. We explore the potential of a transfer learning approach to improve the performance of an argument mining model trained with a small volume of data annotated with the proposed scheme. • We report preliminary results on the prediction of acceptance or rejection of scientific papers in computer science conferences based on the automatic identification of argumentative compone"
yankova-etal-2008-framework,P98-1012,0,\N,Missing
yankova-etal-2008-framework,C98-1012,0,\N,Missing
yankova-etal-2008-framework,W03-0405,0,\N,Missing
yankova-etal-2008-framework,I08-1020,1,\N,Missing
