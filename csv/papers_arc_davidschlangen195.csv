2021.naacl-main.328,Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models,2021,-1,-1,3,0,4240,anne beyer,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, which allow for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective are capable of encoding. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence."
2021.emnlp-main.90,Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental {NLU},2021,-1,-1,3,0,8820,patrick kahardipraja,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs."
2021.alvr-1.7,Reference and coreference in situated dialogue,2021,-1,-1,3,0,4241,sharid loaiciga,Proceedings of the Second Workshop on Advances in Language and Vision Research,0,"In recent years several corpora have been developed for vision and language tasks. We argue that there is still significant room for corpora that increase the complexity of both visual and linguistic domains and which capture different varieties of perceptual and conversational contexts. Working with two corpora approaching this goal, we present a linguistic perspective on some of the challenges in creating and extending resources combining language and vision while preserving continuity with the existing best practices in the area of coreference annotation."
2021.acl-short.85,Targeting the Benchmark: On Methodology in Current Natural Language Processing Research,2021,-1,-1,1,1,4242,david schlangen,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"It has become a common pattern in our field: One group introduces a language task, exemplified by a dataset, which they argue is challenging enough to serve as a benchmark. They also provide a baseline model for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the pattern repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment from this pattern and work out possible argumentations and their parts."
2021.acl-long.546,Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer,2021,-1,-1,3,0,13487,fabian galetzka,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept. In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights. The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency. Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting."
2020.lrec-1.71,A Corpus of Controlled Opinionated and Knowledgeable Movie Discussions for Training Neural Conversation Models,2020,18,0,3,0,13487,fabian galetzka,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Fully data driven Chatbots for non-goal oriented dialogues are known to suffer from inconsistent behaviour across their turns, stemming from a general difficulty in controlling parameters like their assumed background personality and knowledge of facts. One reason for this is the relative lack of labeled data from which personality consistency and fact usage could be learned together with dialogue behaviour. To address this, we introduce a new labeled dialogue dataset in the domain of movie discussions, where every dialogue is based on pre-specified facts and opinions. We thoroughly validate the collected dialogue for adherence of the participants to their given fact and opinion profile, and find that the general quality in this respect is high. This process also gives us an additional layer of annotation that is potentially useful for training models. We introduce as a baseline an end-to-end trained self-attention decoder model trained on this data and show that it is able to generate opinionated responses that are judged to be natural and knowledgeable and show attentiveness."
2020.inlg-1.38,From {``}Before{''} to {``}After{''}: Generating Natural Language Instructions from Image Pairs in a Simple Visual Domain,2020,-1,-1,6,0,16790,robin rojowiec,Proceedings of the 13th International Conference on Natural Language Generation,0,"While certain types of instructions can be com-pactly expressed via images, there are situations where one might want to verbalise them, for example when directing someone. We investigate the task of Instruction Generation from Before/After Image Pairs which is to derive from images an instruction for effecting the implied change. For this, we make use of prior work on instruction following in a visual environment. We take an existing dataset, the BLOCKS data collected by Bisk et al. (2016) and investigate whether it is suitable for training an instruction generator as well. We find that it is, and investigate several simple baselines, taking these from the related task of image captioning. Through a series of experiments that simplify the task (by making image processing easier or completely side-stepping it; and by creating template-based targeted instructions), we investigate areas for improvement. We find that captioning models get some way towards solving the task, but have some difficulty with it, and future improvements must lie in the way the change is detected in the instruction."
2020.emnlp-main.26,Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental {NLU},2020,-1,-1,2,1,798,brielen madureira,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The {``}omni-directional{''} BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2."
W19-8621,Tell Me More: A Dataset of Visual Scene Description Sequences,2019,0,0,3,1,14068,nikolai ilinykh,Proceedings of the 12th International Conference on Natural Language Generation,0,"We present a dataset consisting of what we call image description sequences, which are multi-sentence descriptions of the contents of an image. These descriptions were collected in a pseudo-interactive setting, where the describer was told to describe the given image to a listener who needs to identify the image within a set of images, and who successively asks for more information. As we show, this setup produced nicely structured data that, we think, will be useful for learning models capable of planning and realising such description discourses."
W19-8653,Can Neural Image Captioning be Controlled via Forced Attention?,2019,0,0,3,0,19027,philipp sadler,Proceedings of the 12th International Conference on Natural Language Generation,0,"Learned dynamic weighting of the conditioning signal (attention) has been shown to improve neural language generation in a variety of settings. The weights applied when generating a particular output sequence have also been viewed as providing a potentially explanatory insight in the internal workings of the generator. In this paper, we reverse the direction of this connection and ask whether through the control of the attention of the model we can control its output. Specifically, we take a standard neural image captioning model that uses attention, and fix the attention to predetermined areas in the image. We evaluate whether the resulting output is more likely to mention the class of the object in that area than the normally generated caption. We introduce three effective methods to control the attention and find that these are producing expected results in up to 27.43{\%} of the cases."
W19-5938,From Explainability to Explanation: Using a Dialogue Setting to Elicit Annotations with Justifications,2019,0,1,3,0,23778,nazia attari,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"Despite recent attempts in the field of explainable AI to go beyond black box prediction models, typically already the training data for supervised machine learning is collected in a manner that treats the annotator as a {``}black box{''}, the internal workings of which remains unobserved. We present an annotation method where a task is given to a pair of annotators who collaborate on finding the best response. With this we want to shed light on the questions if the collaboration increases the quality of the responses and if this {``}thinking together{''} provides useful information in itself, as it at least partially reveals their reasoning steps. Furthermore, we expect that this setting puts the focus on explanation as a linguistic act, vs. explainability as a property of models. In a crowd-sourcing experiment, we investigated three different annotation tasks, each in a collaborative dialogical (two annotators) and monological (one annotator) setting. Our results indicate that our experiment elicits collaboration and that this collaboration increases the response accuracy. We see large differences in the annotators{'} behavior depending on the task. Similarly, we also observe that the dialog patterns emerging from the collaboration vary significantly with the task."
W19-0424,Natural Language Semantics With Pictures: Some Language {\\&} Vision Datasets and Potential Uses for Computational Semantics,2019,24,0,1,1,4242,david schlangen,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"Propelling, and propelled by, the {``}deep learning revolution{''}, recent years have seen the introduction of ever larger corpora of images annotated with natural language expressions. We survey some of these corpora, taking a perspective that reverses the usual directionality, as it were, by viewing the images as semantic annotation of the natural language expressions. We discuss datasets that can be derived from the corpora, and tasks of potential interest for computational semanticists that can be defined on those. In this, we make use of relations provided by the corpora (namely, the link between expression and image, and that between two expressions linked to the same image) and relations that we can add (similarity relations between expressions, or between images). Specifically, we show that in this way we can create data that can be used to learn and evaluate lexical and compositional grounded semantics, and we show that the {``}linked to same image{''} relation tracks a semantic implication relation that is recognisable to annotators even in the absence of the linking image as evidence. Finally, as an example of possible benefits of this approach, we show that an exemplar-model-based approach to implication beats a (simple) distributional space-based one on some derived datasets, while lending itself to explainability."
P19-1063,Know What You Don{'}t Know: Modeling a Pragmatic Speaker that Refers to Objects of Unknown Categories,2019,0,1,2,0.706633,1567,sina zarriess,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Zero-shot learning in Language {\&} Vision is the task of correctly labelling (or naming) objects of novel categories. Another strand of work in L{\&}V aims at pragmatically informative rather than {``}correct{''} object descriptions, e.g. in reference games. We combine these lines of research and model zero-shot reference games, where a speaker needs to successfully refer to a novel object in an image. Inspired by models of {``}rational speech acts{''}, we extend a neural generator to become a pragmatic speaker reasoning about uncertain object categories. As a result of this reasoning, the generator produces fewer nouns and names of distractor categories as compared to a literal speaker. We show that this conversational strategy for dealing with novel objects often improves communicative success, in terms of resolution accuracy of an automatic listener."
W18-6906,Being data-driven is not enough: Revisiting interactive instruction giving as a challenge for {NLG},2018,0,1,2,0.784933,1567,sina zarriess,Proceedings of the Workshop on {NLG} for Human{--}Robot Interaction,0,"Modeling traditional NLG tasks with data-driven techniques has been a major focus of research in NLG in the past decade. We argue that existing modeling techniques are mostly tailored to textual data and are not sufficient to make NLG technology meet the requirements of agents which target fluid interaction and collaboration in the real world. We revisit interactive instruction giving as a challenge for datadriven NLG and, based on insights from previous GIVE challenges, propose that instruction giving should be addressed in a setting that involves visual grounding and spoken language. These basic design decisions will require NLG frameworks that are capable of monitoring their environment as well as timing and revising their verbal output. We believe that these are core capabilities for making NLG technology transferrable to interactive systems."
W18-6547,The Task Matters: Comparing Image Captioning and Task-Based Dialogical Image Description,2018,0,2,3,1,14068,nikolai ilinykh,Proceedings of the 11th International Conference on Natural Language Generation,0,"Image captioning models are typically trained on data that is collected from people who are asked to describe an image, without being given any further task context. As we argue here, this context independence is likely to cause problems for transferring to task settings in which image description is bound by task demands. We demonstrate that careful design of data collection is required to obtain image descriptions which are contextually bounded to a particular meta-level task. As a task, we use MeetUp!, a text-based communication game where two players have the goal of finding each other in a visual environment. To reach this goal, the players need to describe images representing their current location. We analyse a dataset from this domain and show that the nature of image descriptions found in MeetUp! is diverse, dynamic and rich with phenomena that are not present in descriptions obtained through a simple image captioning task, which we ran for comparison."
W18-6563,Decoding Strategies for Neural Referring Expression Generation,2018,0,2,2,0.784933,1567,sina zarriess,Proceedings of the 11th International Conference on Natural Language Generation,0,"RNN-based sequence generation is now widely used in NLP and NLG (natural language generation). Most work focusses on how to train RNNs, even though also decoding is not necessarily straightforward: previous work on neural MT found seq2seq models to radically prefer short candidates, and has proposed a number of beam search heuristics to deal with this. In this work, we assess decoding strategies for referring expression generation with neural models. Here, expression length is crucial: output should neither contain too much or too little information, in order to be pragmatically adequate. We find that most beam search heuristics developed for MT do not generalize well to referring expression generation (REG), and do not generally outperform greedy decoding. We observe that beam search heuristics for termination seem to override the model{'}s knowledge of what a good stopping point is. Therefore, we also explore a recent approach called trainable decoding, which uses a small network to modify the RNN{'}s hidden state for better decoding results. We find this approach to consistently outperform greedy decoding for REG."
L18-1333,A Corpus of Natural Multimodal Spatial Scene Descriptions,2018,0,0,2,1,1566,ting han,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-5529,Beyond On-hold Messages: Conversational Time-buying in Task-oriented Dialogue,2017,13,3,3,0,31509,soledad gambino,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"A common convention in graphical user interfaces is to indicate a {``}wait state{''}, for example while a program is preparing a response, through a changed cursor state or a progress bar. What should the analogue be in a spoken conversational system? To address this question, we set up an experiment in which a human information provider (IP) was given their information only in a delayed and incremental manner, which systematically created situations where the IP had the turn but could not provide task-related information. Our data analysis shows that 1) IPs bridge the gap until they can provide information by re-purposing a whole variety of task- and grounding-related communicative actions (e.g. echoing the user{'}s request, signaling understanding, asserting partially relevant information), rather than being silent or explicitly asking for time (e.g. {``}please wait{''}), and that 2) IPs combined these actions productively to ensure an ongoing conversation. These results, we argue, indicate that natural conversational interfaces should also be able to manage their time flexibly using a variety of conversational resources."
W17-3509,Refer-i{TTS}: A System for Referring in Spoken Installments to Objects in Real-World Images,2017,10,0,3,0.865541,1567,sina zarriess,Proceedings of the 10th International Conference on Natural Language Generation,0,"Current referring expression generation systems mostly deliver their output as one-shot, written expressions. We present on-going work on incremental generation of spoken expressions referring to objects in real-world images. This approach extends upon previous work using the words-as-classifier model for generation. We implement this generator in an incremental dialogue processing framework such that we can exploit an existing interface to incremental text-to-speech synthesis. Our system generates and synthesizes referring expressions while continuously observing non-verbal user reactions."
P17-1023,Obtaining referential word meanings from visual and distributional information: Experiments on object naming,2017,29,4,2,0.865541,1567,sina zarriess,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We investigate object naming, which is an important sub-task of referring expression generation on real-world images. As opposed to mutually exclusive labels used in object recognition, object names are more flexible, subject to communicative preferences and semantically related to each other. Therefore, we investigate models of referential word meaning that link visual to lexical information which we assume to be given through distributional word embeddings. We present a model that learns individual predictors for object names that link visual and distributional aspects of word meaning during training. We show that this is particularly beneficial for zero-shot learning, as compared to projecting visual objects directly into the distributional space. In a standard object naming task, we find that different ways of combining lexical and visual information achieve very similar performance, though experiments on model combination suggest that they capture complementary aspects of referential meaning."
I17-2023,Natural Language Informs the Interpretation of Iconic Gestures: A Computational Approach,2017,11,1,3,1,1566,ting han,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"When giving descriptions, speakers often signify object shape or size with hand gestures. Such so-called {`}iconic{'} gestures represent their meaning through their relevance to referents in the verbal content, rather than having a conventional form. The gesture form on its own is often ambiguous, and the aspect of the referent that it highlights is constrained by what the language makes salient. We show how the verbal content guides gesture interpretation through a computational model that frames the task as a multi-label classification task that maps multimodal utterances to semantic categories, using annotated human-human data."
I17-2061,Draw and Tell: Multimodal Descriptions Outperform Verbal- or Sketch-Only Descriptions in an Image Retrieval Task,2017,0,0,2,1,1566,ting han,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"While language conveys meaning largely symbolically, actual communication acts typically contain iconic elements as well: People gesture while they speak, or may even draw sketches while explaining something. Image retrieval prima facie seems like a task that could profit from combined symbolic and iconic reference, but it is typically set up to work either from language only, or via (iconic) sketches with no verbal contribution. Using a model of grounded language semantics and a model of sketch-to-image mapping, we show that adding even very reduced iconic information to a verbal image description improves recall. Verbal descriptions paired with fully detailed sketches still perform better than these sketches alone. We see these results as supporting the assumption that natural user interfaces should respond to multimodal input, where possible, rather than just language alone."
E17-2014,"Is this a Child, a Girl or a Car? Exploring the Contribution of Distributional Similarity to Learning Referential Word Meanings",2017,0,6,2,0.865541,1567,sina zarriess,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"There has recently been a lot of work trying to use images of referents of words for improving vector space meaning representations derived from text. We investigate the opposite direction, as it were, trying to improve visual word predictors that identify objects in images, by exploiting distributional similarity information during training. We show that for certain words (such as entry-level nouns or hypernyms), we can indeed learn better referential word meanings by taking into account their semantic similarity to other words. For other words, there is no or even a detrimental effect, compared to a learning setup that presents even semantically related objects as negative instances."
E17-2079,Grounding Language by Continuous Observation of Instruction Following,2017,17,2,2,1,1566,ting han,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Grounded semantics is typically learnt from utterance-level meaning representations (e.g., successful database retrievals, denoted objects in images, moves in a game). We explore learning word and utterance meanings by continuous observation of the actions of an instruction follower (IF). While an instruction giver (IG) provided a verbal description of a configuration of objects, IF recreated it using a GUI. Aligning these GUI actions to sub-utterance chunks allows a simple maximum entropy model to associate them as chunk meaning better than just providing it with the utterance-final configuration. This shows that semantics useful for incremental (word-by-word) application, as required in natural dialogue, might also be better acquired from incremental settings."
E17-1031,"Joint, Incremental Disfluency Detection and Utterance Segmentation from Speech",2017,0,3,2,1,1530,julian hough,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We present the joint task of incremental disfluency detection and utterance segmentation and a simple deep learning system which performs it on transcripts and ASR results. We show how the constraints of the two tasks interact. Our joint-task system outperforms the equivalent individual task systems, provides competitive results and is suitable for future use in conversation agents in the psychiatric domain."
D17-1100,Deriving continous grounded meaning representations from referentially structured multimodal contexts,2017,22,0,2,0.865541,1567,sina zarriess,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Corpora of referring expressions paired with their visual referents are a good source for learning word meanings directly grounded in visual representations. Here, we explore additional ways of extracting from them word representations linked to multi-modal context: through expressions that refer to the same object, and through expressions that refer to different objects in the same scene. We show that continuous meaning representations derived from these contexts capture complementary aspects of similarity, , even if not outperforming textual embeddings trained on very large amounts of raw text when tested on standard similarity benchmarks. We propose a new task for evaluating grounded meaning representations{---}detection of potentially co-referential phrases{---}and show that it requires precise denotational representations of attribute meanings, which our method provides."
W16-6642,Towards Generating Colour Terms for Referents in Photographs: Prefer the Expected or the Unexpected?,2016,22,3,2,0.993919,1567,sina zarriess,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-3630,Real-Time Understanding of Complex Discriminative Scene Descriptions,2016,17,5,4,0,1597,ramesh manuvinakurike,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
W16-3631,Supporting Spoken Assistant Systems with a Graphical User Interface that Signals Incremental Understanding and Prediction State,2016,11,4,2,1,821,casey kennington,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
W16-3632,Toward incremental dialogue act segmentation in fast-paced interactive dialogue systems,2016,28,7,4,0,1597,ramesh manuvinakurike,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"In this paper, we present and evaluate an approach to incremental dialogue act (DA) segmentation and classification. Our approach utilizes prosodic, lexico-syntactic and contextual features, and achieves an encouraging level of performance in offline corpus-based evaluation as well as in simulated human-agent dialogues. Our approach uses a pipeline of sequential processing steps, and we investigate the contribution of different processing steps to DA segmentation errors. We present our results using both existing and new metrics for DA segmentation. The incremental DA segmentation capability described here may help future systems to allow more natural speech from users and enable more natural patterns of interaction."
W16-3637,"Investigating Fluidity for Human-Robot Interaction with Real-time, Real-world Grounding Strategies",2016,1,6,2,1,1530,julian hough,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,None
P16-1058,Easy Things First: Installments Improve Referring Expression Generation for Objects in Photographs,2016,33,6,2,0.993919,1567,sina zarriess,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
P16-1115,Resolving References to Objects in Photographs using the Words-As-Classifiers Model,2016,29,6,1,1,4242,david schlangen,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"A common use of language is to refer to visually present objects. Modelling it in computers requires modelling the link between language and perception. The xe2x80x9cwords as classifiersxe2x80x9d model of grounded semantics views words as classifiers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words. It was recently shown to perform well in a game-playing scenario with a small number of object types. We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available. Using a pre-trained convolutional neural network to extract image region features, and augmenting these with positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, find bounding box of its referent), while, as we argue, being conceptually simpler and more flexible."
L16-1019,{P}ento{R}ef: A Corpus of Spoken References in Task-oriented Dialogues,2016,17,9,7,0.993919,1567,sina zarriess,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"PentoRef is a corpus of task-oriented dialogues collected in systematically manipulated settings. The corpus is multilingual, with English and German sections, and overall comprises more than 20000 utterances. The dialogues are fully transcribed and annotated with referring expressions mapped to objects in corresponding visual scenes, which makes the corpus a rich resource for research on spoken referring expressions in generation and resolution. The corpus includes several sub-corpora that correspond to different dialogue situations where parameters related to interactivity, visual access, and verbal channel have been manipulated in systematic ways. The corpus thus lends itself to very targeted studies of reference in spontaneous dialogue."
L16-1281,"{DUEL}: A Multi-lingual Multimodal Dialogue Corpus for Disfluency, Exclamations and Laughter",2016,0,1,6,1,1530,julian hough,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present the DUEL corpus, consisting of 24 hours of natural, face-to-face, loosely task-directed dialogue in German, French and Mandarin Chinese. The corpus is uniquely positioned as a cross-linguistic, multimodal dialogue resource controlled for domain. DUEL includes audio, video and body tracking data and is transcribed and annotated for disfluency, laughter and exclamations."
L16-1549,How to Address Smart Homes with a Social Robot? A Multi-modal Corpus of User Interactions with an Intelligent Environment,2016,14,20,18,0,34969,patrick holthaus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In order to explore intuitive verbal and non-verbal interfaces in smart environments we recorded user interactions with an intelligent apartment. Besides offering various interactive capabilities itself, the apartment is also inhabited by a social robot that is available as a humanoid interface. This paper presents a multi-modal corpus that contains goal-directed actions of naive users in attempts to solve a number of predefined tasks. Alongside audio and video recordings, our data-set consists of large amount of temporally aligned sensory data and system behavior provided by the environment and its interactive components. Non-verbal system responses such as changes in light or display contents, as well as robot and apartment utterances and gestures serve as a rich basis for later in-depth analysis. Manual annotations provide further information about meta data like the current course of study and user behavior including the incorporated modality, all literal utterances, language features, emotional expressions, foci of attention, and addressees."
W15-4705,Reading Times Predict the Quality of Generated Text Above and Beyond Human Ratings,2015,34,3,3,0.993919,1567,sina zarriess,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"Typically, human evaluation of NLG output is based on user ratings. We collected ratings and reading time data in a simple, low-cost experimental paradigm for text generation. Participants were presented corpus texts, automatically linearised texts, and texts containing predicted referring expressions and automatic linearisation. We demonstrate that the reading time metrics outperform the ratings in classifying texts according to their quality. Regression analyses showed that self-reported ratings discriminated poorly between the kinds of manipulation, especially between defects in word order and text coherence. In contrast, a combination of objective measures from the low-cost mouse contingent reading paradigm provided very high classification accuracy and thus, greater insight into the actual quality of an automatically generated text."
W15-0124,A Discriminative Model for Perceptually-Grounded Incremental Reference Resolution,2015,25,3,3,1,821,casey kennington,Proceedings of the 11th International Conference on Computational Semantics,0,"A large part of human communication involves referring to entities in the world, and often these entities are objects that are visually present for the interlocutors. A computer system that aims to resolve such references needs to tackle a complex task: objects and their visual features must be determined, the referring expressions must be recognised, extra-linguistic information such as eye gaze or pointing gestures must be incorporated xe2x80x94 and the intended connection between words and world must be reconstructed. In this paper, we introduce a discriminative model of reference resolution that processes incrementally (i.e., word for word), is perceptually-grounded, and improves when interpolated with information from gaze and pointing gestures. We evaluated our model and found that it performed robustly in a realistic reference resolution task, when compared to a generative model."
W15-0125,"Incremental Semantics for Dialogue Processing: Requirements, and a Comparison of Two Approaches",2015,24,6,3,1,1530,julian hough,Proceedings of the 11th International Conference on Computational Semantics,0,"Truly interactive dialogue systems need to construct meaning on at least a word-byword basis. We propose desiderata for incremental semantics for dialogue models and systems, a task not heretofore attempted thoroughly. After laying out the desirable properties we illustrate how they are met by current approaches, comparing two incremental semantic processing frameworks: Dynamic Syntax enriched with Type Theory with Records (DS-TTR) and Robust Minimal Recursion Semantics with incremental processing (RMRS-IP). We conclude these approaches are not significantly different with regards to their semantic representation construction, however their purported role within semantic models and dialogue models is where they diverge."
P15-1029,Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution,2015,24,33,2,1,821,casey kennington,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"An elementary way of using language is to refer to objects. Often, these objects are physically present in the shared environment and reference is done via mention of perceivable properties of the objects. This is a type of language use that is modelled well neither by logical semantics nor by distributional semantics, the former focusing on inferential relations between expressed propositions, the latter on similarity relations between words or phrases. We present an account of word and phrase meaning that is perceptually grounded, trainable, compositional, and xe2x80x98dialogueplausiblexe2x80x99 in that it computes meanings word-by-word. We show that the approach performs well (with an accuracy of 65% on a 1-out-of-32 reference resolution task) on direct descriptions and target/landmark descriptions, even when trained with less than 800 training examples and automatically transcribed utterances."
N15-1031,Incrementally Tracking Reference in Human/Human Dialogue Using Linguistic and Extra-Linguistic Information,2015,35,6,4,1,821,casey kennington,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"A large part of human communication involves referring to entities in the world and often these entities are objects that are visually present for the interlocutors. A system that aims to resolve such references needs to tackle a complex task: objects and their visual features need to be determined, the referring expressions must be recognised, and extra-linguistic information such as eye gaze or pointing gestures need to be incorporated. Systems that can make use of such information sources exist, but have so far only been tested under very constrained settings, such as WOz interactions. In this paper, we apply to a more complex domain a reference resolution model that works incrementally (i.e., word by word), grounds words with visually present properties of objects (such as shape and size), and can incorporate extra-linguistic information. We find that the model works well compared to previous work on the same data, despite using fewer features. We conclude that the model shows potential for use in a realtime interactive dialogue system."
W14-4312,{I}npro{TK}s: A Toolkit for Incremental Situated Processing,2014,9,9,3,1,821,casey kennington,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,"In order to process incremental situated dialogue, it is necessary to accept information from various sensors, each tracking, in real-time, different aspects of the physical situation. We present extensions of the incremental processing toolkit INPROTK which make it possible to plug in such multimodal sensors and to achieve situated, real-time dialogue. We also describe a new module which enables the use in INPROTK of the Google Web Speech API, which offers speech recognition with a very large vocabulary and a wide choice of languages. We illustrate the use of these extensions with a description of two systems handling different situated settings."
W14-0212,"Situationally Aware In-Car Information Presentation Using Incremental Speech Generation: Safer, and More Effective",2014,9,12,6,1,35012,spyros kousidis,Proceedings of the {EACL} 2014 Workshop on Dialogue in Motion,0,"Holding non-co-located conversations while driving is dangerous (Horrey and Wickens, 2006; Strayer et al., 2006), much more so than conversations with physically present, xe2x80x9csituatedxe2x80x9d interlocutors (Drews et al., 2004). In-car dialogue systems typically resemble non-co-located conversations more, and share their negative impact (Strayer et al., 2013). We implemented and tested a simple strategy for making in-car dialogue systems aware of the driving situation, by giving them the capability to interrupt themselves when a dangerous situation is detected, and resume when over. We show that this improves both driving performance and recall of system-presented information, compared to a non-adaptive strategy."
C14-1170,"Situated Incremental Natural Language Understanding using a Multimodal, Linguistically-driven Update Model",2014,34,6,3,1,821,casey kennington,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"A common site of language use is interactive dialogue between two people situated together in shared time and space. In this paper, we present a statistical model for understanding natural human language that works incrementally (i.e., does not wait until the end of an utterance to begin processing), and is grounded by linking semantic entities with objects in a shared space. We describe our model, show how a semantic meaning representation is grounded with properties of real-world objects, and further show that it can ground with embodied, interactive cues such as pointing gestures or eye gaze."
W13-4030,"Interpreting Situated Dialogue Utterances: an Update Model that Uses Speech, Gaze, and Gesture Information",2013,31,28,3,1,821,casey kennington,Proceedings of the {SIGDIAL} 2013 Conference,0,"In situated dialogue, speakers share time and space. We present a statistical model for understanding natural language that works incrementally (i.e., in real, shared time) and is grounded (i.e., links to entities in the shared space). We describe our model with an example, then establish that our model works well on nonsituated, telephony application-type utterances, show that it is effective in grounding language in a situated environment, and further show that it can make good use of embodied cues such as gaze and pointing in a fully multi-modal setting."
W13-4042,"Open-ended, Extensible System Utterances Are Preferred, {E}ven If They Require Filled Pauses",2013,12,3,2,1,21478,timo baumann,Proceedings of the {SIGDIAL} 2013 Conference,0,"In many environments (e. g. sports commentary), situations incrementally unfold over time and often the future appearance of a relevant event can be predicted, but not in all its details or precise timing. We have built a simulation framework that uses our incremental speech synthesis component to assemble in a timely manner complex commentary utterances. In our evaluation, the resulting output is preferred over that from a baseline system that uses a simpler commenting strategy. Even in cases where the incremental system overcommits temporally and requires a filled pause to wait for the upcoming event, the system is preferred over the baseline."
W13-4048,Investigating speaker gaze and pointing behaviour in human-computer interaction with the mint.tools collection,2013,10,9,3,1,35012,spyros kousidis,Proceedings of the {SIGDIAL} 2013 Conference,0,"Can speaker gaze and speaker arm movements be used as a practical information source for naturalistic conversational humanxe2x80x90computer interfaces? To investigate this question, we recorded (with eye tracking and motion capture) a corpus of interactions with a (wizarded) system. In this paper, we describe the recording, analysis infrastructure that we built for such studies, and analysis we performed on these data. We find that with some initial calibration, a xe2x80x9cminimally invasivexe2x80x9d, stationary camera-based setting provides data of sufficient quality to support interaction."
W12-4705,Incremental Construction of Robust but Deep Semantic Representations for Use in Responsive Dialogue Systems,2012,40,0,2,0,28019,andreas peldszus,Proceedings of the Workshop on Advances in Discourse Analysis and its Computational Aspects,0,"It is widely acknowledged that current dialogue systems are held back by a lack of flexibility, both in their turn-taking model (typically, allowing only a strict back-and-forth between user and system) and in their interpretation capabilities (typically, restricted to slot filling). We have developed a component for NLU that attempts to address both of these challenges, by a) constructing robust but deep meaning representations that support a range of further user intention determination techniques from inference / reasoning-based ones to ones based on more basic structures, and b) constructing these representations incrementally and hence providing semantic information on which system reactions can be based concurrently to the ongoing user utterance. The approach is based on an existing semantic representation formalism, Robust Minimal Recursion Semantics, which we have modified to suit incremental construction. We present the modifications, our implementation, and discuss applications within a dialogue system context, showing that the approach indeed promises to meet the requirements for more"
W12-1806,"The Future of Spoken Dialogue Systems is in their Past: Long-Term Adaptive, Conversational Assistants",2012,8,0,1,1,4242,david schlangen,{NAACL}-{HLT} Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data ({SDCTD} 2012),0,None
W12-1814,The {I}npro{TK} 2012 release,2012,15,45,2,1,21478,timo baumann,{NAACL}-{HLT} Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data ({SDCTD} 2012),0,"We describe the 2012 release of our Incremental Processing Toolkit (InproTK), which combines a powerful and extensible architecture for incremental processing with components for incremental speech recognition and, new to this release, incremental speech synthesis. These components work fairly domain-independently; we also provide example implementations of higher-level components such as natural language understanding and dialogue management that are somewhat more tied to a particular domain. We offer this release of the toolkit to foster research in this new and exciting area, which promises to help increase the naturalness of behaviours that can be modelled in such systems."
W12-1641,Combining Incremental Language Generation and Incremental Speech Synthesis for Adaptive Information Presentation,2012,24,34,5,0,5992,hendrik buschmeier,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Participants in a conversation are normally receptive to their surroundings and their interlocutors, even while they are speaking and can, if necessary, adapt their ongoing utterance. Typical dialogue systems are not receptive and cannot adapt while uttering. We present combinable components for incremental natural language generation and incremental speech synthesis and demonstrate the flexibility they can achieve with an example system that adapts to a listener's acoustic understanding problems by pausing, repeating and possibly rephrasing problematic parts of an utterance. In an evaluation, this system was rated as significantly more natural than two systems representing the current state of the art that either ignore the interrupting event or just pause; it also has a lower response time."
W12-1643,{M}arkov {L}ogic {N}etworks for Situated Incremental Natural Language Understanding,2012,22,13,2,1,821,casey kennington,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"We present work on understanding natural language in a situated domain, that is, language that possibly refers to visually present entities, in an incremental, word-by-word fashion. Such type of understanding is required in conversational systems that need to act immediately on language input, such as multi-modal systems or dialogue systems for robots. We explore a set of models specified as Markov Logic Networks, and show that a model that has access to information about the visual context of an utterance, its discourse context, as well as the linguistic structure of the utterance performs best. We explore its incremental properties, and also its use in a joint parsing and understanding module. We conclude that mlns offer a promising framework for specifying such models in a general, possibly domain-independent way."
P12-3018,{INPRO}{\\_}i{SS}: A Component for Just-In-Time Incremental Speech Synthesis,2012,17,29,2,1,21478,timo baumann,Proceedings of the {ACL} 2012 System Demonstrations,0,"We present a component for incremental speech synthesis (iSS) and a set of applications that demonstrate its capabilities. This component can be used to increase the responsivity and naturalness of spoken interactive systems. While iSS can show its full strength in systems that generate output incrementally, we also discuss how even otherwise unchanged systems may profit from its capabilities."
E12-1052,Joint Satisfaction of Syntactic and Pragmatic Constraints Improves Incremental Spoken Language Understanding,2012,20,17,4,0,28019,andreas peldszus,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a model of semantic processing of spoken language that (a) is robust against ill-formed input, such as can be expected from automatic speech recognisers, (b) respects both syntactic and pragmatic constraints in the computation of most likely interpretations, (c) uses a principled, expressive semantic representation formalism (RMRS) with a well-defined model theory, and (d) works continuously (producing meaning representations on a word-by-word basis, rather than only for full utterances) and incrementally (computing only the additional contribution by the new word, rather than re-computing for the whole utterance-so-far).n n We show that the joint satisfaction of syntactic and pragmatic constraints improves the performance of the NLU component (around 10 % absolute, over a syntax-only baseline)."
W11-2015,Predicting the Micro-Timing of User Input for an Incremental Spoken Dialogue System that Completes a User{'}s Ongoing Turn,2011,32,12,2,1,21478,timo baumann,Proceedings of the {SIGDIAL} 2011 Conference,0,"We present the novel task of predicting temporal features of continuations of user input, while that input is still ongoing. We show that the remaining duration of an ongoing word, as well as the duration of the next can be predicted reasonably well, and we put this information to use in a system that synchronously completes a user's speech. While we focus on collaborative completions, the techniques presented here may also be useful for the alignment of back-channels and immediate turn-taking in an incremental SDS, or to synchronously monitor the user's speech fluency for other reasons."
W10-4302,Comparing Local and Sequential Models for Statistical Incremental Natural Language Understanding,2010,22,26,3,0,45070,silvan heintze,Proceedings of the {SIGDIAL} 2010 Conference,0,"Incremental natural language understanding is the task of assigning semantic representations to successively larger prefixes of utterances. We compare two types of statistical models for this task: a) local models, which predict a single class for an input; and b), sequential models, which align a sequence of classes to a sequence of input tokens. We show that, with some modifications, the first type of model can be improved and made to approximate the output of the second, even though the latter is more informative. We show on two different data sets that both types of model achieve comparable performance (significantly better than a baseline), with the first type requiring simpler training data. Results for the first type of model have been reported in the literature; we show that for our kind of data our more sophisticated variant of the model performs better."
W10-4308,Middleware for Incremental Processing in Conversational Agents,2010,3,22,1,1,4242,david schlangen,Proceedings of the {SIGDIAL} 2010 Conference,0,"We describe work done at three sites on designing conversational agents capable of incremental processing. We focus on the 'middleware' layer in these systems, which takes care of passing around and maintaining incremental information between the modules of such agents. All implementations are based on the abstract model of incremental dialogue processing proposed by Schlangen and Skantze (2009), and the paper shows what different instantiations of the model can look like given specific requirements and application areas."
W10-4342,Collaborating on Utterances with a Spoken Dialogue System Using an {ISU}-based Approach to Incremental Dialogue Management,2010,14,33,3,0,43595,okko buss,Proceedings of the {SIGDIAL} 2010 Conference,0,"When dialogue systems, through the use of incremental processing, are not bounded anymore by strict, non-overlapping turn-taking, a whole range of additional interactional devices becomes available. We explore the use of one such device, trial intonation. We elaborate our approach to dialogue management in incremental systems, based on the Information-State-Update approach, and discuss an implementation in a microdomain that lends itself to the use of immediate feedback, trial intonations and expansions. In an overhearer evaluation, the incremental system was judged as significantly more human-like and reactive than a non-incremental version."
W09-3905,"Incremental Reference Resolution: The Task, Metrics for Evaluation, and a {B}ayesian Filtering Model that is Sensitive to Disfluencies",2009,16,46,1,1,4242,david schlangen,Proceedings of the {SIGDIAL} 2009 Conference,0,"In this paper we do two things: a) we discuss in general terms the task of incremental reference resolution (IRR), in particular resolution of exophoric reference, and specify metrics for measuring the performance of dialogue system components tackling this task, and b) we present a simple Bayesian filtering model of IRR that performs reasonably well just using words directly (no structure information and no hand-coded semantics): it picks the right referent out of 12 for around 50% of real-world dialogue utterances in our test corpus. It is also able to learn to interpret not only words but also hesitations, just as humans have shown to do in similar situations, namely as markers of references to hard-to-describe entities."
W09-3943,{TELIDA}: A Package for Manipulation and Visualization of Timed Linguistic Data,2009,9,7,3,0,42129,titus malsburg,Proceedings of the {SIGDIAL} 2009 Conference,0,"We present a toolkit for manipulating and visualising time-aligned linguistic data such as dialogue transcripts or language processing data. The package complements existing editing tools by allowing for conversion between their formats, information extraction from the raw files, and by adding sophisticated, and easily extended methods for visualising the dynamics of dialogue processing. To illustrate the versatility of the package, we describe its use in three different projects at our site."
W09-0509,{RUBISC} - a Robust Unification-Based Incremental Semantic Chunker,2009,20,6,2,1,46804,michaela atterer,"Proceedings of {SRSL} 2009, the 2nd Workshop on Semantic Representation of Spoken Language",0,"We present RUBISC, a new incremental chunker that can perform incremental slot filling and revising as it receives a stream of words. Slot values can influence each other via a unification mechanism. Chunks correspond to sense units, and end-of-sentence detection is done incrementally based on a notion of semantic/pragmatic completeness. One of RUBISC's main fields of application is in dialogue systems where it can contribute to responsiveness and hence naturalness, because it can provide a partial or complete semantics of an utterance while the speaker is still speaking. The chunker is evaluated on a German transcribed speech corpus and achieves a concept error rate of 43.3% and an F-Score of 81.5."
N09-1043,Assessing and Improving the Performance of Speech Recognition for Incremental Systems,2009,7,42,3,1,21478,timo baumann,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In incremental spoken dialogue systems, partial hypotheses about what was said are required even while the utterance is still ongoing. We define measures for evaluating the quality of incremental ASR components with respect to the relative correctness of the partial hypotheses compared to hypotheses that can optimize over the complete input, the timing of hypothesis formation relative to the portion of the input they are about, and hypothesis stability, defined as the number of times they are revised. We show that simple incremental post-processing can improve stability dramatically, at the cost of timeliness (from 90 % of edits of hypotheses being spurious down to 10 % at a lag of 320 ms). The measures are not independent, and we show how system designers can find a desired operating point for their ASR. To our knowledge, we are the first to suggest and examine a variety of measures for assessing incremental ASR and improve performance on this basis."
E09-1081,"A General, Abstract Model of Incremental Dialogue Processing",2009,33,117,1,1,4242,david schlangen,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We present a general model and conceptual framework for specifying architectures for incremental processing in dialogue systems, in particular with respect to the topology of the network of modules that make up the system, the way information flows through this network, how information increments are 'packaged', and how these increments are processed by the modules. This model enables the precise specification of incremental systems and hence facilitates detailed comparisons between systems, as well as giving guidance on designing new systems."
E09-1085,Incremental Dialogue Processing in a Micro-Domain,2009,26,110,2,0,1563,gabriel skantze,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"This paper describes a fully incremental dialogue system that can engage in dialogues in a simple domain, number dictation. Because it uses incremental speech recognition and prosodic analysis, the system can give rapid feedback as the user is speaking, with a very short latency of around 200ms. Because it uses incremental speech synthesis and self-monitoring, the system can react to feedback from the user as the system is speaking. A comparative evaluation shows that naive users preferred this system over a non-incremental version, and that it was perceived as more human-like."
W08-0113,A Simple Method for Resolution of Definite Reference in a Shared Visual Context,2008,8,16,2,0,47833,alexander siebert,Proceedings of the 9th {SIG}dial Workshop on Discourse and Dialogue,0,"We present a method for resolving definite exophoric reference to visually shared objects that is based on a) an automatically learned, simple mapping of words to visual features (visual word semantics), b) an automatically learned, semantically-motivated utterance segmentation (visual grammar), and c) a procedure that, given an utterance, uses b) to combine a) to yield a resolution. We evaluated the method both on a pre-recorded corpus and in an online setting, where it performed with 81% (chance: 14%) and 66% accuracy, respectively. This is comparable to results reported in related work on simpler settings."
C08-2003,Towards Incremental End-of-Utterance Detection in Dialogue Systems,2008,13,19,3,1,46804,michaela atterer,Coling 2008: Companion volume: Posters,0,"We define the task of incremental or 0lag utterance segmentation, that is, the task of segmenting an ongoing speech recognition stream into utterance units, and present first results. We use a combination of hidden event language model, features from an incremental parser, and acoustic / prosodic features to train classifiers on real-world conversational data (from the Switchboard corpus). The best classifiers reach an F-score of around 56%, improving over baseline and related work."
2007.sigdial-1.9,An Implemented Method for Distributed Collection and Assessment of Speech Data,2007,8,3,2,0,47833,alexander siebert,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"We present an approach to decreasing the cost of collecting speech data by a) distributing experimental setups as a downloadable computer program that records data and sends it back to an experiment server and b) by xe2x80x98re-usingxe2x80x99 subjects for instant quality evaluation of the collected data. As an example of the kind of settings in which this approach can be used, we also shortly describe an experiment we have conducted; evaluation of the collected data showed no negative effect of the xe2x80x98unsupervisedxe2x80x99 collection method."
2007.sigdial-1.10,Beyond Repair {--} Testing the Limits of the Conversational Repair System,2007,5,5,1,1,4242,david schlangen,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"We report on an experiment on the effects of inducing acoustic understanding problems in task-oriented dialogue. We found that despite causing real problems w.r.t. task performance, many instances of induced problems were not explicitly repaired by the dialogue participants. Almost all repairs referred to the immediately preceding utterance, with problems in prior utterances left unacknowledged. Clarification requests of certain forms were in this corpus more likely to trigger reformulations than repetitions, unlike in different settings."
2007.sigdial-1.25,Referring under Restricted Interactivity Conditions,2007,9,13,3,0,936,raquel fernandez,Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue,0,"We report results on how the collaborative process of referring in task-oriented dialogue is aected by the restrictive interactivity of a turn-taking policy commonly used in dialogue systems, namely push-to-talk. Our findings show that the restriction did not have a negative effect. Instead, the stricter control imposed at the interaction level favoured longer, more eective referring expressions, and induced a stricter and more structured performance at the level of the task."
P05-1031,Towards Finding and Fixing {F}ragments{---}{U}sing {ML} to Identify Non-Sentential Utterances and their Antecedents in Multi-Party Dialogue,2005,19,4,1,1,4242,david schlangen,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"Non-sentential utterances (e.g., short-answers as in Who came to the party?--- Peter.) are pervasive in dialogue. As with other forms of ellipsis, the elided material is typically present in the context (e.g., the question that a short answer answers). We present a machine learning approach to the novel task of identifying fragments and their antecedents in multiparty dialogue. We compare the performance of several learning algorithms, using a mixture of structural and lexical features, and show that the task of identifying antecedents given a fragment can be learnt successfully (f(0.5) = .76); we discuss why the task of identifying fragments is harder (f(0.5) = .41) and finally report on a combined task (f(0.5) = .38)."
W04-2325,Causes and Strategies for Requesting Clarification in Dialogue,2004,12,45,1,1,4242,david schlangen,Proceedings of the 5th {SIG}dial Workshop on Discourse and Dialogue at {HLT}-{NAACL} 2004,0,"We do two things in this paper. First, we present a model of possible causes for requesting clarifications in dialogue, i.e., we classify types of non-understandings that lead to clarifications. For this we make more precise the models of communication of (Clark, 1996) and (Allwood, 1995), relating them to an independently motivated theory of discourse semantics, SDRT (Asher and Lascarides, 2003). As we show, the lack of such a model is a problem for extant analyses of clarification moves. Second, we combine this model with an extended notion of xe2x80x9cconfidence scorexe2x80x9d that combines speech recognition confidence with different kinds of semantic and pragmatic confidence, and argue that the resulting processing model can produce a more natural clarification and confirmation behaviour than that of current dialogue systems. We close with a description of an experimental implementation of the model."
W04-0607,Feeding {OWL}: Extracting and Representing the Content of Pathology Reports,2004,12,17,1,1,4242,david schlangen,Proceeedings of the Workshop on {NLP} and {XML} ({NLPXML}-2004): {RDF}/{RDFS} and {OWL} in Language Technology,0,"This paper reports on an ongoing project that combines NLP with semantic web technologies to support a content-based storage and retrieval of medical pathology reports. We describe the NLP component of the project (a robust parser) and the background knowledge component (a domain ontology represented in OWL), and how they work together during extraction of domain specific information from natural language reports. The system provides a good example of how NLP techniques can be used to populate the Semantic Web."
W03-2106,The interpretation of non-sentential utterances in dialogue,2003,22,21,1,1,4242,david schlangen,Proceedings of the Fourth {SIG}dial Workshop of Discourse and Dialogue,0,None
