2020.semeval-1.128,L18-1269,0,0.0181996,"r th i position is ei = γ l=0 αl el,i , where γ is a scaling factor, αl is the weight of the l layer, L is the total number of layers, and l = 0 corresponds to the embedding layer. 4.2 Task Specific Regression As mentioned at the beginning of the section, contrast and surprise is the key to humor. To represent the pairwise relationship between two vectors, we derive feature from h = f (x, y) = [x; y; |x − y |; x ∗ y] ∈ R4d , where ; denotes concatenation and ∗ denotes element-wise multiplication. This feature has been used as the input to the classifier in the sentence pair tasks of SentEval (Conneau and Kiela, 2018). To formulate the contrast pair, we either use edit sentence and its context f (u, v), or edit sentence and original sentence f (u, v0 ). We denote the two scenarios as C ONTEXT and O RIGINAL respectively. Finally, we use a classifier to predict the funniness score of the edited headline. zˆ = Classifier(h) ∈ R. The classifier is a two-layer MLP with 256 hidden dimensions. When finetuning transformers we use single-layer linear projection instead, since its large number of parameters have ˆk2 . already given us sufficient flexibility. The optimization objective is mean squared error L = kz −"
2020.semeval-1.128,N19-1423,0,0.0337843,"detecting humor in English news headlines with micro-edits. Specifically, the edited headlines have one selected word or entity that is replaced by editors, which are then graded by the degree of funniness. Accurate scoring of the funniness from micro-edits can serve as a footstone of humorous text generation (Hossain et al., 2020a). Inspired by the incongruity theory (Veale, 2004; Morreall, 2016), we believe that contrast and surprise is a key ingredient of humor. We instantiate this intuition with a contrastive framework. We then systematically compare three widely used models: CBOW, BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019), providing a benchmark for this task. Our best system, based on RoBERTa, achieves compelling performance for both subtasks. Our code is available on GitHub.2 2 Related Work Early humor recognition systems are mostly based on traditional machine learning methods, such as support vector machine, decision tree, Naive Bayes, and k-nearest neighbors (Castro et al., 2016). Besides, an n-gram language model shows good performance (Yan and Pedersen, 2017) in learning a sense of humor from tweets. Yet n-gram models are limited to a small number of context words. Pretrai"
2020.semeval-1.128,N19-1172,0,0.0285966,"n of a whole sentence. Among this family, BERT has been used to assess the humor in tweets and jokes (Mao and Liu, 2019; Weller and Seppi, 2019). Enlightened by these recent advances, we use BERT to judge the funniness of edited news headlines. We additionally experiment with RoBERTa, a robustly optimized variant of BERT. Lastly, several works also attempt to explicitly model incongruity and surprise of humourous text, focusing on homophonic puns. Kao et al. (2016) formalizes incongruity as a mixed effect of ambiguity and distinctiveness, quantified by entropy and Kullback-Leibler divergence. He et al. (2019) proposes a local-global surprisal measure based on the log-likelihood ratio, to assess whether a sentence is a pun. However, we focus on a broader definition of humor, and formulate incongruity as an input pair to a dual encoder framework. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 Competition page: https://competitions.codalab.org/competitions/20970 2 Code: https://github.com/dora-tang/SemEval-2020-Task-7 986 Proceedings of the 14th International Workshop on Semantic Evaluation, pages"
2020.semeval-1.128,N19-1012,0,0.0701281,"e Edit Score Label 33210 California and President Trump are going to war with each other monkeys 1.8 / 1664 What if sociologist had as much influence as economists? donkeys 2.8 / 9934 14279 Chibok girls reunited with families Chibok girls reunited with families smartphones cats 1.8 0.0 1 10920 9866 Gene Cernan, last astronaut on the Moon, dies at 82 Gene Cernan, last astronaut on the Moon, dies at 82 dancer impregnated 1.2 0.8 1 Table 1: Examples in train data of both subtasks. Underlined words in original headlines are to be substituted by the edit words. 3 Task Data The Humicroedit dataset (Hossain et al., 2019) provides the training, development, and test data for this task. We also use additional training data from the FunLines dataset (Hossain et al., 2020b). The dataset statistics are summarized in Appendix A. In Subtask 1, the goal is predicting the funniness score of an edited headline. The score z ranges from 0 to 3, where 0 means not funny and 3 means very funny. In Subtask 2, the goal is to predict the funnier between an edited sentence pair. For labels y ∈ {0, 1, 2}, 0 implies two headlines are equally funny, 1 implies the first is the funnier, and 2 implies the second is the funnier. Examp"
2020.semeval-1.128,2020.semeval-1.98,0,0.127472,"shuning.jin@rutgers.edu, {yinxx325,tang0611,tpederse}@d.umn.edu Abstract We use pretrained transformer-based language models in SemEval-2020 Task 7: Assessing the Funniness of Edited News Headlines. Inspired by the incongruity theory of humor, we use a contrastive approach to capture the surprise in the edited headlines. In the official evaluation, our system gets 0.531 RMSE in Subtask 1, 11th among 49 submissions. In Subtask 2, our system gets 0.632 accuracy, 9th among 32 submissions. 1 Introduction Humor detection is a challenging problem in natural language processing. SemEval-2020 Task 7 (Hossain et al., 2020a)1 focuses on detecting humor in English news headlines with micro-edits. Specifically, the edited headlines have one selected word or entity that is replaced by editors, which are then graded by the degree of funniness. Accurate scoring of the funniness from micro-edits can serve as a footstone of humorous text generation (Hossain et al., 2020a). Inspired by the incongruity theory (Veale, 2004; Morreall, 2016), we believe that contrast and surprise is a key ingredient of humor. We instantiate this intuition with a contrastive framework. We then systematically compare three widely used models"
2020.semeval-1.128,2020.acl-demos.28,0,0.438859,"shuning.jin@rutgers.edu, {yinxx325,tang0611,tpederse}@d.umn.edu Abstract We use pretrained transformer-based language models in SemEval-2020 Task 7: Assessing the Funniness of Edited News Headlines. Inspired by the incongruity theory of humor, we use a contrastive approach to capture the surprise in the edited headlines. In the official evaluation, our system gets 0.531 RMSE in Subtask 1, 11th among 49 submissions. In Subtask 2, our system gets 0.632 accuracy, 9th among 32 submissions. 1 Introduction Humor detection is a challenging problem in natural language processing. SemEval-2020 Task 7 (Hossain et al., 2020a)1 focuses on detecting humor in English news headlines with micro-edits. Specifically, the edited headlines have one selected word or entity that is replaced by editors, which are then graded by the degree of funniness. Accurate scoring of the funniness from micro-edits can serve as a footstone of humorous text generation (Hossain et al., 2020a). Inspired by the incongruity theory (Veale, 2004; Morreall, 2016), we believe that contrast and surprise is a key ingredient of humor. We instantiate this intuition with a contrastive framework. We then systematically compare three widely used models"
2020.semeval-1.128,D14-1162,0,0.0951817,"tokens into vector sequences ˜1:Te , e?1:Tc . The goal is to encode edit sentence, original sentence, context sentence into fixed-length e1:To , e vector representations u, v0 , v ∈ Rd . Importantly, we use span (a.k.a. sub-sentence) representation rather than whole sentences, which corresponds to the underlined ranges in the above M ONKEY E XAMPLE. Denote a span as a tuple of start and end position of contiguous tokens: s = [i, j] for edit, s˜ = [i, k] for original, and s? = [i, i] for the [MASK] token. CBOW We first explore context-independent word representations. We use pretrained GloVe (Pennington et al., 2014) vectors with d = 300 and a vocabulary of 2.2 million words. We use word averaging to 987 2.8 Score Classifier Classifier f (u, v) Feature Vector u tok edit v edit tok tok mask Encoder tok edit edit Span Representation tok Pretrained Transformer Encoder tok tok mask Word Representation tok Input What if don keys had as much influence.... What if [MASK] had as much influence.... Edited Sentence Context Sentence Figure 1: Transformer architecture to predict the funniness score of an edited headline using edit-context sentence pair. The full edited sentence is “What if donkeys had as much influen"
2020.semeval-1.128,N18-1202,0,0.0379024,"RTa, using the PyTorch (Paszke et al., 2019) implementation from HuggingFace Transformers library (Wolf et al., 2019).4 We use bert-base-uncased (L = 12, d = 768, lower-cased) and roberta-base (L = 12, d = 768). Transfer Paradigm When using those pretrained word representations, we consider two transfer paradigms: finetuning (F INETUNE) and not finetuning (F REEZE). In the case of F REEZE, we use fixed word embedding directly as the feature for CBOW. For transformers, we use a weighted average of hidden layers from the frozen encoder, with trainable mixing scalars. This approach follows ELMo (Peters et al., 2018) and the edge probing PL model (Tenney et al., 2019). Specifically, the final aggregatedth embedding for th i position is ei = γ l=0 αl el,i , where γ is a scaling factor, αl is the weight of the l layer, L is the total number of layers, and l = 0 corresponds to the embedding layer. 4.2 Task Specific Regression As mentioned at the beginning of the section, contrast and surprise is the key to humor. To represent the pairwise relationship between two vectors, we derive feature from h = f (x, y) = [x; y; |x − y |; x ∗ y] ∈ R4d , where ; denotes concatenation and ∗ denotes element-wise multiplicat"
2020.semeval-1.128,P16-1162,0,0.0383477,"Missing"
2020.semeval-1.128,D19-1372,0,0.019583,"recognition systems are mostly based on traditional machine learning methods, such as support vector machine, decision tree, Naive Bayes, and k-nearest neighbors (Castro et al., 2016). Besides, an n-gram language model shows good performance (Yan and Pedersen, 2017) in learning a sense of humor from tweets. Yet n-gram models are limited to a small number of context words. Pretrained language models based on Transformer (Vaswani et al., 2017) can obtain contextual information of a whole sentence. Among this family, BERT has been used to assess the humor in tweets and jokes (Mao and Liu, 2019; Weller and Seppi, 2019). Enlightened by these recent advances, we use BERT to judge the funniness of edited news headlines. We additionally experiment with RoBERTa, a robustly optimized variant of BERT. Lastly, several works also attempt to explicitly model incongruity and surprise of humourous text, focusing on homophonic puns. Kao et al. (2016) formalizes incongruity as a mixed effect of ambiguity and distinctiveness, quantified by entropy and Kullback-Leibler divergence. He et al. (2019) proposes a local-global surprisal measure based on the log-likelihood ratio, to assess whether a sentence is a pun. However, we"
2020.semeval-1.128,S17-2064,1,0.828529,"iate this intuition with a contrastive framework. We then systematically compare three widely used models: CBOW, BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019), providing a benchmark for this task. Our best system, based on RoBERTa, achieves compelling performance for both subtasks. Our code is available on GitHub.2 2 Related Work Early humor recognition systems are mostly based on traditional machine learning methods, such as support vector machine, decision tree, Naive Bayes, and k-nearest neighbors (Castro et al., 2016). Besides, an n-gram language model shows good performance (Yan and Pedersen, 2017) in learning a sense of humor from tweets. Yet n-gram models are limited to a small number of context words. Pretrained language models based on Transformer (Vaswani et al., 2017) can obtain contextual information of a whole sentence. Among this family, BERT has been used to assess the humor in tweets and jokes (Mao and Liu, 2019; Weller and Seppi, 2019). Enlightened by these recent advances, we use BERT to judge the funniness of edited news headlines. We additionally experiment with RoBERTa, a robustly optimized variant of BERT. Lastly, several works also attempt to explicitly model incongrui"
2020.semeval-1.255,S19-2106,1,0.915513,"OLID and OLID are different in that tweets in OLID are labeled with categories whereas in SOLID tweets are scored on a continuous scale of 0 to 1 to reflect the collective judgment of the models used as a part of distant supervision. As such the SOLID data did not provide a specific indication as to the boundaries between categories. 3 Assigning Labels to the Experimental Data We made a few significant decisions early on regarding the data. First, we elected not to use the 2019 OLID data. We participated in OffensEval–2019 and observed some potential inconsistencies in the OLID training data (Pedersen, 2019). We also felt that since SOLID was seeded with OLID that there would be no particular advantage to also using OLID. There is a tradeoff here between using a smaller amount of manually annotated data (OLID) versus a much larger sample of potentially noisier distantly supervised data (SOLID). Second, we decided to directly map the 2020 SOLID training data to categories. This required us to draw somewhat perilous and arbitrary boundaries through a real valued space for each task. We studied the distribution of scores in Task A and observed that the median in the SOLID training data was .25, and"
2021.semeval-1.44,N19-1423,0,0.0261692,"Missing"
2021.semeval-1.44,2021.semeval-1.185,0,0.072082,"Missing"
2021.semeval-1.44,2021.semeval-1.45,0,0.0460075,"Missing"
2021.semeval-1.44,2021.ccl-1.108,0,0.0188143,"Missing"
2021.semeval-1.44,2021.semeval-1.58,0,0.0601453,"Missing"
2021.semeval-1.44,2021.semeval-1.60,1,0.837196,"Missing"
2021.semeval-1.44,2020.acl-demos.14,0,0.0391908,"Missing"
2021.semeval-1.44,2021.semeval-1.57,0,0.0744368,"Missing"
2021.semeval-1.60,S17-2097,0,0.0285158,"onsidered, and using the dependencies to extract Subject–Predicate–Object patterns from sentence. 2 Previous Work Two previous SemEval tasks were also concerned with the extraction of relations and key phrases from scientific publications: SemEval 2017 Task 10: (ScienceIE - Extracting Keyphrases and Relations from Scientific Publications) (Augenstein et al., 2017), and SemEval 2018 Task 7: (Semantic Relation Extraction and Classification in Scientific Papers) (G´abor et al., 2018). Many of the approaches in these tasks use neural models to extract entities and their relations. The AI2 system (Ammar et al., 2017) at SemEval-2017 Task 10, which ranked first and second for task scenarios one and three respectively, approached this by building separate entity and relation models, each of which contain layers of LSTMs. The ETHDS3Lab system (Rotsztejn et al., 2018) at SemEval2018 Task 7, which ranked first in three of four subtasks, built an entity and relation classifier using a combination of RNNs and CNNs. Other approaches used supervised machine 1 Code is available at https://github.com/ anmartin94/DuluthSemEval2021Task11. learning algorithms while leveraging grammatical features. The LIPN system (Hern"
2021.semeval-1.60,S17-2091,0,0.0572581,"Missing"
2021.semeval-1.60,D19-1383,0,0.0205699,"he edge of triples by the Duluth system have the same POS tag of VBZ. This shows that the Duluth system sometimes shifts the phrases to the left or right of where they ought to be in the triple pattern. 498 9 Future Work One weakness of this system is that the selection of contributing sentences only uses sentence-level information; the classifier misses useful contextual information such as headers and the predicted class of preceding and following sentences. Future work may be able to address this problem by fine-tuning BERT to classify sequences of sentences rather than isolated sentences (Cohan et al., 2019). Generally, a document-level approach could be beneficial in terms of capturing important context. Another issue is that the end of the system does not have the ability to provide feedback to earlier parts of the pipeline; the only agency it has in terms of contributing sentence selection is the ability to discard a sentence provided by the sentence classifier. Future work could incorporate a neural entity classification model into the entity and triple extraction subsystem, which could be used to validate or invalidate the classification made at the sentence level (Rotsztejn et al., 2018). R"
2021.semeval-1.60,N19-1423,0,0.0295951,"ternational Workshop on Semantic Evaluation (SemEval-2021), pages 490–501 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics segment of the training dataset that we used to test the Duluth system during validation, before the first evaluation phase began. For more information on how this dataset was created, see Appendix B. Our approach1 employs a variety of techniques to address each component of the task. The selection of contribution sentences was done by finetuning the base deBERTa model (He et al., 2021) on the training data, as fine-tuned BERT (Devlin et al., 2019) models have been found to perform well on multi-class text classification tasks (Liu and Wangperawong, 2019). The fine-tuned model is used to classify each sentence as either non-contributing, or one of the twelve information units. The selected contribution sentences were then tagged to indicate likely scientific entities using a Maximum Entropy Markov Model (MEMM) (Bird et al., 2009) that was trained on the noun phrases selected as entities in the training data. A dependency parse (Manning et al., 2014) was then found for each sentence. The dependency parse and entity tags were then leverag"
2021.semeval-1.60,2021.semeval-1.44,1,0.837196,"Missing"
2021.semeval-1.60,S17-2174,0,0.018067,"017) at SemEval-2017 Task 10, which ranked first and second for task scenarios one and three respectively, approached this by building separate entity and relation models, each of which contain layers of LSTMs. The ETHDS3Lab system (Rotsztejn et al., 2018) at SemEval2018 Task 7, which ranked first in three of four subtasks, built an entity and relation classifier using a combination of RNNs and CNNs. Other approaches used supervised machine 1 Code is available at https://github.com/ anmartin94/DuluthSemEval2021Task11. learning algorithms while leveraging grammatical features. The LIPN system (Hernandez et al., 2017) approached SemEval-2017 Task 10 by first filtering possible keyphrases by labeling phrases with their POS sequence. Candidate keyphrases are filtered by comparing the POS tags of the phrase with POS sequences developed from the training data. They then trained a CRF model using the candidate phrases labeled with IOB tags. They were able to improve recall for keyphrase extraction by filtering candidate sentences before using a CRF. 3 Selection of Contribution Sentences We approached the selection of a contribution sentence as a multi-class sentence classification problem with 13 classes, where"
2021.semeval-1.60,P14-5010,0,0.012511,"tuning the base deBERTa model (He et al., 2021) on the training data, as fine-tuned BERT (Devlin et al., 2019) models have been found to perform well on multi-class text classification tasks (Liu and Wangperawong, 2019). The fine-tuned model is used to classify each sentence as either non-contributing, or one of the twelve information units. The selected contribution sentences were then tagged to indicate likely scientific entities using a Maximum Entropy Markov Model (MEMM) (Bird et al., 2009) that was trained on the noun phrases selected as entities in the training data. A dependency parse (Manning et al., 2014) was then found for each sentence. The dependency parse and entity tags were then leveraged to select contributing phrase spans and triples by using the entity tags to determine whether a subject or object noun phrase ought to be considered, and using the dependencies to extract Subject–Predicate–Object patterns from sentence. 2 Previous Work Two previous SemEval tasks were also concerned with the extraction of relations and key phrases from scientific publications: SemEval 2017 Task 10: (ScienceIE - Extracting Keyphrases and Relations from Scientific Publications) (Augenstein et al., 2017), a"
2021.semeval-1.60,S18-1112,0,0.0195588,"2017 Task 10: (ScienceIE - Extracting Keyphrases and Relations from Scientific Publications) (Augenstein et al., 2017), and SemEval 2018 Task 7: (Semantic Relation Extraction and Classification in Scientific Papers) (G´abor et al., 2018). Many of the approaches in these tasks use neural models to extract entities and their relations. The AI2 system (Ammar et al., 2017) at SemEval-2017 Task 10, which ranked first and second for task scenarios one and three respectively, approached this by building separate entity and relation models, each of which contain layers of LSTMs. The ETHDS3Lab system (Rotsztejn et al., 2018) at SemEval2018 Task 7, which ranked first in three of four subtasks, built an entity and relation classifier using a combination of RNNs and CNNs. Other approaches used supervised machine 1 Code is available at https://github.com/ anmartin94/DuluthSemEval2021Task11. learning algorithms while leveraging grammatical features. The LIPN system (Hernandez et al., 2017) approached SemEval-2017 Task 10 by first filtering possible keyphrases by labeling phrases with their POS sequence. Candidate keyphrases are filtered by comparing the POS tags of the phrase with POS sequences developed from the trai"
2021.semeval-1.60,2020.emnlp-demos.6,0,0.0198387,"Missing"
A00-2009,A97-1056,1,0.805975,"Missing"
A00-2009,J98-1005,0,0.0217092,"Missing"
A00-2009,P94-1020,0,0.053346,"Missing"
A00-2009,W99-0623,0,0.143629,"), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)). In natural language processing, ensemble techniques have been successfully applied to p a r t of-speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)). When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense disambiguation might best be improved by combining the output of a number of such classifiers into an ensemble. This paper begins with an introduction to the Naive Bayesian classifier. The features used to represent the context in which ambiguous words occur are presented, followed by the method for selecting the classifiers to include in the ensemble. Then, the line and interest data is described. Experimental results disambiguating th"
A00-2009,H93-1051,0,0.148,"ng the feature set or learning algorithm used in a corpus-based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pealersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often sig"
A00-2009,J98-1006,0,0.106007,"Missing"
A00-2009,W96-0208,0,0.448852,"arning algorithm used in a corpus-based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pealersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly great"
A00-2009,P96-1006,0,0.332297,"used in a corpus-based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pealersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any"
A00-2009,P98-1029,0,\N,Missing
A00-2009,C98-1029,0,\N,Missing
A97-1056,P94-1020,1,0.960352,"by the results of an extensive disambiguation experiment involving 12 ambiguous 388 words (in sections 5 and 6). We discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when selecting models for word-sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker, 1990) which are in turn a subset of the class of log-linear models (Bishop et al., 1975). Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. In a graphical model, variables are either interdependent or conditionally independent of one another. 1 All graphical models have a graphical representation such that each variable in the model is mapped to a node in the graph, and there is an undirected edge between each pair of nodes corresponding to interdependent variables. The sets of completely connected nodes (i.e., cliques) correspond to sets of interdependent variables. Any two nodes that are not directly connected by an edge are conditionally independent given the values of the nodes on the path that connects them. 2. Decompo"
A97-1056,W96-0210,1,0.46849,"Evaluation criteria fall into two broad classes, significance tests and information criteria. This paper considers two significance tests, the exact conditional test (Kreiner, 1987) and the Log-likelihood ratio statistic G 2 (Bishop et al., 1975), and two information criteria, Akaike&apos;s Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Criterion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G 2 is defined as: q = F_,.f, × logei (3) (2) 5 Experimental Data The sense-tagged text and feature set used in these experiments are the same as in (Bruce et al., 1996). The text consists of every sentence from the A C L / D C I Wall Street Journal corpus that contains any of the nouns interest, bill, concern, and drug, any of the verbs close, help, agree, and include, or any of the adjectives chief, public, last, and common. The extracted sentences have been hand-tagged with senses defined in the Longman Dictionary of Contemporary English (LDOCE). There are between 800 and 3,000 sense-tagged sentences for each of the 12 words. This data was randomly divided into training and test samples at a 10:1 ratio. A sentence with an ambiguous word is represented by a"
A97-1056,P91-1017,0,0.029491,"on. However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word-sense disambiguation. ~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). M a x i m u m Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. (Black, 1988) and (Mooney, 1996)) but, while it is a type of model selection, the models are not parametric. SThey recommended a model selection procedure using BSS and the exact conditional test in com"
A97-1056,H93-1051,0,0.197003,"FSS is clearly ilStatistical analysis of NLP d a t a has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in p a r t of-speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word-sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word-sense disambiguation. ~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). M a x i m u m Entropy models have been used to express the interactions among multiple feature"
A97-1056,W96-0208,0,0.212431,"l analysis of NLP d a t a has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in p a r t of-speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word-sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word-sense disambiguation. ~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation). M a x i m u m Entropy models have been used to express the interactions among multiple feature variables (e.g., (Be"
A97-1056,J96-1002,0,0.0330623,"n the path that connects them. 2. Decomposable models are those graphical models that express the joint distribution as the product of the marginal distributions of the variables in the maximal cliques of the graphical representation, scaled by the marginal distributions of variables common to two or more of these maximal sets. Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; (Berger et al., 1996)). 3. Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power (Whittaker, 1990). The joint parameter estimate ""d]~,]~..f3,~, Fl&apos;F2&apos;F3&apos;s is the probability that the feature vector (fl, f~., .1:3,si) will be observed in a training sample where each observation is represented by the feature variables (F1, F~, F3, S). Suppose that the graphical representation of a decomposable model is defined by the two cliques (i.e., marginals) (F1, S) and (F2, F3, S). The frequencies of"
A97-1056,P96-1006,0,0.0898132,"BSS, some pre-determined cutoff, a. An alternative to using a X2 approximation is to define the exact conditional distribution of G 2. The exact conditional distribution of G 2 is the distribu: tion of G ~ values that would be observed for comparable data samples randomly generated from the model being tested. The significance of G 2 based on the exact conditional distribution does not rely on an asymptotic approximation and is accurate for sparse and skewed data samples (Pedersen et al., 1996) 2An alternative feature set for this data is utilized with an exemplar-based learning algorithm in (Ng and Lee, 1996). 390 The sparse nature of our data can be illustrated by interest. There are 6 possible values for the sense variable. Combined with the other feature variables this results in 37,500,000 possible feature vectors (or joint parameters). However, we have a training sample of only 2,100 instances. 6 Experimental Results In total, eight different decomposable models were selected via a model search for each of the 12 words. Each of the eight models is due to a different combination of search strategy and evaluation criterion. Two additional classifiers were evaluated to serve as benchmarks. The d"
A97-1056,H94-1047,1,0.850766,"by the results of an extensive disambiguation experiment involving 12 ambiguous 388 words (in sections 5 and 6). We discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when selecting models for word-sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker, 1990) which are in turn a subset of the class of log-linear models (Bishop et al., 1975). Familiar examples of decomposable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. In a graphical model, variables are either interdependent or conditionally independent of one another. 1 All graphical models have a graphical representation such that each variable in the model is mapped to a node in the graph, and there is an undirected edge between each pair of nodes corresponding to interdependent variables. The sets of completely connected nodes (i.e., cliques) correspond to sets of interdependent variables. Any two nodes that are not directly connected by an edge are conditionally independent given the values of the nodes on the path that connects them. 2. Decompo"
A97-1056,H93-1052,0,\N,Missing
A97-1056,P91-1034,0,\N,Missing
E06-2007,W97-0322,1,0.861126,"d.umn.edu http://senseclusters.sourceforge.net Abstract This paper describes an unsupervised knowledge–lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus. It is based on the use of global criterion functions that assess the quality of a clustering solution. 1 Introduction The goal of word sense discrimination is to cluster the occurrences of a word in context based on its underlying meaning. This is often approached as a problem in unsupervised learning, where the only information available is a large corpus of text (e.g., (Pedersen and Bruce, 1997), (Schu¨ tze, 1998), (Purandare and Pedersen, 2004)). These methods usually require that the number of clusters to be discovered (k) be specified ahead of time. However, in most realistic settings, the value of k is unknown to the user. Word sense discrimination seeks to cluster N contexts, each of which contain a particular target word, into k clusters, where we would like the value of k to be automatically selected. Each context consists of approximately a paragraph of surrounding text, where the word to be discriminated (the target word) is found approximately in the middle of the context."
E06-2007,W04-2406,1,0.925635,"Abstract This paper describes an unsupervised knowledge–lean methodology for automatically determining the number of senses in which an ambiguous word is used in a large corpus. It is based on the use of global criterion functions that assess the quality of a clustering solution. 1 Introduction The goal of word sense discrimination is to cluster the occurrences of a word in context based on its underlying meaning. This is often approached as a problem in unsupervised learning, where the only information available is a large corpus of text (e.g., (Pedersen and Bruce, 1997), (Schu¨ tze, 1998), (Purandare and Pedersen, 2004)). These methods usually require that the number of clusters to be discovered (k) be specified ahead of time. However, in most realistic settings, the value of k is unknown to the user. Word sense discrimination seeks to cluster N contexts, each of which contain a particular target word, into k clusters, where we would like the value of k to be automatically selected. Each context consists of approximately a paragraph of surrounding text, where the word to be discriminated (the target word) is found approximately in the middle of the context. We present a methodology that automatically selects"
E06-2007,J98-1004,0,0.637693,"Missing"
J08-3010,J07-4009,0,0.0233216,"osals and papers, but not both. This overlooks a very happy side-effect that comes from creating releasable code—you will be more efﬁcient in producing new work of your own since you can easily reproduce and extend your own results. There is also a danger that this attitude will evolve over time into a self-perpetuating cycle: “I’ve worked on this for X years, why would I just give it away?” This ignores the fact that “giving it away” will make it easier for others to use your work, because if you don’t make your code available, who is really going to spend years re-implementing what you did? Webber (2007) draws attention to the amount of time our community wastes in writing and reviewing papers that are rejected and eventually abandoned. In a similar vein, we should all think about the time we cost our community when we don’t release software and make anyone who is interested in using or validating our work do their own implementation. If software is released publicly under one of the standard licenses that protects your copyright (e.g., the GNU General Public License3 or the Mozilla Public License4 ) then there is little danger of your work being misappropriated, and you will build a reservoi"
L16-1541,W10-1908,0,0.032136,"First, we present our author profiling dataset for health support forums. Our corpus is the first of its kind for author profiling of medical forum data. Second, we motivate author profile analysis in this type of data. Third, we propose a system that can predict age and gender of health forum users and present benchmarking results for future research in this area. We also discuss interesting findings about the salient topics in the different population groups. 2. Related Work Medical forum data has been used in a variety of different research works since it is a comprehensive source of data. Jha and Elhadad (2010) have tried to predict the cancer stage of a patient by using the text in their posts and their online behavior. They formulated the problem as a multi-class classification problem with four cancer stages. They used unigrams and bigrams as their text based features and also 3394 explored the use of network features with the hypothesis that patients in similar stages of cancer will interact more with each other. A combination of these three features gave them the best results. Rolia et al. (2013) tried to make predictions about the condition a person is suffering from based on similarities of t"
N01-1011,P94-1020,0,0.033456,"ures and allow us to continue re ning and understanding our feature sets. We believe that decision trees meet these criteria. A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morpholog"
N01-1011,J90-1003,0,0.0421101,"where rankings produced by 2 2 G , X , and Fisher&apos;s exact test disagreed, which is not altogether surprising given that low frequency bigrams were excluded. Since all of these statistics produced the same rankings, hereafter we make no distinction among them and simply refer to them generically as the power divergence statistic. 2.2 Dice CoeÆcient The Dice CoeÆcient is a descriptive statistic that provides a measure of association among two words in a corpus. It is similar to pointwise Mutual Information, a widely used measure that was rst introduced for identifying lexical relationships in (Church and Hanks, 1990). Pointwise Mutual Information can be de ned as follows: ( 1 MI w ; w n11  n++ 2 ) = log2 n  n +1 1+ where w1 and w2 represent the two words that make up the bigram. Pointwise Mutual Information quanti es how often two words occur together in a bigram (the numerator) relative to how often they occur overall in the corpus (the denominator). However, there is a curious limitation to pointwise Mutual Information. A bigram w1 w2 that occurs n11 times in the corpus, and whose component words w1 and w2 only occur as a part of that bigram, will result in increasingly strong measures of association"
N01-1011,J93-1003,0,0.0176046,"by n++ . 2.1 The Power Divergence Family (Cressie and Read, 1984) introduce the power divergence family of goodness of t statistics. A number of well known statistics belong to this family, including the likelihood ratio statistic G2 and Pearson&apos;s X 2 statistic. These measure the divergence of the observed (nij ) and expected (mij ) bigram counts, where mij is estimated based on the assumption that the component words in the bigram occur together strictly by chance: n m = + + n++ Given this value, G2 and X 2 are calculated as: ni j ij G 2=2 X nij  log i;j X 2= X( nij i;j nij mij mij )2 mij (Dunning, 1993) argues in favor of G2 over X 2 , especially when dealing with very sparse and skewed data distributions. However, (Cressie and Read, 1984) suggest that there are cases where Pearson&apos;s statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. In light of this, (Pedersen, 1996) presents Fisher&apos;s exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson&apos;s test and the likelihood ratio. Unfortunately it is usually not clear which test is most appropriate for a particular sample of dat"
N01-1011,W96-0208,0,0.101395,"atures and more general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the rst time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than building"
N01-1011,P96-1006,0,0.504247,"inue re ning and understanding our feature sets. We believe that decision trees meet these criteria. A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous w"
N01-1011,A00-2009,1,0.504035,"n the bigram and a particular word sense. Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods. A decision tree with 1 leaf node and no internal nodes (1/1) acts as a majority classi er. A decision tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump. 8 Discussion One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies. We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classi ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. While the accuracy of this approach was as good as any previously published results, the learned models were complex and diÆcult to interpret, in e ect acting as very accurate black boxes. Our experience has been that variations in learning algorithms are far less signi cant contributors to disambiguation accuracy than are variations in the feature set. In other words, an informative feature set will resu"
N01-1011,J96-1001,0,0.0427026,"s that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information. The Dice CoeÆcient overcomes this limitation, and can be de ned as follows: ( 1 2) = Dice w ; w 2  n11 +1 + n1+ n When n11 = n1+ = n+1 the value of Dice(w1 ; w2 ) will be 1 for all values n11 . When the value of n11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice CoeÆcient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice CoeÆcient is also discussed in (Smadja et al., 1996). We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. This software is written in Perl and is freely available from www.d.umn.edu/~tpederse. 3 Learning Decision Trees Decision trees are among the most widely used machine learning algorithms. They perform a general to speci c search of a feature space, adding the most informative features to a tree structure as the search proceeds. The objective is to select a minimal set of features that eÆciently partitions the feature space into classes of observations and assemble them into a tree. In o"
N01-1011,P98-2228,0,0.0109364,"fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the rst time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than building and traversing a tree to perform disambiguation, a list is employed. In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over{trained. However, we believe that fragmentation also re ects on the feature set used for learning. Ours consists of at most approximately 100 binary features. This results in a relatively small feature space that is not as likely to su er from fragmentation as are larger spaces. 10 Future Work Ther"
N01-1011,P94-1013,0,0.0378254,"iguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the rst time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than building and traversing a tree to perform disambiguation, a list is employed. In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over{trained. However, we believe that fragmentation also re ects on the feature set used for learning. Ours consists of at most approximately 100 binary features. This results in a relatively small feature space that is not as likely to su er from fragmentation as are large"
N01-1011,P95-1026,0,0.076431,"erstanding our feature sets. We believe that decision trees meet these criteria. A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation. 9 Related Work Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe th"
N01-1011,C98-2223,0,\N,Missing
N04-3008,W97-0322,1,0.91322,"tests supported by the Ngram Statistics Package. 3 Context Representation Once features are selected, SenseClusters creates a vector for each test instance to be discriminated where each selected feature is represented by an entry/index. Each vector shows if the feature represented by the corresponding index occurs or not in the context of the instance (binary vectors), or how often the feature occurs in the context (frequency vectors). This is referred to as a first order context vector, since this representation directly indicates which features make up the contexts. Here we are following (Pedersen and Bruce, 1997), who likewise took this approach to feature representation. (Sch¨utze, 1998) utilized second order context vectors that represent the context of a target word to be discriminated by taking the average of the first order vectors associated with the unigrams that occur in that context. In SenseClusters we have extended this idea such that these first order vectors can also be based on co–occurrence or bigram features from the training corpus. Both the first and second order context vectors represent the given instances as vectors in a high dimensional word space. This approach suffers from two"
N04-3008,W04-2406,1,0.409074,"enseClusters supports two different representations of context, first order context vectors as used by (Pedersen and Bruce, 1997) and second order context vectors as suggested by (Sch¨utze, 1998). The former is a direct representation of the instances to be clustered in terms of their features, while Availability SenseClusters is an open source software project that is freely distributed under the GNU Public License (GPL) via http://senseclusters.sourceforge.net/ SenseClusters is an ongoing project, and there are already a number of published papers based on its use (e.g., (Purandare, 2003), (Purandare and Pedersen, 2004)). 7 Acknowledgments This work has been partially supported by a National Science Foundation Faculty Early CAREER Development award (Grant #0092784). References Figure 3: Mountain View from gCLUTO the latter uses an indirect representation that averages the first order vector representations of the features that make up the context. Clustering SenseClusters seamlessly integrates CLUTO, a clustering package that provides a wide range of clustering algorithms and criteria functions. CLUTO also provides evaluation functions that report the inter-cluster and intra-cluster similarity, the most disc"
N04-3008,N03-3004,1,0.832967,"t Representations SenseClusters supports two different representations of context, first order context vectors as used by (Pedersen and Bruce, 1997) and second order context vectors as suggested by (Sch¨utze, 1998). The former is a direct representation of the instances to be clustered in terms of their features, while Availability SenseClusters is an open source software project that is freely distributed under the GNU Public License (GPL) via http://senseclusters.sourceforge.net/ SenseClusters is an ongoing project, and there are already a number of published papers based on its use (e.g., (Purandare, 2003), (Purandare and Pedersen, 2004)). 7 Acknowledgments This work has been partially supported by a National Science Foundation Faculty Early CAREER Development award (Grant #0092784). References Figure 3: Mountain View from gCLUTO the latter uses an indirect representation that averages the first order vector representations of the features that make up the context. Clustering SenseClusters seamlessly integrates CLUTO, a clustering package that provides a wide range of clustering algorithms and criteria functions. CLUTO also provides evaluation functions that report the inter-cluster and intra-c"
N04-3008,J98-1004,0,0.189577,"Missing"
N04-3012,W03-1812,0,0.159752,"archers in an interesting array of domains. (Zhang et al., 2003) use it as a source of semantic features for identifying cross–document structural relationships between pairs of sentences found in related documents. (McCarthy et al., 2004) use it in conjunction with a thesaurus derived from raw text in order to automatically identify the predominent sense of a word. (Jarmasz and Szpakowicz, 2003) compares measures of similarity derived from WordNet and Roget’s Thesaurus. The comparisons are based on correlation with human relatedness values, as well as the TOEFL synonym identification tasks. (Baldwin et al., 2003) use WordNet::Similarity to provide an evaluation tool for multiword expressions that are identified via Latent Semantic Analysis. (Diab, 2003) Software Architecture Similarity.pm is the super class of all modules, and provides general services used by all of the measures such as validation of synset identifier input, tracing, and caching of results. There are four modules that provide all of the functionality required by any of the supported measures: PathFinder.pm, ICFinder.pm, DepthFinder.pm, and LCSFinder.pm. PathFinder.pm provides getAllPaths(), which finds all of the paths and their leng"
N04-3012,O97-1002,0,0.88126,"with an overview of the measures supported in WordNet::Similarity, and then provides a brief description of how the package can be used. We close with a summary of research that has employed WordNet::Similarity. 2 Similarity Measures Three of the six measures of similarity are based on the information content of the least common subsumer (LCS) of concepts A and B. Information content is a measure of the specificity of a concept, and the LCS of concepts A and B is the most specific concept that is an ancestor of both A and B. These measures include res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang and Conrath, 1997). The lin and jcn measures augment the information content of the LCS with the sum of the information content of concepts A and B themselves. The lin measure scales the information content of the LCS by this sum, while jcn takes the difference of this sum and the information content of the LCS. The default source for information content for concepts is the sense–tagged corpus SemCor. However, there are also utility programs available with WordNet::Similarity that allow a user to compute information content values from the Brown Corpus, the Penn Treebank, the British National Corpus, or any giv"
N04-3012,P94-1019,0,0.380584,".530371390319309 # railway car versus motor coach > similarity.pl --type WordNet::Similarity::lin car#n bus#n car#n#1 bus#n#1 0.618486790769613 # automobile versus motor coach > similarity.pl --type WordNet::Similarity::lin --allsenses car#n bus#n#1 car#n#1 bus#n#1 0.618486790769613 # automobile versus motor coach car#n#2 bus#n#1 0.530371390319309 # railway car versus motor coach car#n#3 bus#n#1 0.208796988315133 # cable car versus motor coach Figure 1: Command Line Interface Three similarity measures are based on path lengths between a pair of concepts: lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), and path. lch finds the shortest path between two concepts, and scales that value by the maximum path length found in the is–a hierarchy in which they occur. wup finds the depth of the LCS of the concepts, and then scales that by the sum of the depths of the individual concepts. The depth of a concept is simply its distance to the root node. The measure path is a baseline that is equal to the inverse of the shortest path between two concepts. WordNet::Similarity supports two hypothetical root nodes that can be turned on and off. When on, one root node subsumes all of the noun concepts, and a"
N06-4007,W04-2406,1,0.831015,"o cluster the occurrences of a word (or name) found in multiple contexts based on their underlying meaning (or identity). The assumption is made that each discovered cluster will represent a different sense of a word, or the underlying identity of a person or organization that has an ambiguous name. Existing approaches to this problem usually require that the number of clusters to be discovered (k) be specified ahead of time. However, in most realistic settings, the value of k is unknown to the user. Here we describe various cluster stopping measures that are now implemented in SenseClusters (Purandare and Pedersen, 2004) that will group N contexts into k clusters, where the value of k will be automatically determined. Cluster stopping can be viewed as a problem in model selection, since a number of different models (i.e., clustering solutions) are created using different values of k, and the one that best fits the observed data is selected based on a criterion function. This is reminiscent of earlier work on sequential model selection for creating models of word sense disambiguation (e.g., (O’Hara et al., 2000)), where it was found that forward sequential search strategies were most effective. These methods s"
N09-5005,N04-3012,1,0.114664,"p broad coverage sense disambiguation modules that can be deployed in a practical setting without investing huge sums in manual annotation efforts. Our answer is WordNet::SenseRelate::AllWords (SRAW), a method that uses knowledge already available in the lexical database WordNet to assign senses to every content word in text, and as such offers broad coverage and requires no manual annotation of training data. SR-AW finds the sense of each word that is most related or most similar to those of its neighbors in the sentence, according to any of the ten measures available in WordNet::Similarity (Pedersen et al., 2004). It extends WordNet::SenseRelate::TargetWord, a lexical sample word sense disambiguation algorithm that finds the maximum semantic relatedness between a target word and its neighbors (Patwardhan et al., 2003). SR-AW was originally developed by (Michelizzi, 2005) (through version 0.06) and is now being significantly enhanced. 2 Methodology SR-AW processes a text sentence by sentence. It proceeds through each sentence word by word from left to right, centering each content word in a balanced window of context whose size is determined by the user. Note that content words at the start or end of a"
N10-1047,O97-1002,0,0.790098,") c in WordNet, Information Content is defined as the negative log of the probability of that concept (based on the observed frequency counts): IC(c) = −logP (c) Information Content can only be computed for nouns and verbs in WordNet, since these are the only parts of speech where concepts are organized in hierarchies. Since these hierarchies are separate, Information Content measures of similarity can only be applied to pairs of nouns or pairs of verbs. 2 Semantic Similarity Measures There are three Information Content measures implemented in WordNet::Similarity: (res) (Resnik, 1995), (jcn) (Jiang and Conrath, 1997), and (lin) (Lin, 1998). These measures take as input two concepts c1 and c2 (i.e., senses or synsets in WordNet) and output a numeric measure of similarity. These measures all rely to varying degrees on the idea of a least common subsumer (LCS); this is the most specific concept that is a shared ancestor of the two concepts. For example, the LCS of automobile and scooter is vehicle. The Resnik (res) measure simply uses the Information Content of the LCS as the similarity value: res(c1 , c2 ) = IC(LCS(c1 , c2 )) The Resnik measure is considered somewhat coarse, since many different pairs of co"
N10-1047,H93-1061,0,0.277532,"nrath measures attempt to refine the Resnik measure by augmenting it with the Information Content of the individual concepts being measured in two different ways: 2∗res(c1 ,c2 ) IC(c1 )+IC(c2 ) 1 IC(c1 )+IC(c2 )−2∗res(c1 ,c2 ) lin(c1 , c2 ) = jcn(c1 , c2 ) = All three of these measures have been widely used in the NLP literature, and have tended to perform well in a wide range of applications such as word sense disambiguation, paraphrase detection, and Question Answering (c.f., (Resnik, 1999)). 3 Experimental Data Information Content in WordNet::Similarity is (by default) derived from SemCor (Miller et al., 1993), a manually sense–tagged subset of the Brown Corpus. It is made up of approximately 676,000 words, of which 226,000 are sense–tagged. SemCor was originally created using sense–tags from version 1.6 of WordNet, and has been mapped to subsequent versions to stay current.3 This paper uses version 3.0 of WordNet and SemCor. WordNet::Similarity also includes a utility (rawtextFreq.pl) that allows a user to derive Information Content values from any corpus of plain text. This utility is used with the untagged version of SemCor and with various portions of the English GigaWord corpus (1st edition) t"
N10-1047,W06-2501,1,0.856083,"C have been scored for similarity, while WS is scored for relatedness, which is a more general and less well–defined notion than similarity. For example aspirin and headache are clearly related, but they aren’t really similar. 4 Experimental Results Table 1 shows the Spearman’s rank correlation of several other measures of similarity and relatedness in WordNet::Similarity with the gold standards discussed above. The WordNet::Similarity vector relatedness measure achieves the highest correlation, followed closely by the adapted lesk measure. These results are consistent with previous findings (Patwardhan and Pedersen, 2006). This table also shows results for several path–based measures.4 Table 2 shows the correlation of jcn, res, and lin when Information Content is derived from 1) the sense-tagged version of SemCor (semcor), 2) SemCor without sense tags (semcor-raw), and 3) steadily increasing subsets of the 133 million word xie portion of the English GigaWord corpus. These subsets start with the entire first month of xie (199501, from January 1995) and then two months (19950102), three months (199501-03), up through all of 1995 (199501-12). Thereafter the increments are annual, with two years of data (1995-1996"
N10-1047,N04-3012,1,0.314135,"ary because each occurrence of a more specific concept also implies the occurrence of the more general ancestor concepts. When a corpus is sense–tagged, mapping occurrences of a word to a concept is straightforward (since each sense of a word corresponds with a concept or synset in WordNet). However, if the text has not been sense–tagged then all of the possible senses of a given word are incremented (as are their ancestors). For example, if tree (as a plant) occurs in a sense–tagged text, then only the concept associated 1 These experiments were done with version 2.05 of WordNet::Similarity (Pedersen et al., 2004). 329 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 329–332, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics with tree as a kind of plant would be incremented. If the text is untagged, then all of the possible senses of tree would be incremented (such as the mathematical sense of tree, a shoe tree, a plant, etc.) In this case the frequency of all the occurrences of a word are divided equally among the different possible senses. Thus, if a word occurs 42 times in a corpus and there are six possible se"
N13-3007,O97-1002,0,0.322723,"Missing"
N13-3007,W06-2501,1,0.869331,"Missing"
N13-3007,N04-3012,1,0.567278,"redundancy within clinical records, while (Mathur and Dinakarpandian, 2011) used them to help identify similar diseases. UMLS::Similarity has also enabled the development and evaluation of new measures by allowing them to be compared to existing methods, e.g., (Pivovarov and Elhadad, 2012). Finally, UMLS::Similarity can serve as a building block in other NLP systems, for example UMLS::SenseRelate (McInnes et al., 2011) is a word sense disambiguation system for medical text based on semantic similarity and relatedness. 3 UMLS::Similarity UMLS::Similarity is a descendent of WordNet::Similarity (Pedersen et al., 2004), which implements various measures of similarity and relatedness for WordNet.5 However, the structure, nature, and size of the UMLS is quite different from WordNet, and the adaptations from WordNet were not always straightforward. One very significant difference, for example, is that the UMLS is stored in a MySQL database while WordNet has its own customized storage format. As a result, the core of UMLS::Similarity is different and offers a great deal of functionality specific to the UMLS. Table 1 lists the measures currently provided in UMLS::Similarity (as of version 1.27). The Web interfac"
N13-3007,P94-1019,0,\N,Missing
P05-3019,P04-1036,0,0.00872633,"et al., 1993) developed a context vector approach for performing word sense disambiguation. Their algorithm built co-occurrence vectors from dictionary definitions using Longman’s Dictionary of Contemporary English (LDOCE). They then determined the extent of overlap between the sum of the vectors of the words in the context and the sum of the vectors of the words in each of the definitions (of the target word). For vectors, the extent of overlap is defined as the dot product of the vectors. The meaning of the target word that had the maximum overlap was selected as the answer. More recently, (McCarthy et al., 2004) present a method that performs disambiguation by determing the most frequent sense of a word in a particular domain. This is based on measuring the relatedness of the different possible senses of a target word (using WordNet::Similarity) to a set of words associated with a particular domain that have been identified using distributional methods. The relatedness scores between a target word and the members of this set are scaled by the distributional similarity score. 5 Availability SenseRelate::TargetWord is written in Perl and is freely distributed under the Gnu Public License. It is availab"
P05-3019,N04-3012,1,0.636798,"window of context around the target word is to be used, and whether or not all the parts of speech of a word should be considered. 3.2 Programming Interface SenseRelate::TargetWord is distributed as a Perl package. It is programmed in object-oriented Perl as a group of Perl classes. Objects of these classes can be instantiated in user programs, and methods can be called on these objects. The package requires that the Perl interface to WordNet, WordNet::QueryData1 be installed on the system. The disambiguation algorithms also require that the semantic relatedness measures WordNet::Similarity (Pedersen et al., 2004) be installed. 3.3 Graphical User Interface We have developed a graphical interface for the package in order to conveniently access the disambiguation modules. The GUI is written in Gtk-Perl – a Perl API to the Gtk toolkit. Unlike the command line interface, the graphical interface is not tied to any input file format. The interface allows the user to input text, and to select the word to disambiguate. It also provides the user with numerous configuration options corresponding to the various customizations described above. 1 http://search.cpan.org/dist/WordNet-QueryData 4 Related Work 6 There"
P05-3027,W97-0322,1,0.848632,"Missing"
P05-3027,W04-2406,1,0.777003,"ilar contexts in text. It relies on lexical features to build first and second order representations of contexts, which are then clustered using unsupervised methods. It was originally developed to discriminate among contexts centered around a given target word, but can now be applied more generally. It also supports methods that create descriptive and discriminating labels for the discovered clusters. 1 Introduction SenseClusters seeks to group together units of text (referred to as contexts) that are similar to each other using lexical features and unsupervised clustering. Our initial work (Purandare and Pedersen, 2004) focused on word sense discrimination, which takes as input contexts that each contain a given target word, and produces as output clusters that are presumed to correspond to the different senses of the word. This follows the hypothesis of (Miller and Charles, 1991) that words that occur in similar contexts will have similar meanings. We have shown that these methods can be extended to proper name discrimination (Pedersen et al., 2005). People, places, or companies often share the same name, and this can cause a considerable amount of confusion when carrying out Web search or other information"
P05-3027,J98-1004,0,0.254481,"Missing"
P13-1166,O97-1002,0,0.00955661,":// search.cpan.org/dist/WordNet-Similarity/. 1693 measure Spearman ρ Kendall τ ranking min max min max variation path based similarity path 0.70 0.78 0.55 0.62 1-8 wup 0.70 0.79 0.53 0.61 1-6 lch 0.70 0.78 0.55 0.62 1-7 path based information content res 0.65 0.75 0.26 0.57 4-11 lin 0.49 0.73 0.36 0.53 6-10 jcn 0.46 0.73 0.32 0.55 5, 7-11 path based relatedness hso 0.73 0.80 0.36 0.41 1-3,5-10 dictionary and corpus based relatedness vpairs 0.40 0.70 0.26 0.50 7-11 vector 0.48 0.92 0.33 0.76 1,2,4,6-11 lesk 0.66 0.83 -0.02 0.61 1-8,11,12 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spear"
P13-1166,J06-1003,0,0.0172015,"es the difference in rank as its basis to calculate a correlation, where Kendall τ uses the number of items with the correct rank. The low Kendall τ for lesk is the result of three pairs receiving a score that is too high. Other pairs that get a relatively accurate score are pushed one place down in rank. Because only items that receive the exact same rank help to increase τ , such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ . We included τ , because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results"
P13-1166,W08-2206,0,0.013078,"at receive the exact same rank help to increase τ , such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ . We included τ , because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results vary enough to change the identity of the measure that yields the best performance. Table 1 reveals a wide variation in ranking relative to alternative approaches. Results in Table 2 show that it is common for the ranking of a score to change due to variations that are not at the core of the method. This study s"
P13-1166,P05-1045,0,0.0057573,"Missing"
P13-1166,W06-2501,1,0.861219,"n the methodology, such as data sets, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by R"
P13-1166,N10-1047,1,0.4317,"s, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Gooden"
P13-1166,P94-1019,0,0.0317356,"us research, address the following questions: 1) Which properties have an impact on the performance of WordNet similarity measures? 2) How much does the performance of individual measures vary? 3) How do commonly used measures compare when the variation of their performance are taken into account? 3.2 Methodology and first observations The questions above were addressed in two stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3 Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4 WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 measure Spearman ρ Kendall τ ranking min max min max variation path based similarity path 0.70 0.78 0.55 0.62 1-8 wup 0.70 0.79 0.53 0.61 1-6 lch 0.70 0.78 0.55 0.62 1-7 path based information content res 0.65 0.75 0.26 0.57 4-11 lin 0.49 0.73 0.36 0.53 6-10 jcn 0.46 0.73 0.32 0.55 5, 7-11 path based relatedness hso 0.73 0.80 0"
P13-1166,J08-3010,1,\N,Missing
P13-1166,J04-4004,0,\N,Missing
P13-1166,J03-4003,0,\N,Missing
S01-1034,A00-2009,1,0.835825,"Missing"
S01-1034,N01-1011,1,0.765634,"Missing"
S07-1086,H94-1111,0,0.210132,"Missing"
S07-1086,N06-2015,0,0.0258006,"f semantic relatedness defined in the WordNet::Similarity software package (Pedersen et al., 2004), which is a suite of Perl modules implementing a number WordNet-based measures of semantic relatedness. For this work, we used the Context Vector measure (Patwardhan and Pedersen, 2006). The relatedness of concepts is computed based on word co-occurrence statistics derived from WordNet glosses. Given two WordNet senses, this module returns a score between 0 and 1, indicating the relatedness of the two senses. Our system relies on WordNet as its sense inventory. However, this task used OntoNotes (Hovy et al., 2006) as the sense inventory. OntoNotes word senses are groupings of similar WordNet senses. Thus, we used the training data answer key to generate a mapping between the OntoNotes senses of the given lexical elements and their corresponding WordNet senses. We had to manually create the mappings for some of the WordNet senses, which had no corresponding OntoNotes senses. The sense selection algorithm performed all of its computations with respect to the WordNet senses, and finally the OntoNotes sense corresponding to the selected WordNet sense of the target word was output as the answer for each ins"
S07-1086,P05-3019,1,0.244613,"Missing"
S07-1086,N04-3012,1,0.733627,". Suppose the target word t has T senses, enumerated as t1 , t2 , . . . , tT . Also, suppose w1 , w2 , . . . , w2n are the words in the context of t, each having W1 , W2 , . . . , W2n senses, respectively. Then for each ti a score is computed as score(ti ) = 2n X j=1 max (relatedness(ti , wjk )) k=1 to Wj where wjk is the k th sense of word wj . The sense ti of target word t with the highest score is selected as the intended sense of the target word. The relatedness between two word senses is computed using a measure of semantic relatedness defined in the WordNet::Similarity software package (Pedersen et al., 2004), which is a suite of Perl modules implementing a number WordNet-based measures of semantic relatedness. For this work, we used the Context Vector measure (Patwardhan and Pedersen, 2006). The relatedness of concepts is computed based on word co-occurrence statistics derived from WordNet glosses. Given two WordNet senses, this module returns a score between 0 and 1, indicating the relatedness of the two senses. Our system relies on WordNet as its sense inventory. However, this task used OntoNotes (Hovy et al., 2006) as the sense inventory. OntoNotes word senses are groupings of similar WordNet"
S07-1086,W06-2501,1,\N,Missing
S07-1087,S07-1002,0,0.0497709,"n task. For this task SenseClusters was configured to construct representations of the instances to be clustered using the centroid of word cooccurrence vectors that replace the words in an instance. These instances are then clustered using k–means where the number of clusters is discovered automatically using the Adapted Gap Statistic. In these experiments SenseClusters did not use any information outside of the raw untagged text that was to be clustered, and no tuning of the system was performed using external corpora. 1 2 Introduction The object of the sense induction task of S ENSEVAL -4 (Agirre and Soroa, 2007) was to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. The task data consisted of the combination of the test and training data (minus the sense tags) from the English lexical sample task. Each instance is a context of several sentences which contains an occurrence of a given word that serves as the target of sense induction. SenseClusters is based on the presumption that words that occur in similar contexts will have similar meanings. This intuition has been presented as both the Distributional Hypothesis (Harris, 1968) and the Methodology in S"
S07-1087,W06-1669,0,0.0736703,"Missing"
S07-1087,N06-4007,1,0.830607,"n, single letter words, and numbers (with the exception of years) were eliminated. Each of the contexts that contain a particular target word is represented by a single vector that is the average (or the centroid) of all the co-occurrence vectors found for the words that make up the context. This results in a context by feature matrix, where the features are the words that occur with the words in the contexts (i.e., second order co– occurrences). The k–means algorithm is used for clustering the contexts, where the number of clusters is automatically discovered using the Adapted Gap Statistic (Pedersen and Kulkarni, 2006). The premise of this method is to create a randomized sample of data with the same characteristics of the observed data (i.e., the contexts to be clustered). This is done by fixing the marginal totals of the context by feature matrix and then generating randomized values that are consistent with those marginal totals. This creates a matrix that is can be viewed as being from the same population as the observed data, except that the data is essentially noise (because it is randomly generated). The randomized data is clustered for successive values of k from 1 to some upper limit (the number of"
S07-1087,W04-2406,1,0.895827,"Missing"
S07-1087,W97-0322,1,\N,Missing
S10-1081,S07-1002,0,0.171078,"Semantic Evaluation, ACL 2010, pages 363–366, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics providing a useful and reasonable way of evaluating system results. Table 1: Duluth-WSI Distinctions name Duluth-WSI Duluth-WSI-Gap Duluth-WSI-SVD Duluth-WSI-Co Duluth-WSI-Co-Gap options bigrams, no SVD, PK2 bigrams, no SVD, Gap bigrams, SVD, PK2 co-occur, no SVD, PK2 co-occur, no SVD, Gap 5.1 Evaluation Measures Each participating system was scored by three different evaluation methods: the V-measure (Rosenberg and Hirschberg, 2007), the supervised recall measure (Agirre and Soroa, 2007), and the paired F-score (Artiles et al., 2009). The results of the evaluation are in some sense confusing - a system that ranks near the top according to one measure may rank at the bottom or middle of another. There was not any single system that did well according to all of the different measures. The situation is so extreme that in some cases a system would perform near the top in one measure, and then below random baselines in another. These stark differences suggest a real need for continued development of other methods for evaluating unsupervised sense induction. One minimum expectation"
S10-1081,D09-1056,0,0.243989,"Missing"
S10-1081,S10-1011,0,0.255169,"enses is automatically predicted by either the PK2 measure or Adapted Gap Statistic (Pedersen and Kulkarni, 2006). In the Duluth systems the co-occurrence matrices are either based on order-dependent bigrams or unordered pairs of words, both of which can be separated by up to some given number of intervening words. Bigrams are used to preserve distinctions between collocations such as cat house and house cat, whereas co–occurrences do not consider order and would treat these two as being equivalent. Introduction 2 Duluth-WSI systems The Duluth systems in the sense induction task of SemEval-2 (Manandhar et al., 2010) were based on SenseClusters (v1.01), a freely available open source software package which relies on the premise that words with similar meanings will occur in similar contexts (Purandare and Pedersen, 2004). The data for the sense induction task included 100 ambiguous words made up of 50 nouns and 50 verbs. There were a total of 8,915 test instances and 879,807 training instances provided. Note that neither the training nor the test data was sense tagged. The training data was made available as a resource for participants, with the understanding that system evaluation would be done on the te"
S10-1081,N06-4007,1,0.890182,"d co–occurrence matrices from the task test data to create a second order co–occurrence representation of those test instances. The senses of words were induced by clustering these instances, where the number of clusters was automatically predicted. The Duluth-Mix system was a variation of WSI that used the combination of training and test data to create the co-occurrence matrix. The Duluth-R system was a series of random baselines. instance. All the test instances for a word are clustered, and the number of senses is automatically predicted by either the PK2 measure or Adapted Gap Statistic (Pedersen and Kulkarni, 2006). In the Duluth systems the co-occurrence matrices are either based on order-dependent bigrams or unordered pairs of words, both of which can be separated by up to some given number of intervening words. Bigrams are used to preserve distinctions between collocations such as cat house and house cat, whereas co–occurrences do not consider order and would treat these two as being equivalent. Introduction 2 Duluth-WSI systems The Duluth systems in the sense induction task of SemEval-2 (Manandhar et al., 2010) were based on SenseClusters (v1.01), a freely available open source software package whic"
S10-1081,S07-1087,1,0.895808,"Missing"
S10-1081,W04-2406,1,0.860071,"grams or unordered pairs of words, both of which can be separated by up to some given number of intervening words. Bigrams are used to preserve distinctions between collocations such as cat house and house cat, whereas co–occurrences do not consider order and would treat these two as being equivalent. Introduction 2 Duluth-WSI systems The Duluth systems in the sense induction task of SemEval-2 (Manandhar et al., 2010) were based on SenseClusters (v1.01), a freely available open source software package which relies on the premise that words with similar meanings will occur in similar contexts (Purandare and Pedersen, 2004). The data for the sense induction task included 100 ambiguous words made up of 50 nouns and 50 verbs. There were a total of 8,915 test instances and 879,807 training instances provided. Note that neither the training nor the test data was sense tagged. The training data was made available as a resource for participants, with the understanding that system evaluation would be done on the test instances only. The organizers held back a gold standard annotation of the test data that was only used for evaluation. Five Duluth-WSI systems participated in this task, six Duluth-Mix systems, and five D"
S10-1081,D07-1043,0,\N,Missing
S12-1070,S12-1047,0,0.0257494,"@d.umn.edu Abstract This paper describes the Duluth systems that participated in Task 2 of SemEval–2012. These systems were unsupervised and relied on variations of the Gloss Vector measure found in the freely available software package WordNet::Similarity. This method was moderately successful for the Class-Inclusion, Similar, Contrast, and Non-Attribute categories of semantic relations, but mimicked a random baseline for the other six categories. 1 Introduction This paper describes the Duluth systems that participated in Task 2 of SemEval–2012, Measuring the Degree of Relational Similarity (Jurgens et al., 2012). The goal of the task was to rank sets of word pairs according to the degree to which they represented an underlying category of semantic relation. A highly ranked pair would be considered a good or prototypical example of the relation. For example, given the relation Y functions as an X the pair weapon:knife (X:Y) would likely be considered more representative of that relation than would be tool:spoon. The task included word pairs from 10 different categories of relational similarity, each with a number of subcategories. In total the evaluation data consisted of 69 files, each containing a s"
S12-1070,W06-2501,1,0.894406,":knife (X:Y) would likely be considered more representative of that relation than would be tool:spoon. The task included word pairs from 10 different categories of relational similarity, each with a number of subcategories. In total the evaluation data consisted of 69 files, each containing a set of approximately 40 word pairs. While training examples were also provided, these were not used by the Duluth systems. The system–generated rankings were compared with gold standard data created via Amazon Mechanical Turk. The Duluth systems relied on the Gloss Vector measure of semantic relatedness (Patwardhan and Pedersen, 2006) as implemented in WordNet::Similarity (Pedersen et al., 2004)1 . This quantifies the degree of semantic relatedness between two word senses. It does not, however, discover or indicate the nature of the relation between the words. When given two words as input (as was the case in this task), it measures the relatedness of all possible combinations of word senses associated with this pair and reports the highest resulting score. Note that throughout this paper we use word and word sense somewhat interchangeably. In general it may be assumed that the term word or examples of words refers to a wo"
S12-1070,N04-3012,1,0.762788,"Missing"
S12-1070,J98-1004,0,0.452851,"Missing"
S12-1070,J06-1003,0,\N,Missing
S13-2036,D09-1056,0,0.0756641,"Missing"
S13-2036,J13-3008,0,0.0550089,"Missing"
S13-2036,P05-3027,1,0.875369,"for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text was converted to upper case prior to processing. The goal was to group the 64 snippets for each query into k distinct clusters, where k was automatically determined by the PK2 method of SenseClu"
S13-2036,S10-1011,0,0.0196157,"edition) (Graff and Cieri, 2003). This created a corpus of approximately 3.6 million words which resulted in a co-occurrence matrix prior to SVD of 9,853 x 10,995 with 43,199 non-zero values. After SVD the co–occurrence matrix was 9,853 by 300. 3 Results Various measures were reported by the task organizers, including F1 (F1-13), the Rand Index (RI), the Adjusted Rand Index (ARI), and the Jaccard Coefficient. More details can be found in (Di Marco and Navigli, 2013). In addition we computed the paired F-Score (F10) (Artiles et al., 2009) as used in the 2010 SemEval word sense induction task (Manandhar et al., 2010) and the F-Measure (F-SC), which is provided by SenseClusters. This allows for the comparison of results from this task with the 2010 task and various results from SenseClusters. The organizers also provided scores for S-recall and S-precision (Zhai et al., 2003), however for these to be meaningful the results for each cluster must be output in ranked order. The Duluth systems did not make a ranking distinction among the instances in each cluster, and so these scores are not particularly meaningful for the Duluth systems. 3.1 Comparisons to Baselines Table 1 includes the results of the three s"
S13-2036,S13-2035,0,0.0135442,"seClusters. The query terms are treated exactly like any other word in the snippets, which is called headless clustering in SenseClusters. The Duluth systems that participated in task 11 of SemEval–2013 carried out word sense induction (WSI) in order to cluster Web search results. They relied on an approach that represented Web snippets using second–order co– occurrences. These systems were all implemented using SenseClusters, a freely available open source software package. 2.1 Common aspects to all systems 1 Introduction The goal of task 11 of SemEval–2013 was to cluster Web search results (Navigli and Vannella, 2013). The test data consisted of the top 64 Google results for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both"
S13-2036,N06-4007,1,0.786029,"tent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text was converted to upper case prior to processing. The goal was to group the 64 snippets for each query into k distinct clusters, where k was automatically determined by the PK2 method of SenseClusters (Pedersen and Kulkarni, 2006a; Pedersen and Kulkarni, 2006b). Each discovered cluster represents a different underlying meaning of the given query term that resulted in those snippets being returned. Word sense induction was carried out separately on the Web snippets associated with each query term, meaning that the algorithm was run 100 times and clustered 64 Web page snippets each time. In second–order context clustering, the words in a context (i.e., Web snippet) to be clustered are replaced by vectors that are derived from some corpus of text. The corpora used are among the main differences in the Duluth systems. Onc"
S13-2036,E06-2007,1,0.629016,"tent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text was converted to upper case prior to processing. The goal was to group the 64 snippets for each query into k distinct clusters, where k was automatically determined by the PK2 method of SenseClusters (Pedersen and Kulkarni, 2006a; Pedersen and Kulkarni, 2006b). Each discovered cluster represents a different underlying meaning of the given query term that resulted in those snippets being returned. Word sense induction was carried out separately on the Web snippets associated with each query term, meaning that the algorithm was run 100 times and clustered 64 Web page snippets each time. In second–order context clustering, the words in a context (i.e., Web snippet) to be clustered are replaced by vectors that are derived from some corpus of text. The corpora used are among the main differences in the Duluth systems. Onc"
S13-2036,S07-1087,1,0.751278,"for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text was converted to upper case prior to processing. The goal was to group the 64 snippets for each query into k distinct clusters, where k was automatically determined by the PK2 method of SenseClusters (Pedersen and Kulkarni, 2006a; Pedersen and Kulkarni, 2006b). Each discovered cluster represents a different underlying meaning of the given query term that resulted in those snippets being"
S13-2036,S10-1081,1,0.81876,"ters, a freely available open source software package. 2.1 Common aspects to all systems 1 Introduction The goal of task 11 of SemEval–2013 was to cluster Web search results (Navigli and Vannella, 2013). The test data consisted of the top 64 Google results for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). The input to sys1, sys7, and sys9 consists"
S13-2036,W04-2406,1,0.812042,"d of the top 64 Google results for each of 100 potentially ambiguous queries, for a total of 6,400 test instances. The Web snippets returned for each query were clustered and evaluated separately, with an overall evaluation score provided for each system. The problem of Web page clustering is one of the use cases envisioned for SenseClusters (Pedersen and Kulkarni, 2007; Pedersen, 2010a), a freely available open source software package developed at the University of Minnesota, Duluth starting in 2002. It supports first and second–order clustering of contexts using both co–occurrence matrices (Purandare and Pedersen, 2004; Kulkarni and Pedersen, 2005) and Latent Semantic Analysis (Landauer and Dumais, 1997). SenseClusters has participated in various forms at different SenseEval and SemEval shared tasks, including SemEval-2007 (Pedersen, 2007), SemEval2010 (Pedersen, 2010b) and also in an i2b2 clinical medicine task (Pedersen, 2006). The input to sys1, sys7, and sys9 consists of 64 Web search snippets, each approximately 25 words in length. All text was converted to upper case prior to processing. The goal was to group the 64 snippets for each query into k distinct clusters, where k was automatically determined"
S13-2036,D07-1043,0,0.0812116,"Missing"
S14-2040,N04-3012,1,0.83584,"the CLSS shared task (Jurgens et al., 2014) included 4 subtasks where pairs of different granularity were measured for semantic similarity. These included : word-2sense (w2s), phrase-2-word (p2w), sentence-2phrase (s2p), and paragraph-2-sentence (g2s). In addition to different levels of granularity, these pairs included slang, jargon and other examples of non–standard English. We were drawn to this task because of our long– standing interest in semantic similarity. We have pursued approaches ranging from those that rely on structured knowledge sources like WordNet (e.g., WordNet::Similarity) (Pedersen et al., 2004) to those that use distributional information found This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://umls-similarity.sourceforge.net 247 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 247–251, Dublin, Ireland, August 23-24, 2014. 2 Measures WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009). It has been applied to both word sense d"
S14-2040,W04-2406,1,0.789853,"Missing"
S14-2040,S14-2003,0,0.0240617,"ond–order (Vector) overlaps to measure similarity. The first–order overlaps fared well according to Spearman’s correlation (top 5) but less so relative to Pearson’s. Most systems performed at comparable levels for both Spearman’s and Pearson’s measure, which suggests the Duluth approach is potentially unique among the participating systems. 1 Introduction Cross–Level Semantic Similarity (CLSS) is a novel variation on the problem of semantic similarity. As traditionally formulated, pairs of words, pairs of phrases, or pairs of sentences are scored for similarity. However, the CLSS shared task (Jurgens et al., 2014) included 4 subtasks where pairs of different granularity were measured for semantic similarity. These included : word-2sense (w2s), phrase-2-word (p2w), sentence-2phrase (s2p), and paragraph-2-sentence (g2s). In addition to different levels of granularity, these pairs included slang, jargon and other examples of non–standard English. We were drawn to this task because of our long– standing interest in semantic similarity. We have pursued approaches ranging from those that rely on structured knowledge sources like WordNet (e.g., WordNet::Similarity) (Pedersen et al., 2004) to those that use di"
S14-2040,J98-1004,0,0.548317,"Missing"
S14-2040,W06-2501,1,0.921368,"nsion is a well–known possibility, although in the Duluth systems we opted to expand words with their co–occurrence vectors. This follows from an approach to word sense discrimination developed by (Sch¨utze, 1998). Once words are expanded then all the vectors in a definition are averaged together and this averaged vector becomes the representation of the definition. This idea was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009), and has been applied to word sense disambiguation and semantic similarity (Patwardhan, 2003; Patwardhan and Pedersen, 2006; Liu et al., 2012). The co–occurrences for the words in the definitions can come from any corpus of text. Once a co–occurrence matrix is constructed, then each word in each definition is replaced by its vector from that matrix. If no such vector is found the word is removed from the definition. Then, all the vectors representing a definition are averaged together, and this vector is used to measure against other vectors created in the same way. The scores returned by the Vector measure are between 0 and 1 (inclusive) where 1.00 means exactly the same and 0 means no similarity. 248 3 Duluth Sy"
S15-2076,P98-1012,0,0.156257,"dered a target co–occurrence. In run2 any bigram that occurred 5 or more times in the WordNet 3.0 gloss corpus was used as a feature. In run3 any unigram that occurred 2 or more times in the contexts to be clustered was used as a feature. We used the nearly 400 word stoplist from the Ngram Statistics Package3 (Banerjee and Pedersen, 2003) for all three of our runs. Any bigram or co– occurrence where both words are stop words was not used as a feature, and any unigram in the stoplist was likewise discarded. 3 Results and Analysis Official results from task 15 are based on the B– cubed F–score (Bagga and Baldwin, 1998). In addition to reporting those values, we also carried out our own analysis using the SenseClusters F–measure. 3.1 B–cubed F–score Table 3.1 shows the B–Cubed F–scores as reported by the task organizers. Note that the baseline system assigns all contexts to a single cluster or sense. Prior to the evaluation we designated run1 as our official submission, since we felt that this system 3 http://ngram.sourceforge.net 440 was likely to be most successful with this task. This was based on our pre–evaluation tuning with the training data which had been made available by the task organizers. This p"
S15-2076,E06-2007,1,0.727324,", and treat this task as an unsupervised word sense discrimination or induction problem, and use the freely available open-source software package SenseClusters1 . 2 Systems We submitted three runs for subtask 2 : run1, run2, and run3. These three systems share a few basic characteristics, but differ in important respects. All use SenseClusters, and all utilize the same relatively simple pre–processing. Text was converted to lower case, and numeric values were all converted to a single string. Also, all three runs automatically determined the number of clusters (senses) using the PK2 measure (Pedersen and Kulkarni, 2006). This measure looks at the degree of change in the clustering criterion function, and stops the clustering process when the criterion function begins to plateau. This indicates that additional clustering of the data is not improving the quality of the clusters, and that further divisions will break apart relatively homogeneous senses. There are however important differences between the systems. Runs run1 and run2 rely on second– order co–occurrences, run1 uses words that co– occur near the target verb as features, and run2 uses words that occur anywhere in the contexts to be clustered. Both r"
S15-2076,S07-1087,1,0.905586,"Given different syntactic and semantic features discovered in subtask 1, it would be possible to pursue subtask 2 using a more rule based approach. However, the Duluth systems do not explicitly account for syntax or semantics and do not try to identify these kinds of patterns. While we believe such approaches are extremely useful, we are primarily interested in exploring the limits of methods that depend on purely lexical features. As a result, the Duluth systems rely on clustering target verbs based on the context in which they occur (e.g., (Sch¨utze, 1998), (Purandare and Pedersen, 2004), (Pedersen, 2007)). This follows from the distributional hypothesis (Harris, 1954). Simply put, words that are used in similar contexts may often have similar meanings. However, words with different meanings can also be used in similar contexts (e.g., antonyms) so results are often noisy. 438 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 438–442, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics The Duluth systems take a knowledge-lean approach (Pedersen, 1997), and treat this task as an unsupervised word sense discrimination or indu"
S15-2076,S10-1081,1,0.837735,"same number of clusters as found in the gold standard data, in general there were no cases where it differed radically. On average the Microcheck data had 4.3 senses, while run1 discovered 3.7. For the Wingspread data there were 3.0 senses, while run1 discovered 2.7. While the results show that the clusters themselves are noisy, in general we are pleased that our ability to predict the number of clusters is reasonably accurate. 4 Conclusions SenseClusters has participated in numerous SensEval and SemEval shared tasks that have included word sense discrimination and induction (Pedersen, 2007; Pedersen, 2010; Pedersen, 2013). In all of these prior events, the most frequent sense baseline has proven hard to beat. In general assigning all instances of a target verb to a single cluster replicates most frequent sense performance. The results in this subtask are similar, and suggest that for the moment, automatic word sense discrimination is still not a viable replacement for human lexicographic expertise. 441 adapt advise afflict ascertain ask attain avert avoid begrudge belch bludgeon bluff boo brag breeze sue teeter tense totter wing total (w) total N 182 230 179 7 573 240 240 242 19 24 32 25 36 29"
S15-2076,S13-2036,1,0.79393,"clusters as found in the gold standard data, in general there were no cases where it differed radically. On average the Microcheck data had 4.3 senses, while run1 discovered 3.7. For the Wingspread data there were 3.0 senses, while run1 discovered 2.7. While the results show that the clusters themselves are noisy, in general we are pleased that our ability to predict the number of clusters is reasonably accurate. 4 Conclusions SenseClusters has participated in numerous SensEval and SemEval shared tasks that have included word sense discrimination and induction (Pedersen, 2007; Pedersen, 2010; Pedersen, 2013). In all of these prior events, the most frequent sense baseline has proven hard to beat. In general assigning all instances of a target verb to a single cluster replicates most frequent sense performance. The results in this subtask are similar, and suggest that for the moment, automatic word sense discrimination is still not a viable replacement for human lexicographic expertise. 441 adapt advise afflict ascertain ask attain avert avoid begrudge belch bludgeon bluff boo brag breeze sue teeter tense totter wing total (w) total N 182 230 179 7 573 240 240 242 19 24 32 25 36 29 12 247 28 37 19"
S15-2076,W04-2406,1,0.676974,"milar to semantic role labeling. Given different syntactic and semantic features discovered in subtask 1, it would be possible to pursue subtask 2 using a more rule based approach. However, the Duluth systems do not explicitly account for syntax or semantics and do not try to identify these kinds of patterns. While we believe such approaches are extremely useful, we are primarily interested in exploring the limits of methods that depend on purely lexical features. As a result, the Duluth systems rely on clustering target verbs based on the context in which they occur (e.g., (Sch¨utze, 1998), (Purandare and Pedersen, 2004), (Pedersen, 2007)). This follows from the distributional hypothesis (Harris, 1954). Simply put, words that are used in similar contexts may often have similar meanings. However, words with different meanings can also be used in similar contexts (e.g., antonyms) so results are often noisy. 438 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 438–442, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics The Duluth systems take a knowledge-lean approach (Pedersen, 1997), and treat this task as an unsupervised word sense disc"
S15-2076,J98-1004,0,0.442126,"Missing"
S16-1207,N15-1169,0,0.0678419,"Missing"
S16-1207,N04-3012,1,0.562626,"ch Duluth system carried out the same preprocessing on both the WordNet and OtherDict glosses. In addition, the WordNet glosses were extended using additional information from WordNet such as the glosses of its hypernyms, hyponyms, derived forms, and meronyms. This follows naturally from the structure of WordNet and the intuitions that underlie the Extended Gloss Overlap measure (Banerjee and Pedersen, 2003b), which is 1328 Proceedings of SemEval-2016, pages 1328–1331, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics implemented in WordNet::Similarity (Pedersen et al., 2004) and UMLS::Similarity (McInnes et al., 2009). Unfortunately there was not time to expand the OtherDict glosses in similar ways, although this is at least a possibility since some other dictionaries (such as Wiktionary) provide hyponyms and hypernyms, among other relations. Task 14 allowed two kinds of systems to participate : resource–aware that only used dictionary content, and constrained that used other resources beyond dictionaries. The Duluth systems are considered resource–aware since they only use information from WordNet and OtherDict. 2 Systems All the Duluth systems start by pre–proc"
S17-2064,P12-1094,0,0.0299041,"Ramen #SingleLifeIn3Words” are trigrams. An N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: P (wn |w1n−1 ) ≈ P (wn |wn−2 , wn−1 ) (1) The assumption that the probability of a word depends only on a small number of previous words is called a Markov assumption (Markov, 2006). Given this assumption the probability of a sentence can be estimated as follows: P (w1n ) ≈ n Y P (wk |wk−2 , wk−1 ) (2) k=1 In a study on how phrasing affects memorability, (Danescu-Niculescu-Mizil et al., 2012) take a language model approach to measure the distinctiveness of memorable movie quotes. They do this 385 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 385–389, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics • Subtask B – Given a set of tweets associated with one hashtag, rank tweets from the funniest to the least funny. by evaluating a quote with respect to a “common language” model built from the newswire sections of the Brown corpus (Kucera and Francis, 1967). They find that movie quotes which are less"
S17-2064,P13-2121,0,0.092139,"Missing"
S17-2064,P15-1070,0,0.013937,"roduction tears in Ramen #SingleLifeIn3Words Humor is an expression of human uniqueness and intelligence and has drawn attention in diverse areas such as linguistics, psychology, philosophy and computer science. Computational humor draws from all of these fields and is a relatively new area of study. There is some history of systems that are able to generate humor (e.g., (Stock and Strapparava, 2003), ¨ (Ozbal and Strapparava, 2012)). However, humor detection remains a less explored and challenging problem (e.g., (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)). SemEval-2017 Task 6 (Potash et al., 2017) also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the prog"
S17-2064,P12-1074,0,0.0173048,"N-gram language models, ranking highly in the task evaluation. This paper discusses the results of our system in the development and evaluation stages and from two post-evaluation runs. 1 Introduction tears in Ramen #SingleLifeIn3Words Humor is an expression of human uniqueness and intelligence and has drawn attention in diverse areas such as linguistics, psychology, philosophy and computer science. Computational humor draws from all of these fields and is a relatively new area of study. There is some history of systems that are able to generate humor (e.g., (Stock and Strapparava, 2003), ¨ (Ozbal and Strapparava, 2012)). However, humor detection remains a less explored and challenging problem (e.g., (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)). SemEval-2017 Task 6 (Potash et al., 2017) also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a"
S17-2064,S17-2004,0,0.308529,"is an expression of human uniqueness and intelligence and has drawn attention in diverse areas such as linguistics, psychology, philosophy and computer science. Computational humor draws from all of these fields and is a relatively new area of study. There is some history of systems that are able to generate humor (e.g., (Stock and Strapparava, 2003), ¨ (Ozbal and Strapparava, 2012)). However, humor detection remains a less explored and challenging problem (e.g., (Mihalcea and Strapparava, 2006), (Zhang and Liu, 2014), (Shahaf et al., 2015), (Miller and Gurevych, 2015)). SemEval-2017 Task 6 (Potash et al., 2017) also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program. “tears”, “in”, “Ramen” and “#SingleLife"
S17-2070,S17-2005,0,0.0546711,"n all four configurations the Gloss Vector measure WordNet::Similarity::vector was used as the measure of semantic relatedness. If more than two sense changes result from these different configurations, then we say that a pun has occurred in the sentence. Systems The evaluation data for each subtask was individual sentences that are independent of each other. All sentences were tokenized so that each alphanumeric string was separated from any adjacent punctuation, and all text was converted to lowercase. Multi-word expressions (compounds) found in WordNet were identified. SemEval–2017 Task 7 (Miller et al., 2017) focused on pun identification, and was divided into three subtasks. 2.2 Subtask 2 In Subtask 2 the evaluation data consists of the instances from Subtask 1 that contain puns. The task is to identify the punning word. We took two approaches to this subtask, however both were informed by our observation that punned words often occur later in sentences. The first (run 1) was to rely on our word sense disambiguation results from Subtask 1 and identify the last word which changed senses between different runs of the WordNet::SenseRelate::AllWords disambiguation algorithm. We relied on two of the f"
S17-2070,P05-3019,1,0.659155,"of candy that was in mint condition. The pun relies on the oscillation between the flavor mint and the compound mint condition, where candy interacts with mint and mint condition interacts with collection. A heterographic pun relies on a different kind of oscillation, that is between two words that nearly sound alike, rhyme, or are nearly spelled the same. 1 http://senserelate.sourceforge.net 416 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 416–420, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics getWord (Patwardhan et al., 2005) and AllWords (Pedersen and Kolhatkar, 2009). Both have the goal of finding the assignment of senses in a context that maximizes their overall semantic relatedness (Patwardhan et al., 2003) according to measures in WordNet::Similarity2 (Pedersen et al., 2004). We relied on the Extended Gloss Overlaps measure (lesk) (Banerjee and Pedersen, 2003) and the Gloss vector measure (vector) (Patwardhan and Pedersen, 2006). The intuition behind a Lesk measure is that related words will be defined using some of the same words, and that recognizing these overlaps can serve as a means of identifying relati"
S17-2070,W06-2501,1,0.567258,"gs of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 416–420, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics getWord (Patwardhan et al., 2005) and AllWords (Pedersen and Kolhatkar, 2009). Both have the goal of finding the assignment of senses in a context that maximizes their overall semantic relatedness (Patwardhan et al., 2003) according to measures in WordNet::Similarity2 (Pedersen et al., 2004). We relied on the Extended Gloss Overlaps measure (lesk) (Banerjee and Pedersen, 2003) and the Gloss vector measure (vector) (Patwardhan and Pedersen, 2006). The intuition behind a Lesk measure is that related words will be defined using some of the same words, and that recognizing these overlaps can serve as a means of identifying relationships between words (Lesk, 1986). The Extended Gloss overlap measure (hereafter simply lesk) extends this idea by considering not only the definitions of the words themselves, but also concatenates the definitions of words that are directly related via hypernym, hyponym, and other relations according to WordNet. The Gloss Vector measure (hereafter simply vector) extends this idea by representing each word in a"
S17-2070,N09-5005,1,0.828036,"Missing"
S17-2070,N04-3012,1,0.526264,"llation, that is between two words that nearly sound alike, rhyme, or are nearly spelled the same. 1 http://senserelate.sourceforge.net 416 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 416–420, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics getWord (Patwardhan et al., 2005) and AllWords (Pedersen and Kolhatkar, 2009). Both have the goal of finding the assignment of senses in a context that maximizes their overall semantic relatedness (Patwardhan et al., 2003) according to measures in WordNet::Similarity2 (Pedersen et al., 2004). We relied on the Extended Gloss Overlaps measure (lesk) (Banerjee and Pedersen, 2003) and the Gloss vector measure (vector) (Patwardhan and Pedersen, 2006). The intuition behind a Lesk measure is that related words will be defined using some of the same words, and that recognizing these overlaps can serve as a means of identifying relationships between words (Lesk, 1986). The Extended Gloss overlap measure (hereafter simply lesk) extends this idea by considering not only the definitions of the words themselves, but also concatenates the definitions of words that are directly related via hype"
S18-1060,S18-1003,0,0.0432325,"Missing"
S18-1060,D14-1162,0,0.0965107,"Missing"
S18-1060,E17-2106,0,0.0298094,"mples. Dynamic embeddings are jointly trained with other parts of 396 2. Emojis are tied to its context only, enhancing or revealing the true underlying meaning. This can vary depending on the position or content of the tweet. often the case for Twitter. In our system, we use a multi-channel CNN layer for feature extraction. Since tweets are generally short, higher-level features for classification may not exist. Instead of adding depth to our network, we add diversity to the kernel sizes in order to keep as much n-gram information as possible. A similar model is used in (Ruder et al., 2016) (Shrestha et al., 2017). The multi-channel convolution layer consists of three parallel convolution units. They have a similar structure but different convolutional kernel sizes. Each convolution unit consists of two steps, the convolution and pooling. The convolution step is to calculate the convolutions of the resulting vector sequence from the subword embedding vector sequence T = (s1 , s2 , ..., sL ) and convolution kernels k1:M ∈ Rd,M ×r , where M is the kernel number, r is the kernel size. The kernel size represents the context window of feature extraction and the kernel number represents the number of pattern"
S18-1060,P16-1162,0,\N,Missing
S18-1077,E17-2017,0,0.141504,"Missing"
S18-1077,S18-1003,0,0.0388035,"Missing"
S18-1077,P12-2018,0,0.0364489,"ernational Workshop on Semantic Evaluation (SemEval-2018), pages 482–485 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics 3.3.1 that are able to predict instances in the low frequency classes correctly. Given that we decided to employ oversampling in order to try to improve our results on the low frequency classes which had the negative effect of degrading our performance on the more frequent classes. 3 3.1 MNB is a probabilistic classifier based on integer feature counts. It is simple yet powerful for text classification, especially for short documents (Wang and Manning, 2012). To eliminate zero counts, we use additive smoothing with a parameter value of 0.5. System Description 3.3.2 Preprocessing 3.3.3 Random Forest (RF): As an enhancement of decision tree, we use the RF classifier, which ensembles a multitude of decision trees. By fitting on sub-samples of the dataset, RF improves accuracy and reduces overfitting by averaging. We use 20 trees here. 3.4 Ensemble Classifier We build an ensemble classifier to combine the strengths of a collection of base models. The ensemble method is soft voting, where the calibrated member classifiers cast weighted votes for class"
S18-1082,W14-2608,0,0.0173775,"are phrases that are indicative of discourse segments. Examples of these include however, on the other hand, and in my opinion. These have been cited as being more common in ironic content (Far´ıas et al., 2016). Our system employs a list of 53 discourse markers. Also based on Farias et al, we measure intensifiers, like very and really that make adjectives stronger. In addition to these, we build on our curated lists of irony-indicative words with features like swear words and top words, as well as textual markers of laughter like lol and haha, which was noted to be common in ironic content (Buschmeier et al., 2014). Another word list included interjections to detect irony. Interjections are word that express feeling rather than meaning, for example, words like wow, gosh, and jeez. Indicative Parts of Speech: We also built features that measured the prevalence of several parts of speech thought to be indicative of ironic content. These included adjectives, adverbs, prepositions, and named entities. All of these features were identified by the NLTK1 POS tagger in order to count their occurrence. These counts were then normalized for the length of the tweet. Adjectives and adverbs occur more frequently in"
S18-1082,W16-0425,0,0.0150102,"training set of 3,834 tweets for both subtasks. Introduction 3 ALANIS With the invention and growth of various social networking sites, irony and other creative linguistic devices have become increasingly prevalent in online content. Particularly when considering microblogging platforms like Twitter, which encourage users to share their thoughts and opinions on a wide variety of topics, the use of irony can be extremely common. This can have strong implications for various problems in natural language processing, which often have difficulty in processing this ironic content (e.g., (Liu, 2012; Ghosh and Veale, 2016; Maynard and Greenwood, 2014)), thus motivating the development of an accurate irony detection system. While irony has many varying definitions, it is defined by the SemEval task organizers as a trope or figurative language whose actual meaning differs from what is literally enunciated. Our system, ALANIS (Automated Location and Naming of Ironic Sentences), uses a manually developed feature set and a logistic regression classifier for subtask A and a voted classifier for subtask B, achieving mean accuracies of .650 and .607 respectively on the training set and .512 and .434 Our system, ALANIS"
S18-1082,W07-0101,0,0.0463505,"o detect irony. Interjections are word that express feeling rather than meaning, for example, words like wow, gosh, and jeez. Indicative Parts of Speech: We also built features that measured the prevalence of several parts of speech thought to be indicative of ironic content. These included adjectives, adverbs, prepositions, and named entities. All of these features were identified by the NLTK1 POS tagger in order to count their occurrence. These counts were then normalized for the length of the tweet. Adjectives and adverbs occur more frequently in ironic tweets than non-ironic according to (Kreuz and Caucci, 2007). We hypothesized that prepositions and named entities would occur more often in situational irony, due to the likely need to explain the situation. Content Features: ALANIS also employs a number of features relevant to the content of the tweet in order to identify irony. These include Word Count, Punctuation, and URLs. According to Farias et al, ironic tweets tend to have excessive punctuation to catch the eyes of readers and to stress a point. Examples include ”It is really 1 worth it!!!” or ”Okay...”. Thus, heavy punctuation sometimes implies irony. Farias et al. also identified that ironic"
S18-1082,maynard-greenwood-2014-cares,0,0.0297327,"weets for both subtasks. Introduction 3 ALANIS With the invention and growth of various social networking sites, irony and other creative linguistic devices have become increasingly prevalent in online content. Particularly when considering microblogging platforms like Twitter, which encourage users to share their thoughts and opinions on a wide variety of topics, the use of irony can be extremely common. This can have strong implications for various problems in natural language processing, which often have difficulty in processing this ironic content (e.g., (Liu, 2012; Ghosh and Veale, 2016; Maynard and Greenwood, 2014)), thus motivating the development of an accurate irony detection system. While irony has many varying definitions, it is defined by the SemEval task organizers as a trope or figurative language whose actual meaning differs from what is literally enunciated. Our system, ALANIS (Automated Location and Naming of Ironic Sentences), uses a manually developed feature set and a logistic regression classifier for subtask A and a voted classifier for subtask B, achieving mean accuracies of .650 and .607 respectively on the training set and .512 and .434 Our system, ALANIS, uses manually developed feat"
S18-1149,S13-1005,0,0.157727,"is-a relationship where a hypernym is a generalization of a hyponym. The objective of SemEval–2018 Task 9 (Camacho-Collados et al., 2018) is to generate a ranked list of hypernyms when given an input hyponym and a vocabulary of candidate hypernyms. For example, the input hyponym lemongrass could yield the hypernyms [grass, oil plant, herb], where herb would be the best candidate. This scenario is illustrated in Figure 1, where the three leaf nodes are hyponyms and the root is a hypernym. 2.1 UMBC Corpus Our training corpus is the University of Maryland, Baltimore County (UMBC) WebBase Corpus (Han et al., 2013). It contains 3 billion words from paragraphs obtained from more than 100 million web pages over various domains. We use the 28 GB tokenized version of UMBC corpus which is part-of-speech tagged and divided among 408 files. There is also a vocabulary file with 218,755 unigram, bigram and trigram hypernym terms provided by task organizers. This file defines the set of possible candidate hypernyms. Figure 1: Hypernym-hyponym example Note that hypernym discovery is distinct from hypernym detection, where the problem is to detect if a hyponym-hypernym relationship exists between a given pair, such"
S18-1149,C92-2082,0,0.577738,"Missing"
S18-1149,P14-1113,0,\N,Missing
S18-1149,S18-1115,0,\N,Missing
S19-2106,S19-2007,0,0.0907283,"Missing"
S19-2106,W18-5117,0,0.0374165,"Missing"
S19-2106,W17-3006,0,0.0640059,"Missing"
S19-2106,W17-1101,0,0.024661,"ches to Identify and Categorize Offensive Tweets Ted Pedersen Department of Computer Science University of Minnesota Duluth, MN 55812 USA tpederse@d.umn.edu Abstract guage that is offensive may simply violate community standards regarding the use of profanity, but in other cases may cross over to become abusive, threatening, or dangerous. Detecting such language has proven to be a challenging problem, at least in part because it remains difficult to make distinctions between the casual and even friendly use of profanity versus more serious forms of offensive language (Fortuna and Nunes, 2018; Schmidt and Wiegand, 2017). This paper describes the Duluth systems that participated in SemEval–2019 Task 6, Identifying and Categorizing Offensive Language in Social Media (OffensEval). For the most part these systems took traditional Machine Learning approaches that built classifiers from lexical features found in manually labeled training data. However, our most successful system for classifying a tweet as offensive (or not) was a rule-based black–list approach, and we also experimented with combining the training data from two different but related SemEval tasks. Our best systems in each of the three OffensEval ta"
S19-2106,N19-1144,0,0.0766983,"Missing"
S19-2106,S19-2010,0,0.0665995,"Missing"
S19-2162,P18-1022,0,0.0322808,"n bold face in Table 4. It is significant to note that one of the person names that appears as a hyperpartisan feature is Hitler, suggesting that he may have been used as a basis for comparison in such articles. The name Arpaio 951 stance on gun control. It is a very emotional piece, however, it also provides facts and figures to bolster the position of the author. We believe it is the latter which caused the LR to (incorrectly) classify it as mainstream. We also noticed that the 30 H articles in our test data had on average much larger word counts (1,178.9) versus the 30 M articles (503.4). (Potthast et al., 2018) used average paragraph length as a feature when detecting H news, and this seems like it would have been a useful feature in this task as well. refers to a controversial sheriff in Arizona who ran for re-election in 2016 (and was defeated). Maria is Hurricane Maria, which devastated Puerto Rico in September 2017. The recovery from this natural disaster became a political issue and so its use as a feature in hyperpartisan news seems likely. The mainstream features (in upper case) include Donald and Twitter. Candidate (and now President) Trump is well known as an enthusiastic Twitter user, so t"
S19-2162,S19-2145,0,0.0624082,"Missing"
S19-2162,C18-1287,0,0.0692044,"Missing"
W02-0806,J96-2004,0,0.104774,"Missing"
W02-0806,S01-1034,1,0.717258,"duluth2 as one of its members. The same relationship exists between duluth7 and duluth8 in the Spanish lexical sample, and comparable behavior is seen in Table 3. A more surprising case is the even higher level of agreement between the most common sense baseline and the lesk corpus baseline shown in Table 2. This is not necessarily expected, and suggests that lesk corpus may not be finding a significant number of matches between the Senseval contexts and the WordNet glosses (as the lesk algorithm would hope to do) but instead may be relying on a simple default in many cases. In previous work (Pedersen, 2001) we propose a 50-25-25 rule that suggests that about half of the instances in a supervised word sense disambiguation evaluation will be fairly easy for most systems to resolve, another quarter will be harder but possible for at least some systems, and that the final quarter will be very difficult for any system to resolve. Table 2: Pairwise Agreement English system pair both-one-zero kappa Nouns common lesk corp 0.49 0.06 0.44 0.87 duluth2 duluth3 0.60 0.08 0.32 0.82 0.53 0.11 0.36 0.78 lesk corp umcp duluth2 duluthB 0.54 0.14 0.32 0.70 iit1 iit2 0.24 0.13 0.63 0.69 duluth3 duluthB 0.56 0.15 0"
W02-0812,J98-1006,0,0.0984978,"Missing"
W02-0812,P96-1006,0,0.0670792,"Missing"
W02-0812,A00-2009,1,0.885284,"Missing"
W02-0812,N01-1011,1,0.834233,"m, and then profiles the S ENSEVAL -1 and S ENSEVAL 2 lexical sample data that is used in this evaluation. There are two types of analysis presented. First, the accuracy of the member classifiers in the Duluth38 ensemble are evaluated individually and in pairwise combinations. Second, the agreement between Duluth38 and the top two participating systems in S ENSEVAL -1 and S ENSEVAL -2 is compared. This paper concludes with a review of the origins of our approach. Since the focus here is on analysis, implementation level details are not extensively discussed. Such descriptions can be found in (Pedersen, 2001b) or (Pedersen, 2002). 2 Lexical Features Unigram features represent words that occur five or more times in the training examples associated with a given target word. A stop–list is used to eliminate high frequency function words as features. For example, if the target word is water and the training example is I water the flowering flowers, the unigrams water, flowering and flowers are evaluated as possible unigram features. No stemming or other morphological processing is performed, so flowering and flowers are considered as distinct unigrams. I and the are not considered as possible feature"
W02-0812,S01-1034,1,0.753921,"m, and then profiles the S ENSEVAL -1 and S ENSEVAL 2 lexical sample data that is used in this evaluation. There are two types of analysis presented. First, the accuracy of the member classifiers in the Duluth38 ensemble are evaluated individually and in pairwise combinations. Second, the agreement between Duluth38 and the top two participating systems in S ENSEVAL -1 and S ENSEVAL -2 is compared. This paper concludes with a review of the origins of our approach. Since the focus here is on analysis, implementation level details are not extensively discussed. Such descriptions can be found in (Pedersen, 2001b) or (Pedersen, 2002). 2 Lexical Features Unigram features represent words that occur five or more times in the training examples associated with a given target word. A stop–list is used to eliminate high frequency function words as features. For example, if the target word is water and the training example is I water the flowering flowers, the unigrams water, flowering and flowers are evaluated as possible unigram features. No stemming or other morphological processing is performed, so flowering and flowers are considered as distinct unigrams. I and the are not considered as possible feature"
W03-0301,P01-1005,0,0.0158626,"Missing"
W03-0301,J93-2003,0,0.0943816,"total of 27 submissions from the seven teams, where 14 sets of results were submitted for the English-French subtask, and 13 for the Romanian-English subtask. Of the 27 total submissions, there were 17 in the Limited resources subtask, and 10 in the Unlimited resources subtask. Tables 2 and 3 show all of the submissions for each team in the two subtasks, and provide a brief description of their approaches. While each participating system was unique, there were a few unifying themes. Four teams had approaches that relied (to varying degrees) on an IBM model of statistical machine translation (Brown et al., 1993). UMD was a straightforward implementation of IBM Model 2, BiBr employed a boosting procedure in deriving an IBM Model 1 lexicon, Ralign used IBM Model 2 as a foundation for their recursive splitting procedure, and XRCE used IBM Model 4 as a base for alignment with lemmatized text and bilingual lexicons. Two teams made use of syntactic structure in the text to be aligned. ProAlign satisfies constraints derived from a dependency tree parse of the English sentence being 3 The two teams that did not participate in English-French were Fourday and RACAI. aligned. BiBr also employs syntactic constra"
W03-0301,W03-0305,0,0.0844355,"Missing"
W03-0301,W03-0306,0,0.0345882,"Missing"
W03-0301,W03-0302,0,0.0687715,"Missing"
W03-0301,C00-2163,0,0.0833749,"xts, provided together with manually determined word alignments. The main purpose of these data was to enable participants to better understand the format required for the word alignment result files. Trial sets consisted of 37 English-French, and 17 Romanian-English aligned sentences. guaranteed to be a subset of the Probable alignment set. The annotators did not produce any NULL alignments. Instead, we assigned NULL alignments as a default backup mechanism, which forced each word to belong to at least one alignment. The English-French aligned data were produced by Franz Och and Hermann Ney (Och and Ney, 2000). For Romanian-English, annotators were instructed to assign an alignment to all words, with specific instructions as to when to assign a NULL alignment. Annotators were not asked to assign a Sure or Probable label. Instead, we had an arbitration phase, where a third annotator judged the cases where the first two annotators disagreed. Since an inter-annotator agreement was reached for all word alignments, the final resulting alignments were considered to be Sure alignments. 3 Evaluation Measures Evaluations were performed with respect to four different measures. Three of them – precision, reca"
W03-0301,W03-0304,0,0.0515658,"Missing"
W03-0301,W03-0309,1,0.824107,"Missing"
W03-0301,W03-0308,0,0.039329,"Missing"
W03-0301,C02-1002,0,0.0524515,"Missing"
W03-0301,W03-0303,0,0.0445012,"Missing"
W03-0309,J93-2003,0,0.041289,"better. We also varied the Model 2 distortion parameters among the values 2, 4, and 6, but did not observe any significant differences in performance as a result. 1 Introduction Word alignment is a crucial part of any Machine Translation system, since it is the process of determining which words in a given source and target language sentence pair are translations of each other. This is a token level task, meaning that each word (token) in the source text is aligned with its corresponding translation in the target text. The Duluth Word Alignment System is a Perl implementation of IBM Model 2 (Brown et al., 1993). It learns a probabilistic model from sentence aligned parallel text that can then be used to align the words in another such text (that was not a part of the training process). A parallel text consists of a source language text and its translation into some target language. If we have determined which sentences are translations of each other then the text is said to be sentence aligned, where we call a source and target language sentence that are translations of each other a sentence pair. (Brown et al., 1993) introduced five statistical translation models (IBM Models 1 – 5). In general a st"
W03-0309,C00-2163,0,0.38419,"l it converges. 3 System Components The Duluth Word Alignment System consists of two preprocessing programs (plain2snt and snt2matrix) and one that learns the word alignment model (model2). These are all implemented in Perl. The plain2snt program converts raw sentence aligned parallel text into the snt format, where each word type in the source and target text is represented as a unique integer. This program also outputs two vocabulary files for the source and target languages that list the word types and their integer values. This is closely modeled after what is done in the GIZA++ tool kit (Och and Ney, 2000b). The snt2matrix program takes the snt file from plain2snt as input and outputs two files. The first is an adjacency list of possible word translations for each sentence pair. The second file consists of a table of alignment positions that were observed in the training corpora. The value of the distortion factor determines which positions may be aligned with each other. The program model2 implements IBM Model 2 as discussed in the previous section. This program requires the vocabulary files, the snt file, the alignment positional probability file and the adjacency list file created by the pl"
W03-0309,P00-1056,0,0.218514,"l it converges. 3 System Components The Duluth Word Alignment System consists of two preprocessing programs (plain2snt and snt2matrix) and one that learns the word alignment model (model2). These are all implemented in Perl. The plain2snt program converts raw sentence aligned parallel text into the snt format, where each word type in the source and target text is represented as a unique integer. This program also outputs two vocabulary files for the source and target languages that list the word types and their integer values. This is closely modeled after what is done in the GIZA++ tool kit (Och and Ney, 2000b). The snt2matrix program takes the snt file from plain2snt as input and outputs two files. The first is an adjacency list of possible word translations for each sentence pair. The second file consists of a table of alignment positions that were observed in the training corpora. The value of the distortion factor determines which positions may be aligned with each other. The program model2 implements IBM Model 2 as discussed in the previous section. This program requires the vocabulary files, the snt file, the alignment positional probability file and the adjacency list file created by the pl"
W03-0309,C88-1016,0,\N,Missing
W03-0309,J90-2002,0,\N,Missing
W04-0802,S01-1014,0,0.0294776,"HKUST me ts systems are maximum entropy classifiers. The HKUST comb t and HKUST comb ts systems are voted classifiers that combine a new Kernel PCA model with a maximum entropy model and a boosting–based model. The HKUST comb2 t and HKUST comb2 ts are voted classifiers that combine a new Kernel PCA model with a maximum entropy model, a boosting–based model, and a Naive Bayesian model. 5.4 UMD The UMD team from the University of Maryland entered (UMD–SST) in the t task. UMD–SST is a supervised sense tagger based on the Support Vector Machine learning algorithm, and is described more fully in (Cabezas et al., 2001). 5.5 Duluth The Duluth team from the University of Minnesota, Duluth had one system (Duluth-ELSS) that participated in the t task. This system is an ensemble of three bagged decision trees, each based on a different type of lexical feature. This system was known as Duluth3 in S ENSEVAL -2, and it is described more fully in (Pedersen, 2001). 6 Results All systems attempted all of the test instances, so precision and recall are identical, hence we report Table 2: t Subtask Results System nusmlst HKUST comb t HKUST comb2 t HKUST me t FL-MIX FC-MIX UMD-SST Duluth-ELSS Baseline (majority) Accuracy"
W04-0802,W02-0817,1,0.825136,"her than using the sense inventory from a dictionary we follow the suggestion of (Resnik and Yarowsky, 1999) and use the translations of the target words into a second language. In this task for S ENSEVAL -3, the contexts are in English, and the “sense tags” for the English target words are their translations in Hindi. This paper outlines some of the major issues that arose in the creation of this task, and then describes the participating systems and summarizes their results. 2 Open Mind Word Expert The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations 1 . To overcome the current lack of tagged data and the limitations imposed by the creation of such data using trained lexicographers, the Open Mind Word 1 Multilingual Open Mind Word Expert can be accessed at http://teach-computers.org/word-expert/english-hindi Expert system enables the collection of semantically annotated corpora over the Web. Tagged examples are collected using a Web-based application that allows contributors to annotate words with their meanings. The tagging exercise proceeds as follows. For each target word the system extracts a set"
W04-0802,S01-1034,1,0.83931,"based model, and a Naive Bayesian model. 5.4 UMD The UMD team from the University of Maryland entered (UMD–SST) in the t task. UMD–SST is a supervised sense tagger based on the Support Vector Machine learning algorithm, and is described more fully in (Cabezas et al., 2001). 5.5 Duluth The Duluth team from the University of Minnesota, Duluth had one system (Duluth-ELSS) that participated in the t task. This system is an ensemble of three bagged decision trees, each based on a different type of lexical feature. This system was known as Duluth3 in S ENSEVAL -2, and it is described more fully in (Pedersen, 2001). 6 Results All systems attempted all of the test instances, so precision and recall are identical, hence we report Table 2: t Subtask Results System nusmlst HKUST comb t HKUST comb2 t HKUST me t FL-MIX FC-MIX UMD-SST Duluth-ELSS Baseline (majority) Accuracy 63.4 62.0 61.4 60.6 60.3 60.3 59.4 58.2 51.9 Table 3: ts Subtask Results System nusmlsts FL-MIX FC-MIX HKUST comb ts HKUST comb2 ts HKUST me ts Baseline (majority) Accuracy 67.3 64.1 64.1 63.8 63.8 60.8 55.8 the single Accuracy figure. Tables 2 and 3 show results for the t and ts subtasks, respectively. We note that the participating syste"
W04-0839,W02-1006,0,0.0787142,"Missing"
W04-0839,J92-1001,0,0.128176,"Missing"
W04-0839,W04-2404,1,0.635174,"Missing"
W04-0839,P96-1006,0,0.270991,"Missing"
W04-0839,N01-1011,1,0.926133,"Missing"
W04-0839,S01-1034,1,0.85835,"Missing"
W04-0839,W02-0806,1,0.903012,"Missing"
W04-0839,J01-3001,0,0.14163,"Missing"
W04-0850,W04-2404,1,0.915722,"aive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. However, the Duluth-ELSS system only uses a three member ensemble to explore the efficacy of combinations of different lexical features via simple ensembles. We plan to carry out a more detailed analysis of the degree to which unigram, bigram, and co–occurrence features are useful sources of information for disambiguation. We will also conduct an analysis of the complementary and redundant nature of lexical and syntactic features, as we have done in (Mohammad and Pedersen, 2004a) for the S ENSEVAL -1, S ENSEVAL 2, and line, hard, serve, and interest data. The SyntaLex system (Mohammad and Pedersen, 2004b) also participated in the English lexical sample task of S ENSEVAL –3 and is a sister system to DuluthELSS. It uses lexical and syntactic features with bagged decision trees and serves as a convenient point of comparison. We are particularly interested to see if there are words that are better disambiguated using syntactic versus lexical features, and in determining how to best combine classifiers based on different feature sets in order to attain improved accuracy."
W04-0850,W04-0839,1,0.87156,"aive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. However, the Duluth-ELSS system only uses a three member ensemble to explore the efficacy of combinations of different lexical features via simple ensembles. We plan to carry out a more detailed analysis of the degree to which unigram, bigram, and co–occurrence features are useful sources of information for disambiguation. We will also conduct an analysis of the complementary and redundant nature of lexical and syntactic features, as we have done in (Mohammad and Pedersen, 2004a) for the S ENSEVAL -1, S ENSEVAL 2, and line, hard, serve, and interest data. The SyntaLex system (Mohammad and Pedersen, 2004b) also participated in the English lexical sample task of S ENSEVAL –3 and is a sister system to DuluthELSS. It uses lexical and syntactic features with bagged decision trees and serves as a convenient point of comparison. We are particularly interested to see if there are words that are better disambiguated using syntactic versus lexical features, and in determining how to best combine classifiers based on different feature sets in order to attain improved accuracy."
W04-0850,W96-0208,0,0.0406518,"ed with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features. Therefore, our interest in these experiments is more in the effect of the different features sets than in the variations that would be possible if we used learning algorithms other than decision trees. We are satisfied that decision trees are a reasonable choice of learning algorithm. They have a long history of use in word sense disambiguation, dating back to early work by (Black, 1988), and have fared well in comparative studies such as (Mooney, 1996) and (Pedersen and Bruce, 1997). In the former they were used with unigram features and in the latter they were used with a small set of features that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. In (Pedersen, 2001a) we introduced the use of decision trees based strictly on bigram features. While we might squeeze out a few extra points of performance by using more complicated methods, we believe that this would obscure our ability to study and understand the effects of different kinds of features. Decision trees have the furthe"
W04-0850,A00-2009,1,0.847489,"cal sample task came from WordSmyth. Despite this our system relied on WordNet verb senses and glosses to make relatedness judgments, and then used a mapping from WordNet senses to WordSmyth to produce reportable answers. There were 178 instances where the WordNet sense found by our system was not mapped to WordSmyth. Rather than attempt to create our own mapping of WordNet to WordSmyth, we simply threw these instances out of the evaluation set, which does lead to somewhat less coverage for the unsupervised system for the verbs. 5 Future Work The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. However, the Duluth-ELSS system only uses a three member ensemble to explore the efficacy of combinations of different lexical features via simple ensembles. We plan to carry out a more detailed analysis of the degree to which unigram, bigram, and co–occurrence features are useful sources of information for disambiguation. We will also conduct an analysis of the complementary and redundant nature of lexical"
W04-0850,N01-1011,1,0.930049,"le tasks for the English, Spanish, Catalan, Basque, Romanian and MultiLingual English-Hindi data. The unsupervised system uses measures of semantic relatedness to find the sense of the target word that is most related to the senses of its neighbors. It participated in the English lexical sample task. 1 Introduction The Duluth systems participated in various lexical sample tasks in S ENSEVAL -3, using both supervised and unsupervised methodologies. The supervised lexical sample system that participated in S ENSEVAL -3 is the Duluth3 (English) or Duluth8 (Spanish) system as used in S ENSEVAL 2 (Pedersen, 2001b). It has been renamed for S ENSEVAL -3 as Duluth-xLSS, where x is a one letter abbreviation of the language to which it is being applied, and LSS stands for Lexical Sample Supervised. The idea behind this system is to learn three bagged decision trees, one using unigram features, another using bigram features, and a third using co–occurrences with the target word as features. This system only uses surface lexical features, so it can be easily applied to a wide range of languages. For S ENSEVAL -3 this system participated in the English, Spanish, Basque, Catalan, Romanian, and MultiLingual (E"
W04-0850,S01-1034,1,0.845933,"le tasks for the English, Spanish, Catalan, Basque, Romanian and MultiLingual English-Hindi data. The unsupervised system uses measures of semantic relatedness to find the sense of the target word that is most related to the senses of its neighbors. It participated in the English lexical sample task. 1 Introduction The Duluth systems participated in various lexical sample tasks in S ENSEVAL -3, using both supervised and unsupervised methodologies. The supervised lexical sample system that participated in S ENSEVAL -3 is the Duluth3 (English) or Duluth8 (Spanish) system as used in S ENSEVAL 2 (Pedersen, 2001b). It has been renamed for S ENSEVAL -3 as Duluth-xLSS, where x is a one letter abbreviation of the language to which it is being applied, and LSS stands for Lexical Sample Supervised. The idea behind this system is to learn three bagged decision trees, one using unigram features, another using bigram features, and a third using co–occurrences with the target word as features. This system only uses surface lexical features, so it can be easily applied to a wide range of languages. For S ENSEVAL -3 this system participated in the English, Spanish, Basque, Catalan, Romanian, and MultiLingual (E"
W04-2404,P94-1020,0,0.075095,"Missing"
W04-2404,J98-1006,0,0.0653756,"Missing"
W04-2404,H93-1051,0,0.0642397,"Missing"
W04-2404,W02-1006,0,0.0554747,"etter by letter, and a period of time. The intended sense, a charm or incantation, can be identified based on the context, which in this case includes bewitching and a reference to a famous young wizard. Word sense disambiguation is often approached by supervised learning techniques. The training data consists of sentences which have potential target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments that measure the redundan"
W04-2404,P97-1009,0,0.0647713,"Missing"
W04-2404,J92-1001,0,0.14749,"ich have potential target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble. We find that simple combinations of lexical and syntactic features can result in very high disam"
W04-2404,P96-1006,0,0.060125,"ial target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble. We find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy,"
W04-2404,N01-1011,1,0.94804,"ntation, to read out letter by letter, and a period of time. The intended sense, a charm or incantation, can be identified based on the context, which in this case includes bewitching and a reference to a famous young wizard. Word sense disambiguation is often approached by supervised learning techniques. The training data consists of sentences which have potential target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments"
W04-2404,S01-1034,1,0.905324,"ntation, to read out letter by letter, and a period of time. The intended sense, a charm or incantation, can be identified based on the context, which in this case includes bewitching and a reference to a famous young wizard. Word sense disambiguation is often approached by supervised learning techniques. The training data consists of sentences which have potential target words tagged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments"
W04-2404,W02-0806,1,0.911048,"Missing"
W04-2404,J01-3001,0,0.0867841,"ged by a human expert with their intended sense. Numerous learning algorithms, such as, Naive Bayesian classifiers, Decision Trees and Neural Networks have been used to learn models of disambiguation. However, both (Pedersen, 2001a) and (Lee and Ng, 2002) suggest that different learning algorithms result in little change in overall disambiguation results, and that the real determiner of accuracy is the set of features that are employed. Previous work has shown that using different combinations of features is advantageous for word sense disambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)). However, less attention is paid to determining what the minimal set of features necessary to attain high accuracy disambiguation are. In this paper we present experiments that measure the redundancy in disambiguation accuracy achieved by classifiers using two different sets of features, and we also determine an upper bound on the accuracy that could be attained via the combination of such classifiers into an ensemble. We find that simple combinations of lexical and syntactic features can result in very high disambiguation accuracy, via an extensive set of exper"
W04-2404,P95-1026,0,0.444294,"Missing"
W04-2406,E99-1028,0,0.0109163,"ich contexts are the most similar to each other. This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts. Hence, word sense discrimination reduces to the problem of finding classes of similar contexts such that each class represents a single word sense. Put another way, contexts that are grouped together in the same class represent a particular word sense. While there has been some previous work in sense discrimination (e.g., (Sch¨utze, 1992), (Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998), (Sch¨utze, 1998), (Fukumoto and Suzuki, 1999)), by comparison it is much less than that devoted to word sense disambiguation, which is the process of assigning a meaning to a word from a predefined set of possibilities. However, solutions to disambiguation usually require the availability of an external knowledge source or manually created sense– tagged training data. As such these are knowledge intensive methods that are difficult to adapt to new domains. By contrast, word sense discrimination is an unsupervised clustering problem. This is an attractive methodology because it is a knowledge lean approach based on evidence found in simpl"
W04-2406,W97-0322,1,0.902151,"of grouping these instances of the target word together by determining which contexts are the most similar to each other. This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts. Hence, word sense discrimination reduces to the problem of finding classes of similar contexts such that each class represents a single word sense. Put another way, contexts that are grouped together in the same class represent a particular word sense. While there has been some previous work in sense discrimination (e.g., (Sch¨utze, 1992), (Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998), (Sch¨utze, 1998), (Fukumoto and Suzuki, 1999)), by comparison it is much less than that devoted to word sense disambiguation, which is the process of assigning a meaning to a word from a predefined set of possibilities. However, solutions to disambiguation usually require the availability of an external knowledge source or manually created sense– tagged training data. As such these are knowledge intensive methods that are difficult to adapt to new domains. By contrast, word sense discrimination is an unsupervised clustering problem. This is an attractive methodolo"
W04-2406,N03-3004,1,0.812338,"nce in a separate cluster and merge a pair of clusters at each iteration until there is only a single cluster remaining. Divisive methods start with all instances in the same cluster and split one cluster into two during each iteration until all instances are in their own cluster. The most widely known criteria functions used with hierarchical agglomerative algorithms are single link, complete link, and average link, also known as UPGMA. (Sch¨utze, 1998) points out that single link clustering tends to place all instances into a single elongated cluster, whereas (Pedersen and Bruce, 1997) and (Purandare, 2003) show that hierarchical agglomerative clustering using average link (via McQuitty’s method) fares well. Thus, we have chosen to use average link/UPGMA as our criteria function for the agglomerative experiments. In similarity space, each instance can be viewed as a node in a weighted graph. The weights on edges joining two nodes indicate their pairwise similarity as measured by the cosine between the context vectors that represent the pair of instances. When agglomerative clustering starts, each node is in its own cluster and is considered to be the centroid of that cluster. At each iteration,"
W04-2406,J98-1004,0,0.880692,"Missing"
W05-0809,C00-2163,0,\N,Missing
W05-0809,J93-2003,0,\N,Missing
W05-0809,W05-0817,0,\N,Missing
W05-0809,W05-0814,0,\N,Missing
W05-0809,W05-0808,0,\N,Missing
W05-0809,W05-0811,0,\N,Missing
W05-0809,W05-0810,0,\N,Missing
W05-0809,W05-0801,0,\N,Missing
W05-0809,W05-0819,0,\N,Missing
W05-0809,W05-0818,0,\N,Missing
W05-0809,W05-0812,0,\N,Missing
W05-0809,W03-0301,1,\N,Missing
W05-0809,J03-1002,0,\N,Missing
W05-0809,W05-0815,0,\N,Missing
W05-0809,W03-0320,1,\N,Missing
W05-0809,W05-0813,0,\N,Missing
W06-2004,N03-2023,0,0.0540143,"Missing"
W06-2004,W97-0322,1,0.687559,"have been specified in a manually created list. Note that with smaller numbers of contexts (usually 200 or fewer), we lower the frequency threshold to two or more. In general PMI is known to have a bias towards pairs of words (bigrams) that occur a small number of times and only with each other. In this work that is a desirable quality, since that will tend to identify pairs of words that are very strongly associated with each other and also provide unique discriminating information. Pedersen, 2004), which builds upon earlier work in word sense discrimination, including (Schu¨ tze, 1998) and (Pedersen and Bruce, 1997). Our method treats each occurrence of an ambiguous name as a context that is to be clustered with other contexts that also include the same name. In this paper, each context consists of about 50 words, where the ambiguous name is generally in the middle of the context. The goal is to cluster similar contexts together, based on the presumption that the occurrences of a name that appear in similar contexts will refer to the same underlying entity. This approach is motivated by both the distributional hypothesis (Harris, 1968) and the strong contextual hypothesis (Miller and Charles, 1991). 2.1"
W06-2004,E06-2007,1,0.824583,"ntext is now represented by the vector of words with which it occurred in the feature selection data. If a word does not have a corresponding entry in the co–occurrence matrix, then it is simply removed from the context. After all the words in the context are checked, then all of the vectors that are selected are averaged together to create a vector representation of the context. Then these contexts are clustered into a pre– specified number of clusters using the k–means algorithm. Note that we are currently developing methods to automatically select the number of clusters in the data (e.g., (Pedersen and Kulkarni, 2006)), although we have not yet applied them to this particular work. 3 4 Experimental Data We use data in four languages in these experiments, Bulgarian, English, Romanian, and Spanish. The Language Salad 4.1 Raw Corpora In this paper, we explore the creation of a second order representation for a set of evaluation contexts using three different sets of feature selection data. The co–occurrence matrix may be derived from the evaluation contexts themselves, or from a separate set of contexts in a different language, or from the combination of these two (the Salad or Mix). For example, suppose we h"
W06-2004,W04-2406,1,0.773666,"experimentally) reveals that Ronaldo the soccer player tends to occur more so than any other single entity named Ronaldo, so while there is a bit more noise for Ronaldo, there is not really a significant ambiguity. For the Spanish results we only note one pair (George Bush - Tony Blair) where the Mix of English and Spanish results in the best performance. 31 the mix of English and evaluation contexts, in order to perform more accurate name discrimination. contexts based on the number of features they have in common with other evaluation contexts. In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. However, we see examples in these results that suggests this may not always be the case. In the Bulgarian results, the largest number of Bulgarian contexts are for NATO-USA, but the Mix performs quite a bit better than Bulgarian only. In the case of Romanian, again NATO-USA has the largest number of contexts, but the Mix still does better than Romanian only. And in Spanish, Mexico-India has the largest number of contexts and English-only does better. Thus, even in cases where w"
W06-2004,J98-1004,0,\N,Missing
W06-2501,O97-1002,0,0.993392,"e bat, hitting the ball into the stands. A reader likely uses domain knowledge of sports along with the realization that the baseball Ted Pedersen Department of Computer Science University of Minnesota, Duluth Duluth, MN, 55812, USA tpederse@d.umn.edu senses of hitting, bat, ball and stands are all semantically related, in order to determine that the event being described is a baseball game. Consequently, a number of techniques have been proposed over the years, that attempt to automatically compute the semantic relatedness of concepts to correspond closely with human judgments (Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Leacock and Chodorow, 1998). It has also been shown that these techniques prove useful for tasks such as word sense disambiguation (Patwardhan et al., 2003), real-word spelling correction (Budanitsky and Hirst, 2001) and information extraction (Stevenson and Greenwood, 2005), among others. In this paper we introduce a WordNet-based measure of semantic relatedness inspired by Harris’ Distributional Hypothesis (Harris, 1985). The distributional hypothesis suggests that words that are similar in meaning tend to occur in similar linguistic contexts. Additionally, numerous studies (Car"
W06-2501,C94-1049,0,0.0365321,"of the target word in LDOCE, and that meaning is selected whose vector is mathematically closest to that of the context. Our approach differs from theirs in two primary respects. First, rather than creating an aggregate vector for the context we compare the vector of each meaning of the ambiguous word with the vectors of each of the meanings of the words in the context. This adds another level of indirection in the comparison and attempts to use only the relevant meanings of the context words. Secondly, we use the structure of WordNet to augment the short glosses with other related glosses. (Niwa and Nitta, 1994) compare dictionary based vectors with co–occurrence based vectors, where the vector of a word is the probability that an origin word occurs in the context of the word. These two representations are evaluated by applying them to real world applications and quantifying the results. Both measures are first applied to word sense disambiguation and then to the learning of positives or negatives, where it is required to determine whether a word has a positive or negative connotation. It was observed that the co– occurrence based idea works better for the word sense disambiguation and the dictionary"
W06-2501,J98-1004,0,0.739337,"Missing"
W06-2501,P05-1047,0,0.0389194,"and stands are all semantically related, in order to determine that the event being described is a baseball game. Consequently, a number of techniques have been proposed over the years, that attempt to automatically compute the semantic relatedness of concepts to correspond closely with human judgments (Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Leacock and Chodorow, 1998). It has also been shown that these techniques prove useful for tasks such as word sense disambiguation (Patwardhan et al., 2003), real-word spelling correction (Budanitsky and Hirst, 2001) and information extraction (Stevenson and Greenwood, 2005), among others. In this paper we introduce a WordNet-based measure of semantic relatedness inspired by Harris’ Distributional Hypothesis (Harris, 1985). The distributional hypothesis suggests that words that are similar in meaning tend to occur in similar linguistic contexts. Additionally, numerous studies (Carnine et al., 1984; Miller and Charles, 1991; McDonald and Ramscar, 2001) have shown that context plays a vital role in defining the meanings of words. (Landauer and Dumais, 1997) describe a context vector-based method that simulates learning of word meanings from raw text. (Sch¨utze, 199"
W07-1002,W02-1401,0,0.0166536,"mbiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 2005) use the χ2 measure based on statistics obtained from WWW search engines to compute valu"
W07-1002,J93-1003,0,0.0123881,"of nominal compounds extracted from MEDLINE abstracts. The novel contribution of our study is in demonstrating and validating a corpus-based method for determining the syntactic structure of medical terms that relies on using the statistical measure of association, the Log Likelihood Ratio, described in the following section. 3 Log Likelihood Ratio The Log Likelihood Ratio (G2 ) is a “goodness of fit” statistic first proposed by (Wilks, 1938) to test if a given piece of data is a sample from a set of data with a specific distribution described by a hypothesized model. It was later applied by (Dunning, 1993) as a way to determine if a sequence of N words (Ngram) came from an independently distributed sample. (Pedersen et al., 1996) pointed out that there exists theoretical assumptions underlying the G2 measure that were being violated therefore making them unreliable for significance testing. (Moore, 2004) provided additional evidence that although G2 may not be useful for determining the significance of an event, its near equivalence to mutual information makes it an appropriate measure of word association. (McInnes, 2004) applied G2 to the task of extracting three and four word collocations fro"
W07-1002,P96-1003,0,0.0370763,"erred. (Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 20"
W07-1002,J93-1005,0,0.221735,"rmining the structure of medical terms. Third, we describe the training corpus and discuss the compilation of a test set of medical terms and human expert annotation of those terms. Last, we present the results of a preliminary validation of the method and discuss several possible future directions. 2 Previous Work The problem of resolving structural ambiguity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and de10 pendency. The proponents of the adjacency model ((Liberman and Sproat, 199"
W07-1002,N04-1016,0,0.103494,", there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] is preferred. (Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993"
W07-1002,E99-1005,0,0.0245296,"ceptability of A[YZ] one would compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 2005) use the χ2 measure based on statistics obtained from WWW search engines to compute values to determine acceptability of a syntactic analysis for nomina"
W07-1002,P95-1007,0,0.147887,"ed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and de10 pendency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] is preferred. (Lauer and Dras, 1994) and (Lau"
W07-1002,W04-3243,0,0.0150446,"he following section. 3 Log Likelihood Ratio The Log Likelihood Ratio (G2 ) is a “goodness of fit” statistic first proposed by (Wilks, 1938) to test if a given piece of data is a sample from a set of data with a specific distribution described by a hypothesized model. It was later applied by (Dunning, 1993) as a way to determine if a sequence of N words (Ngram) came from an independently distributed sample. (Pedersen et al., 1996) pointed out that there exists theoretical assumptions underlying the G2 measure that were being violated therefore making them unreliable for significance testing. (Moore, 2004) provided additional evidence that although G2 may not be useful for determining the significance of an event, its near equivalence to mutual information makes it an appropriate measure of word association. (McInnes, 2004) applied G2 to the task of extracting three and four word collocations from raw text. G2 , formally defined for trigrams in Equation 4, compares the observed frequency counts with the counts that would be expected if the words in the trigram (3-gram; a sequence of three words) corresponded to the hypothesized model. G2 = 2 ∗ X x,y,z nxyz ∗ log( nxyz ) mxyz (4) The parameter n"
W07-1002,W05-0603,0,0.19441,"ould compute the acceptability of A[XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. (Lapata and Keller, 2004) results also support this assertion. The difference between the approaches within the two models is the computation of acceptability. Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)). One of the main problems with using frequency counts or statistical methods for structural ambiguity resolution is the sparseness of data; however, (Resnik and Hearst, 1993) used conceptual associations (associations between groups of terms deemed to form conceptual units) in order to alleviate this problem. (Lapata and Keller, 2004) use the document counts returned by WWW search engines. (Nakov and Hearst, 2005) use the χ2 measure based on statistics obtained from WWW search engines to compute values to determine acceptability of a syntactic analysis for nominal compounds. This method is t"
W07-1002,A97-1056,1,0.604216,"g asleep” as right-branching (Model 3) where the words “staying asleep” are grouped together. This is an example based on informal observations; however, it does suggest a utility in constructing frame-based representation of at least some clinical terms. 7 Limitations The main limitation of the G2 method is the exponential growth in the number of models to be evaluated with the growth in the length of the term. This limitation can be partly alleviated by either only considering adjacent models and limiting the length to 5-6 words, or using a forward or backward sequential search proposed by (Pedersen et al., 1997) for the problem of selecting models for the Word Sense Disambiguation task. 8 Conclusions and Future Work This paper presented a simple but effective method based on G2 to determine the internal structure of three-word noun phrase medical terms. The ability to determine the syntactic structure that gives rise to a particular semantic interpretation of a medical term may enable accurate mapping of unstructured medical text to standardized terminologies and nomenclatures. Future directions to improve the accuracy of our method include determining how other measures of association, such as dice"
W07-1002,J93-2005,0,0.00942693,"uity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and de10 pendency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z] is preferred. (Lauer and D"
W07-1002,W93-0307,0,0.136242,"resolving structural ambiguity has been previously addressed in the computational linguistics literature. There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993), to corpus-based, which is the approach discussed in this paper. (Marcus, 1980) presents an early example of a corpus-based approach to syntactic ambiguity resolution. One type of structural ambiguity that has received much attention has to do with nominal compounds as seen in the work of (Resnik, 1993), (Resnik and Hearst, 1993), (Pustejovsky et al., 1993), and (Lauer, 1995). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and de10 pendency. The proponents of the adjacency model ((Liberman and Sproat, 1992), (Resnik, 1993) and (Pustejovsky et al., 1993)) argue that, given a three word noun phrase XYZ, there are two possible analyzes [[XY]Z] and [X[YZ]]. The correct analysis is chosen based on the “acceptability” of the adjacent bigrams A[XY] and A[YZ]. If A[XY] is more acceptable than A[YZ], then the left-branching analysis [[XY]Z"
W11-0317,W04-0813,0,0.0301456,"Missing"
W11-0317,J93-1003,0,0.0597321,". A short example of the extended definition for the acronym FDP when referring to fructose diphosphate is: “ Diphosphoric acid esters of fructose. The fructose diphosphate isomer is most prevalent. fructose diphosphate.” After the extended definition is obtained, we create the second-order vector by first creating a word by word co-occurrence matrix in which the rows represent the content words in the long-forms, extended definition, and the columns represent words that co-occur in Medline abstracts with the words in the definition. Each cell in this matrix contains the Log Likelihood Ratio (Dunning (1993)) of the word found in the row and the word in the column. Second, each word in the long-forms, extended definition is replaced by its corresponding vector, as given in the co-occurrence matrix. The centroid of these vectors constitutes the second order co-occurrence vector used to represent the long-form. For example, given the example corpus containing two instances: 1) The metabolites, glucose fructose and their phosphoric acid esters are changed due to the effect of glycolytic enzymes, and 2) The phosphoric acid combined with metabolites decreases the intensity. Figure 1 shows how the seco"
W11-0317,J98-1001,0,0.168676,"Missing"
W11-0317,P02-1021,1,0.799146,"Missing"
W11-0317,W01-0516,0,0.0880658,"Missing"
W11-0317,W06-2501,1,0.867577,"is most similar is assigned to the ambiguous word. FEATURES Extended Definition for Fructose Diphosphate intensity decreases combined enzymes effect fructose glycolyte esters changed acid esters acid phosphoric fructose glucose metabolites disphosphoric 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .3 0 .2 0 0 0 0 .5 0 0 0 0 0 0 0 0 0 .1 0 0 0 0 .1 0 0 0 0 0 0 0 0 0 0 0 0 0 diphosphate 0 0 0 0 0 0 0 0 0 0 0 0 0 isomer 0 0 0 0 0 0 0 0 0 0 0 0 0 prevalent 0 0 0 0 0 0 0 0 0 0 0 0 0 2nd order vector for Fructose Diphosphate 0 .1 0 .3 .5 .2 0 0 0 0 .1 0 0 Figure 1: 2nd Order Vector for Fructose Diphosphate (FDP) Patwardhan and Pedersen (2006) introduce a vector measure to determine the relatedness between pairs of concepts. In this measure, a second order co-occurrence vector is created for each concept using the words in each of the concepts definition and calculating the cosine between the two vectors. This method has been used in the task of WSD by calculating the relatedness between each possible sense of the ambiguous word and its surrounding context. The context whose sum is the most similar is assigned to the ambiguous word. Second-order co-occurrence vectors were first introduced by Sch¨utze (1992) for the task of word sen"
W11-0317,W04-2406,1,0.930802,"the relatedness between pairs of concepts. In this measure, a second order co-occurrence vector is created for each concept using the words in each of the concepts definition and calculating the cosine between the two vectors. This method has been used in the task of WSD by calculating the relatedness between each possible sense of the ambiguous word and its surrounding context. The context whose sum is the most similar is assigned to the ambiguous word. Second-order co-occurrence vectors were first introduced by Sch¨utze (1992) for the task of word sense discrimination and later extended by Purandare and Pedersen (2004). As noted by Pedersen (2010), disambiguation requires a sense-inventory in which the long-forms are known ahead of time, where as in discrimination this information is not known a priori. 6 Method In our method, a second-order co-occurrence vector is created for each possible long-form of the 148 acronym, and the acronym itself. The appropriate long-form of the acronym is then determined by computing a cosine between the vector representing the ambiguous acronym and each of the vectors representing the long-forms. The long-form whose vector has the smallest angle between it and the acronym ve"
W11-0317,W09-1309,0,0.682925,"a rate of 11,000 per year. Supervised and semi-supervised methods have been used successfully for acronym disambiguation 145 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 145–153, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics but are limited in scope due to the need for sufficient training data. Liu et al. (2004) state that an acronym could have approximately 16 possible long-forms in Medline but could not obtain a sufficient number of instances for each of the acronym-long-form pairs for their experiments. Stevenson et al. (2009) cite a similar problem indicating that acronym disambiguation methods that do not require training data, regardless if it is created manually or automatically, are needed. In this paper, we introduce a novel knowledgebased method to disambiguate acronyms using second-order co-occurrence vectors. This method does not rely on training data, and therefore, is not limited to disambiguating only commonly occurring possible long-forms. These vectors are created using the first-order features obtained from the UMLS about the acronym’s long-forms and second-order features obtained from Medline. We sh"
W11-0821,J90-1003,0,0.589175,"Missing"
W11-0821,C00-1027,0,0.067938,"Missing"
W11-0821,J93-1003,0,0.236722,"Missing"
W11-0821,W04-2406,1,0.859272,"Missing"
W11-0821,J93-1007,0,0.789973,"Missing"
W11-1306,W11-1304,0,0.0434702,"Missing"
W11-1306,J90-1003,0,0.450994,"y the organizers for evaluation. 33 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 33–37, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics 2.1 Collocation Discovery The Ngram Statistics Package (Text::NSP) (Banerjee and Pedersen, 2003) was used to measure the association between the training pairs based on frequency count data collected from the corpus. All thirteen measures in the Ngram Statistics Package were employed, including the Log-likelihood Ratio (ll) (Dunning, 1993), Pointwise Mutual Information (pmi) (Church and Hanks, 1990), Mutual Information (tmi) (Church and Hanks, 1990), PoissonStirling (ps) (Church, 2000), Fisher’s Exact Test (leftFisher, rightFisher, and twotailed) (Pedersen et al., 1996), Jaccard Coefficient (jaccard), Dice Coefficient (dice), Phi Coefficient (phi), t-score (tscore) (Church and Hanks, 1990), Pearson’s Chi-Squared Test (x2), and the Odds Ratio (odds). These measure the co-occurrence of word pairs (bigrams) relative to their individual frequencies and assess how likely it is that the word pair is occurring together by chance (and is therefore likely compositional) or has some significant pa"
W11-1306,C00-1027,0,0.022888,"mpositionality (DiSCo’2011), pages 33–37, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics 2.1 Collocation Discovery The Ngram Statistics Package (Text::NSP) (Banerjee and Pedersen, 2003) was used to measure the association between the training pairs based on frequency count data collected from the corpus. All thirteen measures in the Ngram Statistics Package were employed, including the Log-likelihood Ratio (ll) (Dunning, 1993), Pointwise Mutual Information (pmi) (Church and Hanks, 1990), Mutual Information (tmi) (Church and Hanks, 1990), PoissonStirling (ps) (Church, 2000), Fisher’s Exact Test (leftFisher, rightFisher, and twotailed) (Pedersen et al., 1996), Jaccard Coefficient (jaccard), Dice Coefficient (dice), Phi Coefficient (phi), t-score (tscore) (Church and Hanks, 1990), Pearson’s Chi-Squared Test (x2), and the Odds Ratio (odds). These measure the co-occurrence of word pairs (bigrams) relative to their individual frequencies and assess how likely it is that the word pair is occurring together by chance (and is therefore likely compositional) or has some significant pattern of occurrence as a pair (in which case it is non-compositional). More formally, ma"
W11-1306,J93-1003,0,0.0941565,"ter a separate set of 174 test pairs were provided by the organizers for evaluation. 33 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 33–37, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics 2.1 Collocation Discovery The Ngram Statistics Package (Text::NSP) (Banerjee and Pedersen, 2003) was used to measure the association between the training pairs based on frequency count data collected from the corpus. All thirteen measures in the Ngram Statistics Package were employed, including the Log-likelihood Ratio (ll) (Dunning, 1993), Pointwise Mutual Information (pmi) (Church and Hanks, 1990), Mutual Information (tmi) (Church and Hanks, 1990), PoissonStirling (ps) (Church, 2000), Fisher’s Exact Test (leftFisher, rightFisher, and twotailed) (Pedersen et al., 1996), Jaccard Coefficient (jaccard), Dice Coefficient (dice), Phi Coefficient (phi), t-score (tscore) (Church and Hanks, 1990), Pearson’s Chi-Squared Test (x2), and the Odds Ratio (odds). These measure the co-occurrence of word pairs (bigrams) relative to their individual frequencies and assess how likely it is that the word pair is occurring together by chance (and"
W15-1206,W15-1204,0,0.0826437,"itter Users for Depression and PTSD with Lexical Decision Lists Ted Pedersen Department of Computer Science University of Minnesota Duluth, MN, 55812, USA tpederse@d.umn.edu 2 Abstract This paper describes various systems from the University of Minnesota, Duluth that participated in the CLPsych 2015 shared task. These systems learned decision lists based on lexical features found in training data. These systems typically had average precision in the range of .70 – .76, whereas a random baseline attained .47 – .49. 1 Introduction The Duluth systems that participated in the CLPsych Shared Task (Coppersmith et al., 2015) explore the degree to which a simple Machine Learning method can successfully identify Twitter users who suffer from Depression or Post Traumatic Stress Disorder (PTSD). Our approach was to build decision lists of Ngrams found in training Tweets that had been authored by users who had disclosed a diagnosis of Depression or PTSD. The resulting lists were applied to the Tweets of other Twitter users who served as a held–out test sample. The test users were then ranked based on the likelihood that they suffered from Depression or PTSD. This ranking depends on the number of Ngrams found in their"
W15-2602,P14-5010,1,0.0161718,"Missing"
W16-0322,W15-1202,0,0.0269119,"ost of the errors of our system were due to new vocabulary not present in the training set. We tried to account for this with the use of a smoothing parameter in the classifier but more work is needed in this respect. One way could be to train a word embedding using the unlabeled data in such a way that semantic similarities of words not present in the training samples can be modeled in the test set. 6 Related work In the previous versions of the workshop some systems have been proposed to solve similar challenging problems using some or similar features to the ones we used in our system. In (Mitchell et al., 2015) a system was developed for quantifying the language of schizophrenia in social media based on the LIWC lexicon. This study also showed that character ngrams over specific tweets in the user’s history can be used to separate schizophrenia sufferers from a control group. In (Pedersen, 2015) a system based on decision lists was developed to identify Twitter users who suffer from Depression or Post Traumatic Stress Disorder (PTSD). The features in this system are based on n-grams of up to 6 words. In this system, the usage of larger n-grams performed better 174 than bigrams. In our experiments, w"
W16-0322,W15-1206,1,0.82427,"ay that semantic similarities of words not present in the training samples can be modeled in the test set. 6 Related work In the previous versions of the workshop some systems have been proposed to solve similar challenging problems using some or similar features to the ones we used in our system. In (Mitchell et al., 2015) a system was developed for quantifying the language of schizophrenia in social media based on the LIWC lexicon. This study also showed that character ngrams over specific tweets in the user’s history can be used to separate schizophrenia sufferers from a control group. In (Pedersen, 2015) a system based on decision lists was developed to identify Twitter users who suffer from Depression or Post Traumatic Stress Disorder (PTSD). The features in this system are based on n-grams of up to 6 words. In this system, the usage of larger n-grams performed better 174 than bigrams. In our experiments, we only tried with n-grams up to length 3 and found that the best performing system in the cross-validation of the training data was obtained using bigrams. 7 Conclusion In this paper, we have briefly described our submission to the CLPsych 2016 shared task. We found that the best result wa"
W16-6203,W16-0307,0,0.0772871,"how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental ailments. Users in these forums can either initiate a thread, or reply to a thread initiated by others. We focused on the forums Depression, Relationship Health, and Brain/Nervous System Disorders. Wh"
W16-6203,P14-5010,1,0.020172,"Missing"
W16-6203,W15-2602,1,0.830778,"While these contributions obviously do not represent a solution to depression, we believe they form a significant first step towards understanding how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental ailments. Users in these forums can either initiate a"
W16-6203,W14-4108,0,0.0313908,"tely predict which users will withdraw from a forum. While these contributions obviously do not represent a solution to depression, we believe they form a significant first step towards understanding how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental"
W16-6203,D13-1170,0,0.00917784,"Missing"
W17-2313,C02-1144,0,0.0999654,"Missing"
W17-2313,W16-2922,0,0.12475,"eural network) that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip–gram approach. These approaches have been used in numerous recent papers. Muneeb et al. (2015) trained both the Skip– gram and CBOW models over the PubMed Central Open Access (PMC) corpus of approximately 1.25 million articles. They evaluated the models on a subset of the UMNSRS data, removing word pairs that did not occur in their training corpus more than ten times. Chiu et al. (2016) evaluated both the the Skip–gram and CBOW models over the PMC corpus and PubMed. They also evaluated the models on a subset of the UMNSRS ignoring those words that did not appear in their training corpus. Pakhomov et al. (2016) trained CBOW model over three different types of corpora: clinical (clinical notes from the Fairview Health System), biomedical (PMC corpus), and general English (Wikipedia). They evaluated their method using a subset of the UMNSRS restricting to single word term pairs and removing those not found within their training corpus. Sajadi et al. (2015) trained the Skip–gram"
W17-2313,N16-1162,0,0.0230988,"d a sufficiently large level of similarity. We found that eliminating less similar pairs improved the overall results (to a point). In the future, we plan to explore metrics to automatically determine the threshold cutoff appropriate for a given dataset and measure. We also plan to explore additional features that can be integrated with a second–order vector measure that will reduce the noise but still provide sufficient information to quantify relatedness. We are particularly interested in approaches that learn word, phrase, and sentence embeddings from structured corpora such as literature (Hill et al., 2016a) and dictionary entries (Hill et al., 2016b). Such embeddings could be integrated into a second–order vector or be used on their own. Finally, we compared our proposed method to other distributional approaches, focusing on those that used word embeddings. Our results showed that integrating semantic similarity measures into second–order co–occurrence vectors obtains the same or higher correlation with human judgments as do various different word embedding approaches. However, a direct comparison was not possible due to variations in the subsets of the UMNSRS evaluation dataset used. In the f"
W17-2313,Q16-1002,0,0.0229083,"d a sufficiently large level of similarity. We found that eliminating less similar pairs improved the overall results (to a point). In the future, we plan to explore metrics to automatically determine the threshold cutoff appropriate for a given dataset and measure. We also plan to explore additional features that can be integrated with a second–order vector measure that will reduce the noise but still provide sufficient information to quantify relatedness. We are particularly interested in approaches that learn word, phrase, and sentence embeddings from structured corpora such as literature (Hill et al., 2016a) and dictionary entries (Hill et al., 2016b). Such embeddings could be integrated into a second–order vector or be used on their own. Finally, we compared our proposed method to other distributional approaches, focusing on those that used word embeddings. Our results showed that integrating semantic similarity measures into second–order co–occurrence vectors obtains the same or higher correlation with human judgments as do various different word embedding approaches. However, a direct comparison was not possible due to variations in the subsets of the UMNSRS evaluation dataset used. In the f"
W17-2313,W15-3820,0,0.0301975,"ave become a popular method for measuring semantic relatedness in the biomedical domain. This is a neural network based approach that learns a representation of a word by word co–occurrence matrix. The basic idea is that the neural network learns a series of weights (the hidden layer within the neural network) that either maximizes the probability of a word given its context, referred to as the continuous bag of words (CBOW) approach, or that maximizes the probability of the context given a word, referred to as the Skip–gram approach. These approaches have been used in numerous recent papers. Muneeb et al. (2015) trained both the Skip– gram and CBOW models over the PubMed Central Open Access (PMC) corpus of approximately 1.25 million articles. They evaluated the models on a subset of the UMNSRS data, removing word pairs that did not occur in their training corpus more than ten times. Chiu et al. (2016) evaluated both the the Skip–gram and CBOW models over the PMC corpus and PubMed. They also evaluated the models on a subset of the UMNSRS ignoring those words that did not appear in their training corpus. Pakhomov et al. (2016) trained CBOW model over three different types of corpora: clinical (clinical"
W17-2313,islam-inkpen-2006-second,0,0.0458321,"o–occurrence matrix, much like many of the more traditional approaches. However, despite these successes distributional methods do not perform well when data is very sparse (which is common). One possible solution is to use second–order co–occurrence vectors (Sch¨utze, 1992; Sch¨utze, 1998). In this approach the similarity between two words is not strictly based on their co–occurrence frequencies, but rather on the frequencies of the other words which occur with both of them (i.e., second order co–occurrences). This approach has been shown to be successful in quantifying semantic relatedness (Islam and Inkpen, 2006; Pedersen et al., 2007). However, while more robust in the face of sparsity, second–order methods can result in significant amounts of noise, where contextual information that is overly general is included and does not contribute to quantifying the semantic relatedness between the two concepts. Our goal then is to discover methods that automatically reduce the amount of noise in a second– order co–occurrence vector. We achieve this by incorporating pairwise semantic similarity scores Vector space methods that measure semantic similarity and relatedness often rely on distributional information"
W17-2313,O97-1002,0,0.52287,"h(lcs(c1 , c2 ), x) (4) pks = − log P 2.1.2 lin = Feature–based Measures Feature–based methods represent each concept as a set of features and then measure the overlap or sharing of features to measure similarity. In particular, each concept is represented as the set of their ancestors, and similarity is a ratio of the intersection and union of these features. Maedche and Staab (2001) quantify the similarity between two concepts as the ratio of the intersection over their union as shown in Equation 5. T |A(c1 ) A(c2 )| S cmatch = (5) |A(c1 ) A(c2 )| 2 ∗ IC(lcs(c1 , c2 )) IC(c1 ) + IC(c2 ) (8) Jiang and Conrath (1997) define the distance between two concepts to be the sum of the information content of the two concepts minus twice the information content of the concepts’ LCS. We modify this from a distance to a similarity measure by taking the reciprocal of the distance (Equation 9). Note that the denominator of jcn is very similar to the numerator of batet. 1 IC(c1 ) + IC(c2 ) − 2 ∗ IC(lcs(c1 , c2 )) (9) Batet et al. (2011) extend this by excluding any Pirr´o and Euzenat (2010) define the similarshared features (in the numerator) as shown in ity between two concepts as the information conEquation 6. tent o"
W17-2313,J98-1004,0,0.663044,"Missing"
W17-2313,W06-2501,1,0.873002,"the terms associated with the CUI to provide a rough estimate of its frequency. The information content measures then use this information to calculate the probability of a concept. Another alternative is the use of Intrinsic Information Content. It assess the informativeness of concept based on its placement within a taxonomy by considering the number of incoming (ancestors) relative to outgoing (descendant) links (S´anchez et al., 2011) (Equation 12). IC(c) = −log( |leaves(c)| |subsumers(c)| +1 max leaves + 1 ) related concepts to the gloss of the concept itself (and then finding overlaps). Patwardhan and Pedersen (2006) adapted this measure to second–order co–occurrence vectors. In this approach, a vector is created for each word in a concept’s definition that shows which words co–occur with it in a corpus. These word vectors are averaged to create a single co-occurrence vector for the concept. The similarity between the concepts is calculated by taking the cosine between the concepts second–order vectors. Liu et al. (2012) modified and extended this measure to be used to quantify the relatedness between biomedical and clinical terms in the UMLS. The work in this paper can be seen as a further extension of P"
W17-2313,C04-1146,0,0.0606096,"ment of Computer Science Virginia Commonwealth University Richmond, VA 23284 USA btmcinnes@vcu.edu Abstract of biomedical terminologies and ontologies (Bodenreider and Burgun, 2004). There is a long history in using distributional methods to discover semantic similarity and relatedness (e.g., (Lin and Pantel, 2002; Reisinger and Mooney, 2010; Radinsky et al., 2011; Yih and Qazvinian, 2012)). These methods are all based on the distributional hypothesis, which holds that two terms that are distributionally similar (i.e., used in the same context) will also be semantically similar (Harris, 1954; Weeds et al., 2004). Recently word embedding techniques such as word2vec (Mikolov et al., 2013) have become very popular. Despite the prominent role that neural networks play in many of these approaches, at their core they remain distributional techniques that typically start with a word by word co–occurrence matrix, much like many of the more traditional approaches. However, despite these successes distributional methods do not perform well when data is very sparse (which is common). One possible solution is to use second–order co–occurrence vectors (Sch¨utze, 1992; Sch¨utze, 1998). In this approach the similar"
W17-2313,C02-1090,0,0.0767036,"der/narrower (RB/RN). 2.1 1 spath(c1 , c2 ) wup = 2 ∗ depth(lcs(c1 , c2 )) depth(c1 ) + depth(c2 ) (2) Zhong et al. (2002) take a very similar approach and again scale the depth of the LCS by the sum of the depths of the two concepts (Equation 3), where m(c) = k −depth(c) . The value of k was set to 2 based on their recommendations. Similarity Measures zhong = Measures of semantic similarity can be classified into three broad categories : path–based, feature– based and information content (IC). Path–based similarity measures use the structure of a taxon2 ∗ m(lcs(c1 , c2 )) m(c1 ) + m(c2 ) (3) Pekar and Staab (2002) offer another variation on path, where the shortest path of the two concepts to the LCS is used, in addition to the shortest 108 individual concept’s information content (Equation 8). Note that lin has the same form as wup and zhong, and is in effect using information content as a measure of specificity (rather than depth). If there is more than one possible LCS, the LCS with the greatest IC is chosen. bath between the LCS and the root of the taxonomy (Equation 4). spath(lcs(c1 , c2 ), root) x=c1 ,c2 ,root spath(lcs(c1 , c2 ), x) (4) pks = − log P 2.1.2 lin = Feature–based Measures Feature–ba"
W17-2313,N12-1077,0,0.0225872,"ving Correlation with Human Judgments by Integrating Semantic Similarity with Second–Order Vectors Ted Pedersen Department of Computer Science University of Minnesota Duluth, MN 55812 USA tpederse@d.umn.edu Bridget T. McInnes Department of Computer Science Virginia Commonwealth University Richmond, VA 23284 USA btmcinnes@vcu.edu Abstract of biomedical terminologies and ontologies (Bodenreider and Burgun, 2004). There is a long history in using distributional methods to discover semantic similarity and relatedness (e.g., (Lin and Pantel, 2002; Reisinger and Mooney, 2010; Radinsky et al., 2011; Yih and Qazvinian, 2012)). These methods are all based on the distributional hypothesis, which holds that two terms that are distributionally similar (i.e., used in the same context) will also be semantically similar (Harris, 1954; Weeds et al., 2004). Recently word embedding techniques such as word2vec (Mikolov et al., 2013) have become very popular. Despite the prominent role that neural networks play in many of these approaches, at their core they remain distributional techniques that typically start with a word by word co–occurrence matrix, much like many of the more traditional approaches. However, despite these"
W17-2313,W16-6106,0,0.015105,"using a subset of the UMNSRS restricting to single word term pairs and removing those not found within their training corpus. Sajadi et al. (2015) trained the Skip–gram model over CUIs identified by MetaMap on the OHSUMED corpus, a collection of 348,566 biomedical research articles. They evaluated the method on the complete UMNSRS, MiniMayoSRS and the MayoSRS datasets; any subset information about the dataset was not explicitly stated therefore we believe a direct comparison may be possible. In addition, a previous work very closely related to ours is a retrofitting vector method proposed by Yu et al. (2016) that incorporates ontological information into a vector representation by includTable 3: Threshold Correlation with vector-f aith # UMNSRS MiniMayoSRS T bigrams sim rel MD coder 0 838,353 0.59 0.42 0.58 0.63 0.1 197,189 0.58 0.41 0.57 0.63 0.2 121,839 0.58 0.41 0.58 0.63 0.3 71,353 0.63 0.46 0.54 0.55 0.4 45,335 0.64 0.48 0.50 0.51 0.5 29,734 0.66 0.49 0.49 0.53 0.6 19,347 0.65 0.49 0.52 0.56 0.7 11,946 0.64 0.48 0.53 0.55 0.8 7,349 0.64 0.49 0.53 0.56 0.9 4,731 0.62 0.49 0.53 0.57 measures. The increase in the correlation for the UMNSRS tagged for similarity is statistically significant (p ≤"
W17-2313,N10-1013,0,0.0737253,"Missing"
W96-0210,C92-2099,0,0.0693075,"Missing"
W96-0210,J93-3003,0,0.0402987,"Missing"
W96-0210,J93-2004,0,0.0406796,"Missing"
W96-0210,H94-1048,0,0.074479,"Missing"
W97-0322,P94-1020,1,0.829631,": help - Feature Set C Figure 6: concern - Feature Set A 7 Actual attention share money Discovered attention share money 53 6 302 58 187 255 108 4 1140 219 197 1697 Discovered attention share money 280 3 78 240 197 63 559 0 693 1079 200 834 361 500 1252 2113 361 500 1252 2113 7.1 Ward - 1170 correct Actual attention share money Discovered attention share money 127 230 4 134 364 2 320 124 808 581 718 814 Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create. It seems more reasonable to assume that such text will not usually be available and a t t e m p t to pursue unsupervised approaches that rely only on the features in a text that can be automatically identified. McQuitty - 1380 correct Actual attention share money Related Bootstrapping Bootstrapping approaches requir"
W97-0322,W96-0210,1,0.807078,"Missing"
W97-0322,P91-1019,0,0.00999615,"CE to define neighborhoods of related words. Conceptually, the neighborhood of a word is a type of equivalence class. It is composed of all other words that co-occur with the designated word a significant number of times in the LDOCE sense definitions. These neighborhoods are used to increase the number of words in the LDOCE sense definitions, while still maintaining some measure of lexical cohesion. The &quot;expanded&quot; sense definitions are then compared to the context of an ambiguous word, and the sensedefinition with the greatest number of word overlaps with the context is selected as correct. (Guthrie et al., 1991) propose that neighborhoods be subject dependent. They suggest that a word should potentially have different neighborhoods corresponding to the different LDOCE subject code. Subjectspecific neighborhoods are composed of words having at least one sense marked with that subject code. 7.3 EM algorithm The only other application of the EM algorithm to word-sense disambiguation is described in (Gale, Church, and Yarowsky, 1995). There the EM algorithm is used as part of a supervised learning algorithm to distinguish city names from people&apos;s names. A narrow window of context, one or two words to eit"
W97-0322,H93-1051,0,0.132589,"In order to use the EM algorithm, the parametric form of the model representing the data must be known. In these experiments, we assume that the model form is the Naive Bayes (Duda and Hart, 1973). In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. This assumption is based on the sucwhere count i is the current estimate of the expected count and P(Sm [Ym) is formulated using 0. cess of the Naive Bayes model when applied to supervised word-sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), (Leacock, Towell, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. Unfortunately there is little to be done in this case other than reducing the dimensionality of the problem so that fewer parameters are estimated. Second, if the likelihood function is very irregular it may always converge to a local maxima and not find the global maximum. In this case, an alternative is to use the more computatio"
W97-0322,J93-2004,0,0.030933,"Missing"
W97-0322,W96-0208,0,0.0946593,"arametric form of the model representing the data must be known. In these experiments, we assume that the model form is the Naive Bayes (Duda and Hart, 1973). In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. This assumption is based on the sucwhere count i is the current estimate of the expected count and P(Sm [Ym) is formulated using 0. cess of the Naive Bayes model when applied to supervised word-sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), (Leacock, Towell, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. Unfortunately there is little to be done in this case other than reducing the dimensionality of the problem so that fewer parameters are estimated. Second, if the likelihood function is very irregular it may always converge to a local maxima and not find the global maximum. In this case, an alternative is to use the more computationally expensive m"
W97-0322,P96-1006,0,0.0542741,"- Feature Set A 7 Actual attention share money Discovered attention share money 53 6 302 58 187 255 108 4 1140 219 197 1697 Discovered attention share money 280 3 78 240 197 63 559 0 693 1079 200 834 361 500 1252 2113 361 500 1252 2113 7.1 Ward - 1170 correct Actual attention share money Discovered attention share money 127 230 4 134 364 2 320 124 808 581 718 814 Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create. It seems more reasonable to assume that such text will not usually be available and a t t e m p t to pursue unsupervised approaches that rely only on the features in a text that can be automatically identified. McQuitty - 1380 correct Actual attention share money Related Bootstrapping Bootstrapping approaches require a small amount of disambiguated te"
W97-0322,A97-1056,1,0.80441,"f the model representing the data must be known. In these experiments, we assume that the model form is the Naive Bayes (Duda and Hart, 1973). In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. This assumption is based on the sucwhere count i is the current estimate of the expected count and P(Sm [Ym) is formulated using 0. cess of the Naive Bayes model when applied to supervised word-sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), (Leacock, Towell, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. Unfortunately there is little to be done in this case other than reducing the dimensionality of the problem so that fewer parameters are estimated. Second, if the likelihood function is very irregular it may always converge to a local maxima and not find the global maximum. In this case, an alternative is to use the more computationally expensive method of Gibbs Sampling (Geman and"
W97-0322,P93-1024,0,0.0514077,"bt that the &quot;one sense per collocation&quot; rule (Yarowsky, 1993) would still hold for a larger number of senses. In future work we will evaluate using the &quot;one sense per collocation&quot; rule to seed our various methods. This may help in dealing with very skewed distributions of senses since we currently select collocations based simply on frequency. 7.2 Clustering Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., (Rosenfeld, Huang, and Schneider, 1969), (Kiss, 1973), (Ritter and Kohonen, 1989), (Pereira, Tishby, and Lee, 1993), (Sch/itze, 1993), (Resnik, 1995a)). An early application of clustering to word-sense disambiguation is described in (Sch/itze, 1992). There words are represented in terms of the cooccurrence statistics of four letter sequences. This representation uses 97 features to characterize a word, where each feature is a linear combination of letter four-grams formulated by a singular value decomposition of a 5000 by 5000 matrix of letter fourgram co-occurrence frequencies. The weight associated with each feature reflects all usages of the word in the sample. A context vector is formed for each occur"
W97-0322,W95-0105,0,0.0745943,"ky, 1993) would still hold for a larger number of senses. In future work we will evaluate using the &quot;one sense per collocation&quot; rule to seed our various methods. This may help in dealing with very skewed distributions of senses since we currently select collocations based simply on frequency. 7.2 Clustering Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., (Rosenfeld, Huang, and Schneider, 1969), (Kiss, 1973), (Ritter and Kohonen, 1989), (Pereira, Tishby, and Lee, 1993), (Sch/itze, 1993), (Resnik, 1995a)). An early application of clustering to word-sense disambiguation is described in (Sch/itze, 1992). There words are represented in terms of the cooccurrence statistics of four letter sequences. This representation uses 97 features to characterize a word, where each feature is a linear combination of letter four-grams formulated by a singular value decomposition of a 5000 by 5000 matrix of letter fourgram co-occurrence frequencies. The weight associated with each feature reflects all usages of the word in the sample. A context vector is formed for each occurrence of an ambiguous word by summ"
W97-0322,C92-2070,0,0.228525,"value is missing. In order to use the EM algorithm, the parametric form of the model representing the data must be known. In these experiments, we assume that the model form is the Naive Bayes (Duda and Hart, 1973). In this model, all features are conditionally independent given the value of the classification feature, i.e., the sense of the ambiguous word. This assumption is based on the sucwhere count i is the current estimate of the expected count and P(Sm [Ym) is formulated using 0. cess of the Naive Bayes model when applied to supervised word-sense disambiguation (e.g. (Gale, Church, and Yarowsky, 1992), (Leacock, Towell, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). There are two potential problems when using the EM algorithm. First, it is computationally expensive and convergence can be slow for problems with large numbers of model parameters. Unfortunately there is little to be done in this case other than reducing the dimensionality of the problem so that fewer parameters are estimated. Second, if the likelihood function is very irregular it may always converge to a local maxima and not find the global maximum. In this case, an alt"
W97-0322,H93-1052,0,0.0432213,"279 988 1267 EM - 763 correct EM - 1040 correct Figure 8: help - Feature Set C Figure 6: concern - Feature Set A 7 Actual attention share money Discovered attention share money 53 6 302 58 187 255 108 4 1140 219 197 1697 Discovered attention share money 280 3 78 240 197 63 559 0 693 1079 200 834 361 500 1252 2113 361 500 1252 2113 7.1 Ward - 1170 correct Actual attention share money Discovered attention share money 127 230 4 134 364 2 320 124 808 581 718 814 Work Word-sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., (Black, 1988), (Yarowsky, 1992), (Yarowsky, 1993), (Leacock, Towell, and Voorhees, 1993), (Bruce and Wiebe, 1994), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create. It seems more reasonable to assume that such text will not usually be available and a t t e m p t to pursue unsupervised approaches that rely only on the features in a text that can be automatically identified. McQuitty - 1380 correct Actual attention s"
W97-0322,P95-1026,0,0.722602,"that can be automatically identified. McQuitty - 1380 correct Actual attention share money Related Bootstrapping Bootstrapping approaches require a small amount of disambiguated text in order to initialize the unsupervised learning algorithm. An early example of such an approach is described in (Hearst, 1991). A supervised learning algorithm is trained with a small amount of manually sense tagged text and applied to a held out test set. Those examples in the test set that are most confidently disambiguated are added to the training sample. A more recent bootstrapping approach is described in (Yarowsky, 1995). This algorithm requires a small number of training examples to serve as a seed. There are a variety of options discussed for 361 500 1252 2113 EM - 1299 correct Figure 7: interest - Feature Set B 204 automatically selecting seeds; one is to identify collocations that uniquely distinguish between senses. For plant, the collocations manufacturing plant and living plant make such a distinction. Based on 106 examples of manufacturing plant and 82 examples of living plant this algorithm is able to distinguish between two senses of plant for 7,350 examples with 97 percent accuracy. Experiments wit"
W97-1005,C94-2195,0,0.0357484,"d by the cardinality of values (a.k.a. levels) of these variables (8); for the IBM data, there are 2.13 × 1013 cells in the table (9). Each cell in the table corresponds to a unique combination of the variable values and all combinations are represented in the table. I described the problem that was on the paper(2) On the paper, I described the problem. (3) In this paper, we address only the type of PPA problem illustrated above and don't consider other less frequent PPA problems. For the linguistic details of the problem, the reader can refer to (Hirst, 1987). We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al., 1994) to objectively compare the performances of the systems. Both data were extracted from the Penn Treebank Wall Street Journal (WSJ) Corpus (Marcus et al., 1993). In order to distinguish these data from each other, we call the former one BSzR data and the latter one IBM data. Both PPA data were formatted in tuples with five variables (4), which denote the class (i.e., the PPA attachment site) and the features (i.e., verb, object noun, preposition and PP noun) in the respective order• Values of these variables for the above example (1) are illustrated in (5), where"
W97-1005,P94-1020,1,0.921921,"Missing"
W97-1005,W95-0103,0,0.251226,"h 1 2 3 noun verb Considering that there are 27,937 PPA observations in the training and test data together, a search space of more than 21 trillion possible distinct cases (represented in the cells of contingency table) indicates that the data is extremely sparse. To solve PPA problem, NLP researchers designed domain specific classifier systems. Those systems can be categorized in two classes: ] shots bar 1. Rule based systems (Boggess et al., 1991), (Brill and Resnik, 1994) pot (Aticorp 2. Statistical and information theoretic approaches (Hindle and Rooth, 1993), (Ratnaparkhi et al., 1994),(Collins and Brooks, 1995), (Franz, 1996) : 5162 5163 option rebate 66&quot;25 Table 1: Substitution of variable values for associated integer labels at the Levels column. The number of levels of five variables are 2, 3845, 5162, 81 and 6625. Using lexical collocations to determine PPA with statistical techniques was first proposed by (Hindle and Rooth, 1993). They suggested a score called Lexical Association to predict PPA. It is a log likelihood ratio of probability estimates of two PPA sites. The probability of attachment was based on the frequencies of the 2-tuples (B, D), and (C, D), where B, C, D stand for the variabl"
W97-1005,J96-1002,0,0.016546,"Missing"
W97-1005,J93-1005,0,0.0144457,"h success plus end they 3845 3846 dev£1ops chunks Koch 1 2 3 noun verb Considering that there are 27,937 PPA observations in the training and test data together, a search space of more than 21 trillion possible distinct cases (represented in the cells of contingency table) indicates that the data is extremely sparse. To solve PPA problem, NLP researchers designed domain specific classifier systems. Those systems can be categorized in two classes: ] shots bar 1. Rule based systems (Boggess et al., 1991), (Brill and Resnik, 1994) pot (Aticorp 2. Statistical and information theoretic approaches (Hindle and Rooth, 1993), (Ratnaparkhi et al., 1994),(Collins and Brooks, 1995), (Franz, 1996) : 5162 5163 option rebate 66&quot;25 Table 1: Substitution of variable values for associated integer labels at the Levels column. The number of levels of five variables are 2, 3845, 5162, 81 and 6625. Using lexical collocations to determine PPA with statistical techniques was first proposed by (Hindle and Rooth, 1993). They suggested a score called Lexical Association to predict PPA. It is a log likelihood ratio of probability estimates of two PPA sites. The probability of attachment was based on the frequencies of the 2-tuples"
W97-1005,J93-2004,0,0.0362218,"of the variable values and all combinations are represented in the table. I described the problem that was on the paper(2) On the paper, I described the problem. (3) In this paper, we address only the type of PPA problem illustrated above and don't consider other less frequent PPA problems. For the linguistic details of the problem, the reader can refer to (Hirst, 1987). We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al., 1994) to objectively compare the performances of the systems. Both data were extracted from the Penn Treebank Wall Street Journal (WSJ) Corpus (Marcus et al., 1993). In order to distinguish these data from each other, we call the former one BSzR data and the latter one IBM data. Both PPA data were formatted in tuples with five variables (4), which denote the class (i.e., the PPA attachment site) and the features (i.e., verb, object noun, preposition and PP noun) in the respective order• Values of these variables for the above example (1) are illustrated in (5), where (A, B, C, D, E) C B A E D I I 2 2 i 2 1 1 1 2 2 1 2 L,1010 0 0 0 0 0]01010 ... • .. 2 1 1 ... .-. ... 5162 3845 1 • '' 0 ... 0 • .• 0 ..• 0[0 .-- 0 ... 1 2 0[0 0 ] 0 I Table 2: The PPA data"
W97-1005,H94-1048,0,0.0677975,"s (a.k.a. levels) of these variables (8); for the IBM data, there are 2.13 × 1013 cells in the table (9). Each cell in the table corresponds to a unique combination of the variable values and all combinations are represented in the table. I described the problem that was on the paper(2) On the paper, I described the problem. (3) In this paper, we address only the type of PPA problem illustrated above and don't consider other less frequent PPA problems. For the linguistic details of the problem, the reader can refer to (Hirst, 1987). We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al., 1994) to objectively compare the performances of the systems. Both data were extracted from the Penn Treebank Wall Street Journal (WSJ) Corpus (Marcus et al., 1993). In order to distinguish these data from each other, we call the former one BSzR data and the latter one IBM data. Both PPA data were formatted in tuples with five variables (4), which denote the class (i.e., the PPA attachment site) and the features (i.e., verb, object noun, preposition and PP noun) in the respective order• Values of these variables for the above example (1) are illustrated in (5), where (A, B, C, D, E) C B A E D I I 2"
