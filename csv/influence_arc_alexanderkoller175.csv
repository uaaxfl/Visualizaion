2020.acl-main.463,2014.lilt-9.5,0,0.0324575,"unication generally, relies on joint attention and intersubjectivity: the ability to be aware of what another human is attending to and guess what they are intending to communicate. Human children do not learn meaning from form alone and we should not expect machines to do so either. 7 Distributional semantics Distributional semanticists have long been aware that grounding distributional representations in the real world is challenging. The lexical similarity relations learned by distributional models trained on text don’t in themselves connect any of those words to the world (Herbelot, 2013; Baroni et al., 2014; Erk, 2016; Emerson, 2020), and the distributions of words may not match the distribution of things in the world (consider four-legged dogs). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos (Hossain et al., 2019) or other modalities (Kiela and Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been Engl"
2020.acl-main.463,2020.emnlp-main.703,0,0.112501,"nd Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5190 emotional stress (McDuff and Kapoor, 2019) or eye gaze (Koller et al., 2012), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2020). We agree that this is an exciting avenue of research. From this literature we can see that the slogan “meaning is use” (often attributed to Wittgenstein, 1953), refers not to “use” as “distribution in a text corpus” but rather that language is used in the real world to convey communicative intents to real people. Speakers distill their past experience of language use into what we call “meaning” here, and produce new attempts at using language based on this; this attempt is successful if the listener correctly deduces the speaker’s communicative intent. Thus, standing meanings evolve over tim"
2020.acl-main.463,D15-1075,0,0.0145669,"guage and ask top-down questions. Neural methods are not the first bottom-up success in NLP; they will probably not be the last. Second, be aware of the limitations of tasks: Artificial tasks like bAbI (Weston et al., 2016) can help get a field of research off the ground, but there is no reason to assume that the distribution of language in the test data remotely resembles the distribution of real natural language; thus evaluation results on such tasks must be interpreted very carefully. Similar points can be made about crowdsourced NLI datasets such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015), which do not represent questions that any particular person really wanted to ask about a text, but the somewhat unnatural communicative situation of crowdsourcing work. If a system does better on such a task than the inter-annotator agreement,9 the task probably has statistical artifacts that do not represent meaning. In the vision community, Barbu et al. (2019) offer a novel dataset which explicitly tries to achieve a more realistic distribution of task data; it would be interesting to explore similar ideas for language. Third, value and support the work of carefully creating new tasks (see"
2020.acl-main.463,P16-1141,0,0.0352501,"“distribution in a text corpus” but rather that language is used in the real world to convey communicative intents to real people. Speakers distill their past experience of language use into what we call “meaning” here, and produce new attempts at using language based on this; this attempt is successful if the listener correctly deduces the speaker’s communicative intent. Thus, standing meanings evolve over time as speakers can different experiences (e.g. McConnellGinet, 1984), and a reflection of such change can be observed in their changing textual distribution (e.g. Herbelot et al., 2012; Hamilton et al., 2016). 8 On climbing the right hills What about systems which are trained on a task that is not language modeling — say, semantic parsing, or reading comprehension tests — and that use word embeddings from BERT or some other large LM as one component? Numerous papers over the past couple of years have shown that using such pretrained embeddings can boost the accuracy of the downstream system drastically, even for tasks that are clearly related to meaning. Our arguments do not apply to such scenarios: reading comprehension datasets include information which goes beyond just form, in that they specif"
2020.acl-main.463,P19-1356,0,0.0151443,"’) that aims to answer this question. The methodology of probing tasks (e.g. Adi et al., 2017; Ettinger et al., 2018) has been used to show that 1 https://www.nytimes.com/2018/11/18/technology/artific ial-intelligence-language.html, accessed 2019/12/04 2 https://www.business2community.com/seo/what-t o-do-about-bert-googles-recent-local-algorithm-updat e-02259261, accessed 2019/12/04 3 https://www.blog.google/products/search/search-langu age-understanding-bert/, accessed 2019/12/04 large LMs learn at least some information about phenomena such as English subject-verb agreement (Goldberg, 2019; Jawahar et al., 2019), constituent types, dependency labels, NER, and (core) semantic role types (again, all in English) (Tenney et al., 2019).4 Hewitt and Manning (2019) find information analogous to unlabeled dependency structures in the word vectors provided by ELMo and BERT (trained on English). And of course it is well established that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al., 2015) and semantic (lexical similarity, e.g. Rubenstein and Goodenough, 1965; Mikolov et al., 2013). Others have looked more closely at the success of the large LMs on apparently m"
2020.acl-main.463,P15-2038,0,0.0208776,"are that grounding distributional representations in the real world is challenging. The lexical similarity relations learned by distributional models trained on text don’t in themselves connect any of those words to the world (Herbelot, 2013; Baroni et al., 2014; Erk, 2016; Emerson, 2020), and the distributions of words may not match the distribution of things in the world (consider four-legged dogs). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos (Hossain et al., 2019) or other modalities (Kiela and Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5190 emotional stress (McDuff and Kapoor, 2019) or eye gaze (Koller et al., 2012), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2020). We agree that t"
2020.acl-main.463,D15-1293,0,0.0166871,"cists have long been aware that grounding distributional representations in the real world is challenging. The lexical similarity relations learned by distributional models trained on text don’t in themselves connect any of those words to the world (Herbelot, 2013; Baroni et al., 2014; Erk, 2016; Emerson, 2020), and the distributions of words may not match the distribution of things in the world (consider four-legged dogs). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos (Hossain et al., 2019) or other modalities (Kiela and Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5190 emotional stress (McDuff and Kapoor, 2019) or eye gaze (Koller et al., 2012), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2"
2020.acl-main.463,W12-1604,1,0.615316,"t match the distribution of things in the world (consider four-legged dogs). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos (Hossain et al., 2019) or other modalities (Kiela and Clark, 2015; Kiela et al., 2015). Another is to look to interaction data, e.g. a dialogue corpus with success annotations, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5190 emotional stress (McDuff and Kapoor, 2019) or eye gaze (Koller et al., 2012), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2020). We agree that this is an exciting avenue of research. From this literature we can see that the slogan “meaning is use” (often attributed to Wittgenstein, 1953), refers not to “use” as “distribution in a text corpus” but rather that language is used in the real world to convey communicative intents to real people. Speakers distill their past e"
2020.acl-main.463,J82-2005,0,0.626132,"Missing"
2020.acl-main.463,N15-1144,0,0.0152241,"earch-langu age-understanding-bert/, accessed 2019/12/04 large LMs learn at least some information about phenomena such as English subject-verb agreement (Goldberg, 2019; Jawahar et al., 2019), constituent types, dependency labels, NER, and (core) semantic role types (again, all in English) (Tenney et al., 2019).4 Hewitt and Manning (2019) find information analogous to unlabeled dependency structures in the word vectors provided by ELMo and BERT (trained on English). And of course it is well established that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al., 2015) and semantic (lexical similarity, e.g. Rubenstein and Goodenough, 1965; Mikolov et al., 2013). Others have looked more closely at the success of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “reasoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding"
2020.acl-main.463,P19-1334,0,0.162369,"s of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “reasoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribution of lexical cues for each label. Similarly, McCoy et al. (2019) find that BERT’s performance on the English Multi-genre Natural Language Inference dataset (Williams et al., 2018) is predicated on its ability to leverage syntactic heuristics involving overlap (of full constituents, subsequences, or simply bags of words). In a dataset carefully designed to frustrate such heuristics, BERT’s performance falls to significantly below chance. In this brief overview of BERTology papers we have highlighted both the extent to which there is evidence that large LMs can learn aspects of linguistic formal structure (e.g. agreement, dependency structure), and how their"
2020.acl-main.463,N13-1090,0,0.175839,"ormation about phenomena such as English subject-verb agreement (Goldberg, 2019; Jawahar et al., 2019), constituent types, dependency labels, NER, and (core) semantic role types (again, all in English) (Tenney et al., 2019).4 Hewitt and Manning (2019) find information analogous to unlabeled dependency structures in the word vectors provided by ELMo and BERT (trained on English). And of course it is well established that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al., 2015) and semantic (lexical similarity, e.g. Rubenstein and Goodenough, 1965; Mikolov et al., 2013). Others have looked more closely at the success of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “reasoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribut"
2020.acl-main.463,P19-1459,0,0.243112,"vectors provided by ELMo and BERT (trained on English). And of course it is well established that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al., 2015) and semantic (lexical similarity, e.g. Rubenstein and Goodenough, 1965; Mikolov et al., 2013). Others have looked more closely at the success of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “reasoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribution of lexical cues for each label. Similarly, McCoy et al. (2019) find that BERT’s performance on the English Multi-genre Natural Language Inference dataset (Williams et al., 2018) is predicated on its ability to leverage syntactic heuristics involving overlap (of full constituents, subsequences, or simply bags of words)."
2020.acl-main.463,S15-2153,0,0.0570915,"Missing"
2020.acl-main.463,W19-4102,0,0.0129123,"ess possible counterarguments to our main thesis. 2 Large LMs: Hype and analysis Publications talking about the application of large LMs to meaning-sensitive tasks tend to describe the models with terminology that, if interpreted at face value, is misleading. Here is a selection from academically-oriented pieces (emphasis added): (1) (2) (3) In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task. (Devlin et al., 2019) Using BERT, a pretraining language model, has been successful for single-turn machine comprehension . . . (Ohsugi et al., 2019) The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demon5185 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185–5198 c July 5 - 10, 2020. 2020 Association for Computational Linguistics strates their potential as unsupervised open-domain QA systems. (Petroni et al., 2019) If the highlighted terms are meant to describe human-analogous understanding, comprehension, or recall of factual knowledge, then these are gross overclaims. If, instead, they are intended as technical terms, they should be expli"
2020.acl-main.463,S19-1012,0,0.0133783,"l., 2018; Hewitt and Manning, 2019). Equating these with meaning ignores a core function of language, which is to convey communicative intents. “But meaning could be learned from . . . ”. As we discussed in §7, if form is augmented with grounding data of some kind, then meaning can conceivably be learned to the extent that the communicative intent is represented in that data. In addition, certain tasks are designed in a way that specific forms are declared as representing certain semantic relations of interest. Examples of this include NLI datasets (Dagan et al., 2006; Rajpurkar et al., 2016; Ostermann et al., 2019) which pair input/output tuples of linguistic forms with an explicit semantic relation (e.g. text + hypothesis + “entailed”). Similarly, control codes, or tokens like tl;dr, have been used to prompt large LMs to perform summarization and other tasks (Radford et al., 2019; Keskar et al., 2019). Here forms are explicitly declared at test time to represent certain semantic relations, which together with the distributional similarity between e.g. tl;dr and other phrases such as in summary, may be enough to bootstrap a successful neural summarizer. Depending on one’s perspective, one may argue that"
2020.acl-main.463,D19-1250,0,0.0464819,"Missing"
2020.acl-main.463,D16-1264,0,0.183368,"l, cultivate humility towards language and ask top-down questions. Neural methods are not the first bottom-up success in NLP; they will probably not be the last. Second, be aware of the limitations of tasks: Artificial tasks like bAbI (Weston et al., 2016) can help get a field of research off the ground, but there is no reason to assume that the distribution of language in the test data remotely resembles the distribution of real natural language; thus evaluation results on such tasks must be interpreted very carefully. Similar points can be made about crowdsourced NLI datasets such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015), which do not represent questions that any particular person really wanted to ask about a text, but the somewhat unnatural communicative situation of crowdsourcing work. If a system does better on such a task than the inter-annotator agreement,9 the task probably has statistical artifacts that do not represent meaning. In the vision community, Barbu et al. (2019) offer a novel dataset which explicitly tries to achieve a more realistic distribution of task data; it would be interesting to explore similar ideas for language. Third, value and support the work of car"
2020.acl-main.463,D19-1286,0,0.0859084,"Missing"
2020.acl-main.463,N18-1101,0,0.0155805,"ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches. Niven and Kao (2019) find that BERT’s unreasonably good performance on the English Argument Reasoning Comprehension Task (Habernal et al., 2018) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribution of lexical cues for each label. Similarly, McCoy et al. (2019) find that BERT’s performance on the English Multi-genre Natural Language Inference dataset (Williams et al., 2018) is predicated on its ability to leverage syntactic heuristics involving overlap (of full constituents, subsequences, or simply bags of words). In a dataset carefully designed to frustrate such heuristics, BERT’s performance falls to significantly below chance. In this brief overview of BERTology papers we have highlighted both the extent to which there is evidence that large LMs can learn aspects of linguistic formal structure (e.g. agreement, dependency structure), and how their apparent ability to “reason” is sometimes a mirage built on leveraging artifacts in the training data (i.e. form,"
2020.coling-main.212,W19-3405,0,0.0106278,"erent manner. Human evaluation show that our model substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my groceries . i put my groceries in my cart and left . Figure 1: A story about grocery shopping generat"
2020.coling-main.212,N18-1204,0,0.0226861,"s the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my groceries . i put my groceries in my cart and left . Figure 1: A story about grocery shopping generated by Zhai et al. (2019), which is globally coherent but really bo"
2020.coling-main.212,P18-1082,0,0.0297658,"establish global coherence; and (2) a detailer, which supplies relevant details to the story in a locally coherent manner. Human evaluation show that our model substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register"
2020.coling-main.212,P19-1254,0,0.0215458,"herence; and (2) a detailer, which supplies relevant details to the story in a locally coherent manner. Human evaluation show that our model substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my gro"
2020.coling-main.212,D16-1032,0,0.0229484,"substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my groceries . i put my groceries in my cart and left . Figure 1: A story about grocery shopping generated by Zhai et al. (2019), which is globally c"
2020.coling-main.212,N16-1014,0,0.0275405,"ch incorporates little information. This is the main technical challenge of the paper, which we tackle with the following efforts. (1) To select the content of the current segment, we condition the decoding process on its most important context: the previous regular event e− and the successive regular event e+ . Thus at each decoding step t, it produces: qt = Dec(ϕw (tokt−1 ); att(dt−1 , fi ); ϕe (e− ); ϕe (e+ )) (3) Here, Dec(·) is once again a single layer Bi-LSTM. (2) The detailer also adopts the maximum mutual information (MMI) objective. First used in conjunction with SEQ 2 SEQ models by Li et al. (2016), the technique promotes the generation of specific, meaningful texts by moderately suppressing generic language generations. The idea is, instead of maximizing data likelihood, one could maximize the mutual information I(c, s) between the context c and the generation s to promote the correspondence between the two, thus improving the informativeness of the text. The MMI decoding objective generalizes to: s = arg max[log(p(s|c)) − λ · log(p(s))] (4) s which is the maximum likelihood objective minus a language model term, so it is also termed an anti-LM objective. In practice, equivalently, we"
2020.coling-main.212,D15-1166,0,0.0458126,"Missing"
2020.coling-main.212,L16-1555,0,0.320966,"Missing"
2020.coling-main.212,D14-1162,0,0.0880954,"gment by segment, alternating between the decoders to address one event at a time. Thus technically, the neural part of the model receives (1) the input sequence, i.e. segments s = hs1 . . . si−1 i that are already generated and (2) the agenda a = he1 . . . en i that consists of n events in total as input; as the output, it generates the next text segment si which corresponds to the current event ei . Encoder We encode the input sequence in an agenda-aware manner: fi = Enc(ϕw (s<i ); ϕe (e<i )) (1) Here ϕw (·) denotes the word embeddings, which we initialize with pre-trained Glove embeddings (Pennington et al. (2014)); ϕe (·) is the event embeddings, which we initialize randomly. The embedding of each token in the history s<i is concatenated (;) with the embedding ϕe (ei ) of its corresponding event ei so the encoding process is aware of the agenda. Enc(·) is a single layer Bi-LSTM sequence encoder. The outcome fi is a list of vectors, each of which collects the features of its corresponding input token. Outliner The outliner generates the text segment corresponding to the next event ei in the agenda, if it is regular. Here we use a Bi-LSTM sequence decoder which has access to a dot-product attention (Luo"
2020.coling-main.212,N16-3012,1,0.863989,"Missing"
2020.coling-main.212,W19-3404,1,0.663415,"vant details to the story in a locally coherent manner. Human evaluation show that our model substantially improves the informativeness of generated text while retaining its coherence, outperforming a number of baselines. 1 Introduction Story generation is the task of automatically crafting stories. Recent neural story generation systems have been able to produce coherent stories. Global coherence could be established by conditioning language models on the longer-term intention of the text. One could provide a topic (see, e.g. Fan et al. (2018), Fan et al. (2019)), a list of events (see, e.g. Zhai et al. (2019), Martin et al. (2018), Ammanabrolu et al. (2019)) or entities (see, e.g. Kiddon et al. (2016), Clark et al. (2018)) etc., to guide the generation process. But apart from being globally coherent, a story also needs to be interesting to engage its readers. yesterday i went grocery shopping . i made a list of my list and drove to the grocery store . when i entered the store , i grabbed a shopping cart and pushed the cart down to the meat aisle . i got all my items , and crossed items on my list . i went to the checkout register and paid for my groceries . i put my groceries in my cart and left ."
2020.coling-main.252,P10-1008,0,0.236317,"neatly. A crowdsourcing evaluation shows that the choice of abstraction level matters to users, and that an abstraction strategy which balances low-level and high-level object descriptions compares favorably to ones which don’t. 1 Introduction Technical instructions in complex environments can often be stated at different levels of abstraction. For instance, a natural language generation (NLG) system for tech support might instruct a human instruction follower (IF) to either plug “the broadband cable into the broadband filter” or “the thin white cable with grey ends into the small white box” (Janarthanam and Lemon, 2010). Depending on how much the IF knows about the domain, the first, high-level instruction may be difficult to understand, or the second, detailed instruction may be imprecise and annoyingly verbose. An effective instruction generation system will thus adapt the level of abstraction to the user. In this paper, we investigate the generation of instructions at different levels of abstraction in the context of the computer game “Minecraft”. Minecraft offers a virtual 3D environment in which the player can mine materials, craft items, and construct complex objects, such as buildings and machines. Th"
2020.coling-main.252,W19-8601,1,0.623548,"Missing"
2020.coling-main.252,2020.sigdial-1.7,1,0.742555,"Missing"
2020.coling-main.252,P19-1537,0,0.129226,"al block. However, it can be more effective to generate more high-level instructions. In the bridge-building example shown in Fig. 1, it is probably better to simply say “build a railing on the other side” instead of explaining where to place the seven individual blocks – provided the IF knows what a railing looks like. Minecraft is the best-selling video game of all time (200 million users), which means that there is a large pool of potential experimental subjects for evaluating NLG systems. Minecraft has been used previously as a platform for experimentation in AI and in particular for NLG (Narayan-Chen et al., 2019; K¨ohn and Koller, 2019). We present an instruction giving (IG) system which guides the user in constructing complex objects in Minecraft, such as houses and bridges. The system consists of two parts: a hierarchical planning system based on Hierarchical Task Networks (HTN) (Ghallab et al., 2004; Bercher et al., 2019), which computes a structured instruction plan for how to to explain the construction; and a chart-based generation system which inputs individual plan steps and generates the actual instruction sentences (K¨ohn and Koller, 2019). Planning systems generate plans based on expressiv"
2020.coling-main.267,P13-1023,0,0.0239539,"ically-grounded rules. Our work significantly increases the match in compositional structure between MRs and improves multi-task learning (MTL) in a low-resource setting, serving as a proof of concept for future broad-scale cross-MR normalization. 1 Introduction Graph-based representations of sentence meaning offer an expressive and flexible means of modeling natural language semantics. In recent years, a number of different graphbanks have annotated large corpora with graph-based semantic representations of various types (Oepen and Lønning, 2006; Ivanova et al., 2012; Banarescu et al., 2013; Abend and Rappoport, 2013). Because of differences in graphbank design principles, individual graphs can differ greatly and often in fundamental strategies (Oepen et al., 2019). Fig. 1 illustrates distinct graphs from the three graphbanks of the SemEval 2015 Shared Task on Semantic Dependency Parsing, which we take as our focus in this paper. The graphs visibly differ with respect to edge labels, edge directions, the treatment of a periphrastic verb construction, and coordination. In this paper, we develop a methodology to (i) understand the nature of the differences across graphbanks and (ii) normalize annotations fro"
2020.coling-main.267,W13-2322,0,0.0163729,"phenomena using linguistically-grounded rules. Our work significantly increases the match in compositional structure between MRs and improves multi-task learning (MTL) in a low-resource setting, serving as a proof of concept for future broad-scale cross-MR normalization. 1 Introduction Graph-based representations of sentence meaning offer an expressive and flexible means of modeling natural language semantics. In recent years, a number of different graphbanks have annotated large corpora with graph-based semantic representations of various types (Oepen and Lønning, 2006; Ivanova et al., 2012; Banarescu et al., 2013; Abend and Rappoport, 2013). Because of differences in graphbank design principles, individual graphs can differ greatly and often in fundamental strategies (Oepen et al., 2019). Fig. 1 illustrates distinct graphs from the three graphbanks of the SemEval 2015 Shared Task on Semantic Dependency Parsing, which we take as our focus in this paper. The graphs visibly differ with respect to edge labels, edge directions, the treatment of a periphrastic verb construction, and coordination. In this paper, we develop a methodology to (i) understand the nature of the differences across graphbanks and (i"
2020.coling-main.267,K19-2006,1,0.833187,"ormalized (e.g. both dependency trees analyze the noun as the head and the adjective as the modifier), while others are still present (e.g. ignored tokens). 2992 2.2 Compositional Semantic Parsing with the AM Algebra The Apply-Modify (AM) Algebra was initially presented in Groschwitz et al. (2018) for compositional AMR parsing. It provides the latent compositional structures we aim to normalize, in the form of AM dependency trees (explained below). Lindemann et al. (2019) (hereafter referred to as L19) extended the AM algebra to parsing four additional graph-based MRs (DM, PAS, PSD, and EDS); Donatelli et al. (2019) extended this work to UCCA. L19 achieved state of the art results for all formalisms in question with pretrained BERT embeddings and manual heuristics tailored to each graphbank. We adapt these heuristics to find uniform structures that generalize across graphbanks. To illustrate the algebra, consider the example AM dependency is the tree in Fig. 3e, which constructs the PAS graph in Fig. 3b. The verb_ARG1 verb_ARG2 det_ARG1 edges correspond to AM algebra operations, and each word adj_ARG1 is assigned a graph constant, specifically an as-graph, written S lazy cat below the word (as a visual h"
2020.coling-main.267,P18-1170,1,0.934239,"nces across graphbanks and (ii) normalize annotations from different graphbanks at the level of their compositional structures. This approach allows us to identify which design differences between graphbanks are linguistically meaningful and important for the design of future corpora; as well as to identify more unified approaches to certain structures to potentially facilitate multi-task learning (MTL). In Section 2 we build upon Lindemann et al. (2019), who used AM dependency trees to represent the compositional structure of graph-based meaning representations (MRs) based on the AM algebra (Groschwitz et al., 2018). We detail a new methodology for identifying and quantifying mismatches between the compositional structures assigned to the same sentence by different graphbanks; we then present our extended AM+ algebra, with which we can systematically reshape these compositional structures to align them across graphbanks (Section 3). Finally, we provide key examples of how our methodology normalizes specific linguistic phenomena that pose challenges to MR design and differ in representation across graphbanks (Section 4). Using our methods, we increase the match between the AM dependency trees (composition"
2020.coling-main.267,W12-3602,0,0.0209165,"majority of divergent phenomena using linguistically-grounded rules. Our work significantly increases the match in compositional structure between MRs and improves multi-task learning (MTL) in a low-resource setting, serving as a proof of concept for future broad-scale cross-MR normalization. 1 Introduction Graph-based representations of sentence meaning offer an expressive and flexible means of modeling natural language semantics. In recent years, a number of different graphbanks have annotated large corpora with graph-based semantic representations of various types (Oepen and Lønning, 2006; Ivanova et al., 2012; Banarescu et al., 2013; Abend and Rappoport, 2013). Because of differences in graphbank design principles, individual graphs can differ greatly and often in fundamental strategies (Oepen et al., 2019). Fig. 1 illustrates distinct graphs from the three graphbanks of the SemEval 2015 Shared Task on Semantic Dependency Parsing, which we take as our focus in this paper. The graphs visibly differ with respect to edge labels, edge directions, the treatment of a periphrastic verb construction, and coordination. In this paper, we develop a methodology to (i) understand the nature of the differences"
2020.coling-main.267,P19-1450,1,0.934823,"aarland.de Abstract The emergence of a variety of graph-based meaning representations (MRs) has sparked an important conversation about how to adequately represent semantic structure. MRs exhibit structural differences that reflect different theoretical and design considerations, presenting challenges to uniform linguistic analysis and cross-framework semantic parsing. Here, we ask the question of which design differences between MRs are meaningful and semantically-rooted, and which are superficial. We present a methodology for normalizing discrepancies between MRs at the compositional level (Lindemann et al., 2019), finding that we can normalize the majority of divergent phenomena using linguistically-grounded rules. Our work significantly increases the match in compositional structure between MRs and improves multi-task learning (MTL) in a low-resource setting, serving as a proof of concept for future broad-scale cross-MR normalization. 1 Introduction Graph-based representations of sentence meaning offer an expressive and flexible means of modeling natural language semantics. In recent years, a number of different graphbanks have annotated large corpora with graph-based semantic representations of vari"
2020.coling-main.267,J93-2004,0,0.0742382,"positional structure across graphbanks is successful and can be applied at broad-scale and potentially to more complex graphbanks (Section 6). 2 2.1 Background The Graphbanks We focus on the three graphbanks of the SemEval 2015 Shared Task on Semantic Dependency Parsing (SDP): (i) DELPH-IN MRS-Derived Semantic Dependencies (DM), (ii) Enju Predicate–Argument Structures (PAS), and (iii) Prague Semantic Dependencies (PSD) (Oepen et al., 2015). All graphbanks have parallel semantic annotations over the same English texts: the Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al. (1993)). The graphbanks are bilexical, node-ordered, labeled, directed graphs, representing core semantic information like predicate-argument relations. The SDP graphbanks make different choices about which linguistic information to represent and how to represent it (first columns of Table 2) (Oepen et al., 2016). Representative graphs of the three graphbanks containing linguistic phenomena of interest (modification, coordination, copula, prepositional phrase) are shown in Fig. 1. While in all three graphs each token of the sentence corresponds to at most one node, there may also be tokens which are"
2020.coling-main.267,oepen-lonning-2006-discriminant,0,0.0718417,"hat we can normalize the majority of divergent phenomena using linguistically-grounded rules. Our work significantly increases the match in compositional structure between MRs and improves multi-task learning (MTL) in a low-resource setting, serving as a proof of concept for future broad-scale cross-MR normalization. 1 Introduction Graph-based representations of sentence meaning offer an expressive and flexible means of modeling natural language semantics. In recent years, a number of different graphbanks have annotated large corpora with graph-based semantic representations of various types (Oepen and Lønning, 2006; Ivanova et al., 2012; Banarescu et al., 2013; Abend and Rappoport, 2013). Because of differences in graphbank design principles, individual graphs can differ greatly and often in fundamental strategies (Oepen et al., 2019). Fig. 1 illustrates distinct graphs from the three graphbanks of the SemEval 2015 Shared Task on Semantic Dependency Parsing, which we take as our focus in this paper. The graphs visibly differ with respect to edge labels, edge directions, the treatment of a periphrastic verb construction, and coordination. In this paper, we develop a methodology to (i) understand the natu"
2020.coling-main.267,S15-2153,0,0.0646328,"Missing"
2020.coling-main.267,L16-1630,0,0.0294177,"Missing"
2020.coling-main.267,K19-2001,0,0.0305537,"Missing"
2020.coling-main.267,P18-1018,0,0.0168897,"ordination before being filled with the argument, resulting in an additional A PP child for the coordination (see Fig. 8(d, f)). In such cases, PAS and PSD L19 AM trees exhibit the OTHER pattern. If the conjuncts share a common argument like the subject “mice” in Fig. 8a, we add respective source annotations to the OP1- and OP2-sources (Fig. 8c). Fig. 8e shows the partial result after evaluating the A PP OP1 and A PP OP2 operations in this example. Prepositions and Their Effects. Though typically understood as a single morphosyntactic class in English, prepositions are notoriously polysemous (Schneider et al., 2018). This is reflected in their varied pattern signature: [CONN / CONN / EMPTY], [EMPTY / CONN / EMPTY], [BASIC / BASIC / MOD]. We find that most prepositions have the CONN pattern or are ignored in the graph (compare for example the preposition “in” in Fig. 1 in DM and PAS versus PSD). DM and PAS use strategy (ii) (see Section 4.3) for most prepositions and encode them as nodes, whereas PSD uses strategy (i) and encodes them as edges. See the preposition “in” in Fig. 1, to which the PSD ‘LOC’ edge corresponds. In the AM trees, these prepositions have the [CONN / CONN / EMPTY] signature. Note tha"
2020.coling-main.267,S14-2008,0,\N,Missing
2020.crac-1.4,W13-2322,0,0.0775248,"chael Roth Institute for NLP University of Stuttgart {tatianak|koller}@coli.uni-saarland.de michael.roth@ims.uni-stuttgart.de Abstract This work addresses coreference resolution in Abstract Meaning Representation (AMR) graphs, a popular formalism for semantic parsing. We evaluate several current coreference resolution techniques on a recently published AMR coreference corpus, establishing baselines for future work. We also demonstrate that coreference resolution can improve the accuracy of a state-ofthe-art semantic parser on this corpus. 1 Introduction Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) are a popular type of symbolic semantic representation for semantic parsing. AMRs are labeled directed graphs whose nodes represent entities, events, properties, and states; the edges represent semantic relations between the nodes. For instance, in the example AMRs of Fig. 2, the predicate node c describes a come-back relation between the ARG1 “I” and the ARG3 “this”. AMR is designed to abstract over the way in which a certain piece of meaning was expressed in language; thus “the destruction of the room by the boy” and “the boy destroyed the room” are represented by the same graph. In the ex"
2020.crac-1.4,P13-2131,0,0.0349698,"tokens. We collapsed the coreferent nodes by replacing all edges into a node for a coreferent token by edges into the first node of the coreference chain; see O’Gorman et al. (2018) for details. For example, in Fig. 2 (a) there are three coreferent nodes i:i, i2:i and i3:i. Since all three nodes represent the same entity the corresponding edges can be rearranged to point to the same node i:i as shown in Fig. 2 (b). We evaluated the performance of Lindemann’s parser, with and without the added coreference information, on the complete MS-AMR test data. To this end, we computed the Smatch score (Cai and Knight, 2013) for the predicted vs. gold document-level graphs. Table 3 shows the micro- and macro-average Smatch precision, recall and f-score for the documents from the test set. The left column indicates the scores obtained by comparing the gold AMRs with coreference to the ones generated by the parser without coreference. The middle column shows the scores for the gold MS-AMR graphs versus the parser output augmented with coreference predictions. The overall improvement in f-score is around three points Smatch f-score. The right column shows the scores obtained by augmenting Lindemann’s parser output w"
2020.crac-1.4,P16-1061,0,0.0629468,"Missing"
2020.crac-1.4,2020.tacl-1.5,0,0.118877,"mentions are not pieces of text as in other coreference annotation schemes, but nodes in the AMR graphs. The annotation also specifies what implicit roles of predicate nodes the entity fills. In this paper, we make two contributions. First, we evaluate the performance of different coreference resolution tools on the MS-AMR annotations. We evaluate these on the token level (by projecting the coreference annotations from the nodes to the sentences) and on the node level (by projecting the tools’ coreference predictions to the nodes of the graphs) and find that AllenNLP with SpanBERT embeddings (Joshi et al., 2020) generally performs best. Second, we show for the first time how the output of a coreference system can be integrated into the predictions of a state-of-the-art AMR parser. We use the neural semantic parser of Lindemann et al. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 33 Proceedings of the 3rd Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2020), pages 33–38, Barcelona, Spain (online), December 12, 2020. a) k: know-01 :ARG0 a3: amr-unknown m: multi-sentence"
2020.crac-1.4,J13-4004,0,0.0793353,"Missing"
2020.crac-1.4,D17-1018,0,0.0951134,"Missing"
2020.crac-1.4,P19-1450,1,0.921324,"the SpanBERT version of AllenNLP achieves the best results in all metrics. 4 AMR parsing with coreference Coreference is not an isolated task in MS-AMR parsing; in order to predict the gold annotations, coreference information needs to be incorporated into AMR graphs predicted by a semantic parser. We thus 35 macro-average: micro-average: AMR parser P R F 0.57 0.52 0.54 0.57 0.50 0.53 AMR parser + AllenNLP P R F 0.61 0.54 0.57 0.60 0.53 0.56 AMR parser + oracle P R F 0.63 0.56 0.59 0.63 0.55 0.58 Table 3: Smatch evaluation of document-level coreference annotations. extended the AMR parser of Lindemann et al. (2019) with coreference information. First, we prepared gold annotations at the document level. For this, we combined the individual AMRs from each document into a single graph to represent document-level annotations. The coreference chains were extracted from the gold annotations of the MS-AMR corpus, and coreferent nodes in the document graph were merged following the procedure described in (O’Gorman et al., 2018). Second, we ran Lindemann’s parser on each sentence separately and combined the predicted AMR graphs into a document-level graph. Then we ran SpanBERT AllenNLP (henceforth just AllenNLP)"
2020.crac-1.4,C18-1313,0,0.171972,"Missing"
2020.crac-1.4,D14-1162,0,0.0832495,"Missing"
2020.emnlp-main.323,W17-6810,1,0.767659,"e others make use of derivation trees that compositionally evaluate to graphs (Groschwitz et al., 2018; Chen et al., 2018; Fancellu et al., 2019; Lindemann et al., 2019). AM dependency parsing belongs to the latter category. 3 Background We begin by sketching the AM dependency parser of Groschwitz et al. (2018). 3.1 AM dependency trees Groschwitz et al. (2018) use AM dependency trees to represent the compositional structure of a semantic graph. Each token is assigned a graph constant representing its lexical meaning; dependency labels correspond to operations of the Apply-Modify (AM) algebra (Groschwitz et al., 2017; Groschwitz, 2019), which combine graphs into bigger ones. Fig. 2 illustrates how an AM dependency tree (a) evaluates to a graph (b), based on the graph constants in Fig. 1. Each graph constant is an as-graph, which means it has special node markers called sources, drawn in red, as well as a root M S O[S] S Figure 1: Elementary as-graphs Gwant , Gwriter , Gsleep , and Gsound . marked in bold. These markers are used to combine graphs with the algebra’s operations. For instance, the MOD M operation in Fig. 2a combines the head Gsleep with its modifier Gsoundly by plugging the root of Gsleep int"
2020.emnlp-main.323,P18-1170,1,0.816913,"ell-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy. 1 Introduction Over the past few years, the accuracy of neural semantic parsers which parse English sentences into graph-based semantic representations has increased substantially (Dozat and Manning, 2018; Zhang et al., 2019; He and Choi, 2020; Cai and Lam, 2020). Most of these parsers use a neural model which can freely predict node labels and edges, and most of them are tailored to a specific type of graphbank. Among the high-accuracy semantic parsers, the AM dependency parser of Groschwitz et al. (2018) stands out in that it implements the Principle of Compositionality from theoretical semantics in a neural framework. By parsing into AM dependency trees, which represent the compositional structure of the sentence and evaluate deterministically into graphs, this parser can abstract away surface details of the individual graphbanks. It was the first semantic parser which worked well across multiple graphbanks, and set new states of the art on several of them (Lindemann et al., 2019). However, the commitment to linguistic principles comes at a cost: the AM dependency parser is slow. A key part"
2020.emnlp-main.323,Q16-1023,0,0.0217668,"ypedness with O(n2 ) parsing complexity, achieve a speed of several thousand tokens per second across all graphbanks, and even improve the parsing accuracy over previous AM dependency parsers by up to 1.6 points F-score. 2 Related work In transition-based parsing, a dependency tree is built step by step using nondeterministic transitions. A classifier is trained to choose transitions deter3929 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3929–3951, c November 16–20, 2020. 2020 Association for Computational Linguistics ministically (Nivre, 2008; Kiperwasser and Goldberg, 2016). Transition-based parsing has also been used for constituency parsing (Dyer et al., 2016) and graph parsing (Damonte et al., 2017). We build most directly upon the top-down parser of Ma et al. (2018). Unlike most other transition-based parsers, our parser implements hard symbolic constraints in order to enforce well-typedness. Such constraints can lead transition systems into dead ends, requiring the parser to backtrack (Ytrestøl, 2011) or return partial analyses (Zhang and Clark, 2011). Our transition system carefully avoids dead ends. Shi and Lee (2018) take hard valency constraints into ac"
2020.emnlp-main.323,N03-1016,0,0.219124,"-typedness. Such constraints can lead transition systems into dead ends, requiring the parser to backtrack (Ytrestøl, 2011) or return partial analyses (Zhang and Clark, 2011). Our transition system carefully avoids dead ends. Shi and Lee (2018) take hard valency constraints into account in chart-based syntactic dependency parsing, avoiding dead ends by relaxing the constraints slightly in practice. A* parsing is a method for speeding up agendabased chart parsers, which takes items off the agenda based on a heuristic estimate of completion cost. A* parsing has been used successfully for PCFGs (Klein and Manning, 2003), TAG (Bladier et al., 2019), and other grammar formalisms. Our work is based most closely on the CCG A* parser of Lewis and Steedman (2014). Most approaches that produce semantic graphs (see Koller et al. (2019) for an overview) model distributions over graphs directly (Dozat and Manning, 2018; Zhang et al., 2019; He and Choi, 2020; Cai and Lam, 2020), while others make use of derivation trees that compositionally evaluate to graphs (Groschwitz et al., 2018; Chen et al., 2018; Fancellu et al., 2019; Lindemann et al., 2019). AM dependency parsing belongs to the latter category. 3 Background We"
2020.emnlp-main.323,P19-4002,1,0.795096,"ead ends. Shi and Lee (2018) take hard valency constraints into account in chart-based syntactic dependency parsing, avoiding dead ends by relaxing the constraints slightly in practice. A* parsing is a method for speeding up agendabased chart parsers, which takes items off the agenda based on a heuristic estimate of completion cost. A* parsing has been used successfully for PCFGs (Klein and Manning, 2003), TAG (Bladier et al., 2019), and other grammar formalisms. Our work is based most closely on the CCG A* parser of Lewis and Steedman (2014). Most approaches that produce semantic graphs (see Koller et al. (2019) for an overview) model distributions over graphs directly (Dozat and Manning, 2018; Zhang et al., 2019; He and Choi, 2020; Cai and Lam, 2020), while others make use of derivation trees that compositionally evaluate to graphs (Groschwitz et al., 2018; Chen et al., 2018; Fancellu et al., 2019; Lindemann et al., 2019). AM dependency parsing belongs to the latter category. 3 Background We begin by sketching the AM dependency parser of Groschwitz et al. (2018). 3.1 AM dependency trees Groschwitz et al. (2018) use AM dependency trees to represent the compositional structure of a semantic graph. Eac"
2020.emnlp-main.323,D14-1107,0,0.169131,"al analyses (Zhang and Clark, 2011). Our transition system carefully avoids dead ends. Shi and Lee (2018) take hard valency constraints into account in chart-based syntactic dependency parsing, avoiding dead ends by relaxing the constraints slightly in practice. A* parsing is a method for speeding up agendabased chart parsers, which takes items off the agenda based on a heuristic estimate of completion cost. A* parsing has been used successfully for PCFGs (Klein and Manning, 2003), TAG (Bladier et al., 2019), and other grammar formalisms. Our work is based most closely on the CCG A* parser of Lewis and Steedman (2014). Most approaches that produce semantic graphs (see Koller et al. (2019) for an overview) model distributions over graphs directly (Dozat and Manning, 2018; Zhang et al., 2019; He and Choi, 2020; Cai and Lam, 2020), while others make use of derivation trees that compositionally evaluate to graphs (Groschwitz et al., 2018; Chen et al., 2018; Fancellu et al., 2019; Lindemann et al., 2019). AM dependency parsing belongs to the latter category. 3 Background We begin by sketching the AM dependency parser of Groschwitz et al. (2018). 3.1 AM dependency trees Groschwitz et al. (2018) use AM dependency"
2020.emnlp-main.323,P19-1450,1,0.865758,"tailored to a specific type of graphbank. Among the high-accuracy semantic parsers, the AM dependency parser of Groschwitz et al. (2018) stands out in that it implements the Principle of Compositionality from theoretical semantics in a neural framework. By parsing into AM dependency trees, which represent the compositional structure of the sentence and evaluate deterministically into graphs, this parser can abstract away surface details of the individual graphbanks. It was the first semantic parser which worked well across multiple graphbanks, and set new states of the art on several of them (Lindemann et al., 2019). However, the commitment to linguistic principles comes at a cost: the AM dependency parser is slow. A key part of the parser is that AM dependency trees must be well-typed according to a type system which ensures that the semantic valency of each word is respected. Existing algorithms compute all items along a parsing schema that encodes the type constraints; they parse e.g. the AMRBank at less than three tokens per second. In this paper, we present two fast and accurate parsing algorithms for AM dependency trees. We first present an A* parser which searches through the parsing schema of Gro"
2020.emnlp-main.323,P18-1130,0,0.0933686,"parsing schema of Groschwitz et al.’s “projective parser” efficiently (§4). We extend the supertag-factored heuristic of Lewis and Steedman’s (2014) A* parser for CCG with a heuristic for dependency edge scores. This parser achieves a speed of up to 2200 tokens/s on semantic dependency parsing (Oepen et al., 2015), at no loss in accuracy. On AMR corpora (Banarescu et al., 2013), it achieves a speedup of 10x over previous work, but still does not exceed 30 tokens/second. We therefore develop an entirely new transitionbased parser for AM dependency trees, inspired by the stack-pointer parser of Ma et al. (2018) for syntactic dependency parsing (§5). The key challenge here is to adhere to complex symbolic constraints – the AM algebra’s type system – without running into dead ends. This is hard for a greedy transition system and in other settings requires expensive workarounds, such as backtracking. We ensure that our parser avoids dead ends altogether. We define two variants of the transition-based parser, which choose types for words either before predicting the outgoing edges or after, and introduce a neural model for predicting transitions. In this way, we guarantee well-typedness with O(n2 ) pars"
2020.emnlp-main.323,J08-4003,0,0.0355977,"rantee well-typedness with O(n2 ) parsing complexity, achieve a speed of several thousand tokens per second across all graphbanks, and even improve the parsing accuracy over previous AM dependency parsers by up to 1.6 points F-score. 2 Related work In transition-based parsing, a dependency tree is built step by step using nondeterministic transitions. A classifier is trained to choose transitions deter3929 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3929–3951, c November 16–20, 2020. 2020 Association for Computational Linguistics ministically (Nivre, 2008; Kiperwasser and Goldberg, 2016). Transition-based parsing has also been used for constituency parsing (Dyer et al., 2016) and graph parsing (Damonte et al., 2017). We build most directly upon the top-down parser of Ma et al. (2018). Unlike most other transition-based parsers, our parser implements hard symbolic constraints in order to enforce well-typedness. Such constraints can lead transition systems into dead ends, requiring the parser to backtrack (Ytrestøl, 2011) or return partial analyses (Zhang and Clark, 2011). Our transition system carefully avoids dead ends. Shi and Lee (2018) take"
2020.emnlp-main.323,S15-2153,0,0.258938,"Missing"
2020.emnlp-main.323,D18-1159,0,0.0126392,"nistically (Nivre, 2008; Kiperwasser and Goldberg, 2016). Transition-based parsing has also been used for constituency parsing (Dyer et al., 2016) and graph parsing (Damonte et al., 2017). We build most directly upon the top-down parser of Ma et al. (2018). Unlike most other transition-based parsers, our parser implements hard symbolic constraints in order to enforce well-typedness. Such constraints can lead transition systems into dead ends, requiring the parser to backtrack (Ytrestøl, 2011) or return partial analyses (Zhang and Clark, 2011). Our transition system carefully avoids dead ends. Shi and Lee (2018) take hard valency constraints into account in chart-based syntactic dependency parsing, avoiding dead ends by relaxing the constraints slightly in practice. A* parsing is a method for speeding up agendabased chart parsers, which takes items off the agenda based on a heuristic estimate of completion cost. A* parsing has been used successfully for PCFGs (Klein and Manning, 2003), TAG (Bladier et al., 2019), and other grammar formalisms. Our work is based most closely on the CCG A* parser of Lewis and Steedman (2014). Most approaches that produce semantic graphs (see Koller et al. (2019) for an"
2020.emnlp-main.323,P11-3011,0,0.0329938,"atural Language Processing, pages 3929–3951, c November 16–20, 2020. 2020 Association for Computational Linguistics ministically (Nivre, 2008; Kiperwasser and Goldberg, 2016). Transition-based parsing has also been used for constituency parsing (Dyer et al., 2016) and graph parsing (Damonte et al., 2017). We build most directly upon the top-down parser of Ma et al. (2018). Unlike most other transition-based parsers, our parser implements hard symbolic constraints in order to enforce well-typedness. Such constraints can lead transition systems into dead ends, requiring the parser to backtrack (Ytrestøl, 2011) or return partial analyses (Zhang and Clark, 2011). Our transition system carefully avoids dead ends. Shi and Lee (2018) take hard valency constraints into account in chart-based syntactic dependency parsing, avoiding dead ends by relaxing the constraints slightly in practice. A* parsing is a method for speeding up agendabased chart parsers, which takes items off the agenda based on a heuristic estimate of completion cost. A* parsing has been used successfully for PCFGs (Klein and Manning, 2003), TAG (Bladier et al., 2019), and other grammar formalisms. Our work is based most closely on the C"
2020.emnlp-main.323,D19-1392,0,0.0778472,"Missing"
2020.emnlp-main.323,P11-1069,0,0.0303512,"c November 16–20, 2020. 2020 Association for Computational Linguistics ministically (Nivre, 2008; Kiperwasser and Goldberg, 2016). Transition-based parsing has also been used for constituency parsing (Dyer et al., 2016) and graph parsing (Damonte et al., 2017). We build most directly upon the top-down parser of Ma et al. (2018). Unlike most other transition-based parsers, our parser implements hard symbolic constraints in order to enforce well-typedness. Such constraints can lead transition systems into dead ends, requiring the parser to backtrack (Ytrestøl, 2011) or return partial analyses (Zhang and Clark, 2011). Our transition system carefully avoids dead ends. Shi and Lee (2018) take hard valency constraints into account in chart-based syntactic dependency parsing, avoiding dead ends by relaxing the constraints slightly in practice. A* parsing is a method for speeding up agendabased chart parsers, which takes items off the agenda based on a heuristic estimate of completion cost. A* parsing has been used successfully for PCFGs (Klein and Manning, 2003), TAG (Bladier et al., 2019), and other grammar formalisms. Our work is based most closely on the CCG A* parser of Lewis and Steedman (2014). Most app"
2020.sigdial-1.7,W09-0628,1,0.672559,"s. While interaction in the physical world is often a desirable goal, it places a huge burden on automatic agents as perception is a hard problem, raising the barrier of setting up such experiments significantly. On the other end, interactions on a custom-built platform may be a good fit to explore specific phenomena, but they do not scale easily to different or complex problems. A good example for a custom-built virtual 3D world is the GIVE challenge, where an instruction system must guide a player to press a specific sequence of buttons in a 3D environment while avoiding to step into traps (Byron et al., 2009; Striegnitz et al., 2011). We instead use a generalpurpose 3D environment. We release an experimentation platform based on Minecraft (see Figure 1). Minecraft is a game in which the players are situated in a 3D world, which mainly consists of blocks. The game can either be played locally as a single-player game or one can join an online server and play with others. The players can move around, place and remove blocks, and even craft new blocks or items. As such, Minecraft 53 Proceedings of the SIGdial 2020 Conference, pages 53–56 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Co"
2020.sigdial-1.7,W19-8601,1,0.385809,"Missing"
2020.sigdial-1.7,P19-1537,0,0.0856116,"be combined into high-level objects, and special blocks even enable building circuits, resulting in Turing-complete machinery. Minecraft contains different game modes: a survival mode, which focuses on exploration and survival in the game world, and the creative mode, focusing on building. We make use of the creative mode. This feature-richness makes Minecraft a perfect environment for the evaluation of all kinds of intelligent agents (Johnson et al., 2016), from reinforcement learning agents (Guss et al., 2019), to instruction receiving (Szlam et al., 2019) and instruction giving assistants (Narayan-Chen et al., 2019). Its popularity (Minecraft is the most sold game of all time), together with the client-server architecture make Minecraft a tool well-suited for crowd-sourcing with volunteers from all over the world. Moreover, there are tons of instruction videos for Minecraft on the internet which could be used as auxiliary datasets for offline instruction giving. This addresses several of the limitations Collaborative human-computer interaction can occur in different environments. While interaction in the physical world is often a desirable goal, it places a huge burden on automatic agents as perception i"
2020.sigdial-1.7,W11-2845,1,0.573484,"in the physical world is often a desirable goal, it places a huge burden on automatic agents as perception is a hard problem, raising the barrier of setting up such experiments significantly. On the other end, interactions on a custom-built platform may be a good fit to explore specific phenomena, but they do not scale easily to different or complex problems. A good example for a custom-built virtual 3D world is the GIVE challenge, where an instruction system must guide a player to press a specific sequence of buttons in a 3D environment while avoiding to step into traps (Byron et al., 2009; Striegnitz et al., 2011). We instead use a generalpurpose 3D environment. We release an experimentation platform based on Minecraft (see Figure 1). Minecraft is a game in which the players are situated in a 3D world, which mainly consists of blocks. The game can either be played locally as a single-player game or one can join an online server and play with others. The players can move around, place and remove blocks, and even craft new blocks or items. As such, Minecraft 53 Proceedings of the SIGdial 2020 Conference, pages 53–56 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics Ar"
2021.emnlp-main.554,P08-1090,0,0.0584137,"”); definite noun phrases of 80 on predicting edges given gold information frequently drop their determiners ((a) “eggs”; (c) about the nodes; the output graphs are less detailed “milk,” “egg whites”); many arguments are elided ¨ than ours. Ozgen (2019) achieves an F-Score of 75 or left implicit ((b) “beat ∅”; (c) “pour ∅ over”); on the same task and presents a subtask of creating 5 https://github.com/coli-saar/ara action graphs similar to ours in Section 3.4. 6931 Event alignment. Our work shares an interest in modelling procedural knowledge with the detection and alignment of script events. Chambers and Jurafsky (2008, 2009) identified event types from text according to their predicate-argument structures and behavior in event chains via countbased statistics. We capture similar information in a crowdsourcing task reminiscent of Wanzare et al. (2016, 2017) to automatically align actions without all surface text (Regneri et al., 2010). large [Sf] other-mod baking powder [F] sugar [F] t-comp t-comp targ targ targ BEAT [Ac] MIX TOGETHER [Ac] targ salt [F] f-eq targ f-eq vanilla [F] flour mixture [F] targ eggs [F] flour [F] dest butter [F] ADD [Ac] 3 eggs [F] bowl [T] bowl [T] Parsing Recipes into Graphs f-eq"
2021.emnlp-main.554,P09-1068,0,0.151941,"Missing"
2021.emnlp-main.554,2020.acl-main.440,0,0.0880717,"lained with different levels of detail: recipe (a) explains the process with three distinct cooking actions (in bold); recipe (b) with eight distinct actions; and recipe (c) with nineteen actions. 1 3 For now, we understand an action as synonymous to a semantic event. We use the former term for consistency with other recipe work. The set of recipes in Fig. 1 provides more general information about how to make waffles than each individual recipe can. To our knowledge, only one previous work focuses on alignment of instructions across recipes to facilitate recipe interpretation on a dish level: Lin et al. (2020) (henceforth referred to as L’20) present the Microsoft Research Multimodal Aligned Recipe Corpus to align English recipe text instructions to each other and to video sequences. L’20 focus on alignment of instructions as sentences. Yet, as sentences can be quite long and contain multiple actions, defining instructions at the sentence level often glosses over the relationship between individual actions and excludes the 2 http://myhappytribe.com/recipes/ waffles/ 3 http://www.tastygardener.com/waffles/ 4 http://bakesbychichi.com/2014/03/ waffles.html 6930 Proceedings of the 2021 Conference on Em"
2021.emnlp-main.554,N15-1017,0,0.0741647,"Missing"
2021.emnlp-main.554,N18-1202,0,0.0420388,"into consecutive outputs. 3.2 Tagging Recipes We split the parsing task into two steps. In the first step, we tag the tokens of a recipe with their respective node types. We implement the sequence tagger as a neural network (NN) with a two-layered BiLSTM encoder generating predictions in a CRF output layer: Figure 2: Full graph for recipe (b) (Fig. 1) in the style of Y’20. Actions are displayed as diamonds, foods as circles, tools as rectangles, and all else as ellipses. entity (r-NE)’ tagging task. Contrary to common expectation, we find that this tagger performs better with ELMo embeddings (Peters et al., 2018) than with BERT embeddings (Devlin et al., 2019) (Table 1). Trained and evaluated on Y’20’s 300-r corpus, our tagger performs two points better than Y’20’s tagger and reaches Y’20’s inter-annotator agreement. 3.3 Parsing Recipes In the second step, we predict the edges of the graph. We use the biaffine dependency parser by Dozat and Manning (2017), implemented by Gardner et al. (2018). The model consists of a biaffine ~y = CRF (BiLST M (2) (BiLST M (1) (~x))), classifier upon a three-layered BiLSTM with mulwhere ~y are the predicted tags over the input tilingual BERT embeddings. The model take"
2021.emnlp-main.554,N16-3012,0,0.0261248,"Missing"
2021.emnlp-main.554,P10-1100,1,0.741579,"”); on the same task and presents a subtask of creating 5 https://github.com/coli-saar/ara action graphs similar to ours in Section 3.4. 6931 Event alignment. Our work shares an interest in modelling procedural knowledge with the detection and alignment of script events. Chambers and Jurafsky (2008, 2009) identified event types from text according to their predicate-argument structures and behavior in event chains via countbased statistics. We capture similar information in a crowdsourcing task reminiscent of Wanzare et al. (2016, 2017) to automatically align actions without all surface text (Regneri et al., 2010). large [Sf] other-mod baking powder [F] sugar [F] t-comp t-comp targ targ targ BEAT [Ac] MIX TOGETHER [Ac] targ salt [F] f-eq targ f-eq vanilla [F] flour mixture [F] targ eggs [F] flour [F] dest butter [F] ADD [Ac] 3 eggs [F] bowl [T] bowl [T] Parsing Recipes into Graphs f-eq targ targ milk [F] The main contribution of this paper is a corpus of action alignments between action graphs of cooking recipes. Basing our corpus on unannotated recipe texts from L’20, we are dependent on an accurate tagger and parser for pre-processing. The tagger identifies the alignable actions in a recipe, and the"
2021.emnlp-main.554,2020.acl-main.462,0,0.014374,"hs and of recipe parsers. Compared to previous work, our corpus and model represent alignments at the level of individual actions and not of entire sentences. In refining the sentence-level alignments of L’20 to the cooking action level, we find that judging when and how specific actions “correspond” is a complex task that requires intricate knowledge of the cooking domain. Alternatively, the complexity of recipe interpretation can be framed as a matter of recognizing nuances in how meaning is construed in recipe text given the genre and its preferred syntactic constructions (Langacker, 1993; Trott et al., 2020). Looking ahead, our work lays a foundation for research which automatically aggregates multiple recipe graphs for the same dish, identifying common and distinct parts of the different recipes. This opens up a variety of applications in the cooking domain, including dialogue systems which can explain a recipe at different levels of abstraction. Acknowledgments Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Associat"
2021.emnlp-main.554,W17-0901,0,0.0289324,"Missing"
2021.emnlp-main.554,L16-1556,0,0.0146954,"gen (2019) achieves an F-Score of 75 or left implicit ((b) “beat ∅”; (c) “pour ∅ over”); on the same task and presents a subtask of creating 5 https://github.com/coli-saar/ara action graphs similar to ours in Section 3.4. 6931 Event alignment. Our work shares an interest in modelling procedural knowledge with the detection and alignment of script events. Chambers and Jurafsky (2008, 2009) identified event types from text according to their predicate-argument structures and behavior in event chains via countbased statistics. We capture similar information in a crowdsourcing task reminiscent of Wanzare et al. (2016, 2017) to automatically align actions without all surface text (Regneri et al., 2010). large [Sf] other-mod baking powder [F] sugar [F] t-comp t-comp targ targ targ BEAT [Ac] MIX TOGETHER [Ac] targ salt [F] f-eq targ f-eq vanilla [F] flour mixture [F] targ eggs [F] flour [F] dest butter [F] ADD [Ac] 3 eggs [F] bowl [T] bowl [T] Parsing Recipes into Graphs f-eq targ targ milk [F] The main contribution of this paper is a corpus of action alignments between action graphs of cooking recipes. Basing our corpus on unannotated recipe texts from L’20, we are dependent on an accurate tagger and parser"
2021.emnlp-main.554,2020.lrec-1.638,0,0.244878,"before ((b) “the liquid”; (c) “the flour mixture”). Accurate interpretation of single recipe instructions requires familiarity with situated food ingredients, knowledge of verb semantics to identify how each cooking step relates to the others, and general commonsense about the cooking environment and instructional syntax. Recipe graphs and corpora. Representing recipes as graphs is the dominant choice (Mori et al., 2014; Jermsurawong and Habash, 2015; Kiddon et al., 2015; Yamakata et al., 2016; Chen, 2017; ¨ Chang et al., 2018; Ozgen, 2019). Of relevance to this paper is the recipe corpus of Yamakata et al. (2020) (Y’20), which consists of 300 English recipes annotated with graphs as in Fig. 2. We train a recipe parser on this corpus (Section 3) and use the trained parser to identify actions in the L’20 corpus. As noted earlier, Lin et al. (2020) (L’20) created the Microsoft Research Multimodal Aligned Recipe Corpus of roughly 50k text recipes and 77k recipe videos across 4000 dishes in English. The recipes and video transcriptions were segmented into sentences (but not parsed), and 200 recipe pairs were manually annotated for correspondences between recipe sentences. L’20 cluster dishes based on exact"
2021.spnlp-1.3,W13-2322,0,0.225406,"encodings using latent tree structures. Their tasks are different: they learn to predict continousspace embeddings; we learn to predict symbolic compositional structures. Similar observations hold for self-attention (Vaswani et al., 2017; Kitaev and Klein, 2018). 3 elf S G-elf G-glow charm ARG0 ARG1 O G-charm Figure 1: AM dep-trees and graphs for the fairy that begins to glow. We usually write our example AM dep-trees without alignments as in (b). We include node names where helpful, as in (c), where e.g. b is labeled begin. ARG0 charm S glow ARG0 ARG1 O S G-charmP Figure 2: Graph constants (Banarescu et al., 2013). The AM dep-tree edges are labeled with graph-combining operations, taken from the Apply-Modify (AM) algebra (Groschwitz et al., 2017; Groschwitz, 2019). Graphs are built out of fragments called graph constants (Fig. 2). Each graph constant has a root, marked with a rectangular outline, and may have special node markers called sources (Courcelle and Engelfriet, 2012), drawn in red, which mark the empty slots where other graphs will be inserted. In Fig. 1a, the A PP O operation plugs the root of G-glow into the O source of G-begin. Because G-begin and G-glow both have an S-source, A PP O merge"
2021.spnlp-1.3,2020.acl-main.629,0,0.0625627,"Missing"
2021.spnlp-1.3,P18-1038,0,0.0182485,"ry:[ ] M OD S ARG0 M OD S G-begin A PP O G-begin:[S, O[S]] A PP O G-glow The fairy that begins to glow (a) AM dep-tree with word alignments. The dashed lines connect tokens to their graph constants, and arrows point from heads to arguments, labeled by the operation that puts the graphs together. f:fairy ARG0 fairy ARG1 g:glow (c) AMR G-glow:[S] begin (b) AM dep-tree without ARG0 ARG1 alignments. Nodes are ARG0 labeled with graph conS glow stants, paired with their types for ease of presenta- (d) Partial result: tion. begins to glow begin G-fairy ARG0 ARG1 S O[S] G-begin Peng et al. (2015) and Chen et al. (2018), use CCG and HRG based grammars to parse AMR and EDS (Flickinger et al., 2017). They use a combination of heuristics, hand-annotated compositional structures and sampling to obtain training data for their parsers, in contrast to our joint neural technique. None of these approaches use slot names that carry meaning; to the best of our knowledge this work is the first to learn them from data. Fancellu et al. (2019) use DAG grammars for compositional parsing of Discourse Representation Structures (DRS). Their algorithm for extracting the compositional structure of a graph is deterministic and gr"
2021.spnlp-1.3,W17-6810,1,0.925559,"ct symbolic compositional structures. Similar observations hold for self-attention (Vaswani et al., 2017; Kitaev and Klein, 2018). 3 elf S G-elf G-glow charm ARG0 ARG1 O G-charm Figure 1: AM dep-trees and graphs for the fairy that begins to glow. We usually write our example AM dep-trees without alignments as in (b). We include node names where helpful, as in (c), where e.g. b is labeled begin. ARG0 charm S glow ARG0 ARG1 O S G-charmP Figure 2: Graph constants (Banarescu et al., 2013). The AM dep-tree edges are labeled with graph-combining operations, taken from the Apply-Modify (AM) algebra (Groschwitz et al., 2017; Groschwitz, 2019). Graphs are built out of fragments called graph constants (Fig. 2). Each graph constant has a root, marked with a rectangular outline, and may have special node markers called sources (Courcelle and Engelfriet, 2012), drawn in red, which mark the empty slots where other graphs will be inserted. In Fig. 1a, the A PP O operation plugs the root of G-glow into the O source of G-begin. Because G-begin and G-glow both have an S-source, A PP O merges these nodes, creating a reentrancy, i.e. an undirected cycle, and yielding Fig. 1d, which is in turn attached at S to the root of G-"
2021.spnlp-1.3,copestake-flickinger-2000-open,0,0.272166,"Missing"
2021.spnlp-1.3,P18-1170,1,0.937841,"glow, the AM dependency tree is well-typed; otherwise the tree could not be evaluated to a graph. Thus, the graph constants lexically specify the semantic valency of each word as well as reentrancies due to e.g. control. If a graph has no sources, we say it has the empty type [ ]; if a source in a graph printed here has no annotation, it is assumed to have the empty request (i.e. its argument must have no sources). AM dependency parsing Compositional semantic graph parsing methods do not predict a graph directly, but rather predict a compositional structure which in turn determines the graph. Groschwitz et al. (2018) represent the compositional structure of a graph with AM dependency trees (AM dep-trees for short) like the one in Fig. 1a. It describes the way the meanings of the words – the graph fragments in Fig. 2 – combine to form the semantic graph in Fig. 1c, here an AMR Parsing Groschwitz et al. (2018) use a neural supertagger and dependency parser to predict scores 23 g:glow g:glow ARG0 f:fairy mod ARG0 f:fairy mod-of t:tiny fairy g:P-glow:[f] P-fairy begin A PPf f:P-fairy:[ ] M ODf t:P-tiny:[f] ARG0 ARG1 glow tiny sparkle ARG0 mod-of ARG1 f f f and op1 s f Figure 3: The tiny fairy glows. g P-glow"
2021.spnlp-1.3,P01-1019,0,0.077037,"Missing"
2021.spnlp-1.3,N19-1115,0,0.0249841,"rsers, in contrast to our joint neural technique. None of these approaches use slot names that carry meaning; to the best of our knowledge this work is the first to learn them from data. Fancellu et al. (2019) use DAG grammars for compositional parsing of Discourse Representation Structures (DRS). Their algorithm for extracting the compositional structure of a graph is deterministic and graphbank-independent, but comes at a cost: for example, rules for heads require different versions depending on how often the head is modified, reducing the reusability of the rule. Maillard et al. (2019) and Havrylov et al. (2019) learn compositional, continuous-space neural sentence encodings using latent tree structures. Their tasks are different: they learn to predict continousspace embeddings; we learn to predict symbolic compositional structures. Similar observations hold for self-attention (Vaswani et al., 2017; Kitaev and Klein, 2018). 3 elf S G-elf G-glow charm ARG0 ARG1 O G-charm Figure 1: AM dep-trees and graphs for the fairy that begins to glow. We usually write our example AM dep-trees without alignments as in (b). We include node names where helpful, as in (c), where e.g. b is labeled begin. ARG0 charm S g"
2021.spnlp-1.3,N19-1423,0,0.0230475,"Missing"
2021.spnlp-1.3,P18-1249,0,0.0146058,"or extracting the compositional structure of a graph is deterministic and graphbank-independent, but comes at a cost: for example, rules for heads require different versions depending on how often the head is modified, reducing the reusability of the rule. Maillard et al. (2019) and Havrylov et al. (2019) learn compositional, continuous-space neural sentence encodings using latent tree structures. Their tasks are different: they learn to predict continousspace embeddings; we learn to predict symbolic compositional structures. Similar observations hold for self-attention (Vaswani et al., 2017; Kitaev and Klein, 2018). 3 elf S G-elf G-glow charm ARG0 ARG1 O G-charm Figure 1: AM dep-trees and graphs for the fairy that begins to glow. We usually write our example AM dep-trees without alignments as in (b). We include node names where helpful, as in (c), where e.g. b is labeled begin. ARG0 charm S glow ARG0 ARG1 O S G-charmP Figure 2: Graph constants (Banarescu et al., 2013). The AM dep-tree edges are labeled with graph-combining operations, taken from the Apply-Modify (AM) algebra (Groschwitz et al., 2017; Groschwitz, 2019). Graphs are built out of fragments called graph constants (Fig. 2). Each graph constan"
2021.spnlp-1.3,P19-1450,1,0.0709078,"tional structures for semantic graph parsing Jonas Groschwitz Saarland University jonasg@coli.uni-saarland.de Meaghan Fowlie Utrecht University m.fowlie@uu.nl Alexander Koller Saarland University koller@coli.uni-saarland.de Abstract train a such a parser, the compositional structures must be made explicit. However, these structures are not annotated in most sembanks. For instance, the AM (Apply-Modify) dependency parser of Groschwitz et al. (2018) uses a neural model to predict AM dependency trees, compositional structures that evaluate to semantic graphs. Their parser achieves high accuracy (Lindemann et al., 2019) and parsing speed (Lindemann et al., 2020) across a variety of English semantic graphbanks. To obtain an AM dependency tree for each graph in the corpus, they use hand-written graphbank-specific heuristics. These heuristics cost significant time and expert knowledge to create, limiting the ability of the AM parser to scale to new sembanks. In this paper, we drastically reduce the need for hand-written heuristics for training the AM dependency parser. We first present a graphbankindependent method to compactly represent the relevant compositional structures of a graph in a tree automaton. We t"
2021.spnlp-1.3,2020.emnlp-main.323,1,0.857917,"g Jonas Groschwitz Saarland University jonasg@coli.uni-saarland.de Meaghan Fowlie Utrecht University m.fowlie@uu.nl Alexander Koller Saarland University koller@coli.uni-saarland.de Abstract train a such a parser, the compositional structures must be made explicit. However, these structures are not annotated in most sembanks. For instance, the AM (Apply-Modify) dependency parser of Groschwitz et al. (2018) uses a neural model to predict AM dependency trees, compositional structures that evaluate to semantic graphs. Their parser achieves high accuracy (Lindemann et al., 2019) and parsing speed (Lindemann et al., 2020) across a variety of English semantic graphbanks. To obtain an AM dependency tree for each graph in the corpus, they use hand-written graphbank-specific heuristics. These heuristics cost significant time and expert knowledge to create, limiting the ability of the AM parser to scale to new sembanks. In this paper, we drastically reduce the need for hand-written heuristics for training the AM dependency parser. We first present a graphbankindependent method to compactly represent the relevant compositional structures of a graph in a tree automaton. We then train a neural AM dependency parser dir"
2021.spnlp-1.3,S15-2153,0,0.0459728,"Missing"
2021.spnlp-1.3,K15-1004,0,0.022846,"s G-fairy b:begin G-fairy:[ ] M OD S ARG0 M OD S G-begin A PP O G-begin:[S, O[S]] A PP O G-glow The fairy that begins to glow (a) AM dep-tree with word alignments. The dashed lines connect tokens to their graph constants, and arrows point from heads to arguments, labeled by the operation that puts the graphs together. f:fairy ARG0 fairy ARG1 g:glow (c) AMR G-glow:[S] begin (b) AM dep-tree without ARG0 ARG1 alignments. Nodes are ARG0 labeled with graph conS glow stants, paired with their types for ease of presenta- (d) Partial result: tion. begins to glow begin G-fairy ARG0 ARG1 S O[S] G-begin Peng et al. (2015) and Chen et al. (2018), use CCG and HRG based grammars to parse AMR and EDS (Flickinger et al., 2017). They use a combination of heuristics, hand-annotated compositional structures and sampling to obtain training data for their parsers, in contrast to our joint neural technique. None of these approaches use slot names that carry meaning; to the best of our knowledge this work is the first to learn them from data. Fancellu et al. (2019) use DAG grammars for compositional parsing of Discourse Representation Structures (DRS). Their algorithm for extracting the compositional structure of a graph"
2021.spnlp-1.3,P13-2131,0,0.033289,"Missing"
2021.starsem-1.18,N16-1067,0,0.0212048,"ccurately tags script participants. 1 Figure 1: Descriptions of FIXING A FLAT TIRE from InScript. Script parsing identifies events and participants from surface text. Introduction Script knowledge is a category of common sense knowledge that describes how people conduct everyday activities sequentially (Schank and Abelson, 1977). Script knowledge of a specific scenario, e.g. GROCERY SHOPPING, includes the events that comprise the scenario, the participants involved, and the relations between them. Script knowledge is useful for various downstream NLP applications, such as referent prediction (Ahrendt and Demberg, 2016; Modi et al., 2017), discourse sense classification (Lee et al., 2020), story generation (Zhai et al., 2019, 2020). Script parsing identifies pre-defined sets of script events and participants from surface text (see Figure 1). For a specific scenario, script parsing essentially boils down to determining what each verb and each NP (which we term candidate) refers to in the context of that scenario. Script parsing is an under-investigated, complex task. It is highly contextualized and corresponds to each specific scenario. The task is challenging even for humans: the inter-annotator agreement i"
2021.starsem-1.18,D14-1159,0,0.0858513,"Missing"
2021.starsem-1.18,N19-1412,0,0.0265169,"based on a pre-defined set. They separately train a model for identifying event triggers and a model for finding plausible argument candidates. The features used rely on syntax, semantic roles and some external domain resources. In contrast, we propose a model that jointly learns how to identify as well as label events and participants. Much of the previous work on inferring script knowledge from text is focused around completing an event chain by predicting the missing event (Chambers and Jurafsky, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015), the missing text (Bisk et al., 2019) or both (Pichotta and Mooney, 2016). These approaches consider surface forms of event verbs and syntactic relation types of their arguments (subj, obj), while our task operates on abstract event and participant types. 3 3.1 Method Data and Pre-processing Our work is based on two English corpora, InScript and DeScript. InScript (Modi et al., 2016) includes around 100 stories about each of 10 daily activities (scenarios), e.g., GOING GROCERY SHOPPING, TAKING A BATH , and RIDING IN A PUBLIC BUS . The corpus annotates surface text with event and participant classes, and specifies the candidates a"
2021.starsem-1.18,W11-2138,0,0.00994841,"nce difference compared to others can be significant despite small margins. the style of their language, we also perform explicit domain modelling with (2) Corpus embedding. We follow Stymne et al. (2018) to train a vector representation (the corpus embedding) for each corpus to capture corpus-specific patterns. We concatenate the corpus representation with each candidate representation, to substitute the input term (ψθψ (c)) to the linear classifier γ with ˜c = ψθψ (c; ηθη (d)) (5) Here η(·) denotes the corpus embeddings. Data Augmentation. Secondly, we augment InScript via back-translation (Bojar and Tamchyna, 2011; Sennrich et al., 2016; Xie et al., 2020) to paraphrase the original data and help the model generalize better over the surface text. The stories are translated to French and back with Google Translate. The participant and event annotations are mapped to the paraphrases according to heuristics based on word-level semantics and string matching. The new data was concatenated with the original InScript and both were treated as a single domain. See the appendix for more implementation details1 . In the example below, the event verb takes a different tense and surface form in the back-translation."
2021.starsem-1.18,P08-1090,0,0.0981859,"at denote an occurrence of an event are considered event triggers. However, unlike our approach, these events are not based on a pre-defined set. They separately train a model for identifying event triggers and a model for finding plausible argument candidates. The features used rely on syntax, semantic roles and some external domain resources. In contrast, we propose a model that jointly learns how to identify as well as label events and participants. Much of the previous work on inferring script knowledge from text is focused around completing an event chain by predicting the missing event (Chambers and Jurafsky, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015), the missing text (Bisk et al., 2019) or both (Pichotta and Mooney, 2016). These approaches consider surface forms of event verbs and syntactic relation types of their arguments (subj, obj), while our task operates on abstract event and participant types. 3 3.1 Method Data and Pre-processing Our work is based on two English corpora, InScript and DeScript. InScript (Modi et al., 2016) includes around 100 stories about each of 10 daily activities (scenarios), e.g., GOING GROCERY SHOPPING, TAKING A BATH , and RIDING IN A PUBLI"
2021.starsem-1.18,E12-1034,0,0.0637911,"Missing"
2021.starsem-1.18,2020.findings-emnlp.446,0,0.0246982,"IRE from InScript. Script parsing identifies events and participants from surface text. Introduction Script knowledge is a category of common sense knowledge that describes how people conduct everyday activities sequentially (Schank and Abelson, 1977). Script knowledge of a specific scenario, e.g. GROCERY SHOPPING, includes the events that comprise the scenario, the participants involved, and the relations between them. Script knowledge is useful for various downstream NLP applications, such as referent prediction (Ahrendt and Demberg, 2016; Modi et al., 2017), discourse sense classification (Lee et al., 2020), story generation (Zhai et al., 2019, 2020). Script parsing identifies pre-defined sets of script events and participants from surface text (see Figure 1). For a specific scenario, script parsing essentially boils down to determining what each verb and each NP (which we term candidate) refers to in the context of that scenario. Script parsing is an under-investigated, complex task. It is highly contextualized and corresponds to each specific scenario. The task is challenging even for humans: the inter-annotator agreement is quite modest, at 0.64 and 0.77 Fleiss’ κ for event and participant pa"
2021.starsem-1.18,L16-1555,0,0.216246,"hai et al., 2019, 2020). Script parsing identifies pre-defined sets of script events and participants from surface text (see Figure 1). For a specific scenario, script parsing essentially boils down to determining what each verb and each NP (which we term candidate) refers to in the context of that scenario. Script parsing is an under-investigated, complex task. It is highly contextualized and corresponds to each specific scenario. The task is challenging even for humans: the inter-annotator agreement is quite modest, at 0.64 and 0.77 Fleiss’ κ for event and participant parsing, respectively (Modi et al., 2016). Various factors need to be taken into consideration for this task. (1) At the local level, the basic semantics of the candidates. (2) At the discourse level, the sequence of events and participants should sketch a reasonable agenda for the activity. For example, the events must occur in a feasible order; when an NP is a dependent of an adjacent verb, the predicted participant type must be one that participates in the predicted event. In Figure 1, the same verb found was assigned different event classes: found bike pump as get tools whereas found a small glass shard as examine tire. One would"
2021.starsem-1.18,Q17-1003,0,0.0129177,"cipants. 1 Figure 1: Descriptions of FIXING A FLAT TIRE from InScript. Script parsing identifies events and participants from surface text. Introduction Script knowledge is a category of common sense knowledge that describes how people conduct everyday activities sequentially (Schank and Abelson, 1977). Script knowledge of a specific scenario, e.g. GROCERY SHOPPING, includes the events that comprise the scenario, the participants involved, and the relations between them. Script knowledge is useful for various downstream NLP applications, such as referent prediction (Ahrendt and Demberg, 2016; Modi et al., 2017), discourse sense classification (Lee et al., 2020), story generation (Zhai et al., 2019, 2020). Script parsing identifies pre-defined sets of script events and participants from surface text (see Figure 1). For a specific scenario, script parsing essentially boils down to determining what each verb and each NP (which we term candidate) refers to in the context of that scenario. Script parsing is an under-investigated, complex task. It is highly contextualized and corresponds to each specific scenario. The task is challenging even for humans: the inter-annotator agreement is quite modest, at 0"
2021.starsem-1.18,S17-1016,0,0.076649,"y annotated the scenario-specific types of events and participants to accommodate aligning surface text with data-driven script knowledge. The goal is to identify spans of the text that refer to the events and participants that are typically involved in a script. For the case of the script about fixing a bike, the typical events include riding a bike, noticing a flat tire, getting tools, repairing the tire and testing it. These events and participants are pre-defined for each activity and the task is to label the tokens with these abstract classes, whereby the surface forms vary. The model by Ostermann et al. (2017) is the state of the art for event parsing over InScript which was formulated as a sequence tagging task. The authors used a linear CRF to identify the script events. Its features include syntax, FrameNet (Ruppenhofer et al., 2006) features, pre-trained word embeddings and a number of script-related features encoding script-specific aspects like event order. Our work shares many similarities with Berant et al. (2014). To do question answering in the biological domain, they first build a graph representation of events, participants and their relations given a text about a biological process. To"
2021.starsem-1.18,E14-1024,0,0.0223754,"ed event triggers. However, unlike our approach, these events are not based on a pre-defined set. They separately train a model for identifying event triggers and a model for finding plausible argument candidates. The features used rely on syntax, semantic roles and some external domain resources. In contrast, we propose a model that jointly learns how to identify as well as label events and participants. Much of the previous work on inferring script knowledge from text is focused around completing an event chain by predicting the missing event (Chambers and Jurafsky, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015), the missing text (Bisk et al., 2019) or both (Pichotta and Mooney, 2016). These approaches consider surface forms of event verbs and syntactic relation types of their arguments (subj, obj), while our task operates on abstract event and participant types. 3 3.1 Method Data and Pre-processing Our work is based on two English corpora, InScript and DeScript. InScript (Modi et al., 2016) includes around 100 stories about each of 10 daily activities (scenarios), e.g., GOING GROCERY SHOPPING, TAKING A BATH , and RIDING IN A PUBLIC BUS . The corpus annotates surface text with"
2021.starsem-1.18,P16-1027,0,0.0176525,"They separately train a model for identifying event triggers and a model for finding plausible argument candidates. The features used rely on syntax, semantic roles and some external domain resources. In contrast, we propose a model that jointly learns how to identify as well as label events and participants. Much of the previous work on inferring script knowledge from text is focused around completing an event chain by predicting the missing event (Chambers and Jurafsky, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015), the missing text (Bisk et al., 2019) or both (Pichotta and Mooney, 2016). These approaches consider surface forms of event verbs and syntactic relation types of their arguments (subj, obj), while our task operates on abstract event and participant types. 3 3.1 Method Data and Pre-processing Our work is based on two English corpora, InScript and DeScript. InScript (Modi et al., 2016) includes around 100 stories about each of 10 daily activities (scenarios), e.g., GOING GROCERY SHOPPING, TAKING A BATH , and RIDING IN A PUBLIC BUS . The corpus annotates surface text with event and participant classes, and specifies the candidates according to their syntactic dependen"
2021.starsem-1.18,P10-1100,1,0.631743,"mputational Linguistics parsing: methodology-wise, (1) we propose a hierarchical sequence model, which learns patterns in different granularity levels with different sequence models. (2) We investigate data augmentation approaches to this task. In terms of results, (3) we achieve accurate participant parsing results for the first time and (4) improve the state of the art in event parsing by over 16 points F1-score. 2 Related Work Theoretical considerations of scripts in AI (as described by Schank and Abelson, 1977; Barr and Feigenbaum, 1981) were analyzed in a wide coverage empirical study by Regneri et al. (2010), who crowdsourced event descriptions of several everyday activities (scenarios). They applied unsupervised methods to compute a graph representation of the script’s temporal structure. As a direct extension, Modi et al. (2016) and Wanzare et al. (2016) collected the InScript and DeScript corpora of event descriptions and manually annotated the scenario-specific types of events and participants to accommodate aligning surface text with data-driven script knowledge. The goal is to identify spans of the text that refer to the events and participants that are typically involved in a script. For t"
2021.starsem-1.18,D15-1195,0,0.0451365,"Missing"
2021.starsem-1.18,P16-1009,0,0.0106718,"others can be significant despite small margins. the style of their language, we also perform explicit domain modelling with (2) Corpus embedding. We follow Stymne et al. (2018) to train a vector representation (the corpus embedding) for each corpus to capture corpus-specific patterns. We concatenate the corpus representation with each candidate representation, to substitute the input term (ψθψ (c)) to the linear classifier γ with ˜c = ψθψ (c; ηθη (d)) (5) Here η(·) denotes the corpus embeddings. Data Augmentation. Secondly, we augment InScript via back-translation (Bojar and Tamchyna, 2011; Sennrich et al., 2016; Xie et al., 2020) to paraphrase the original data and help the model generalize better over the surface text. The stories are translated to French and back with Google Translate. The participant and event annotations are mapped to the paraphrases according to heuristics based on word-level semantics and string matching. The new data was concatenated with the original InScript and both were treated as a single domain. See the appendix for more implementation details1 . In the example below, the event verb takes a different tense and surface form in the back-translation. O : when Irider was ri"
2021.starsem-1.18,P18-2098,0,0.0233887,"Missing"
2021.starsem-1.18,L16-1556,0,0.111381,"of results, (3) we achieve accurate participant parsing results for the first time and (4) improve the state of the art in event parsing by over 16 points F1-score. 2 Related Work Theoretical considerations of scripts in AI (as described by Schank and Abelson, 1977; Barr and Feigenbaum, 1981) were analyzed in a wide coverage empirical study by Regneri et al. (2010), who crowdsourced event descriptions of several everyday activities (scenarios). They applied unsupervised methods to compute a graph representation of the script’s temporal structure. As a direct extension, Modi et al. (2016) and Wanzare et al. (2016) collected the InScript and DeScript corpora of event descriptions and manually annotated the scenario-specific types of events and participants to accommodate aligning surface text with data-driven script knowledge. The goal is to identify spans of the text that refer to the events and participants that are typically involved in a script. For the case of the script about fixing a bike, the typical events include riding a bike, noticing a flat tire, getting tools, repairing the tire and testing it. These events and participants are pre-defined for each activity and the task is to label the tok"
2021.starsem-1.18,2020.coling-main.212,1,0.815389,"Missing"
2021.starsem-1.18,W19-3404,1,0.807816,"ntifies events and participants from surface text. Introduction Script knowledge is a category of common sense knowledge that describes how people conduct everyday activities sequentially (Schank and Abelson, 1977). Script knowledge of a specific scenario, e.g. GROCERY SHOPPING, includes the events that comprise the scenario, the participants involved, and the relations between them. Script knowledge is useful for various downstream NLP applications, such as referent prediction (Ahrendt and Demberg, 2016; Modi et al., 2017), discourse sense classification (Lee et al., 2020), story generation (Zhai et al., 2019, 2020). Script parsing identifies pre-defined sets of script events and participants from surface text (see Figure 1). For a specific scenario, script parsing essentially boils down to determining what each verb and each NP (which we term candidate) refers to in the context of that scenario. Script parsing is an under-investigated, complex task. It is highly contextualized and corresponds to each specific scenario. The task is challenging even for humans: the inter-annotator agreement is quite modest, at 0.64 and 0.77 Fleiss’ κ for event and participant parsing, respectively (Modi et al., 201"
C00-1067,P94-1023,0,0.0370887,"Missing"
C00-1067,P98-1058,1,0.930422,"sed on nite set constraint programming. This implementation can be obtained by adapting an existing implementation of a solver for dominance constraints. Finally, we conclude and point to further work in Section 6. tures, 2 Tree Descriptions In this section, we de ne the Constraint Language for DPL structures, CL(DPL), a language of tree descriptions which conservatively extends dominance constraints (Marcus et al., 1983; Rambow et al., 1995; Koller et al., 2000) by variable binding constraints. CL(DPL) is a close relative of the Constraint Language for Lambda Structures (CLLS), presented in (Egg et al., 1998). It is interpreted over DPL structures { trees extended by a variable binding function which can be used to encode formulas of dynamic (or static) predicate logic. We will de ne DPL structures in two steps and then the language to talk about them. 2.1 Tree Structures For the de nitions below, we assume a signature  = f@j2; varj0; 8j1; 9j1; ^j2; manj1, likej2; : : :g of node labels, each of which is equipped with a xed arity n  0. The labels ^; :; 8; : : : are the rst-order connectives. Node labels are ranged over by f; g; a; b, and the arity of a label f is denoted by ar(f ); i.e. if fj 2"
C00-1067,P00-1047,1,0.7705,"do the kind of underspeci ed reasoning described above without enumerating readings. In Section 5, we sketch an implementation of our inference system based on nite set constraint programming. This implementation can be obtained by adapting an existing implementation of a solver for dominance constraints. Finally, we conclude and point to further work in Section 6. tures, 2 Tree Descriptions In this section, we de ne the Constraint Language for DPL structures, CL(DPL), a language of tree descriptions which conservatively extends dominance constraints (Marcus et al., 1983; Rambow et al., 1995; Koller et al., 2000) by variable binding constraints. CL(DPL) is a close relative of the Constraint Language for Lambda Structures (CLLS), presented in (Egg et al., 1998). It is interpreted over DPL structures { trees extended by a variable binding function which can be used to encode formulas of dynamic (or static) predicate logic. We will de ne DPL structures in two steps and then the language to talk about them. 2.1 Tree Structures For the de nitions below, we assume a signature  = f@j2; varj0; 8j1; 9j1; ^j2; manj1, likej2; : : :g of node labels, each of which is equipped with a xed arity n  0. The labels ^;"
C00-1067,P83-1020,0,0.130562,"nstraints. We obtain a procedure that can do the kind of underspeci ed reasoning described above without enumerating readings. In Section 5, we sketch an implementation of our inference system based on nite set constraint programming. This implementation can be obtained by adapting an existing implementation of a solver for dominance constraints. Finally, we conclude and point to further work in Section 6. tures, 2 Tree Descriptions In this section, we de ne the Constraint Language for DPL structures, CL(DPL), a language of tree descriptions which conservatively extends dominance constraints (Marcus et al., 1983; Rambow et al., 1995; Koller et al., 2000) by variable binding constraints. CL(DPL) is a close relative of the Constraint Language for Lambda Structures (CLLS), presented in (Egg et al., 1998). It is interpreted over DPL structures { trees extended by a variable binding function which can be used to encode formulas of dynamic (or static) predicate logic. We will de ne DPL structures in two steps and then the language to talk about them. 2.1 Tree Structures For the de nitions below, we assume a signature  = f@j2; varj0; 8j1; 9j1; ^j2; manj1, likej2; : : :g of node labels, each of which is equ"
C00-1067,P95-1021,0,0.0235256,"a procedure that can do the kind of underspeci ed reasoning described above without enumerating readings. In Section 5, we sketch an implementation of our inference system based on nite set constraint programming. This implementation can be obtained by adapting an existing implementation of a solver for dominance constraints. Finally, we conclude and point to further work in Section 6. tures, 2 Tree Descriptions In this section, we de ne the Constraint Language for DPL structures, CL(DPL), a language of tree descriptions which conservatively extends dominance constraints (Marcus et al., 1983; Rambow et al., 1995; Koller et al., 2000) by variable binding constraints. CL(DPL) is a close relative of the Constraint Language for Lambda Structures (CLLS), presented in (Egg et al., 1998). It is interpreted over DPL structures { trees extended by a variable binding function which can be used to encode formulas of dynamic (or static) predicate logic. We will de ne DPL structures in two steps and then the language to talk about them. 2.1 Tree Structures For the de nitions below, we assume a signature  = f@j2; varj0; 8j1; 9j1; ^j2; manj1, likej2; : : :g of node labels, each of which is equipped with a xed arit"
C00-1067,C98-1056,1,\N,Missing
C02-1113,E91-1028,0,0.0429996,"sk is quite simple for objects which are new to the player. In this case, we generate an indefinite NP containing the type and (if it has one) color of the object, as in the bowl contains a red apple. We use RACER’s retrieval functionality to extract this information from the knowledge base. To refer to an object that the player already has encountered, we try to construct a definite description that, given the player knowledge, uniquely identifies this object. For this purpose we use a variant of Dale and Reiter’s (1995) incremental algorithm, extended to deal with relations between objects (Dale and Haddock, 1991). The properties of the target referent are looked at in some predefined order (e.g. first its type, then its color, its location, parts it may have, . . .). A property is added to the description if at least one other object (a distractor) is excluded from it because it doesn’t share this property. This is done until the description uniquely identifies the target referent. The algorithm uses RACER’s reasoning and retrieval functionality to access the relevant information about the context, which included e.g. computing the properties of the target referent and finding the distracting instance"
C02-1113,P01-1024,0,0.0192802,"n Reference Resolution Content Determination Actions A-Box: World Model A-Box: User Knowledge T-Box Figure 2: The architecture. terface with knowledge bases and a discourse model (drawn as rectangles). There are two separate knowledge bases, which share a set of common definitions: One represents the true state of the world in a world model, the other keeps track of what the player knows about the world. Solid arrows indicate the general flow of information, dashed arrows indicate access to the knowledge bases. The user’s input is first parsed using an efficient parser for dependency grammar (Duchier and Debusmann, 2001). Next, referring expressions are resolved to individuals in the game world. The result is a ground term or a sequence of ground terms that indicates the action(s) the user wants to take. The Actions module looks up these actions in a database (where they are specified in a STRIPS-like format), checks whether the action’s preconditions are met in the world, and, if yes, updates the world state with the effects of the action. The action can also specify effects on the user’s knowledge. This information is further enriched by the Content Determination module; for example, this module computes de"
C02-1113,P02-1003,1,0.834761,"in a database (where they are specified in a STRIPS-like format), checks whether the action’s preconditions are met in the world, and, if yes, updates the world state with the effects of the action. The action can also specify effects on the user’s knowledge. This information is further enriched by the Content Determination module; for example, this module computes detailed descriptions of objects the player wants to look at. The Reference Generation module translates the internal names of individuals into descriptions that can be verbalized. In the last step, an efficient realization module (Koller and Striegnitz, 2002) builds the output sentences according to a TAG grammar. The player knowledge is updated after Reference Generation when the content of the game’s response, including the new information carried e.g. by indefinite NPs, is fully established. If an error occurs at any stage, e.g. because a precondition of the action fails, an error message specifying the reasons for the failure is generated by using the normal generation track (Content Determination, Reference Generation, Realization) of the game. The system is implemented in the programming language Mozart (Mozart Consortium, 1999) and provides"
C02-1113,P01-1061,0,0.0260918,"ics manages to pick out a unique entity, we consider the definite description to be truly ambiguous and return an error message to the user, indicating the ambiguity. 5.2 Resolving Syntactic Ambiguities Another class of ambiguities which we consider are syntactic ambiguities, especially of PP attachment. We try to resolve them, too, by taking referential information into account. In the simplest case, the referring expressions in some of the syntactic readings have no possible referent in the player A-Box at all. If this happens, we filter these readings out and only continue with the others (Schuler, 2001). For example, the sentence unlock the toolbox with the key is ambiguous. In a scenario where there is a toolbox and a key, but the key is not attached to the toolbox, resolution fails for one of the analyses and thereby resolves the syntactic ambiguity. If more than one syntactic reading survives this first test, we perform the same computations as above to filter out possible referents which are either unsalient or violate the player’s knowledge. Sometimes, only one syntactic reading will have a referent in this narrower sense; in this case, we are done. Otherwise, i.e. if more than one synt"
C02-1113,P98-2204,0,0.0308337,"Missing"
C02-1113,C92-2105,0,\N,Missing
C02-1113,C98-2199,0,\N,Missing
C04-1026,P01-1024,1,0.9126,"he underspecification approach. We assume a set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; a grammatical analysis is a tuple with one component for each dimension, and a grammar describes a set of such tuples. While we make no a priori functionality assumptions about the relation of the linguistic dimensions, functional mappings can be obtained as a special case. We formalise our syntax-semantics interface using Extensible Dependency Grammar (XDG), a new grammar formalism which generalises earlier work on Topological Dependency Grammar (Duchier and Debusmann, 2001). The relational syntax-semantics interface is supported by a parser for XDG based on constraint programming. The crucial feature of this parser is that it supports the concurrent flow of possibly partial information between any two dimensions: once additional information becomes available on one dimension, it can be propagated to any other dimension. Grammaticality conditions and preferences (e. g. selectional restrictions) can be specified on their natural level of representation, and inferences on each dimension can help reduce ambiguity on the others. This generalises the idea of underspec"
C04-1026,E03-1054,1,0.834095,"ces on SC . This means that semantic information can be used to disambiguate syntactic ambiguities, and semantic information such as selectional preferences can be stated on their natural level of representation, rather than be forced into the ID dimension directly. Similarly, the introduction of new edges on SC could trigger a similar reasoning process which would infer new PA-edges, and thus indirectly also new ID-edges. Such new edges on SC could come from inferences with world or discourse knowledge (Koller and Niehren, 2000), scope preferences, or interactions with information structure (Duchier and Kruijff, 2003). 4 Traditional Semantics Our syntax-semantics interface represents semantic information as graphs on the PA and SC dimensions. While this looks like a radical departure from traditional semantic formalisms, we consider these graphs simply an alternative way of presenting more traditional representations. We devote the rest of the paper to demonstrating that a pair of a PA and a SC structure can be interpreted as a Montague-style formula, and that a partial analysis on these two dimensions can be seen as an underspecified semantic description. 4.1 Montague-style Interpretation In order to extr"
C04-1026,P98-1077,0,0.0225832,"ctic ambiguity by compiling semantic distinctions into the syntax (Montague, 1974; Steedman, 1999; Moortgat, 2002). This restores functionality, but comes at the price of an artificial blowup of syntactic ambiguity. A second approach is to assume a non-deterministic mapping from syntax to semantics as in generative grammar (Chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation. For LFG, the operation of functional uncertaintainty allows for a restricted form of relationality (Kaplan and Maxwell III, 1988). Finally, underspecification (Egg et al., 2001; Gupta and Lamping, 1998; Copestake et al., 2004) introduces a new level of representation, which can be computed functionally from a syntactic analysis and encapsulates semantic ambiguity in a way that supports the enumeration of all semantic readings by need. In this paper, we introduce a completely relational syntax-semantics interface, building upon the underspecification approach. We assume a set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; a grammatical analysis is a tuple with one component for each dimension, and a grammar describes a set of such tuples."
C04-1026,C88-1060,0,0.253565,"Missing"
C04-1026,C00-1067,1,0.832765,"a prep-child of student on ID. In the other direction, the solver will infer more dominances on SC . This means that semantic information can be used to disambiguate syntactic ambiguities, and semantic information such as selectional preferences can be stated on their natural level of representation, rather than be forced into the ID dimension directly. Similarly, the introduction of new edges on SC could trigger a similar reasoning process which would infer new PA-edges, and thus indirectly also new ID-edges. Such new edges on SC could come from inferences with world or discourse knowledge (Koller and Niehren, 2000), scope preferences, or interactions with information structure (Duchier and Kruijff, 2003). 4 Traditional Semantics Our syntax-semantics interface represents semantic information as graphs on the PA and SC dimensions. While this looks like a radical departure from traditional semantic formalisms, we consider these graphs simply an alternative way of presenting more traditional representations. We devote the rest of the paper to demonstrating that a pair of a PA and a SC structure can be interpreted as a Montague-style formula, and that a partial analysis on these two dimensions can be seen as"
C04-1026,P02-1003,1,0.835191,"e present, and that others are excluded. Partial analyses will play an important role in Section 3.3. Because propagation operates on all dimensions concurrently, the constraint solver can frequently infer information about one dimension from information on another, if there is a multi-dimensional principle linking the two dimensions. These inferences take place while the constraint problem is being solved, and they can often be drawn before the solver commits to any single solution. Because XDG allows us to write grammars with completely free word order, XDG solving is an NPcomplete problem (Koller and Striegnitz, 2002). This means that the worst-case complexity of the solver is exponential, but the average-case complexity for the hand-crafted grammars we experimented with is often better than this result suggests. We hope there are useful fragments of XDG that would guarantee polynomial worst-case complexity. 3 A Relational Syntax-Semantics Interface Now that we have the formal and processing frameworks in place, we can define a relational syntaxsemantics interface for XDG. We will first show how we encode semantics within the XDG framework. Then we will present an example grammar (including some principle"
C04-1026,J93-4001,0,0.0305064,"get bidirectional grammars for free. While the solver is reasonably efficient for many (hand-crafted) grammars, it is an important goal for the future to ensure that it can handle largescale grammars imported from e.g. XTAG (XTAG Research Group, 2001) or induced from treebanks. One way in which we hope to achieve this is to identify fragments of XDG with provably polynomial parsing algorithms, and which contain most useful grammars. Such grammars would probably have to specify word orders that are not completely free, and we would have to control the combinatorics of the different dimensions (Maxwell and Kaplan, 1993). One interesting question is also whether different dimensions can be compiled into a single dimension, which might improve efficiency in some cases, and also sidestep the monostratal vs. multistratal distinction. The crucial ingredient of XDG that make relational syntax-semantics processing possible are the declaratively specified principles. So far, we have only given some examples for principle specifications; while they could all be written as Horn clauses, we have not committed to any particular representation formalism. The development of such a representation formalism will of course b"
C04-1026,P99-1039,0,0.0162758,"nambiguous sentence can have multiple semantic readings. Conversely, a common situation in natural language generation is that one semantic representation can be verbalised in multiple ways. This means that the relation between syntax and semantics is not functional at all, but rather a true m-to-n relation. There is a variety of approaches in the literature on syntax-semantics interfaces for coping with this situation, but none of them is completely satisfactory. One way is to recast semantic ambiguity as syntactic ambiguity by compiling semantic distinctions into the syntax (Montague, 1974; Steedman, 1999; Moortgat, 2002). This restores functionality, but comes at the price of an artificial blowup of syntactic ambiguity. A second approach is to assume a non-deterministic mapping from syntax to semantics as in generative grammar (Chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation. For LFG, the operation of functional uncertaintainty allows for a restricted form of relationality (Kaplan and Maxwell III, 1988). Finally, underspecification (Egg et al., 2001; Gupta and Lamping, 1998; Copestake et al., 2004) introduces a new level of representation, which"
C04-1026,C98-1074,0,\N,Missing
C04-1049,E03-1042,0,0.0279339,") (Deutsches Museum Bonn), its successor M INERVA touring the Smithsonian in Washington (Thrun et al., 2000), and ROBOX at the Swiss National Exhibition Expo02 (Siegwart and et al, 2003). However, dialogue systems used in robotics appear to be mostly restricted to relatively simple finite-state, query/response interaction. The only robots involving dialogue systems that are state-of-the-art in computational linguistics (and that we are aware of) Geert-Jan M. Kruijff Saarland University Saarbr¨ucken, Germany gj@coli.uni-sb.de are those presented by Lemon et al. (2001), Sidner et al. (2003) and Bos et al. (2003), who equipped a mobile robot with an information state based dialogue system. There are two obvious reasons for this gap between research on dialogue systems in robotics on the one hand, and computational linguistics on the other hand. One is that the sheer cost involved in buying or building a robot makes traditional robotics research available to only a handful of research sites. Another is that building a talking robot combines the challenges presented by robotics and natural language processing, which are further exacerbated by the interactions of the two sides. In this paper, we address"
C04-1049,W03-0608,0,0.0567162,"Missing"
C04-1049,W04-2305,0,0.0261462,"h moves the piece using the robot arm. It updates its internal data structures, as well as the GNU Chess representations, computes a move for itself, and sends this move as another message to the RCX. While the dialogue system as it stands already offers some degree of flexibility with regard to move phrasings, there is still plenty of open room for improvements. One is to use even more context information, in order to understand commands like “take it with the rook”. Another is to incorporate recent work on improving recognition results in the chess domain by certain plausibility inferences (Gabsdil, 2004). 3.2 Playing a Shell Game Figure 6 introduces Luigi Legonelli. The robot represents a charismatic Italian shell-game player, and engages a human player in style: Luigi speaks German with a heavy Italian accent, lets the human player win the first round, and then tries to pull several tricks either to cheat or to keep the player interested in the game. Figure 6: A robot playing a shell game. Luigi’s Italian accent was obtained by feeding transliterated German sentences to a speech synthesizer with an Italian voice. Although the resulting accent sounded authentic, listeners who were unfamiliar"
C04-1049,W03-0609,0,0.0273309,"certainty about its surroundings. This limits the ways in which the dialogue designer can use visual context information to help with reference resolution. Robots, being embodied agents, present a host of new challenges beyond the challenges we face in computational linguistics. The interpretation of language needs to be grounded in a way that is both based in perception, and on conceptual structures to allow for generalization over experiences. Naturally, this problem extends to the acquisition of language, where approaches such as (Nicolescu and Matari´c, 2001; Carbonetto and Freitos, 2003; Oates, 2003) have focused on basing understanding entirely in sensory data. Another interesting issue concerns the interpretation of deictic references. Research in multi-modal 8 See also http://www.rescuesystem.org/robocuprescue/ interfaces has addressed the issue of deictic reference, notably in systems that allow for pen-input (see (Oviatt, 2001)). Embodied agents raise the complexity of the issues by offering a broader range of sensory input that needs to be combined (crossmodally) in order to establish possible referents. Acknowledgments. The authors would like to thank LEGO and CLT Sprachtechnologie"
D13-1134,W11-2815,1,0.839165,"ounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actual"
D13-1134,D10-1040,0,0.0392935,"a user must model this grounding process, and should ideally respond to the user’s observed behavior in real time. For instance, if the user of a pedestrian navigation system takes a wrong turn, the system should interpret this as evidence of misunderstanding and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized i"
D13-1134,W12-1604,1,0.820151,"the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistical model of RE resolution based on the listener’s behav"
D13-1134,P07-2031,0,0.0217636,"ng and bring the user back on track. We focus here on the problem of predicting how the user has resolved a referring expression (RE) that was generated by the system, i.e. a noun phrase that is intended to identify some object uniquely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistica"
D13-1134,P13-2014,0,0.0120824,"uely to the listener. A number of authors have recently offered statistical models for parts of this problem. Golland et al. (2010) and Garoufi and Koller (2011) have presented log-linear models for predicting how the listener will resolve a given RE in a given scene; however, these models do not update the probability model based on observing the user’s reactions. Nakano et al. (2007), Buschmeier and Kopp (2012), and Koller et al. (2012) all predict what the listener understood based on their behavior, but do not consider the RE itself in the model. The models of Frank and Goodman (2012) and Vogel et al. (2013) aim at explaining the effect of implicatures on the listener’s RE resolution process in terms of hypothesized interactions, but do not actually support a realtime interaction between a system and a user. In this paper, we show how to predict how the listener has resolved an RE by combining a statistical model of RE resolution based on the RE itself with a statistical model of RE resolution based on the listener’s behavior. To our knowledge, this is the first approach to combine two such models explicitly. We consider the RE grounding problem in the context of interactive, situated natural lan"
D13-1134,W10-4233,1,\N,Missing
D13-1134,W11-2845,1,\N,Missing
E03-1024,P92-1005,0,0.022823,"@ps.uni sb.de stth@coli.uni sb.de Saarland University, Saarbriicken, Germany - Abstract We define a back-and-forth translation between Hole Semantics and dominance constraints, two formalisms used in underspecified semantics. There are fundamental differences between the two, but we show that they disappear on practically useful descriptions. Our encoding bridges a gap between two underspecification formalisms, and speeds up the processing of Hole Semantics. 1 Introduction In the past few years there has been considerable activity in the development of formalisms for underspecified semantics (Alshawi and Crouch, 1992; Reyle, 1993; Bos, 1996; Copestake et al., 1999; Egg et al., 2001). These approaches all aim at controlling the combinatorial explosion of readings of sentences with multiple ambiguities. The common idea is to delay the enumeration of all readings for as long as possible. Instead, they work with a compact underspecified representation for as long as possible, only enumerating readings from this representation by need. At first glance, many of these formalisms seem to be very similar to each other. Now the question arises how deep this similarity is — are all underspecification formalisms basi"
E03-1024,W98-0113,0,0.0322011,"scription like in Fig. 2. It has no plugging in Hole Semantics, as two different things would have to be plugged into one hole, but it is satisfiable as a dominance constraint. It is this fundamental difference that restricts our result in §5, and that we avoid by using chainconnected descriptions. f a b. 4 Figure 2: A description on which Hole Semantics and dominance constraints disagree. 3 Dominance Constraints Dominance constraints are a general framework for the partial description of trees. They have been used in various areas of computational linguistics (Rogers and Vijay-Shanker, 1994; Gardent and Webber, 1998). For underspecified semantics, we consider semantic representations like higherorder formulas as trees. Dominance constraints can be extended to CLLS (Egg et al., 2001), which adds parallelism constraints to model ellipsis and binding constraints to account for variable binding without using variable names. We do not use these extensions here, for simplicity, but all results remain true if we allow binding constraints. 3.1 Syntax and Semantics We assume a signature E of function symbols ranged over by f ,g, each of which is equipped with an arity ar(f) &gt; 0, and an infinite set Vars of variabl"
E09-1052,C04-1180,0,0.0279729,"Missing"
E09-1052,P06-4020,0,0.024148,"f.ed.ac.uk Abstract In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make is to cast RMRS, for the first time, as a logic with a well-defined model theory. Previously, no such model theory existed, and so RMRS had to be used in a somewhat ad-hoc manner that l"
E09-1052,W04-3215,0,0.025199,"Missing"
E09-1052,copestake-flickinger-2000-open,0,0.04806,"ursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make is to cast RMRS, for the first time, as a logic with a well-defined model theory. Previously, no such model theory existed, and so RMRS had to be used in a somewhat ad-hoc manner that left open exactly what any given RMRS representation actually means. This has hindered practical progress, both in terms of understanding the re"
E09-1052,W07-1210,0,0.155823,"g the word lemmas as lexical predicate symbols. Such a semantic representation is highly partial. It will use predicate symbols such as cat n, which might resolve to the predicate symbols cat n 1 or cat n 2 in the complete semantic representation. (Notice the different fonts for the ambiguous and unambiguous predicate symbols.) But most underspecification formalisms (e.g., MRS (Copestake et al., 2005) and CLLS (Egg et al., 2001)) are unable to represent semantic information that is as partial as what we get from a POS tagger because they cannot underspecify predicate-argument structure. RMRS (Copestake, 2007a) is designed to address this problem. In RMRS, the information we get from the POS tagger is as follows: (4) l1 : a1 : l41 : a41 l42 : a42 l5 : a5 : l6 : a6 : l9 : a9 : _dog_n_1 _some_q_1 It exhibits several kinds of ambiguity, including a quantifier scope ambiguity and lexical ambiguities—e.g., the nouns “cat” and “dog” have 8 and 7 WordNet senses respectively. Simplifying slightly by ignoring tense information, two of its readings are shown as logical forms below; these can be represented as trees as shown in Fig. 1. (3) _cat_n_1 y x (1) Every fat cat chased some dog. (2) _some_q_1 ! every"
E09-1052,C04-1185,0,0.0232031,"low Parsing Alexander Koller Saarland University Saarbr¨ucken, Germany koller@mmci.uni-saarland.de Alex Lascarides University of Edinburgh Edinburgh, UK alex@inf.ed.ac.uk Abstract In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make is to cast RMRS,"
E09-1052,D07-1071,0,0.0481026,"Missing"
E09-1053,E03-1036,0,0.170267,"anguage is an bn cn dn . This language is not itself context-free, and therefore any PF-CCG grammar whose language contains it also contains permutations in which the order of the symbols is mixed up. The culprit for this among the restrictions that distinguish PF-CCG from full CCG seems to be that PF-CCG grammars must allow all instances of the application rules. This would mean that the ability of CCG to generate noncontext-free languages (also linguistically relevant ones) hinges crucially on its ability to restrict the allowable instances of rule schemata, for instance, using slash types (Baldridge and Kruijff, 2003). 5 Conclusion In this paper, we have shown how to read derivations of PF-CCG as dependency trees. Unlike previous proposals, our view on CCG dependencies is in line with the mainstream dependency parsing literature, which assumes tree-shaped dependency structures; while our dependency trees are less informative than the CCG derivations themselves, they contain sufficient information to reconstruct the semantic representation. We used our new dependency view to compare the strong generative capacity of PF-CCG with other mildly contextsensitive grammar formalisms. It turns out that the valency"
E09-1053,C04-1180,0,0.0401481,"sms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the"
E09-1053,J07-4004,0,0.0442998,"s tree-shaped. We then use these dependency trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG"
E09-1053,P02-1042,0,0.690196,"exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (McDonald et al., 2005; Nivre et al., 2007) and TAG derivation trees. The price we pay for restricting ourselves to trees is that we derive fewer dependencies than the more powerful approach by Clark et al. (2002). Indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled. However, we show that our dependency trees are still informative enough to reconstruct the semantic representations. The paper is structured as follows. In Section 2, we introduce CCG and the fragment PF-CCG that we consider in this paper, and compare our contribution to earlier research. In Section 3, we then show how to read off a dependency tree from a CCG derivation. Finally, we explore the strong generative capacity of CCG in Section 4 and conclud"
E09-1053,J07-3004,0,0.0454784,"trees to compare the strong generative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic p"
E09-1053,W08-2306,0,0.243319,"rage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)). We show that if we only look at valencies and ignore wo"
E09-1053,P06-1064,0,0.0170593,"rative capacities of CCG and TAG and obtain surprising results: Both formalisms generate the same languages of derivation trees – but the mechanisms they use to bring the words in these trees into a linear order are incomparable. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2001)) is an increasingly popular grammar formalism. Next to being theoretically well-motivated due to its links to combinatory logic and categorial grammar, it is distinguished by the availability of efficient open-source parsers (Clark and Curran, 2007), annotated corpora (Hockenmaier and Steedman, 2007; Hockenmaier, 2006), and mechanisms for wide-coverage semantic construction (Bos et al., 2004). However, there are limits to our understanding of the formal properties of CCG and its relation to other grammar formalisms. In particular, while it is well-known that CCG belongs to a family of mildly context-sensitive formalisms that all generate the same string languages (Vijay-Shanker and Weir, 1994), there are few results about the strong generative capacity of CCG. This makes it difficult to gauge the similarities and differences between CCG and other formalisms in how they model linguistic phenomena such as scr"
E09-1053,P07-1043,1,0.833484,"e technically restricted to the fragment of PF-CCG, and one focus of future work will be to extend them to as large a fragment of CCG as possible. In particular, we plan to extend the lambda notation used in Figure 3 to cover typeraising and higher-order categories. We would then be set to compare the behavior of wide-coverage statistical parsers for CCG with statistical dependency parsers. We anticipate that our results about the strong generative capacity of PF-CCG will be useful to transfer algorithms and linguistic insights between formalisms. For instance, the CRISP generation algorithm (Koller and Stone, 2007), while specified for TAG, could be generalized to arbitrary grammar formalisms that use regular tree languages— given our results, to CCG in particular. On the other hand, we find it striking that CCG and TAG generate the same string languages from the same tree languages by incomparable mechanisms for ordering the words in the tree. Indeed, the exact characterization of the class of CCG-inducable dependency languages is an open issue. This also has consequences for parsing complexity: We can understand why TAG and LCFRS can be parsed in polynomial time from the bounded block-degree of their"
E09-1053,P07-1021,1,0.947562,"pendency tree, it is usually convenient to specify its tree structure and the linear order of its nodes separately. The tree structure encodes the valency structure of the sentence (immediate dominance), whereas the linear precedence of the words is captured by the linear order. For the purposes of this paper, we represent a dependency tree as a pair d = (t, s), where t is a ground term over some suitable alphabet, and s is a linearization of the nodes (term addresses) of t, where by a linearization of a set S we mean a list of elements of S in which each element occurs exactly once (see also Kuhlmann and Möhl (2007)). As examples, consider (f (a, b), [1, ε, 2]) and (f (g(a)), [1 · 1, ε, 1]) . These expressions represent the dependency trees d1 = and a f b d2 = . a f g Notice that it is because of the separate specification of the tree and the order that dependency trees can become non-projective; d2 is an example. A partial dependency tree is a pair (t, s) where t is a term that may contain variables, and s is a linearization of those nodes of t that are not labelled with variables. We restrict ourselves to terms in which each variable appears exactly once, and will also prefix partial dependency trees w"
E09-1053,H05-1066,0,0.0611727,"trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures are trees brings our approach in line with the emerging mainstream in dependency parsing (McDonald et al., 2005; Nivre et al., 2007) and TAG derivation trees. The price we pay for restricting ourselves to trees is that we derive fewer dependencies than the more powerful approach by Clark et al. (2002). Indeed, we do not claim that our dependencies are linguistically meaningful beyond recording the way in which syntactic valencies are filled. However, we show that our dependency trees are still informative enough to reconstruct the semantic representations. The paper is structured as follows. In Section 2, we introduce CCG and the fragment PF-CCG that we consider in this paper, and compare our contribut"
E09-1053,1985.tmi-1.17,0,0.0434485,"Missing"
E09-1053,P90-1001,0,0.101252,"Missing"
E09-1053,P87-1015,0,0.896728,"istic phenomena such as scrambling and relative clauses (Hockenmaier and Young, 2008), and hampers the transfer of algorithms from one formalism to another. In this paper, we propose a new method for deriving a dependency tree from a CCG derivation tree for PF-CCG, a large fragment of CCG. We then explore the strong generative capacity of PF-CCG in terms of dependency trees. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al. (1987)). We show that if we only look at valencies and ignore word order, then the dependency trees induced by a PFCCG grammar form a regular tree language, just as for TAG and LCFRS. To our knowledge, this is the first time that the regularity of CCG’s derivational structures has been exposed. However, if we take the word order into account, then the classes of PF-CCG-induced and TAG-induced dependency trees are incomparable; in particular, CCG-induced dependency trees can be unboundedly non-projective in a way that TAG-induced dependency trees cannot. The fact that all our dependency structures ar"
E09-2009,P08-2050,0,0.0555286,"lenge is a new Internetbased evaluation effort for natural language generation systems. In this paper, we motivate and describe the software infrastructure that we developed to support this challenge. 1 Introduction Natural language generation (NLG) systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard is not appropriate because there can be multiple generated outputs that are equally good, and finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). On the other hand, lab-based evaluations with human subjects to assess each aspect of the system’s functionality are expensive and time-consuming. These characteristics make it hard to compare different systems and measure progress. GIVE (“Generating Instructions in Virtual Environments”) (Koller et al., 2007) is a research challenge for the NLG community designed to provide a new approach to NLG system evaluation. In the GIVE scenario, users try to solve a treasure hunt in a virtual 3D world that they have not seen before. The computer has a complete symbo"
E09-2009,W08-1113,0,0.0796843,"ffort for natural language generation systems. In this paper, we motivate and describe the software infrastructure that we developed to support this challenge. 1 Introduction Natural language generation (NLG) systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard is not appropriate because there can be multiple generated outputs that are equally good, and finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). On the other hand, lab-based evaluations with human subjects to assess each aspect of the system’s functionality are expensive and time-consuming. These characteristics make it hard to compare different systems and measure progress. GIVE (“Generating Instructions in Virtual Environments”) (Koller et al., 2007) is a research challenge for the NLG community designed to provide a new approach to NLG system evaluation. In the GIVE scenario, users try to solve a treasure hunt in a virtual 3D world that they have not seen before. The computer has a complete symbolic representation of the virtual e"
E12-1077,W09-0628,1,\N,Missing
E17-3008,W14-5002,1,0.861986,"Missing"
E17-3008,N04-1035,0,0.227575,"Missing"
E17-3008,J08-3004,0,0.111325,"Missing"
E17-3008,P15-1143,1,0.879917,"t interpretations. Alto supports PCFG-style probability models with both maximum likelihood and expectation maximization estimation. Log-linear probability models are also available, and can be trained with maximum likelihood estimation. All of these functions are available through command-line tools, a Java API, and a GUI, seen in Fig. 2 and 3. We have invested considerable effort into making these algorithms efficient enough for practical use. In particular, many algorithms for wRTGs in Alto are implemented in a lazy fashion, i.e. the rules of the wRTG are only calculated by need; see e.g. (Groschwitz et al., 2015; Groschwitz et al., 2016). Obviously, Alto cannot be as efficient for well-established tasks like PCFG parsing 30 Figure 2: An example IRTG with an English and a semantic interpretation (Alto screenshot). Figure 3: A derivation tree with interpreted values (Alto screenshot). between Context-Free and Tree-Adjoining Grammars in Alto is that CFGs use the simple string algebra outlined in Section 2, whereas for TAG we use a special “TAG string algebra” which defines string wrapping operations (Koller and Kuhlmann, 2012). All algorithms mentioned in Section 3 are generic and do not make any assump"
E17-3008,P16-1192,1,0.916005,"upports PCFG-style probability models with both maximum likelihood and expectation maximization estimation. Log-linear probability models are also available, and can be trained with maximum likelihood estimation. All of these functions are available through command-line tools, a Java API, and a GUI, seen in Fig. 2 and 3. We have invested considerable effort into making these algorithms efficient enough for practical use. In particular, many algorithms for wRTGs in Alto are implemented in a lazy fashion, i.e. the rules of the wRTG are only calculated by need; see e.g. (Groschwitz et al., 2015; Groschwitz et al., 2016). Obviously, Alto cannot be as efficient for well-established tasks like PCFG parsing 30 Figure 2: An example IRTG with an English and a semantic interpretation (Alto screenshot). Figure 3: A derivation tree with interpreted values (Alto screenshot). between Context-Free and Tree-Adjoining Grammars in Alto is that CFGs use the simple string algebra outlined in Section 2, whereas for TAG we use a special “TAG string algebra” which defines string wrapping operations (Koller and Kuhlmann, 2012). All algorithms mentioned in Section 3 are generic and do not make any assumptions about what algebras"
E17-3008,W05-1506,0,0.0607905,"ammar which describes a relation between strings and graphs. When we parse an input string w, we compute a parse chart that describes all grammatically correct derivation trees that interpret to this input string. We do this by computing 3 Algorithms in Alto Alto can read IRTG grammars and corpora from files, and implements a number of core algorithms, including: automatic binarization of monolingual and synchronous grammars (Büchse et al., 2013); computation of parse charts for given input objects; computing the best derivation tree; computing the k-best derivation trees, along the lines of (Huang and Chiang, 2005); and decoding the best derivation tree(s) into output interpretations. Alto supports PCFG-style probability models with both maximum likelihood and expectation maximization estimation. Log-linear probability models are also available, and can be trained with maximum likelihood estimation. All of these functions are available through command-line tools, a Java API, and a GUI, seen in Fig. 2 and 3. We have invested considerable effort into making these algorithms efficient enough for practical use. In particular, many algorithms for wRTGs in Alto are implemented in a lazy fashion, i.e. the rule"
E17-3008,W13-2322,0,0.0598191,"Missing"
E17-3008,W11-2902,1,0.46257,"on. For synchronous grammar formalisms, we also want to decode inputs into outputs, and binarizing grammars becomes nontrivial. Implementing these algorithms requires considerable thought and effort for each new grammar formalism, and can lead to faulty or inefficient prototypes. At the same time, there is a clear sense that these algorithms work basically the same across many different grammar formalisms, and change only in specific details. In this demo, we address this situation by introducing Alto, the Algebraic Language Toolkit. Alto is based on Interpreted Regular Tree Grammars (IRTGs; (Koller and Kuhlmann, 2011)), which separate the derivation process (described by probabilistic regular tree grammars) from the interpretation of a derivation tree into a value of the language. In this way, IRTGs can capture a wide variety of monolingual and synchronous grammar 2 An example grammar Let us look at an example to illustrate the Alto workflow. We will work with a synchronous string-to-graph grammar, which Alto’s GUI displays as in Fig. 2. The first and second column describe a weighted regular tree grammar (wRTG, (Comon et al., 2007)), which specifies how to rewrite nonterminal symbols such as S and NP recu"
E17-3008,W12-4616,1,0.830259,"lazy fashion, i.e. the rules of the wRTG are only calculated by need; see e.g. (Groschwitz et al., 2015; Groschwitz et al., 2016). Obviously, Alto cannot be as efficient for well-established tasks like PCFG parsing 30 Figure 2: An example IRTG with an English and a semantic interpretation (Alto screenshot). Figure 3: A derivation tree with interpreted values (Alto screenshot). between Context-Free and Tree-Adjoining Grammars in Alto is that CFGs use the simple string algebra outlined in Section 2, whereas for TAG we use a special “TAG string algebra” which defines string wrapping operations (Koller and Kuhlmann, 2012). All algorithms mentioned in Section 3 are generic and do not make any assumptions about what algebras are being used. As explained above, the only algebra-specific step is to compute decomposition grammars for input objects. as a parser that was implemented and optimized for this specific grammar formalism. Nonetheless, Alto is fast enough for practical use with treebank-scale gramars, and for less mainstream grammar formalisms can be faster than specialized implementations for these formalisms. For instance, Alto is the fastest published parser for Hyperedge Replacement Grammars (Groschwitz"
E17-3008,N06-1022,0,0.112855,"gramars, and for less mainstream grammar formalisms can be faster than specialized implementations for these formalisms. For instance, Alto is the fastest published parser for Hyperedge Replacement Grammars (Groschwitz et al., 2015). Alto contains multiple algorithms for computing the intersection and inverse homomorphism of RTGs, and a user can choose the combination that works best for their particular grammar formalism (Groschwitz et al., 2016). The most recent version adds further performance improvements through the use of a number of pruning techniques, including coarse-to-fine parsing (Charniak et al., 2006). With these, Section 23 of the WSJ corpus can be parsed in a couple of minutes. 4 In order to implement a new algebra, a user of Alto simply derives a class from the abstract base class Algebra, which amounts to specifying the possible values of the algebra (as a Java class) and implementing the operations of the algebra as Java methods. If Alto is also to parse objects from this algebra, the class needs to implement a method for computing decomposition grammars for the algebra’s values. Alto comes with a number of algebras built in, including string algebras for ContextFree and Tree-Adjoinin"
E17-3008,N16-1026,0,0.0264906,"Missing"
E17-3008,P13-1091,0,\N,Missing
E17-3008,P13-1015,1,\N,Missing
E17-3008,J07-2003,0,\N,Missing
gargett-etal-2010-give,stoia-etal-2008-scare,0,\N,Missing
gargett-etal-2010-give,W06-1412,0,\N,Missing
gargett-etal-2010-give,W09-3907,0,\N,Missing
gargett-etal-2010-give,W09-0628,1,\N,Missing
J13-4008,J99-2004,0,0.0217554,"hich creates a great number of new prefix trees: At each prediction step, thousands of prediction trees can potentially be combined with all prefix trees; this is computationally not feasible. Non-incremental parsers, which do not use the unlexicalized prediction trees, have to deal with the much lower level of ambiguity among canonical trees (about 50 trees per word on average if using a lexicon the size of our canonical lexicon). In our parser implementation, we use supertagging to select only the best prediction trees in each step, which reduces the search space considerably. Supertagging (Bangalore and Joshi 1999) is a common approach used in the context of TAG and CCG parsing; the idea is to limit the elementary trees for each word to those that are evaluated highly by some shallow statistical model. We only use supertagging for prediction trees; for canonical trees, we use all (lexicalized) trees that the grammar contains for the word (rare words are replaced by “UNK”). 1041 Computational Linguistics Volume 39, Number 4 Because our parser must run incrementally, the supertagger should not be allowed to have any look-ahead. We found, however, that not having any look-ahead has a detrimental impact on"
J13-4008,A00-1031,0,0.0260399,"he leaf node on the spine slpredict , and the probability of some tree with first fringe fpredict and category of the leaf node on the spine slpredict given a prefix tree with current fringe fp and estimated POS tag of the next word twi+1 . A further simplification is that we represent the current fringes fpredict and fp as an alphabetically ordered set of the categories occurring on it. The reasoning behind this decision is that the order of nodes is less important than the identity of the nodes as possible integration sites. The supertagging model is smoothed with the procedure described by Brants (2000), as it yielded better results than WittenBell smoothing (which suffers from data sparsity in the supertagging task). We use one level of back-off where we estimate P( fpredict , slpredict |fp , ti+1 ) based only on the most likely integration site np instead of the whole fringe fp : max P(fpredict , tpredict |np , twi+1 ) np (12) The reason for backing off to the most probable integration site is that a fringe with more unique categories should not have a lower probability of a particular tree adjoining into it than a fringe containing the same category, but fewer other categories. 5. Treeban"
J13-4008,P00-1058,0,0.0649511,"re 4, the head of the S node is sleeps and the head of the NP node is Peter), but could also be the non-lexical leaf of a prediction tree (the head of the upper VP node in the third prefix tree is the lower VP node). The head of any node on the spine of a canonical elementary tree is always the lexical anchor. 3.6 Probability Model We are now ready to define the probability model for PLTAG. This model allows us to define a probability distribution over the derivations of any given PLTAG grammar. It makes the same independence assumptions as standard models for probabilistic TAG (Resnik 1992b; Chiang 2000): Any two applications of derivation rules are statistically independent events. We deviate from these models, however, with regard to what these events are. Earlier approaches always modeled the probability of substituting or adjoining the lower elementary tree, given the upper elementary tree and the integration site. This is inconsistent with the incremental perspective we take here, which assumes that the prefix tree is given, and we must decide how to integrate an elementary tree for the next word with it. We therefore model the probability of substituting, adjoining, or verifying the ele"
J13-4008,J03-4003,0,0.0323503,"ows us to distinguish high and low attachment. The probability models are now obtained via maximum likelihood estimation from the training data. Many of the substitution and adjunction events are seen rarely or not at all with their full contexts, which indicates the need for smoothing. We use back-off with deleted interpolation, as detailed in Table 1. The weight for each of these contexts is automatically determined by a variant of Witten-Bell smoothing, which calculates a weight for each of the back-off levels for each context (Witten and Bell 1991). We implemented the version described by Collins (2003). For the verification operation, data sparsity for the probability of the tree template τv is less of an issue because the probability of a tree template verifying a prediction tree is conditioned only on the identity of the prediction tree and the trace feature. 6. Evaluation In order to compare the PLTAG parser to other probabilistic parsers, we evaluated parsing accuracy on the Penn Treebank (PTB). We first converted the PTB into a PLTAG 1047 Computational Linguistics Volume 39, Number 4 treebank as described in Section 5. We then trained the parser on Sections 2–21 of the Penn Treebank an"
J13-4008,P04-1015,0,0.358872,"Missing"
J13-4008,W12-4623,1,0.833782,"r to the use of type raising in incremental derivations in CCG (Steedman 2000). For example, the prediction tree in Figure 1d effectively raises the NP in Figure 1a to type (S/(SNP)) so that it can compose with the adverb in Figure 1c. Prediction trees, however, are more powerful in terms of the incremental derivations they support: Some psycholinguistically crucial 1032 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar constructions (such as object relative clauses) are handled easily by PLTAG, but are not incrementally derivable in standard CCG (Demberg 2012). According to Demberg, this problem can be overcome by generalizing the CCG categories involved (in the case of object relative clauses, the category of the relative pronoun needs to be changed). 3.3 Verification Markers are eliminated from a partial derived tree through a new operation called verification. Recall that markers indicate nodes that were predicted during the derivation, without having been introduced by a word that was actually observed so far. The verification operation removes these markers by matching them with the nodes of the canonical elementary tree for a word in the sent"
J13-4008,W08-2304,1,0.394592,"incorrect. Presumably, the human sentence processor uses prediction mechanisms to enable efficient comprehension in real time. The three concepts of incrementality, connectedness, and prediction are fundamentally interrelated: Maintaining connected partial analyses is only nontrivial if the parsing process is incremental, and prediction means that a connected analysis is required also for words the parser has not yet seen. In this article, we exploit the interrelatedness of incrementality, connectedness, and prediction to develop a parsing model for psycholinguistically motivated TAG (PLTAG; Demberg and Keller 2008b). This formalism augments standard tree-adjoining grammar (TAG; Joshi, Levy, and Takahashi 1975) with a predictive lexicon and a verification operation for validating predicted structures. As we show in Section 2, these operations are motivated by psycholinguistic findings. We argue that our PLTAG parser can form the basis for a new model of human sentence processing. We successfully evaluate the predictions of this model against reading time data from an eye-tracking corpus, showing that it provides a better fit with the psycholinguistic data than the standard surprisal model of human sente"
J13-4008,W12-1706,0,0.0167801,"a (Demberg and Keller 2008a). It is important to note, though, that adding verification cost to the baseline LME model increases model fit significantly, which provides some evidence for effectiveness of the verification cost component. 10 The result for Roark structural surprisal differs from that reported by Demberg and Keller (2008a) and Demberg-Winterfors (2010). This can be attributed to the different outlier removal and more conservative treatment of random effects in the present article. 11 Surprisal has subsequently been reported to be a significant predictor of Dundee reading time by Fossum and Levy (2012), who used a context-free grammar induced using the state-split model of Petrov and Klein (2007) in combination with a standard probabilistic Earley parser to compute surprisal estimates. 1056 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar Table 4 Linear mixed effects models of first-pass time for predictors of theoretical interest: Prediction Theory cost, PLTAG surprisal, PLTAG verification cost, Roark lexical surprisal, and Roark structural surprisal, each residualized against low-level predictors (see text for details). Random intercepts of pa"
J13-4008,N01-1021,0,0.605496,"emental parser is to develop a more realistic model of human language processing. A treebank-based evaluation as in the previous section does not directly provide evidence of psycholinguistic validity; however, a parser with good coverage and high parsing accuracy is a prerequisite for an evaluation on eye-tracking corpora, which Keller (2010) argues are the benchmark for models of human sentence processing. In what follows, we report an evaluation study that uses our PLTAG parser to predict human reading times, and compares its performance on this task to a standard model based on surprisal (Hale 2001). Surprisal assumes that processing difficulty is associated with expectations built up by the sentence processor: A word that is unexpected given its preceding context is harder to process. Mathematically, the amount of surprisal at word wi can be formalized as the negative logarithm of the conditional probability of wi given the preceding words in the sentence w1 . . . wi−1 : Surprisalwi = − log P(wi |w1 . . . wi−1 ) (13) P(w1 . . . wi ) P(w1 . . . wi−1 )  = − log P(τpw ...w ) + log = − log τpw 1 i 1 ...wi  τpw P(τpw 1 ...wi−1 ) 1 ...wi−1 Here, P(τpw ...w ) is the probability of the prefix"
J13-4008,J07-3004,0,0.0406482,"Missing"
J13-4008,C92-2066,0,0.739181,"his property is essential for testing psycholinguistic models on realistic data, including eye-tracking corpora. The PLTAG formalism was first proposed by Demberg-Winterfors (2010), who also presents an earlier version of the parsing algorithm, probability model, implementation, and evaluation described in the current article. 3. The PLTAG Formalism We start by introducing the PLTAG formalism, which we will use throughout the article. 3.1 Incremental TAG Parsing Tree Adjoining Grammar (TAG) is a grammar formalism based on combining trees. In what follows we will focus on lexicalized TAG (TAG; Joshi and Schabes 1992), which is the most widely used version of TAG. In this formalism, a TAG lexicon consists of a finite set of elementary trees whose nodes are labeled with nonterminal or terminal symbols. Each elementary tree contains an anchor, a leaf node labeled with a terminal symbol. At most one other leaf—the foot node—may carry a label of the form A∗, where A is a nonterminal symbol. All other leaves are substitution nodes and labeled with symbols of the form A↓. Elementary trees that contain a foot node are called auxiliary trees; those that contain no foot nodes are initial trees. We will generally ca"
J13-4008,kaeshammer-demberg-2012-german,1,0.886542,"Missing"
J13-4008,W04-0302,0,0.0731294,"Missing"
J13-4008,P10-2012,1,0.943603,"psycholinguistically motivated way. We achieve this by exploiting the fact that these three concepts are closely related: In order to guarantee that the syntactic structure of a sentence prefix is fully connected, it may be necessary to build phrases whose lexical anchors (the words that they relate to) have not been encountered yet. In other words, the parser needs to predict upcoming syntactic structure in order to ensure connectedness. This prediction scheme is complemented by an explicit verification mechanism in our approach. Furthermore, unlike most existing psycholinguistic models (see Keller 2010 for an overview), our model achieves broad coverage and acceptable parsing performance on a standard test corpus. This property is essential for testing psycholinguistic models on realistic data, including eye-tracking corpora. The PLTAG formalism was first proposed by Demberg-Winterfors (2010), who also presents an earlier version of the parsing algorithm, probability model, implementation, and evaluation described in the current article. 3. The PLTAG Formalism We start by introducing the PLTAG formalism, which we will use throughout the article. 3.1 Incremental TAG Parsing Tree Adjoining Gr"
J13-4008,P10-1021,1,0.378233,"Missing"
J13-4008,W04-0308,0,0.157879,"Missing"
J13-4008,N07-1051,0,0.0145811,"the baseline LME model increases model fit significantly, which provides some evidence for effectiveness of the verification cost component. 10 The result for Roark structural surprisal differs from that reported by Demberg and Keller (2008a) and Demberg-Winterfors (2010). This can be attributed to the different outlier removal and more conservative treatment of random effects in the present article. 11 Surprisal has subsequently been reported to be a significant predictor of Dundee reading time by Fossum and Levy (2012), who used a context-free grammar induced using the state-split model of Petrov and Klein (2007) in combination with a standard probabilistic Earley parser to compute surprisal estimates. 1056 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar Table 4 Linear mixed effects models of first-pass time for predictors of theoretical interest: Prediction Theory cost, PLTAG surprisal, PLTAG verification cost, Roark lexical surprisal, and Roark structural surprisal, each residualized against low-level predictors (see text for details). Random intercepts of participant and random slopes under participants for the predictors of interest were also included"
J13-4008,C92-1032,0,0.693292,"plicit prediction and verification mechanism (WCDG includes prediction, but not verification), which means that they cannot be used to model psycholinguistic results that involve verification cost.1 A simple form of prediction can be achieved in a chart parser (incomplete edges in the chart can be seen as predictive), but in order to maintain psycholinguistic plausibility, an arc-eager left-corner parsing strategy needs to be used. Other parsing strategies fail to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson, Dixon, and Lamping 1991; Resnik 1992a). This is an argument against using a top–down parser such as Roark’s for psycholinguistic modeling. Furthermore, it is important to emphasize that a full model of human parsing needs to not only model prediction, but also account for processing difficulty associated with the verification of predictions (we will return to this point in Section 7). None of the existing incremental parsing models includes an explicit verification component. 1 As Demberg and Keller (2009) show, some psycholinguistic results can be accounted for by a model without verification, such as the either . . . or findin"
J13-4008,C92-2065,0,0.795513,"plicit prediction and verification mechanism (WCDG includes prediction, but not verification), which means that they cannot be used to model psycholinguistic results that involve verification cost.1 A simple form of prediction can be achieved in a chart parser (incomplete edges in the chart can be seen as predictive), but in order to maintain psycholinguistic plausibility, an arc-eager left-corner parsing strategy needs to be used. Other parsing strategies fail to predict human processing difficulty that arises in certain cases, such as for center embedding (Thompson, Dixon, and Lamping 1991; Resnik 1992a). This is an argument against using a top–down parser such as Roark’s for psycholinguistic modeling. Furthermore, it is important to emphasize that a full model of human parsing needs to not only model prediction, but also account for processing difficulty associated with the verification of predictions (we will return to this point in Section 7). None of the existing incremental parsing models includes an explicit verification component. 1 As Demberg and Keller (2009) show, some psycholinguistic results can be accounted for by a model without verification, such as the either . . . or findin"
J13-4008,J01-2004,0,0.0519268,"oach, however, allows multiple unconnected subtrees for a sentence prefix and uses a look-ahead of two words, that is, it does not build connected structures. An example of a TAG parser that is both incremental and builds connected structures is the work of Kato, Matsubara, and Inagaki (2004). This comes at the price of strong simplifying assumptions with respect to the TAG formalism, such as not distinguishing modifiers and arguments. (We will return to a discussion of other TAG parsers in Section 6.1.) An example of an incremental parser based on context-free grammars is the one proposed by Roark (2001). That parser uses a top–down algorithm to build fully connected structures; it is also able to compute probabilities for sentence prefixes, which makes it attractive for psycholinguistic modeling, where prefix probabilities are often used to predict human processing difficulty (see Section 7 for details). The Roark parser has been shown to successfully model psycholinguistic data from eye-tracking corpora (Demberg and Keller 2008a; Frank 2009) and other reading time data (Roark et al. 2009). It therefore is a good candidate for a broad-coverage model of human parsing, and 1028 Demberg, Keller"
J13-4008,D09-1034,0,0.0796811,"rs in Section 6.1.) An example of an incremental parser based on context-free grammars is the one proposed by Roark (2001). That parser uses a top–down algorithm to build fully connected structures; it is also able to compute probabilities for sentence prefixes, which makes it attractive for psycholinguistic modeling, where prefix probabilities are often used to predict human processing difficulty (see Section 7 for details). The Roark parser has been shown to successfully model psycholinguistic data from eye-tracking corpora (Demberg and Keller 2008a; Frank 2009) and other reading time data (Roark et al. 2009). It therefore is a good candidate for a broad-coverage model of human parsing, and 1028 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar will serve as a standard of comparison for the model proposed in the current article in Section 7. The Roark parser has been extended with discriminative training (Collins and Roark 2004), resulting in a boost in parsing accuracy. Prefix probabilities cannot be computed straightforwardly in a discriminative framework, however, making this approach less interesting from a psycholinguistic modeling point of view. W"
J13-4008,J10-1001,0,0.0422888,"Missing"
J13-4008,H05-1102,0,0.364427,"ent work has provided evidence for connectedness in a range of other phenomena, including sluicing and ellipsis (Aoshima, Yoshida, and Phillips 2009; Yoshida, Walsh-Dickey, and Sturt 2013). 2.2 Incremental Parsing Models In the previous section, we identified incrementality, connectedness, and prediction as key desiderata for computational models of human parsing. In what follows, we will review work on parsing in computational linguistics in the light of these desiderata. Incremental parsers for a range of grammatical formalisms have been proposed in the literature. An example is the work of Shen and Joshi (2005), who propose an efficient incremental parser for a variant of TAG, spinal TAG. This approach, however, allows multiple unconnected subtrees for a sentence prefix and uses a look-ahead of two words, that is, it does not build connected structures. An example of a TAG parser that is both incremental and builds connected structures is the work of Kato, Matsubara, and Inagaki (2004). This comes at the price of strong simplifying assumptions with respect to the TAG formalism, such as not distinguishing modifiers and arguments. (We will return to a discussion of other TAG parsers in Section 6.1.) A"
J13-4008,P91-1012,0,0.572012,"Missing"
J13-4008,P07-1031,0,0.0242415,"r, we first retrieve the elementary trees for the upcoming lexeme. If the word occurred with more than one POS tag, we choose the POS tag with highest conditional probability given the previous two POS tags. 1042 Demberg, Keller, and Koller Parsing with Psycholinguistically Motivated Tree-Adjoining Grammar of treebank conversion and lexicon induction here; the reader is referred to DembergWinterfors (2010) for full details. Our PLTAG lexicon (both canonical trees and prediction trees) is derived from the Wall Street Journal section of the Penn Treebank, complemented by noun phrase annotation (Vadas and Curran 2007), and Propbank (Palmer, Gildea, and Kingsbury 2003), as well as a slightly modified version of the head percolation table of Magerman (1994). These additional resources are used to determine the elementary trees for a TAG lexicon, following the procedures proposed by Xia, Palmer, and Joshi (2000). This involves first adding noun phrase annotation to the Penn Treebank, and then determining heads with the head percolation table, augmented with more detailed heuristics for noun phrases.4 As a next step, information from Propbank is used to establish argument and modifier status and to determine w"
J13-4008,C88-2147,0,0.119973,"e as only having an upper half, which again makes a whole node. We assume that lexical leaves only have an upper half, too; this makes no difference, as no substitution or adjunction can be performed on those nodes anyway. The process is illustrated in Figure 3, which shows the recombination of node halves from different elementary trees in the adjunction step of Figure 2: Black node halves come from the elementary tree for sleeps, gray node halves from Peter, and white ones from often. The idea of distinguishing upper and lower node halves that are pushed apart by adjunction comes from FTAG (Vijay-Shanker and Joshi 1988), which equips each node half with a separate feature structure; at the end of the derivation process, the upper and lower feature structures of each node are unified with each other. Node halves will also play a crucial role in PLTAG. 3.2 Prediction Trees We have argued earlier that a psycholinguistic model of sentence processing should be incremental. In the context of TAG and related formalisms, this means that a derivation Figure 3 Fine structure of adjunction. The semicircles represent node halves; all node halves from the same elementary tree are drawn in the same color. 1031 Computation"
J13-4008,P10-1121,0,0.0482399,"Missing"
J13-4008,W00-1307,0,0.27522,"Missing"
J13-4008,N01-1023,0,\N,Missing
J13-4008,P95-1037,0,\N,Missing
J13-4008,J05-1004,0,\N,Missing
J15-2002,E03-1036,0,0.74025,"1994), one may restrict backward-crossed composition to instances where X and Y are both verbal categories—that is, functions into the category S of sentences (cf. Steedman 2000, Section 4.2.2). With this restriction the unwanted derivation in Figure 1 can be blocked, and a powerful shot by Rivaldo is still accepted as grammatical. Other syntactic phenomena require other grammar-specific restrictions, including the complete ban of certain combinatory rules (cf. Steedman 2000, Section 4.2.1). Over the past 20 years, CCG has evolved to put more emphasis on supporting fully lexicalized grammars (Baldridge and Kruijff 2003; Steedman and Baldridge 2011), in which as much grammatical information as possible is pushed into the lexicon. This follows the tradition of other frameworks such as Lexicalized Tree-Adjoining Grammar (LTAG) and Head-Driven Phrase Structure Grammar (HPSG). Grammar-specific rule restrictions are not connected to individual lexicon entries, and are therefore avoided. Instead, recent versions of CCG have introduced a new, lexicalized control mechanism in the form of modalities or slash types. The basic idea here is that combinatory rules only apply if the slashes in their input categories have"
J15-2002,T75-2001,0,0.4864,"Missing"
J15-2002,P96-1011,0,0.320562,"Missing"
J15-2002,P10-1055,1,0.827264,"Missing"
J15-2002,J93-4002,0,0.599437,"Missing"
J15-2002,C86-1048,0,0.777914,"Missing"
J15-2002,P88-1034,0,0.797741,"Missing"
K19-2006,P18-1035,0,0.0725274,"Missing"
K19-2006,S19-2002,0,0.328988,"Missing"
K19-2006,P13-1023,0,0.213421,"and obtained state-of-the-art results across all of these graphbanks (Lindemann et al., 2019); we will call this system the ACL-19 parser throughout this paper. Earlier semantic parsers were only available for one or two families of closely related graphbanks; our system was the first to parse accurately across a range of different graphbanks. We took this parser as the starting point of our MRP submission; we explain the minor tweaks that were needed for the MRP flavors of DM, PSD, EDS, and AMR in Section 3. The one MRP graphbank which was not directly supported by the ACL-19 parser is UCCA (Abend and Rappoport, 2013). We thus implemented heuristics for converting UCCA annotations into AM dependency graphs. Certain design decisions in UCCA made this more difficult than for the other graphbanks; we worked around some of these in preprocessing. We describe the details in Section 4. We present detailed evaluation results in Section 5. We also describe a few post-deadline improvements, which bring our parser up to an MRP f-score of 71.6 on AMR and 70.1 on UCCA. Abstract We describe the Saarland University submission to the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conferen"
K19-2006,Q16-1023,0,0.0551947,"ded as the dependency tree in Fig. 2b (Groschwitz et al., 2018). We can now perform AM dependency parsing by training models for the following two tasks: (i) a supertagger to predict the as-graphs for the individual word tokens (such as Gwant ) and (ii) a dependency parser to predict the dependency tree. Together, these two components predict an AM dependency tree, which then evaluates to a graph in the AM algebra as explained above. Both of these tasks can be performed by neural models with high accuracy. We train a BiLSTM to predict a supertag for each token and use the dependency parser of Kiperwasser and Goldberg (2016) to predict dependency trees. To ensure that we obtain well-typed AM dependency trees, we use the fixed-tree decoder algorithm of Groschwitz et al. (2018). 2.3 Decomposition To train the neural supertagging and dependency models, we need AM dependency trees for the training set. However, the available graphbanks contain only sentences with their graph annotations. Thus we have to decompose the graphs in each graphbank into the corresponding AM dependency trees. We do this with handwritten heuristics, which we 67 modM The tall Appo AppS gira↵e wants giraffe want-01 to eat A PP S A PP O Gwant ?"
K19-2006,P17-1112,0,0.0737774,"on to the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference on Computational Natural Language Learning (CoNLL). 1 Introduction In this paper, we describe the semantic parser submitted by Saarland University to the MRP shared task (Oepen et al., 2019)1 . This task consists in learning to accurately map English sentences to graph-based meaning representations across five different graphbanks. There has been substantial previous work on graph parsing for each of the graphbanks in MRP, including DM and PSD (Peng et al., 2017; Dozat and Manning, 2018), EDS (Buys and Blunsom, 2017; Chen et al., 2018), AMR (Flanigan et al., 2014; Buys and Blunsom, 2017; Lyu and Titov; Zhang et al., 2019), and UCCA (Hershcovich et al., 2017, 2018; Jiang et al., 2019). One advantage of our parser is that it works accurately across all graphbanks at the same time. Instead of learning to map directly from sentences to graphs, our parser learns to map sentences to AM dependency trees. Each AM dependency tree consists of a graph for the lexical meaning of each token in the sentence, along with a dependency tree that specifies the words that fill each semantic role of a given predicate. An AM"
K19-2006,P19-1450,1,0.916012,"i, Meaghan Fowlie∗ , Jonas Groschwitz, Alexander Koller, Matthias Lindemann, Mario Mina, Pia Weißenhorn Department of Language Science and Technology, Saarland University ∗ Department of Linguistics, Utrecht University {donatelli|jonasg|koller|mlinde|mariom|piaw}@coli.uni-saarland.de m.fowlie@uu.nl In earlier work, we showed how to accurately predict AM dependency trees for AMR using a neural dependency parser and supertagger (Groschwitz et al., 2018). We extended this parser from AMR to the DM, PAS, PSD, and EDS graphbanks and obtained state-of-the-art results across all of these graphbanks (Lindemann et al., 2019); we will call this system the ACL-19 parser throughout this paper. Earlier semantic parsers were only available for one or two families of closely related graphbanks; our system was the first to parse accurately across a range of different graphbanks. We took this parser as the starting point of our MRP submission; we explain the minor tweaks that were needed for the MRP flavors of DM, PSD, EDS, and AMR in Section 3. The one MRP graphbank which was not directly supported by the ACL-19 parser is UCCA (Abend and Rappoport, 2013). We thus implemented heuristics for converting UCCA annotations in"
K19-2006,P18-1038,0,0.0474942,"Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference on Computational Natural Language Learning (CoNLL). 1 Introduction In this paper, we describe the semantic parser submitted by Saarland University to the MRP shared task (Oepen et al., 2019)1 . This task consists in learning to accurately map English sentences to graph-based meaning representations across five different graphbanks. There has been substantial previous work on graph parsing for each of the graphbanks in MRP, including DM and PSD (Peng et al., 2017; Dozat and Manning, 2018), EDS (Buys and Blunsom, 2017; Chen et al., 2018), AMR (Flanigan et al., 2014; Buys and Blunsom, 2017; Lyu and Titov; Zhang et al., 2019), and UCCA (Hershcovich et al., 2017, 2018; Jiang et al., 2019). One advantage of our parser is that it works accurately across all graphbanks at the same time. Instead of learning to map directly from sentences to graphs, our parser learns to map sentences to AM dependency trees. Each AM dependency tree consists of a graph for the lexical meaning of each token in the sentence, along with a dependency tree that specifies the words that fill each semantic role of a given predicate. An AM dependency tree can"
K19-2006,N19-1423,0,0.078259,"Missing"
K19-2006,K19-2001,0,0.104385,"Missing"
K19-2006,P18-2077,0,0.0340175,"he Saarland University submission to the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference on Computational Natural Language Learning (CoNLL). 1 Introduction In this paper, we describe the semantic parser submitted by Saarland University to the MRP shared task (Oepen et al., 2019)1 . This task consists in learning to accurately map English sentences to graph-based meaning representations across five different graphbanks. There has been substantial previous work on graph parsing for each of the graphbanks in MRP, including DM and PSD (Peng et al., 2017; Dozat and Manning, 2018), EDS (Buys and Blunsom, 2017; Chen et al., 2018), AMR (Flanigan et al., 2014; Buys and Blunsom, 2017; Lyu and Titov; Zhang et al., 2019), and UCCA (Hershcovich et al., 2017, 2018; Jiang et al., 2019). One advantage of our parser is that it works accurately across all graphbanks at the same time. Instead of learning to map directly from sentences to graphs, our parser learns to map sentences to AM dependency trees. Each AM dependency tree consists of a graph for the lexical meaning of each token in the sentence, along with a dependency tree that specifies the words that fill each semantic role"
K19-2006,W11-2927,0,0.477653,"Missing"
K19-2006,P17-1186,0,0.0376801,"tract We describe the Saarland University submission to the shared task on Cross-Framework Meaning Representation Parsing (MRP) at the 2019 Conference on Computational Natural Language Learning (CoNLL). 1 Introduction In this paper, we describe the semantic parser submitted by Saarland University to the MRP shared task (Oepen et al., 2019)1 . This task consists in learning to accurately map English sentences to graph-based meaning representations across five different graphbanks. There has been substantial previous work on graph parsing for each of the graphbanks in MRP, including DM and PSD (Peng et al., 2017; Dozat and Manning, 2018), EDS (Buys and Blunsom, 2017; Chen et al., 2018), AMR (Flanigan et al., 2014; Buys and Blunsom, 2017; Lyu and Titov; Zhang et al., 2019), and UCCA (Hershcovich et al., 2017, 2018; Jiang et al., 2019). One advantage of our parser is that it works accurately across all graphbanks at the same time. Instead of learning to map directly from sentences to graphs, our parser learns to map sentences to AM dependency trees. Each AM dependency tree consists of a graph for the lexical meaning of each token in the sentence, along with a dependency tree that specifies the words th"
K19-2006,P14-1134,0,0.0944776,"Representation Parsing (MRP) at the 2019 Conference on Computational Natural Language Learning (CoNLL). 1 Introduction In this paper, we describe the semantic parser submitted by Saarland University to the MRP shared task (Oepen et al., 2019)1 . This task consists in learning to accurately map English sentences to graph-based meaning representations across five different graphbanks. There has been substantial previous work on graph parsing for each of the graphbanks in MRP, including DM and PSD (Peng et al., 2017; Dozat and Manning, 2018), EDS (Buys and Blunsom, 2017; Chen et al., 2018), AMR (Flanigan et al., 2014; Buys and Blunsom, 2017; Lyu and Titov; Zhang et al., 2019), and UCCA (Hershcovich et al., 2017, 2018; Jiang et al., 2019). One advantage of our parser is that it works accurately across all graphbanks at the same time. Instead of learning to map directly from sentences to graphs, our parser learns to map sentences to AM dependency trees. Each AM dependency tree consists of a graph for the lexical meaning of each token in the sentence, along with a dependency tree that specifies the words that fill each semantic role of a given predicate. An AM dependency tree can be deterministically evaluat"
K19-2006,N18-1202,0,0.132926,"Missing"
K19-2006,W17-6810,1,0.823441,"Titov; Zhang et al., 2019), and UCCA (Hershcovich et al., 2017, 2018; Jiang et al., 2019). One advantage of our parser is that it works accurately across all graphbanks at the same time. Instead of learning to map directly from sentences to graphs, our parser learns to map sentences to AM dependency trees. Each AM dependency tree consists of a graph for the lexical meaning of each token in the sentence, along with a dependency tree that specifies the words that fill each semantic role of a given predicate. An AM dependency tree can be deterministically evaluated to a graph via the AM Algebra (Groschwitz et al., 2017). Thus, the parser compositionally maps sentences to graphs, with the AM dependency trees describing the compositional structure of the meaning representation. We will sketch the background on AM dependency trees in Section 2. 1 2 AM dependency parsing We start by describing the ACL-19 parser (Lindemann et al., 2019). This parser is trained to map sentences into AM dependency trees, which are then deterministically evaluated to graphs in the AM algebra. 2.1 AM Algebra The Apply-Modify Algebra (AM algebra; Groschwitz et al. (2017)) builds graphs from graph http://mrp.nlpl.eu 66 Proceedings of t"
K19-2006,W09-1119,0,0.115687,"Missing"
K19-2006,P18-1170,1,0.936881,"Geat ), the head is still Gwant . We can track the heads through the term, as indicated by the colors in the example term. This allows us to read terms over the AM algebra as AM dependency trees in the following manner. Each operation between two graphs is encoded as a dependency edge from the head to the argument (or modifier respectively), and the edge is labeled with the relevant operation. By aligning the graph fragments to the words in the sentence, we get a dependency tree over the sentence. As a result, the term in Fig. 2a can be unambiguously encoded as the dependency tree in Fig. 2b (Groschwitz et al., 2018). We can now perform AM dependency parsing by training models for the following two tasks: (i) a supertagger to predict the as-graphs for the individual word tokens (such as Gwant ) and (ii) a dependency parser to predict the dependency tree. Together, these two components predict an AM dependency tree, which then evaluates to a graph in the AM algebra as explained above. Both of these tasks can be performed by neural models with high accuracy. We train a BiLSTM to predict a supertag for each token and use the dependency parser of Kiperwasser and Goldberg (2016) to predict dependency trees. To"
K19-2006,P17-1104,0,0.0904471,"g (CoNLL). 1 Introduction In this paper, we describe the semantic parser submitted by Saarland University to the MRP shared task (Oepen et al., 2019)1 . This task consists in learning to accurately map English sentences to graph-based meaning representations across five different graphbanks. There has been substantial previous work on graph parsing for each of the graphbanks in MRP, including DM and PSD (Peng et al., 2017; Dozat and Manning, 2018), EDS (Buys and Blunsom, 2017; Chen et al., 2018), AMR (Flanigan et al., 2014; Buys and Blunsom, 2017; Lyu and Titov; Zhang et al., 2019), and UCCA (Hershcovich et al., 2017, 2018; Jiang et al., 2019). One advantage of our parser is that it works accurately across all graphbanks at the same time. Instead of learning to map directly from sentences to graphs, our parser learns to map sentences to AM dependency trees. Each AM dependency tree consists of a graph for the lexical meaning of each token in the sentence, along with a dependency tree that specifies the words that fill each semantic role of a given predicate. An AM dependency tree can be deterministically evaluated to a graph via the AM Algebra (Groschwitz et al., 2017). Thus, the parser compositionally map"
P00-1047,P98-1058,1,0.793023,"s. Their general satisfiability problem is known to be NP-complete. Here we identify the natural fragment of normal dominance constraints and show that its satisfiability problem is in deterministic polynomial time. 1 Introduction Dominance constraints are used as partial descriptions of trees in problems throughout computational linguistics. They have been applied to incremental parsing (Marcus et al., 1983), grammar formalisms (VijayShanker, 1992; Rambow et al., 1995; Duchier and Thater, 1999; Perrier, 2000), discourse (Gardent and Webber, 1998), and scope underspecification (Muskens, 1995; Egg et al., 1998). Logical properties of dominance constraints have been studied e.g. in (Backofen et al., 1995), and computational properties have been addressed in (Rogers and Vijay-Shanker, 1994; Duchier and Gardent, 1999). Here, the two most important operations are satisfiability testing – does the constraint describe a tree? – and enumerating solutions, i.e. the described trees. Unfortunately, even the satisfiability problem has been shown to be NPcomplete (Koller et al., 1998). This has shed doubt on their practical usefulness. In this paper, we define normal dominance constraints, a natural fragment of"
P00-1047,W98-0113,0,0.0136248,"l descriptions of trees that are widely used in computational linguistics. Their general satisfiability problem is known to be NP-complete. Here we identify the natural fragment of normal dominance constraints and show that its satisfiability problem is in deterministic polynomial time. 1 Introduction Dominance constraints are used as partial descriptions of trees in problems throughout computational linguistics. They have been applied to incremental parsing (Marcus et al., 1983), grammar formalisms (VijayShanker, 1992; Rambow et al., 1995; Duchier and Thater, 1999; Perrier, 2000), discourse (Gardent and Webber, 1998), and scope underspecification (Muskens, 1995; Egg et al., 1998). Logical properties of dominance constraints have been studied e.g. in (Backofen et al., 1995), and computational properties have been addressed in (Rogers and Vijay-Shanker, 1994; Duchier and Gardent, 1999). Here, the two most important operations are satisfiability testing – does the constraint describe a tree? – and enumerating solutions, i.e. the described trees. Unfortunately, even the satisfiability problem has been shown to be NPcomplete (Koller et al., 1998). This has shed doubt on their practical usefulness. In this pape"
P00-1047,J87-1005,0,0.0698996,"this algorithm to enumerate solutions efficiently. An example for an application of normal dominance constraints is scope underspecification: Constraints as in Fig. 1 can serve as underspecified descriptions of the semantic readings of sentences such as (1), considered as the structural trees of the first-order representations. The dotted lines signify dominance relations, which require the upper node to be an ancestor of the lower one in any tree that fits the description. (1) Some representative of every department in all companies saw a sample of each product. The sentence has 42 readings (Hobbs and Shieber, 1987), and it is easy to imagine how the number of readings grows exponentially (or worse) in the length of the sentence. Efficient enumeration of readings from the description is a longstanding problem in scope underspecification. Our polynomial algorithm solves this problem. Moreover, the investigation of graph problems that are closely related to normal constraints allows us to prove that many other underspecification formalisms – e.g. Minimal Recursion Semantics (Copestake et al., 1997) and Hole Semantics (Bos, 1996) – have NP-hard satisfiability problems. Our algorithm can still be used as a p"
P00-1047,P83-1020,0,0.780419,"niversity of the Saarland / ∗ Max-Planck-Institute for Computer Science Saarbr¨ ucken, Germany Abstract Dominance constraints are logical descriptions of trees that are widely used in computational linguistics. Their general satisfiability problem is known to be NP-complete. Here we identify the natural fragment of normal dominance constraints and show that its satisfiability problem is in deterministic polynomial time. 1 Introduction Dominance constraints are used as partial descriptions of trees in problems throughout computational linguistics. They have been applied to incremental parsing (Marcus et al., 1983), grammar formalisms (VijayShanker, 1992; Rambow et al., 1995; Duchier and Thater, 1999; Perrier, 2000), discourse (Gardent and Webber, 1998), and scope underspecification (Muskens, 1995; Egg et al., 1998). Logical properties of dominance constraints have been studied e.g. in (Backofen et al., 1995), and computational properties have been addressed in (Rogers and Vijay-Shanker, 1994; Duchier and Gardent, 1999). Here, the two most important operations are satisfiability testing – does the constraint describe a tree? – and enumerating solutions, i.e. the described trees. Unfortunately, even the"
P00-1047,P95-1021,0,0.153723,"r Science Saarbr¨ ucken, Germany Abstract Dominance constraints are logical descriptions of trees that are widely used in computational linguistics. Their general satisfiability problem is known to be NP-complete. Here we identify the natural fragment of normal dominance constraints and show that its satisfiability problem is in deterministic polynomial time. 1 Introduction Dominance constraints are used as partial descriptions of trees in problems throughout computational linguistics. They have been applied to incremental parsing (Marcus et al., 1983), grammar formalisms (VijayShanker, 1992; Rambow et al., 1995; Duchier and Thater, 1999; Perrier, 2000), discourse (Gardent and Webber, 1998), and scope underspecification (Muskens, 1995; Egg et al., 1998). Logical properties of dominance constraints have been studied e.g. in (Backofen et al., 1995), and computational properties have been addressed in (Rogers and Vijay-Shanker, 1994; Duchier and Gardent, 1999). Here, the two most important operations are satisfiability testing – does the constraint describe a tree? – and enumerating solutions, i.e. the described trees. Unfortunately, even the satisfiability problem has been shown to be NPcomplete (Kolle"
P00-1047,J92-4004,0,0.0608622,"Missing"
P00-1047,P99-1038,0,0.0125619,"nd integrated it into an algorithm that enumerates all irredundant solved forms in time O(N n4 log n), where N is the number of irredundant solved forms. 5 A constructive solution is one where every node in the model is the image of a variable for which a labeling literal is in the constraint. Informally, this means that the solution only contains “material” “mentioned” in the constraint. This eliminates any doubts about the computational practicability of dominance constraints which were raised by the NPcompleteness result for the general language (Koller et al., 1998) and expressed e.g. in (Willis and Manandhar, 1999). First experiments confirm the efficiency of the new algorithm – it is superior to the NP algorithms especially on larger constraints. On the other hand, we have argued that the problem of finding constructive solutions even of a normal dominance constraint is NPcomplete. This result carries over to other underspecification formalisms, such as Hole Semantics and MRS. In practice, however, it seems that the enumeration algorithm presented here can be adapted to those problems. Acknowledgments. We would like to thank Ernst Althaus, Denys Duchier, Gert Smolka, Sven Thiel, all members of the SFB"
P01-1011,C00-1067,1,0.783806,"ant. Existing underspecification approaches (Reyle, 1993; van Deemter and Peters, 1996; Pinkal, 1996; Bos, 1996) provide a partial solution to this problem. They delay the enumeration of the readings and represent them all at once in a single, compact description. An underspecification formalism that is particularly well suited for describing higher-order formulas is the Constraint Language for Lambda Structures, CLLS (Egg et al., 2001; Erk et al., 2001). CLLS descriptions can be derived compositionally and have been used to deal with a rich class of linguistic phenomena (Koller et al., 2000; Koller and Niehren, 2000). They are based on dominance constraints (Marcus et al., 1983; Rambow et al., 1995) and extend them with parallelism (Erk and Niehren, 2000) and binding constraints. However, lifting -reduction to an operation on underspecified descriptions is not trivial, and to our knowledge it is not known how this can be done. Such an operation – which we will call underspecified -reduction – would essentially reduce all described formulas at once by deriving a description of the reduced formulas. In this paper, we show how underspecified -reductions can be performed in the framework of CLLS. Our approach"
P01-1011,P83-1020,0,0.196211,"r and Peters, 1996; Pinkal, 1996; Bos, 1996) provide a partial solution to this problem. They delay the enumeration of the readings and represent them all at once in a single, compact description. An underspecification formalism that is particularly well suited for describing higher-order formulas is the Constraint Language for Lambda Structures, CLLS (Egg et al., 2001; Erk et al., 2001). CLLS descriptions can be derived compositionally and have been used to deal with a rich class of linguistic phenomena (Koller et al., 2000; Koller and Niehren, 2000). They are based on dominance constraints (Marcus et al., 1983; Rambow et al., 1995) and extend them with parallelism (Erk and Niehren, 2000) and binding constraints. However, lifting -reduction to an operation on underspecified descriptions is not trivial, and to our knowledge it is not known how this can be done. Such an operation – which we will call underspecified -reduction – would essentially reduce all described formulas at once by deriving a description of the reduced formulas. In this paper, we show how underspecified -reductions can be performed in the framework of CLLS. Our approach extends the work presented in (Bodirsky et al., 2001), which"
P01-1011,P95-1021,0,0.0247362,"inkal, 1996; Bos, 1996) provide a partial solution to this problem. They delay the enumeration of the readings and represent them all at once in a single, compact description. An underspecification formalism that is particularly well suited for describing higher-order formulas is the Constraint Language for Lambda Structures, CLLS (Egg et al., 2001; Erk et al., 2001). CLLS descriptions can be derived compositionally and have been used to deal with a rich class of linguistic phenomena (Koller et al., 2000; Koller and Niehren, 2000). They are based on dominance constraints (Marcus et al., 1983; Rambow et al., 1995) and extend them with parallelism (Erk and Niehren, 2000) and binding constraints. However, lifting -reduction to an operation on underspecified descriptions is not trivial, and to our knowledge it is not known how this can be done. Such an operation – which we will call underspecified -reduction – would essentially reduce all described formulas at once by deriving a description of the reduced formulas. In this paper, we show how underspecified -reductions can be performed in the framework of CLLS. Our approach extends the work presented in (Bodirsky et al., 2001), which defines -reduction con"
P02-1003,W00-2004,0,0.0634366,"Missing"
P02-1003,C92-2092,0,0.490219,"ice despite the high worstcase complexity. We show how to convert TAG generation problems into dependency parsing problems, which is useful because optimizations in recent dependency parsers based on constraint programming tackle exactly the combinatorics that make generation hard. Indeed, initial experiments display promising runtimes. 1 Introduction Existing algorithms for realization from a flat input semantics all have runtimes which are exponential in the worst case. Several different approaches to improving the runtime in practice have been suggested in the literature – e.g. heuristics (Brew, 1992) and factorizations into smaller exponential subproblems (Kay, 1996; Carroll et al., 1999). While these solutions achieve some measure of success in making realization efficient, the contrast in efficiency to parsing is striking both in theory and in practice. The problematic runtimes of generation algorithms are explained by the fact that realization is an NP-complete problem even using just context-free grammars, as Brew (1992) showed in the context of shake-and-bake generation. The first contribution of our paper is a proof of a stronger NP-completeness result: If we allow semantic indices"
P02-1003,P01-1024,0,0.308986,"the grammar, realization is NP-complete even if we fix a single grammar. Our alternative proof shows clearly that the combinatorics in generation come from essentially the same sources as in parsing for free word order languages. It has been noted in the literature that this problem, too, becomes NP-complete very easily (Barton et al., 1987). The main point of this paper is to show how to encode generation with a variant of tree-adjoining grammars (TAG) as a parsing problem with dependency grammars (DG). The particular variant of DG we use, Topological Dependency Grammar (TDG) (Duchier, 2002; Duchier and Debusmann, 2001), was developed specifically with efficient parsing for free word order languages in mind. The mere existence of this encoding proves TDG’s parsing problem NP-complete as well, a result which has been conjectured but never formally shown so far. But it turns out that the complexities that arise in generation problems in practice seem to be precisely of the sort that the TDG parser can handle well. Initial experiments with generating from the XTAG grammar (XTAG Research Group, 2001) suggest that our generation system is competitive with state-of-theart chart generators, and indeed seems to run"
P02-1003,P01-1028,0,0.136144,"lution, which multiplies out to 312 = 13 · 4! different realizations. (The 13 basic realizations correspond to different syntactic frames for the main verb in the XTAG grammar, e.g. for topicalized or passive constructions.) 5.4 More Complex Semantics So far, we have only considered TAG grammars in which each elementary tree is assigned a semantics that contains precisely one atom. However, there are cases where an elementary tree either has an empty semantics, or a semantics that contains multiple atoms. The first case can be avoided by exploiting TAG’s extended domain of locality, see e.g. (Gardent and Thater, 2001). The simplest possible way for dealing with the second case is to preprocess the input into several 1 A newer version of Carroll et al.’s system generates (1) in 420 milliseconds (Copestake, p.c.). Our times were measured on a 700 MHz Pentium-III PC. different parsing problems. In a first step, we collect all possible instantiations of LTAG lexical entries matching subsets of the semantics. Then we construct all partitions of the input semantics in which each block in the partition is covered by a lexical entry, and build a parsing problem in which each block is one symbol in the input to the"
P02-1003,P96-1027,0,0.676608,"generation problems into dependency parsing problems, which is useful because optimizations in recent dependency parsers based on constraint programming tackle exactly the combinatorics that make generation hard. Indeed, initial experiments display promising runtimes. 1 Introduction Existing algorithms for realization from a flat input semantics all have runtimes which are exponential in the worst case. Several different approaches to improving the runtime in practice have been suggested in the literature – e.g. heuristics (Brew, 1992) and factorizations into smaller exponential subproblems (Kay, 1996; Carroll et al., 1999). While these solutions achieve some measure of success in making realization efficient, the contrast in efficiency to parsing is striking both in theory and in practice. The problematic runtimes of generation algorithms are explained by the fact that realization is an NP-complete problem even using just context-free grammars, as Brew (1992) showed in the context of shake-and-bake generation. The first contribution of our paper is a proof of a stronger NP-completeness result: If we allow semantic indices in the grammar, realization is NP-complete even if we fix a single"
P02-1003,J94-1004,0,0.0339052,"articular restriction of our encoding and ways of overcoming it. 5.1 The Encoding Let G be a grammar as described in Section 2; i.e. lexical entries are of the form (ϕ, T ), where ϕ is a flat semantics and T is a TAG elementary tree whose nodes are decorated with semantic indices. We make the following simplifying assumptions. First, we assume that the nodes of the elementary trees of G are not labelled with feature structures. Next, we assume that whenever we can adjoin an auxiliary tree at a node, we can adjoin arbitrarily many trees at this node. The idea of multiple adjunction is not new (Schabes and Shieber, 1994), but it is simplified here because we disregard complex adjunction constraints. We will discuss these two restrictions in the conclusion. Finally, we assume that every lexical semantics ϕ has precisely one member; this restriction will be lifted in Section 5.4. Now let’s say we want to find the realizations of the input semantics S = {ϕ1 , . . . , ϕn }, using the grammar G. The input “sentence” of the parsing subst S,e ,1 t P, bs N su start mary m, 1 subst N P, c,1 ,1 subst N ,c adjN ,c buy car indef red Figure 4: Dependency tree for “Mary buys a red car.” problem we construct is the sequence"
P02-1003,P97-1026,0,0.0255917,"e research in Section 7. α1 2 The Realization Problem In this paper, we deal with the subtask of natural language generation known as surface realization: given a grammar and a semantic representation, the problem is to find a sentence which is grammatical according to the grammar and expresses the content of the semantic representation. We represent the semantic input as a multiset (bag) of ground atoms of predicate logic, such as {buy(e,a,b), name(a,mary) car(b)}. To encode syntactic information, we use a tree-adjoining grammar without feature structures (Joshi and Schabes, 1997). Following Stone and Doran (1997) and Kay (1996), we enhance this TAG grammar with a syntax-semantics interface in which nonterminal nodes of the elementary trees are equipped with index variables, which can be bound to individuals in the semantic input. We assume that the root node, all substitution nodes, and all nodes that admit adjunction carry such index variables. We also assign a semantics to every elementary tree, so that lexical entries are pairs of the form (ϕ, T ), where ϕ is a multiset of semantic atoms, and T is an initial or auxiliary tree, e.g. S:x ( {buy(x,y,z)}, NP:y  VP:x V:x NP:z  ) buys When the lexicon"
P04-1032,P92-1005,0,0.344327,"uchss,koller,stth}@coli.uni-sb.de Abstract We show that a practical translation of MRS descriptions into normal dominance constraints is feasible. We start from a recent theoretical translation and verify its assumptions on the outputs of the English Resource Grammar (ERG) on the Redwoods corpus. The main assumption of the translation— that all relevant underspecified descriptions are nets—is validated for a large majority of cases; all non-nets computed by the ERG seem to be systematically incomplete. 1 Introduction Underspecification is the standard approach to dealing with scope ambiguity (Alshawi and Crouch, 1992; Pinkal, 1996). The readings of underspecified expressions are represented by compact and concise descriptions, instead of being enumerated explicitly. Underspecified descriptions are easier to derive in syntax-semantics interfaces (Egg et al., 2001; Copestake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language"
P04-1032,copestake-flickinger-2000-open,0,0.0533312,"nd concise descriptions, instead of being enumerated explicitly. Underspecified descriptions are easier to derive in syntax-semantics interfaces (Egg et al., 2001; Copestake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language which is used in large-scale HPSG grammars, such as the English Resource Grammar (ERG) (Copestake and Flickinger, 2000). The main advantage of dominance constraints is that they can be solved very efficiently (Althaus et al., 2003; Bodirsky et al., 2004). Niehren and Thater (2003) defined, in a theoretical paper, a translation from MRS into normal dominance constraints. This translation clarified the precise relationship between these two related formalisms, and made the powerful meta-theory of dominance constraints accessible to MRS. Their goal was to also make the large grammars for MRS ∗ Supported by the CHORUS project of the SFB 378 of the DFG. and the efficient constraint solvers for dominance constraints"
P04-1032,1995.tmi-1.2,0,0.0357009,"nt underspecified descriptions are nets—is validated for a large majority of cases; all non-nets computed by the ERG seem to be systematically incomplete. 1 Introduction Underspecification is the standard approach to dealing with scope ambiguity (Alshawi and Crouch, 1992; Pinkal, 1996). The readings of underspecified expressions are represented by compact and concise descriptions, instead of being enumerated explicitly. Underspecified descriptions are easier to derive in syntax-semantics interfaces (Egg et al., 2001; Copestake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language which is used in large-scale HPSG grammars, such as the English Resource Grammar (ERG) (Copestake and Flickinger, 2000). The main advantage of dominance constraints is that they can be solved very efficiently (Althaus et al., 2003; Bodirsky et al., 2004). Niehren and Thater (2003) defined, in a theoretical paper, a translation from MRS into normal do"
P04-1032,P01-1019,0,0.0646999,"the Redwoods corpus. The main assumption of the translation— that all relevant underspecified descriptions are nets—is validated for a large majority of cases; all non-nets computed by the ERG seem to be systematically incomplete. 1 Introduction Underspecification is the standard approach to dealing with scope ambiguity (Alshawi and Crouch, 1992; Pinkal, 1996). The readings of underspecified expressions are represented by compact and concise descriptions, instead of being enumerated explicitly. Underspecified descriptions are easier to derive in syntax-semantics interfaces (Egg et al., 2001; Copestake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language which is used in large-scale HPSG grammars, such as the English Resource Grammar (ERG) (Copestake and Flickinger, 2000). The main advantage of dominance constraints is that they can be solved very efficiently (Althaus et al., 2003; Bodirsky et al., 2004). Niehren and Thater"
P04-1032,P03-1047,1,0.529018,"stake et al., 2001), useful in applications such as machine translation (Copestake et al., 1995), and can be resolved by need. Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics (MRS) (Copestake et al., 2004) and dominance constraints (Egg et al., 2001). MRS is the underspecification language which is used in large-scale HPSG grammars, such as the English Resource Grammar (ERG) (Copestake and Flickinger, 2000). The main advantage of dominance constraints is that they can be solved very efficiently (Althaus et al., 2003; Bodirsky et al., 2004). Niehren and Thater (2003) defined, in a theoretical paper, a translation from MRS into normal dominance constraints. This translation clarified the precise relationship between these two related formalisms, and made the powerful meta-theory of dominance constraints accessible to MRS. Their goal was to also make the large grammars for MRS ∗ Supported by the CHORUS project of the SFB 378 of the DFG. and the efficient constraint solvers for dominance constraints available to the other formalism. However, Niehren and Thater made three technical assumptions: 1. that EP-conjunction can be resolved in a preprocessing step; 2"
P04-1032,C02-2025,0,0.0619071,"s an interesting side effect, we also compare the run-times of the constraint-solvers we used, and we find that the dominance constraint solver typically outperforms the MRS solver, often by significant margins. Grammar and Resources. We use the English Resource Grammar (ERG), a large-scale HPSG grammar, in connection with the LKB system, a grammar development environment for typed feature grammars (Copestake and Flickinger, 2000). We use the system to parse sentences and output MRS constraints which we then translate into dominance constraints. As a test corpus, we use the Redwoods Treebank (Oepen et al., 2002) which contains 6612 sentences. We exclude the sentences that cannot be parsed due to memory capacities or words and grammatical structures that are not included in the ERG, or which produce ill-formed MRS expressions (typically violating M1) and thus base our evaluation on a corpus containing 6242 sentences. In case of syntactic ambiguity, we only use the first reading output by the LKB system. To enumerate the solutions of MRS constraints and their translations, we use the MRS solver built into the LKB system and a solver for weakly normal dominance constraints (Bodirsky et al., 2004), prop"
P04-1051,P87-1022,0,0.664659,"al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (Lapata, 2003). But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved. The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses. Mellish et al. (1998) and Karamanis and Manurung (2002) present a"
P04-1051,W02-2111,1,0.933808,"nces into a locally coherent discourse. The algorithm runs very efficiently on a variety of coherence measures from the literature. We also show that the discourse ordering problem is NP-complete and cannot be approximated. 1 Introduction One central problem in discourse generation and summarisation is to structure the discourse in a way that maximises coherence. Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Man"
P04-1051,W00-1411,0,0.182413,"t al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (Lapata, 2003). But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved. The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses. Mellish et al. (1998) and Karamanis and Manurung (2002) present algorithms based on genet"
P04-1051,P03-1069,0,0.4319,"iscourse. The algorithm runs very efficiently on a variety of coherence measures from the literature. We also show that the discourse ordering problem is NP-complete and cannot be approximated. 1 Introduction One central problem in discourse generation and summarisation is to structure the discourse in a way that maximises coherence. Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) o"
P04-1051,W98-1411,0,0.0179647,"Missing"
P04-1051,J99-3001,0,0.0233628,"at all. Based on these concepts, CT classifies the transitions between subsequent utterances into different types. Table 1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH - SHIFT , and ROUGH - SHIFT , which are predicted to be less and less coherent in this order (Brennan et al., 1987). Kibble and Power (2000) define three further classes of transitions: COHER ENCE and SALIENCE , which are both defined in Table 1 as well, and NOCB, the class of transitions for which Cb(ui ) is undefined. Finally, a transition is considered to satisfy the CHEAPNESS constraint (Strube and Hahn, 1999) if Cb(ui ) = Cp(ui−1 ). Table 2 summarises some cost functions from the literature, in the reconstruction of Karamanis et al. (2004). Each line shows the name of the coherence measure, the arity d from Definition 1, and the initial and transition cost functions. To fit the definitions in one line, we use terms of the form fk , which abbreviate applications of f to the last k arguments of the cost functions, i.e. f (ud−k+1 , . . . , ud ). The most basic coherence measure, M.NOCB (Karamanis and Manurung, 2002), simply assigns to each NOCB transition the cost 1 and to every other transition the"
P04-1051,P04-1050,1,\N,Missing
P05-3003,W02-1503,0,0.0773241,"ost efficient solver for scope underspecification; it also converts between different underspecification formalisms and counts readings. Our tool makes the practical use of large-scale grammars with (underspecified) semantic output more feasible, and can be used in grammar debugging. 1 Introduction One of the most exciting recent developments in computational linguistics is that large-scale grammars which compute semantic representations are becoming available. Examples for such grammars are the HPSG English Resource Grammar (ERG) (Copestake and Flickinger, 2000) and the LFG ParGram grammars (Butt et al., 2002); a similar resource is being developed for the XTAG grammar (Kallmeyer and Romero, 2004). But with the advent of such grammars, a phenomenon that is sometimes considered a somewhat artificial toy problem of theoretical semanticists becomes a very practical challenge: the presence of scope ambiguities. Because grammars often uniformly treat noun phrases as quantifiers, even harmless-looking sentences can have surprisingly many readings. The median number of scope readings for the sentences in the Rondane Treebank (distributed with the ERG) is 55, but the treebank also contains extreme cases su"
P05-3003,copestake-flickinger-2000-open,0,0.422574,"vel terminus at Flåm. (Rondane 650) We present the currently most efficient solver for scope underspecification; it also converts between different underspecification formalisms and counts readings. Our tool makes the practical use of large-scale grammars with (underspecified) semantic output more feasible, and can be used in grammar debugging. 1 Introduction One of the most exciting recent developments in computational linguistics is that large-scale grammars which compute semantic representations are becoming available. Examples for such grammars are the HPSG English Resource Grammar (ERG) (Copestake and Flickinger, 2000) and the LFG ParGram grammars (Butt et al., 2002); a similar resource is being developed for the XTAG grammar (Kallmeyer and Romero, 2004). But with the advent of such grammars, a phenomenon that is sometimes considered a somewhat artificial toy problem of theoretical semanticists becomes a very practical challenge: the presence of scope ambiguities. Because grammars often uniformly treat noun phrases as quantifiers, even harmless-looking sentences can have surprisingly many readings. The median number of scope readings for the sentences in the Rondane Treebank (distributed with the ERG) is 55"
P05-3003,W04-3321,0,0.0173775,"erent underspecification formalisms and counts readings. Our tool makes the practical use of large-scale grammars with (underspecified) semantic output more feasible, and can be used in grammar debugging. 1 Introduction One of the most exciting recent developments in computational linguistics is that large-scale grammars which compute semantic representations are becoming available. Examples for such grammars are the HPSG English Resource Grammar (ERG) (Copestake and Flickinger, 2000) and the LFG ParGram grammars (Butt et al., 2002); a similar resource is being developed for the XTAG grammar (Kallmeyer and Romero, 2004). But with the advent of such grammars, a phenomenon that is sometimes considered a somewhat artificial toy problem of theoretical semanticists becomes a very practical challenge: the presence of scope ambiguities. Because grammars often uniformly treat noun phrases as quantifiers, even harmless-looking sentences can have surprisingly many readings. The median number of scope readings for the sentences in the Rondane Treebank (distributed with the ERG) is 55, but the treebank also contains extreme cases such as (1) below, which according to the ERG has about 2.4 trillion (1012 ) readings: In o"
P05-3003,C00-1067,1,0.748174,"Missing"
P05-3003,P03-1047,1,0.928535,"vers) are built for different underspecification formalisms. To help alleviate this problem, utool can read and write underspecified descriptions and write out solutions in a variety of different formats: 60 50 40 30 20 10 • dominance graphs; 0 0 5 10 15 20 25 30 35 40 • descriptions of Minimal Recursion Semantics; • descriptions of Hole Semantics. The input and output functionality is provided by codecs, which translate between descriptions in one of these formalisms and the internal dominance graph format. The codecs for MRS and Hole Semantics are based on the (non-trivial) translations in (Koller et al., 2003; Niehren and Thater, 2003) and are only defined on nets, i.e. constraints whose graphs satisfy certain structural restrictions. This is not a very limiting restriction in practice (Flickinger et al., 2005). utool also allows the user to test efficiently whether a description is a net. In practice, utool can be used to convert descriptions between the three underspecification formalisms. Because the codecs work with concrete syntaxes that are used in existing systems, utool can be used as a drop-in replacement e.g. in the LKB grammar development system (Copestake and Flickinger, 2000). 2.3 Run"
P06-1052,P04-1032,1,0.849762,"demand that the holes are “plugged” by roots while realising the dominance edges as dominance, as in the two configurations (of five) shown to the right. These configurations are trees that encode semantic representations of the sentence. We will freely read configurations as ground terms over the signature Σ. 2.1 Hypernormally connected graphs Throughout this paper, we will only consider hypernormally connected (hnc) dominance graphs. Hnc graphs are equivalent to chain-connected dominance constraints (Koller et al., 2003), and are closely related to dominance nets (Niehren and Thater, 2003). Fuchss et al. (2004) have presented a corpus study that strongly suggests that all dominance graphs that are generated by current largescale grammars are (or should be) hnc. Technically, a graph G is hypernormally connected iff each pair of nodes is connected by a simple hypernormal path in G. A hypernormal path (Althaus et al., 2003) in G is a path in the undirected version Gu of G that does not use two dominance edges that are incident to the same hole. Hnc graphs have a number of very useful structural properties on which this paper rests. One which is particularly relevant here is that we can predict in which"
P06-1052,P05-3003,1,0.639277,"he first time in the open. (Rondane 892) Introduction Underspecification is nowadays the standard approach to dealing with scope ambiguities in computational semantics (van Deemter and Peters, 1996; Copestake et al., 2004; Egg et al., 2001; Blackburn and Bos, 2005). The basic idea behind it is to not enumerate all possible semantic representations for each syntactic analysis, but to derive a single compact underspecified representation (USR). This simplifies semantics construction, and current algorithms support the efficient enumeration of the individual semantic representations from an USR (Koller and Thater, 2005b). A major promise of underspecification is that it makes it possible, in principle, to rule out entire subsets of readings that we are not interested in wholesale, without even enumerating them. For instance, real-world sentences with scope ambiguities often have many readings that are semantically equivalent. Subsequent modules (e.g. for doing inference) will typically only be interested in one reading from each equivalence class, and all others could be deleted. This situation is illustrated by the following two (out of many) sentences from the Rondane treebank, which is distributed with F"
P06-1052,W05-1105,1,0.58061,"he first time in the open. (Rondane 892) Introduction Underspecification is nowadays the standard approach to dealing with scope ambiguities in computational semantics (van Deemter and Peters, 1996; Copestake et al., 2004; Egg et al., 2001; Blackburn and Bos, 2005). The basic idea behind it is to not enumerate all possible semantic representations for each syntactic analysis, but to derive a single compact underspecified representation (USR). This simplifies semantics construction, and current algorithms support the efficient enumeration of the individual semantic representations from an USR (Koller and Thater, 2005b). A major promise of underspecification is that it makes it possible, in principle, to rule out entire subsets of readings that we are not interested in wholesale, without even enumerating them. For instance, real-world sentences with scope ambiguities often have many readings that are semantically equivalent. Subsequent modules (e.g. for doing inference) will typically only be interested in one reading from each equivalence class, and all others could be deleted. This situation is illustrated by the following two (out of many) sentences from the Rondane treebank, which is distributed with F"
P06-1052,W06-3904,1,0.837563,"n terms of rewrite rules that permute quantifiers without changing the semantics of the readings. The particular USRs we work with are underspecified chart representations, which can be computed from dominance graphs (or USRs in some other underspecification formalisms) efficiently (Koller and Thater, 2005b). We evaluate the performance of the algorithm on the Rondane treebank and show that it reduces the median number of readings from 56 to 4, by up to a factor of 666.240 for individual USRs, while running in negligible time. To our knowledge, our algorithm and its less powerful predecessor (Koller and Thater, 2006) are the first redundancy elimination algorithms in the literature that operate on the level of USRs. There has been previous research on enumerating only some representatives of each equivalence class (Vestre, 1991; Chaves, 2003), but these approaches don’t maintain underspecification: After running their algorithms, they are left with a set of readings rather than an underspecified representation, i.e. we could no longer run other algorithms on an USR. The paper is structured as follows. We will first define dominance graphs and review the necessary background theory in Section 2. We will th"
P06-1052,P03-1047,1,0.843619,"aph can serve as an USR for the sentence “a representative of a company saw a sample” if we demand that the holes are “plugged” by roots while realising the dominance edges as dominance, as in the two configurations (of five) shown to the right. These configurations are trees that encode semantic representations of the sentence. We will freely read configurations as ground terms over the signature Σ. 2.1 Hypernormally connected graphs Throughout this paper, we will only consider hypernormally connected (hnc) dominance graphs. Hnc graphs are equivalent to chain-connected dominance constraints (Koller et al., 2003), and are closely related to dominance nets (Niehren and Thater, 2003). Fuchss et al. (2004) have presented a corpus study that strongly suggests that all dominance graphs that are generated by current largescale grammars are (or should be) hnc. Technically, a graph G is hypernormally connected iff each pair of nodes is connected by a simple hypernormal path in G. A hypernormal path (Althaus et al., 2003) in G is a path in the undirected version Gu of G that does not use two dominance edges that are incident to the same hole. Hnc graphs have a number of very useful structural properties on whi"
P06-1052,C02-2025,0,0.0999359,"r earlier algorithm (Koller and Thater, 2006), which computed a chart with four configurations for the graph in which 1 and 2 are existential and 3 is universal, as opposed to the three equivalence classes of this graph’s configurations. 5 Evaluation In this final section, we evaluate the the effectiveness and efficiency of the elimination algorithm: We run it on USRs from a treebank and measure how many readings are redundant, to what extent the algorithm eliminates this redundancy, and how much time it takes to do this. Resources. The experiments are based on the Rondane corpus, a Redwoods (Oepen et al., 2002) style corpus which is distributed with the English Resource Grammar (Flickinger, 2002). The corpus contains analyses for 1076 sentences from the tourism domain, which are associated with USRs based upon Minimal Recursion Semantics (MRS). The MRS representations are translated into dominance graphs using the open-source utool tool (Koller and Thater, 2005a), which is restricted to MRS representations whose translations are hnc. By restricting ourselves to such MRSs, we end up with a data set of 999 dominance graphs. The average number of scope bearing operators in the data set is 6.5, and the"
P06-1052,E91-1044,0,0.7474,"n some other underspecification formalisms) efficiently (Koller and Thater, 2005b). We evaluate the performance of the algorithm on the Rondane treebank and show that it reduces the median number of readings from 56 to 4, by up to a factor of 666.240 for individual USRs, while running in negligible time. To our knowledge, our algorithm and its less powerful predecessor (Koller and Thater, 2006) are the first redundancy elimination algorithms in the literature that operate on the level of USRs. There has been previous research on enumerating only some representatives of each equivalence class (Vestre, 1991; Chaves, 2003), but these approaches don’t maintain underspecification: After running their algorithms, they are left with a set of readings rather than an underspecified representation, i.e. we could no longer run other algorithms on an USR. The paper is structured as follows. We will first define dominance graphs and review the necessary background theory in Section 2. We will then introduce our notion of equivalence in Section 3, and present the redundancy elimination algorithm in Section 4. In Section 5, we describe the evaluation of the algorithm on the Rondane corpus. Finally, Section 6"
P06-1052,W06-3905,0,\N,Missing
P07-1043,J95-3003,0,0.0603557,"ation problem of TAG grammars with semantic and pragmatic information into a planning problem stated in the widely used Planning Domain Definition Language (PDDL, McDermott (2000)). The encoding provides a clean separation between computation and linguistic modelling and is open to future extensions. It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners (Blum and Furst, 1997; Kautz and Selman, 1998; Hoffmann and Nebel, 2001). While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions 336 of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavior, as earlier systems did; this allows us to get by with a much simpler logic. The problem we solve encompasses the generation of referring expressions (REs) as a"
P07-1043,P02-1003,1,0.87332,"precondition of v. These causal links are drawn as bold edges in Fig. 3. The mapping is unique for substitution edges because subst atoms are removed by every action that has them as their precondition. There may be multiple action instances in the plan that introduce the same atom canadjoin(A, u). In this case, we can freely choose one of these instances as the parent. 3 Sentence generation as planning Now we extend this encoding to deal with semantics and referring expressions. 3.1 Communicative goals In order to use the planner as a surface realization algorithm for TAG along the lines of Koller and Striegnitz (2002), we attach semantic content to each elementary tree and require that the sentence achieves a certain communicative goal. We also use a knowledge base that specifies the speaker’s knowledge, and require that we can only use trees that express information in this knowledge base. We follow Stone et al. (2003) in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms; but unlike in earlier approaches, we use the semantic roles in t as the arguments of these atoms. For instance, the semantic content of the “likes” tree in Fig. 1 is {like(self, ag, pat)} (see a"
P07-1043,W98-1419,1,0.807693,"Missing"
P08-1026,copestake-flickinger-2000-open,0,0.144585,"Missing"
P08-1026,N04-1014,0,0.0319917,"he soft disjointness edges as as angled double-headed arrows. Each soft edge is annotated with its weight. The hard backbone of this dominance graph is our example graph from Fig. 1, so it has the same five configurations. The weighted graph assigns a weight of 8 to configuration (a), a weight of 1 to (d), and a weight of 9 to (e); this is also the configuration of maximum weight. 5.2 Weighted tree grammars In order to compute the maximal-weight configuration of a weighted dominance graph, we will first translate it into a weighted regular tree grammar. A weighted regular tree grammar (wRTG) (Graehl and Knight, 2004) is a 5-tuple G = (S, N, Σ, R, c) such 224 that G0 = (S, N, Σ, R) is a regular tree grammar and c : R → R is a function that assigns each production rule a weight. G accepts the same language of trees as G0 . It assigns each derivation a cost equal to the product of the costs of the production rules used in this derivation, and it assigns each tree in the language a cost equal to the sum of the costs of its derivations. Thus wRTGs define weights in a way that is extremely similar to PCFGs, except that we don’t require any weights to sum to one. Given a weighted, hypernormally connected dominan"
P08-1026,J03-1004,0,0.44626,"mination amounts to intersection of regular tree up, plus dominance or outscoping relations between languages. Furthermore, we show how to define a these building blocks. This has been a very suc- PCFG-style cost model on RTGs and compute best cessful approach, but recent algorithms for elimi- readings of deterministic RTGs efficiently, and illusnating subsets of readings have pushed the expres- trate this model on a machine learning based model 218 Proceedings of ACL-08: HLT, pages 218–226, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics of scope preferences (Higgins and Sadock, 2003). To our knowledge, this is the first efficient algorithm for computing best readings of a scope ambiguity in the literature. The paper is structured as follows. In Section 2, we will first sketch the existing standard approach to underspecification. We will then define regular tree grammars and show how to see them as an underspecification formalism in Section 3. We will present the new redundancy elimination algorithm, based on language intersection, in Section 4, and show how to equip RTGs with weights and compute best readings in Section 5. We conclude in Section 6. 2 Underspecification pe"
P08-1026,C00-1067,1,0.919121,"Missing"
P08-1026,P05-3003,1,0.943422,"rtant here, but note that virtually all underspecified descriptions that are produced by current grammars are nets (Flickinger et al., 2005). For the rest of the paper, we restrict ourselves to dominance graphs that are hypernormally connected. 3 We will now recall the definition of regular tree grammars and show how they can be used as an underspecification formalism. 3.1 The key idea behind scope underspecification is to describe all readings of an ambiguous expression with a single, compact underspecified representation (USR). This simplifies semantics construction, and current algorithms (Koller and Thater, 2005a) support the efficient enumeration of readings from an USR when it is necessary. Furthermore, it is possible to perform certain semantic processing tasks such as eliminating redundant readings (see Section 4) directly on the level of underspecified representations without explicitly enumerating individual readings. Under the “standard model” of scope underspecification, readings are considered as formulas or trees. USRs specify the “semantic material” common to all readings, plus dominance or outscopes relations between these building blocks. In this paper, we consider dominance graphs (Egg"
P08-1026,W05-1105,1,0.947348,"rtant here, but note that virtually all underspecified descriptions that are produced by current grammars are nets (Flickinger et al., 2005). For the rest of the paper, we restrict ourselves to dominance graphs that are hypernormally connected. 3 We will now recall the definition of regular tree grammars and show how they can be used as an underspecification formalism. 3.1 The key idea behind scope underspecification is to describe all readings of an ambiguous expression with a single, compact underspecified representation (USR). This simplifies semantics construction, and current algorithms (Koller and Thater, 2005a) support the efficient enumeration of readings from an USR when it is necessary. Furthermore, it is possible to perform certain semantic processing tasks such as eliminating redundant readings (see Section 4) directly on the level of underspecified representations without explicitly enumerating individual readings. Under the “standard model” of scope underspecification, readings are considered as formulas or trees. USRs specify the “semantic material” common to all readings, plus dominance or outscopes relations between these building blocks. In this paper, we consider dominance graphs (Egg"
P08-1026,W06-3904,1,0.875959,"arser avoids comput- approach for specifying sets of trees in theoretical ing all scope readings. Instead, it computes a single computer science, and are closely related to regucompact underspecified description for each parse. lar tree transducers as used e.g. in recent work on One can then strengthen the underspecified descrip- statistical MT (Knight and Graehl, 2005) and gramtion to efficiently eliminate subsets of readings that mar formalisms (Shieber, 2006). We show that the were not intended in the given context (Koller and “dominance charts” proposed by Koller and Thater Niehren, 2000; Koller and Thater, 2006); so when (2005b) can be naturally seen as regular tree gramthe individual readings are eventually computed, the mars; using their algorithm, classical underspecified number of remaining readings is much smaller and descriptions (dominance graphs) can be translated much closer to the actual perceived ambiguity of the into RTGs that describe the same sets of readings. However, RTGs are trivially expressively complete sentence. In the past few years, a “standard model” of scope because every finite tree language is also regular. We underspecification has emerged: A range of for- exploit this inc"
P08-1026,N06-1045,0,0.0228615,"of computing the best tree is NPcomplete (Sima’an, 1996). However, if the weighted regular tree automaton corresponding to the wRTG is deterministic, every tree has only one derivation, and thus computing best trees becomes easy again. The tree automata for dominance charts are always deterministic, and the automata for RTGs as in Section 3.2 (whose terminals correspond to the graph’s node labels) are also typically deterministic if the variable names are part of the quantifier node labels. Furthermore, there are algorithms for determinizing weighted tree automata (Borchardt and Vogler, 2003; May and Knight, 2006), which could be applied as preprocessing steps for wRTGs. 6 Conclusion In this paper, we have shown how regular tree grammars can be used as a formalism for scope underspecification, and have exploited the power of this view in a novel, simpler, and more complete algorithm for redundancy elimination and the first efficient algorithm for computing the best reading of a scope ambiguity. In both cases, we have adapted standard algorithms for RTGs, which illustrates the usefulness of using such a well-understood formalism. In the worst case, the RTG for a scope ambiguity is exponential in the num"
P08-1026,P03-1047,1,0.908669,"for computing best readings of a scope ambiguity in the literature. The paper is structured as follows. In Section 2, we will first sketch the existing standard approach to underspecification. We will then define regular tree grammars and show how to see them as an underspecification formalism in Section 3. We will present the new redundancy elimination algorithm, based on language intersection, in Section 4, and show how to equip RTGs with weights and compute best readings in Section 5. We conclude in Section 6. 2 Underspecification pernormally connected dominance graphs, or dominance nets (Niehren and Thater, 2003). The precise definition of dominance nets is not important here, but note that virtually all underspecified descriptions that are produced by current grammars are nets (Flickinger et al., 2005). For the rest of the paper, we restrict ourselves to dominance graphs that are hypernormally connected. 3 We will now recall the definition of regular tree grammars and show how they can be used as an underspecification formalism. 3.1 The key idea behind scope underspecification is to describe all readings of an ambiguous expression with a single, compact underspecified representation (USR). This simpl"
P08-1026,C02-2025,0,0.0706165,"Missing"
P08-1026,P08-2062,1,0.795162,"Missing"
P08-1026,E06-1048,0,0.165256,"malism. Regular stake and Flickinger (2000)). The key idea behind tree grammars (Comon et al., 2007) are a standard underspecification is that the parser avoids comput- approach for specifying sets of trees in theoretical ing all scope readings. Instead, it computes a single computer science, and are closely related to regucompact underspecified description for each parse. lar tree transducers as used e.g. in recent work on One can then strengthen the underspecified descrip- statistical MT (Knight and Graehl, 2005) and gramtion to efficiently eliminate subsets of readings that mar formalisms (Shieber, 2006). We show that the were not intended in the given context (Koller and “dominance charts” proposed by Koller and Thater Niehren, 2000; Koller and Thater, 2006); so when (2005b) can be naturally seen as regular tree gramthe individual readings are eventually computed, the mars; using their algorithm, classical underspecified number of remaining readings is much smaller and descriptions (dominance graphs) can be translated much closer to the actual perceived ambiguity of the into RTGs that describe the same sets of readings. However, RTGs are trivially expressively complete sentence. In the past"
P08-1026,C96-2215,0,0.0646807,"Missing"
P08-1026,E91-1044,0,0.572228,"mproved redundancy elimination algorithm. 4.1 ings of U, but every reading in U is semantically equivalent to some reading in U 0 . For instance, the following sentence from the Rondane treebank is analyzed as having six quantifiers and 480 readings by the ERG grammar; these readings fall into just two semantic equivalence classes, characterized by the relative scope of “the lee of” and “a small hillside”. A redundancy elimination would therefore ideally reduce the underspecified description to one that has only two readings (one for each class). Redundancy elimination Redundancy elimination (Vestre, 1991; Chaves, 2003; Koller and Thater, 2006) is the problem of deriving from an USR U another USR U 0 , such that the readings of U 0 are a proper subset of the read221 Based on this definition, Koller and Thater (2006) present an algorithm (henceforth, KT06) that deletes rules from a dominance chart and thus removes subsets of readings from the USR. The KT06 algorithm is fast and quite effective in practice. However, it essentially predicts for each production rule of a dominance chart whether each configuration that can be built with this rule is equivalent to a configuration that can be built w"
P08-1026,P06-1052,1,\N,Missing
P08-2062,copestake-flickinger-2000-open,0,0.03363,"Missing"
P08-2062,W98-0113,0,0.0183577,"n extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser. In either case, only partial information on discourse structure is available. To handle such information, underspecification formalisms can be used. Underspecification was originally introduced in computational semantics to model structural ambiguity without disjunctively enumerating the readings, and later applied to discourse parsing (Gardent and Webber, 1998; Schilder, 2002). However, while the existing algorithms for underspecification processing work well for semantic structures, they were not designed for discourse structures, which can be much larger. Indeed, it has never been shown that underspecified discourse reprentations (UDRs) can be processed efficiently, since the general-purpose implementations are too slow for that task. In this paper, we present a new way to implement and process discourse underspecification in terms of regular tree grammars (RTGs). RTGs are Underspecified Discourse Representation Following annotation schemes like"
P08-2062,P05-3003,1,0.9468,"b-f). For example, Fig. (1e) is derived by expanding the start symbol with the first rule in Fig. 2. This determines that the tree root is labelled with Condition; we then derive the left subtree from the NT {1} and the right subtree from the NT {3; 7}. The NTs in the grammar correspond to subgraphs in the dominance graph: The NT {1; 7} represents the subgraph {1, 2, 3, 4, 5, 6, 7} (i.e. the whole graph); the NT {1} represents the subgraph containing only the fragment 1; and so forth. The trees that can be derived from each nonterminal correspond exactly to the configurations of the subgraph. Koller and Thater (2005b) presented an algorithm for generating, from a very general class of dominance graphs, an RTG that describes exactly the same trees. For each subgraph S that is to be the LHS of a rule, the algorithm determines the free fragments of S, i.e. the fragments that may serve as the root of one of its configurations, by a certain graph algorithm. For every free fragment in S with n holes and a root label f , the algorithm generates a new rule of the form S → f (S1 , . . . , Sn ), where each Si corresponds to the remaining subgraph under the i-th hole. The procedure calls itself recursively on the s"
P08-2062,W05-1105,1,0.939728,"b-f). For example, Fig. (1e) is derived by expanding the start symbol with the first rule in Fig. 2. This determines that the tree root is labelled with Condition; we then derive the left subtree from the NT {1} and the right subtree from the NT {3; 7}. The NTs in the grammar correspond to subgraphs in the dominance graph: The NT {1; 7} represents the subgraph {1, 2, 3, 4, 5, 6, 7} (i.e. the whole graph); the NT {1} represents the subgraph containing only the fragment 1; and so forth. The trees that can be derived from each nonterminal correspond exactly to the configurations of the subgraph. Koller and Thater (2005b) presented an algorithm for generating, from a very general class of dominance graphs, an RTG that describes exactly the same trees. For each subgraph S that is to be the LHS of a rule, the algorithm determines the free fragments of S, i.e. the fragments that may serve as the root of one of its configurations, by a certain graph algorithm. For every free fragment in S with n holes and a root label f , the algorithm generates a new rule of the form S → f (S1 , . . . , Sn ), where each Si corresponds to the remaining subgraph under the i-th hole. The procedure calls itself recursively on the s"
P08-2062,P08-1026,1,0.73579,"pper fragment, fragments 2i − 1 and 2i + 1 are lower fragments, and there are dominance edges from the left hole of 2i to the root of 2i − 1 and from the right hole of 2i to the root of 2i + 1 (and possibly further dominance edges). These numbers are shown in circles in Fig. (1a). In discourse dominance graphs, upper fragments correspond to discourse relations, and lower fragments correspond to EDUs; the EDUs are ordered according to their appearance in the text, and the upper fragments connect the two text spans to which they are adjacent. 3 Underspecified Processing for Discourses Recently, Koller et al. (2008) showed how to process dominance graphs with regular tree grammars (Comon et al., 2007, RTGs). RTGs are a grammar formalism that describes sets of trees using production rules which rewrite non-terminal symbols (NTs) into terms consisting of tree constructors and possibly further NTs. A tree (without NTs) is accepted by the grammar if it can be derived by a sequence of rule applications from a given start symbol. An example RTG is shown in Fig. 2; its start symbol is {1; 7}, and it describes exactly the five trees in 246 {1; 7} → Cond({1}, {3; 7}) [1] {3; 7} → Contr({3; 5}, {7}) [1] {1; 7} → C"
P08-2062,W04-0213,0,0.135359,"vide the first possibility for computing an underspecified discourse description and a best discourse representation efficiently enough to process even the longest discourses in the RST Discourse Treebank. 2 1 § Alexander Koller a.koller@ed.ac.uk § University of Edinburgh Introduction Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications). But discourse structures cannot always be described completely, either due to genuine ambiguity (Stede, 2004) or to the limitations of a discourse parser. In either case, only partial information on discourse structure is available. To handle such information, underspecification formalisms can be used. Underspecification was originally introduced in computational semantics to model structural ambiguity without disjunctively enumerating the readings, and later applied to discourse parsing (Gardent and Webber, 1998; Schilder, 2002). However, while the existing algorithms for underspecification processing work well for semantic structures, they were not designed for discourse structures, which can be mu"
P08-2062,C08-2009,1,\N,Missing
P08-2062,P08-1000,0,\N,Missing
P09-2076,P08-2050,0,0.0421031,"U. of Edinburgh Robert.Dale@mq.edu.au Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach to NLG evaluation: By connecting NLG systems to experimental subjects over the Internet, it achieves a true task-based evaluation at a much lower cost. Indeed, the first GIVE Challenge acquired data from over 1100 experimental subjects online. However, it still remains to be shown that the results that can be obtained in this way are in fact comparable to more established task-bas"
P09-2076,J09-1008,0,0.0108405,"In this paper, we validate this novel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internetbased approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out. 1 dbyron@ccs.neu.edu justine@northwestern.edu Sara Dalzel-Job U. of Edinburgh Robert.Dale@mq.edu.au Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent"
P09-2076,W09-0628,1,0.930466,"tems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach to NLG evaluation: By connecting NLG systems to experimental subjects over the Internet, it achieves a true task-based evaluation at a much lower cost. Indeed, the first GIVE Challenge acquired data from over 1100 experimental subjects online. However, it still remains to be shown that the results that can be obtained in this way are in fact comparable to more established task-based evaluation efforts, which are based on a carefully selected subject pool and carried out in a controlled laboratory Justine Cassell Northwestern U. Johanna Moor"
P09-2076,W08-1131,0,0.0426993,"ovel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internetbased approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out. 1 dbyron@ccs.neu.edu justine@northwestern.edu Sara Dalzel-Job U. of Edinburgh Robert.Dale@mq.edu.au Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach t"
P09-2076,W09-0629,0,\N,Missing
P10-1004,C00-1067,1,0.745991,"weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about anaphoric accessibility; later, Koller et al. (2008) presented and evaluated an algorithm for redundancy elimination, which removes readings from an USR based on logical equivalence. Our system generalizes the latter approach and applies it to a new inference problem (weakest readings) which they could not solve. This paper builds closely upon Koller and Thater (2010), which lays the formal groundwork for the 3.1 Dominance graphs A (labelled) dominance graph D (Althaus et al., 2003) is a directed graph that consists of a"
P10-1004,P05-3003,1,0.884123,"system, and then construct a tree grammar for the relative normal forms of the combined system. This algorithm performs redundancy elimination and computes weakest readings at the same time, and in our example retains only a single configuration, namely Resources. For our experiment, we use the Rondane treebank (version of January 2006), a “Redwoods style” (Oepen et al., 2002) treebank containing underspecified representations (USRs) in the MRS formalism (Copestake et al., 2005) for sentences from the tourism domain. Our implementation of the relative normal forms algorithm is based on Utool (Koller and Thater, 2005), which (among other things) can translate a large class of MRS descriptions into hypernormally connected dominance graphs and further into RTGs as in Section 3. The implementation exploits certain properties of RTGs computed from dominance graphs to maximize efficiency. We will make this implementation publically available as part of the next Utool release. We use Utool to automatically translate the 999 MRS descriptions for which this is possible into RTGs. To simplify the specification of the rewrite systems, we restrict ourselves to the subcorpus in which all scope-taking operators (labels"
P10-1004,bos-2008-lets,0,0.0298507,"these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings – the minimal (least informative) readings with respect to this order – only express “safe” information that is common to all other read30 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 30–39, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computatio"
P10-1004,W02-1503,0,0.0605201,"Missing"
P10-1004,P03-1047,1,0.915748,"regular tree grammars are called regular tree languages (RTLs), and regular tree grammars are equivalent to finite tree automata, which are defined essentially like the well-known finite string automata, except that they assign states to the nodes in a tree rather than the positions in a string. Regular tree languages enjoy many of the closure properties of regular string languages. In particular, we will later exploit that RTLs are closed under intersection and complement. 3.3 Dominance graphs as RTGs An important class of dominance graphs are hypernormally connected (hnc) dominance graphs (Koller et al., 2003). The precise definition of hnc graphs is not important here, but note that virtually all underspecified descriptions that are produced by current grammars are hypernormally connected (Flickinger et al., 2005), and we will restrict ourselves to hnc graphs for the rest of the paper. Every hypernormally connected dominance graph D can be automatically translated into an equivalent RTG GD that generates exactly the same configurations (Koller et al., 2008); the RTG in Fig. 3 is an example. The nonterminals of GD are 32 always hnc subgraphs of D. In the worst case, GD can be exponentially bigger t"
P10-1004,copestake-flickinger-2000-open,0,0.0639585,"Missing"
P10-1004,P08-1026,1,0.874212,"with respect to this order – only express “safe” information that is common to all other read30 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 30–39, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 2008). We compute the weakest readings by intersecting these grammars with other grammars representing the rewrite rules. This approach can be used much more generally than just for the computation of weakest readings; we illustrate this by showing how a more general version of the redundancy elimination algorithm by Koller et al. (2008) can be seen as a special case of our construction. Thus our system can serve as a general framework for removing unintended readings from an underspecified representation. The paper is structured as follows. Section 2 starts by reviewing related work. We recall dominance graphs, regular tree grammars, and the basic ideas of underspecification in Section 3, before we show how to compute weakest readings (Section 4) and logical equivalences (Section 5). In Section 6, we define a weakening rewrite system for the ERG and evaluate it on the Rondane Treebank. Section 7 concludes and points to futur"
P10-1004,C08-1066,0,0.038867,"Missing"
P10-1004,C02-2025,0,0.169586,"s makes it possible to work with semantic representations derived by deep large-scale grammars. 1 Introduction Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the obse"
P10-1004,J08-3004,0,0.0200498,"ly once. 4.2 The key idea of the construction is to represent the relation →R in terms of a context tree transducer M, and characterize the relative normal forms of a tree language L in terms of the pre-image of L under M. Like ordinary regular tree transducers (Comon et al., 2007), context tree transducers read an input tree, assigning states to the nodes, while emitting an output tree. But while ordinary transducers read the input tree symbol by symbol, a context tree transducer can read multiple symbols at once. In this way, they are equivalent to the extended left-hand side transducers of Graehl et al. (2008). We will now define context tree transducers. Let Σ be a ranked signature, and let Xm be a set of m variables. We write Con(m) (Σ) for the contexts with m holes, i.e. those trees in T (Σ ∪ Xm ) in which each element of Xm occurs exactly once, and always as a leaf. If C ∈ Con(m) (Σ), then C[t1 , . . . ,tm ] = C[t1 /x1 , . . . ,tm /xm ], where x1 , . . . , xm are the variables from left to right. A (top-down) context tree transducer from Σ to ∆ is a 5-tuple M = (Q, Σ, ∆, q0 , δ ). Σ and ∆ are ranked signatures, Q is a finite set of states, and q0 ∈ Q is the start state. δ is a finite set of tra"
P10-1004,J03-1004,0,0.311779,"(ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al., 2002), to parse wide-coverage text and assign it deep semantic representations. While applications should benefit from these very precise semantic representations, their usefulness is limited by the presence of semantic ambiguity: On the Rondane Treebank (Oepen et al., 2002), the ERG computes an average of several million semantic representations for each sentence, even when the syntactic analysis is fixed. The problem of appropriately selecting one of them to work with would ideally be solved by statistical methods (Higgins and Sadock, 2003) or knowledge-based inferences. However, no such approach has been worked out in sufficient detail to support the disambiguation of treebank sentences. As an alternative, Bos (2008) proposes to compute the weakest reading of each sentence and then use it instead of the “true” reading of the sentence. This is based on the observation that the readings of a semantically ambiguous sentence are partially ordered with respect to logical entailment, and the weakest readings – the minimal (least informative) readings with respect to this order – only express “safe” information that is common to all o"
P10-1004,E95-1001,0,0.0473012,"nce, the signature of the trees in Fig. 1 is {∀x |2, ∃y |2, compz |0, . . .}. Finite constructor trees can be seen as ground terms over Σ that respect the arities. We write T (Σ) for the finite constructor trees over Σ. Related work The idea of deriving a single approximative semantic representation for ambiguous sentences goes back to Hobbs (1983); however, Hobbs only works his algorithm out for a restricted class of quantifiers, and his representations can be weaker than our weakest readings. Rules that weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically licensed special cases. The work presented here is related to other approaches that reduce the set of readings of an underspecified semantic representation (USR). Koller and Niehren (2000) showed how to strengthen a dominance constraint using information about an"
P10-1004,P83-1009,0,0.666373,"f , g, a, . . .}, each of which is equipped with an arity ar( f ) ≥ 0. We take a (finite constructor) tree t as a finite tree in which each node is labelled with a symbol of Σ, and the number of children of the node is exactly the arity of this symbol. For instance, the signature of the trees in Fig. 1 is {∀x |2, ∃y |2, compz |0, . . .}. Finite constructor trees can be seen as ground terms over Σ that respect the arities. We write T (Σ) for the finite constructor trees over Σ. Related work The idea of deriving a single approximative semantic representation for ambiguous sentences goes back to Hobbs (1983); however, Hobbs only works his algorithm out for a restricted class of quantifiers, and his representations can be weaker than our weakest readings. Rules that weaken one reading into another were popular in the 1990s underspecification literature (Reyle, 1995; Monz and de Rijke, 2001; van Deemter, 1996) because they simplify logical reasoning with underspecified representations. From a linguistic perspective, Kempson and Cormack (1981) even go so far as to claim that the weakest reading should be taken as the “basic” reading of a sentence, and the other readings only seen as pragmatically li"
P10-1004,E91-1044,0,0.234609,"em R, in time linear in the size of GD and linear in the size of R. This is a dramatic improvement over the best previous algorithm, which was quadratic in |conf(D)|. 5 Redundancy elimination, revisited The construction we just carried out – characterize the configurations we find interesting as the relative normal forms of an annotated rewrite system R, translate it into a transducer MR , and intersect conf(D) with the complement of the pre-image under MR – is more generally useful than just for the computation of weakest readings. We illustrate this on the problem of redundancy elimination (Vestre, 1991; Chaves, 2003; Koller et al., 2008) by showing how a variant of the algorithm of Koller et al. (2008) falls out of our technique as a special case. Redundancy elimination is the problem of computing, from a dominance graph D, another dominance graph D0 such that conf(D0 ) ⊆ conf(D) and An example Consider an annotated rewrite system that contains rule (1) plus the following rewrite rule: [−] ∃z (P, ∀x (Q, R)) → ∀x (∃z (P, Q), R) {6}{q} ¯ → repr-ofx,z Figure 4: RTG for the weakest readings of Fig. 1. −1 L(G0 ) = L(GD ) ∩ τM (L(GD )). R 4.4 {5}{q} ¯ → compz (3) This rewrite system translates in"
P10-1004,P08-1000,0,\N,Missing
P10-1055,T75-2001,0,0.544577,"ages. Two things are worth noting. First, our result shows that the ability of CCG to generate non-context-free languages does not hinge on the availability of substitution and type-raising rules: The derivations of G1 only use generalized compositions. Neither does it require the use of functional argument categories: The grammar G1 is first-order in the sense of Koller and Kuhlmann (2009). Proof. To see the inclusion, it suffices to note that pure CCG when restricted to application rules is the same as AB-grammar, the classical categorial formalism investigated by Ajdukiewicz and BarHillel (Bar-Hillel et al., 1964). This formalism is weakly equivalent to context-free grammar. Second, it is important to note that if the composition degree n is restricted to 0 or 1, pure CCG actually collapses to context-free expressive power. This is clear for n D 0 because of the equivalence to AB grammar. For n D 1, observe that the arity of the result of a composition is at most as high as 3.1 CFG ¨ PCCG 537 that of each premise. This means that the arity of any derived category is bounded by the maximal arity of lexical categories in the grammar, which together with Lemma 1 implies that there is only a finite set of"
P10-1055,C04-1180,0,0.0341993,"t-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, w"
P10-1055,J07-4004,0,0.0370889,"of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is"
P10-1055,P96-1011,0,0.391099,"ct the application of individual rules. This means that these formalisms cannot be fully lexicalized, in the sense that certain languages can only be described by selecting language-specific rules. Our result generalizes Koller and Kuhlmann’s (2009) result for pure first-order CCG. Our proof is not as different as it looks at first glance, as their construction of mapping a CCG derivation to a valency tree and back to a derivation provides a different transformation on derivation trees. Our transformation is also technically related to the normal form construction for CCG parsing presented by Eisner (1996). Of course, at the end of the day, the issue that is more relevant to computational linguistics than a formalism’s ability to generate artificial languages such as L3 is how useful it is for modeling natural languages. CCG, and multi-modal CCG in particular, has a very good track record for this. In this sense, our formal result can also be understood as a contribution to a discussion about the expressive power that is needed to model natural languages. Acknowledgments We have profited enormously from discussions with Jason Baldridge and Mark Steedman, and would also like to thank the anonymo"
P10-1055,P02-1043,0,0.0156084,"G is strictly smaller than that of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The gener"
P10-1055,W08-2306,0,0.193753,"nguages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-free grammar can. The discussion of the (weak and strong) generative capacity of CCG and TAG has recently been revived (Hockenmaier and Young, 2008; Koller and Kuhlmann, 2009). In particular, Koller and Kuhlmann (2009) have shown that CCGs that are pure (i.e., they can only use generalized composition rules, and there is no way to restrict the instances of these rules that may be used) and first-order (i.e., all argument categories are atomic) can not generate an b n c n . This shows that the generative capacity of at least first-order CCG crucially relies on its ability to restrict rule instantiations, and is at odds with the general conception of CCG as a fully lexicalized formalism, in which all grammars use one and the same set of un"
P10-1055,E09-1053,1,0.930194,"rsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-free grammar can. The discussion of the (weak and strong) generative capacity of CCG and TAG has recently been revived (Hockenmaier and Young, 2008; Koller and Kuhlmann, 2009). In particular, Koller and Kuhlmann (2009) have shown that CCGs that are pure (i.e., they can only use generalized composition rules, and there is no way to restrict the instances of these rules that may be used) and first-order (i.e., all argument categories are atomic) can not generate an b n c n . This shows that the generative capacity of at least first-order CCG crucially relies on its ability to restrict rule instantiations, and is at odds with the general conception of CCG as a fully lexicalized formalism, in which all grammars use one and the same set of universal rules. A question th"
P10-1055,W06-1637,0,0.0149884,"ammar (TAG). Our result also carries over to a multi-modal extension of CCG. 1 Introduction Combinatory Categorial Grammar (CCG) (Steedman, 2001; Steedman and Baldridge, 2010) is an expressive grammar formalism with formal roots in combinatory logic (Curry et al., 1958) and links to the type-logical tradition of categorial grammar (Moortgat, 1997). It has been successfully used for a wide range of practical tasks, such as data-driven parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007), wide-coverage semantic construction (Bos et al., 2004), and the modelling of syntactic priming (Reitter et al., 2006). It is well-known that CCG can generate languages that are not context-free (which is necessary to capture natural languages), but can still be parsed in polynomial time. Specifically, VijayShanker and Weir (1994) identified a version of CCG that is weakly equivalent to Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997) and other mildly context-sensitive grammar formalisms, and can generate non-context-free languages such as an b n c n . The generative capacity of CCG is commonly attributed to its flexible composition rules, which allow it to model more complex word orders that context-fr"
P10-1055,P88-1034,0,0.702937,"string w is called the yield of the resulting derivation tree. A derivation tree is complete, if the last category is the final category of G. The language generated by G, denoted by L.G/, is formed by the yields of all complete derivation trees. 2.4 Degree Restrictions Work on CCG generally assumes an upper bound on the degree of composition rules that can be used in derivations. We also employ this restriction, and only consider grammars with compositions of some bounded (but arbitrary) degree n  0.1 CCG with unbounded-degree compositions is more expressive than bounded-degree CCG or TAG (Weir and Joshi, 1988). Bounded-degree grammars have a number of useful properties, one of which we mention here. The following lemma rephrases Lemma 3.1 in Vijay-Shanker and Weir (1994). Lemma 2 For every grammar G, there is a finite number of categories that can occur as secondary premises in derivations of G. Proof. The arity of a secondary premise c can be written as m C n, where m is the arity of the first argument of the corresponding primary premise, and n is the degree of the rule applied. Since each argument is an argument of some lexical category of G (Lemma 1), and since n is assumed to be bounded, both"
P10-1055,E03-1036,0,\N,Missing
P10-1100,P98-1013,0,0.0249075,"Missing"
P10-1100,N03-1003,0,0.0131605,"sually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns different phrases which describe the same event together with the temporal ordering constraints. Jones and Thompson (2003) describe an approach to identifying different natural language realizations for the same event considering the temporal structure of a scenario. However, they don’t aim to acquire or represent the temporal structure of the whole script in the end. In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). Unlike Barzilay and Lee, we do not tackle the general paraphrase problem, but only consider whether two phrases describe the same event in the context of the same 3 Scripts Before we delve into the technical details, let us establish some terminology. In this paper, we distinguish scenarios, as classes of human activities, from scripts, which are stereotypical models of the internal structure of these activities. Where EATING IN A RESTAURANT is a scenario, the script describes a number of events, such as ordering and leaving, that must occur in a certain order in order to constitute an EATIN"
P10-1100,D08-1073,0,0.016894,"al script graph. We evaluate our system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we b"
P10-1100,P08-1090,0,0.655159,"al script graph. We evaluate our system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we b"
P10-1100,P09-1068,0,0.571795,"ur system in Section 6 and conclude in Section 7. 2 Related Work Approaches to learning script-like knowledge are not new. For instance, Mooney (1990) describes an early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information a"
P10-1100,P07-2044,0,0.0085379,"n early attempt to acquire causal chains, and Smith and Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information about scripts is usually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns differ"
P10-1100,W03-0418,0,0.0610793,"Missing"
P10-1100,P06-1095,0,0.0099398,"nd Arnold (2009) use a graph-based algorithm to learn temporal script structures. However, to our knowledge, such approaches have never been shown to generalize sufficiently for wide coverage application, and none of them was rigorously evaluated. More recently, there have been a number of approaches to automatically learning event chains from corpora (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Manshadi et al., 2008). These systems typically employ a method for classifying temporal relations between given event descriptions (Chambers et al., 2007; Chambers and Jurafsky, 2008a; Mani et al., 2006). They achieve impressive performance at extracting high-level descriptions of procedures such as a CRIMINAL PROCESS. Because our approach involves directly asking people for event sequence descriptions, it can focus on acquiring specific scripts from arbitrary domains, and we can control the level of granularity at which scripts are described. Furthermore, we believe that much information about scripts is usually left implicit in texts and is therefore easier to learn from our more explicit data. Finally, our system automatically learns different phrases which describe the same event together"
P10-1100,D08-1027,0,0.0700099,"Missing"
P10-1100,C98-1013,0,\N,Missing
P10-1100,J82-1004,0,\N,Missing
P10-1159,W08-1107,1,0.772521,"void misunderstandings. The SCRISP system Our system receives as input a plan for what the IF should do to solve the task, and successively takes object-manipulating actions as the commu6.3 Comparison with Baseline B Baseline B is a corrected and improved version of the “Austin” system (Chen and Karpov, 2009), 1580 one of the best-performing systems of the GIVE-1 Challenge. Baseline B, like the original “Austin” system, issues navigation instructions by precomputing the shortest path from the IF’s current location to the target, and generates REs using the description logic based algorithm of Areces et al. (2008). Unlike the original system, which inflexibly navigates the user all the way to the target, Baseline B starts off with navigation, and opportunistically instructs the IF to push a button once it has become visible and can be described by a distinguishing RE. We fixed bugs in the original implementation of the RE generation module, so that Baseline B generates only unambiguous REs. The module nonetheless naively treats all adjectives as intersective and is not sensitive to the context of their comparison set. Specifically, a button cannot be referred to as “the right red button” if it is not t"
P10-1159,W09-3929,0,0.0206842,"s (Perrault and Allen, 1980; Appelt, 1985) provided compelling intuitions for this connection, but were not computationally viable. The research we report here can be seen as combining Appelt’s idea of using planning for sentence-level NLG with a computationally benign variant of Perrault et al.’s approach of modeling the intended perlocutionary effects of a speech act as the effects of a planning operator. Our work is linked to a growing body of very recent work that applies modern planning research to various problems in NLG (Steedman and Petrick, 2007; Brenner and Kruijff-Korbayov´a, 2008; Benotti, 2009). It is directly based on Koller and Stone’s (2007) reimplementation of the SPUD generator (Stone et al., 2003) with planning. As far as we know, ours is the first system in the SPUD tradi1574 (a) S:self (b) NP:self NP:subj ↓ VP:self NP:j ↓ N:self the NP:obj ↓ V:self NP:j pushes semcontent: {button(self)} N* semcontent: {red(self)} NP:b1 ↓ pushes NP:b1 John N:b1 the NP:self N:self red VP:e V:e button semcontent: {push(self,subj,obj)} S:e N:b1 John semcontent: {John(self)} red button N* Figure 1: (a) An example grammar; (b) a derivation of “John pushes the red button” using (a). tion that expli"
P10-1159,J03-1003,0,0.0762495,"Missing"
P10-1159,W09-0608,0,0.0442081,"e context-dependent modifier and model the effect of their linear order on the meaning of the phrase. In modeling the context, we focus on the non-linguistic context and the influence of each of the RE’s words; this is in contrast to previous research on contextsensitive generation of REs, which mainly focused on the discourse context (Krahmer and Theune, 2002). Our interpretation of context-dependent modifiers picks up ideas by Kamp and Partee (1995) and implements them in a practical system, while our method of ordering modifiers is linguistically informed by the class-based paradigm (e.g., Mitchell (2009)). On the other hand, our work also stands in a tradition of NLG research that is based on AI planning. Early approaches (Perrault and Allen, 1980; Appelt, 1985) provided compelling intuitions for this connection, but were not computationally viable. The research we report here can be seen as combining Appelt’s idea of using planning for sentence-level NLG with a computationally benign variant of Perrault et al.’s approach of modeling the intended perlocutionary effects of a speech act as the effects of a planning operator. Our work is linked to a growing body of very recent work that applies"
P10-1159,J80-3003,0,0.821667,"n the non-linguistic context and the influence of each of the RE’s words; this is in contrast to previous research on contextsensitive generation of REs, which mainly focused on the discourse context (Krahmer and Theune, 2002). Our interpretation of context-dependent modifiers picks up ideas by Kamp and Partee (1995) and implements them in a practical system, while our method of ordering modifiers is linguistically informed by the class-based paradigm (e.g., Mitchell (2009)). On the other hand, our work also stands in a tradition of NLG research that is based on AI planning. Early approaches (Perrault and Allen, 1980; Appelt, 1985) provided compelling intuitions for this connection, but were not computationally viable. The research we report here can be seen as combining Appelt’s idea of using planning for sentence-level NLG with a computationally benign variant of Perrault et al.’s approach of modeling the intended perlocutionary effects of a speech act as the effects of a planning operator. Our work is linked to a growing body of very recent work that applies modern planning research to various problems in NLG (Steedman and Petrick, 2007; Brenner and Kruijff-Korbayov´a, 2008; Benotti, 2009). It is direc"
P10-1159,P07-1043,1,0.327928,"navigation instructions (intended to manipulate the non-linguistic context in a certain way) and referring expressions (which exploit the non-linguistic context). Although such subdialogues are common in SCARE, we are not aware of any previous research that can generate them in a computationally feasible manner. This paper presents an approach to generation which is able to model the effect of an utterance on the non-linguistic context, and to intentionally generate utterances such as the above as part of a process of referring to objects. Our approach builds upon the CRISP generation system (Koller and Stone, 2007), which translates generation problems into planning problems and solves these with an AI planner. We extend the CRISP planning operators with the perlocutionary effects that uttering a particular word has on the physical environment if it is understood correctly; more specifically, on the position and orientation of the hearer. This allows the planner to predict the nonlinguistic context in which a later part of the utterance will be interpreted, and therefore to search for contexts that allow the use of simple REs. As a result, the work of referring to an object gets distributed over multipl"
P10-1159,P99-1018,0,0.0481665,"ordered correctly, and the workload of modeling interactions with the non-linguistic context is limited to a single place in the encoding. 5.2 Adjective word order One final challenge that arises in our system is to generate the adjectives in the correct order, which on top of semantically valid must be linguistically acceptable. In particular, it is known that some types of adjectives are limited with respect to the word order in which they can occur in a noun phrase. For instance, “large foreign financial firms” sounds perfectly acceptable, but “? foreign large financial firms” sounds odd (Shaw and Hatzivassiloglou, 1999). In our setting, some adjective orders are forbidden because only one order produces a correct and distinguishing descripFigure 7: The IF’s view of the scene in Fig. 3, as rendered by the GIVE client. tion of the target referent (cf. “upper left” vs. “left upper” example above). However, there are also other constraints at work: “? the red left button” is rather odd even when it is a semantically correct description, whereas “the left red button” is fine. To ensure that SCRISP chooses to generate these adjectives correctly, we follow a class-based approach to the premodifier ordering problem"
P10-1159,2007.sigdial-1.47,0,0.0114369,"tion of NLG research that is based on AI planning. Early approaches (Perrault and Allen, 1980; Appelt, 1985) provided compelling intuitions for this connection, but were not computationally viable. The research we report here can be seen as combining Appelt’s idea of using planning for sentence-level NLG with a computationally benign variant of Perrault et al.’s approach of modeling the intended perlocutionary effects of a speech act as the effects of a planning operator. Our work is linked to a growing body of very recent work that applies modern planning research to various problems in NLG (Steedman and Petrick, 2007; Brenner and Kruijff-Korbayov´a, 2008; Benotti, 2009). It is directly based on Koller and Stone’s (2007) reimplementation of the SPUD generator (Stone et al., 2003) with planning. As far as we know, ours is the first system in the SPUD tradi1574 (a) S:self (b) NP:self NP:subj ↓ VP:self NP:j ↓ N:self the NP:obj ↓ V:self NP:j pushes semcontent: {button(self)} N* semcontent: {red(self)} NP:b1 ↓ pushes NP:b1 John N:b1 the NP:self N:self red VP:e V:e button semcontent: {push(self,subj,obj)} S:e N:b1 John semcontent: {John(self)} red button N* Figure 1: (a) An example grammar; (b) a derivation of “"
P10-1159,stoia-etal-2008-scare,0,0.0649373,"Missing"
P10-1159,J06-2002,0,0.060354,"Missing"
P10-1159,N06-2040,0,\N,Missing
P13-1015,J09-4009,0,0.0974598,"recognition problem is cubic. In the case of linear context-free rewriting systems (LCFRSs, (Weir, 1988)) the rule-by-rule technique also applies to every grammar, as long as an increased fanout it permitted (Rambow and Satta, 1999). There are also grammar formalisms for which the rule-by-rule technique is not complete. In the case of SCFGs, not every grammar has an equivalent representation of rank 2 in the first place (Aho and Ullman, 1969). Even when such a representation exists, it is not always possible to compute it rule by rule. Nevertheless, the rule-by-rule binarization algorithm of Huang et al. (2009) is very useful in practice. In this paper, we offer a generic approach for transferring the rule-by-rule binarization technique to new grammar formalisms. At the core of our approach is a binarization algorithm that can be adapted to a new formalism by changing a parameter at runtime. Thus it only needs to be implemented once, and can then be reused for a variety of formalisms. More specifically, our algorithm requires the user to (i) encode the grammar formalism as a subclass of interpreted regular tree grammars (IRTGs, (Koller and Kuhlmann, 2011)) and (ii) supply a collection of b-rules, wh"
P13-1015,W11-2902,1,0.853033,"theless, the rule-by-rule binarization algorithm of Huang et al. (2009) is very useful in practice. In this paper, we offer a generic approach for transferring the rule-by-rule binarization technique to new grammar formalisms. At the core of our approach is a binarization algorithm that can be adapted to a new formalism by changing a parameter at runtime. Thus it only needs to be implemented once, and can then be reused for a variety of formalisms. More specifically, our algorithm requires the user to (i) encode the grammar formalism as a subclass of interpreted regular tree grammars (IRTGs, (Koller and Kuhlmann, 2011)) and (ii) supply a collection of b-rules, which represent equivalence of grammars syntactically. Our algorithm then replaces, in a given grammar, each rule of rank greater than 2 by an equivalent collection of rules of rank 2, if such a collection is licensed by the b-rules. We define completeness of b-rules in a way that ensures that if any equivalent collection of rules of rank 2 exists, the algorithm finds one. As a consequence, the algorithm binarizes every grammar that can be binarized rule by rule. Step (i) is possible for all the grammar formalisms mentioned above. We show Step (ii) fo"
P13-1015,W12-4616,1,0.753987,"table algebras. The string algebra in Table 1 yields context-free languages, more complex string al147 con3 b c α h1 d con4 h2 ←−[ α α α 7−→ 1 2 3 d a b c Figure 3: Derivation tree and semantic terms. con2 x1 x2 con2 x1 x2 h01 (b) h02 ←−[ α00 7−→ (c) a con2 x3 {x1 } x1 x3 x1 a x2 con2 t2 : con2 x3 x3 x1 a x2 con2 con2 con2 x2 x3 con2 x2 τ : {x1 , x2 , x3 } {x1 , x2 } {x2 , x3 } x2 a x1 {x3 } {x1 } x1 {x2 } con2 x1 con2 x2 x1 x2 x1 x2 a con2 x2 h′1 x3 ←−[ con2 x1 α′ ξ: t1 : con2 x1 {x2 } {x3 } con2 x1 (e) {x1 , x3 } {x1 } x2 (d) {x1 , x2 , x3 } {x3 } con2 gebras yield tree-adjoining languages (Koller and Kuhlmann, 2012), and algebras over other domains can yield languages of trees, graphs, or other objects. Furthermore, IRTGs with n = 1 describe languages that are subsets of the algebra’s domain, n = 2 yields synchronous languages or tree transductions, and so on. x1 x2 x2 x2 t2 : con2 h′2 α′′ x1 x1 x3 7−→ con2 x3 a con2 x1 x2 Figure 5: Outline of the binarization algorithm. parsing and translation will be cheaper. Now we want to construct the binary rules systematically. In the example, we proceed as follows (cf. Fig. 5). For each of the terms h1 (α) and h2 (α) (Fig. 5a), we consider all terms that satisfy"
P13-1015,J07-2003,0,0.172874,"isms by simply varying a formal parameter. We apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation. 1 Introduction Binarization amounts to transforming a given grammar into an equivalent grammar of rank 2, i.e., with at most two nonterminals on any righthand side. The ability to binarize grammars is crucial for efficient parsing, because for many grammar formalisms the parsing complexity depends exponentially on the rank of the grammar. It is also critically important for tractable statistical machine translation (SMT). Syntaxbased SMT systems (Chiang, 2007; Graehl et al., 2008) typically use some type of synchronous grammar describing a binary translation relation between strings and/or trees, such as synchronous context-free grammars (SCFGs) (Lewis and Stearns, 1966; Chiang, 2007), synchronous tree-substitution grammars (Eisner, 2003), synchronous tree-adjoining grammars (Nesson et al., 2006; DeNeefe and Knight, 2009), and tree-tostring transducers (Yamada and Knight, 2001; Graehl et al., 2008). These grammars typically have a large number of rules, many of which have rank greater than two. The classical approach to binarization, as known from"
P13-1015,D09-1076,0,0.0229703,"for efficient parsing, because for many grammar formalisms the parsing complexity depends exponentially on the rank of the grammar. It is also critically important for tractable statistical machine translation (SMT). Syntaxbased SMT systems (Chiang, 2007; Graehl et al., 2008) typically use some type of synchronous grammar describing a binary translation relation between strings and/or trees, such as synchronous context-free grammars (SCFGs) (Lewis and Stearns, 1966; Chiang, 2007), synchronous tree-substitution grammars (Eisner, 2003), synchronous tree-adjoining grammars (Nesson et al., 2006; DeNeefe and Knight, 2009), and tree-tostring transducers (Yamada and Knight, 2001; Graehl et al., 2008). These grammars typically have a large number of rules, many of which have rank greater than two. The classical approach to binarization, as known from the Chomsky normal form transformation for context-free grammars (CFGs), proceeds rule by rule. It replaces each rule of rank greater than 2 by an equivalent collection of rules of rank 2. All CFGs can be binarized in this 145 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 145–154, c Sofia, Bulgaria, August 4-9 2013. 20"
P13-1015,P03-2041,0,0.0538822,"minals on any righthand side. The ability to binarize grammars is crucial for efficient parsing, because for many grammar formalisms the parsing complexity depends exponentially on the rank of the grammar. It is also critically important for tractable statistical machine translation (SMT). Syntaxbased SMT systems (Chiang, 2007; Graehl et al., 2008) typically use some type of synchronous grammar describing a binary translation relation between strings and/or trees, such as synchronous context-free grammars (SCFGs) (Lewis and Stearns, 1966; Chiang, 2007), synchronous tree-substitution grammars (Eisner, 2003), synchronous tree-adjoining grammars (Nesson et al., 2006; DeNeefe and Knight, 2009), and tree-tostring transducers (Yamada and Knight, 2001; Graehl et al., 2008). These grammars typically have a large number of rules, many of which have rank greater than two. The classical approach to binarization, as known from the Chomsky normal form transformation for context-free grammars (CFGs), proceeds rule by rule. It replaces each rule of rank greater than 2 by an equivalent collection of rules of rank 2. All CFGs can be binarized in this 145 Proceedings of the 51st Annual Meeting of the Association"
P13-1015,J03-1006,0,0.0918366,"pty in the restricted case if and only if it is empty in the general case. We call the b-rules b1 , . . . , b1 complete on G if the equation holds for every α ∈ Σ. Now we show how to effectively compute binarization terms with respect to b1 , . . . , bn , along the lines of Section 3.3. More specifically, we construct an RTG for each of the T sets (i) bi (hi (α)), (ii) b0i = v(bi (hi (α))), (iii) i b0i , and (iv) b00i = bi (hi (α))∩v −1 (τ ) (given τ ). Then we can select τ from (iii) and ti from (iv) using a standard algorithm, such as the Viterbi algorithm or Knuth’s algorithm (Knuth, 1977; Nederhof, 2003; Huang and Chiang, 2005). The effectiveness of our procedure stems from the fact that we only manipulate RTGs and never enumerate languages. The construction for (i) is recursive, following the definition of bi . The base case is a language {xj }, for which the RTG is easy. For the recursive case, we use the fact that regular tree languages are closed under substitution (G´ecseg and Steinby, 1997, Prop. 7.3). Thus we obtain an RTG Gi with L(Gi ) = bi (hi (α)). For (ii) and (iv), we need the following auxiliary This result suggests the following procedure for obtaining binarization terms. Firs"
P13-1015,N04-1035,0,0.563711,"hildren. Let ∆ be a signature. A ∆-algebra A consists of a nonempty set A called the domain and, for each symbol f ∈ ∆ with rank k, a total function f A : Ak → A, the operation associated with f . We can evaluate any term t in T∆ (Xk ) in A, to obtain a k-ary operation tA over the domain. In particular, terms in T∆ evaluate to elements of A. For instance, in the string algebra shown in Table 1, the term con2 (a, b) evaluates to ab, and the term con2 (con2 (x2 , a), x1 ) evaluates to a binary operation f such that, e.g., f (b, c) = cab. rithm to tree-to-string transducers (Graehl et al., 2008; Galley et al., 2004), which describe relations between strings in one language and parse trees of another, which means that existing methods for binarizing SCFGs and LCFRSs cannot be directly applied to these systems. To our knowledge, our binarization algorithm is the first to binarize such transducers. We illustrate the effectiveness of our system by binarizing a large treeto-string transducer for English-German SMT. Plan of the paper. We start by defining IRTGs in Section 2. In Section 3, we define the general outline of our approach to rule-by-rule binarization for IRTGs, and then extend this to an efficient"
P13-1015,2006.amta-papers.15,0,0.0241382,"e grammars is crucial for efficient parsing, because for many grammar formalisms the parsing complexity depends exponentially on the rank of the grammar. It is also critically important for tractable statistical machine translation (SMT). Syntaxbased SMT systems (Chiang, 2007; Graehl et al., 2008) typically use some type of synchronous grammar describing a binary translation relation between strings and/or trees, such as synchronous context-free grammars (SCFGs) (Lewis and Stearns, 1966; Chiang, 2007), synchronous tree-substitution grammars (Eisner, 2003), synchronous tree-adjoining grammars (Nesson et al., 2006; DeNeefe and Knight, 2009), and tree-tostring transducers (Yamada and Knight, 2001; Graehl et al., 2008). These grammars typically have a large number of rules, many of which have rank greater than two. The classical approach to binarization, as known from the Chomsky normal form transformation for context-free grammars (CFGs), proceeds rule by rule. It replaces each rule of rank greater than 2 by an equivalent collection of rules of rank 2. All CFGs can be binarized in this 145 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 145–154, c Sofia, Bu"
P13-1015,N10-1118,0,0.0188855,"es the same algebras as the original IRTG. Our algorithm extends to grammars of arbitrary fanout (such as synchronous tree-adjoining grammar (Koller and Kuhlmann, 2012)), but unlike LCFRS-based approaches to binarization, it will not increase the fanout to ensure binarizability. In the future, we will explore IRTG binarization with fanout increase. This could be done by binarizing into an IRTG with a more complicated algebra (e.g., of string tuples). We might compute binarizations that are optimal with respect to some measure (e.g., fanout (Gomez-Rodriguez et al., 2009) or parsing complexity (Gildea, 2010)) by keeping track of this measure in the b-rule and taking intersections of weighted tree automata. General approach Our binarization algorithm can be used to solve the general rule-by-rule binarization problem for a specific grammar formalism, provided that one can find appropriate b-rules. More precisely, we need to devise a class C of IRTGs over the same sequence A1 , . . . , An of algebras that encodes the grammar formalism, together with brules b1 , . . . , bn over A1 , . . . , An that are complete on every grammar in C, as defined in Section 4. We have already seen the b-rules for SCFGs"
P13-1015,P01-1067,0,0.50994,"s the parsing complexity depends exponentially on the rank of the grammar. It is also critically important for tractable statistical machine translation (SMT). Syntaxbased SMT systems (Chiang, 2007; Graehl et al., 2008) typically use some type of synchronous grammar describing a binary translation relation between strings and/or trees, such as synchronous context-free grammars (SCFGs) (Lewis and Stearns, 1966; Chiang, 2007), synchronous tree-substitution grammars (Eisner, 2003), synchronous tree-adjoining grammars (Nesson et al., 2006; DeNeefe and Knight, 2009), and tree-tostring transducers (Yamada and Knight, 2001; Graehl et al., 2008). These grammars typically have a large number of rules, many of which have rank greater than two. The classical approach to binarization, as known from the Chomsky normal form transformation for context-free grammars (CFGs), proceeds rule by rule. It replaces each rule of rank greater than 2 by an equivalent collection of rules of rank 2. All CFGs can be binarized in this 145 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 145–154, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics be an alpha"
P13-1015,N09-1061,0,0.0246669,"have taken the perspective that the binarized IRTG uses the same algebras as the original IRTG. Our algorithm extends to grammars of arbitrary fanout (such as synchronous tree-adjoining grammar (Koller and Kuhlmann, 2012)), but unlike LCFRS-based approaches to binarization, it will not increase the fanout to ensure binarizability. In the future, we will explore IRTG binarization with fanout increase. This could be done by binarizing into an IRTG with a more complicated algebra (e.g., of string tuples). We might compute binarizations that are optimal with respect to some measure (e.g., fanout (Gomez-Rodriguez et al., 2009) or parsing complexity (Gildea, 2010)) by keeping track of this measure in the b-rule and taking intersections of weighted tree automata. General approach Our binarization algorithm can be used to solve the general rule-by-rule binarization problem for a specific grammar formalism, provided that one can find appropriate b-rules. More precisely, we need to devise a class C of IRTGs over the same sequence A1 , . . . , An of algebras that encodes the grammar formalism, together with brules b1 , . . . , bn over A1 , . . . , An that are complete on every grammar in C, as defined in Section 4. We ha"
P13-1015,J08-3004,0,0.494872,"varying a formal parameter. We apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation. 1 Introduction Binarization amounts to transforming a given grammar into an equivalent grammar of rank 2, i.e., with at most two nonterminals on any righthand side. The ability to binarize grammars is crucial for efficient parsing, because for many grammar formalisms the parsing complexity depends exponentially on the rank of the grammar. It is also critically important for tractable statistical machine translation (SMT). Syntaxbased SMT systems (Chiang, 2007; Graehl et al., 2008) typically use some type of synchronous grammar describing a binary translation relation between strings and/or trees, such as synchronous context-free grammars (SCFGs) (Lewis and Stearns, 1966; Chiang, 2007), synchronous tree-substitution grammars (Eisner, 2003), synchronous tree-adjoining grammars (Nesson et al., 2006; DeNeefe and Knight, 2009), and tree-tostring transducers (Yamada and Knight, 2001; Graehl et al., 2008). These grammars typically have a large number of rules, many of which have rank greater than two. The classical approach to binarization, as known from the Chomsky normal fo"
P13-1015,W05-1506,0,0.167346,"icted case if and only if it is empty in the general case. We call the b-rules b1 , . . . , b1 complete on G if the equation holds for every α ∈ Σ. Now we show how to effectively compute binarization terms with respect to b1 , . . . , bn , along the lines of Section 3.3. More specifically, we construct an RTG for each of the T sets (i) bi (hi (α)), (ii) b0i = v(bi (hi (α))), (iii) i b0i , and (iv) b00i = bi (hi (α))∩v −1 (τ ) (given τ ). Then we can select τ from (iii) and ti from (iv) using a standard algorithm, such as the Viterbi algorithm or Knuth’s algorithm (Knuth, 1977; Nederhof, 2003; Huang and Chiang, 2005). The effectiveness of our procedure stems from the fact that we only manipulate RTGs and never enumerate languages. The construction for (i) is recursive, following the definition of bi . The base case is a language {xj }, for which the RTG is easy. For the recursive case, we use the fact that regular tree languages are closed under substitution (G´ecseg and Steinby, 1997, Prop. 7.3). Thus we obtain an RTG Gi with L(Gi ) = bi (hi (α)). For (ii) and (iv), we need the following auxiliary This result suggests the following procedure for obtaining binarization terms. First, determine whether the"
P13-1015,2006.amta-papers.8,0,0.0612249,"Missing"
P15-1143,W13-2322,0,0.264356,"in training synchronous string-to-graph grammars, and when generating strings from them. We present two algorithms for graph parsing (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drew"
P15-1143,P13-1091,0,0.512083,"ng, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars for graph-based semantics construction. One problem that is only partially understood in the context of semantic parsing with explicit grammars is graph parsing, i.e. the computation of the possible analyses the grammar assigns to an input graph (as opposed to string). This"
P15-1143,W11-2902,1,0.540337,"ve no in-boundary edges in common, i.e. E1 ∩ E2 = ∅; (c) for every source node v of β1 , the last edge on the path in SG from v to the closest source node of β2 is not an in-boundary edge of β2 , and vice versa. Furthermore, if these conditions hold, we have T (β1 ||β2 ) = T (β1 ) ||T (β2 ), where we define β1 ||β2 = (E1 ∪ E2 , φ1 ∪ φ2 ). 4 S-graph grammars We are now ready to define s-graph grammars, which describe languages of s-graphs. We also introduce graph parsing and relate s-graph grammars to HRGs. 4.1 Grammars for languages of s-graphs We use interpreted regular tree grammars (IRTGs; Koller and Kuhlmann (2011)) to describe languages of s-graphs. IRTGs are a very general mechanism for describing languages over and relations between arbitrary algebras. They separate conceptually the generation of a grammatical derivation from its interpretation as a string, tree, graph, or some other object. Consider, as an example, the tiny grammar in Fig. 3; see Koller (2015) for linguistically meaningful grammars. The left column consists of a regular tree grammar G (RTG; see e.g. Comon et al. (2008)) with two rules. This RTG describes a regular language L(G) of derivation trees (in general, it may be infinite). I"
P15-1143,S14-2082,0,0.0278873,"Missing"
P15-1143,S14-2008,0,0.0363705,"string-to-graph grammars, and when generating strings from them. We present two algorithms for graph parsing (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As"
P15-1143,W12-6207,0,0.0273395,"ng a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars for graph-based semantics construction. One problem that is only partially understood in the context of semantic parsing with explicit grammars is graph parsing, i.e. the computation of the possible analyses the grammar assigns to an input graph (as opposed to string). This problem arises whenever one trie"
P15-1143,P14-1134,0,0.0540867,"p-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistically reasonable grammars"
P15-1143,C12-1083,0,0.0328734,"ng (bottom-up and top-down) with s-graph grammars. On the related problem of graph parsing with hyperedge replacement grammars, our implementations outperform the best previous system by several orders of magnitude. 1 Introduction The recent years have seen an increased interest in semantic parsing, the problem of deriving a semantic representation for natural-language expressions with data-driven methods. With the recent availability of graph-based meaning banks (Banarescu et al., 2013; Oepen et al., 2014), much work has focused on computing graph-based semantic representations from strings (Jones et al., 2012; Flanigan et al., 2014; Martins and Almeida, 2014). One major approach to graph-based semantic parsing is to learn an explicit synchronous grammar which relates strings with graphs. One can then apply methods from statistical parsing to parse the string and read off the graph. Chiang et al. (2013) and Quernheim and Knight (2012) represent this mapping of a (latent) syntactic structure to a graph with a grammar formalism called hyperedge replacement grammar (HRG; (Drewes et al., 1997)). As an alternative to HRG, Koller (2015) introduced s-graph grammars and showed that they support linguistica"
P15-2133,D13-1134,1,0.853049,"n Interaction Group, Saarland University, Saarbr¨ucken, Germany 2 Department of Linguistics, University of Potsdam, Potsdam, Germany {nikkol |masta}@coli.uni-saarland.de {martin.villalba |alexander.koller}@uni-potsdam.de Abstract of human interlocutors but neither study exploits listeners’ eye movements. In contrast, Koller et al. (2012) designed a task in which a natural language generation (NLG) system gives instructions to a human player in virtual environment whose eye movements were tracked. They outperformed similar systems in both successful reference resolution and listener confusion. Engonopoulos et al. (2013) attempted to predict the resolution of an RE, achieving good performance by combining two probabilistic log–linear models: a semantic model Psem that analyzes the semantics of a given instruction, and an observational model Pobs that inspects the player’s behavior. However, they did not include listener’s gaze. They observed that the accuracy for Pobs reaches its highest point at a relatively late stage in an interaction. Similar observations are reported by Kennington and Schlangen (2014): they compare listener gaze and an incremental update model (IUM) as predictors for the resolution of an"
P15-2133,gargett-etal-2010-give,1,0.747151,"listener’s behavior in order to determine whether their communicated message was received and understood. This phenomenon is known as grounding, it is well established in the dialogue literature (Clark, 1996), and it plays an important role in collaborative tasks and goal–oriented conversations. Solving a collaborative task in a shared environment is an effective way of studying the alignment of communication channels (Clark and Krych, 2004; Hanna and Brennan, 2007). In situated spoken conversations ambiguous linguistic expressions are common, where additional modalities are available. While Gargett et al. (2010) studied instruction giving and following in virtual environments, Brennan et al. (2013) examined pedestrian guidance in outdoor real environments. Both studies investigate the interaction 2 Problem definition We address the research question of how to automatically predict an RE resolution, i.e., answering the question of which entity in a virtual environment has been understood by the listener af812 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 812–817, c"
P15-2133,W12-1604,1,0.915242,"Missing"
P15-2133,W10-4233,1,\N,Missing
P16-1192,W13-2322,0,0.0419457,"e, which means the terms are much more complex than for the PCFG and there are much fewer terminal symbols with the same homomorphic term. As a consequence, condensing the invhom is much less helpful. However, the sibling-finder algorithm excels at maintaining state information within each elementary tree, yielding a 1000x speedup over the naive bottom-up algorithm when it was cancelled. Graphs. Finally, we parsed a corpus of graphs instead of strings, using the 13681-rule graph grammar of Groschwitz et al. (2015) to parse the 1258 graphs with up to 10 nodes from the “Little Prince” AMR-Bank (Banarescu et al., 2013). The top-down algorithms are slow in this experiment, confirming Groschwitz et al.’s findings. Again, the sibling-finder algorithm outperforms all other algorithms. Note that Groschwitz et al.’s parser (“GKT 15” in Fig. 7) shares much code with our system. It uses the same decomposition automata, but a less mature version of the sibling-finder method which fully computes the invhom automaton. Our new system achieves a 9x speedup for parsing the whole corpus, compared to GKT 15. 7 Related Work Describing parsing algorithms at a high level of abstraction has a long tradition in computational li"
P16-1192,2000.iwpt-1.9,0,0.148422,"word that did not occur in the sentence). PCFG. We extracted a binarized context-free grammar with 6929 rules from Section 00 of the Penn Treebank, and parsed the sentences of Section 00 with it. The homorphism in the corresponding IRTG assigns every terminal symbol a constant or the term ∗(x1 , x2 ), as in Fig. 1. As a consequence, the condensed automaton optimization from Section 5 outperforms all other algo2049 rithms, achieving a 100x speedup over the naive bottom-up algorithm when it was cancelled. TAG. We also extracted a tree-adjoining grammar from Section 00 of the PTB as described by Chen and Vijay-Shanker (2000), converted it to an IRTG as described by Koller and Kuhlmann (2012), and binarized it, yielding an IRTG with 26652 rules. Each term h(r) in this grammar represents an entire TAG elementary tree, which means the terms are much more complex than for the PCFG and there are much fewer terminal symbols with the same homomorphic term. As a consequence, condensing the invhom is much less helpful. However, the sibling-finder algorithm excels at maintaining state information within each elementary tree, yielding a 1000x speedup over the naive bottom-up algorithm when it was cancelled. Graphs. Finally,"
P16-1192,P13-1091,0,0.189945,"Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algorithm which recursively explores substructures of the input object and assigns them nonterminal symbols, but their exact relationship is rarely made explicit. On the practical side, this parsing algorithm and its extensions (e.g. to EM training)"
P16-1192,J07-2003,0,0.0686948,"ext-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the s"
P16-1192,J07-4004,0,0.0800609,"e present techniques that speed up tree-automata-based parsing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. T"
P16-1192,P01-1033,0,0.189005,"Missing"
P16-1192,N04-1035,0,0.15297,"rsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algo"
P16-1192,J08-3004,0,0.0781866,"best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algorithm which recursive"
P16-1192,J13-1006,0,0.0898079,"ased parsing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and"
P16-1192,W11-2902,1,0.472049,"erspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algorithm which recursively explores substructures of the input object and assigns them nonterminal symbols, but their exact relationship is rarely made explicit. On the practical side, this parsing algorithm and its extensions (e.g. to EM training) have to be implemented and optimized from scratch for each new grammar formalism. Thus, development time is spent on reinventing wheels that are slightly different from previous ones, and the resulting implementations still tend to underperform. Koller and Kuhlmann (2011) introduced Interpreted Regular Tree Grammars (IRTGs) in order to address this situation. An IRTG represents a language by describing a regular language of derivation trees, each of which is mapped to a term over some algebra and evaluated there. Grammars from a wide range of monolingual and synchronous formalisms can be mapped into IRTGs by using different algebras: Context-free and treeadjoining grammars use string algebras of different kinds, graph grammars can be captured by using graph algebras, and so on. In addition, IRTGs come with a universal parsing algorithm based on closure results"
P16-1192,W12-4616,1,0.749415,"string is an element of L(G). We assume that no two rules of M use the same terminal symbol; this is generally not required in tree automata, but every IRTG can be brought into this convenient form. Furthermore, we focus (but only for simplicity of presentation) on IRTGs that use a single string-algebra interpretation, as in Fig. 1. Such grammars capture context-free grammars. However, IRTGs can capture a wide variety of grammar formalisms by using different algebras. For instance, an interpretation that uses a TAG string algebra (or TAG derived-tree algebra) models a tree-adjoining grammar (Koller and Kuhlmann, 2012), and an interpretation into an s-graph algebra models a hyperedge replacement graph grammar (HRG, Groschwitz et al. (2015)). By using multiple algebras, IRTGs can also represent synchronous grammars and (bottom-up) treeto-tree and tree-to-string transducers. In general, any grammar formalism whose grammars describe derivations in terms of a finite set of states can typically be converted into IRTG. 2.3 Parsing IRTGs Koller and Kuhlmann (2011) present a uniform parsing algorithm for IRTGs based on tree automata. The (monolingual) parsing problem of IRTG consists in determining, for an IRTG G a"
P16-1192,D14-1107,0,0.0236714,"t speed up tree-automata-based parsing, to the point that it becomes practically feasible on realistic data when applied to context-free, TAG, and graph parsing. For graph parsing, we obtain the best runtimes in the literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges"
P16-1192,P15-1079,0,0.017374,"literature. 1 Introduction Grammar formalisms that go beyond context-free grammars have recently enjoyed renewed attention throughout computational linguistics. Classical grammar formalisms such as TAG (Joshi and Schabes, 1997) and CCG (Steedman, 2001) have been equipped with expressive statistical models, and high-performance parsers have become available (Clark and Curran, 2007; Lewis and Steedman, 2014; Kallmeyer and Maier, 2013). Synchronous grammar formalisms such as synchronous context-free grammars (Chiang, 2007) and tree-to-string transducers (Galley et al., 2004; Graehl et al., 2008; Seemann et al., 2015) are being used as models that incorporate syntactic information in statistical machine translation. Synchronous string-to-tree (Wong and Mooney, 2006) and string-to-graph grammars (Chiang et al., 2013) have been applied to semantic parsing; and so forth. Each of these grammar formalisms requires its users to develop new algorithms for parsing and training. This comes with challenges that are both practical and theoretical. From a theoretical perspective, many of these algorithms are basically the same, in that they rest upon a CKY-style parsing algorithm which recursively explores substructur"
P16-1192,N06-1056,0,0.0924091,"Missing"
P16-1192,P15-1143,1,0.727586,"in tree automata, but every IRTG can be brought into this convenient form. Furthermore, we focus (but only for simplicity of presentation) on IRTGs that use a single string-algebra interpretation, as in Fig. 1. Such grammars capture context-free grammars. However, IRTGs can capture a wide variety of grammar formalisms by using different algebras. For instance, an interpretation that uses a TAG string algebra (or TAG derived-tree algebra) models a tree-adjoining grammar (Koller and Kuhlmann, 2012), and an interpretation into an s-graph algebra models a hyperedge replacement graph grammar (HRG, Groschwitz et al. (2015)). By using multiple algebras, IRTGs can also represent synchronous grammars and (bottom-up) treeto-tree and tree-to-string transducers. In general, any grammar formalism whose grammars describe derivations in terms of a finite set of states can typically be converted into IRTG. 2.3 Parsing IRTGs Koller and Kuhlmann (2011) present a uniform parsing algorithm for IRTGs based on tree automata. The (monolingual) parsing problem of IRTG consists in determining, for an IRTG G and an input object a ∈ A, a representation of the set parses(a) = {τ ∈ L(M ) |h(τ )A = a}, i.e. of the derivation trees tha"
P17-1063,W11-2852,0,0.0317532,"bases; that is, by improved handling of misunderstandings. Engonopoulos et al. (2013) detect misunderstandings of REs in interactive NLG through the use of a statistical model. Their model also predicts the object to which a misunderstood RE was incorrectly resolved. Moving from misunderstanding detection to error correction, Zarrieß and Schlangen (2016) present an interactive NLG algorithm which is capable of referring in installments, in that it can generate multiple REs that are designed to correct misunderstandings of earlier REs to the same object. The interactive NLG system developed by Akkersdijk et al. (2011) generates both reflective and anticipative feedback based on what a user does and sees. Their error detection and correction strategy distinguishes a fixed set of possible situations where feedback is necessary, and defines custom, hard-coded RE generation sub-strategies for each one. None of these systems generate REs marked for focus. We start by introducing the problem of generating corrective REs in an interactive NLG setting. We use examples from the GIVE Challenge (Koller et al., 2010) throughout the paper; however, the algorithm itself is domain-independent. GIVE is a shared task in wh"
P17-1063,W10-4342,0,0.0226712,"selected a different object, shown in a second screenshot. They were then asked to select which one of two corrections they considered better, where “better” was intentionally left unspecific. Figs. 7 and 8 show examples for each domain. The full set of stimuli is available as supplementary material. To maintain annotation quality in our crowdsourcing setting, we designed test items with a 7 Evaluation To test whether our algorithm for contrastive REs assigns contrastive focus correctly, we evaluated it against several baselines in crowdsourced pairwise comparison overhearer experiments. Like Buß et al. (2010), we opted for an overhearer experiment to focus our evaluation on the effects of contrastive feedback, as opposed to the challenges presented by the navigational and timing aspects of a fully interactive system. 7.1 Domains and stimuli We created the stimuli for our experiments from two different domains. We performed a first experiment with scenes from the GIVE Challenge, while a second experiment replaced these scenes with stimuli from the “People” domain of the TUNA Reference Corpus (van der Sluis et al., 2007). This corpus consists of photographs of men annotated with nine attributes, suc"
P17-1063,W14-5002,1,0.908244,"Missing"
P17-1063,D13-1134,1,0.934175,"marking. 1 Introduction Interactive natural language generation (NLG) systems face the task of detecting when they have been misunderstood, and reacting appropriately to fix the problem. For instance, even when the system generated a semantically correct referring expression (RE), the user may still misunderstand it, i.e. resolve it to a different object from the one the system intended. In an interactive setting, such as a dialogue system or a pedestrian navigation system, the system can try to detect such misunderstandings – e.g. by predicting what the hearer understood from their behavior (Engonopoulos et al., 2013) – and to produce further utterances which resolve the misunderstanding and get the hearer to identify the intended object after all. When humans correct their own REs, they routinely employ contrastive focus (Rooth, 1992; Krifka, 2008) to clarify the relationship to the original RE. Say that we originally described an object b as “the blue button”, but the hearer approaches a button b0 which is green, thus providing evidence that they misunderstood the RE to mean b0 . In this 2 Related Work The notion of focus has been extensively studied in the literature on theoretical semantics and prag678"
P17-1063,W13-2104,0,0.030082,"ive REs, but the attributes that receive contrastive focus have to be specified by hand. Krahmer and Theune (2002) extend the Incremental Algorithm (Dale and Reiter, 1995) so it can mark attributes as contrastive. This is a fully automatic algorithm for contrastive REs, but it inherits all the limitations of the Incremental Algorithm, such as its reliance on a fixed attribute order. Neither of these two approaches evaluates the quality of the contrastive REs it generates. Finally, some work has addressed the issue of generating texts that realize the discourse relation contrast. For instance, Howcroft et al. (2013) show how to choose contrastive discourse connectives (but, while, . . . ) when generating restaurant descriptions, thus increasing human ratings for naturalness. Unlike their work, the research presented in this paper is not about discourse relations, but about assigning focus in contrastive REs. matics, see e.g. Krifka (2008) and Rooth (1997) for overview papers. Krifka follows Rooth (1992) in taking focus as “indicat(ing) the presence of alternatives that are relevant for the interpretation of linguistic expressions”; focus then establishes a contrast between an object and these alternative"
P17-1063,P16-1058,0,0.0865286,"ystems that detect and respond to misunderstandings. Misu et al. (2014) present an error analysis of an in-car dialogue system which shows that more than half the errors can only be resolved through further clarification dialogues, as opposed to better sensors and/or databases; that is, by improved handling of misunderstandings. Engonopoulos et al. (2013) detect misunderstandings of REs in interactive NLG through the use of a statistical model. Their model also predicts the object to which a misunderstood RE was incorrectly resolved. Moving from misunderstanding detection to error correction, Zarrieß and Schlangen (2016) present an interactive NLG algorithm which is capable of referring in installments, in that it can generate multiple REs that are designed to correct misunderstandings of earlier REs to the same object. The interactive NLG system developed by Akkersdijk et al. (2011) generates both reflective and anticipative feedback based on what a user does and sees. Their error detection and correction strategy distinguishes a fixed set of possible situations where feedback is necessary, and defines custom, hard-coded RE generation sub-strategies for each one. None of these systems generate REs marked for"
P17-1063,W96-0417,0,0.251404,"y that we originally described an object b as “the blue button”, but the hearer approaches a button b0 which is green, thus providing evidence that they misunderstood the RE to mean b0 . In this 2 Related Work The notion of focus has been extensively studied in the literature on theoretical semantics and prag678 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 678–687 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1063 address the generation of contrastive REs directly. Milosavljevic and Dale (1996) outline strategies for generating clarificatory comparisons in encyclopedic descriptions. Their surface realizer can generate contrastive REs, but the attributes that receive contrastive focus have to be specified by hand. Krahmer and Theune (2002) extend the Incremental Algorithm (Dale and Reiter, 1995) so it can mark attributes as contrastive. This is a fully automatic algorithm for contrastive REs, but it inherits all the limitations of the Incremental Algorithm, such as its reliance on a fixed attribute order. Neither of these two approaches evaluates the quality of the contrastive REs it"
P17-1063,W14-4304,0,0.0267264,"ocus. In speech, focus is typically marked through intonation and pitch accents (Levelt, 1993; Pierrehumbert and Hirschberg, 1990; Steube, 2001), while concepts that can be taken for granted are deaccented and/or deleted. Developing systems which realize precise pitch contours for focus in text-to-speech settings is an ongoing research effort. We therefore realize focus in written language in this paper, by capitalizing the focused word. We also experiment with deletion of background words. There is substantial previous work on interactive systems that detect and respond to misunderstandings. Misu et al. (2014) present an error analysis of an in-car dialogue system which shows that more than half the errors can only be resolved through further clarification dialogues, as opposed to better sensors and/or databases; that is, by improved handling of misunderstandings. Engonopoulos et al. (2013) detect misunderstandings of REs in interactive NLG through the use of a statistical model. Their model also predicts the object to which a misunderstood RE was incorrectly resolved. Moving from misunderstanding detection to error correction, Zarrieß and Schlangen (2016) present an interactive NLG algorithm which"
P18-1170,W13-2322,0,0.605465,"mlinde|mfowlie|koller}@coli.uni-saarland.de mark.johnson@mq.edu.au Abstract We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines. 1 Introduction Over the past few years, Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) have become a popular target representation for semantic parsing. AMRs are graphs which describe the predicate-argument structure of a sentence. Because they are graphs and not trees, they can capture reentrant semantic relations, such as those induced by control verbs and coordination. However, it is technically much more challenging to parse a string into a graph than into a tree. For instance, grammar-based approaches (Peng et al., 2015; Artzi et al., 2015) require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in fa"
P18-1170,S17-2157,0,0.230807,"Missing"
P18-1170,P13-2131,0,0.343584,"Missing"
P18-1170,E17-1051,0,0.661226,"connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs. Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015). In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with c"
P18-1170,S14-2080,0,0.0299105,"with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs. Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015). In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models re"
P18-1170,P99-1059,0,0.0174222,"Skip rules allow us to extend a substring such that it covers tokens which do not correspond to a graph fragment (i.e., their AM term is ⊥), introducing IGNORE edges. After all possible items have been derived, we extract the best well-typed tree from the item of the form ([1, n], r, τ ) with the highest score, where τ = [ ]. Because we keep track of the head indices, the projective decoder is a bilexical parsing algorithm, and shares a parsing complexity of O(n5 ) with other bilexical algorithms such as the Collins parser. It could be improved to a complexity of O(n4 ) using the algorithm of Eisner and Satta (1999). 6.2 Fixed-tree decoder The fixed-tree decoder computes the best unlabeled dependency tree tr for w, using the edge scores ω(i → k), and then computes the best AM dependency tree for w whose unlabeled version is tr . The Chu-Liu-Edmonds algorithm produces a forest of dependency trees, which we want to combine into tr . We choose the tree whose root r has the highest score for being the root of the AM dependency tree and make the roots of all others children of r. At this point, the shape of tr is fixed. We choose 1836 s = ω(G[i]) Init (i, ∅, τ (G)) : s cal phenomena such as control and coordi"
P18-1170,S16-1186,0,0.141531,"Missing"
P18-1170,P14-1134,0,0.571487,"ore idea of this paper is to parse a string into a graph by instead parsing a string into a dependencystyle tree representation of the graph’s compositional structure, represented as terms of the ApplyModify (AM) algebra (Groschwitz et al., 2017). The values of the AM algebra are annotated so[s] person ARG0 s m manner sleep ARG0 G0 AR s write G1 3 want AR scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, gui"
P18-1170,P17-1043,0,0.250353,"o[s] person ARG0 s m manner sleep ARG0 G0 AR s write G1 3 want AR scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs. Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015). In contrast to these, our"
P18-1170,W17-6810,1,0.676651,"ree. For instance, grammar-based approaches (Peng et al., 2015; Artzi et al., 2015) require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in far more ways than trees. Neural sequence-to-sequence models, which do very well on string-to-tree parsing (Vinyals et al., 2014), can be applied to AMRs but face the challenge that graphs cannot easily be represented as sequences (van Noord and Bos, 2017a,b). In this paper, we tackle this challenge by making the compositional structure of the AMR explicit. As in our previous work, Groschwitz et al. (2017), we view an AMR as consisting of atomic graphs representing the meanings of the individual words, which were combined compositionally using linguistically motivated operations for combining a head with its arguments and modifiers. We represent this structure as terms over the AM algebra as defined in Groschwitz et al. (2017). This previous work had no parser; here we show that the terms of the AM algebra can be viewed as dependency trees over the string, and we train a dependency parser to map strings into such trees, which we then evaluate into AMRs in a postprocessing step. The dependency p"
P18-1170,Q16-1023,0,0.513701,"algebra as defined in Groschwitz et al. (2017). This previous work had no parser; here we show that the terms of the AM algebra can be viewed as dependency trees over the string, and we train a dependency parser to map strings into such trees, which we then evaluate into AMRs in a postprocessing step. The dependency parser relies on type information, which encodes the semantic valencies of the atomic graphs, to guide its decisions. More specifically, we combine a neural supertagger for identifying the elementary graphs for the individual words with a neural dependency model along the lines of Kiperwasser and Goldberg (2016) for identifying the operations of the algebra. One key challenge is that the resulting term of the AM algebra must be semantically well-typed. This makes the decoding problem NP-complete. We present two approximation algorithms: one which takes the unlabeled dependency tree as given, and one which assumes that all dependencies are projective. We evaluate on two data sets, achieving state-of-the-art results on one and near state-of-theart results on the other (Smatch f-scores of 71.0 and 70.2 respectively). Our approach clearly outperforms strong but non-compositional baselines. Plan of the pa"
P18-1170,D17-1160,0,0.0348753,"that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require. More generally, our use of semantic types to restrict our parser is reminiscent of Kwiatkowski et al. (2010), Krishnamurthy et al. (2017) and Zhang et al. (2017), and the idea of deriving semantic representations from dependency trees is also present in Reddy et al. (2017). sound Figure 1: Elementary as-graphs Gwant , Gwriter , Gsleep , and Gsound for the words “want”, “writer”, “sleep”, and “soundly” respectively. graphs, or as-graphs: directed graphs with node and edge labels in which certain nodes have been designated as sources (Courcelle and Engelfriet, 2012) and annotated with type information. Some examples of as-graphs are shown in Fig. 1. Each as-graph has exactly one root, indicated by the bold outline. The sources ar"
P18-1170,D10-1119,0,0.0525177,"ependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require. More generally, our use of semantic types to restrict our parser is reminiscent of Kwiatkowski et al. (2010), Krishnamurthy et al. (2017) and Zhang et al. (2017), and the idea of deriving semantic representations from dependency trees is also present in Reddy et al. (2017). sound Figure 1: Elementary as-graphs Gwant , Gwriter , Gsleep , and Gsound for the words “want”, “writer”, “sleep”, and “soundly” respectively. graphs, or as-graphs: directed graphs with node and edge labels in which certain nodes have been designated as sources (Courcelle and Engelfriet, 2012) and annotated with type information. Some examples of as-graphs are shown in Fig. 1. Each as-graph has exactly one root, indicated by the"
P18-1170,P18-1037,0,0.156382,"thods from supertagging and dependency parsing to map a string into a well-typed AM term, which it then evaluates into an AMR. The AM term represents the compositional semantic structure of the AMR explicitly, allowing us to use standard treebased parsing techniques. The projective parser currently computes the complete parse chart. In future work, we will speed it up through the use of pruning techniques. We will also look into more principled methods for splitting the AMRs into elementary as-graphs to replace our hand-crafted heuristics. In particular, advanced methods for alignments, as in Lyu and Titov (2018), seem promising. Overcoming the need for heuristics also seems to be a crucial ingredient for applying our method to other semantic representations. Acknowledgements We would like to thank the anonymous reviewers for their comments. We thank Stefan Gr¨unewald for his contribution to our PyTorch implementation, and want to acknowledge the inspiration obtained from Nguyen et al. (2017). We also extend our thanks to the organizers and participants of the Oslo CAS Meaning Construction workshop on Universal Dependencies. This work was supported by the DFG grant KO 2916/2-1 and a Macquarie Universi"
P18-1170,P14-5010,0,0.00502875,"spell out the tokens of “Agatha Christie”, and a link to a wiki entry. Before training, we replace each “name” node, its children, and the corresponding span in the sentence with a special NAME token, and we completely remove wiki edges. In this example, this leaves us with only a “person” and a NAME node. Further, we replace numbers and some date patterns with NUMBER and DATE tokens. On the training data this is straightforward, since names and dates are explicitly annotated in the AMR. At evaluation time, we detect dates and numbers with regular expressions, and names with Stanford CoreNLP (Manning et al., 2014). We also use Stanford CoreNLP for our POS tags. Each elementary as-graph generated by the procedure of Section 4.2 has a unique node whose label corresponds most closely to the aligned word (e.g. the “want” node in Gwant and the “write” node in Gwriter ). We replace these node labels with LEX in preprocessing, reducing the number of different elementary as-graphs from 28730 to 2370. We factor the supertagger model of Section 5.1 such that the unlexicalized version of G[i] and the label for LEX are predicted separately. At evaluation, we re-lexicalize all LEX nodes in the predicted AMR. For wo"
P18-1170,D10-1004,0,0.0352221,"· p(τ |pi , pi−1 ) for any τ 6= τi and δ is a hyperparameter controlling the bias towards the aligned supertag. We train the model using K&G’s original DyNet implementation. Their algorithm uses a hinge loss function, which maximizes the score difference between the gold dependency tree and the best predicted dependency tree, and therefore requires parsing each training instance in each iteration. Because the AM dependency trees are highly non-projective, we replaced the projective parser used in the off-the-shelf implementation by the Chu-Liu-Edmonds algorithm implemented in the TurboParser (Martins et al., 2010), improving the LAS on the development set by 30 points. 5.3 ω(G[i]) = log softmax(W · vi + b) ... ... Local edge model We also trained a local edge score model, which uses a cross-entropy rather than a hinge loss and therefore avoids the repeated parsing at training 1835 time. Instead, we follow the intuition that every node in a dependency tree has at most one incoming edge, and train the model to score the correct incoming edge as high as possible. This model takes inputs xi = (wi , pi ). We define the edge and edge label scores as in Section 5.2, with tanh replaced by ReLU. We further add"
P18-1170,S16-1166,0,0.0538152,"onal Linguistics (Long Papers), pages 1831–1841 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics The AM algebra A core idea of this paper is to parse a string into a graph by instead parsing a string into a dependencystyle tree representation of the graph’s compositional structure, represented as terms of the ApplyModify (AM) algebra (Groschwitz et al., 2017). The values of the AM algebra are annotated so[s] person ARG0 s m manner sleep ARG0 G0 AR s write G1 3 want AR scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by ex"
P18-1170,S17-2090,0,0.0416879,"stics (Long Papers), pages 1831–1841 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics The AM algebra A core idea of this paper is to parse a string into a graph by instead parsing a string into a dependencystyle tree representation of the graph’s compositional structure, represented as terms of the ApplyModify (AM) algebra (Groschwitz et al., 2017). The values of the AM algebra are annotated so[s] person ARG0 s m manner sleep ARG0 G0 AR s write G1 3 want AR scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017). Methods from dependency parsing have been shown to be very successful for AMR parsing. For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compos"
P18-1170,K17-3014,1,0.895076,"Missing"
P18-1170,K15-1004,0,0.300822,"h achieve state-of-the-art accuracy and outperform strong baselines. 1 Introduction Over the past few years, Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) have become a popular target representation for semantic parsing. AMRs are graphs which describe the predicate-argument structure of a sentence. Because they are graphs and not trees, they can capture reentrant semantic relations, such as those induced by control verbs and coordination. However, it is technically much more challenging to parse a string into a graph than into a tree. For instance, grammar-based approaches (Peng et al., 2015; Artzi et al., 2015) require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in far more ways than trees. Neural sequence-to-sequence models, which do very well on string-to-tree parsing (Vinyals et al., 2014), can be applied to AMRs but face the challenge that graphs cannot easily be represented as sequences (van Noord and Bos, 2017a,b). In this paper, we tackle this challenge by making the compositional structure of the AMR explicit. As in our previous work, Groschwitz et al. (2017), we view an AMR as consisting of atom"
P18-1170,D14-1162,0,0.0796083,"Missing"
P18-1170,D17-1009,0,0.0379769,"Missing"
P18-1170,W17-7306,0,0.0482189,"Missing"
P18-1170,N15-1040,0,0.276438,"d dependency parser. Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results. We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs. Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015). In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016). The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015). In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details). As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require. More generall"
P18-1170,D17-1125,0,0.0470735,"Missing"
P18-2099,J99-2004,0,0.483884,"Missing"
P18-2099,N06-1022,0,0.10422,"Missing"
P18-2099,P13-1091,0,0.0260834,"re 5: TAG parsing speed as a function of sentence length. 5 Conclusion Chart constraints, computed by a neural tagger, robustly accelerate parsers both for PCFGs and for more expressive formalisms such as TAG. Even highly effective pruning techniques such as CTF and supertagging can be further improved through chart constraints, indicating that they target different sources of complexity. By interpreting chart constraints in terms of allowable chart items, we can apply them to arbitrary chart parsers, including ones for grammar formalisms that describe objects other than strings, e.g. graphs (Chiang et al., 2013; Groschwitz et al., 2015). The primary challenge here is to develop a high-precision tagger that identifies allowable subgraphs, which requires moving beyond LSTMs. An intriguing question is to what extent chart constraints can speed up parsing algorithms that do not use charts. It is known that chart constraints can speed up context-free shift-reduce parsers (Chen et al., 2017). It would be interesting to see how a neural parser, such as (Dyer et al., 2016), would benefit from chart constraints calculated by a neural tagger. Acknowledgments. We are grateful to Jonas Groschwitz, Christoph Tei"
P18-2099,J07-4004,0,0.140073,"Missing"
P18-2099,J03-4003,0,0.455504,"Missing"
P18-2099,N16-1024,0,0.0672174,"Missing"
P18-2099,E17-3008,1,0.826965,"with Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)), a grammar formalism which generalizes PCFG, TAG, and many others. Chart parsers for IRTG describe substructures of the input object as states of a finite tree automaton D. When we encode a PCFG as an IRTG, these states are of the form [i, k]; when we encode a TAG grammar, they are of the form [i, j, k, l]. Thus chart constraints describe allowable states of this automaton, and we can prune the chart simply by restricting D to rules that use only allowable states. In the experiments below, we use the Alto IRTG parser (Gontrum et al., 2017), modified to implement chart constraints as allowable states. We convert the PCFG and TAG grammars into IRTG grammars and use the parsing algorithms of Groschwitz et al. (2016): “condensed intersection” for PCFG parsing and the “sibling-finder” algorithm for TAG. Both of these implement the CKY algorithm and compute charts which correspond to the parsing schemata sketched above. 3 Neural chart-constraint tagging Roark et al. predict the begin and end constraints for a string w using a log-linear model with manually designed features. We replace this with a neural tagger (Fig. 1a), which reads"
P18-2099,P16-1192,1,0.856702,"substructures of the input object as states of a finite tree automaton D. When we encode a PCFG as an IRTG, these states are of the form [i, k]; when we encode a TAG grammar, they are of the form [i, j, k, l]. Thus chart constraints describe allowable states of this automaton, and we can prune the chart simply by restricting D to rules that use only allowable states. In the experiments below, we use the Alto IRTG parser (Gontrum et al., 2017), modified to implement chart constraints as allowable states. We convert the PCFG and TAG grammars into IRTG grammars and use the parsing algorithms of Groschwitz et al. (2016): “condensed intersection” for PCFG parsing and the “sibling-finder” algorithm for TAG. Both of these implement the CKY algorithm and compute charts which correspond to the parsing schemata sketched above. 3 Neural chart-constraint tagging Roark et al. predict the begin and end constraints for a string w using a log-linear model with manually designed features. We replace this with a neural tagger (Fig. 1a), which reads the input sentence token by token and jointly predicts for each string position whether it is in B and/or E. Technically, our tagger is a two-layer bidirectional LSTM (Kiperwas"
P18-2099,P15-1143,1,0.853851,"ed as a function of sentence length. 5 Conclusion Chart constraints, computed by a neural tagger, robustly accelerate parsers both for PCFGs and for more expressive formalisms such as TAG. Even highly effective pruning techniques such as CTF and supertagging can be further improved through chart constraints, indicating that they target different sources of complexity. By interpreting chart constraints in terms of allowable chart items, we can apply them to arbitrary chart parsers, including ones for grammar formalisms that describe objects other than strings, e.g. graphs (Chiang et al., 2013; Groschwitz et al., 2015). The primary challenge here is to develop a high-precision tagger that identifies allowable subgraphs, which requires moving beyond LSTMs. An intriguing question is to what extent chart constraints can speed up parsing algorithms that do not use charts. It is known that chart constraints can speed up context-free shift-reduce parsers (Chen et al., 2017). It would be interesting to see how a neural parser, such as (Dyer et al., 2016), would benefit from chart constraints calculated by a neural tagger. Acknowledgments. We are grateful to Jonas Groschwitz, Christoph Teichmann, Stefan Thater, and"
P18-2099,D17-1180,0,0.0410222,"Missing"
P18-2099,Q16-1023,0,0.0242118,". (2016): “condensed intersection” for PCFG parsing and the “sibling-finder” algorithm for TAG. Both of these implement the CKY algorithm and compute charts which correspond to the parsing schemata sketched above. 3 Neural chart-constraint tagging Roark et al. predict the begin and end constraints for a string w using a log-linear model with manually designed features. We replace this with a neural tagger (Fig. 1a), which reads the input sentence token by token and jointly predicts for each string position whether it is in B and/or E. Technically, our tagger is a two-layer bidirectional LSTM (Kiperwasser and Goldberg, 2016; Lewis et al., 2016; Kummerfeld and Klein, 2017). In each time step, it reads as input a pair xi = (wi , pi ) of one-hot encodings of a word wi and a POS tag pi , and embeds them into dense vectors (using pretrained GloVe word embeddings (Pennington et al., 2014) for wi and learned POS tag embeddings for pi ). It then computes the probability that a constituent begins (ends) at position i from the concatenation vi = viF 2 ◦ viB2 of the hidden states v F 2 and v B2 of the second forward and backward LSTM at position i: acc 97.6 96.7 93.7 B prec 97.4 98.8 99.6 recall 97.8 95.2 87.9 acc 98.1 97."
P18-2099,W11-2902,1,0.818488,"e (see Fig. 1b): [X , i, j, k, l] [Y, j, r, s, k] [Y 0 , i, r, s, l] Assuming begin and end constraints as above, we define allowable TAG items as follows. First, an item [X , i, j, k, l] is not allowable if i ∈ B or l ∈ E. Second, if j and k are not NULL, then the item is not allowable if j ∈ B or k ∈ E (else there will be 627 no constituent from j to k at which the item could be adjoined). Otherwise, the item is allowable. 2.4 θ 0.5 0.9 0.99 Allowable states for IRTG parsing Allowable items have a particularly direct interpretation when parsing with Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)), a grammar formalism which generalizes PCFG, TAG, and many others. Chart parsers for IRTG describe substructures of the input object as states of a finite tree automaton D. When we encode a PCFG as an IRTG, these states are of the form [i, k]; when we encode a TAG grammar, they are of the form [i, j, k, l]. Thus chart constraints describe allowable states of this automaton, and we can prune the chart simply by restricting D to rules that use only allowable states. In the experiments below, we use the Alto IRTG parser (Gontrum et al., 2017), modified to implement chart constraints as allowabl"
P18-2099,Q17-1031,0,0.0283326,"and the “sibling-finder” algorithm for TAG. Both of these implement the CKY algorithm and compute charts which correspond to the parsing schemata sketched above. 3 Neural chart-constraint tagging Roark et al. predict the begin and end constraints for a string w using a log-linear model with manually designed features. We replace this with a neural tagger (Fig. 1a), which reads the input sentence token by token and jointly predicts for each string position whether it is in B and/or E. Technically, our tagger is a two-layer bidirectional LSTM (Kiperwasser and Goldberg, 2016; Lewis et al., 2016; Kummerfeld and Klein, 2017). In each time step, it reads as input a pair xi = (wi , pi ) of one-hot encodings of a word wi and a POS tag pi , and embeds them into dense vectors (using pretrained GloVe word embeddings (Pennington et al., 2014) for wi and learned POS tag embeddings for pi ). It then computes the probability that a constituent begins (ends) at position i from the concatenation vi = viF 2 ◦ viB2 of the hidden states v F 2 and v B2 of the second forward and backward LSTM at position i: acc 97.6 96.7 93.7 B prec 97.4 98.8 99.6 recall 97.8 95.2 87.9 acc 98.1 97.2 93.0 E prec 98.7 99.4 99.7 recall 98.7 96.7 90."
P18-2099,D14-1162,0,0.0797143,"Missing"
P18-2099,C92-2065,0,0.725803,"Missing"
P18-2099,J12-4002,0,0.035676,"Missing"
P18-2099,W17-6317,1,0.884628,"Missing"
P18-2099,C10-2168,0,0.0634313,"Missing"
P18-2099,N09-2047,0,\N,Missing
P19-1008,P16-1231,0,0.0362824,"at derivations should be aligned (which is natural for a lexicalized grammar), Thm. 1 implies that CCG with lambda-style semantic composition is more semantically expressive than with unification-style composition. Indeed, lambda-style compositional mechanisms are the dominant approach in CCG (Steedman, 2001; Baldridge and Kruijff, 2002; Artzi et al., 2015). Furthermore, under the alignment assumptions of Section 4, no unification-style compositional mechanism can describe string-meaning relations like CSD. This includes neural models. For instance, most transition-based parsers (Nivre, 2008; Andor et al., 2016; Dyer et al., 2016) are projective, in that the parsing operations can only concatenate two substrings on the top of the stack if they are adjacent in the string. Such transition systems can therefore not be extended to transitionbased semantic parsers (Damonte et al., 2017) without (a) losing expressive capacity, (b) giving up compositionality, (c) adding mechanisms for non-projectivity (G´omez-Rodr´ıguez et al., 2018), or (d) using a lambda-style semantic algebra. Thus our result clarifies how to build an effective and accurate semantic parser. We have focused on whether a grammar formalism"
P19-1008,E17-1051,0,0.0170999,"nant approach in CCG (Steedman, 2001; Baldridge and Kruijff, 2002; Artzi et al., 2015). Furthermore, under the alignment assumptions of Section 4, no unification-style compositional mechanism can describe string-meaning relations like CSD. This includes neural models. For instance, most transition-based parsers (Nivre, 2008; Andor et al., 2016; Dyer et al., 2016) are projective, in that the parsing operations can only concatenate two substrings on the top of the stack if they are adjacent in the string. Such transition systems can therefore not be extended to transitionbased semantic parsers (Damonte et al., 2017) without (a) losing expressive capacity, (b) giving up compositionality, (c) adding mechanisms for non-projectivity (G´omez-Rodr´ıguez et al., 2018), or (d) using a lambda-style semantic algebra. Thus our result clarifies how to build an effective and accurate semantic parser. We have focused on whether a grammar formalism is projective or not, while holding the semantic algebra fixed. In the future, it would be interesting to explore how a unification-style compositional mechanism can be converted to a lambdastyle mechanism with unbounded placeholders. This would allow us to specify and train"
P19-1008,P02-1041,0,0.120701,"rmalism that has been widely used in semantic parsing (Bos, 2008; Artzi et al., 2015; Lewis et al., 2016). While a potentially infinite set of syntactic categories can be used in the parses of a single CCG grammar, CCG derivations are still projective in our sense. Thus, if one assumes that derivations should be aligned (which is natural for a lexicalized grammar), Thm. 1 implies that CCG with lambda-style semantic composition is more semantically expressive than with unification-style composition. Indeed, lambda-style compositional mechanisms are the dominant approach in CCG (Steedman, 2001; Baldridge and Kruijff, 2002; Artzi et al., 2015). Furthermore, under the alignment assumptions of Section 4, no unification-style compositional mechanism can describe string-meaning relations like CSD. This includes neural models. For instance, most transition-based parsers (Nivre, 2008; Andor et al., 2016; Dyer et al., 2016) are projective, in that the parsing operations can only concatenate two substrings on the top of the stack if they are adjacent in the string. Such transition systems can therefore not be extended to transitionbased semantic parsers (Damonte et al., 2017) without (a) losing expressive capacity, (b)"
P19-1008,N16-1024,0,0.0218767,"d be aligned (which is natural for a lexicalized grammar), Thm. 1 implies that CCG with lambda-style semantic composition is more semantically expressive than with unification-style composition. Indeed, lambda-style compositional mechanisms are the dominant approach in CCG (Steedman, 2001; Baldridge and Kruijff, 2002; Artzi et al., 2015). Furthermore, under the alignment assumptions of Section 4, no unification-style compositional mechanism can describe string-meaning relations like CSD. This includes neural models. For instance, most transition-based parsers (Nivre, 2008; Andor et al., 2016; Dyer et al., 2016) are projective, in that the parsing operations can only concatenate two substrings on the top of the stack if they are adjacent in the string. Such transition systems can therefore not be extended to transitionbased semantic parsers (Damonte et al., 2017) without (a) losing expressive capacity, (b) giving up compositionality, (c) adding mechanisms for non-projectivity (G´omez-Rodr´ıguez et al., 2018), or (d) using a lambda-style semantic algebra. Thus our result clarifies how to build an effective and accurate semantic parser. We have focused on whether a grammar formalism is projective or no"
P19-1008,W13-2322,0,0.114025,"Missing"
P19-1008,E03-1030,0,0.0526055,"Missing"
P19-1008,P18-1170,1,0.889863,"ot. Lambda calculus (Montague, 1974; Blackburn and Bos, 2005) assumes that the number of placeholders (lambda-bound variables) can grow unboundedly with the length and complexity of the sentence. By contrast, many methods which are based on unification (Copestake et al., 2001) or graph merging (Courcelle and Engelfriet, 2012; Chiang et al., 2013) assume a fixed set of placeholders, i.e. the number of placeholders is bounded. Methods based on bounded placeholders are popular both in the design of hand-written grammars (Bender et al., 2002) and in semantic parsing for graphs (Peng et al., 2015; Groschwitz et al., 2018). However, it is not clear that all relations between language and semantic representations can be expressed with a bounded number of placeholders. The situation is particularly challenging 65 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 65–79 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics <> (a) foundations in Section 3. We will then prove the reduced semantic expressive capacity for aligned generative devices in Section 4 and for CFTGs in Section 5. We conclude with a discussion of the practical im"
P19-1008,W02-1502,0,0.194859,"iffer in whether they assume that the number of placeholders is bounded or not. Lambda calculus (Montague, 1974; Blackburn and Bos, 2005) assumes that the number of placeholders (lambda-bound variables) can grow unboundedly with the length and complexity of the sentence. By contrast, many methods which are based on unification (Copestake et al., 2001) or graph merging (Courcelle and Engelfriet, 2012; Chiang et al., 2013) assume a fixed set of placeholders, i.e. the number of placeholders is bounded. Methods based on bounded placeholders are popular both in the design of hand-written grammars (Bender et al., 2002) and in semantic parsing for graphs (Peng et al., 2015; Groschwitz et al., 2018). However, it is not clear that all relations between language and semantic representations can be expressed with a bounded number of placeholders. The situation is particularly challenging 65 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 65–79 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics <> (a) foundations in Section 3. We will then prove the reduced semantic expressive capacity for aligned generative devices in Section"
P19-1008,P18-1248,0,0.0270571,"Missing"
P19-1008,W08-2222,0,0.0591728,"y-edges generated by the lower part of a separated derivation. The full proof is in the appendix; we sketch the main ideas here. Let p denote the pumping height of T. There is a maximal number of string tokens and edges that a context of height at most p can generate under 72 Lemma 9. For any t ∈ T, t is x, y, l0 -separated for some x/y ∈ Sep. mantic expressive capacity. However, Thm. 1 may have a clearer practical impact on the development of compositional semantic parsers. Consider, for instance, the case of CCG, a lexicalized grammar formalism that has been widely used in semantic parsing (Bos, 2008; Artzi et al., 2015; Lewis et al., 2016). While a potentially infinite set of syntactic categories can be used in the parses of a single CCG grammar, CCG derivations are still projective in our sense. Thus, if one assumes that derivations should be aligned (which is natural for a lexicalized grammar), Thm. 1 implies that CCG with lambda-style semantic composition is more semantically expressive than with unification-style composition. Indeed, lambda-style compositional mechanisms are the dominant approach in CCG (Steedman, 2001; Baldridge and Kruijff, 2002; Artzi et al., 2015). Furthermore, u"
P19-1008,P13-1091,0,0.317867,"ode of a syntax tree. These partial semantic representations usually contain placeholders at which arguments and modifiers are attached in later composition steps. Approaches to semantic parsing differ in whether they assume that the number of placeholders is bounded or not. Lambda calculus (Montague, 1974; Blackburn and Bos, 2005) assumes that the number of placeholders (lambda-bound variables) can grow unboundedly with the length and complexity of the sentence. By contrast, many methods which are based on unification (Copestake et al., 2001) or graph merging (Courcelle and Engelfriet, 2012; Chiang et al., 2013) assume a fixed set of placeholders, i.e. the number of placeholders is bounded. Methods based on bounded placeholders are popular both in the design of hand-written grammars (Bender et al., 2002) and in semantic parsing for graphs (Peng et al., 2015; Groschwitz et al., 2018). However, it is not clear that all relations between language and semantic representations can be expressed with a bounded number of placeholders. The situation is particularly challenging 65 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 65–79 c Florence, Italy, July 28 - A"
P19-1008,W15-0127,1,0.838843,"ings “es Huus” and “aastriiche”, which are then wrapped around “l¨ond h¨alfed” further up in the derivation. If the grammar is projective, then for any context C there exist two strings left(C) and right(C) such that for any tree t, yd(C[t]) = left(C)·yd(t)· right(C). 3.2 3.3 The HR algebra The specific unification-style semantic algebra we use in this paper is the HR algebra (Courcelle and Engelfriet, 2012). This choice encompasses much of the recent literature on compositional semantic parsing with graphs, based e.g. on Hyperedge Replacement Grammars (Chiang et al., 2013; Peng et al., 2015; Koller, 2015) and the AM algebra (Groschwitz et al., 2018). The values of the HR algebra are s-graphs: directed, edge-labeled graphs, some of whose nodes may be designated as sources, written in angle brackets. S-graphs can be combined using the forget, rename, and merge operations. Rename rena→b changes an a-source node into a b-source node. Forget fa makes it so the a-source node in the s-graph is no longer a source node. Merge || combines two s-graphs while unifying nodes with the same source annotation. For instance, the sARG1 Hans graphs hrti −−→ hoi and hoi are merged into ARG0 Hans . hrti −−→ hoi Th"
P19-1008,J08-4003,0,\N,Missing
P19-1008,D15-1198,0,\N,Missing
P19-1008,K15-1004,0,\N,Missing
P19-1008,N16-1026,0,\N,Missing
P19-1008,P01-1019,0,\N,Missing
P19-1450,W15-0126,1,0.906506,"Missing"
P19-1450,W13-2322,0,0.407625,"ccuracy further, setting new states of the art on DM, PAS, PSD, AMR 2015 and EDS. 1 Introduction Over the past few years, a wide variety of semantic graphbanks have become available. Although these corpora all pair natural-language sentences with graph-based semantic representations, they differ greatly in the design of these graphs (Kuhlmann and Oepen, 2016). Some, in particular the DM, PAS, and PSD corpora of the SemEval shared task on Semantic Dependency Parsing (Oepen et al., 2015), use the tokens of the sentence as nodes and connect them with semantic relations. By contrast, the AMRBank (Banarescu et al., 2013) represents the meaning of each word using a nontrivial concept graph; the EDS graphbank (Flickinger et al., 2017) encodes MRS representations (Copestake et al., 2005) as graphs with a many-to-many relation between tokens and nodes. In EDS, graph nodes are explicitly aligned with the tokens; in AMR, the alignments are implicit. The graphbanks also exhibit structural differences in their modeling of e.g. coordination or copula. Because of these differences in annotation schemes, the best performing semantic parsers are typically designed for one or very few specific graphbanks. For instance, th"
P19-1450,W17-6810,1,0.565359,"s to predict it through neural dependency parsing and supertagging. We show how to heuristically compute the latent compositional structures of the graphs of DM, PAS, PSD, and EDS. This base parser already performs near the state of the art across all six graphbanks. We improve it further by using pretrained BERT embeddings (Devlin et al., 2019) and multi-task learning. With this, we set new states of the art on DM, PAS, PSD, AMR 2015, as well as (among systems that do not use specialized knowledge about the corpus) on EDS. 2 Semantic parsing with the AM algebra The Apply-Modify (AM) Algebra (Groschwitz et al., 2017; Groschwitz, 2019) builds graphs from smaller graph fragments called as-graphs. Fig. 1b shows some as-graphs from which the AMR in Fig. 1a can be constructed. Take for example the graph Gwant . Some of its nodes are marked with red sources, here S and O. These represent ‘argument slots’ to be filled. The O-source in Gwant is annotated with type [S], which will be explained below. Further, in each as-graph, one node is marked as a 4576 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4576–4585 c Florence, Italy, July 28 - August 2, 2019. 2019 Assoc"
P19-1450,P18-1170,1,0.836186,"to EDS or AMR. Conversely, top AMR parsers (Lyu and Titov, 2018) invest heavily into identifying AMR-specific alignments and concepts, which may not be useful in other graphbanks. Hershcovich et al. (2018) parse across different semantic graphbanks (UCCA, DM, AMR), but focus on UCCA and do poorly on DM. The system of Buys and Blunsom (2017) set a state of the art on EDS at the time, but does poorly on AMR. In this paper, we present a single semantic parser that does very well across all of DM, PAS, PSD, EDS and AMR (2015 and 2017). Our system is based on the compositional neural AMR parser of Groschwitz et al. (2018), which represents each graph with its compositional tree structure and learns to predict it through neural dependency parsing and supertagging. We show how to heuristically compute the latent compositional structures of the graphs of DM, PAS, PSD, and EDS. This base parser already performs near the state of the art across all six graphbanks. We improve it further by using pretrained BERT embeddings (Devlin et al., 2019) and multi-task learning. With this, we set new states of the art on DM, PAS, PSD, AMR 2015, as well as (among systems that do not use specialized knowledge about the corpus) o"
P19-1450,P18-1035,0,0.137318,"The graphbanks also exhibit structural differences in their modeling of e.g. coordination or copula. Because of these differences in annotation schemes, the best performing semantic parsers are typically designed for one or very few specific graphbanks. For instance, the currently best system for DM, PAS, and PSD (Dozat and Manning, ∗ Equal contribution 2018) assumes dependency graphs and cannot be directly applied to EDS or AMR. Conversely, top AMR parsers (Lyu and Titov, 2018) invest heavily into identifying AMR-specific alignments and concepts, which may not be useful in other graphbanks. Hershcovich et al. (2018) parse across different semantic graphbanks (UCCA, DM, AMR), but focus on UCCA and do poorly on DM. The system of Buys and Blunsom (2017) set a state of the art on EDS at the time, but does poorly on AMR. In this paper, we present a single semantic parser that does very well across all of DM, PAS, PSD, EDS and AMR (2015 and 2017). Our system is based on the compositional neural AMR parser of Groschwitz et al. (2018), which represents each graph with its compositional tree structure and learns to predict it through neural dependency parsing and supertagging. We show how to heuristically compute"
P19-1450,N18-1135,0,0.0772896,"xperimented with randomized heuristics on DM. The F-score dropped by up to 18 points. BERT. The use of BERT embeddings is highly effective across the board. We set a new state of the art (without gold syntax) on all graphbanks except AMR 2017; note that Zhang et al. (2019) also use BERT. The improvement is particularly pronounced in the out-of-domain evaluations, illustrating BERT’s ability to transfer across domains. Multi-task learning. Multi-task learning has been shown to substantially improve accuracy on various semantic parsing tasks (Stanovsky and Dagan, 2018; Hershcovich et al., 2018; Peng et al., 2018). It is particularly easy to apply here, because we have converted all graphbanks into a uniform format (supertags and AM dependency trees). We explored several multi-task approaches during development, namely Freda (Daum´e III, 2007; Peng et al., 2017), the Freda generalization of Lu et al. (2016) and the method of Stymne et al. (2018). We found Freda to work best and use it for evaluation. Our setup compares most directly to Peng et al.’s “Freda1” model, concatenating the output of a graphbank-specific BiLSTM with that of a shared BiLSTM, using graphbank-specific MLPs for supertags and edges"
P19-1450,D14-1162,0,0.0880539,"Missing"
P19-1450,D18-1263,0,0.0998232,"ng and sourcenaming heuristics from Section 3.2, we experimented with randomized heuristics on DM. The F-score dropped by up to 18 points. BERT. The use of BERT embeddings is highly effective across the board. We set a new state of the art (without gold syntax) on all graphbanks except AMR 2017; note that Zhang et al. (2019) also use BERT. The improvement is particularly pronounced in the out-of-domain evaluations, illustrating BERT’s ability to transfer across domains. Multi-task learning. Multi-task learning has been shown to substantially improve accuracy on various semantic parsing tasks (Stanovsky and Dagan, 2018; Hershcovich et al., 2018; Peng et al., 2018). It is particularly easy to apply here, because we have converted all graphbanks into a uniform format (supertags and AM dependency trees). We explored several multi-task approaches during development, namely Freda (Daum´e III, 2007; Peng et al., 2017), the Freda generalization of Lu et al. (2016) and the method of Stymne et al. (2018). We found Freda to work best and use it for evaluation. Our setup compares most directly to Peng et al.’s “Freda1” model, concatenating the output of a graphbank-specific BiLSTM with that of a shared BiLSTM, using g"
P19-1450,S14-2008,0,\N,Missing
P19-1450,P14-5010,0,\N,Missing
P19-1450,W11-2927,0,\N,Missing
P19-1450,P13-2131,0,\N,Missing
P19-1450,J16-4009,0,\N,Missing
P19-1450,E17-2039,0,\N,Missing
P19-1450,P18-1038,0,\N,Missing
P19-1450,P18-2077,0,\N,Missing
P19-1450,N19-1423,0,\N,Missing
P19-1450,P07-1033,0,\N,Missing
P19-1450,D16-1095,0,\N,Missing
P19-4002,W13-2322,0,0.637356,"ive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the sub"
P19-4002,C16-1056,0,0.056597,"Missing"
P19-4002,basile-etal-2012-developing,0,0.0527628,"hen apply dynamic programming to search for the best graph, as well as transition-based methods, which learn to make individual parsing decisions for each token in the sentence. Some neural techniques also make use of an encoder-decoder architecture, as in neural machine translation. • Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013); • Graph-Based Minimal Recursion Semantics (EDS and DMRS; Oepen and Lønning, 2006; Copestake, 2009); • Abstract Meaning Representation (AMR; Banarescu et al., 2013); • Non-Graph Representations: Discourse Representation Structures (DRS; Basile et al., 2012); Compositionality Semantic parsers also differ with respect to whether they assume that the graph-based semantic representations are constructed compositionally. Some approaches follow standard linguistic practice in assuming that the graphs have a latent compositional structure and try to reconstruct it explicitly or implicitly during parsing. Others are more agnostic and simply predict the edges of the target graph without regard to such linguistic assumptions. • Contrastive review of selected examples across frameworks; • Availability of training and evaluation data; shared tasks; state-of"
P19-4002,P14-1134,0,0.0312129,"state-of-the-art empirical results. (4) Parsing into Semantic Graphs Structural information Finally, semantic parsers differ with respect to how structure information is represented. Some model the target graph directly, whereas others use probability models that score a tree which evaluates to the target graph (e.g. a syntactic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexan"
P19-4002,C04-1180,0,0.239598,"Missing"
P19-4002,P18-1170,1,0.908645,"Missing"
P19-4002,P17-1112,0,0.504659,"enabling lexical decomposition (e.g. of causatives or comparatives). Frameworks instantiating this flavor of semantic graphs include Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the su"
P19-4002,hajic-etal-2012-announcing,0,0.136582,"Missing"
P19-4002,S16-1167,0,0.0477873,"tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this graph flavor include CCG word–word dependencies (CCD; Hockenmaier and Steedman, 2007), Enju Predicate– Argument Structures (PAS; Miyao and Tsujii, 7 3 Processing Semantic Graphs three hours of presentation. The references below are illustrative of the content in each block; in the tutorial itself, we will present one or two approaches per block in detail while treating others more superficially. The creation of large-scale, high-quality semantic graph banks has driven research on semantic parsing, where a system is trained to map from natura"
P19-4002,J94-1007,0,0.785093,"Missing"
P19-4002,P17-1104,0,0.0797035,"ed methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the"
P19-4002,S15-2153,1,0.918102,"Missing"
P19-4002,P18-1035,0,0.283731,"satives or comparatives). Frameworks instantiating this flavor of semantic graphs include Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release"
P19-4002,S14-2008,1,0.898615,"Missing"
P19-4002,J07-3004,0,0.0229765,"in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this graph flavor include CCG word–word dependencies (CCD; Hockenmaier and Steedman, 2007), Enju Predicate– Argument Structures (PAS; Miyao and Tsujii, 7 3 Processing Semantic Graphs three hours of presentation. The references below are illustrative of the content in each block; in the tutorial itself, we will present one or two approaches per block in detail while treating others more superficially. The creation of large-scale, high-quality semantic graph banks has driven research on semantic parsing, where a system is trained to map from natural-language sentences to graphs. There is now a dizzying array of different semantic parsing algorithms, and it is a challenge to keep trac"
P19-4002,W12-3602,1,0.935246,"almost every Semantic Evaluation (SemEval) exercise since 2014. These shared tasks were based on a variety of different corpora with graph-based meaning annotations (graph banks), which differ both in their formal properties and in the facets of meaning they aim to represent. The relevance of this tutorial is to clarify this landscape 6 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 6–11 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2008), DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) and Prague Semantic Dependencies (PSD; a simplification of the tectogrammatical structures of Hajiˇc et al., 2012). Tutorial slides and additional materials are available at the following address: https://github.com/cfmrp/tutorial 2 Semantic Graph Banks In the first part of the tutorial, we will give a systematic overview of the available semantic graph banks. On the one hand, we will distinguish graph banks with respect to the facets of natural language meaning they aim to represent. For instance, some graph banks focus on predicate–argument structure, perhaps with some extensions for polari"
P19-4002,P10-5006,0,0.0128625,"cipants will be enabled to identify genuine content differences between frameworks as well as to tease apart more superficial variation, for example in terminology or packaging. Furthermore, major current processing techniques for semantic graphs will be reviewed against a highlevel inventory of families of approaches. This part of the tutorial will emphasize reflections on codependencies with specific graph flavors or frameworks, on worst-case and typical time and space complexity, as well as on what guarantees (if any) are obtained on the wellformedness and correctness of output structures. Kate and Wong (2010) suggest a definition of semantic parsing as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” This view brings along a tacit expectation to map (more or less) directly from a linguistic surface form to an actionable encoding of its intended meaning, e.g. in a database query or even programming language. In this tutorial, we embrace a broader perspective on semantic parsing as it has come to be viewed commonly in recent years. We will review graph-based meaning representations that ai"
P19-4002,P17-1186,0,0.115805,"Missing"
P19-4002,P17-1014,0,0.0545964,"nderspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philolo"
P19-4002,P18-1171,0,0.187888,"ng of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philology, Computer Scienc"
P19-4002,Q15-1040,0,0.0475384,"ical results. (4) Parsing into Semantic Graphs Structural information Finally, semantic parsers differ with respect to how structure information is represented. Some model the target graph directly, whereas others use probability models that score a tree which evaluates to the target graph (e.g. a syntactic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD"
P19-4002,C08-1095,0,0.198112,"odels. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic par"
P19-4002,J16-4009,1,0.825702,"icate–argument structure, perhaps with some extensions for polarity or tense, whereas others capture (some) scopal phenomena. Furthermore, while the graphs in most graph banks do not have a precisely defined model theory in the sense of classical linguistic semantics, there are still underlying intuitions about what the nodes of the graphs mean (individual entities and eventualities in the world vs. more abstract objects to which statements about scope and presupposition can attach). We will discuss the different intuitions that underly different graph banks. On the other hand, we will follow Kuhlmann and Oepen (2016) in classifying graph banks with respect to the relationship they assume between the tokens of the sentence and the nodes of the graph (called anchoring of graph fragments onto input sub-strings). We will distinguish three flavors of semantic graphs, which by degree of anchoring we will call type (0) to type (2). While we use ‘flavor’ to refer to formally defined sub-classes of semantic graphs, we will reserve the term ‘framework’ for a specific linguistic approach to graph-based meaning representation (typically cast in a particular graph flavor, of course). Type (1) A more general form of an"
P19-4002,D18-1263,0,0.0131,"guities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philology, Computer Science, and Computational Linguis"
P19-4002,P19-1450,1,0.642351,"tic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he"
P19-4002,N15-1040,0,0.0436829,"on: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for A"
P19-4002,S16-1166,0,0.0365445,"unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic f"
P19-4002,D18-1194,0,0.126774,"Missing"
P19-4002,S17-2090,0,0.0283489,"in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this"
P19-4002,J08-1002,0,0.0102092,", and it is a challenge to keep track of their respective strengths and weaknesses. Different parsing approaches are, of course, more or less effective for graph banks of different flavors (and, at times, even specific frameworks). We will discuss these interactions in the tutorial and organize the research landscape on graph-based semantic parsing along three dimensions. (1) Linguistic Foundations: Layers of Sentence Meaning (2) Formal Foundations: Graphs Labeled Directed (3) Meaning Representation Frameworks and Graph Banks • Bi-Lexical semantic dependencies (Hockenmaier and Steedman, 2007; Miyao and Tsujii, 2008; Hajiˇc et al., 2012; Ivanova et al., 2012; Che et al., 2016); Decoding strategy Semantic parsers differ with respect to the type of algorithm that is used to compute the graph. These include factorizationbased methods, which factorize the score of a graph into parts for smaller substrings and can then apply dynamic programming to search for the best graph, as well as transition-based methods, which learn to make individual parsing decisions for each token in the sentence. Some neural techniques also make use of an encoder-decoder architecture, as in neural machine translation. • Universal Co"
R11-1064,P08-2012,0,0.119345,"p the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), which share roles (frame ele"
R11-1064,P04-1051,1,0.796122,"dered temporally. This guided way of learning script data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scrip"
R11-1064,H92-1045,0,0.0355937,"Missing"
R11-1064,P98-1013,0,0.12353,"variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), which share roles (frame elements) specifying people and objects involved in the events. The supervised approach of Mani et al. (2006) learns temporal event relations from TimeBank (Pustejovsky et al., 2006). All of these approaches rely on elaborate manual annotation efforts, and so it is unclear how they would scale to wide-coverage resources. Chambers and Jurafsky (2008; 2009) exploit coreference chains and co-occurrence frequency of verbs in te"
R11-1064,P03-1054,0,0.00700172,"Missing"
R11-1064,N06-1046,0,0.0281613,"guided way of learning script data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants."
R11-1064,P06-1095,0,0.085404,"Missing"
R11-1064,P10-1124,0,0.0200376,"ipt data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a st"
R11-1064,J93-2004,0,0.0366212,"Missing"
R11-1064,W10-4305,0,0.0136123,"ral information by itself is worthless: high precision loss makes align even worse than the na¨ıve baseline. (6) (7) Overall precision and recall is averaged over all tokens in the annotation. Overall F1 score is then computed as follows: F1 = 2 ∗ precision ∗ recall precision + recall (8) Unlike in coreference resolution, we have the problem that we compare gold-standard annotations against tokens extracted from automatic parses. However, the b3 -metric is only applicable if the gold standard and the test data contain the same set of tokens. Thus we apply b3sys , a variant of b3 introduced by Cai and Strube (2010). b3sys extends the gold standard and the test set such that both contain the same set of tokens. Roughly speaking, every token that appears in the gold standard but not in the test set is copied to the latter and treated as singleton set, and vice versa. See Cai and Strube for details. With the inaccurate parser, noun phrases are often parsed incompletely, missing modifiers or relative clauses. We therefore consider a participant description as equivalent with a gold standard phrase if they have the same head. This relaxed scoring metric evaluates the system realistically by punishing parsing"
R11-1064,P08-1090,0,0.602925,"and computational linguistics, including commonsense reasoning for text understanding (Cullingford, 1977; Mueller, 2004), information extraction (Rau et al., 1989) and automated storytelling (Swanson and Gordon, 2008). But there is hardly an area where the discrepancy between the felt importance of a type of knowledge and the inability to provide any substantial amount of this knowledge for serious applications is greater. Recently, several groups have tackled the problem using unsupervised methods for learning script-like knowledge from text corpora or data obtained through web experiments (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010). For the first time, they open up a perspective to wide-coverage resources of script knowledge. However, each of these approaches handles only specific aspects of script 463 Proceedings of Recent Advances in Natural Language Processing, pages 463–470, Hissar, Bulgaria, 12-14 September 2011. 1 2 3 4 5 6 7 ESD 1 ESD 2 ESD3 put food on plate open microwave put plate in close microwave press start put food in bowl open door put food inside close door enter time push button ... put food on dish open oven place dish in oven close select desired le"
R11-1064,P09-1068,0,0.111637,"s, including commonsense reasoning for text understanding (Cullingford, 1977; Mueller, 2004), information extraction (Rau et al., 1989) and automated storytelling (Swanson and Gordon, 2008). But there is hardly an area where the discrepancy between the felt importance of a type of knowledge and the inability to provide any substantial amount of this knowledge for serious applications is greater. Recently, several groups have tackled the problem using unsupervised methods for learning script-like knowledge from text corpora or data obtained through web experiments (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010). For the first time, they open up a perspective to wide-coverage resources of script knowledge. However, each of these approaches handles only specific aspects of script 463 Proceedings of Recent Advances in Natural Language Processing, pages 463–470, Hissar, Bulgaria, 12-14 September 2011. 1 2 3 4 5 6 7 ESD 1 ESD 2 ESD3 put food on plate open microwave put plate in close microwave press start put food in bowl open door put food inside close door enter time push button ... put food on dish open oven place dish in oven close select desired length strengths and weaknesses"
R11-1064,P10-1100,1,0.934831,"Department of Computational Linguistics, Saarland University {regneri,pinkal}@coli.uni-saarland.de † Department of Linguistics, University of Potsdam koller@ling.uni-potsdam.de ‡ Department of Information Science and Language Technology, University of Hildesheim Josef.Ruppenhofer@uni-hildesheim.de Abstract information: Chambers and Jurafsky (2009) learn narrative schemas and their participants; they group verbs into schemas by virtue of shared participants assuming that this is an indicator for being part of the same stereotypical activity, without knowing the actual scenarios. The system of Regneri et al. (2010) learns the temporal order of events occurring in specific stereotypical scenarios, but does not determine participants. In this paper, we present a system that automatically learns sets of participants associated with specific scenarios. We take the approach of Regneri et al. as our starting point. In this earlier work, several experimental subjects described what happens in a given scenario in a web experiment; the system then learns what event descriptions from different subjects refer to the same event, and how they are temporally ordered, using Multiple Sequence Alignment (Durbin et al.,"
R11-1064,H94-1010,0,0.343914,"Missing"
R11-1064,N07-1030,0,0.0286859,"scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), w"
R11-1064,C98-1013,0,\N,Missing
S14-2081,J93-2004,0,0.0456754,"tered in both the closed track and the open track of the challenge, recording a peak average labeled F1 score of 78.60. 1 Introduction In the semantic dependency parsing (SDP) task of SemEval 2014, the meaning of a sentence is represented in terms of binary head-argument relations between the lexical units – bi-lexical dependencies (Oepen et al., 2014). Since words can be semantic dependents of multiple other words, this framework results in graph representations of sentence meaning. For the SDP task, three such annotation layers are provided on top of the WSJ text of the Penn Treebank (PTB) (Marcus et al., 1993): – DM: the reduction of DeepBank HPSG annotation (Flickinger et al., 2012) into bi-lexical dependencies following (Oepen and Lønning, 2006; Ivanova et al., 2012), – PAS: the predicate-argument structures derived from the training set of the Enju HPSG parser (Miyao et al., 2004) and – PCEDT: a subset of the tectogrammatical annotation layer from the English side of the Prague Czech-English Dependency Treebank (Cinkov´a et al., 2009). 2 Data and Systems We present the basic statistics for the SDP training sets in Table 1. The graphs contain no cycles, i.e., all SDP meaning representations are d"
S14-2081,D12-1133,0,0.0291587,"on the training data, use the converted training sets to train syntactic dependency parsers (Bohnet, 2010) and utilize the parsing models on the development and test data. The parsing outputs are converted back to graphs by simply re-flipping all the edges denoted as flipped. 2.3 PAS 2.4 Parsing and Top Node Detection The same syntactic parser and top node detector are used in both LOCAL and DFS. Both systems ran in the closed SDP track, with no additional features for learning, and in the open track, where they used the SDP companion data, i.e., the outputs of a syntactic dependency parser (Bohnet and Nivre, 2012) and phrase-based parser (Petrov et al., 2006) as additional features. Our choice of parser was based on the high non-projectivity of the resulting trees, while parsers of (Bohnet and Nivre, 2012; Bohnet et al., 2013) could also be used, among others. We use the parser out of the box, i.e., without any parameter tuning or additional features other than what was previously listed for the open track. Top node detection is implemented separately, by training a sequence labeling model (Lafferty et al., 2001; Kudo, 2005) on tokens and part-ofspeech tags from the training sets. Its accuracy is given"
S14-2081,Q13-1034,0,0.047291,"Missing"
S14-2081,C10-1011,0,0.0664041,"roduce the coordinated nodes. We conclude that edges in reentrancies, for which the source nodes have zero indegree, could be flipped by changing places of their source and target nodes and encoding the switch in the edge labels by appending the suffix flipped to the existing labels. This is the basis for our first system: LOCAL. In it, we locally flip all edges in reentrancies for which the source node has zero indegree and run the BASELINE conversion on the resulting graphs. We apply this conversion on the training data, use the converted training sets to train syntactic dependency parsers (Bohnet, 2010) and utilize the parsing models on the development and test data. The parsing outputs are converted back to graphs by simply re-flipping all the edges denoted as flipped. 2.3 PAS 2.4 Parsing and Top Node Detection The same syntactic parser and top node detector are used in both LOCAL and DFS. Both systems ran in the closed SDP track, with no additional features for learning, and in the open track, where they used the SDP companion data, i.e., the outputs of a syntactic dependency parser (Bohnet and Nivre, 2012) and phrase-based parser (Petrov et al., 2006) as additional features. Our choice of"
S14-2081,P13-1091,0,0.030687,"nce, with word forms as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic dependency parsers. The secondary benefit is insight into the st"
S14-2081,S14-2008,0,0.19623,"Missing"
S14-2081,P06-1055,0,0.0168421,"sets to train syntactic dependency parsers (Bohnet, 2010) and utilize the parsing models on the development and test data. The parsing outputs are converted back to graphs by simply re-flipping all the edges denoted as flipped. 2.3 PAS 2.4 Parsing and Top Node Detection The same syntactic parser and top node detector are used in both LOCAL and DFS. Both systems ran in the closed SDP track, with no additional features for learning, and in the open track, where they used the SDP companion data, i.e., the outputs of a syntactic dependency parser (Bohnet and Nivre, 2012) and phrase-based parser (Petrov et al., 2006) as additional features. Our choice of parser was based on the high non-projectivity of the resulting trees, while parsers of (Bohnet and Nivre, 2012; Bohnet et al., 2013) could also be used, among others. We use the parser out of the box, i.e., without any parameter tuning or additional features other than what was previously listed for the open track. Top node detection is implemented separately, by training a sequence labeling model (Lafferty et al., 2001; Kudo, 2005) on tokens and part-ofspeech tags from the training sets. Its accuracy is given in Table 3. We use only the tokens and parts"
S14-2081,N10-1138,0,0.0151698,"koller@ling.uni-potsdam.de Abstract tence, with word forms as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic dependency parsers. The"
S14-2081,C08-1095,0,0.203579,"am zagic@uni-potsdam.de koller@ling.uni-potsdam.de Abstract tence, with word forms as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic depen"
S14-2081,J13-4006,0,0.046933,"as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic dependency parsers. The secondary benefit is insight into the structure of the semantic r"
S14-2081,W12-3602,0,0.0838369,"parsing (SDP) task of SemEval 2014, the meaning of a sentence is represented in terms of binary head-argument relations between the lexical units – bi-lexical dependencies (Oepen et al., 2014). Since words can be semantic dependents of multiple other words, this framework results in graph representations of sentence meaning. For the SDP task, three such annotation layers are provided on top of the WSJ text of the Penn Treebank (PTB) (Marcus et al., 1993): – DM: the reduction of DeepBank HPSG annotation (Flickinger et al., 2012) into bi-lexical dependencies following (Oepen and Lønning, 2006; Ivanova et al., 2012), – PAS: the predicate-argument structures derived from the training set of the Enju HPSG parser (Miyao et al., 2004) and – PCEDT: a subset of the tectogrammatical annotation layer from the English side of the Prague Czech-English Dependency Treebank (Cinkov´a et al., 2009). 2 Data and Systems We present the basic statistics for the SDP training sets in Table 1. The graphs contain no cycles, i.e., all SDP meaning representations are directed acyclic graphs (DAGs). DM and PAS are automatically derived from HPSG annotations, while PCEDT is based on manual tectogrammatical annotation. This is ref"
S14-2081,W13-1810,0,0.017541,"tsdam.de Abstract tence, with word forms as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic dependency parsers. The secondary benefit i"
S14-2081,oepen-lonning-2006-discriminant,0,\N,Missing
W05-1105,copestake-flickinger-2000-open,0,0.817703,"e a sequence of four solvers, ranging from a purely logic-based saturation algorithm (Koller et al., 1998) over a solver based on constraint programming (Duchier and Niehren, 2000) to efficient solvers based on graph algorithms (Bodirsky et al., 2004). The first three solvers have been described in the literature before, but we also present a new variant of the graph solver that uses caching to obtain a considerable speedup. Finally we present a new evaluation that compares all four solvers with each other and with a different underspecification solver from the LKB grammar development system (Copestake and Flickinger, 2000). The paper is structured as follows. We will first sketch the problem that our algorithms solve (Section 2). Then we present the solvers (Section 3) and conclude with the evaluation (Section 4). 2 The Problem The problem we use to illustrate the progress towards efficient solvers is that of enumerating all readings of an underspecified description. Underspecification is a technique for dealing with the combinatorial problems associated with quantifier scope ambiguities, certain semantic ambiguities that occur in sentences such as the following: ∀x → ∃y ∧ stud ∃y ∧ x read book y y x ∀x book →"
W05-1105,P03-1047,1,0.710933,"). The treebank solver even goes down. This demonstrates an imcontains syntactic annotations for sentences from proved management of the combinatorial explothe tourism domain such as (4) above, together sion. It is also interesting that the line of the setwith corresponding semantic representations. constraint solver is almost parallel to that of the The semantics is represented using MRS de- graph solver, which means that the solver really scriptions, which we convert into normal domi- does exploit a polynomial fragment on real-world nance constraints using the translation specified by data. Niehren and Thater (2003). The translation is reThe LKB solver performs very well for smaller stricted to MRS constraints having certain struc- constraints (which make up about half of the data tural properties (called nets). The treebank con- set): Except for the chart algorithm introduced in tains 961 MRS constrains, 879 of which are nets. this paper, it outperforms all other solvers. For For the runtime evaluation, we restricted the larger constraints, however, the LKB solver gets test set to the 852 nets with less than one mil- very slow. What isn’t visible in this graph is that lion solved forms. The distribution"
W06-3904,copestake-flickinger-2000-open,0,0.0682435,"rrent algorithms support the efficient enumeration of readings from an USR [10]. In addition, underspecification has the potential for eliminating incorrect or redundant readings by inferences based on context or world knowledge, without even enumerating them. For instance, sentences with scope ambiguities often have readings which are semantically equivalent. In this case, we typically need to retain only one reading from each equivalence class. This situation is illustrated by the following two sentences from the Rondane treebank, which is distributed with the English Resource Grammar (ERG; [5]), a broad-coverage HPSG grammar. (1) For travellers going to Finnmark there is a bus service from Oslo to Alta through Sweden. (Rondane 1262) (2) We quickly put up the tents in the lee of a small hillside and cook for the first time in the open. (Rondane 892) For the two example sentences, the ERG (Version 01-2006) derives USRs with seven and six quantifiers, respectively, that correspond to various types of noun phrases (including proper names and pronouns). The USR for (1) describes 3960 readings, which are all semantically equivalent to each other. On the other hand, the USR for (2) has 48"
W06-3904,P04-1032,1,0.886026,"G is a path in the undirected version Gu of G that does not use two dominance edges that are incident to the same hole. We say that G is hypernormally connected (hnc) iff each pair of nodes is connected by a simple hypernormal path in G. Hnc graphs are equivalent to chain-connected dominance constraints [9], and are closely related to dominance nets [11]. The results in this paper are restricted to hnc graphs, but this does not limit the applicability of our results: an empirical study suggests that all dominance graphs that are generated by current large-scale grammars are (or should be) hnc [8]. The key property of hnc dominance graphs is that their solved forms correspond to configurations, and we will freely switch between solved forms and their corresponding configurations. Another important property of hnc graphs which we will use extensively in the proofs below is that it is possible to predict which holes of fragments can dominate other fragments in a solved form. Lemma 2.2 Let G be a hnc graph with free fragment F. Then all weakly connected components of G − F are hnc. Proposition 2.3 Let F1 , F2 be fragments in a hnc dominance graph G. If there is a solved form S of G in whi"
W06-3904,P03-1047,1,0.89333,"still exponentially smaller than the entire set of readings because common subgraphs (such as {2, 5, 7} in the example) are represented only once. Thus the chart can still serve as an underspecified representation. 2.2 Hypernormally connected dominance graphs A hypernormal path [1] in a graph G is a path in the undirected version Gu of G that does not use two dominance edges that are incident to the same hole. We say that G is hypernormally connected (hnc) iff each pair of nodes is connected by a simple hypernormal path in G. Hnc graphs are equivalent to chain-connected dominance constraints [9], and are closely related to dominance nets [11]. The results in this paper are restricted to hnc graphs, but this does not limit the applicability of our results: an empirical study suggests that all dominance graphs that are generated by current large-scale grammars are (or should be) hnc [8]. The key property of hnc dominance graphs is that their solved forms correspond to configurations, and we will freely switch between solved forms and their corresponding configurations. Another important property of hnc graphs which we will use extensively in the proofs below is that it is possible to p"
W06-3904,W05-1105,1,0.917191,"largescale grammars. To our knowledge, it is the first redundancy elimination algorithm which maintains underspecification, rather than just enumerating non-redundant readings. 1 Introduction Underspecification is the standard approach to dealing with scope ambiguities in computational semantics [12,6,7,2]. The basic idea is to not enumerate all possible semantic representations for each syntactic analysis, but to derive a single compact underspecified representation (USR). This simplifies semantics construction, and current algorithms support the efficient enumeration of readings from an USR [10]. In addition, underspecification has the potential for eliminating incorrect or redundant readings by inferences based on context or world knowledge, without even enumerating them. For instance, sentences with scope ambiguities often have readings which are semantically equivalent. In this case, we typically need to retain only one reading from each equivalence class. This situation is illustrated by the following two sentences from the Rondane treebank, which is distributed with the English Resource Grammar (ERG; [5]), a broad-coverage HPSG grammar. (1) For travellers going to Finnmark there"
W06-3904,E91-1044,0,0.657686,"changing the semantics of the readings. The particular USRs we work with are underspecified chart representations, which can be computed from dominance graphs (or USRs in some other underspecification formalisms) efficiently [10]. The algorithm can deal with many interesting cases, but is incomplete in the sense that the resulting USR may still describe multiple equivalent readings. To our knowledge, this is the first algorithm in the literature for redundancy elimination on the level of USRs. There has been previous research on enumerating only some representatives of each equivalence class [13,4], but these approaches don’t maintain underspecification: After running their algorithms, we have a set of readings rather than an underspecified representation. Plan of the paper. We will first define dominance graphs and review the necessary background theory in Section 2. We will then give a formal definition of equivalence and derive some first results in Section 3. Section 4 presents the redundancy elimination algorithm. Finally, Section 5 concludes and points to further work. 2 Dominance Graphs The basic underspecification formalism we assume here are labelled dominance graphs [1]. Domin"
W08-1107,E91-1028,0,0.0887749,"gic (DL) that denotes exactly the set of individuals that we want to refer to. This very natural idea has been mentioned in passing before (Krahmer et al., 2003; Gardent and Striegnitz, 2007); however, we take it one step further by proposing DL as an interlingua for comparing the REs produced by different approaches to GRE. In this way, we can organize existing GRE approaches in an expressiveness hierarchy. For instance, the classical Dale and Reiter algorithms compute purely conjunctive formulas; van Deemter (2002) extends this language by adding the other propositional connectives, whereas Dale and Haddock (1991) extends it by allowing existential quantification. Furthermore, the view of GRE as a problem of computing DL formulas with a given extension allows us to apply existing algorithms for the latter problem to obtain efficient algorithms for GRE. We present algorithms that compute such formulas for the description logics EL (which allows only conjunction and existential quantification) and ALC (which also allows negation). These algorithms effectively compute REs for all individuals in the domain at the same time, which allows them to systematically avoid the infinite regress problem. The EL algo"
W08-1107,P89-1009,0,0.784035,"t. This view offers a new unifying perspective under which existing GRE algorithms can be compared. We also show that by applying existing algorithms for computing simulation classes in description logic, we can obtain extremely efficient algorithms for relational referring expressions without any danger of running into infinite regress. 1 Kristina Striegnitz Union College Schenectady, NY, US striegnk@union.edu Introduction The generation of referring expressions (GRE) is one of the most active and successful research areas in natural language generation. Building upon Dale and Reiter’s work (Dale, 1989; Dale and Reiter, 1995), various researchers have added extensions such as reference to sets (Stone, 2000), more expressive logical connectives (van Deemter, 2002), and relational expressions (Dale and Haddock, 1991). Referring expressions (REs) involving relations, in particular, have received increasing attention recently; especially in the context of spatial referring expressions in situated generation (e.g. (Kelleher and Kruijff, 2006)), where it seems particularly natural to use expressions such as “the book on the table”. However, the classical algorithm by Dale and Haddock (1991) was r"
W08-1107,P02-1013,0,0.546444,"s a good idea to handle sentence planning and realization in a single module; for instance, SPUD can use its awareness of the syntactic context to generate succinct REs as in “take the rabbit from the hat”. We hope that the ideas we have explored here for efficient and expressive RE generation can eventually be combined with recent efficient algorithms for integrated sentence planning and realization, such as in Koller and Stone (2007). One problem that arises in our approach is that 47 GRE algorithm Dale and Reiter (1995) van Deemter (2002) Dale and Haddock (1991) Kelleher and Kruijff (2006) Gardent (2002) both algorithms derive some measure of efficiency from their freedom to build formulas without having to respect any linguistic constraints. It seems straightforward, for instance, to extend Krahmer et al.’s (2003) approach such that it only considers subgraphs that can actually be realized, because their algorithm proceeds by a genuine search for uniquely identifying subgraphs, and will simply take a different branch of the search if some subgraph is useless. This would be harder in our case. Our algorithms don’t search in the same way; if we disallow certain refinements of a partition, we h"
W08-1107,P97-1027,0,0.126133,"in the ALC algorithm; it will be hard for any realizer to find a reasonable way of expressing a formula like ¬∃R.(¬P u¬Q) as a smooth noun phrase. Although we agree with van Deemter (2002) and others that the careful use of negation and disjunction can improve REs, these connectives must not be overused. Thus we consider the formulas computed by the EL algorithm “safer” with respect to realization. Of course, we share the problem of interfacing GRE and realization with every other approach that separates these two modules, i.e., almost the entire GRE literature (notable exceptions are, e.g., Horacek (1997) and SPUD (Stone and Webber, 1998)). 200 100 0 10 20 30 40 50 60 70 80 90 100 Figure 4: Average runtimes (in ms) of the two algorithms on random models with different numbers of individuals. In principle, we believe that it is a good idea to handle sentence planning and realization in a single module; for instance, SPUD can use its awareness of the syntactic context to generate succinct REs as in “take the rabbit from the hat”. We hope that the ideas we have explored here for efficient and expressive RE generation can eventually be combined with recent efficient algorithms for integrated sente"
W08-1107,P06-1131,0,0.547382,"The generation of referring expressions (GRE) is one of the most active and successful research areas in natural language generation. Building upon Dale and Reiter’s work (Dale, 1989; Dale and Reiter, 1995), various researchers have added extensions such as reference to sets (Stone, 2000), more expressive logical connectives (van Deemter, 2002), and relational expressions (Dale and Haddock, 1991). Referring expressions (REs) involving relations, in particular, have received increasing attention recently; especially in the context of spatial referring expressions in situated generation (e.g. (Kelleher and Kruijff, 2006)), where it seems particularly natural to use expressions such as “the book on the table”. However, the classical algorithm by Dale and Haddock (1991) was recently shown to be unable to generate satisfying REs in practice (Viethen and Dale, 2006). Furthermore, the Dale and Haddock algorithm and most of its successors (such as (Kelleher and Kruijff, 2006)) are vulnerable to the problem of “infinite regress”, where the algorithm jumps 42 i.e., propositional logic); and ELU (¬) (EL plus disjunction and atomic negation). Below, we will use a key notion of formula preservation that we call similari"
W08-1107,P07-1043,1,0.66706,"100 0 10 20 30 40 50 60 70 80 90 100 Figure 4: Average runtimes (in ms) of the two algorithms on random models with different numbers of individuals. In principle, we believe that it is a good idea to handle sentence planning and realization in a single module; for instance, SPUD can use its awareness of the syntactic context to generate succinct REs as in “take the rabbit from the hat”. We hope that the ideas we have explored here for efficient and expressive RE generation can eventually be combined with recent efficient algorithms for integrated sentence planning and realization, such as in Koller and Stone (2007). One problem that arises in our approach is that 47 GRE algorithm Dale and Reiter (1995) van Deemter (2002) Dale and Haddock (1991) Kelleher and Kruijff (2006) Gardent (2002) both algorithms derive some measure of efficiency from their freedom to build formulas without having to respect any linguistic constraints. It seems straightforward, for instance, to extend Krahmer et al.’s (2003) approach such that it only considers subgraphs that can actually be realized, because their algorithm proceeds by a genuine search for uniquely identifying subgraphs, and will simply take a different branch of"
W08-1107,J03-1003,0,0.860596,"Missing"
W08-1107,W98-1419,0,0.572558,", ||ϕn ||is a partition of ∆, we weaken this invariant to the requirement that there are no m ≥ 2 pairwise different indices 1 ≤ i1 , . . . , im ≤ n such that ||ϕi1 ||= ||ϕi2 ||∪ . . . ∪ ||ϕim ||. We call ϕi1 subsumed if such a decomposition exists. Because it maintains a weaker invariant, the set RE may contain more formulas at the same time in the EL algorithm than in the ALC algorithm. Given that ∆ has an exponential number of subsets, there is a risk that the EL algorithm might have worst-case in bathtub b1 in flower f2 flower f1 Figure 1: (a) The Dale and Haddock (1991) scenario; (b) the Stone and Webber (1998) scenario. other individual b to which a is similar; otherwise, any formula that is satisfied by a is also satisfied by b. Conversely, if we know that a is not similar to any other individual, then there is a formula that is satisfied by a and not by anything else; this formula can serve as a unique singular RE. In other words, we can reduce the L-GRE problem for a given model to the problem of computing the L-similarity sets of this model. Notice that this use of similarity sets can be seen as a generalization of van Deemter’s (2002) “satellite sets” to relational descriptions. In the rest of"
W08-1107,W00-1416,0,0.415139,"so show that by applying existing algorithms for computing simulation classes in description logic, we can obtain extremely efficient algorithms for relational referring expressions without any danger of running into infinite regress. 1 Kristina Striegnitz Union College Schenectady, NY, US striegnk@union.edu Introduction The generation of referring expressions (GRE) is one of the most active and successful research areas in natural language generation. Building upon Dale and Reiter’s work (Dale, 1989; Dale and Reiter, 1995), various researchers have added extensions such as reference to sets (Stone, 2000), more expressive logical connectives (van Deemter, 2002), and relational expressions (Dale and Haddock, 1991). Referring expressions (REs) involving relations, in particular, have received increasing attention recently; especially in the context of spatial referring expressions in situated generation (e.g. (Kelleher and Kruijff, 2006)), where it seems particularly natural to use expressions such as “the book on the table”. However, the classical algorithm by Dale and Haddock (1991) was recently shown to be unable to generate satisfying REs in practice (Viethen and Dale, 2006). Furthermore, th"
W08-1107,W06-1420,0,0.207306,"Missing"
W08-1107,J02-1003,0,0.469802,"Missing"
W08-1107,W06-1410,0,0.630814,"such as reference to sets (Stone, 2000), more expressive logical connectives (van Deemter, 2002), and relational expressions (Dale and Haddock, 1991). Referring expressions (REs) involving relations, in particular, have received increasing attention recently; especially in the context of spatial referring expressions in situated generation (e.g. (Kelleher and Kruijff, 2006)), where it seems particularly natural to use expressions such as “the book on the table”. However, the classical algorithm by Dale and Haddock (1991) was recently shown to be unable to generate satisfying REs in practice (Viethen and Dale, 2006). Furthermore, the Dale and Haddock algorithm and most of its successors (such as (Kelleher and Kruijff, 2006)) are vulnerable to the problem of “infinite regress”, where the algorithm jumps 42 i.e., propositional logic); and ELU (¬) (EL plus disjunction and atomic negation). Below, we will use a key notion of formula preservation that we call similarity. For any DL L, we will say that an individual i is L-similar to i0 in a given model M if for any formula ϕ ∈ L such that i ∈ ||ϕ||, we also have i0 ∈ ||ϕ||. Equivalently, there is no L-formula that holds of i but not of i0 . We say that the L-"
W09-0628,P08-2050,0,0.0293624,"ure of the task imposes high demands on the system’s efficiency). But if Why a new NLG evaluation paradigm? The GIVE Challenge addresses a need for a new evaluation paradigm for natural language generation (NLG). NLG systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard using automatic comparison algorithms has limited value because there can be multiple generated outputs that are equally good. Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). Human assessments of system outputs are preferred, but lab-based evaluations that allow human subjects to assess each aspect of the system’s functionality are expensive and time-consuming, thereby favoring larger labs with adequate resources to conduct human subjects studies. Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it dif166 extended to two-way dialog, the task can also involve issues of prosody generation (i.e., research on text/concept-to-speech generation), discour"
W09-0628,W08-1113,0,0.0343662,"he system’s efficiency). But if Why a new NLG evaluation paradigm? The GIVE Challenge addresses a need for a new evaluation paradigm for natural language generation (NLG). NLG systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard using automatic comparison algorithms has limited value because there can be multiple generated outputs that are equally good. Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). Human assessments of system outputs are preferred, but lab-based evaluations that allow human subjects to assess each aspect of the system’s functionality are expensive and time-consuming, thereby favoring larger labs with adequate resources to conduct human subjects studies. Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it dif166 extended to two-way dialog, the task can also involve issues of prosody generation (i.e., research on text/concept-to-speech generation), discourse generation, and human-robot inte"
W09-0628,E09-2009,1,0.68788,"onnects to the Matchmaker and is randomly assigned an NLG server and a game world. The client and NLG server then communicate over the course of one game. At the end of the game, the client displays a questionnaire to the user, and the game log and questionnaire data are uploaded to the Matchmaker and stored in a database. Note that this division allows the challenge to be conducted without making any assumptions about the internal structure of an NLG system. The GIVE software is implemented in Java and available as an open-source Google Code project. For more details about the software, see (Koller et al., 2009). 3.2 3.3 Materials Figs. 3–5 show the layout of the three evaluation worlds. The worlds were intended to provide varying levels of difficulty for the direction-giving systems and to focus on different aspects of the problem. World 1 is very similar to the development world that the research teams were given to test their system on. World 2 was intended to focus on object descriptions - the world has only one room which is full of objects and buttons, many of which cannot be distinguished by simple descriptions. World 3, on the other hand, puts more emphasis on navigation directions as the wor"
W09-0628,W06-1412,1,0.744385,"appeal to younger students, the task can also be used as a pedagogical exercise to stimulate interest among secondary-school students in the research challenges found in NLG or Computational Linguistics more broadly. Embedding the NLG task in a virtual world encourages the participating research teams to consider communication in a situated setting. This makes the NLG task quite different than in other NLG challenges. For example, experiments have shown that human instruction givers make the instruction follower move to a different location in order to use a simpler referring expression (RE) (Stoia et al., 2006). That is, RE generation becomes a very different problem than the classical non-situated Dale & Reiter style RE generation, which focuses on generating REs that are single noun phrases in the context of an unchanging world. On the other hand, because the virtual environments scenario is so open-ended, it – and specifically the instruction-giving task – can potentially be of interest to a wide range of NLG researchers. This is most obvious for research in sentence planning (GRE, aggregation, lexical choice) and realization (the real-time nature of the task imposes high demands on the system’s"
W10-4233,gargett-etal-2010-give,1,0.375374,"Missing"
W10-4233,E09-2009,1,0.722559,"in the virtual world. This is in contrast to GIVE-1, where players could only turn by 90 degree increments, and jump forward and backward by discrete steps. This feature of the way the game controls were set Method Following the approach from the GIVE-1 Challenge (Koller et al., 2010), we connected the NLG systems to users over the Internet. In each game run, one user and one NLG system were paired up, with the system trying to guide the user to success in a specific game world. 3.1 Software infrastructure We adapted the GIVE-1 software to the GIVE-2 setting. The GIVE software infrastructure (Koller et al., 2009a) consists of three different modules: The client, which is the program which the user runs on their machine to interact with the virtual world (see Fig. 1); a collection of NLG servers, which generate instructions in real-time and send them to the client; and a matchmaker, which chooses a random NLG server and virtual world for each incoming connection from a client and stores the game results in a database. The most visible change compared to GIVE-1 was to modify the client so it permitted free movement in the virtual world. This change further necessitated a number of modifications to the"
W10-4233,P09-2076,1,0.840837,"in the virtual world. This is in contrast to GIVE-1, where players could only turn by 90 degree increments, and jump forward and backward by discrete steps. This feature of the way the game controls were set Method Following the approach from the GIVE-1 Challenge (Koller et al., 2010), we connected the NLG systems to users over the Internet. In each game run, one user and one NLG system were paired up, with the system trying to guide the user to success in a specific game world. 3.1 Software infrastructure We adapted the GIVE-1 software to the GIVE-2 setting. The GIVE software infrastructure (Koller et al., 2009a) consists of three different modules: The client, which is the program which the user runs on their machine to interact with the virtual world (see Fig. 1); a collection of NLG servers, which generate instructions in real-time and send them to the client; and a matchmaker, which chooses a random NLG server and virtual world for each incoming connection from a client and stores the game results in a database. The most visible change compared to GIVE-1 was to modify the client so it permitted free movement in the virtual world. This change further necessitated a number of modifications to the"
W10-4233,W11-2830,0,\N,Missing
W10-4233,W11-2848,0,\N,Missing
W10-4233,W11-2846,0,\N,Missing
W10-4233,W11-2849,0,\N,Missing
W10-4233,W11-2847,0,\N,Missing
W10-4233,W11-2851,1,\N,Missing
W10-4233,W11-2852,0,\N,Missing
W10-4233,W11-2850,0,\N,Missing
W10-4416,W00-1401,0,0.0752884,"Missing"
W10-4416,P10-1159,1,0.777888,"atic in the number of lexicalized trees in the grammar, current planning algorithms are also too slow to be used for longer sentences. An obvious issue for future research is to apply improved smoothing techniques to deal with the data sparseness. Planning runtimes should be improved by further tweaking the exact planning problems we generate, and will benefit from any future improvements in metric planning. It is interesting to note that the extensions we made to CRISP to accommodate statistical generation here are compatible with recent work in which CRISP is applied to situated generation (Garoufi and Koller, 2010); we expect that this will be true for other future extensions to CRISP as 133 Daniel Bauer, Alexander Koller well. Finally, we have only evaluated PCRISP on a surface realization problem in this paper. It would be interesting to carry out an extrinsic, task-based evaluation of PCRISP that also addresses sentence planning. Acknowledgments. We are grateful to Owen Rambow for providing us with the Chen WSJ-TAG corpus and to Malte Helmert and Silvia Richter for their help with running LAMA, another metric planner with which we experimented. We thank Konstantina Garoufi and Owen Rambow for helpful"
W10-4416,P95-1034,0,0.303852,"ccuracy which we think are worth exploring further in future work. Plan of the paper. We start by putting our research in the context of related work in Section 2 and reviewing CRISP in Section 3. We then describe PCRISP, our probabilistic extension of CRISP, in 127 Daniel Bauer, Alexander Koller Section 4 and evaluate it in Section 5. We conclude in Section 6. 2 Related Work Statistical methods are popular for surface realization, but have not been used in systems that integrate sentence planning. Most statistical generation approaches follow a generate-and-select strategy, first proposed by Knight and Hatzivassiloglou (1995) in their NITROGEN system. Such systems generate a set of candidate sentences using a (possibly overgenerating) grammar and then select the best output sentence by applying a statistical language model. This family includes systems such as HALogen (Langkilde and Knight, 1998; Langkilde, 2000) and OpenCCG (White and Baldridge, 2003). The FERGUS system (Bangalore and Rambow, 2000) is a variant of this approach which, like PCRISP, employs TAG. It first assigns elementary trees to each entity in the input sentence plan using a statistical tree model and then computes the most likely derivation usi"
W10-4416,P07-1043,1,0.744469,"natural language strings by a surface realizer (Reiter and Dale, 2000). An alternative approach is integrated sentence generation, in which both steps are performed by the same algorithm, as in the SPUD system (Stone et al., 2003). An integrated algorithm can sometimes generate better and more succinct sentences (Stone and Webber, 1998). SPUD itself gives up some of this advantage by using a greedy search heuristic for efficiency reasons. The CRISP system, a recent reimplementation of SPUD using search techniques from AI planning, achieves high efficiency without sacrificing complete search (Koller and Stone, 2007; Koller and Hoffmann, 2010). While CRISP is efficient enough to perform well on large-scale grammars (Koller and Hoffmann, 2010), such grammars tend to offer many different ways to express the same semantic representation. This makes it necessary for the generation system to be able to compute not just grammatical sentences, but to identify which of these sentences are good. This problem is exacerbated when using treebankderived grammars, which tend to underspecify the actual constraints on grammaticality and instead rely on statistical information learned from the treebank. Indeed, there hav"
W10-4416,P98-1116,0,0.58719,"2010), such grammars tend to offer many different ways to express the same semantic representation. This makes it necessary for the generation system to be able to compute not just grammatical sentences, but to identify which of these sentences are good. This problem is exacerbated when using treebankderived grammars, which tend to underspecify the actual constraints on grammaticality and instead rely on statistical information learned from the treebank. Indeed, there have been a number of systems for statistical generation, which can exploit such information to rank sentences appropriately (Langkilde and Knight, 1998; White and Baldridge, 2003; Belz, 2008). However, to our knowledge, all such systems are currently restricted to performing surface realization, and must rely on separate modules to perform sentence planning. In this paper, we bring these two strands of research together for the first time. We present the PCRISP system, which redefines the SPUD generation problem in terms of probabilistic TAG grammars (PTAG, (Resnik, 1992)) and then extends CRISP to solving the probabilistic SPUD generation problem using metric planning (Fox and Long, 2002; Hoffmann, 2003). We evaluate PCRISP on a PTAG treeba"
W10-4416,A00-2023,0,0.0405258,"evaluate it in Section 5. We conclude in Section 6. 2 Related Work Statistical methods are popular for surface realization, but have not been used in systems that integrate sentence planning. Most statistical generation approaches follow a generate-and-select strategy, first proposed by Knight and Hatzivassiloglou (1995) in their NITROGEN system. Such systems generate a set of candidate sentences using a (possibly overgenerating) grammar and then select the best output sentence by applying a statistical language model. This family includes systems such as HALogen (Langkilde and Knight, 1998; Langkilde, 2000) and OpenCCG (White and Baldridge, 2003). The FERGUS system (Bangalore and Rambow, 2000) is a variant of this approach which, like PCRISP, employs TAG. It first assigns elementary trees to each entity in the input sentence plan using a statistical tree model and then computes the most likely derivation using only these trees with an n-gram model on the output sentence. An alternative to the n-gram based generate and select approach is to use a probabilistic grammar model, like PTAG, trained on automatic parses (Zhong and Stent, 2005). A related approach uses a model over local decisions of the"
W10-4416,C92-2065,0,0.657149,"from the treebank. Indeed, there have been a number of systems for statistical generation, which can exploit such information to rank sentences appropriately (Langkilde and Knight, 1998; White and Baldridge, 2003; Belz, 2008). However, to our knowledge, all such systems are currently restricted to performing surface realization, and must rely on separate modules to perform sentence planning. In this paper, we bring these two strands of research together for the first time. We present the PCRISP system, which redefines the SPUD generation problem in terms of probabilistic TAG grammars (PTAG, (Resnik, 1992)) and then extends CRISP to solving the probabilistic SPUD generation problem using metric planning (Fox and Long, 2002; Hoffmann, 2003). We evaluate PCRISP on a PTAG treebank extracted from the Wall Street Journal Corpus (Chen and Shanker, 2004). The evaluation reveals a tradeoff between coverage, efficiency, and accuracy which we think are worth exploring further in future work. Plan of the paper. We start by putting our research in the context of related work in Section 2 and reviewing CRISP in Section 3. We then describe PCRISP, our probabilistic extension of CRISP, in 127 Daniel Bauer, Al"
W10-4416,W98-1419,0,0.202169,"fs between coverage, efficiency, and accuracy. 1 Introduction Many sentence generation systems are organized in a pipeline architecture, in which the input semantic representation is first enriched, e.g. with referring expressions, by a sentence planner and only then transformed into natural language strings by a surface realizer (Reiter and Dale, 2000). An alternative approach is integrated sentence generation, in which both steps are performed by the same algorithm, as in the SPUD system (Stone et al., 2003). An integrated algorithm can sometimes generate better and more succinct sentences (Stone and Webber, 1998). SPUD itself gives up some of this advantage by using a greedy search heuristic for efficiency reasons. The CRISP system, a recent reimplementation of SPUD using search techniques from AI planning, achieves high efficiency without sacrificing complete search (Koller and Stone, 2007; Koller and Hoffmann, 2010). While CRISP is efficient enough to perform well on large-scale grammars (Koller and Hoffmann, 2010), such grammars tend to offer many different ways to express the same semantic representation. This makes it necessary for the generation system to be able to compute not just grammatical"
W10-4416,W03-2316,0,0.532832,"o offer many different ways to express the same semantic representation. This makes it necessary for the generation system to be able to compute not just grammatical sentences, but to identify which of these sentences are good. This problem is exacerbated when using treebankderived grammars, which tend to underspecify the actual constraints on grammaticality and instead rely on statistical information learned from the treebank. Indeed, there have been a number of systems for statistical generation, which can exploit such information to rank sentences appropriately (Langkilde and Knight, 1998; White and Baldridge, 2003; Belz, 2008). However, to our knowledge, all such systems are currently restricted to performing surface realization, and must rely on separate modules to perform sentence planning. In this paper, we bring these two strands of research together for the first time. We present the PCRISP system, which redefines the SPUD generation problem in terms of probabilistic TAG grammars (PTAG, (Resnik, 1992)) and then extends CRISP to solving the probabilistic SPUD generation problem using metric planning (Fox and Long, 2002; Hoffmann, 2003). We evaluate PCRISP on a PTAG treebank extracted from the Wall"
W10-4416,C00-1007,0,\N,Missing
W10-4416,C98-1112,0,\N,Missing
W11-2815,W10-4416,1,0.848312,"human or simulated users. However, because of the complexity of reinforcement learning, this has for the greatest part been applied to RE generation only in the most rudimentary way, e.g. to distinguish whether or not to use jargon in a technical dialogue (Janarthanam and Lemon, 2010). Decision-making problems of a broader scope have started getting addressed by such techniques only very recently (Dethlefs et al., 2011). Finally, NLG systems based on planning, such as Koller and Stone (2007), typically optimize for RE size instead of either humanlikeness or understandability. One exception is Bauer and Koller (2010), where sentence generation with a probabilistic grammar formalism is performed using a metric planner. That work generates REs which are probable and therefore in a certain sense humanlike; yet it focuses on syntactic choice and does not take understandability into account, neither has it been evaluated on RE generation tasks. 3 Planning utterances in situated context We build upon CRISP (Koller and Stone, 2007), a planning-based NLG model which encodes sentence generation with tree-adjoining grammars (TAG; (Joshi and Schabes, 1997)) as an automated planning problem. The CRISP model solves th"
W11-2815,2007.mtsummit-ucnlg.14,0,0.0187141,"is one of the best-studied problems in natural language generation (NLG). Traditional approaches (Dale and Reiter, 1995) have focused on defining the range of possible valid REs (e.g., as those REs that describe the target object uniquely) and on simple heuristics for choosing one valid RE (e.g., minimal REs). Recently, the question of how to choose the best RE out of the possible ones has gained increasing attention (Krahmer et al., 2003; Viethen et al., 2008). This process has been accelerated by the systematic evaluation of RE generation systems in the context of RE generation challenges (Belz and Gatt, 2007; Gatt and Belz, 2010). Almost all of these approaches optimize the humanlikeness of the NLG system, i.e. the similarity between system-generated REs and human121 A second characteristic of most existing RE generation systems is that they are limited to generating single noun phrases in isolation. By contrast, planning-based approaches (Appelt, 1985; Stone et al., 2003; Koller and Stone, 2007) generate REs in the context of an entire sentence or even discourse (Garoufi and Koller, 2010), and can therefore exploit and manipulate the linguistic and non-linguistic context in order to produce succ"
W11-2815,P08-2050,0,0.139092,"Missing"
W11-2815,W11-2011,0,0.0193565,"Missing"
W11-2815,gargett-etal-2010-give,1,0.8523,"Es (Stone and Webber, 1998). However, these approaches have not been combined with corpus-based measures of humanlikeness or understandability of REs. In this paper, we present the mSCRISP system, which extends the planning-based approach to NLG with a statistical model of RE understandability. mSCRISP uses a metric planner (Hoffmann, 2002) to compute the best REs that refer uniquely to the target referent, and thus combines statistical and symbolic reasoning. We obtain the cost model by training a maximum entropy (maxent) classifier on a corpus of human-generated instruction giving sessions (Gargett et al., 2010) in which every RE can be automatically annotated with a measure of how easy it was for the hearer to resolve. Although mSCRISP is in principle capable of generating complete inProceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 121–131, c Nancy, France, September 2011. 2011 Association for Computational Linguistics struction discourses, we only evaluate its RE generation component here. It turns out that mSCRISP generates more understandable REs than a purely symbolic baseline, according to our model’s estimation of understandability. Furthermore, mSCRISP ge"
W11-2815,P10-1159,1,0.922356,"been accelerated by the systematic evaluation of RE generation systems in the context of RE generation challenges (Belz and Gatt, 2007; Gatt and Belz, 2010). Almost all of these approaches optimize the humanlikeness of the NLG system, i.e. the similarity between system-generated REs and human121 A second characteristic of most existing RE generation systems is that they are limited to generating single noun phrases in isolation. By contrast, planning-based approaches (Appelt, 1985; Stone et al., 2003; Koller and Stone, 2007) generate REs in the context of an entire sentence or even discourse (Garoufi and Koller, 2010), and can therefore exploit and manipulate the linguistic and non-linguistic context in order to produce succinct REs (Stone and Webber, 1998). However, these approaches have not been combined with corpus-based measures of humanlikeness or understandability of REs. In this paper, we present the mSCRISP system, which extends the planning-based approach to NLG with a statistical model of RE understandability. mSCRISP uses a metric planner (Hoffmann, 2002) to compute the best REs that refer uniquely to the target referent, and thus combines statistical and symbolic reasoning. We obtain the cost m"
W11-2815,W11-2851,1,0.827734,"sed in the low successfulness dataset. The distinction is relevant because mSCRISP does not attempt to mimic human IG choices under all circumstances; it only does so when it believes that the human IG choices are highly successful. If this is not the case, it makes different choices—those that a more successful IG might make in the situation. 6.4 Task-based evaluation To verify the model’s performance in the context of real interactions with human IFs, we entered mSCRISP and the correct RE generating baseline EqualCosts as participating NLG systems for the 2011 edition of the GIVE Challenge (Garoufi and Koller, 2011; Striegnitz et al., 2011). Both systems operate by first generating an RE (the first-attempt RE) for a given button target as soon as the IF is in the target’s room and can see the target. Subsequently, the systems issue follow-up REs at regular intervals until the IF responds with a manipulation act or navigates away from the target. Follow-up REs may differ from first-attempt REs, especially for the mSCRISP system, which relies for its attribute selection on several dynamically changing context features of the scene (see Table 2). Indeed, mSCRISP issues follow-up REs that are difresol. succ"
W11-2815,W07-2307,0,0.15154,"Missing"
W11-2815,D10-1040,0,0.0466523,"n a system to produce REs that are easy to understand by humans. There are a number of related systems which optimize for understandability. Paraboni et al. (2007) present two rule-based RE generation systems which can deliberately produce redundant REs, and evaluate these systems to show that they out1 http://www.give-challenge.org/research/ page.php?id=give-2.5-index 122 perform earlier systems in terms of understandability. On the other hand, their approach is not corpusbased and is therefore harder to fine-tune to the communicative needs of hearers using empirically determined parameters. Golland et al. (2010) present a maximum entropy model which acts optimally with respect to a hearer model; but their system is focussed on spatial descriptions of objects in nondynamic scenes. Furthermore, dialogue and NLG systems based on reinforcement learning optimize their expected utility for human or simulated users. However, because of the complexity of reinforcement learning, this has for the greatest part been applied to RE generation only in the most rudimentary way, e.g. to distinguish whether or not to use jargon in a technical dialogue (Janarthanam and Lemon, 2010). Decision-making problems of a broad"
W11-2815,P10-1008,0,0.0253935,"rs using empirically determined parameters. Golland et al. (2010) present a maximum entropy model which acts optimally with respect to a hearer model; but their system is focussed on spatial descriptions of objects in nondynamic scenes. Furthermore, dialogue and NLG systems based on reinforcement learning optimize their expected utility for human or simulated users. However, because of the complexity of reinforcement learning, this has for the greatest part been applied to RE generation only in the most rudimentary way, e.g. to distinguish whether or not to use jargon in a technical dialogue (Janarthanam and Lemon, 2010). Decision-making problems of a broader scope have started getting addressed by such techniques only very recently (Dethlefs et al., 2011). Finally, NLG systems based on planning, such as Koller and Stone (2007), typically optimize for RE size instead of either humanlikeness or understandability. One exception is Bauer and Koller (2010), where sentence generation with a probabilistic grammar formalism is performed using a metric planner. That work generates REs which are probable and therefore in a certain sense humanlike; yet it focuses on syntactic choice and does not take understandability"
W11-2815,P07-1043,1,0.862016,"s gained increasing attention (Krahmer et al., 2003; Viethen et al., 2008). This process has been accelerated by the systematic evaluation of RE generation systems in the context of RE generation challenges (Belz and Gatt, 2007; Gatt and Belz, 2010). Almost all of these approaches optimize the humanlikeness of the NLG system, i.e. the similarity between system-generated REs and human121 A second characteristic of most existing RE generation systems is that they are limited to generating single noun phrases in isolation. By contrast, planning-based approaches (Appelt, 1985; Stone et al., 2003; Koller and Stone, 2007) generate REs in the context of an entire sentence or even discourse (Garoufi and Koller, 2010), and can therefore exploit and manipulate the linguistic and non-linguistic context in order to produce succinct REs (Stone and Webber, 1998). However, these approaches have not been combined with corpus-based measures of humanlikeness or understandability of REs. In this paper, we present the mSCRISP system, which extends the planning-based approach to NLG with a statistical model of RE understandability. mSCRISP uses a metric planner (Hoffmann, 2002) to compute the best REs that refer uniquely to"
W11-2815,J03-1003,0,0.216694,"Missing"
W11-2815,J07-2004,0,0.229797,"Missing"
W11-2815,W06-1412,0,0.124993,"mSCRISP is based in Section 3. Section 4 lays out how we obtain a maximum entropy model of RE attribute preferences from our corpus, and Section 5 shows how we bring the two approaches together using metric planning. We present the evaluation in Section 6 and conclude in Section 7. 2 Related work Our work stands in a recent tradition of approaches that attempt to learn optimal RE generation strategies from corpora. For instance, Viethen et al. (2008) tune the parameters of the graph-based algorithm of Krahmer et al. (2003) by learning attribute costs from the TUNA corpus (Gatt et al., 2007). Stoia et al. (2006) share with us a focus on situated generation in a virtual environment. They train a decision tree learner using a wide range of context features, including dialog history, spatio-visual information and features capturing relations between objects in the scene. The context features we use in this paper are partially inspired by theirs. However, our work differs from this line of research in that we do not primarily attempt to replicate the REs produced by humans, but to train a system to produce REs that are easy to understand by humans. There are a number of related systems which optimize for"
W11-2815,W98-1419,0,0.431134,"Belz, 2010). Almost all of these approaches optimize the humanlikeness of the NLG system, i.e. the similarity between system-generated REs and human121 A second characteristic of most existing RE generation systems is that they are limited to generating single noun phrases in isolation. By contrast, planning-based approaches (Appelt, 1985; Stone et al., 2003; Koller and Stone, 2007) generate REs in the context of an entire sentence or even discourse (Garoufi and Koller, 2010), and can therefore exploit and manipulate the linguistic and non-linguistic context in order to produce succinct REs (Stone and Webber, 1998). However, these approaches have not been combined with corpus-based measures of humanlikeness or understandability of REs. In this paper, we present the mSCRISP system, which extends the planning-based approach to NLG with a statistical model of RE understandability. mSCRISP uses a metric planner (Hoffmann, 2002) to compute the best REs that refer uniquely to the target referent, and thus combines statistical and symbolic reasoning. We obtain the cost model by training a maximum entropy (maxent) classifier on a corpus of human-generated instruction giving sessions (Gargett et al., 2010) in wh"
W11-2815,W08-1109,0,0.0178233,"easy to understand for a hearer. However, we do not assume that the same attribute types are easy to understand (i.e., have high successfulness) in all possible contexts. A color attribute may be easier to understand in a scene where there are no distractors of the same color as the referent—not just because it is conspicuous, but also because the hearer will not be visually distracted by similar distractors. Conversely, if a visually salient landmark is available for describing the target referent, it might be harder to process the referent’s color than its location relative to the landmark (Viethen and Dale, 2008). We model this connection of the RE resolution process with the currently visible scene through a collection of ten context features, which we list in Table 2. For our experiments, we extract most of these features from the corpus automatically, except for the Round and ReferenceAttempt features, which we annotated manually. For each object relations and referent’s distinctiveness feature, we consider as scope of comparison (near the referent, in the referent’s room or in the whole virtual world) the one that yields best results in Subsection 6.1. Note that some context features (such as Micr"
W11-2815,viethen-etal-2008-controlling,0,0.336161,"ting Instructions in Virtual Environments verifies the higher RE success scores of the system. 1 Introduction The generation of referring expressions (REs) is one of the best-studied problems in natural language generation (NLG). Traditional approaches (Dale and Reiter, 1995) have focused on defining the range of possible valid REs (e.g., as those REs that describe the target object uniquely) and on simple heuristics for choosing one valid RE (e.g., minimal REs). Recently, the question of how to choose the best RE out of the possible ones has gained increasing attention (Krahmer et al., 2003; Viethen et al., 2008). This process has been accelerated by the systematic evaluation of RE generation systems in the context of RE generation challenges (Belz and Gatt, 2007; Gatt and Belz, 2010). Almost all of these approaches optimize the humanlikeness of the NLG system, i.e. the similarity between system-generated REs and human121 A second characteristic of most existing RE generation systems is that they are limited to generating single noun phrases in isolation. By contrast, planning-based approaches (Appelt, 1985; Stone et al., 2003; Koller and Stone, 2007) generate REs in the context of an entire sentence"
W11-2815,W10-4233,1,\N,Missing
W11-2815,W11-2845,1,\N,Missing
W11-2845,W11-2852,1,0.815999,"Missing"
W11-2845,W11-2849,1,0.873641,"Missing"
W11-2845,W11-2850,1,0.812133,"Missing"
W11-2845,W11-2847,0,0.044264,"Missing"
W11-2845,W11-2846,0,0.0573081,"Missing"
W11-2845,gargett-etal-2010-give,1,0.597608,"ous GIVE editions, these systems employ more varied approaches and are better grounded in the existing CL and NLG literature. Systems A, C, L, and T are rule-based systems using hand-designed strategies. System A focuses on user engagement, T and C both focus on giving appropriate feedback to the user with C implementing the grounding model of Traum (1999), and L uses a strategy for generating referring expressions based on the Salmon-Alt and Romary (2000) approach to modeling the salience of objects. System B uses decision trees learned from a corpus of human interactions in the GIVE domain (Gargett et al., 2010) augmented with additional annotations. System P1 uses the same corpus to learn to predict the understandability of referring expressions. The model acquired in this way is integrated into an NLG strategy based on planning. System P2 serves as a baseline for comparison against P1. Finally, system CL selects instructions from a corpus of human-human interactions in the evaluation worlds that the CL team collected during the internal evaluation phase. See the individual system descriptions in this volume for more details about each system. 3.5 Recruiting subjects We used a variety of avenues to"
W11-2845,W11-2851,1,0.742,"Missing"
W11-2845,W11-2830,0,0.0400344,"Missing"
W11-2845,E09-2009,1,0.723585,"k, in combination with more complex evaluation worlds, the success rate was substantially worse in GIVE-2 than in GIVE-1. GIVE-2.5 is an opportunity to learn from the GIVE-2 experiences and improve on these results. 3 Evaluation Method See (Koller et al., 2010a) for a detailed presentation of the GIVE data collection method. This section describes the aspects specific to GIVE-2.5, such as the timeline, the evaluation worlds, the participating NLG systems, and our strategy for recruiting subjects. 3.1 Software infrastructure GIVE-2.5 reuses the software infrastructure from GIVE-2 described in (Koller et al., 2009) and (Koller et al., 2010b). Parts of the code were rewritten to improve how the visibility of objects is computed and how messages are sent between the components of the GIVE infrastructure: matchmaker, NLG system, and 3D client. The code is freely available at http://code.google.com/p/give2. 3.2 Timeline GIVE-2.5 was first announced in July 2010. Interested research teams could start development right away, since the software interface would be the same as in GIVE-2. The participating teams had to make their systems available for an internal evaluation period by May 23, 2011. This allowed th"
W11-2845,W11-2848,0,0.0837085,"Missing"
W11-2845,W10-4233,1,\N,Missing
W11-2851,gargett-etal-2010-give,1,0.447805,"n this paper, we report on our participation in the third installment of GIVE (GIVE-2.5; Striegnitz et al. (2011)). Although most of the work on the generation of referring expressions (REs) to 307 While we cannot present the RE generation modules in detail here (see Garoufi and Koller (2011) for that), note that P1 implements the hybrid model mSCRISP of Garoufi and Koller, which extends the planning-based approach to sentence generation (Koller and Stone, 2007) with a statistical model of RE success. This model was learnt from a corpus of human instruction giving sessions in the GIVE domain (Gargett et al., 2010), in which every RE was annotated with a measure of how easy it has been for the hearer to resolve. System P1 is therefore designed to optimize the REs it generates for understandability. On the other hand, system P2 is an implementation of the baseline model EqualCosts of Garoufi and Koller. This is a purely symbolic model that always computes a correct and unique RE, but does so without any empirical guidance about expected understandability. System P2 behaves in the exact same way as P1 in all respects, with the exception of the RE generation module. It therefore serves as a qualified basel"
W11-2851,W11-2815,1,0.869581,"Konstantina Garoufi and Alexander Koller Area of Excellence “Cognitive Sciences” University of Potsdam, Germany {garoufi, akoller}@uni-potsdam.de Abstract date has focused either on logical properties of REs, such as uniqueness and minimality, or on their degree of similarity to human-produced expressions (see Krahmer and van Deemter (To appear) for a comprehensive survey), we believe that it would be desirable to optimize a system directly for usefulness. We therefore approach the RE generation task with a model that aims at computing the unique RE which is fastest for the hearer to resolve (Garoufi and Koller, 2011). The purpose of the Potsdam NLG systems P1 and P2 at the challenge was to assess with a task-based evaluation to what extent the model actually manages to do so. We present the Potsdam natural language generation systems P1 and P2 of the GIVE-2.5 Challenge. The systems implement two different referring expression generation models from Garoufi and Koller (2011) while behaving identically in all other respects. In particular, P1 combines symbolic and corpus-based methods for the generation of successful referring expressions, while P2 is based on a purely symbolic model which serves as a quali"
W11-2851,P07-1043,1,0.855776,"luation, GIVE has been successful in attracting both a large number of volunteers for the IF role and a high level of interest from the research community. In this paper, we report on our participation in the third installment of GIVE (GIVE-2.5; Striegnitz et al. (2011)). Although most of the work on the generation of referring expressions (REs) to 307 While we cannot present the RE generation modules in detail here (see Garoufi and Koller (2011) for that), note that P1 implements the hybrid model mSCRISP of Garoufi and Koller, which extends the planning-based approach to sentence generation (Koller and Stone, 2007) with a statistical model of RE success. This model was learnt from a corpus of human instruction giving sessions in the GIVE domain (Gargett et al., 2010), in which every RE was annotated with a measure of how easy it has been for the hearer to resolve. System P1 is therefore designed to optimize the REs it generates for understandability. On the other hand, system P2 is an implementation of the baseline model EqualCosts of Garoufi and Koller. This is a purely symbolic model that always computes a correct and unique RE, but does so without any empirical guidance about expected understandabili"
W11-2851,W10-4233,1,\N,Missing
W11-2851,W11-2845,1,\N,Missing
W11-2851,N06-2040,0,\N,Missing
W11-2902,J09-4009,0,0.0211078,"and T2∗ are interpretations of the same derivation tree t ∈ L(G). As above, we can parse synchronously by parsing separately for the two interpretations and intersecting the results. This yields a parsing complexity for SCFG parsing of O(nm+1 · nm+1 ), where n1 and n2 are the lengths 1 2 of the input strings and m is the rank of the RTG G. Unlike in the monolingual case, this is now consistent with the result that the membership problem of SCFGs is NP-complete (Satta and Peserico, 2005). The reason for the intractability of SCFG parsing is that SCFGs, in general, cannot be binarized. However, Huang et al. (2009) define the class of binarizable SCFGs, which can be brought into a weakly equivalent normal form in which all production rules are binary and the membership problem can be solved in time O(n31 · n32 ). The key property of binarizable SCFGs, in our terms, is that if r is any production rule pair of the SCFG, h1 (r) and h2 (r) can be chosen in such a way that they can be transformed into each other by locally swapping the subterms of a node. For instance, an SCFG rule pair hA → A1 A2 A3 A4 , B → B3 B4 B2 B1 i can be represented by h1 (r) = (x1 • x2 ) • (x3 • x4 ) and h2 (r) = (x4 • x3 ) • (x1 •"
W11-2902,P89-1018,0,0.0582684,"her formalisms, such as STSG, the nonterminals of Ga generally record non7 terminals of G and positions in the input objects, as encoded in the nonterminals of D(a1 ), . . . , D(an ); the spans [i, k] occurring in CKY parse items simply happen to be the nonterminals of the D(a) for the string algebra. In fact, we maintain that the fundamental purpose of a chart is to act as a device for generating the set of derivation trees for an input. This treegenerating nature of parse charts is made explicit by modeling them directly as RTGs; the well-known view of parse charts as context-free grammars (Billot and Lang, 1989) captures the same intuition, but abuses context-free grammars (which are primarily string-generating devices) as tree description formalisms. One difference between the two views is that regular tree languages are closed under intersection, which means that parse charts that are modeled as RTGs can be easily restricted by external constraints (see Koller and Thater (2010) for a related approach), whereas this is hard in the context-free view. 4.4 S0,7 VP1,7 VP1,7 NP2,7 N3,7 VP1,4 NP2,4 PP4,7 → → → → → → → → r1 (NP0,1 , VP1,7 ) r3 (V1,2 , NP2,7 ) r5 (VP1,4 , PP4,7 ) r2 (Det2,3 , N3,7 ) r4 (N3,"
W11-2902,J07-2003,0,0.0716943,"which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 1 Introduction Over the past years, grammar formalisms that relate pairs of grammatical structures have received much attention. These formalisms include synchronous grammars (Lewis and Stearns, 1968; Shieber and Schabes, 1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of formalisms are available, it is useful to take a step back and look for a generalized model that explains the precise formal relationship between them. There is a long tradition of such research on monolingual"
W11-2902,E09-1053,1,0.843824,"(LCFRS, VijayShanker et al. (1987)). LCFRSs are essentially GCFGs with a “yield” homomorphism that maps objects of A to strings or tuples of strings. Therefore every grammar formalism that can be seen as an LCFRS, including certain dependency grammar formalisms (Kuhlmann, 2010), can be phrased as string-generating IRTGs. One particular advantage that our framework has over LCFRS is that we do not need to impose a bound on the length of the string tuples. This makes it possible to model formalisms such as combinatory categorial grammar (Steedman, 2001), which may be arbitrarily discontinuous (Koller and Kuhlmann, 2009). 7 Conclusion In this paper, we have defined interpreted RTGs, a grammar formalism that generalizes over a wide range of existing formalisms, including various synchronous grammars, tree transducers, and LCFRS. We presented a generic parser for IRTGs; to apply it to a new type of IRTG, we merely need to define how to compute decomposition grammars D(a) for input objects a. This makes it easy to define synchronous grammars that are heterogeneous both in the grammar formalism and in the objects that each of its dimensions describes. The purpose of our paper was to pull together a variety of exi"
W11-2902,P10-1004,1,0.838507,"as a device for generating the set of derivation trees for an input. This treegenerating nature of parse charts is made explicit by modeling them directly as RTGs; the well-known view of parse charts as context-free grammars (Billot and Lang, 1989) captures the same intuition, but abuses context-free grammars (which are primarily string-generating devices) as tree description formalisms. One difference between the two views is that regular tree languages are closed under intersection, which means that parse charts that are modeled as RTGs can be easily restricted by external constraints (see Koller and Thater (2010) for a related approach), whereas this is hard in the context-free view. 4.4 S0,7 VP1,7 VP1,7 NP2,7 N3,7 VP1,4 NP2,4 PP4,7 → → → → → → → → r1 (NP0,1 , VP1,7 ) r3 (V1,2 , NP2,7 ) r5 (VP1,4 , PP4,7 ) r2 (Det2,3 , N3,7 ) r4 (N3,4 , PP4,7 ) r3 (V1,2 , NP2,4 ) r2 (Det2,3 , N3,4 ) r6 (P4,5 , NP5,7 ) NP5,7 NP0,1 V1,2 Det2,3 N3,4 P4,5 Det5,6 N6,7 → → → → → → → → r2 (Det5,6 , N6,7 ) r7 r11 r8 r9 r12 r8 r10 Figure 5: A “parse chart” RTG for the sentence “Sue watches the man with the telescope”. 5 Membership and Binarization A binarization transforms an m-ary grammar into an equivalent binary one. Binari"
W11-2902,P03-2041,0,0.0299975,"work that generalizes a variety of monolingual and synchronous grammar formalisms for parsing and translation. Our framework is based on regular tree grammars that describe derivation trees, which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 1 Introduction Over the past years, grammar formalisms that relate pairs of grammatical structures have received much attention. These formalisms include synchronous grammars (Lewis and Stearns, 1968; Shieber and Schabes, 1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of formalisms are avai"
W11-2902,P10-1109,0,0.0401155,"(a1 , . . . , an ) = bin−1 (L(H)); this is where the exponential blowup can happen. The constructions in Sections 5.1 and 5.2 are both special cases of this generalized approach, which however also maintains a clear connection to the strong generative capacity. It is not obvious to us that it is necessary that the homomorphisms h2i must be delabelings for the membership algorithm to be optimal. Exploring this landscape, which ties in with the very active current research on binarization, is an interesting direction for future research. 6 into a more powerful algebra. This approach is taken by Maletti (2010), who uses an ordinary tree homomorphism to map a derivation tree t into a tree t0 of ‘building instructions’ for a derived tree, and then applies a function ·E to execute these building instructions and build the TAG derived tree. Maletti’s approach fits nicely into our framework if we assume an algebra in which the building instruction symbols are interpreted according to ·E . Synchronous tree-adjoining grammars (Shieber and Schabes, 1990) can be modeled simply as an RTG with two separate TAG interpretations. We can separately choose to interpret each side as trees or strings, as described i"
W11-2902,N10-1035,1,0.339654,"Missing"
W11-2902,J08-3004,0,0.0901766,"egular tree grammars – which generalizes both synchronous grammars, tree transducers, and LCFRSstyle monolingual grammars. A grammar of this formalism consists of a regular tree grammar (RTG, Comon et al. (2007)) defining a language of derivation trees and an arbitrary number of interpretations which map these trees into objects of arbitrary algebras. This allows us to capture a wide variety of (synchronous and monolingual) grammar formalisms. We can also model heterogeneous synchronous languages, which relate e.g. trees with strings; this is necessary for applications in machine translation (Graehl et al., 2008) and in parsing strings with synchronous tree grammars. Second, we also provide parsing and decoding algorithms for our framework. The key concept that we introduce is that of a regularly decomposable algebra, where the set of all terms that evaluate to a given object form a regular tree language. Once an algorithm that computes a compact representation of this language is known, parsing algorithms follow from a generic construction. All important algebras in natural language processing that we are aware of – in particular the standard algebras of strings and trees – are regularly decomposable"
W11-2902,P96-1016,0,0.0867666,"e present a formal framework that generalizes a variety of monolingual and synchronous grammar formalisms for parsing and translation. Our framework is based on regular tree grammars that describe derivation trees, which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 1 Introduction Over the past years, grammar formalisms that relate pairs of grammatical structures have received much attention. These formalisms include synchronous grammars (Lewis and Stearns, 1968; Shieber and Schabes, 1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of form"
W11-2902,C92-2065,0,0.263568,"apping derivation trees into derived trees. This cannot easily be done by a homomorphic interpretation in an ordinary tree algebra. One way to deal with this, which is taken by Shieber (2006), is to replace homomorphisms by a more complex class of tree translation functions called embedded pushdown tree transducers. A second approach is to interpret homomorphically 10 these assume that all RTG rule applications are statistically independent. That is, the canonical probabilistic version of context-free grammars is PCFG, and the canonical probabilistic version of tree-adjoining grammar is PTAG (Resnik, 1992). A final point is that Graehl et al. invest considerable effort into defining different versions of their transducer training algorithms for the tree-to-tree and tree-to-string translation cases. The core of their paper, in our terms, is to define synchronous parsing algorithms to compute an RTG of derivation trees for (tree, tree) and (tree, string) input pairs. In their setup, these two cases are formally completely different objects, and they define two separate algorithms for these problems. Our approach is more modular: The training and parsing algorithms can be fully generic, and all th"
W11-2902,H05-1101,0,0.0108654,"ons of the RTG G into string algebras T1∗ and T2∗ ; as above, the synchronization is ensured by requiring that related strings in T1∗ and T2∗ are interpretations of the same derivation tree t ∈ L(G). As above, we can parse synchronously by parsing separately for the two interpretations and intersecting the results. This yields a parsing complexity for SCFG parsing of O(nm+1 · nm+1 ), where n1 and n2 are the lengths 1 2 of the input strings and m is the rank of the RTG G. Unlike in the monolingual case, this is now consistent with the result that the membership problem of SCFGs is NP-complete (Satta and Peserico, 2005). The reason for the intractability of SCFG parsing is that SCFGs, in general, cannot be binarized. However, Huang et al. (2009) define the class of binarizable SCFGs, which can be brought into a weakly equivalent normal form in which all production rules are binary and the membership problem can be solved in time O(n31 · n32 ). The key property of binarizable SCFGs, in our terms, is that if r is any production rule pair of the SCFG, h1 (r) and h2 (r) can be chosen in such a way that they can be transformed into each other by locally swapping the subterms of a node. For instance, an SCFG rule"
W11-2902,W08-2319,0,0.190571,"Missing"
W11-2902,P01-1061,0,0.0303276,"eous both in the grammar formalism and in the objects that each of its dimensions describes. The purpose of our paper was to pull together a variety of existing research and explain it in a new, unified light: We have not shown how to do something that was not possible before, only how to do it in a uniform way. Nonetheless, we expect that future work will benefit from the clarified formal setup we have proposed here. In particular, we believe that the view of parse charts as RTGs may lead to future algorithms which exploit their closure under intersection, e.g. to reduce syntactic ambiguity (Schuler, 2001). Generalized Context-Free Grammars Finally, the view we advocate here embraces a tradition of grammar formalisms going back to generalized context-free grammar (GCFG, Pollard (1984)), which follows itself research in theoretical computer science (Mezei and Wright, 1967; Goguen et al., 1977). A GCFG grammar can be seen as an RTG over a signature Σ whose trees are evaluated as terms of some Σ-algebra A. This is a special case of an IRTG, in which the homomorphism is simply the identical function on TΣ , and the algebra is A. In fact, we could have equivalently defined an IRTG as an RTG whose tr"
W11-2902,C90-3045,0,0.551406,"applied to existing grammar formalisms. We present a formal framework that generalizes a variety of monolingual and synchronous grammar formalisms for parsing and translation. Our framework is based on regular tree grammars that describe derivation trees, which are interpreted in arbitrary algebras. We obtain generic parsing algorithms by exploiting closure properties of regular tree languages. 1 Introduction Over the past years, grammar formalisms that relate pairs of grammatical structures have received much attention. These formalisms include synchronous grammars (Lewis and Stearns, 1968; Shieber and Schabes, 1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shie"
W11-2902,W04-3312,0,0.0677419,"ysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of formalisms are available, it is useful to take a step back and look for a generalized model that explains the precise formal relationship between them. There is a long tradition of such research on monolingual grammar formalisms, where e.g. linear context-free rewriting systems (LCFRS, Vijay-Shanker et al. (1987)) generalize various mildly context-sensitive formalisms. However, few such results exist for synchronous formalisms. A notable exception is the work by Shieber (2004), who unified synchronous treeadjoining grammars with tree transducers. 2 Proceedings of the 12th International Conference on Parsing Technologies, pages 2–13, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. 2 Formal Foundations where { ti /xi |i ∈ [n] } represents the substitution that replaces all occurrences of xi with the respective ti . A homomorphism is called linear if every term h(f ) contains each variable at most once; and a delabeling if every term h(f ) is of the form g(xπ(1) , . . . , xπ(n) ) where n is the rank of f and π a permutation"
W11-2902,E06-1048,0,0.529935,"1990; Shieber, 1994; Rambow and Satta, 1996; Eisner, 2003) and tree transducers (Comon et al., 2007; Graehl et al., 2008). Weighted variants of both of families of formalisms have been used for machine translation (Graehl et al., 2008; Chiang, 2007), where one tree represents a parse of a sentence in one language and the other a parse in the other language. Synchronous grammars and tree transducers are also useful as models of the syntax-semantics interface; here one tree represents the syntactic analysis of a sentence and the other the semantic analysis (Shieber and Schabes, 1990; Nesson and Shieber, 2006). When such a variety of formalisms are available, it is useful to take a step back and look for a generalized model that explains the precise formal relationship between them. There is a long tradition of such research on monolingual grammar formalisms, where e.g. linear context-free rewriting systems (LCFRS, Vijay-Shanker et al. (1987)) generalize various mildly context-sensitive formalisms. However, few such results exist for synchronous formalisms. A notable exception is the work by Shieber (2004), who unified synchronous treeadjoining grammars with tree transducers. 2 Proceedings of the 1"
W11-2902,P87-1015,0,0.816122,"Missing"
W11-2902,W90-0102,0,\N,Missing
W12-1604,W10-4203,0,0.0678075,"right feedback can be challenging. For example, in the original interaction from which we took the screenshot of Fig. 1, the system instructed the user to “push the right button to the right of the green button”, referring to the rightmost blue button in the scene. In response, the user first walked hesitantly towards the far pair of buttons (green and blue), and then turned to face the other pair, as seen in Fig. 3. A typical NLG system used Figure 3: The scene of Fig. 1, after the user moved and turned in response to a referring expression. in the GIVE Challenge (e.g., Dionne et al. (2009), Denis (2010), Racca et al. (2011)) may try to predict how the user might resolve the RE based on the visibility of objects, timing data, or distances. Relying only on such data, however, even a human observer could have difficulties in interpreting the user’s reaction; the user in Fig. 3 ended up closer to the green and blue buttons, but the other buttons (the two blue ones) are, to similar degrees, visually in focus. The contribution of this paper is to present a method for monitoring the communicative success of an RE based on eyetracking. We start from the hypothesis that when the user resolves an RE t"
W12-1604,W09-0610,0,0.0243656,"Missing"
W12-1604,gargett-etal-2010-give,1,0.803842,"dback once for every newly approached or inspected button and will not repeat this feedback unless the user has approached or inspected another button in the meantime. Example interactions of a user with each of the three systems are presented in Appendix A. 4 Evaluation We set up a human evaluation study in order to assess the performance of the eyetracking system as compared against the two baselines on the situated instruction giving task. For this, we record participant interactions with the three systems employed in three different virtual environments. These environments were taken from Gargett et al. (2010); they vary as to the visual and spatial properties of the objects they contain. One of these environments is shown in Fig. 2. Overall, 31 participants (12 females) were tested. All reported their English skills 34 as fluent, and all were capable of completing the tasks. Their mean age was 27.6 years. 4.1 Task and procedure A faceLAB eyetracking system (http://www. seeingmachines.com/product/facelab) remotely monitored participants’ eye movements on a 24-inch monitor, as in Fig. 4 and 5 of Appendix B. Before the experiment, participants received written instructions that described the task and"
W12-1604,W11-2851,1,0.824887,"rates positive or negative feedback based on this. 3.2 NLG systems To demonstrate the usefulness of the eyetrackingbased approach, we implemented and compared three different NLG systems. All of these use an identical module for generating navigation instructions, which guides the user to a specific location, as well as object manipulation instructions such as “push the blue button”; “the blue button” 33 is an RE that describes an object to the user. The systems generate REs that are optimized for being easy for the hearer to understand, according to a corpus-based model of understandability (Garoufi and Koller, 2011). The model was trained on human instructions produced in a subset of the virtual environments we use in this work. The resulting system computes referring expressions that are correct and uniquely describe the referent as seen by the hearer at the moment in which generation starts. Unlike in the original GIVE Challenge, the generated instructions are converted to speech by the Mary text-to-speech system (Schr¨oder and Trouvain, 2003) and presented via loudspeaker. At any point, the user may press the ‘H’ key on their keyboard to indicate that they are confused and request a clarification. Thi"
W12-1604,I11-1010,0,0.0627909,"as for the speaker’s monitoring process. In the context of situated dialog systems, previous studies have employed robots and virtual agents as speakers to explore how and when speaker gaze helps human hearers to ground referring expressions (Foster, 2007). For instance, Staudte and Crocker (2011) show that an agent can make it easier for the (human) hearer to resolve a system-generated RE by looking at the intended referent, using head and eye movements. Conversely, the performance of a system for resolving human-produced REs can be improved by taking the (human) speaker’s gaze into account (Iida et al., 2011). Gaze has also been used to track the general dynamics of a dialog, such as turn taking (Jokinen et al., in press). Here we are interested in monitoring the hearer’s gaze in order to determine whether they have understood an RE. To our knowledge, there has been no research on this; in particular, not in dynamic 3D environments. The closest earlier work of which we are aware comes from the context of the GIVE Challenge, a shared task for interactive, situated natural language generation systems. These systems typically approximate hearer gaze as visibility of objects on the screen and monitor"
W12-1604,W11-2848,0,0.0140529,"can be challenging. For example, in the original interaction from which we took the screenshot of Fig. 1, the system instructed the user to “push the right button to the right of the green button”, referring to the rightmost blue button in the scene. In response, the user first walked hesitantly towards the far pair of buttons (green and blue), and then turned to face the other pair, as seen in Fig. 3. A typical NLG system used Figure 3: The scene of Fig. 1, after the user moved and turned in response to a referring expression. in the GIVE Challenge (e.g., Dionne et al. (2009), Denis (2010), Racca et al. (2011)) may try to predict how the user might resolve the RE based on the visibility of objects, timing data, or distances. Relying only on such data, however, even a human observer could have difficulties in interpreting the user’s reaction; the user in Fig. 3 ended up closer to the green and blue buttons, but the other buttons (the two blue ones) are, to similar degrees, visually in focus. The contribution of this paper is to present a method for monitoring the communicative success of an RE based on eyetracking. We start from the hypothesis that when the user resolves an RE to a certain object, t"
W12-1604,E09-1085,0,0.0159897,"plan recognition or abductive or epistemic reasoning (see e.g. Young et al. (1994), Hirst et al. (1994)); in practice, many systems use more streamlined (Traum, 1994) or statistical methods (Paek and Horvitz, 1999). Most dialog systems focus on the verbal interaction of the system and user, and the user’s utterances are 31 therefore the primary source of evidence in the monitoring process. Some incremental dialog systems can monitor the user’s verbal reactions to the system’s utterances in real time, and continuously update the grounding state while the system utterance is still in progress (Skantze and Schlangen, 2009; Buss and Schlangen, 2010). In this paper, we focus on the generation side of a dialog system—the user is the hearer—and on monitoring the user’s extralinguistic reactions, in particular their gaze. Tanenhaus et al. (1995) and Allopenna et al. (1998) showed that subjects in psycholinguistic experiments who hear an RE visually attend to the object to which they resolve the RE. The “visual world” experimental paradigm exploits this by presenting objects on a computer screen and using an eyetracker to monitor the subject’s gaze. This research uses gaze only as an experimental tool and not as par"
W12-1604,W11-2845,1,\N,Missing
W12-4616,W05-1502,0,0.0213285,"entative, strikes a balance between the other two approaches. This research has done much to clarify the formal connections between TAG and other formalisms in terms of generative capacity. It has not been particularly productive with respect to finding new algorithms for parsing, training, and (in the synchronous case) decoding. This is regrettable because standard parsing algorithms for TAG (Vijay-Shanker and Joshi, 1985; Shieber et al., 1995) are complicated, require relatively involved correctness proofs, and are hard to teach. A similar criticism applies to parsing algorithms for LCFRSs (Burden and Ljunglöf, 2005). So far, no new parsing algorithms have arisen from Shieber’s work on bimorphisms, or from the CFTL-based view. Indeed, Maletti (2010) leaves the development of such algorithms as an open problem. This paper makes two contributions. First, we show how a number of the formal perspectives on TAG mentioned above can be recast in a uniform way as interpreted regular tree grammars (IRTGs, Koller and Kuhlmann (2011)). IRTGs capture the fundamental idea of generating strings, derived trees, or other objects from a regular tree language, and allow us to make the intuitive differences in how different"
W12-4616,N10-1035,1,0.845858,"ction 4. Unfortunately, this has the effect that the algebra contains operations of rank greater than two, which increases the parsing complexity. 5.1 concA .w1 ; w2 / D .w1 w21 ; w22 / T concA .w1 ; w2 / D .w11 ; w12 w2 / Finally, there is a binary partial wrapping operation wrap, which is defined if its first argument is a string pair. This operation wraps its first argument around the second, as follows: T wrapA .w1 ; w2 / D w11 w2 w12 T wrapA .w1 ; w2 / D .w11 w21 ; w22 w12 / Notice that these operations closely mirror the operations for well-nested LCFRSs with fan-out 2 that were used in Gómez-Rodríguez et al. (2010). 5.2 An IRTG for TAG String Languages We can use AT to construct, for any given TAG grammar G over some alphabet A of terminal symbols, an IRTG G D .G ; .hs ; AT // such that L.G/ consists of exactly the strings that are generated by G. We again describe the derivation trees using a ˙-RTG G . Say that AT is a -algebra. It remains to construct a homomorphism hs from T˙ to T . This is most easily done by defining a second homomorphism hst that maps from TD to AT . hst effectively reads off the yield of a tree or context, and is defined by mapping each operation symbol of TD to a term over A"
W12-4616,J09-4009,0,0.111425,"Missing"
W12-4616,W11-2902,1,0.922779,"Joshi, 1985; Shieber et al., 1995) are complicated, require relatively involved correctness proofs, and are hard to teach. A similar criticism applies to parsing algorithms for LCFRSs (Burden and Ljunglöf, 2005). So far, no new parsing algorithms have arisen from Shieber’s work on bimorphisms, or from the CFTL-based view. Indeed, Maletti (2010) leaves the development of such algorithms as an open problem. This paper makes two contributions. First, we show how a number of the formal perspectives on TAG mentioned above can be recast in a uniform way as interpreted regular tree grammars (IRTGs, Koller and Kuhlmann (2011)). IRTGs capture the fundamental idea of generating strings, derived trees, or other objects from a regular tree language, and allow us to make the intuitive differences in how different perspectives divide the labour over the various modules formally precise. Second, we introduce two new algebras. One captures TAG string languages; the other describes TAG derived tree languages. We show that both of these algebras are regularly decomposable, which means that the very modular algorithms that are available for parsing, training, and decoding of IRTGs can be applied to TAG. As an immediate conse"
W12-4616,P10-1109,0,0.214187,"tree is generated, then this derivation tree is mapped into a term over some algebra and evaluated there. Under this view, one can take different perspectives on how the labour of generating a string or derived tree should be divided between the mapping process and the algebra. In a way that we will make precise, linear context-free rewriting systems (LCFRSs, Weir (1988)) push much of the work into the algebra; Shieber (2006)’s analysis of synchronous TAG as bimorphisms puts the burden mostly on the mapping procedure; and a line of research using context-free tree languages (CFTLs), of which Maletti (2010) is a recent representative, strikes a balance between the other two approaches. This research has done much to clarify the formal connections between TAG and other formalisms in terms of generative capacity. It has not been particularly productive with respect to finding new algorithms for parsing, training, and (in the synchronous case) decoding. This is regrettable because standard parsing algorithms for TAG (Vijay-Shanker and Joshi, 1985; Shieber et al., 1995) are complicated, require relatively involved correctness proofs, and are hard to teach. A similar criticism applies to parsing algo"
W12-4616,C92-2065,0,0.410573,"Missing"
W12-4616,W08-2319,0,0.187265,"Missing"
W12-4616,W04-3312,0,0.0338373,"m substituting and adjoining t1 , t2 , and t3 into ˛1 at appropriate places. Using such functions, one can directly interpret the tree ˛1 .˛2 .nop/; nop; ˇ1 .nop// as the derived tree in Fig. 2c. Therefore, for the IRTG GL D .G0 ; .id; AL //, where id is the identity homomorphism on T˙0 , L.GL / is exactly the set of derived trees of G. One could instead obtain an IRTG for describing the string language of G by using an interpretation into a ˙0 -algebra of strings and string tuples in which the elementary trees evaluate to appropriate generalized concatenation operations. STAG as Bimorphisms. Shieber (2004) proposes a different perspective on the generative process of synchronous tree substitution grammar (STSG). He builds upon earlier work on bimorphisms and represents an STSG as an IRTG .G0 ; .h1 ; T1 /; .h2 ; T2 //, where T1 and T2 are appropriate term algebras. In this construction, the homomorphisms must carry some of the load: In an STSG whose left-hand side contains the trees ˛1 and ˛2 from Fig. 2a, we would have h1 .˛1 / D S.x1 ; VP .sleeps//. Shieber (2006) later extended this approach to STAG by replacing the tree homomorphisms with embedded push-down transducers, a more powerful t"
W12-4616,E06-1048,0,0.143524,"isms. A common approach in this line of research has been to conceive the way in which TAG generates a string or a derived tree from a grammar as a two-step process: first a derivation tree is generated, then this derivation tree is mapped into a term over some algebra and evaluated there. Under this view, one can take different perspectives on how the labour of generating a string or derived tree should be divided between the mapping process and the algebra. In a way that we will make precise, linear context-free rewriting systems (LCFRSs, Weir (1988)) push much of the work into the algebra; Shieber (2006)’s analysis of synchronous TAG as bimorphisms puts the burden mostly on the mapping procedure; and a line of research using context-free tree languages (CFTLs), of which Maletti (2010) is a recent representative, strikes a balance between the other two approaches. This research has done much to clarify the formal connections between TAG and other formalisms in terms of generative capacity. It has not been particularly productive with respect to finding new algorithms for parsing, training, and (in the synchronous case) decoding. This is regrettable because standard parsing algorithms for TAG ("
W12-4616,P85-1011,0,0.695953,"’s analysis of synchronous TAG as bimorphisms puts the burden mostly on the mapping procedure; and a line of research using context-free tree languages (CFTLs), of which Maletti (2010) is a recent representative, strikes a balance between the other two approaches. This research has done much to clarify the formal connections between TAG and other formalisms in terms of generative capacity. It has not been particularly productive with respect to finding new algorithms for parsing, training, and (in the synchronous case) decoding. This is regrettable because standard parsing algorithms for TAG (Vijay-Shanker and Joshi, 1985; Shieber et al., 1995) are complicated, require relatively involved correctness proofs, and are hard to teach. A similar criticism applies to parsing algorithms for LCFRSs (Burden and Ljunglöf, 2005). So far, no new parsing algorithms have arisen from Shieber’s work on bimorphisms, or from the CFTL-based view. Indeed, Maletti (2010) leaves the development of such algorithms as an open problem. This paper makes two contributions. First, we show how a number of the formal perspectives on TAG mentioned above can be recast in a uniform way as interpreted regular tree grammars (IRTGs, Koller and K"
W12-4616,P87-1015,0,0.573716,"Missing"
W14-5002,D10-1040,0,0.0637283,"Missing"
W14-5002,W08-1107,1,0.825404,"s is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruijff, 2006), logicbased (Areces et al., 2008) and graph-based (Krahmer et al., 2003) methods to the problem, and overall has been one of the success stories of NLG. However, in practice, human speakers frequently overspecify, i.e. they include information in an RE beyond what is necessary to make it distinguishing (Wardlow Lane and Ferreira, 2008; Koolen et al., 2011). An NLG system, too, might include redundant information in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we present an NLG system that directly us"
W14-5002,J08-3004,0,0.029925,"ociation for Computational Linguistics with probabilistic listener models. 2 gorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts fo"
W14-5002,C12-1083,0,0.0173023,"ional Linguistics with probabilistic listener models. 2 gorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challe"
W14-5002,W08-1127,0,0.0454178,"according to this model from the chart. 1 Introduction The fundamental challenge in the generation of referring expressions (REG) is to compute an RE which is effective, i.e. understood as intended by the listener. Throughout the history of REG, we have approximated this as the problem of generating distinguishing REs, i.e. REs that are only satisfied by a unique individual in the domain. This has been an eminently successful approach, as documented e.g. in the overview article of Krahmer and van Deemter (2012) and a variety of recent shared tasks involving RE generation (Gatt and Belz, 2010; Belz et al., 2008; Koller et al., 2010). Nonetheless, reducing effectiveness to uniqueness is limiting in several ways. First, in complex, real-world scenes it may not be feasible to generate fully distinguishing REs, or these may have to be exceedingly complicated. It is also not necessary to generate distinguishing REs in such situations, because listeners are very capable of taking the discourse and task context into account to resolve even ambiguous REs. Conversely, listeners can misunderstand even a distinguishing RE, so uniqueness is no guarantee for success. We propose instead to define and train a prob"
W14-5002,C00-1062,0,0.0773871,"re widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruij"
W14-5002,P96-1027,0,0.51399,"babilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it"
W14-5002,J07-2003,0,0.066913,"14. c 2014 Association for Computational Linguistics with probabilistic listener models. 2 gorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first"
W14-5002,P06-1131,0,0.0331174,"nd Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruijff, 2006), logicbased (Areces et al., 2008) and graph-based (Krahmer et al., 2003) methods to the problem, and overall has been one of the success stories of NLG. However, in practice, human speakers frequently overspecify, i.e. they include information in an RE beyond what is necessary to make it distinguishing (Wardlow Lane and Ferreira, 2008; Koolen et al., 2011). An NLG system, too, might include redundant information in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we pres"
W14-5002,W11-2902,1,0.707168,"G = (G, IS , IR ) of an RTG G over some signature ⌃, together with a string interpretation IS over some alphabet and a relational interpretation IR over some universe U , both of which interpret the symbols in ⌃. We assume that every terminal symbol r 2 ⌃ occurs in at most one rule, and that the nonterminals of G are pairs Ab of a syntactic category A and a semantic index b = ix(Ab ). A semantic index indicates the individual in U to which a given constituent is meant to refer, see e.g. (Kay, 1996; Stone et al., 2003). Note that SIGs can be seen as specific Interpreted Regular Tree Grammars (Koller and Kuhlmann, 2011) with a set and a string interpretation. We ignore the start symbol of G. Instead, we say that given some individual b 2 U and syntactic category A, the set of referring expressions for b is REG (A, b) = {t 2 LAb (G) |IR (t) = {b}}, i.e. we define an RE as a derivation tree that G can derive from Ab and whose relational interpretation is {b}. From t, we can read off the string IS (t).1 3.3 for all a 2 button: Na ! buttona IS (buttona ) = button IR (buttona ) = button for all a 2 round: Na ! rounda (Na ) IS (rounda )(w1 ) = round • w1 IR (rounda )(R1 ) = round 1 R1 for all a 2 square: Na ! squ"
W14-5002,P07-1043,1,0.795583,"RE 6 Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 6–15, Philadelphia, Pennsylvania, 19 June 2014. c 2014 Association for Computational Linguistics with probabilistic listener models. 2 gorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realizat"
W14-5002,D13-1134,1,0.356671,"al. (2013) describe a stochastic algorithm that computes human-like, non-relational REs that may not be distinguishing. Golland et al. (2010) are close to our proposal in spirit, in that they use a loglinear probability model of RE resolution to compute a possibly non-distinguishing RE. However, they use a trivial REG algorithm which is limited to grammars that only permit a (small) finite set of REs for each referent. This is in contrast to general REG, where there is typically an infinite set of valid REs, especially when relational REs (“the button to the left of the plant”) are permitted. Engonopoulos et al. (2013) describe how to update an estimate for P (a|t) based on a log-linear model based on observations of the listener’s behavior. They use a shallow model based on a string t and not an RE derived from a grammar, and they do not discuss how to generate the best t. The al3 Grammars for RE generation We define a new grammar formalism that we use for REG, which we call semantically intepreted grammar (SIG). SIG is a synchronous grammar formalism that relates natural language strings with the sets of objects in a given domain which they describe. It uses regular tree grammars (RTGs) to describe langua"
W14-5002,D13-1197,0,0.01959,"2011). An NLG system, too, might include redundant information in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we present an NLG system that directly uses a probabilistic model of RE resolution, and is capable of generating ambiguous REs if it predicts that the listener will understand them. Most existing REG algorithms focus on generating distinguishing REs, and then select the one that is best according to some criterion, e.g. most human-like (Krahmer et al., 2003; FitzGerald et al., 2013) or most likely to be understood (Garoufi and Koller, 2013). By contrast, Mitchell et al. (2013) describe a stochastic algorithm that computes human-like, non-relational REs that may not be distinguishing. Golland et al. (2010) are close to our proposal in spirit, in that they use a loglinear probability model of RE resolution to compute a possibly non-distinguishing RE. However, they use a trivial REG algorithm which is limited to grammars that only permit a (small) finite set of REs for each referent. This is in contrast to general REG, where there is typically an infinite set of valid REs,"
W14-5002,P12-1039,0,0.0226327,"it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. Related Work RE generation is the task of generating a naturallanguage expression th"
W14-5002,J12-1006,0,0.0612559,"Missing"
W14-5002,J03-1003,0,0.424235,"Missing"
W14-5002,D11-1149,0,0.0195143,"ur formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. Related"
W14-5002,N13-1137,0,0.03348,"Missing"
W14-5002,P07-1121,0,0.0149377,"p here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or re"
W14-5002,W09-0629,0,\N,Missing
W15-0126,S14-2081,1,0.886398,"ns and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at the semantic dependency graphs"
W15-0126,C10-1011,0,0.057073,"g and untrimming, since conservative trimming does not increase the label sets. 5 Graph Parsing We proceed to evaluate our tree approximations in graph parsing. Here, our previously outlined parsing pipeline is applied: training graphs are converted to trees using different pre-processing approximations, parsers are trained and applied on test data, outputs are converted to graphs and evaluated against the gold standard graphs. We observe the labeled F1 scores (LF) and exact matches (LM). Experiment Setup For dependency tree parsing, we use the mate-tools state-of-the-art graphbased parser of Bohnet (2010). As in the shared task, we experiment in two tracks: the open track, and the closed track. In the closed track, for training the parser, we use only the features available in the SDP training data, i.e., word forms, parts of speech and lemmas. In the open track, we also pack additional features from the SDP companion dataset – automatic dependency and phrase-based parses of the SDP training and testing sets – as well as the Brown clustering features (Brown et al., 1992). For top node detection, we use a sequence tagger based on conditional random fields (CRFs). To guess the top nodes in the c"
W15-0126,J92-4003,0,0.089862,"and exact matches (LM). Experiment Setup For dependency tree parsing, we use the mate-tools state-of-the-art graphbased parser of Bohnet (2010). As in the shared task, we experiment in two tracks: the open track, and the closed track. In the closed track, for training the parser, we use only the features available in the SDP training data, i.e., word forms, parts of speech and lemmas. In the open track, we also pack additional features from the SDP companion dataset – automatic dependency and phrase-based parses of the SDP training and testing sets – as well as the Brown clustering features (Brown et al., 1992). For top node detection, we use a sequence tagger based on conditional random fields (CRFs). To guess the top nodes in the closed track, we use words and POS tags as features, while we add the companion syntactic features in the open track. 5.1 Results The evaluation results are listed in Table 3. The overall performance of our basic DFS tree approximation parser is identical to the one of Agi´c and Koller (2014) in the closed track. In the open track, however, we improve by 2-3 points in LF due to better top node detection, and improved tree parser accuracy due to the introduction of additio"
W15-0126,S14-2080,0,0.0199245,"ork, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree app"
W15-0126,C96-1058,0,0.12084,"a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to wi"
W15-0126,W12-3602,1,0.861041,"aphs from SDP 2014. The three SDP annotation layers over WSJ text stem from different semantic representations, but all result in directed acyclic graphs (DAGs) for describing sentence semantics. The three representations can be characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while DM HPSG annotations were manual, the annotations for training Enju were automatically constructed from the Penn Treebank bracketings by Miyao et al. (2004). 3. PCEDT originates from the English part of the Prague Czech–English Dependency Treebank. In this project, the WSJ part of PTB was translated into Czech, and both sides w"
W15-0126,S14-2068,0,0.0196154,"n the SDP evaluation framework. This system implements a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of t"
W15-0126,J93-2004,0,0.0542364,"Missing"
W15-0126,P13-2109,0,0.0284804,"y. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored"
W15-0126,S14-2082,0,0.0349885,"between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also"
W15-0126,S14-2056,1,0.866199,"he single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at the semantic dependency graphs from SDP 2014. The three SDP annotation layers over WSJ text stem from different semantic representations, but all result in directed acyclic graphs (DAGs) for describing sentence semantics. The three representations can be characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while D"
W15-0126,J08-1002,0,0.0147916,"characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while DM HPSG annotations were manual, the annotations for training Enju were automatically constructed from the Penn Treebank bracketings by Miyao et al. (2004). 3. PCEDT originates from the English part of the Prague Czech–English Dependency Treebank. In this project, the WSJ part of PTB was translated into Czech, and both sides were manually in accordance with the Prague-style rules for tectogrammatical analysis (Cinkov´a et al., 2009). The dataset is postprocessed by the task organizers to match the requirements for bi-lexical dependencies. Nodes in SDP"
W15-0126,P05-1013,0,0.0536753,"wo types of edge removal, which we name deletion and trimming. In deletion, it is not possible to reconstruct the removed edge in post-processing, i.e. the removed edge is permanently lost. In trimming, by contrast, the removed edge can be reconstructed – or untrimmed – in post-processing, either deterministically or with a certain success rate. In the shared task, a number of systems approached trimming through label overloading. In label overloading, a deletion of one edge is recorded in another kept edge, similar to encoding non-projective dependency arcs in pseudo-projective tree parsing (Nivre and Nilsson, 2005). In post-processing, the information stored in overloaded labels is used to attempt edge untrimming. We proceed to explore several ways of performing tree approximations, which include a mixture of edge removals via deletion and trimming. 3.1 Baselines Three baselines are used in this research. We re-implement the official SDP shared task baseline, and the local edge flipping and depth-first flipping systems of Agi´c and Koller (2014). OFFICIAL : The official baseline tree approximation only performs deletions and artificial edge insertions to satisfy the dependency tree constraints. No trimm"
W15-0126,S14-2008,1,0.883555,"evaluation framework. This system implements a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of t"
W15-0126,S14-2012,0,0.0192021,"properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy,"
W15-0126,C08-1095,0,0.0793274,"roaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outp"
W15-0126,S14-2034,0,0.235629,"ovel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at th"
W15-0126,S14-2027,0,0.217765,"Missing"
W15-0126,oepen-lonning-2006-discriminant,1,\N,Missing
W15-0127,P02-1041,0,0.0978417,"on formalism. Under the traditional view, the input representation of the semantic construction process is a syntactic parse tree; see e.g. Blackburn and Bos (2005) for a detailed presentation. One key challenge is to make sure that a semantic predicate combines with its semantic arguments correctly. In Montague Grammar, this is achieved by constructing through functional application of a suitably constructed lambda-term for the functor. Many unification-based approaches to semantic construction, e.g. in the context of HPSG (Copestake et al., 2001), TAG (Gardent and Kallmeyer, 2003), and CCG (Baldridge and Kruijff, 2002) use explicit argument slots that are identified either by a globally valid name (such as “subject”) or by a syntactic argument position. Copestake et al. (2001), in particular, conclude from their experiences from designing large-scale HPSG grammars that explicitly named arguments simplify the specification of the syntax-semantics interface and allow rules that generalize better over different syntactic structures. Their semantic algebra assigns to each syntactic constituent a semantic representation consisting of an MRS, together with hooks that make specific variable names of the MRS availa"
W15-0127,W13-2322,0,0.621116,"tlemoyer and Collins, 2005; Chiang et al., 2013). The idea in semantic parsing is to make a system learn the mapping of the string into the semantic representation automatically, with or without the use of an explicit syntactic representation as an intermediate step. The focus is thus on automatically induced grammars, if the grammar is not left implicit altogether. Training such data-driven models requires semantically annotated corpora. One recent development is the release of several corpora in which English sentences were annotated with graphs that represent the meanings of the sentences (Banarescu et al., 2013; Oepen et al., 2014). There has already been a body of research on semantic parsing for graphs based on these corpora, but so far, the ideas underlying these semantic parsers have not been connected to classical approaches to semantic construction. Flanigan et al. (2014) and Martins and Almeida (2014) show how to adapt data-driven dependency parsers from trees to graphs. These approaches do not use explicit grammars, in contrast to all work in the classical tradition. Chiang et al. (2013) do use explicit Hyperedge Replacement Grammars (HRGs, Drewes et al., 1997) for semantic parsing. However,"
W15-0127,P13-1015,1,0.889246,"Missing"
W15-0127,P13-1091,0,0.389342,"d-written toy grammars, sketch the use of s-graph grammars for data-driven semantic parsing, and discuss formal aspects. 1 Introduction Semantic construction is the problem of deriving a formal semantic representation from a natural-language expression. The classical approach, starting with Montague (1974), is to derive semantic representations from syntactic analyses using hand-written rules. More recently, semantic construction has enjoyed renewed attention in mainstream computational linguistics in the form of semantic parsing (see e.g. Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Chiang et al., 2013). The idea in semantic parsing is to make a system learn the mapping of the string into the semantic representation automatically, with or without the use of an explicit syntactic representation as an intermediate step. The focus is thus on automatically induced grammars, if the grammar is not left implicit altogether. Training such data-driven models requires semantically annotated corpora. One recent development is the release of several corpora in which English sentences were annotated with graphs that represent the meanings of the sentences (Banarescu et al., 2013; Oepen et al., 2014). The"
W15-0127,P01-1019,0,0.201531,"ntague (1974), who used higher-order logic as a semantic representation formalism. Under the traditional view, the input representation of the semantic construction process is a syntactic parse tree; see e.g. Blackburn and Bos (2005) for a detailed presentation. One key challenge is to make sure that a semantic predicate combines with its semantic arguments correctly. In Montague Grammar, this is achieved by constructing through functional application of a suitably constructed lambda-term for the functor. Many unification-based approaches to semantic construction, e.g. in the context of HPSG (Copestake et al., 2001), TAG (Gardent and Kallmeyer, 2003), and CCG (Baldridge and Kruijff, 2002) use explicit argument slots that are identified either by a globally valid name (such as “subject”) or by a syntactic argument position. Copestake et al. (2001), in particular, conclude from their experiences from designing large-scale HPSG grammars that explicitly named arguments simplify the specification of the syntax-semantics interface and allow rules that generalize better over different syntactic structures. Their semantic algebra assigns to each syntactic constituent a semantic representation consisting of an MR"
W15-0127,P07-2009,0,0.0389633,"d on deriving lambda terms, and are not obviously applicable to deriving graphs. 2.2 Graphs as semantic representations The recent data-driven turn of semantic parsing motivates the development of a number of semantically annotated corpora (“sembanks”). Traditional semantic representations, such as higher-order logic, have shown to be challenging to annotate. Even the Groningen Meaning Bank (Bos et al., 2014), the bestknown attempt at annotating a corpus with traditional semantic representations (DRT), uses a semiautomatic approach to semantic annotation, in which outputs of the Boxer system (Curran et al., 2007) are hand-corrected. A larger portion of recent sembanks use graphs as semantic representations. This strikes a middle ground, which is mostly restricted to predicate-argument structure, but can cover phenomena such as control, coordination, and coreference. In this paper, we will take the hand-annotated graphs of 229 want 1 an ne r Y girl 1 Y 2 2 1 1 ARG careless 0 want ARG poss m G1 anyone AR A ARG0 book 1 RG X ARG0 read i y rit la po A ARG1 0 RG believe 1 Figure 1: Left: an AMR for the sentence “I do not want anyone to read my book carelessly”; right: two example HRG rules. the AMR-Bank (Ba"
W15-0127,P14-1134,0,0.26562,"focus is thus on automatically induced grammars, if the grammar is not left implicit altogether. Training such data-driven models requires semantically annotated corpora. One recent development is the release of several corpora in which English sentences were annotated with graphs that represent the meanings of the sentences (Banarescu et al., 2013; Oepen et al., 2014). There has already been a body of research on semantic parsing for graphs based on these corpora, but so far, the ideas underlying these semantic parsers have not been connected to classical approaches to semantic construction. Flanigan et al. (2014) and Martins and Almeida (2014) show how to adapt data-driven dependency parsers from trees to graphs. These approaches do not use explicit grammars, in contrast to all work in the classical tradition. Chiang et al. (2013) do use explicit Hyperedge Replacement Grammars (HRGs, Drewes et al., 1997) for semantic parsing. However, HRGs capture semantic dependencies in a way that is not easily mapped to conventional intuitions about semantic construction, and indeed their use in linguistically motivated models of the syntax-semantics interface has not yet been demonstrated. But just because we use"
W15-0127,E03-1030,0,0.174336,"-order logic as a semantic representation formalism. Under the traditional view, the input representation of the semantic construction process is a syntactic parse tree; see e.g. Blackburn and Bos (2005) for a detailed presentation. One key challenge is to make sure that a semantic predicate combines with its semantic arguments correctly. In Montague Grammar, this is achieved by constructing through functional application of a suitably constructed lambda-term for the functor. Many unification-based approaches to semantic construction, e.g. in the context of HPSG (Copestake et al., 2001), TAG (Gardent and Kallmeyer, 2003), and CCG (Baldridge and Kruijff, 2002) use explicit argument slots that are identified either by a globally valid name (such as “subject”) or by a syntactic argument position. Copestake et al. (2001), in particular, conclude from their experiences from designing large-scale HPSG grammars that explicitly named arguments simplify the specification of the syntax-semantics interface and allow rules that generalize better over different syntactic structures. Their semantic algebra assigns to each syntactic constituent a semantic representation consisting of an MRS, together with hooks that make sp"
W15-0127,C12-1083,0,0.0882993,"stics, and do not use an explicit grammar; they are thus very different from traditional approaches to semantic construction. Chiang et al. (2013) present a statistical parser for synchronous string/graph grammars based on hyperedge replacement grammars (HRGs, Drewes et al., 1997). HRGs manipulate hypergraphs, which may contain hyperedges with an arbitrary number k of endpoints, labeled with nonterminal symbols. Each rule application replaces one such hyperedge with the graph on the right-hand side, identifying the endpoints of the nonterminal hyperedge with the “external nodes” of the graph. Jones et al. (2012) and Jones et al. (2013) describe a number of ways to infer HRGs from corpora. However, previous work has not demonstrated the suitability of HRG for linguistically motivated semantic construction. Typical published examples, such as the HRG rules from Chiang et al. (2013) shown in Fig. 1 on the right, are designed for succinctness of explanation, not for linguistic adequacy (in the figure, the external nodes are drawn shaded). Part of the problem is that HRG rules take a primarily top-down perspective on graph construction (in contrast to most work on compositional semantic construction) and"
W15-0127,W13-1810,0,0.0510693,"explicit grammar; they are thus very different from traditional approaches to semantic construction. Chiang et al. (2013) present a statistical parser for synchronous string/graph grammars based on hyperedge replacement grammars (HRGs, Drewes et al., 1997). HRGs manipulate hypergraphs, which may contain hyperedges with an arbitrary number k of endpoints, labeled with nonterminal symbols. Each rule application replaces one such hyperedge with the graph on the right-hand side, identifying the endpoints of the nonterminal hyperedge with the “external nodes” of the graph. Jones et al. (2012) and Jones et al. (2013) describe a number of ways to infer HRGs from corpora. However, previous work has not demonstrated the suitability of HRG for linguistically motivated semantic construction. Typical published examples, such as the HRG rules from Chiang et al. (2013) shown in Fig. 1 on the right, are designed for succinctness of explanation, not for linguistic adequacy (in the figure, the external nodes are drawn shaded). Part of the problem is that HRG rules take a primarily top-down perspective on graph construction (in contrast to most work on compositional semantic construction) and combine arbitrarily comp"
W15-0127,W11-2902,1,0.641952,"At the same time, s-graph grammars make use of an explicit inventory of semantic argument positions. Thus they also lend themselves to writing linguistically interpretable grammars by hand. 228 Proceedings of the 11th International Conference on Computational Semantics, pages 228–238, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics The paper is structured as follows. In Section 2, we will briefly review the state of the art in semantic parsing, especially for graph-based semantic representations. We will also introduce Interpreted Regular Tree Grammars (IRTGs) (Koller and Kuhlmann, 2011), which will serve as the formal foundation of sgraph grammars. In Section 3, we will then define s-graph grammars as IRTGs with an interpretation into the algebra of s-graphs (Courcelle and Engelfriet, 2012). In Section 4, we will illustrate the linguistic use of s-graph grammars by giving toy grammars for a number of semantic phenomena. We conclude by discussing a number of formal aspects, such as parsing complexity, training, and the equivalence to HRG in Section 5. 2 Previous Work We start by reviewing some related work. 2.1 Semantic construction and semantic parsing In this paper, we take"
W15-0127,W12-4616,1,0.843928,"re the V-string). It then evaluates hi (t) over the algebra Ai , obtaining an element of Ai . Altogether, the language of an IRTG is defined as L(G) = {hJh1 (t)KA1 , . . . , Jhk (t)KAk i |t ∈ L(G)}. In the example, the pair hJohn opens the box, Hans o¨ ffnet die Kistei is one element of L(G). Generally, IRTGs with two string-algebra interpretations are strongly equivalent to synchronous context-free grammars, just as IRTGs with a single string-algebra interpretation represent context-free grammars. By choosing different algebras, IRTGs can also represent (synchronous) tree-adjoining grammars (Koller and Kuhlmann, 2012), tree-to-string transducers (B¨uchse et al., 2013), etc. 3 An algebra of s-graphs We can use IRTGs to describe mappings between strings and graph-based semantic representations. Let an s-graph grammar be an IRTG with two interpretations: one interpretation (hs , T ∗ ) into a string algebra T ∗ as described above, and one interpretation (hg , Ag ) into an algebra Ag of graphs. In this section, we define a graph algebra for this purpose. The graph algebra we use here is the HR algebra of Courcelle (1993), see also Courcelle and Engelfriet (2012). The values of the HR algebra are s-graphs. An s-"
W15-0127,S14-2082,0,0.100208,"lly induced grammars, if the grammar is not left implicit altogether. Training such data-driven models requires semantically annotated corpora. One recent development is the release of several corpora in which English sentences were annotated with graphs that represent the meanings of the sentences (Banarescu et al., 2013; Oepen et al., 2014). There has already been a body of research on semantic parsing for graphs based on these corpora, but so far, the ideas underlying these semantic parsers have not been connected to classical approaches to semantic construction. Flanigan et al. (2014) and Martins and Almeida (2014) show how to adapt data-driven dependency parsers from trees to graphs. These approaches do not use explicit grammars, in contrast to all work in the classical tradition. Chiang et al. (2013) do use explicit Hyperedge Replacement Grammars (HRGs, Drewes et al., 1997) for semantic parsing. However, HRGs capture semantic dependencies in a way that is not easily mapped to conventional intuitions about semantic construction, and indeed their use in linguistically motivated models of the syntax-semantics interface has not yet been demonstrated. But just because we use graphs as semantic representati"
W15-0127,S14-2008,0,0.0570773,"automatically induced grammars, if the grammar is not left implicit altogether. Training such data-driven models requires semantically annotated corpora. One recent development is the release of several corpora in which English sentences were annotated with graphs that represent the meanings of the sentences (Banarescu et al., 2013; Oepen et al., 2014). There has already been a body of research on semantic parsing for graphs based on these corpora, but so far, the ideas underlying these semantic parsers have not been connected to classical approaches to semantic construction. Flanigan et al. (2014) and Martins and Almeida (2014) show how to adapt data-driven dependency parsers from trees to graphs. These approaches do not use explicit grammars, in contrast to all work in the classical tradition. Chiang et al. (2013) do use explicit Hyperedge Replacement Grammars (HRGs, Drewes et al., 1997) for semantic parsing. However, HRGs capture semantic dependencies in a way that is not easily mapped to conventional intuitions about semantic construction, and indeed their use in linguistically motivated models of the syntax-semantics interface has not yet been demonstrated. But just because we use"
W15-0127,P07-1121,0,0.291867,"construction. We illustrate this with a number of hand-written toy grammars, sketch the use of s-graph grammars for data-driven semantic parsing, and discuss formal aspects. 1 Introduction Semantic construction is the problem of deriving a formal semantic representation from a natural-language expression. The classical approach, starting with Montague (1974), is to derive semantic representations from syntactic analyses using hand-written rules. More recently, semantic construction has enjoyed renewed attention in mainstream computational linguistics in the form of semantic parsing (see e.g. Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Chiang et al., 2013). The idea in semantic parsing is to make a system learn the mapping of the string into the semantic representation automatically, with or without the use of an explicit syntactic representation as an intermediate step. The focus is thus on automatically induced grammars, if the grammar is not left implicit altogether. Training such data-driven models requires semantically annotated corpora. One recent development is the release of several corpora in which English sentences were annotated with graphs that represent the meanings of the senten"
W15-0127,E03-1000,0,\N,Missing
W16-2402,C12-1021,0,0.0612034,"Missing"
W16-2402,P15-1143,1,0.924955,"e definition of the probability G(T ) must decompose into smaller derivation steps along the rules of A. Second the number of rules of A cannot be too large, as they must all be visited. The first assumption is violated when e.g. non-local features (Huang, 2008) are used to define probabilities or when probabilities are defined by recurrent neural nets that use hidden states derived from whole subtrees (Socher et al., 2013; Dyer et al., 2016). The second assumption is violated when e.g. tree automata are used to represent parse charts for combinatorially complex objects like in graph parsing (Groschwitz et al., 2015). When semiring techniques are not applicable, it is necessary to use approximation techniques. One popular technique is the use of Monte Carlo methods, i.e. sampling. It is often based on Markov Chain Monte Carlo (Gamerman and Lopes, 2006) or Particle Monte Carlo (Capp´e et al., 2007) approaches and requires minimal knowledge about the expected value being approximated. In this work we develop an importance sampler based on pRTAs which can be used to approximate expected values in settings where exact solutions are infeasible. One can efficiently sample from a pRTA making it a suitable tool f"
W16-2402,P08-1067,0,0.0826899,"Teichmann University of Potsdam Kasimir Wansing Leipzig University Alexander Koller University of Potsdam {chriteich|akoller}@uni-potsdam.de kasimir.wansing@uni-leipzig.de Abstract semiring operations on the automaton defining LA (Li and Eisner, 2009). Li and Eisner (2009) give an exact semiring solution if two key assumptions can be made. First the definition of the probability G(T ) must decompose into smaller derivation steps along the rules of A. Second the number of rules of A cannot be too large, as they must all be visited. The first assumption is violated when e.g. non-local features (Huang, 2008) are used to define probabilities or when probabilities are defined by recurrent neural nets that use hidden states derived from whole subtrees (Socher et al., 2013; Dyer et al., 2016). The second assumption is violated when e.g. tree automata are used to represent parse charts for combinatorially complex objects like in graph parsing (Groschwitz et al., 2015). When semiring techniques are not applicable, it is necessary to use approximation techniques. One popular technique is the use of Monte Carlo methods, i.e. sampling. It is often based on Markov Chain Monte Carlo (Gamerman and Lopes, 200"
W16-2402,N07-1018,0,0.0175182,"011). The symmetric Dirichlet Distribution is parametrized by a concentration parameter γ. The rule weights become more likely to be concentrated on a few of the rules for each left hand side as γ goes toward 0. Therefore many trees will be improbable according to G. We obtain a complete evaluation problem by giving the structure of the automata used. As stated, we use the same automaton to specify G and Aθ save for θ which we initialize to be 0 17 have to move through the relevant domain with small steps (Chung et al., 2013) or use a good proposal distribution in order to generate new trees (Johnson et al., 2007). Because it is difficult to adapt Markov Chain Monte Carlo algorithms (Liang et al., 2010) the proposal distribution for generating new trees needs to be specified by the user in advance. Particle Monte Carlo Methods (Capp´e et al., 2007; B¨orschinger et al., 2012) are related to importance sampling and would allow for more adaptive proposals, but have not been used this way for the structured outputs used in natural language processing. The idea of using adaptive versions of importance sampling has become much more prevalent in the last years (Douc et al., 2007; Lian, 2011; Ryu and Boyd, 201"
W16-2402,W11-2902,1,0.561409,"s that are optimal as measured by the Kullback-Leibler Divergence. Theorem 1. In Algorithm 1 we almost surely  g(ti )f (ti ) have for each n: lim Snorm = PA s(n) ps→∞ EG(T ) [f (T )]. 5 θ (n) 1. For every dimension k and sample size ps: "" "" ## ohθ(n) ,ki (ti ) EP ps (S) Ss(n) (n) PA (n) (ti ) Aθ 5.1 θ equals ∇o Evaluation We created an implementation of our approach in order to investigate its behavior. It, along with all evaluation data and documentation, is available via https://bitbucket.org/tclup/alto/ wiki/AdaptiveImportanceSampling as part of Alto, an Interpreted Regular Tree Grammar (Koller and Kuhlmann, 2011) toolkit. We encourage readers to use the sampler in their own experiments. In this section we will evaluate the ability of our algorithm to learn good proposal weights in an artificial data experiment that specifically focuses on this aspect. This will show whether the theoretical guarantees correspond to practical benefits. In future work we will evaluate the algorithm in End-to-End NLP experiments to see how it interacts with a larger tool chain. Estimates will converge to EG(T ) [f (T )] as stated by Theorem 1 even if SGD did not improve the fit between PAθ and G. But it would be reassurin"
W16-2402,D09-1005,0,0.470576,"language processing (NLP) often requires the computation of expected values P G(T = t)f (t) where EG(T ) [f (T )] = t∈LA the random variable T takes values from the language LA of a probabilistic regular tree automaton (pRTA) A, f measures a quantity of interest and G is a probability distribution on LA . A tree automaton provides a natural generalization of acyclic hypergraphs and latent variable grammars that are often used in natural language processing to express e.g. a parse chart or all the ways a word could be decomposed into morphemes. For different choices of G and f we obtain e.g. (Li and Eisner, 2009): Feature Expectations for conditional random fields. Kullback-Leibler Divergence to compare the predictions of different probability models. Expected Loss for minimum risk decoding. Log-Likelihood for predicting the next word in language models. Gradients of those quantities for optimization. Exact computation of these values is feasible if LA is small or additional assumptions can be made, e.g. if the expected value is defined via 11 Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 11–20, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Li"
W16-2402,N16-1024,0,0.0565407,"ct semiring operations on the automaton defining LA (Li and Eisner, 2009). Li and Eisner (2009) give an exact semiring solution if two key assumptions can be made. First the definition of the probability G(T ) must decompose into smaller derivation steps along the rules of A. Second the number of rules of A cannot be too large, as they must all be visited. The first assumption is violated when e.g. non-local features (Huang, 2008) are used to define probabilities or when probabilities are defined by recurrent neural nets that use hidden states derived from whole subtrees (Socher et al., 2013; Dyer et al., 2016). The second assumption is violated when e.g. tree automata are used to represent parse charts for combinatorially complex objects like in graph parsing (Groschwitz et al., 2015). When semiring techniques are not applicable, it is necessary to use approximation techniques. One popular technique is the use of Monte Carlo methods, i.e. sampling. It is often based on Markov Chain Monte Carlo (Gamerman and Lopes, 2006) or Particle Monte Carlo (Capp´e et al., 2007) approaches and requires minimal knowledge about the expected value being approximated. In this work we develop an importance sampler ba"
W16-2402,N06-1044,0,0.0214839,"cording to PAql (R) and make q1 , . . . , qn its children. This produces a derivation d and tree t given by m(d). Note that the probability of generating any derivationtree pair ht, di this way is given by the multiplication of rule probabilities matching the definition of PA (T, D). For our case this means that we can also sample PA (t). The sampling of derivations can fail in three ways. First, there may be states for which there are no rules to expand them. The sampling procedure we have outlined is undefined when such a state is reached. Secondly, it is possible that a pRTA is deficient1 (Nederhof and Satta, 2006). This means that there is a non-zero probability that the sampling procedure we have outlined keeps expanding without ever stopping. Finally the number of rules for a single state might be so large that it is impossible to iterate over them efficiently to select one for a sample. The techniques for solving these problems are outside the scope of this paper and we therefore make three additional assumptions in addition to our condition that a pRTA A is unambiguous: charts that occur e.g. in context free parsing and also graph parsing (Groschwitz et al., 2015). Li and Eisner (2009) give an algo"
W16-2402,P13-1045,0,0.0100163,"uni-leipzig.de Abstract semiring operations on the automaton defining LA (Li and Eisner, 2009). Li and Eisner (2009) give an exact semiring solution if two key assumptions can be made. First the definition of the probability G(T ) must decompose into smaller derivation steps along the rules of A. Second the number of rules of A cannot be too large, as they must all be visited. The first assumption is violated when e.g. non-local features (Huang, 2008) are used to define probabilities or when probabilities are defined by recurrent neural nets that use hidden states derived from whole subtrees (Socher et al., 2013; Dyer et al., 2016). The second assumption is violated when e.g. tree automata are used to represent parse charts for combinatorially complex objects like in graph parsing (Groschwitz et al., 2015). When semiring techniques are not applicable, it is necessary to use approximation techniques. One popular technique is the use of Monte Carlo methods, i.e. sampling. It is often based on Markov Chain Monte Carlo (Gamerman and Lopes, 2006) or Particle Monte Carlo (Capp´e et al., 2007) approaches and requires minimal knowledge about the expected value being approximated. In this work we develop an i"
W17-3520,I05-1015,0,0.0585862,"of the realizer and the sentence planner and take advantage of opportunities for succinct realizations. However, integrated sentence planning multiplies the complexities of two hard combinatorial problems, and thus existing, search-based algorithms can be inefficient or fail to find a good soluBy contrast, chart-based algorithms have been shown in parsing to remain efficient and accurate even for large inputs because they support structure sharing and very effective pruning techniques. Chart algorithms have been successfully applied to surface realization (White, 2004; Gardent and Kow, 2005; Carroll and Oepen, 2005; Schwenger et al., 2016), but in RE generation, most algorithms are not chartbased, see e.g. (Dale and Reiter, 1995; Krahmer et al., 2003). One exception is the chart-based RE generation of Engonopoulos and Koller (2014). In this paper, we present a chart-based algorithm for integrated surface realization and RE generation. This makes it possible – to our knowledge, for the first time – to apply chart-based pruning techniques to integrated sentence generation. Our algorithm extends the chart-based RE generation algorithm of Engonopoulos and Koller (2014) by keeping track of the semantic conte"
W17-3520,W14-5002,1,0.836361,"s existing, search-based algorithms can be inefficient or fail to find a good soluBy contrast, chart-based algorithms have been shown in parsing to remain efficient and accurate even for large inputs because they support structure sharing and very effective pruning techniques. Chart algorithms have been successfully applied to surface realization (White, 2004; Gardent and Kow, 2005; Carroll and Oepen, 2005; Schwenger et al., 2016), but in RE generation, most algorithms are not chartbased, see e.g. (Dale and Reiter, 1995; Krahmer et al., 2003). One exception is the chart-based RE generation of Engonopoulos and Koller (2014). In this paper, we present a chart-based algorithm for integrated surface realization and RE generation. This makes it possible – to our knowledge, for the first time – to apply chart-based pruning techniques to integrated sentence generation. Our algorithm extends the chart-based RE generation algorithm of Engonopoulos and Koller (2014) by keeping track of the semantic content that has been expressed by each chart item. Because it is modular on the string side, the same algorithm can be used to generate with context-free grammars or TAGs, with or without feature structures, at no expense in"
W17-3520,W05-1605,0,0.0257585,"n can balance the needs of the realizer and the sentence planner and take advantage of opportunities for succinct realizations. However, integrated sentence planning multiplies the complexities of two hard combinatorial problems, and thus existing, search-based algorithms can be inefficient or fail to find a good soluBy contrast, chart-based algorithms have been shown in parsing to remain efficient and accurate even for large inputs because they support structure sharing and very effective pruning techniques. Chart algorithms have been successfully applied to surface realization (White, 2004; Gardent and Kow, 2005; Carroll and Oepen, 2005; Schwenger et al., 2016), but in RE generation, most algorithms are not chartbased, see e.g. (Dale and Reiter, 1995; Krahmer et al., 2003). One exception is the chart-based RE generation of Engonopoulos and Koller (2014). In this paper, we present a chart-based algorithm for integrated surface realization and RE generation. This makes it possible – to our knowledge, for the first time – to apply chart-based pruning techniques to integrated sentence generation. Our algorithm extends the chart-based RE generation algorithm of Engonopoulos and Koller (2014) by keeping tr"
W17-3520,E17-3008,1,0.821257,"generation. This makes it possible – to our knowledge, for the first time – to apply chart-based pruning techniques to integrated sentence generation. Our algorithm extends the chart-based RE generation algorithm of Engonopoulos and Koller (2014) by keeping track of the semantic content that has been expressed by each chart item. Because it is modular on the string side, the same algorithm can be used to generate with context-free grammars or TAGs, with or without feature structures, at no expense in runtime efficiency. An open-source implementation of our algorithm, based on the Alto system (Gontrum et al., 2017), can be found at bitbucket.org/tclup/alto. 2 Chart-based integrated generation We first describe the grammar formalism we use. Then we explain the sentence generation algorithm and discuss its runtime performance. 139 Proceedings of The 10th International Natural Language Generation conference, pages 139–143, c Santiago de Compostela, Spain, September 4-7 2017. 2017 Association for Computational Linguistics (a) IR (b) (c) IN {e} ←−−−−−− sleepe,r−−−→ {sleep(e, r2 ), rabbit(r2 ), white(r2 )} 2 [∩2 ]1 sleep uniqr2 ∩1 rabbit white def r2 rabbitr2 ] sleep(e, r2 ) ] for all e, a ∈ sleep: Se → sleep"
W17-3520,W11-2902,1,0.694994,"(R1 )]1 IN (sleepe,a )(N1 ) = {sleep(e, a)} ] N1 for all a ∈ rabbit: Na → rabbita (Adja ) IS (rabbita )(w1 ) = w1 • rabbit IR (rabbita )(R1 ) = rabbit ∩1 R1 IN (rabbita )(N1 ) = {rabbit(a)} ] N1 rabbit(r2 ) white(r2 ) whiter2 (d) IS : •(•(the, •(white, rabbit)), sleeps) = ”the white rabbit sleeps” Figure 1: A derivation tree (b) with its interpretations (a, c, d). 2.1 Semantically interpreted grammars We describe the integrated sentence generation problem in terms of semantically interpreted grammars (SIGs) (Engonopoulos and Koller, 2014), a special case of Interpreted Regular Tree Grammars (Koller and Kuhlmann, 2011). We introduce SIGs by example, and refer to Engonopoulos and Koller (2014) for detailed definitions. An example SIG grammar is shown in Fig. 2. At the core of each grammar rule is a rule of the form Aa → f (Bb , . . . , Zz ). The symbols A, . . . , Z are nonterminals such as S, NP, VP, and a, . . . , z are semantic indices, i.e. constants for individuals indicating to which object in the model a natural-language expression is intended to refer. These core rules allow us to recursively derive a derivation tree, such as the one shown in Fig. 1b, representing the abstract syntactic structure of"
W17-3520,W12-4616,1,0.790507,"Missing"
W17-3520,P07-1043,1,0.743043,"rgued (Stone et al., 2003) that the strict distinction between surface realization and sentence planning in the classical NLG pipeline (Reiter and Dale, 2000) can cause difficulties for an NLG system. Generation decisions that look good to the sentence planner may be hard or impossible for the realizer to express in natural language. Furthermore, a standalone sentence planner must compute each RE separately, thus missing out on opportunities for succinct REs that are ambiguous in isolation but correct in context (Stone and Webber, 1998). Algorithms such as SPUD (Stone et al., 2003) and CRISP (Koller and Stone, 2007) perform surface realization and parts of sentence planning, including RE generation, in an integrated fashion. Such integrated algorithms for sentence generation can balance the needs of the realizer and the sentence planner and take advantage of opportunities for succinct realizations. However, integrated sentence planning multiplies the complexities of two hard combinatorial problems, and thus existing, search-based algorithms can be inefficient or fail to find a good soluBy contrast, chart-based algorithms have been shown in parsing to remain efficient and accurate even for large inputs be"
W17-3520,P02-1003,1,0.46025,"the requirement for semantic uniqueness to the verb that subcategorizes for the RE. This is in contrast to the standard assumption that it is the definite article that requires uniqueness, but permits us a purely grammar-based treatment of mutually constraining REs which requires no further reasoning capabilities. 2.4 Chart generation with heuristics Our algorithm can enumerate all subsets N of the true semantic atoms in the model, and thus has worst-case exponential runtime. This is probably unavoidable, given that surface realization and the generation of shortest REs are both NP-complete (Koller and Striegnitz, 2002; Dale and Reiter, 1995). However, because it is a chart-based algorithm, we can use heuristics to avoid computing the whole chart, and thus speed up the search for the best solution. To get an impression of this, assume that we are looking for a short sentence; other optimization criteria are also possible. We first compute the full chart CR for the IR part of the input alone, using essentially the same algorithm as Engonopoulos and Koller (2014). From CR we compute the distance of each chart item to a goal item, i.e. the minimal number of rules that must be applied to the item to produce a g"
W17-3520,J03-1003,0,0.197727,"Missing"
W17-3520,C16-1144,0,0.0822323,"sentence planner and take advantage of opportunities for succinct realizations. However, integrated sentence planning multiplies the complexities of two hard combinatorial problems, and thus existing, search-based algorithms can be inefficient or fail to find a good soluBy contrast, chart-based algorithms have been shown in parsing to remain efficient and accurate even for large inputs because they support structure sharing and very effective pruning techniques. Chart algorithms have been successfully applied to surface realization (White, 2004; Gardent and Kow, 2005; Carroll and Oepen, 2005; Schwenger et al., 2016), but in RE generation, most algorithms are not chartbased, see e.g. (Dale and Reiter, 1995; Krahmer et al., 2003). One exception is the chart-based RE generation of Engonopoulos and Koller (2014). In this paper, we present a chart-based algorithm for integrated surface realization and RE generation. This makes it possible – to our knowledge, for the first time – to apply chart-based pruning techniques to integrated sentence generation. Our algorithm extends the chart-based RE generation algorithm of Engonopoulos and Koller (2014) by keeping track of the semantic content that has been expresse"
W17-3520,W98-1419,0,0.481885,"upports efficient search through chart pruning. 1 Introduction It has long been argued (Stone et al., 2003) that the strict distinction between surface realization and sentence planning in the classical NLG pipeline (Reiter and Dale, 2000) can cause difficulties for an NLG system. Generation decisions that look good to the sentence planner may be hard or impossible for the realizer to express in natural language. Furthermore, a standalone sentence planner must compute each RE separately, thus missing out on opportunities for succinct REs that are ambiguous in isolation but correct in context (Stone and Webber, 1998). Algorithms such as SPUD (Stone et al., 2003) and CRISP (Koller and Stone, 2007) perform surface realization and parts of sentence planning, including RE generation, in an integrated fashion. Such integrated algorithms for sentence generation can balance the needs of the realizer and the sentence planner and take advantage of opportunities for succinct realizations. However, integrated sentence planning multiplies the complexities of two hard combinatorial problems, and thus existing, search-based algorithms can be inefficient or fail to find a good soluBy contrast, chart-based algorithms hav"
W17-6201,N09-2047,0,0.0149921,"oshi (1988) equips each node in each elementary tree with a “top” and “bottom” feature structure. These are unified with each other at the end of the derivation if no auxiliary tree is adjoined at this node; otherwise they are unified with feature structures from the root and foot node of such an auxiliary tree. FTAG has been used successfully in largescale grammar engineering, such as in the XTAG grammar (XTAG Research Group, 2001). One aspect in which FTAG has lagged behind other feature grammar formalisms, such as HPSG and LFG, is in parsing. Recent efficient parsers for TAG, such as MICA (Bangalore et al., 2009) and Alto (Koller and Kuhlmann, 2012; Groschwitz et al., 2016), do not support feature structures. The recent TuLiPA parser (Kallmeyer et al., 2010) does support feature structures in FTAG parsing, but can be inefficient in practice because it enu1 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 1–10, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017.    WORD AGR boy &quot; NUM GEN (a) sg masc  #      SUBJ AGR h AGR 1 h 1 NUM i pl (b)  α1 i  β1 the Figure 1: Two example feature structures."
W17-6201,E17-3008,1,0.811818,". In our example, the intersection Cw of Cw0 with 8 5.4 Acknowledgments. I am grateful to the reviewers for their insightful and constructive comments, and to Jessica Grasso, Jonas Groschwitz, Christoph Teichmann, and Stefan Thater for discussions. I am also indebted to the students in my Grammar Formalisms classes at the University of Potsdam for demanding a faster FTAG parser, and those at Saarland University for being the first users of the system described here. Implementation We have implemented the FS algebra and the filter grammar described above in the context of the Alto IRTG parser (Gontrum et al., 2017). Alto is available open-source from https:// bitbucket.org/tclup/alto. Our implementation uses well-known efficient algorithms for unification (Tomabechi, 1991) and subsumption checking (Malouf et al., 2000). Subsumption checking is needed because as the intersection algorithm discovers new candidate rules for Cw , it has to check whether another rule with an equal feature structure already exists; this equality checking is performed by testing whether each FS subsumes the other one. Parsing efficiency could be improved further by replacing this with asymmetric subsumption checks and retainin"
W17-6201,P16-1192,1,0.931453,"terpreted Regular Tree Grammar (IRTG, Koller and Kuhlmann (2011)) by extending the TAG-to-IRTG encoding of Koller and Kuhlmann (2012) with an additional interpretation into feature structures. This interpretation maps each derivation tree into a term over a novel algebra of feature structures, in a similar way as c-structures are mapped into fstructures in LFG (Kaplan and Bresnan, 1982). This term can be evaluated to a value in the algebra if and only if all unifications required by the grammar succeed. We then show how known algorithms for IRTG chart parsing – which can be efficient for TAG (Groschwitz et al., 2016) – extend naturally to FTAG parsing. We offer a view on FTAG which brings it more in line with other feature-based grammar formalisms, in that the distinction between top and bottom feature structures is represented correctly, but requires no special treatment by the parser. This simplifies, for instance, the use of existing unification algorithms. At the same time, we offer a very general and modular approach to checking feature unification on a parse chart; no unpacking of the individual derivation trees is required in our algorithm. This approach generalizes straightforFTAG, the extension o"
W17-6201,P15-1143,1,0.907732,"Missing"
W17-6201,P86-1038,0,0.762585,"e subsumed by both. Plan of the paper. We will review FTAG in Section 2 and IRTGs and the TAG-to-IRTG encoding in Section 3. We will then define the feature structure algebra and show how it can be used to encode FTAG in Section 4. We show how to do efficient and modular chart parsing for FTAG in Section 5. Section 6 concludes. 2.1 top: [num: X, det: +] α2 wardly to other mechanisms for filtering derivation trees, as long as they can be expressed in terms of finite-state constraints on trees. 2 NP S top: [num: X] TAG and feature structures Feature structures Feature structures (Shieber, 1986; Kasper and Rounds, 1986; Carpenter, 1992) are used in many grammar formalisms to represent grammatical information. Intuitively, a feature structure (FS) assigns values to a finite set of features; these values may either be atomic, or they may be feature structures themselves. For instance, the FS in Fig. 1a specifies that the WORD feature has the atomic value ‘boy’, and the value of the AGR feature is a feature structure with the features NUM and GEN. Technically, feature structures represent directed, acyclic graphs in which both the top-level FS and all of the FSs it contains are nodes. If F has a feature FT, an"
W17-6201,W11-2902,1,0.912322,"of checking features on the parse chart directly. On the theoretical side, Schmitz and Le Roux (2008) explain FTAG through a feature-based formalism for describing languages of derivation trees, with unclear implications for parsing efficiency. Overall, the sense is that because of the complex interaction of unification and adjunction in FTAG, implementing efficient FTAG parsers is uncomfortable and something that tends to get avoided. In this paper, we offer a simple and modular approach to efficient parsing with FTAG. We encode an FTAG grammar into an Interpreted Regular Tree Grammar (IRTG, Koller and Kuhlmann (2011)) by extending the TAG-to-IRTG encoding of Koller and Kuhlmann (2012) with an additional interpretation into feature structures. This interpretation maps each derivation tree into a term over a novel algebra of feature structures, in a similar way as c-structures are mapped into fstructures in LFG (Kaplan and Bresnan, 1982). This term can be evaluated to a value in the algebra if and only if all unifications required by the grammar succeed. We then show how known algorithms for IRTG chart parsing – which can be efficient for TAG (Groschwitz et al., 2016) – extend naturally to FTAG parsing. We"
W17-6201,W12-4616,1,0.542237,"side, Schmitz and Le Roux (2008) explain FTAG through a feature-based formalism for describing languages of derivation trees, with unclear implications for parsing efficiency. Overall, the sense is that because of the complex interaction of unification and adjunction in FTAG, implementing efficient FTAG parsers is uncomfortable and something that tends to get avoided. In this paper, we offer a simple and modular approach to efficient parsing with FTAG. We encode an FTAG grammar into an Interpreted Regular Tree Grammar (IRTG, Koller and Kuhlmann (2011)) by extending the TAG-to-IRTG encoding of Koller and Kuhlmann (2012) with an additional interpretation into feature structures. This interpretation maps each derivation tree into a term over a novel algebra of feature structures, in a similar way as c-structures are mapped into fstructures in LFG (Kaplan and Bresnan, 1982). This term can be evaluated to a value in the algebra if and only if all unifications required by the grammar succeed. We then show how known algorithms for IRTG chart parsing – which can be efficient for TAG (Groschwitz et al., 2016) – extend naturally to FTAG parsing. We offer a view on FTAG which brings it more in line with other feature-"
W17-6201,K15-1004,0,0.0720124,"Missing"
W17-6201,W08-2319,0,0.0269189,"Missing"
W17-6201,P91-1041,0,0.70125,"Missing"
W17-6201,C88-2147,0,0.729079,"ility of efficient chart parsers. This is in part because of the complex interaction of adjunction and unification, which makes such parsers inconvenient to implement. We present a novel, simple algebra for feature structures and show how FTAG can be encoded as an Interpreted Regular Tree Grammar using this algebra. This yields a straightforward, efficient chart parsing algorithm for FTAG. 1 Introduction Like many other grammar formalisms, treeadjoining grammars (TAG) have been extended with feature structures to model linguistic phenomena such as agreement conveniently. The FTAG formalism of Vijay-Shanker and Joshi (1988) equips each node in each elementary tree with a “top” and “bottom” feature structure. These are unified with each other at the end of the derivation if no auxiliary tree is adjoined at this node; otherwise they are unified with feature structures from the root and foot node of such an auxiliary tree. FTAG has been used successfully in largescale grammar engineering, such as in the XTAG grammar (XTAG Research Group, 2001). One aspect in which FTAG has lagged behind other feature grammar formalisms, such as HPSG and LFG, is in parsing. Recent efficient parsers for TAG, such as MICA (Bangalore e"
W17-6201,W07-2207,0,0.0891645,"Missing"
W17-6202,W11-2902,1,0.879289,"of a partial function from licensing features to strings. These functions are just the string parts of the MG operations, separated out from the feature calculus. The values of the algebra are strings paired with a partial function from lic to strings, i.e. Σ∗ × [lic Σ∗ ], which we call minimalist string tuples. We define |lic |+ 1 Merge operations and |lic|2 + |lic |+ 1 Move operations as follows, ∀f,g ∈ lic. merge 1 and move 1 are for final 3.1 IRTGs merge/move, so the string of the merging or movAn interpreted regular tree grammar ing element concatenates (·) with the main string, (IRTG) (Koller and Kuhlmann, 2011) on the right for Merge and on the left for Move. G = hG, (h1 , A1 ), . . . , (hn , An )i derives nmerge 2 f is for f-storing Merge, and move 2 g-f is tuples of objects, such as strings or trees from for f-storing Move triggered by g. derivation trees in G. A given t ∈ G is interpreted merge 1 (hs, Si, ht, Ti) = hs · t, S ⊕ Ti merge 2f (hs, Si, ht, Ti) = hs, S ⊕ T ⊕ {f 7→ t}iinto the n algebras A1 , . . . An by means of the n tree homomorphisms h1 , . . . , hn . For a move 1f (hs, Si) = hS(f) · s, S − fi given i ≤ n, hi (t) is a term of the signature of move 2f-g (hs, Si) = hs, (S − f) algebra"
W17-6202,W00-2023,0,0.186115,"Missing"
W17-6202,N10-1035,0,0.0489802,"Missing"
W17-6202,W04-3312,0,0.141022,"Missing"
W17-6202,E06-1048,0,0.0726203,"Missing"
W17-6202,P87-1015,0,0.805002,"ifference between an MCFG and an MG is that an MCFG rule is unrestricted in how it concatenates strings; in an MG, only the workspace can be made by concatenation; the movers are simply pooled into one function, never concatenated with one another. In this sense, MG equivalents of MCFGs are a subclass of general MCFGs of the same fanout, one Mildly context sensitive grammars (Joshi, 1985) frequently come with constants that limit the number of pieces being manipulated by the grammar. In Multiple Context-Free Grammars (MCFGs) (Seki et al., 1991) and Linear ContextFree Rewrite Systems (LCFRSs) (Vijay-Shanker et al., 1987) these are the rank – the maximum number of daughters/arguments a rule can have, and the fanout – the maximum number of elements in a tuple. In Minimalist Grammars it is the number of licensing features k, which limits the number of movers via the SMC. The maximum number of elements in a minimalist item is therefore k + 1 – all possible movers plus the workspace. Transforming an MG into an MCFG yields a grammar with rank 2 and fanout k + 1 (Michaelis, 1998). Our O(n2k+3 · 2k ) expressed in terms of 17 and Merge is restricted to prevent movement from within a specifier. The result is grammar th"
W17-6317,N03-1016,0,0.0944627,"on. This means that the number of rules in the inverse automaton differs depending on the grammar with which we are parsing. The evaluation grammars and fine-to-coarse mappings are available as supplementary material for this paper, and our coarseto-fine parser is implemented as part of the Alto Toolkit.1 PCFG evaluation. First, we reproduce the known result that CTF parsing speeds up PCFG parsing. We read off a PCFG from the parse trees of the WSJ portion of the Penn Treebank (Sections 02–21), using the gold POS tags as terminal symbols; binarize it with the “inside” binarization strategy of Klein and Manning (2003); and convert it to an IRTG. This yields an IRTG grammar with 23817 rules and 8202 nonterminals, of which 8131 were created during binarization. We then parsed the sentences in Section 23 of up to 40 words, both without pruning and with the CTF parser (longer sentences were infeasible with the unpruned parser). For CTF we used the fourlevel fine-to-coarse mapping from Charniak et al. (2006) and a threshold of θ = 10−5 . We also apply the fine-to-coarse mapping to the nonterminals introduced during binarization. If the nonterminal ‘NP’ is mapped to ‘HP’ for the level k, then a nonterminal ‘NPii"
W17-6317,W13-2322,0,0.0248521,"preserves the distinction between nonterminals at the root of initial trees and those at the root of auxiliary trees.2 We tried the threshold values θ = 10−5 and θ = 10−9 . The results are shown in Table 1 (bottom). Unexpectedly, while CTF pruning with these thresholds already reduces the f-score, the parsing time barely improves. HRG evaluation. We also evaluated CTF on parsing Abstract Meaning Representation (AMR) graphs with hyperedge replacement grammars (HRGs, Chiang et al. (2013)). We use the HRG grammar of Groschwitz et al. (2015), which was induced from the “Little Prince” AMR corpus (Banarescu et al., 2013) and converted to an IRTG. This grammar describes how a graph can be constructed from atomic parts. It uses complex nonterminal symbols such as N0 {0, 1, 2}, indicating that the nonterminal should derive a subgraph with three sources 0, 1, and 2 (these describe nodes at which further edges can be added during the derivation), and the 1-source should be the AMR’s “root” node. The symbol before the curly brack– – – – – – – – – 629 7 739 9 3245 31 3887 36 1 invhom 9042 – 8963 12 – 9930 13 – 35476 13 – 34353 15 9 sat 0.04 – 0.04 0.03 – 0.05 0.04 – 0.12 0.03 – 0.13 0.05 0.04 Table 2: Results for HR"
W17-6317,W15-0127,1,0.843225,"string concatenation, we obtain “John loves Mary” ∈ L(G). For simplicity we will identify rules of G with their labels, i.e. we simply write r1 for the first rule in Fig. 1. IRTGs can capture different grammar formalisms by varying the algebra into which derivation trees are interpreted. For instance, TAG requires a string algebra with a string wrapping operation (Koller and Kuhlmann, 2012). It is also possible to extend the IRTG formalism in order to allow for multiple homomorphisms and algebras, which is useful for mapping between inputs and outputs and can be used e.g. in semantic parsing (Koller, 2015). Grammars from different formalisms also tend to vary in the complexity of the homomorphism h. For instance, all binary rules of a PCFG in CNF map to simple concatenation (cf. r1 , r2 in Fig. 1). By contrast, IRTG encodings of TAG grammars can use h to associate entire elementary trees with a single rule. Parsing for IRTGs proceeds in three steps. First, given an input object w ∈ A, we compute a decomposition grammar Dw which generates all terms over A that evaluate to w. Then we compute the inverse homomorphism (invhom) of Dw , i.e. an RTG Iw with L(Iw ) = {t ∈ TΣ |h(t) ∈ L(Dw )}. This RTG t"
W17-6317,N06-1022,0,0.4611,"treebanks. Because these properties do not depend on the specifics of our formalism, they would generalize to formalism specific implementations of CTF for TAG or HRG. We generalize coarse-to-fine parsing to grammar formalisms that are more expressive than PCFGs and/or describe languages of trees or graphs. We evaluate our algorithm on PCFG, PTAG, and graph parsing. While we achieve the expected performance gains on PCFGs, coarse-tofine does not help for PTAG and can even slow down parsing for graphs. We discuss the implications of this finding. 1 Introduction 2 Coarse-to-fine (CTF) parsing (Charniak et al., 2006) is one of the most effective pruning techniques for PCFG parsing. Its basic idea is to simplify a grammar by systematically merging nonterminals together. One then parses the input with the simple grammar, and uses statistics calculated from the resulting parse chart to constrain parsing with the original grammar. This can speed up parsing by an order of magnitude at no loss in accuracy (Charniak et al., 2006; Petrov and Klein, 2007). We present an algorithm for CTF parsing for grammar formalisms that are more expressive than PCFGs – to our knowledge, for the first time. More precisely, we ex"
W17-6317,W11-2902,1,0.81033,"ing. Its basic idea is to simplify a grammar by systematically merging nonterminals together. One then parses the input with the simple grammar, and uses statistics calculated from the resulting parse chart to constrain parsing with the original grammar. This can speed up parsing by an order of magnitude at no loss in accuracy (Charniak et al., 2006; Petrov and Klein, 2007). We present an algorithm for CTF parsing for grammar formalisms that are more expressive than PCFGs – to our knowledge, for the first time. More precisely, we extend CTF parsing to Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)), a very general grammar formalism which captures PCFGs, tree-adjoining grammars (TAGs, Joshi and Schabes (1997)), hyperedge replacement graph grammars (HRGs, Chiang et al. (2013)), and many others. Our direct application of CTF to expressive grammar formalism contrasts with related work (van Cranenburgh, 2012; Zhang and Krieger, 2011) which limits entries for the parse chart of the expressive formalism using the parse chart of a PCFG approximation. Interpreted Regular Tree Grammars Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)) generalize a wide range of grammar formal"
W17-6317,W12-4616,1,0.92978,")), hyperedge replacement graph grammars (HRGs, Chiang et al. (2013)), and many others. Our direct application of CTF to expressive grammar formalism contrasts with related work (van Cranenburgh, 2012; Zhang and Krieger, 2011) which limits entries for the parse chart of the expressive formalism using the parse chart of a PCFG approximation. Interpreted Regular Tree Grammars Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)) generalize a wide range of grammar formalisms, including (probabilistic) context-free grammars (PCFGs), treeadjoining grammars (Joshi and Schabes, 1997; Koller and Kuhlmann, 2012), hyperedge replacement grammars (Chiang et al., 2013; Groschwitz et al., 2015), as well as synchronous and transducer versions of these formalisms. They achieve this by distinguishing carefully between the generation of grammatical derivation trees and the way in which these derivation trees are interpreted as values of some algebra. Formally, a (monolingual) IRTG G = (G, h, A) consists of a weighted regular tree grammar (RTG, (Comon et al., 2007)) G over some signature Σ of node labels, an algebra A over some signature ∆ into which the derivation trees are interpreted, and a tree homomorphis"
W17-6317,P13-1091,0,0.313247,"esulting parse chart to constrain parsing with the original grammar. This can speed up parsing by an order of magnitude at no loss in accuracy (Charniak et al., 2006; Petrov and Klein, 2007). We present an algorithm for CTF parsing for grammar formalisms that are more expressive than PCFGs – to our knowledge, for the first time. More precisely, we extend CTF parsing to Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)), a very general grammar formalism which captures PCFGs, tree-adjoining grammars (TAGs, Joshi and Schabes (1997)), hyperedge replacement graph grammars (HRGs, Chiang et al. (2013)), and many others. Our direct application of CTF to expressive grammar formalism contrasts with related work (van Cranenburgh, 2012; Zhang and Krieger, 2011) which limits entries for the parse chart of the expressive formalism using the parse chart of a PCFG approximation. Interpreted Regular Tree Grammars Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)) generalize a wide range of grammar formalisms, including (probabilistic) context-free grammars (PCFGs), treeadjoining grammars (Joshi and Schabes, 1997; Koller and Kuhlmann, 2012), hyperedge replacement grammars (Chiang e"
W17-6317,E12-1047,0,0.0605201,"Missing"
W17-6317,P16-1192,1,0.852796,"e of an item in a coarse chart to approximately the sum of the weights of the finer items it represents; if suitable data is available, these weights could also be re-estimated from a treebank (Charniak et al., 2006; Petrov and Klein, 2007). In the example, we obtain HP → R1 (HP, HP) and HP → R2 (HP, HP) with R1 = {r1 , r2 } and R2 = {r3 }. This construction generalizes easily to multiple CTF levels. 4 Evaluation Using this algorithm, we can do CTF parsing for all grammar formalisms that can be encoded as IRTGs. We evaluate it on PCFG, TAG, and graph parsing, using the efficient algorithms of Groschwitz et al. (2016) to compute the coarsest charts. These algorithms are lazy and try to avoid computing rules of the inverse homomorphism grammar which cannot participate in a derivation. This means that the number of rules in the inverse automaton differs depending on the grammar with which we are parsing. The evaluation grammars and fine-to-coarse mappings are available as supplementary material for this paper, and our coarseto-fine parser is implemented as part of the Alto Toolkit.1 PCFG evaluation. First, we reproduce the known result that CTF parsing speeds up PCFG parsing. We read off a PCFG from the pars"
W17-6317,W11-2923,0,0.0282407,"t al., 2006; Petrov and Klein, 2007). We present an algorithm for CTF parsing for grammar formalisms that are more expressive than PCFGs – to our knowledge, for the first time. More precisely, we extend CTF parsing to Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)), a very general grammar formalism which captures PCFGs, tree-adjoining grammars (TAGs, Joshi and Schabes (1997)), hyperedge replacement graph grammars (HRGs, Chiang et al. (2013)), and many others. Our direct application of CTF to expressive grammar formalism contrasts with related work (van Cranenburgh, 2012; Zhang and Krieger, 2011) which limits entries for the parse chart of the expressive formalism using the parse chart of a PCFG approximation. Interpreted Regular Tree Grammars Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)) generalize a wide range of grammar formalisms, including (probabilistic) context-free grammars (PCFGs), treeadjoining grammars (Joshi and Schabes, 1997; Koller and Kuhlmann, 2012), hyperedge replacement grammars (Chiang et al., 2013; Groschwitz et al., 2015), as well as synchronous and transducer versions of these formalisms. They achieve this by distinguishing carefully betwe"
W17-6317,P15-1143,1,0.947218,"thers. Our direct application of CTF to expressive grammar formalism contrasts with related work (van Cranenburgh, 2012; Zhang and Krieger, 2011) which limits entries for the parse chart of the expressive formalism using the parse chart of a PCFG approximation. Interpreted Regular Tree Grammars Interpreted Regular Tree Grammars (IRTGs, Koller and Kuhlmann (2011)) generalize a wide range of grammar formalisms, including (probabilistic) context-free grammars (PCFGs), treeadjoining grammars (Joshi and Schabes, 1997; Koller and Kuhlmann, 2012), hyperedge replacement grammars (Chiang et al., 2013; Groschwitz et al., 2015), as well as synchronous and transducer versions of these formalisms. They achieve this by distinguishing carefully between the generation of grammatical derivation trees and the way in which these derivation trees are interpreted as values of some algebra. Formally, a (monolingual) IRTG G = (G, h, A) consists of a weighted regular tree grammar (RTG, (Comon et al., 2007)) G over some signature Σ of node labels, an algebra A over some signature ∆ into which the derivation trees are interpreted, and a tree homomorphism h : TΣ → T∆ that maps derivation trees into terms over the algebra. The RTG G"
W17-6317,P01-1044,0,0.130571,"do obtain the expected performance gains on PCFGs, and the Self mapping yields comparable HRG performance to the unpruned parser. IRTGs allow us to use the same infrastructure for CTF parsing with TAGs and HRGs which we used for CTF parsing with PCFGs. There are systematic structural differences between the PCFG, PTAG, and HRG grammars which explain the differences in the usefulness of CTF. One difference is the number of nonterminals from which a substructure can be derived. In treebank-induced PCFGs, most substrings of sufficient length can be derived from almost every phrasal nonterminal (Klein and Manning, 2001). This is reflected in a measure called saturation, which we formalize as the number of chart nonterminals (A0 , J0 ) that occur in the edges of the chart, divided for comparability by the total number of nonterminals A in GC and the number of invhom nonterminals J used in the chart. We only compute this measure for nonterminals A0 that were present in the grammar before binarization. For the unpruned PCFG parser, we obtain a mean saturation of 0.22, confirming Klein and Manning’s findings. By contrast, mean saturation is 0.04 for the unpruned HRG parser and 0.03 for the unpruned TAG parser. T"
W17-6810,D15-1198,0,0.216455,"ning Representations (AMRs), graphs which represent the predicate-argument structure of the sentences. Such work builds upon the AMRBank (Banarescu et al., 2013), a corpus in which each sentence has been manually annotated with an AMR. The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR. This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (Jones et al., 2012, 2013; Artzi et al., 2015; Peng et al., 2015). In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra (Courcelle, 1993), is huge (Groschwitz et al., 2015). This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances. Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics. We propose a novel method to gene"
W17-6810,P13-1091,0,0.0319973,"3a. This set of terms is riddled with spurious ambiguity and linguistically bizarre analyses, such as the term shown in Fig. 3b. Two strange aspects of this example are: one, prince becomes an X-source by first switching X and rt and then switching them back; this step is unnecessary and inconsistent with the corresponding process for rose here. Two, prince and rose are combined with empty argument connectors before love finally is inserted as the predicate, despite these roles being originally defined in love’s semantic frame. Not only does this make graph parsing computationally expensive (Chiang et al., 2013; Groschwitz et al., 2015), it also makes grammar induction difficult. For example, Bayesian algorithms sample random terms from the AMRBank and attempt to discover grammatical structures that are shared across different training instances. When the number of possible terms is huge, the chance that no two rules share any grammatical structure increases, undermining the grammar induction process. Existing systems therefore apply heuristics to constrain the space of allowable HR terms. However, these heuristics are typically ad-hoc, and not motivated on linguistic grounds. Thus there is a risk t"
W17-6810,P01-1019,0,0.223306,"sly connected to research that models the compositional mapping from strings to AMRs with grammars – using either synchronous grammars (Jones et al., 2012; Peng et al., 2015) or CCG (Artzi et al., 2015; Misra and Artzi, 2016). Not all AMR parsers learn explicit grammars (Flanigan et al., 2014; Peng et al., 2017). However, we believe that these, too, may benefit from access to the compositional structure of the AMRs, which the algebra we present makes easier to compute. The operations our algebra uses to combine semantic representations are closely related to those of the “semantic algebra” of Copestake et al. (2001), which was intended to reflect universal semantic combination operations for large-scale handwritten HPSG grammars. More distantly, the ability of our semantic representations to select the type of its arguments echoes the use of types in Montague Grammar and in CCG (Steedman, 2001), applied to graphs. 3 Algebras for constructing graphs We start by reviewing the HR algebra and discussing some of its shortcomings in the context of grammar induction. Then we introduce the apply-modify graph algebra, which tackles these shortcomings in a linguistically adequate way. Notation: For a given (partia"
W17-6810,P14-1134,0,0.092468,"For example, Figure 1: AMR of The destruction of Rome and Rome was destroyed have the same AMR; among snake swallows its prey without chewing. others, tense and determiners are dropped. The availability of the AMRBank has spawned much research on semantic parsing into AMR representations. The work in this paper is most obviously connected to research that models the compositional mapping from strings to AMRs with grammars – using either synchronous grammars (Jones et al., 2012; Peng et al., 2015) or CCG (Artzi et al., 2015; Misra and Artzi, 2016). Not all AMR parsers learn explicit grammars (Flanigan et al., 2014; Peng et al., 2017). However, we believe that these, too, may benefit from access to the compositional structure of the AMRs, which the algebra we present makes easier to compute. The operations our algebra uses to combine semantic representations are closely related to those of the “semantic algebra” of Copestake et al. (2001), which was intended to reflect universal semantic combination operations for large-scale handwritten HPSG grammars. More distantly, the ability of our semantic representations to select the type of its arguments echoes the use of types in Montague Grammar and in CCG (S"
W17-6810,P16-1192,1,0.844109,"ise quantitative analysis, e.g. in the context of grammar induction, for future work. Runtime. We finish by measuring the mean runtimes to compute the decomposition automata (Fig. 13c). Once again, we find that the AM algebra solidly outperforms the HR algebra. The runtimes of HR-S3 are too slow to be useful in practice, whereas even the highest-coverage algebra AM-C2 decomposes even large graphs in seconds. Moreover, the runtimes for AM-C1 are faster than even for the very low-coverage HR-S2 algebra. The previously fastest parser for graphs using hyperedge replacement grammars was the one of Groschwitz et al. (2016), which used Interpreted Regular Tree Grammars (IRTGs) (Koller and Kuhlmann, 2011) together with the HR algebra. Because we have seen how to compute decomposition automata for the AM algebra in Section 5, we can do graph parsing with IRTGs over the AM algebra instead. The fact that decomposition automata for the AM algebra are smaller and faster to compute promises a further speed-up for graph parsing as well, making wide-coverage graph parsing for large graphs feasible. 7 Conclusion In this paper, we have introduced the apply-modify (AM) algebra for graphs. The AM algebra replaces the general"
W17-6810,P15-1143,1,0.718877,"th an AMR. The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR. This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (Jones et al., 2012, 2013; Artzi et al., 2015; Peng et al., 2015). In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra (Courcelle, 1993), is huge (Groschwitz et al., 2015). This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances. Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics. We propose a novel method to generate a constrained set of derivations directly from an AMR, but without losing linguistically significant phenomena and parses. To this end we present an apply-modify (AM) graph algebra for combining graphs using operations that re"
W17-6810,C12-1083,0,0.208863,"sentences to Abstract Meaning Representations (AMRs), graphs which represent the predicate-argument structure of the sentences. Such work builds upon the AMRBank (Banarescu et al., 2013), a corpus in which each sentence has been manually annotated with an AMR. The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR. This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (Jones et al., 2012, 2013; Artzi et al., 2015; Peng et al., 2015). In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra (Courcelle, 1993), is huge (Groschwitz et al., 2015). This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances. Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics. We prop"
W17-6810,W13-1810,1,0.90823,"Missing"
W17-6810,W15-0127,1,0.946888,"nction between F and f as in the definition above. But as a general principle in this paper, this common notation always refers to the function in text and definitions, and to the symbol in terms such as in Figures 2a and 2b. 3.1 S-graphs and the HR algebra love rt A standard algebra for the theoretical literature for describing graphs ARG0 ARG1 is the HR algebra of Courcelle (1993). It is very closely related to prince rose hyperedge replacement grammars (Drewes et al., 1997), which have been used extensively for grammars of AMR languages (Chiang et al., (a) AMR 2013; Peng et al., 2015), and Koller (2015) showed explicitly how to do compositional semantic construction using the HR algebra. fX The objects of the HR algebra are s-graphs G = (g, S), consisting of a graph g (here, directed and with node and edge labels) and a par|| tial function S : S V , which maps sources from a fixed finite set S of source names to nodes of g. Sources thus serve as external, inter|| Nlove pretable names for some of the nodes. If we have S(a) = v, then we call v an a-source of G. An example of an s-graph with a root-source || (rt) and a subject-source (S) is shown in Fig. 2f. Sources are marked in renrt7→X diagr"
W17-6810,W11-2902,1,0.922807,"never forgotten, and each can therefore be used only on one node in the derivation. Two COREF sources with the same index will be automatically merged together during the usual A PP and MOD operations, due to the semantics of the underlying merge operation of the HR algebra. COREF sources increase runtimes and the number of possible terms per graph significantly (see Section 6), and thus we limit the number of COREF sources to zero to two in practice. 5.3 Obtaining the set of terms We can compactly represent the set of all AM terms that evaluate to a given AMR G in a decomposition automaton (Koller and Kuhlmann, 2011), a chart-like data structure in which shared subterms are represented only once. We can enumerate the terms from this automaton. To enumerate all rules of the decomposition automaton, we explore it bottom-up, with Algorithm 1. We first find all constants for G in Line 2, as described in Section 5.1, and then repeatedly apply A PP and MOD operations (Lines 3 onward; the set O contains all relevant A PP and MOD operations). The constants and the successful operation applications are stored as rules in the automaton. To ensure that the resulting terms evaluate to the input graph G, we use subgra"
W17-6810,D16-1183,0,0.0305352,"because they gloss over certain details of the syntactic realization. For example, Figure 1: AMR of The destruction of Rome and Rome was destroyed have the same AMR; among snake swallows its prey without chewing. others, tense and determiners are dropped. The availability of the AMRBank has spawned much research on semantic parsing into AMR representations. The work in this paper is most obviously connected to research that models the compositional mapping from strings to AMRs with grammars – using either synchronous grammars (Jones et al., 2012; Peng et al., 2015) or CCG (Artzi et al., 2015; Misra and Artzi, 2016). Not all AMR parsers learn explicit grammars (Flanigan et al., 2014; Peng et al., 2017). However, we believe that these, too, may benefit from access to the compositional structure of the AMRs, which the algebra we present makes easier to compute. The operations our algebra uses to combine semantic representations are closely related to those of the “semantic algebra” of Copestake et al. (2001), which was intended to reflect universal semantic combination operations for large-scale handwritten HPSG grammars. More distantly, the ability of our semantic representations to select the type of its"
W17-6810,K15-1004,0,0.57415,"(AMRs), graphs which represent the predicate-argument structure of the sentences. Such work builds upon the AMRBank (Banarescu et al., 2013), a corpus in which each sentence has been manually annotated with an AMR. The training instances in the AMRBank are annotated only with the AMRs themselves, not with the structure of a compositional derivation of the AMR. This poses a challenge for semantic parsing, especially for approaches which induce a grammar from the data and thus must make this compositional structure explicit in order to learn rules (Jones et al., 2012, 2013; Artzi et al., 2015; Peng et al., 2015). In general, the number of ways in which an AMR graph can be built from its atomic parts, e.g. using the generic graph-combining operations of the HR algebra (Courcelle, 1993), is huge (Groschwitz et al., 2015). This makes grammar induction computationally expensive and undermines its ability to discover grammatical structures that can be shared across multiple instances. Existing approaches therefore resort to heuristics that constrain the space of possible analyses, often with limited regard to the linguistic reality of these heuristics. We propose a novel method to generate a constrained s"
W17-6810,E17-1035,0,0.0512184,"AMR of The destruction of Rome and Rome was destroyed have the same AMR; among snake swallows its prey without chewing. others, tense and determiners are dropped. The availability of the AMRBank has spawned much research on semantic parsing into AMR representations. The work in this paper is most obviously connected to research that models the compositional mapping from strings to AMRs with grammars – using either synchronous grammars (Jones et al., 2012; Peng et al., 2015) or CCG (Artzi et al., 2015; Misra and Artzi, 2016). Not all AMR parsers learn explicit grammars (Flanigan et al., 2014; Peng et al., 2017). However, we believe that these, too, may benefit from access to the compositional structure of the AMRs, which the algebra we present makes easier to compute. The operations our algebra uses to combine semantic representations are closely related to those of the “semantic algebra” of Copestake et al. (2001), which was intended to reflect universal semantic combination operations for large-scale handwritten HPSG grammars. More distantly, the ability of our semantic representations to select the type of its arguments echoes the use of types in Montague Grammar and in CCG (Steedman, 2001), appl"
W17-6810,W13-2322,0,\N,Missing
W18-5018,J96-1002,0,0.351105,"keness of REs compared to training individual user models. We start with a basic model of the way in which people produce and comprehend language. In order to generalize over production and comprehension, we will simply say that a human language user exhibits a certain behavior b among a range of possible behaviors, in response to a stimulus s. The behavior of a speaker is the utterance b they produce in order to achieve a communicative goal s; the behavior of a listener is the meaning b which they assign to the utterance s they hear. Given this terminology, we define a basic loglinear model (Berger et al., 1996) of language use as follows: exp(ρ · φ(b, s)) P (b|s; ρ) = P (1) 0 b0 exp(ρ · φ(b , s)) The ability to adapt to the comprehension and production preferences of a user is especially important in the context of a dialog system, where there are multiple chances of interacting with the same user. Some methods adapt to dialog system users by explicitly modeling the users’ knowledge state. An early example is Paris (1988); she selects a discourse plan for a user, depending on their level of domain knowledge ranging between novice and expert, but provides no mechanism for inferring the group to which"
W18-5018,E12-1077,1,0.883731,"Missing"
W18-5018,W14-5002,1,0.858434,"cifically on the generation of referring expressions (REs). Using (1) as a comprehension model, Engonopoulos et al. (2013) developed an RE generation model in which the stimulus s = (r, c) consists of an RE r and a visual context c of the GIVE Challenge (Striegnitz et al., 2011), as illustrated in Fig. 1. The behavior is the object b in the visual scene to which the user will resolve the RE. Thus for instance, when we consider the RE r =“the blue button” in the context of Fig. 1, the log-linear model may assign a higher probability to the button on the right than to the one in the background. Engonopoulos and Koller (2014) develop an algorithm for generating the RE r which maximizes P (b∗ |s; ρ), where b∗ is the intended referent in this setting. Conversely, log-linear models can also be used to directly capture how a human speaker would refer to an object in a given scene. In this case, the stimulus s = (a, c) consists of the target object a and 172 σ (π) σ (ρ) π ρ(g) 1≤g≤K g b(d) Figure 1: A visual scene and a system-generated instruction from the GIVE challenge. d ∈ D(u) u∈U the visual context c, and the behavior b is the RE. We follow Ferreira and Paraboni (2014) in training individual models for the differ"
W18-5018,D13-1134,1,0.903605,"group membership based on observable behavior, and use it to generate utterances that are suitable to the user’s true group. We evaluate our model on two tasks involving the generation of referring expressions (RE). First, we predict the use of spatial relations in humanlike REs in the GRE3D domain (Viethen and Dale, 2010) using a log-linear production model in the spirit of Ferreira and Paraboni (2014). Second, we predict the comprehension of generated REs, in a synthetic dataset based on data from the GIVE Challenge domain (Striegnitz et al., 2011) with the log-linear comprehension model of Engonopoulos et al. (2013). In both cases, we show that our model discovers user groups in the training data and infers the group of unseen users with high confidence after only a few interactions during testing. In the GRE3D domain, our system outperformed a strong baseline which used demographic information for the users. 2 Related Work Differences between individual users have a substantial impact on language comprehension. Factors that play a role include level of expertise and spatial ability (Benyon and Murray, 1993); age (H¨auser et al., 2017); gender (Dr¨ager and Koller, 171 Proceedings of the SIGDIAL 2018 Conf"
W18-5018,W17-3536,0,0.0221456,"are also reflected in the way people produce language. Viethen and Dale (2008) present a corpus study of human-produced REs (GRE3D3) for simple visual scenes, where they note two clearly distinguishable groups of speakers, one that always uses a spatial relation and one that never does. Ferreira and Paraboni (2014) show that a model using speaker-specific information outperforms a generic model in predicting the attributes used by a speaker when producing an RE. However, their system needs to have seen the particular speaker in training, while our system can dynamically adapt to unseen users. Ferreira and Paraboni (2017) also demonstrate that splitting speakers in predefined groups and training each group separately improves the human likeness of REs compared to training individual user models. We start with a basic model of the way in which people produce and comprehend language. In order to generalize over production and comprehension, we will simply say that a human language user exhibits a certain behavior b among a range of possible behaviors, in response to a stimulus s. The behavior of a speaker is the utterance b they produce in order to achieve a communicative goal s; the behavior of a listener is th"
W18-5018,J14-4006,0,0.177802,". 1 Introduction People vary widely both in their linguistic preferences when producing language and in their ability to understand specific natural-language expressions, depending on what they know about the domain, their age and cognitive capacity, and many other factors. It has long been recognized that effective NLG systems should therefore adapt to the current user, in order to generate language which works well for them. This adaptation needs to address all levels of the NLG pipeline, including discourse planning (Paris, 1988), sentence planning (Walker et al., 2007), and RE generation (Janarthanam and Lemon, 2014), and depends on many features of the user, including level of expertise and language proficiency, age, and gender. Existing techniques for adapting the output of an NLG system have shortcomings which limit their practical usefulness. Some systems need user-specific information in training (Ferreira and Paraboni, 2014) and therefore cannot generalize to unseen users. Other systems assume that each user in the training data is annotated with their group, which allows them to learn a model from the data of each group. However, hand-designed user groups may not reflect the true variability of the"
W18-5018,J88-3006,0,0.722458,"assign unseen users to the correct groups as they interact with the system. 1 Introduction People vary widely both in their linguistic preferences when producing language and in their ability to understand specific natural-language expressions, depending on what they know about the domain, their age and cognitive capacity, and many other factors. It has long been recognized that effective NLG systems should therefore adapt to the current user, in order to generate language which works well for them. This adaptation needs to address all levels of the NLG pipeline, including discourse planning (Paris, 1988), sentence planning (Walker et al., 2007), and RE generation (Janarthanam and Lemon, 2014), and depends on many features of the user, including level of expertise and language proficiency, age, and gender. Existing techniques for adapting the output of an NLG system have shortcomings which limit their practical usefulness. Some systems need user-specific information in training (Ferreira and Paraboni, 2014) and therefore cannot generalize to unseen users. Other systems assume that each user in the training data is annotated with their group, which allows them to learn a model from the data of"
W18-5018,W08-1109,0,0.0153488,"line which used demographic information for the users. 2 Related Work Differences between individual users have a substantial impact on language comprehension. Factors that play a role include level of expertise and spatial ability (Benyon and Murray, 1993); age (H¨auser et al., 2017); gender (Dr¨ager and Koller, 171 Proceedings of the SIGDIAL 2018 Conference, pages 171–179, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics 2012); or language proficiency (Koller et al., 2010). 3 Individual differences are also reflected in the way people produce language. Viethen and Dale (2008) present a corpus study of human-produced REs (GRE3D3) for simple visual scenes, where they note two clearly distinguishable groups of speakers, one that always uses a spatial relation and one that never does. Ferreira and Paraboni (2014) show that a model using speaker-specific information outperforms a generic model in predicting the attributes used by a speaker when producing an RE. However, their system needs to have seen the particular speaker in training, while our system can dynamically adapt to unseen users. Ferreira and Paraboni (2017) also demonstrate that splitting speakers in prede"
W18-5018,U10-1013,0,0.0648768,"Missing"
W19-8601,W14-5002,1,0.847726,"icit track of (finite) sets of possible referents and intersecting properties until the set becomes singleton. By contrast, a unique indefinite RE to an object which does not exist yet must distinguish it from 1 Note that type itself is also a feature to distinguish objects of different types. 3 type distinguishing features 3 bridge {type, corner1 , corner3 } {type, corner1 , width, length} {type, corner1 , corner2 } {type, corner1 , length, orientation} In this paper, we build upon the chart generation algorithm of (Koller and Engonopoulos, 2017) for Semantically Interpreted Grammars (SIGs, (Engonopoulos and Koller, 2014)). SIGs are synchronous grammars which compositionally relate natural-language expressions with their possible referents; they are a special case of IRTGs (Koller and Kuhlmann, 2011). The K&E algorithm performs surface realization together with the generation of definite REs; we extend it to indefinites here. At the core of a SIG is a regular tree gramtop mar (RTG, (Comon et al., 2007)) which blue generates a language of derivation trees. These Figure 4: Derivation tree derivation trees rep- t for “The block on top of the blue block”. resent the underlying structure of the referring expression"
W19-8601,W11-2902,1,0.709558,"ust distinguish it from 1 Note that type itself is also a feature to distinguish objects of different types. 3 type distinguishing features 3 bridge {type, corner1 , corner3 } {type, corner1 , width, length} {type, corner1 , corner2 } {type, corner1 , length, orientation} In this paper, we build upon the chart generation algorithm of (Koller and Engonopoulos, 2017) for Semantically Interpreted Grammars (SIGs, (Engonopoulos and Koller, 2014)). SIGs are synchronous grammars which compositionally relate natural-language expressions with their possible referents; they are a special case of IRTGs (Koller and Kuhlmann, 2011). The K&E algorithm performs surface realization together with the generation of definite REs; we extend it to indefinites here. At the core of a SIG is a regular tree gramtop mar (RTG, (Comon et al., 2007)) which blue generates a language of derivation trees. These Figure 4: Derivation tree derivation trees rep- t for “The block on top of the blue block”. resent the underlying structure of the referring expression; Fig. 4 shows such a derivation tree. Derivation trees are built by repeatedly replacing nonterminal symbols using the production rules of the RTG (see the first lines in the toy gr"
W19-8601,J12-1006,0,0.0599495,"Missing"
W19-8601,J03-1003,0,0.10638,"Missing"
