2021.conll-1.36,Does referent predictability affect the choice of referential form? A computational approach using masked coreference resolution,2021,-1,-1,3,1,11381,laura aina,Proceedings of the 25th Conference on Computational Natural Language Learning,0,"It is often posited that more predictable parts of a speaker{'}s meaning tend to be made less explicit, for instance using shorter, less informative words. Studying these dynamics in the domain of referring expressions has proven difficult, with existing studies, both psycholinguistic and corpus-based, providing contradictory results. We test the hypothesis that speakers produce less informative referring expressions (e.g., pronouns vs. full noun phrases) when the context is more informative about the referent, using novel computational estimates of referent predictability. We obtain these estimates training an existing coreference resolution system for English on a new task, masked coreference resolution, giving us a probability distribution over referents that is conditioned on the context but not the referring expression. The resulting system retains standard coreference resolution performance while yielding a better estimate of human-derived referent predictability than previous attempts. A statistical analysis of the relationship between model output and mention form supports the hypothesis that predictability affects the form of a mention, both its morphosyntactic type and its length."
2021.blackboxnlp-1.37,Controlled tasks for model analysis: Retrieving discrete information from sequences,2021,-1,-1,2,1,12128,ionutteodor sorodoc,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"In recent years, the NLP community has shown increasing interest in analysing how deep learning models work. Given that large models trained on complex tasks are difficult to inspect, some of this work has focused on controlled tasks that emulate specific aspects of language. We propose a new set of such controlled tasks to explore a crucial aspect of natural language processing that has not received enough attention: the need to retrieve discrete information from sequences. We also study model behavior on the tasks with simple instantiations of Transformers and LSTMs. Our results highlight the beneficial role of decoder attention and its sometimes unexpected interaction with other components. Moreover, we show that, for most of the tasks, these simple models still show significant difficulties. We hope that the community will take up the analysis possibilities that our tasks afford, and that a clearer understanding of model behavior on the tasks will lead to better and more transparent models."
2020.lrec-1.710,Object Naming in Language and Vision: A Survey and a New Dataset,2020,-1,-1,3,1,18053,carina silberer,Proceedings of the 12th Language Resources and Evaluation Conference,0,"People choose particular names for objects, such as dog or puppy for a given dog. Object naming has been studied in Psycholinguistics, but has received relatively little attention in Computational Linguistics. We review resources from Language and Vision that could be used to study object naming on a large scale, discuss their shortcomings, and create a new dataset that affords more opportunities for analysis and modeling. Our dataset, ManyNames, provides 36 name annotations for each of 25K objects in images selected from VisualGenome. We highlight the challenges involved and provide a preliminary analysis of the ManyNames data, showing that there is a high level of agreement in naming, on average. At the same time, the average number of name types associated with an object is much higher in our dataset than in existing corpora for Language and Vision, such that ManyNames provides a rich resource for studying phenomena like hierarchical variation (chihuahua vs. dog), which has been discussed at length in the theoretical literature, and other less well studied phenomena like cross-classification (cake vs. dessert)."
2020.coling-main.172,Humans Meet Models on Object Naming: A New Dataset and Analysis,2020,-1,-1,4,1,18053,carina silberer,Proceedings of the 28th International Conference on Computational Linguistics,0,"We release ManyNames v2 (MN v2), a verified version of an object naming dataset that contains dozens of valid names per object for 25K images. We analyze issues in the data collection method originally employed, standard in Language {\&} Vision (L{\&}V), and find that the main source of noise in the data comes from simulating a naming context solely from an image with a target object marked with a bounding box, which causes subjects to sometimes disagree regarding which object is the target. We also find that both the degree of this uncertainty in the original data and the amount of true naming variation in MN v2 differs substantially across object domains. We use MN v2 to analyze a popular L{\&}V model and demonstrate its effectiveness on the task of object naming. However, our fine-grained analysis reveals that what appears to be human-like model behavior is not stable across domains, e.g., the model confuses people and clothing objects much more frequently than humans do. We also find that standard evaluations underestimate the actual effectiveness of the naming model: on the single-label names of the original dataset (Visual Genome), it obtains â27{\%} accuracy points than on MN v2, that includes all valid object names."
2020.acl-main.384,Probing for Referential Information in Language Models,2020,-1,-1,3,1,12128,ionutteodor sorodoc,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Language models keep track of complex information about the preceding context {--} including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too."
W19-0410,Don{'}t Blame Distributional Semantics if it can{'}t do Entailment,2019,55,0,2,0.666667,11384,matthijs westera,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"Distributional semantics has had enormous empirical success in Computational Linguistics and Cognitive Science in modeling various semantic phenomena, such as semantic similarity, and distributional models are widely used in state-of-the-art Natural Language Processing systems. However, the theoretical status of distributional semantics within a broader theory of language and cognition is still unclear: What does distributional semantics model? Can it be, on its own, a fully adequate model of the meanings of linguistic expressions? The standard answer is that distributional semantics is not fully adequate in this regard, because it falls short on some of the central aspects of formal semantic approaches: truth conditions, entailment, reference, and certain aspects of compositionality. We argue that this standard answer rests on a misconception: These aspects do not belong in a theory of expression meaning, they are instead aspects of speaker meaning, i.e., communicative intentions in a particular context. In a slogan: words do not refer, speakers do. Clearing this up enables us to argue that distributional semantics on its own is an adequate model of expression meaning. Our proposal sheds light on the role of distributional semantics in a broader theory of language and cognition, its relationship to formal semantics, and its place in computational models."
P19-1324,Putting Words in Context: {LSTM} Language Models and Lexical Ambiguity,2019,26,0,3,1,11381,laura aina,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In neural network models of language, words are commonly represented using context-invariant representations (word embeddings) which are then put in context in the hidden layers. Since words are often ambiguous, representing the contextually relevant information is not trivial. We investigate how an LSTM language model deals with lexical ambiguity in English, designing a method to probe its hidden representations for lexical and contextual information about words. We find that both types of information are represented to a large extent, but also that there is room for improvement for contextual information."
N19-1210,Short-Term Meaning Shift: A Distributional Exploration,2019,0,4,3,0,21574,marco tredici,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We present the first exploration of meaning shift over short periods of time in online communities using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard model for meaning shift detection on short-term meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this."
N19-1378,What do Entity-Centric Models Learn? Insights from Entity Linking in Multi-Party Dialogue,2019,0,0,5,1,11381,laura aina,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Humans use language to refer to entities in the external world. Motivated by this, in recent years several models that incorporate a bias towards learning entity representations have been proposed. Such entity-centric models have shown empirical success, but we still know little about why. In this paper we analyze the behavior of two recently proposed entity-centric models in a referential task, Entity Linking in Multi-party Dialogue (SemEval 2018 Task 4). We show that these models outperform the state of the art on this task, and that they do better on lower frequency entities than a counterpart model that is not entity-centric, with the same model size. We argue that making models entity-centric naturally fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed."
S18-1008,{AMORE}-{UPF} at {S}em{E}val-2018 Task 4: {B}i{LSTM} with Entity Library,2018,5,0,5,1,11381,laura aina,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes our winning contribution to SemEval 2018 Task 4: Character Identification on Multiparty Dialogues. It is a simple, standard model with one key innovation, an entity library. Our results show that this innovation greatly facilitates the identification of infrequent characters. Because of the generic nature of our model, this finding is potentially relevant to any task that requires the effective learning from sparse or imbalanced data."
D18-1323,"How to represent a word and predict it, too: Improving tied architectures for language modelling",2018,0,4,3,0.952381,22877,kristina gulordava,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Recent state-of-the-art neural language models share the representations of words given by the input and output mappings. We propose a simple modification to these architectures that decouples the hidden state from the word embedding prediction. Our architecture leads to comparable or better results compared to previous tied models and models without tying, with a much smaller number of parameters. We also extend our proposal to word2vec models, showing that tying is appropriate for general word prediction tasks."
W17-6904,Living a discrete life in a continuous world: Reference in cross-modal entity tracking,2017,22,0,1,1,11383,gemma boleda,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-3515,Talking about the world with a distributed model,2017,0,0,1,1,11383,gemma boleda,Proceedings of the 10th International Conference on Natural Language Generation,0,"We use language to talk about the world, and so reference is a crucial property of language. However, modeling reference is particularly difficult, as it involves both continuous and discrete as-pects of language. For instance, referring expressions like {``}the big mug{''} or {``}it{''} typically contain content words ({``}big{''}, {``}mug{''}), which are notoriously fuzzy or vague in their meaning, and also fun-ction words ({``}the{''}, {``}it{''}) that largely serve as discrete pointers. Data-driven, distributed models based on distributional semantics or deep learning excel at the former, but struggle with the latter, and the reverse is true for symbolic models. I present ongoing work on modeling reference with a distribu-ted model aimed at capturing both aspects, and learns to refer directly from reference acts."
S17-1012,"Distributed Prediction of Relations for Entities: The Easy, The Difficult, and The Impossible",2017,0,1,2,0.952381,5573,abhijeet gupta,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Word embeddings are supposed to provide easy access to semantic relations such as {``}male of{''} (man{--}woman). While this claim has been investigated for concepts, little is known about the distributional behavior of relations of (Named) Entities. We describe two word embedding-based models that predict values for relational attributes of entities, and analyse them. The task is challenging, with major performance differences between relations. Contrary to many NLP tasks, high difficulty for a relation does not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pick up by word embeddings."
E17-2013,Instances and concepts in distributional space,2017,25,6,1,1,11383,gemma boleda,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Instances ({``}Mozart{''}) are ontologically distinct from concepts or classes ({``}composer{''}). Natural language encompasses both, but instances have received comparatively little attention in distributional semantics. Our results show that instances and concepts differ in their distributional properties. We also establish that instantiation detection ({``}Mozart {--} composer{''}) is generally easier than hypernymy detection ({``}chemist {--} scientist{''}), and that results on the influence of input representation do not transfer from hyponymy to instantiation."
W16-3211,"{``}Look, some Green Circles!{''}: Learning to Quantify from Images",2016,13,7,3,0,33012,ionut sorodoc,Proceedings of the 5th Workshop on Vision and Language,0,"In this paper, we investigate whether a neural network model can learn the meaning of natural language quantifiers (no, some and all) from their use in visual contexts. We show that memory networks perform well in this task, and that explicit counting is not necessary to the systemxe2x80x99s performance, supporting psycholinguistic evidence on the acquisition of quantifiers."
P16-1144,The {LAMBADA} dataset: Word prediction requiring a broad discourse context,2016,15,8,8,0,15539,denis paperno,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text."
J16-4002,Formal Distributional Semantics: Introduction to the Special Issue,2016,72,14,1,1,11383,gemma boleda,Computational Linguistics,0,"Formal Semantics and Distributional Semantics are two very influential semantic frameworks in Computational Linguistics. Formal Semantics is based on a symbolic tradition and centered around the inferential properties of language. Distributional Semantics is statistical and data-driven, and focuses on aspects of meaning related to descriptive content. The two frameworks are complementary in their strengths, and this has motivated interest in combining them into an overarching semantic framework: a Formal Distributional Semantics. Given the fundamentally different natures of the two paradigms, however, building an integrative framework poses significant theoretical and engineering challenges. The present issue of Computational Linguistics advances the state of the art in Formal Distributional Semantics; this introductory article explains the motivation behind it and summarizes the contributions of previous work on the topic, providing the necessary background for the articles that follow."
D16-1123,Convolutional Neural Network Language Models,2016,23,16,3,0,5766,ngocquan pham,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-2712,Distributional Semantics in Use,2015,43,3,2,0,1053,raffaella bernardi,"Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics",0,"In this position paper we argue that an adequate semantic model must account for language in use, taking into account how discourse context affects the meaning of words and larger linguistic units. Distributional semantic models are very attractive models of meaning mainly because they capture conceptual aspects and are automatically induced from natural language data. However, they need to be extended in order to account for language use in a discourse or dialogue context. We discuss phenomena that the new generation of distributional semantic models should capture, and propose concrete tasks on which they could be tested."
D15-1002,Distributional vectors encode referential attributes,2015,30,20,2,0.952381,5573,abhijeet gupta,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany). In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants). In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes. We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to xe2x80x9cobjectifyxe2x80x9d distributional representations for entities, anchoring them more firmly in the external world in measurable ways."
S14-2141,{UT}exas: Natural Language Semantics using Distributional Semantics and Probabilistic Logic,2014,25,12,3,0.833333,37162,islam beltagy,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We represent natural language semantics by combining logical and distributional information in probabilistic logic. We use Markov Logic Networks (MLN) for the RTE task, and Probabilistic Soft Logic (PSL) for the STS task. The system is evaluated on the SICK dataset. Our best system achieves 73% accuracy on the RTE task, and a Pearsonxe2x80x99s correlation of 0.71 on the STS task."
C14-1097,Inclusive yet Selective: Supervised Distributional Hypernymy Detection,2014,31,80,3,0,3621,stephen roller,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found. We find that this hypothesis only holds when it is applied to relevant dimensions. We propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion."
W13-0104,Intensionality was only alleged: On adjective-noun composition in distributional semantics,2013,-1,-1,1,1,11383,gemma boleda,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,None
S13-1002,{M}ontague Meets {M}arkov: Deep Semantics with Probabilistic Logical Form,2013,42,57,3,0.833333,37162,islam beltagy,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks (MLNs). We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives. We also show that distributional phrase similarity, used as textual inference rules created on the fly, improves its performance."
S12-1023,Regular polysemy: A distributional model,2012,42,14,1,1,11383,gemma boleda,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Many types of polysemy are not word specific, but are instances of general sense alternations such as ANIMAL-FOOD. Despite their pervasiveness, regular alternations have been mostly ignored in empirical computational semantics. This paper presents (a) a general framework which grounds sense alternations in corpus data, generalizes them above individual words, and allows the prediction of alternations for new words; and (b) a concrete unsupervised implementation of the framework, the Centroid Attribute Model. We evaluate this model against a set of 2,400 ambiguous words and demonstrate that it outperforms two baselines."
P12-1015,Distributional Semantics in Technicolor,2012,36,209,2,0,10786,elia bruni,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance."
J12-3005,Modeling Regular Polysemy: A Study on the Semantic Classification of {C}atalan Adjectives,2012,81,13,1,1,11383,gemma boleda,Computational Linguistics,0,"We present a study on the automatic acquisition of semantic classes for Catalan adjectives from distributional and morphological information, with particular emphasis on polysemous adjectives. The aim is to distinguish and characterize broad classes, such as qualitative (gran xe2x80x98bigxe2x80x99) and relational (pulmonar xe2x80x98pulmonaryxe2x80x99) adjectives, as well as to identify polysemous adjectives such as economic (xe2x80x98economic xe2x88xa3 cheapxe2x80x99). We specifically aim at modeling regular polysemy, that is, types of sense alternations that are shared across lemmata. To date, both semantic classes for adjectives and regular polysemy have only been sparsely addressed in empirical computational linguistics.Two main specific questions are tackled in this article. First, what is an adequate broad semantic classification for adjectives? We provide empirical support for the qualitative and relational classes as defined in theoretical work, and uncover one type of adjective that has not received enough attention, namely, the event-related class. Se..."
D12-1112,First Order vs. Higher Order Modification in Distributional Semantics,2012,25,10,1,1,11383,gemma boleda,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Adjectival modification, particularly by expressions that have been treated as higher-order modifiers in the formal semantics tradition, raises interesting challenges for semantic composition in distributional semantic models. We contrast three types of adjectival modifiers -- intersectively used color terms (as in white towel, clearly first-order), subsectively used color terms (white wine, which have been modeled as both first- and higher-order), and intensional adjectives (former bassist, clearly higher-order) -- and test the ability of different composition strategies to model their behavior. In addition to opening up a new empirical domain for research on distributional semantics, our observations concerning the attested vectors for the different types of adjectives, the nouns they modify, and the resulting noun phrases yield insights into modification that have been little evident in the formal semantics literature to date."
W11-1501,"Extending the tool, or how to annotate historical language varieties",2011,10,16,2,0,44322,cristina sanchezmarco,"Proceedings of the 5th {ACL}-{HLT} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"We present a general and simple method to adapt an existing NLP tool in order to enable it to deal with historical varieties of languages. This approach consists basically in expanding the dictionary with the old word variants and in retraining the tagger with a small training corpus. We implement this approach for Old Spanish.n n The results of a thorough evaluation over the extended tool show that using this method an almost state-of-the-art performance is obtained, adequate to carry out quantitative studies in the humanities: 94.5% accuracy for the main part of speech and 92.6% for lemma. To our knowledge, this is the first time that such a strategy is adopted to annotate historical language varieties and we believe that it could be used as well to deal with other non-standard varieties of languages."
reese-etal-2010-wikicorpus,{W}ikicorpus: A Word-Sense Disambiguated Multilingual {W}ikipedia Corpus,2010,16,21,2,0,45828,samuel reese,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This article presents a new freely available trilingual corpus (Catalan, Spanish, English) that contains large portions of the Wikipedia and has been automatically enriched with linguistic information. To our knowledge, this is the largest such corpus that is freely available to the community: In its present version, it contains over 750 million words. The corpora have been annotated with lemma and part of speech information using the open source library FreeLing. Also, they have been sense annotated with the state of the art Word Sense Disambiguation algorithm UKB. As UKB assigns WordNet senses, and WordNet has been aligned across languages via the InterLingual Index, this sort of annotation opens the way to massive explorations in lexical semantics that were not possible before. We present a first attempt at creating a trilingual lexical resource from the sense-tagged Wikipedia corpora, namely, WikiNet. Moreover, we present two by-products of the project that are of use for the NLP community: An open source Java-based parser for Wikipedia pages developed for the construction of the corpus, and the integration of the WSD algorithm UKB in FreeLing."
peris-etal-2010-adn,{ADN}-Classifier:Automatically Assigning Denotation Types to Nominalizations,2010,13,9,3,0,43378,aina peris,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents the ADN-Classifier, an Automatic classification system of Spanish Deverbal Nominalizations aimed at identifying its semantic denotation (i.e. event, result, underspecified, or lexicalized). The classifier can be used for NLP tasks such as coreference resolution or paraphrase detection. To our knowledge, the ADN-Classifier is the first effort in acquisition of denotations for nominalizations using Machine Learning.We compare the results of the classifier when using a decreasing number of Knowledge Sources, namely (1) the complete nominal lexicon (AnCora-Nom) that includes sense distictions, (2) the nominal lexicon (AnCora-Nom) removing the sense-specific information, (3) nominalizationsÂ context information obtained from a treebank corpus (AnCora-Es) and (4) the combination of the previous linguistic resources. In a realistic scenario, that is, without sense distinction, the best results achieved are those taking into account the information declared in the lexicon (89.40{\%} accuracy). This shows that the lexicon contains crucial information (such as argument structure) that corpus-derived features cannot substitute for."
sanroma-boleda-2010-database,The Database of {C}atalan Adjectives,2010,6,1,2,0,46067,roser sanroma,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present the Database of Catalan Adjectives (DCA), a database with 2,296 adjective lemmata enriched with morphological, syntactic and semantic information. This set of adjectives has been collected from a fragment of the Corpus Textual Informatitzat de la Llengua Catalana of the Institut dÂEstudis Catalans and constitutes a representative sample of the adjective class in Catalan as a whole. The database includes both manually coded and automatically extracted information regarding the most prominent properties used in the literature regarding the semantics of adjectives, such as morphological origin, suffix (if any), predicativity, gradability, adjective position with respect to the head noun, adjective modifiers, or semantic class. The DCA can be useful for NLP applications using adjectives (from POS-taggers to Opinion Mining applications) and for linguistic analysis regarding the morphological, syntactic, and semantic properties of adjectives. We now make it available to the research community under a Creative Commons Attribution Share Alike 3.0 Spain license."
sanchez-marco-etal-2010-annotation,Annotation and Representation of a Diachronic Corpus of {S}panish,2010,8,16,2,0,44322,cristina sanchezmarco,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this article we describe two different strategies for the automatic tagging of a Spanish diachronic corpus involving the adaptation of existing NLP tools developed for modern Spanish. In the initial approach we follow a state-of-the-art strategy, which consists on standardizing the spelling and the lexicon. This approach boosts POS-tagging accuracy to 90, which represents a raw improvement of over 20{\%} with respect to the results obtained without any pre-processing. In order to enable non-expert users in NLP to use this new resource, the corpus has been integrated into IAC (Corpora Interface Access). We discuss the shortcomings of the initial approach and propose a new one, which does not consist in adapting the source texts to the tagger, but rather in modifying the tagger for the direct treatment of the old variants.This second strategy addresses some important shortcomings in the previous approach and is likely to be useful not only in the creation of diachronic linguistic resources but also for the treatment of dialectal or non-standard variants of synchronic languages as well."
melero-etal-2010-language,Language Technology Challenges of a {`}Small{'} Language ({C}atalan),2010,3,2,2,0,4983,maite melero,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we present a brief snapshot of the state of affairs in computational processing of Catalan and the initiatives that are starting to take place in an effort to bring the field a step forward, by making a better and more efficient use of the already existing resources and tools, by bridging the gap between research and market, and by establishing periodical meeting points for the community. In particular, we present the results of the First Workshop on the Computational Processing of Catalan, which succeeded in putting together a fair representation of the research in the area, and received attention from both the industry and the administration. Aside from facilitating communication among researchers and between developers and users, the Workshop provided the organizers with valuable information about existing resources, tools, developers and providers. This information has allowed us to go a step further by setting up a ÂharvestingÂ procedure which will hopefully build the seed of a portal-catalogue-observatory of language resources and technologies in Catalan."
vandeghinste-etal-2008-evaluation,Evaluation of a Machine Translation System for Low Resource Languages: {METIS}-{II},2008,16,11,10,0,34184,vincent vandeghinste,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper we describe the METIS-II system and its evaluation on each of the language pairs: Dutch, German, Greek, and Spanish to English. The METIS-II system envisaged developing a data-driven approach in which no parallel corpus is required, and in which no full parser or extensive rule sets are needed. We describe evalution on a development test set and on a test set coming from Europarl, and compare our results with SYSTRAN. We also provide some further analysis, researching the impact of the number and source of the reference translations and analysing the results according to test text type. The results are expectably lower for the METIS system, but not at an unatainable distance from a mature system like SYSTRAN."
D07-1018,Modelling Polysemy in Adjective Classes by Multi-Label Classification,2007,33,8,1,1,11383,gemma boleda,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"This paper assesses the role of multi-label classification in modelling polysemy for language acquisition tasks. We focus on the acquisition of semantic classes for Catalan adjectives, and show that polysemy acquisition naturally suits architectures used for multilabel classification. Furthermore, we explore the performance of information drawn from different levels of linguistic description, using feature sets based on morphology, syntax, semantics, and n-gram distribution. Finally, we demonstrate that ensemble classifiers are a powerful and adequate way to combine different types of linguistic evidence: a simple, majority voting ensemble classifier improves the accuracy from 62.5% (best single classifier) to 84%."
W06-1704,{CUCW}eb: A {C}atalan corpus built from the Web,2006,13,14,1,1,11383,gemma boleda,Proceedings of the 2nd International Workshop on Web as Corpus,0,"This paper presents CUCWeb, a 166 million word corpus for Catalan built by crawling the Web. The corpus has been annotated with NLP tools and made available to language users through a flexible web interface. The developed architecture is quite general, so that it can be used to create corpora for other languages."
W05-1009,Morphology vs. Syntax in Adjective Class Acquisition,2005,12,6,1,1,11383,gemma boleda,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,"This paper discusses the role of morphological and syntactic information in the automatic acquisition of semantic classes for Catalan adjectives, using decision trees as a tool for exploratory data analysis. We show that a simple mapping from the derivational type to the semantic class achieves 70.1% accuracy; syntactic function reaches a slightly higher accuracy of 73.5%. Although the accuracy scores are quite similar with the two resulting classifications, the kinds of mistakes are qualitatively very different. Morphology can be used as a baseline classification, and syntax can be used as a clue when there are mismatches between morphology and semantics."
2005.mtsummit-ebmt.1,An n-gram Approach to Exploiting a Monolingual Corpus for Machine Translation,2005,-1,-1,2,0.714286,13064,toni badia,Workshop on example-based machine translation,0,None
W04-3214,The Influence of Argument Structure on Semantic Role Assignment,2004,17,1,2,0,411,sebastian pado,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
C04-1161,Acquisition of Semantic Classes for Adjectives from Distributional Evidence,2004,10,11,1,1,11383,gemma boleda,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper, we present a clustering experiment directed at the acquisition of semantic classes for adjectives in Catalan, using only shallow distributional features.We define a broad-coverage classification for adjectives based on Ontological Semantics. We classify along two parameters (number of arguments and ontological kind of denotation), achieving reliable agreement results among human judges. The clustering procedure achieves a comparable agreement score for one of the parameters, and a little lower for the other."
alsina-etal-2002-catcg,{CATCG}: a general purpose parsing tool applied,2002,8,14,3,0,53375,alex alsina,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper focuses on the language processing tool being developed at our centre and briefly describes two of its applications. CATCG, our morphosyntactic analyser, is designed to deal with general written Catalan text. In CATCG the whole processing task has been divided into specific subtasks and for each one of them we try to apply the best strategy available. The most relevant properties of our system are its robustness, the fact that we have given reusability a very high priority, and the goal of acquiring linguistic information by fully automatic means. The paper is structured as follows: section 1 and 2 explicate and describe the global architecture of CATCG. Section 3 shows the output of CATCG and gives data on its performance. Section 4 describes two projects to which CATCG is being applied: BancTrad and PrADo. Section 5 presents our plans for future work. Section 6 closes the paper with some conclusions. 1. CATCG: A modular architecture CATCG aims at providing an automatic analysis of free running text in Catalan. It is a modular system that allows the possibility to choose the best strategy available for each specific task. The system has evolved from being a tagging tool to being a partial parser, and will include semantic and pragmatic information in the future. Our interest is to tag texts with linguistic information, so that operations that are performed on them can be based not only on surface information (basically word forms) but also on their linguistic structure. CATCG is being developed to achieve a linguistic parsing of running text as precise and detailed as possible. It is a general purpose parser, because we have conceived its tag sets as non-task specific as well as model independent. We can think of a wide range of further applications for it (from grammar checking to information extraction). Deep linguistic analysis has proven to be impossible in one shot, since neither the resources nor the techniques are fully available at one given moment in time. We therefore Figure 1. Architecture of CATCG gave priority to being able to (1) process and extract information from texts from the very beginning; and (2) add new modules if and when they were available. 2. A description of CATCG CATCG consists of five modules, most of them divided in several sub-modules (some of which are in turn further modularised, see Figure 1): pre-processing, morphological tag mapping, morphological tag disambiguation, syntactic tag mapping and syntactic tag disambiguation. Each of the modules can be modified without affecting the rest. Furthermore, a progressive improvement of the whole processing can be obtained as new modules are available. Technically speaking, it incorporates two programming languages: Prolog and Perl; and two formalisms: SEGMORF (an extension of the ALEP formalism, Badia & Tuells 1997) and the Constraint Grammar (CG) formalism (Karlsson et al. 1995, Tapanainen 1996). Perl is used both as a glue language and to perform data extraction and detection tasks in the pre-processing and the morphological modules. The pre-processing phase is performed by a text handler that detects dates, abbreviations, entities and figures. It also verticalises the text and decomposes verbclitic combinations. Its output is a verticalised text, with mark-up tags for the above mentioned elements as well as for sentences and paragraphs. The SEGMORF formalism and SWI-Prolog have been used to develop CATMORF, a two-level morphology based analyser for Catalan developed in our group a few years ago (see section 2.1.1 for further details). The CG formalism was used to develop three constraint grammars: one for morphological disambiguation, another one for syntactic mapping and a third one for syntactic disambiguation. Karlsson et al. (1995) states that CG is 'a language-independent formalism for surfaceoriented, morphology-based parsing of unrestricted text. [...] The constraints discard as many alternatives as possible [...] with the proviso that no genuine ambiguities should be obliterated'. Therefore it seemed to us that CG fit best for our purposes. Morphological analysis As for morphology, CATCG employs the tag set proposed in Morel et al. 1997, which follows in great part the EAGLES standards. It amounts to ca. 350 tags, though actually only about 200 are being used. The tag set allows for underspecification both in the main categories (noun, verb, etc.) and in the category features (gender, number, aspect, etc.). It also provides partial subcategorisation information for verbs, once their lemmata have been identified. For further details on the tag set, see Badia et al. 2001 and Morel et al. 1997. 2.1.1. Morphological tag mapping This task is realised by a word form dictionary. The dictionary is generated by CATMORF (the old morphological analysis tool), which has been converted into a form generator. The system is actually 10 times faster than it used to be when we used CATMORF as a run-in-time analyser. Besides, we still profit from the advantages of the old analyser, using it to create and update the word form dictionary. The morphological mapping is not context-sensitive (in contrast to the syntactic mapping; see section 2.2.1), so every word form receives all of its possible readings. CATMORF, written in SWI-Prolog, was the first wide-coverage two-level morphological analyser for Catalan, see Badia et al. 1998. It models morphotactics in a (DCG-like) unification word grammar, and morphographemics in SEGMORF, an extension of the ALEP (Advanced Language Engineering Platform) morphographemic formalism. The lexicon was semiautomatically built out of the DIEC (see DIEC), see Tuells 1998 for details. This MRD was based on a recent general purpose dictionary for Catalan. The information extracted was each headword, its part of speech, and the inflectional paradigm of nouns, adjectives and verbs. Around 68000 lexical entries were automatically added this way, and only around 2800 (800 nouns, 2000 verbs) were added manually. 2.1.2. Morphological tag disambiguation As mentioned above, we have developed a CG-based morphological disambiguation engine for Catalan. DeMCat (Desambiguador Morfologic per al Catala, DeMCat) includes over 1000 rules. The basic strategy is to select or remove certain tags according to the constraints imposed by the surrounding context. Rules have been developed on a trial-and-error basis and the development and test corpora have been collected from the web or, occasionally, manually built. In the rules, there is a TARGET-TAG on which the rule is going to operate. There is also an OPERATOR that indicates whether the target tag is going to be selected or removed. And finally a CONTEXT that specifies the surrounding words and/or tags needed in order for the rule to apply. Context positions are indicated with positive (right of target) or negative (left of target) integers. Zero is the target word (usually a set of words or tags). The CG formalism provides also other devices such as Kleene's star, the possibility to work with relative or absolute positions, and careful modes to control the rule application. It also makes it possible to use heuristic disambiguation by means of weighted rules. For example, this Rule 1 states that the reading Pron(oun) must be removed from words that can be read as Det(erminant) and have a Prep(osition) right at their left side. Of course, using Kleene's stars, careful mode and related contexts can make such rules pretty complex."
