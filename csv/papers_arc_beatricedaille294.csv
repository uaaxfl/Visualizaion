2021.jeptalnrecital-taln.10,Caract{\\'e}risation des relations s{\\'e}mantiques entre termes multi-mots fond{\\'e}e sur l{'}analogie (Semantic relations recognition between multi-word terms by means of analogy ),2021,-1,-1,2,1,5602,yizhe wang,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"La terminologie d{'}un domaine rend compte de la structure du domaine gr{\^a}ce aux relations entre ses termes. Dans cet article, nous nous int{\'e}ressons {\`a} la caract{\'e}risation des relations terminologiques qui existent entre termes multi-mots (MWT) dans les espaces vectoriels distributionnels. Nous avons constitu{\'e} un jeu de donn{\'e}es compos{\'e} de MWT en fran{\c{c}}ais du domaine de l{'}environnement, reli{\'e}s par des relations s{\'e}mantiques lexicales. Nous pr{\'e}sentons une exp{\'e}rience dans laquelle ces relations s{\'e}mantiques entre MWT sont caract{\'e}ris{\'e}es au moyen de l{'}analogie. Les r{\'e}sultats obtenus permettent d{'}envisager un processus automatique pour aider {\`a} la structuration des terminologies."
2020.lrec-1.97,Books of Hours. the First Liturgical Data Set for Text Segmentation.,2020,-1,-1,2,0.262848,16811,amir hazem,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The Book of Hours was the bestseller of the late Middle Ages and Renaissance. It is a historical invaluable treasure, documenting the devotional practices of Christians in the late Middle Ages. Up to now, its textual content has been scarcely studied because of its manuscript nature, its length and its complex content. At first glance, it looks too standardized. However, the study of book of hours raises important challenges: (i) in image analysis, its often lavish ornamentation (illegible painted initials, line-fillers, etc.), abbreviated words, multilingualism are difficult to address in Handwritten Text Recognition (HTR); (ii) its hierarchical entangled structure offers a new field of investigation for text segmentation; (iii) in digital humanities, its textual content gives opportunities for historical analysis. In this paper, we provide the first corpus of books of hours, which consists of Latin transcriptions of 300 books of hours generated by Handwritten Text Recognition (HTR) - that is like Optical Character Recognition (OCR) but for handwritten and not printed texts. We designed a structural scheme of the book of hours and annotated manually two books of hours according to this scheme. Lastly, we performed a systematic evaluation of the main state of the art text segmentation approaches."
2020.computerm-1.7,A study of semantic projection from single word terms to multi-word terms in the environment domain,2020,-1,-1,2,1,5602,yizhe wang,Proceedings of the 6th International Workshop on Computational Terminology,0,"The semantic projection method is often used in terminology structuring to infer semantic relations between terms. Semantic projection relies upon the assumption of semantic compositionality: the relation that links simple term pairs remains valid in pairs of complex terms built from these simple terms. This paper proposes to investigate whether this assumption commonly adopted in natural language processing is actually valid. First, we describe the process of constructing a list of semantically linked multi-word terms (MWTs) related to the environmental field through the extraction of semantic variants. Second, we present our analysis of the results from the semantic projection. We find that contexts play an essential role in defining the relations between MWTs."
2020.computerm-1.9,Towards Automatic Thesaurus Construction and Enrichment.,2020,-1,-1,2,0.262848,16811,amir hazem,Proceedings of the 6th International Workshop on Computational Terminology,0,"Thesaurus construction with minimum human efforts often relies on automatic methods to discover terms and their relations. Hence, the quality of a thesaurus heavily depends on the chosen methodologies for: (i) building its content (terminology extraction task) and (ii) designing its structure (semantic similarity task). The performance of the existing methods on automatic thesaurus construction is still less accurate than the handcrafted ones of which is important to highlight the drawbacks to let new strategies build more accurate thesauri models. In this paper, we will provide a systematic analysis of existing methods for both tasks and discuss their feasibility based on an Italian Cybersecurity corpus. In particular, we will provide a detailed analysis on how the semantic relationships network of a thesaurus can be automatically built, and investigate the ways to enrich the terminological scope of a thesaurus by taking into account the information contained in external domain-oriented semantic sets."
2020.computerm-1.13,{T}erm{E}val 2020: {TALN}-{LS}2{N} System for Automatic Term Extraction,2020,-1,-1,4,0.262848,16811,amir hazem,Proceedings of the 6th International Workshop on Computational Terminology,0,"Automatic terminology extraction is a notoriously difficult task aiming to ease effort demanded to manually identify terms in domain-specific corpora by automatically providing a ranked list of candidate terms. The main ways that addressed this task can be ranged in four main categories: (i) rule-based approaches, (ii) feature-based approaches, (iii) context-based approaches, and (iv) hybrid approaches. For this first TermEval shared task, we explore a feature-based approach, and a deep neural network multitask approach -BERT- that we fine-tune for term extraction. We show that BERT models (RoBERTa for English and CamemBERT for French) outperform other systems for French and English languages."
2020.coling-main.549,Hierarchical Text Segmentation for Medieval Manuscripts,2020,-1,-1,2,0.262848,16811,amir hazem,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we address the segmentation of books of hours, Latin devotional manuscripts of the late Middle Ages, that exhibit challenging issues: a complex hierarchical entangled structure, variable content, noisy transcriptions with no sentence markers, and strong correlations between sections for which topical information is no longer sufficient to draw segmentation boundaries. We show that the main state-of-the-art segmentation methods are either inefficient or inapplicable for books of hours and propose a bottom-up greedy approach that considerably enhances the segmentation results. We stress the importance of such hierarchical segmentation of books of hours for historians to explore their overarching differences underlying conception about Church."
W19-8617,{KPT}imes: A Large-Scale Dataset for Keyphrase Generation on News Documents,2019,0,1,3,0,4246,ygor gallina,Proceedings of the 12th International Conference on Natural Language Generation,0,"Keyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text. Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations. In this paper we present KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset, we show how editors tag documents, and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain. The dataset is available online at https:// github.com/ygorg/KPTimes."
W19-4730,Towards Automatic Variant Analysis of Ancient Devotional Texts,2019,-1,-1,2,0.266636,16811,amir hazem,Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,0,"We address in this paper the issue of text reuse in liturgical manuscripts of the middle ages. More specifically, we study variant readings of the Obsecro Te prayer, part of the devotional Books of Hours often used by Christians as guidance for their daily prayers. We aim at automatically extracting and categorising pairs of words and expressions that exhibit variant relations. For this purpose, we adopt a linguistic classification that allows to better characterize the variants than edit operations. Then, we study the evolution of Obsecro Te texts from a temporal and geographical axis. Finally, we contrast several unsupervised state-of-the-art approaches for the automatic extraction of Obsecro Te variants. Based on the manual observation of 772 Obsecro Te copies which show more than 21,000 variants, we show that the proposed methodology is helpful for an automatic study of variants and may serve as basis to analyze and to depict useful information from devotional texts."
2019.jeptalnrecital-tia.1,Terminology systematization for Cybersecurity domain in {I}talian Language,2019,-1,-1,2,0,27312,claudia lanza,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Terminologie et Intelligence Artificielle (atelier TALN-RECITAL {\\textbackslash}{\\&} IC),0,"This paper aims at presenting the first steps to improve the quality of the first draft of an Italian thesaurus for Cybersecurity terminology that has been realized for a specific project activity in collaboration with CybersecurityLab at Informatics and Telematics Institute (IIT) of the National Council of Research (CNR) in Italy. In particular, the paper will focus, first, on the terminological knowledge base built to retrieve the most representative candidate terms of Cybersecurity domain in Italian language, giving examples of the main gold standard repositories that have been used to build this semantic tool. Attention will be then given to the methodology and software employed to configure a system of NLP rules to get the desired semantic results and to proceed with the enhancement of the candidate terms selection which are meant to be inserted in the controlled vocabulary."
2019.jeptalnrecital-court.28,R{\\'e}utilisation de Textes dans les Manuscrits Anciens (Text Reuse in Ancient Manuscripts),2019,-1,-1,2,0.266636,16811,amir hazem,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Nous nous int{\'e}ressons dans cet article {\`a} la probl{\'e}matique de r{\'e}utilisation de textes dans les livres liturgiques du Moyen {\^A}ge. Plus particuli{\`e}rement, nous {\'e}tudions les variations textuelles de la pri{\`e}re Obsecro Te souvent pr{\'e}sente dans les livres d{'}heures. L{'}observation manuelle de 772 copies de l{'}Obsecro Te a montr{\'e} l{'}existence de plus de 21 000 variantes textuelles. Dans le but de pouvoir les extraire automatiquement et les cat{\'e}goriser, nous proposons dans un premier temps une classification lexico-s{\'e}mantique au niveau n-grammes de mots pour ensuite rendre compte des performances de plusieurs approches {\'e}tat-de-l{'}art d{'}appariement automatique de variantes textuelles de l{'}Obsecro Te."
L18-1045,Word Embedding Approach for Synonym Extraction of Multi-Word Terms,2018,0,1,2,0.266636,16811,amir hazem,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1056,Towards a Diagnosis of Textual Difficulties for Children with Dyslexia,2018,0,0,2,0.952381,17731,solen quiniou,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Children's books are generally designed for children of a certain age group. For underage children or children with reading disorders, like dyslexia, there may be passages of the books that are difficult to understand. This can be due to words not known in the vocabulary of underage children, to words made of complex subparts (to pronounce, for example), or to the presence of anaphoras that have to be resolved by the children during the reading. In this paper, we present a study on diagnosing the difficulties appearing in French children's books. We are more particularly interested on the difficulties coming from pronouns that can disrupt the story comprehension for children with dyslexia and we focus on the subject pronouns  il  and  elle  (corresponding to the pronoun  it ). We automatically identify the pleonastic pronouns (e.g., in  it's raining ) and the pronominal anaphoras, as well as the referents of the pronominal anaphoras. We also detect difficult anaphoras that are more likely to lead to miscomprehension from the children: this is the first step to diagnose the textual difficulties of children's books. We evaluate our approach on several French children's books that were manually annotated by a speech therapist. Our first results show that we are able to detect half of the difficult anaphorical pronouns."
P16-4003,Terminology Extraction with Term Variant Detection,2016,0,5,2,0,33660,damien cram,Proceedings of {ACL}-2016 System Demonstrations,0,None
L16-1190,Evaluating Lexical Similarity to build Sentiment Similarity,2016,4,0,3,0,34912,gregoire jadi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this article, we propose to evaluate the lexical similarity information provided by word representations against several opinion resources using traditional Information Retrieval tools. Word representation have been used to build and to extend opinion resources such as lexicon, and ontology and their performance have been evaluated on sentiment analysis tasks. We question this method by measuring the correlation between the sentiment proximity provided by opinion resources and the semantic similarity provided by word representations using different correlation coefficients. We also compare the neighbors found in word representations and list of similar opinion words. Our results show that the proximity of words in state-of-the-art word representations is not very effective to build sentiment similarity."
L16-1304,{T}erm{ITH}-Eval: a {F}rench Standard-Based Resource for Keyphrase Extraction Evaluation,2016,9,1,5,1,35031,adrien bougouin,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Keyphrase extraction is the task of finding phrases that represent the important content of a document. The main aim of keyphrase extraction is to propose textual units that represent the most important topics developed in a document. The output keyphrases of automatic keyphrase extraction methods for test documents are typically evaluated by comparing them to manually assigned reference keyphrases. Each output keyphrase is considered correct if it matches one of the reference keyphrases. However, the choice of the appropriate textual unit (keyphrase) for a topic is sometimes subjective and evaluating by exact matching underestimates the performance. This paper presents a dataset of evaluation scores assigned to automatically extracted keyphrases by human evaluators. Along with the reference keyphrases, the manual evaluations can be used to validate new evaluation measures. Indeed, an evaluation measure that is highly correlated to the manual evaluation is appropriate for the evaluation of automatic keyphrase extraction methods."
L16-1496,Bilingual Lexicon Extraction at the Morpheme Level Using Distributional Analysis,2016,23,0,2,0.266636,16811,amir hazem,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Bilingual lexicon extraction from comparable corpora is usually based on distributional methods when dealing with single word terms (SWT). These methods often treat SWT as single tokens without considering their compositional property. However, many SWT are compositional (composed of roots and affixes) and this information, if taken into account can be very useful to match translational pairs, especially for infrequent terms where distributional methods often fail. For instance, the English compound \textit{xenograft} which is composed of the root \textit{xeno} and the lexeme \textit{graft} can be translated into French compositionally by aligning each of its elements (\textit{xeno} with \textit{x{\'e}no} and \textit{graft} with \textit{greffe}) resulting in the translation: \textit{x{\'e}nogreffe}. In this paper, we experiment several distributional modellings at the morpheme level that we apply to perform compositional translation to a subset of French and English compounds. We show promising results using distributional analysis at the root and affix levels. We also show that the adapted approach significantly improve bilingual lexicon extraction from comparable corpora compared to the approach at the word level."
L16-1690,Ambiguity Diagnosis for Terms in Digital Humanities,2016,12,1,1,1,5603,beatrice daille,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Among all researches dedicating to terminology and word sense disambiguation, little attention has been devoted to the ambiguity of term occurrences. If a lexical unit is indeed a term of the domain, it is not true, even in a specialised corpus, that all its occurrences are terminological. Some occurrences are terminological and other are not. Thus, a global decision at the corpus level about the terminological status of all occurrences of a lexical unit would then be erroneous. In this paper, we propose three original methods to characterise the ambiguity of term occurrences in the domain of social sciences for French. These methods differently model the context of the term occurrences: one is relying on text mining, the second is based on textometry, and the last one focuses on text genre properties. The experimental results show the potential of the proposed approaches and give an opportunity to discuss about their hybridisation."
C16-1277,Keyphrase Annotation with Graph Co-Ranking,2016,15,0,3,1,35031,adrien bougouin,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Keyphrase annotation is the task of identifying textual units that represent the main content of a document. Keyphrase annotation is either carried out by extracting the most important phrases from a document, keyphrase extraction, or by assigning entries from a controlled domain-specific vocabulary, keyphrase assignment. Assignment methods are generally more reliable. They provide better-formed keyphrases, as well as keyphrases that do not occur in the document. But they are often silent on the contrary of extraction methods that do not depend on manually built resources. This paper proposes a new method to perform both keyphrase extraction and keyphrase assignment in an integrated and mutual reinforcing manner. Experiments have been carried out on datasets covering different domains of humanities and social sciences. They show statistically significant improvements compared to both keyphrase extraction and keyphrase assignment state-of-the art methods."
2016.jeptalnrecital-poster.17,Extraction d{'}expressions-cibles de l{'}opinion : de l{'}anglais au fran{\\c{c}}ais (Opinion Target Expression extraction : from {E}nglish to {F}rench),2016,-1,-1,4,0,34912,gregoire jadi,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"Dans cet article, nous pr{\'e}sentons le d{\'e}veloppement d{'}un syst{\`e}me d{'}extraction d{'}expressions-cibles pour l{'}anglais et sa transposition au fran{\c{c}}ais. En compl{\'e}ment, nous avons r{\'e}alis{\'e} une {\'e}tude de l{'}efficacit{\'e} des traits en anglais et en fran{\c{c}}ais qui tend {\`a} montrer qu{'}il est possible de r{\'e}aliser un syst{\`e}me d{'}extraction d{'}expressions-cibles ind{\'e}pendant du domaine. Pour finir, nous proposons une analyse comparative des erreurs commises par nos syst{\`e}mes en anglais et fran{\c{c}}ais et envisageons diff{\'e}rentes solutions {\`a} ces probl{\`e}mes."
2016.jeptalnrecital-poster.28,Segmentation automatique d{'}un texte en rh{\\`e}ses (Automatic segmentation of a text into rhesis),2016,-1,-1,4,0,35940,victor pineau,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"La segmentation d{'}un texte en rh{\`e}ses, unit{\'e}s-membres signifiantes de la phrase, permet de fournir des adaptations de celui-ci pour faciliter la lecture aux personnes dyslexiques. Dans cet article, nous proposons une m{\'e}thode d{'}identification automatique des rh{\`e}ses bas{\'e}e sur un apprentissage supervis{\'e} {\`a} partir d{'}un corpus que nous avons annot{\'e}. Nous comparons celle-ci {\`a} l{'}identification manuelle ainsi qu{'}{\`a} l{'}utilisation d{'}outils et de concepts proches, tels que la segmentation d{'}un texte en chunks."
2016.jeptalnrecital-long.14,Extraction de lexiques bilingues {\\`a} partir de corpus comparables sp{\\'e}cialis{\\'e}s {\\`a} travers une langue pivot (Bilingual lexicon extraction from specialized comparable corpora using a pivot language),2016,-1,-1,3,0,35947,alexis linard,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"L{'}extraction de lexiques bilingues {\`a} partir de corpus comparables se r{\'e}alise traditionnellement en s{'}appuyant sur deux langues. Des travaux pr{\'e}c{\'e}dents en extraction de lexiques bilingues {\`a} partir de corpus parall{\`e}les ont d{\'e}montr{\'e} que l{'}utilisation de plus de deux langues peut {\^e}tre utile pour am{\'e}liorer la qualit{\'e} des alignements extraits. Nos travaux montrent qu{'}il est possible d{'}utiliser la m{\^e}me strat{\'e}gie pour des corpus comparables. Nous avons d{\'e}fini deux m{\'e}thodes originales impliquant des langues pivots et nous les avons {\'e}valu{\'e}es sur quatre langues et deux langues pivots en particulier. Nos exp{\'e}rimentations ont montr{\'e} que lorsque l{'}alignement entre la langue source et la langue pivot est de bonne qualit{\'e}, l{'}extraction du lexique en langue cible s{'}en trouve am{\'e}lior{\'e}e."
2016.jeptalnrecital-long.18,Mod{\\'e}lisation unifi{\\'e}e du document et de son domaine pour une indexation par termes-cl{\\'e}s libre et contr{\\^o}l{\\'e}e (Unified document and domain-specific model for keyphrase extraction and assignment ),2016,-1,-1,3,1,35031,adrien bougouin,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"Dans cet article, nous nous int{\'e}ressons {\`a} l{'}indexation de documents de domaines de sp{\'e}cialit{\'e} par l{'}interm{\'e}diaire de leurs termes-cl{\'e}s. Plus particuli{\`e}rement, nous nous int{\'e}ressons {\`a} l{'}indexation telle qu{'}elle est r{\'e}alis{\'e}e par les documentalistes de biblioth{\`e}ques num{\'e}riques. Apr{\`e}s analyse de la m{\'e}thodologie de ces indexeurs professionnels, nous proposons une m{\'e}thode {\`a} base de graphe combinant les informations pr{\'e}sentes dans le document et la connaissance du domaine pour r{\'e}aliser une indexation (hybride) libre et contr{\^o}l{\'e}e. Notre m{\'e}thode permet de proposer des termes-cl{\'e}s ne se trouvant pas n{\'e}cessairement dans le document. Nos exp{\'e}riences montrent aussi que notre m{\'e}thode surpasse significativement l{'}approche {\`a} base de graphe {\'e}tat de l{'}art."
W15-3405,Attempting to Bypass Alignment from Comparable Corpora via Pivot Language,2015,29,3,2,0,35947,alexis linard,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"Alignment enhanced. from comparable corpora usually involves two languages, one source and one target language. Previous works on bilingual lexicon extraction from parallel corpora demonstrated that more than two languages can be useful to improve the alignments. Our works have investigated to which extent a third language could be interesting to bypass the original alignment. We have defined two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been"
2015.jeptalnrecital-court.17,Extraction de Contextes Riches en Connaissances en corpus sp{\\'e}cialis{\\'e}s,2015,-1,-1,3,0,27613,firas hmida,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Les banques terminologiques et les dictionnaires sont des ressources pr{\'e}cieuses qui facilitent l{'}acc{\`e}s aux connaissances des domaines sp{\'e}cialis{\'e}s. Ces ressources sont souvent assez pauvres et ne proposent pas toujours pour un terme {\`a} illustrer des exemples permettant d{'}appr{\'e}hender le sens et l{'}usage de ce terme. Dans ce contexte, nous proposons de mettre en {\oe}uvre la notion de Contextes Riches en Connaissances (CRC) pour extraire directement de corpus sp{\'e}cialis{\'e}s des exemples de contextes illustrant son usage. Nous d{\'e}finissons un cadre unifi{\'e} pour exploiter tout {\`a} la fois des patrons de connaissances et des collocations avec une qualit{\'e} acceptable pour une r{\'e}vision humaine."
2015.jeptalnrecital-court.20,"Vers un diagnostic d{'}ambigu{\\\\\i}t{\\'e} des termes candidats d{'}un texte""",2015,14,0,2,0,587,gael lejeune,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Les recherches autour de la d{\'e}sambigu{\""\i}sation s{\'e}mantique traitent de la question du sens {\`a} accorder {\`a} diff{\'e}rentes occurrences d{'}un mot ou plus largement d{'}une unit{\'e} lexicale. Dans cet article, nous nous int{\'e}ressons {\`a} l{'}ambigu{\""\i}t{\'e} d{'}un terme en domaine de sp{\'e}cialit{\'e}. Nous posons les premiers jalons de nos recherches sur une question connexe que nous nommons le diagnostic d{'}ambigu{\""\i}t{\'e}. Cette t{\^a}che consiste {\`a} d{\'e}cider si une occurrence d{'}un terme est ou n{'}est pas ambigu{\""e}. Nous mettons en {\oe}uvre une approche d{'}apprentissage supervis{\'e}e qui exploite un corpus d{'}articles de sciences humaines r{\'e}dig{\'e}s en fran{\c{c}}ais dans lequel les termes ambigus ont {\'e}t{\'e} d{\'e}tect{\'e}s par des experts. Le diagnostic s{'}appuie sur deux types de traits : syntaxiques et positionnels. Nous montrons l{'}int{\'e}r{\^e}t de la structuration du texte pour {\'e}tablir le diagnostic d{'}ambigu{\""\i}t{\'e}."
W14-5702,Splitting of Compound Terms in non-Prototypical Compounding Languages,2014,12,2,2,0,35009,elizaveta clouet,Proceedings of the First Workshop on Computational Approaches to Compound Analysis ({C}om{AC}om{A} 2014),0,"Compounding is present in a large variety of languages in different proportions. Compound rate in the text obviously depends on the language, but also on the genre and the domain. Scientific and technical texts are especially conducive to compounding, even in the languages that are not traditionally admitted as highly compounding ones. In this article we address compound splitting of specialized terms. We propose a multi-lingual method of compound recognition and splitting, which uses corpus frequencies, lexical data and optionally linguistic rules. This is a supervised method which requires a small amount of segmented compounds as input. We evaluate the method on two languages that rarely serve as a material for automatic splitting systems: English and Russian. The results obtained are competitive with those of a state-of-the-art corpus-driven approach."
daille-hazem-2014-semi,Semi-compositional Method for Synonym Extraction of Multi-Word Terms,2014,24,2,1,1,5603,beatrice daille,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Automatic synonyms and semantically related word extraction is a challenging task, useful in many NLP applications such as question answering, search query expansion, text summarization, etc. While different studies addressed the task of word synonym extraction, only a few investigations tackled the problem of acquiring synonyms of multi-word terms (MWT) from specialized corpora. To extract pairs of synonyms of multi-word terms, we propose in this paper an unsupervised semi-compositional method that makes use of distributional semantics and exploit the compositional property shared by most MWT. We show that our method outperforms significantly the state-of-the-art."
F14-1002,The impact of domains for Keyphrase extraction (Influence des domaines de sp{\\'e}cialit{\\'e} dans l{'}extraction de termes-cl{\\'e}s) [in {F}rench],2014,-1,-1,3,1,35031,adrien bougouin,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
I13-1046,Ranking Translation Candidates Acquired from Comparable Corpora,2013,14,3,2,0,41661,rima harastani,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Domain-specific bilingual lexicons extracted from domain-specific comparable corpora provide for one term a list of ranked translation candidates. This study proposes to re-rank these translation candidates. We suggest that a term and its translation appear in comparable sentences that can be extracted from domainspecific comparable corpora. For a source term and a list of translation candidates, we propose a method to identify and align the best source and target sentences that contain the term and its translation candidates. We report results with two language pairs (French-English and FrenchGerman) using domain-specific comparable corpora. Our method significantly improves the top 1, top 5 and top 10 precisions of a domain-specific bilingual lexicon, and thus, provides a better user-"
I13-1062,{T}opic{R}ank: Graph-Based Topic Ranking for Keyphrase Extraction,2013,25,89,3,1,35031,adrien bougouin,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Keyphrase extraction is the task of identifying single or multi-word expressions that represent the main topics of a document. In this paper we present TopicRank, a graph-based keyphrase extraction method that relies on a topical representation of the document. Candidate keyphrases are clustered into topics and used as vertices in a complete graph. A graph-based ranking model is applied to assign a significance score to each topic. Keyphrases are then generated by selecting a candidate from each of the topranked topics. We conducted experiments on four evaluation datasets of different languages and domains. Results show that TopicRank significantly outperforms state-of-the-art methods on three datasets."
F13-3011,Apopsis Demonstrator for Tweet Analysis (D{\\'e}monstrateur Apopsis pour l{'}analyse des tweets) [in {F}rench],2013,0,0,3,0,30952,sebastian saldarriaga,Proceedings of TALN 2013 (Volume 3: System Demonstrations),0,None
F13-3013,{TTC} {T}erm{S}uite - Terminological Alignment from Comparable Corpora ({TTC} {T}erm{S}uite alignement terminologique {\\`a} partir de corpus comparables) [in {F}rench],2013,-1,-1,1,1,5603,beatrice daille,Proceedings of TALN 2013 (Volume 3: System Demonstrations),0,None
F13-2008,Multilingual Compound Splitting (Segmentation Multilingue des Mots Compos{\\'e}s) [in {F}rench],2013,0,0,2,0,36788,elizaveta loginovaclouet,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
F13-1023,"Identification, Alignment, and Tranlsation of Relational Adjectives from Comparable Corpora (Identification, alignement, et traductions des adjectifs relationnels en corpus comparables) [in {F}rench]",2013,0,0,2,0,41661,rima harastani,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
F12-2011,Compositionnalit{\\'e} et contextes issus de corpus comparables pour la traduction terminologique (Compositionality and Context for Bilingual Lexicon Extraction from Comparable Corpora) [in {F}rench],2012,0,0,2,0.46866,17779,emmanuel morin,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
C12-1046,Extraction of Domain-Specific Bilingual Lexicon from Comparable Corpora: Compositional Translation and Ranking,2012,28,14,2,0,30007,estelle delpech,Proceedings of {COLING} 2012,0,"This paper proposes a method for extracting translations of morphologically constructed terms from comparable corpora. The method is based on compositional translation and exploits translation equivalences at the morpheme-level, which allows for the generation of fertile translations (translation pairs in which the target term has more words than the source term). Ranking methods relying on corpus-based and translation-based features are used to select the best candidate translation. We obtain an average precision of 91% on the Top1 candidate translation. The method was tested on two language pairs (English-French and English-German) and with a small specialized comparable corpora (400k words per language)."
C12-1110,Revising the Compositional Method for Terminology Acquisition from Comparable Corpora,2012,27,8,2,0.46866,17779,emmanuel morin,Proceedings of {COLING} 2012,0,"In this paper, we present a new method that improves the alignment of equivalent terms monolingually acquired from bilingual comparable corpora: the Compositional Method with Context-Based Projection (CMCBP). Our overall objective is to identify and to translate high specialized terminology made up of multi-word terms acquired from comparable corpora. Our evaluation in the medical domain and for two pairs of languages demonstrates that CMCBP outperforms the state-of-art compositional approach commonly used for translationally equivalent multi-word term discovery from comparable corpora."
2012.amta-papers.5,Identification of Fertile Translations in Comparable Corpora: A Morpho-Compositional Approach,2012,20,2,2,0,30007,estelle delpech,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,This paper defines a method for lexicon in the biomedical domain from comparable corpora. The method is based on compositional translation and exploits morpheme-level translation equivalences. It can generate translations for a large variety of morphologically constructed words and can also generate {'}fertile{'} translations. We show that fertile translations increase the overall quality of the extracted lexicon for English to French translation.
I11-2003,{TTC} {T}erm{S}uite - A {UIMA} Application for Multilingual Terminology Extraction from Comparable Corpora,2011,6,13,2,0,44752,jerome rocheteau,Proceedings of the {IJCNLP} 2011 System Demonstrations,0,"This paper aims at presenting TTC TermSuite: a tool suite for multilingual terminology extraction from comparable corpora. This tool suite offers a userfriendly graphical interface for designing UIMA-based tool chains whose components (i) form a functional architecture, (ii) manage 7 languages of 5 different families, (iii) support standardized file formats, (iv) extract singleand multiword terms languages by languages (v) and align them by pairs of languages."
I11-1173,Reduction of Search Space to Annotate Monolingual Corpora,2011,18,2,3,0,42485,prajol shrestha,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Monolingual corpora which are aligned with similar text segments (paragraphs, sentences, etc.) are used to build and test a wide range of natural language processing applications. The drawback wanting to use them is the lack of publicly available annotated corpora which obligates people to make one themselves. The annotation process is a time consuming and costly task. This paper describes a new corpus-based measure to significantly reduce the search space for a faster and easier manual annotation process for monolingual corpora. This measure can be used in making alignments on different types of text segments. The performance of this measure is evaluated on a manually annotated paragraph corpus, whose alignments are freely available, with promising results."
2011.jeptalnrecital-long.21,Identifier la cible d{'}un passage d{'}opinion dans un corpus multith{\\'e}matique (Identifying the target of an opinion transition in a thematic corpus),2011,-1,-1,3,1,44924,matthieu vernier,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}identification de la cible d{'}une d{'}opinion fait l{'}objet d{'}une attention r{\'e}cente en fouille d{'}opinion. Les m{\'e}thodes existantes ont {\'e}t{\'e} test{\'e}es sur des corpus monoth{\'e}matiques en anglais. Elles permettent principalement de traiter les cas o{\`u} la cible se situe dans la m{\^e}me phrase que l{'}opinion. Dans cet article, nous abordons cette probl{\'e}matique pour le fran{\c{c}}ais dans un corpus multith{\'e}matique et nous pr{\'e}sentons une nouvelle m{\'e}thode pour identifier la cible d{'}une opinion apparaissant hors du contexte phrastique. L{'}{\'e}valuation de la m{\'e}thode montre une am{\'e}lioration des r{\'e}sultats par rapport {\`a} l{'}existant."
2011.jeptalnrecital-demonstration.6,{TTC} {T}erm{S}uite : une cha{\\^\\i}ne de traitement pour la fouille terminologique multilingue ({TTC} {T}erm{S}uite: a processing chain for multilingual terminology mining),2011,-1,-1,1,1,5603,beatrice daille,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,
S10-1038,{UNPMC}: Naive Approach to Extract Keyphrases from Scientific Articles,2010,7,1,3,0,24158,jungyeul park,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"We describe our method for extracting keyphrases from scientific articles which we participate in the shared task of SemEval-2 Evaluation Exercise. Even though general-purpose term extractors along with linguistically-motivated analysis allow us to extract elaborated morphosyntactic variation forms of terms, a naive statistic approach proposed in this paper is very simple and quite efficient for extracting keyphrases especially from well-structured scientific articles. Based on the characteristics of keyphrases with section information, we obtain 18.34% for f-measure using top 15 candidates. We also show further improvement without any complications and we discuss this at the end of the paper."
vernier-etal-2010-learning,Learning Subjectivity Phrases missing from Resources through a Large Set of Semantic Tests,2010,23,0,3,1,44924,matthieu vernier,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In recent years, blogs and social networks have particularly boosted interests for opinion mining research. In order to satisfy real-scale applicative needs, a main task is to create or to enhance lexical and semantic resources on evaluative language. Classical resources of the area are mostly built for english, they contain simple opinion word markers and are far to cover the lexical richness of this linguistic phenomenon. In particular, infrequent subjective words, idiomatic expressions, and cultural stereotypes are missing from resources. We propose a new method, applied on french, to enhance automatically an opinion word lexicon. This learning method relies on linguistic uses of internet users and on semantic tests to infer the degree of subjectivity of many new adjectives, nouns, verbs, noun phrases, verbal phrases which are usually forgotten by other resources. The final appraisal lexicon contains 3,456 entries. We evaluate the lexicon enhancement with and without textual context."
W09-3110,Compilation of Specialized Comparable Corpora in {F}rench and {J}apanese,2009,27,9,3,1,5695,lorraine goeuriot,Proceedings of the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora ({BUCC}),0,"We present in this paper the development of a specialized comparable corpora compilation tool, for which quality would be close to a manually compiled corpus. The comparability is based on three levels: domain, topic and type of discourse. Domain and topic can be filtered with the keywords used through web search. But the detection of the type of discourse needs a wide linguistic analysis. The first step of our work is to automate the detection of the type of discourse that can be found in a scientific domain (science and popular science) in French and Japanese languages. First, a contrastive stylistic analysis of the two types of discourse is done on both languages. This analysis leads to the creation of a reusable, generic and robust typology. Machine learning algorithms are then applied to the typology, using shallow parsing. We obtain good results, with an average precision of 80% and an average recall of 70% that demonstrate the efficiency of this typology. This classification tool is then inserted in a corpus compilation tool which is a text collection treatment chain realized through IBM UIMA system. Starting from two specialized web documents collection in French and Japanese, this tool creates the corresponding corpus."
2009.jeptalnrecital-court.10,Cat{\\'e}gorisation s{\\'e}mantico-discursive des {\\'e}valuations exprim{\\'e}es dans la blogosph{\\`e}re,2009,-1,-1,3,1,44924,matthieu vernier,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Les blogs constituent un support d{'}observations id{\'e}al pour des applications li{\'e}es {\`a} la fouille d{'}opinion. Toutefois, ils imposent de nouvelles probl{\'e}matiques et de nouveaux d{\'e}fis au regard des m{\'e}thodes traditionnelles du domaine. De ce fait, nous proposons une m{\'e}thode automatique pour la d{\'e}tection et la cat{\'e}gorisation des {\'e}valuations localement exprim{\'e}es dans un corpus de blogs multi-domaine. Celle-ci rend compte des sp{\'e}cificit{\'e}s du langage {\'e}valuatif d{\'e}crites dans deux th{\'e}ories linguistiques. L{'}outil d{\'e}velopp{\'e} au sein de la plateforme UIMA vise d{'}une part {\`a} construire automatiquement une grammaire du langage {\'e}valuatif, et d{'}autre part {\`a} utiliser cette grammaire pour la d{\'e}tection et la cat{\'e}gorisation des passages {\'e}valuatifs d{'}un texte. La cat{\'e}gorisation traite en particulier l{'}aspect axiologique de l{'}{\'e}valuation, sa configuration d{'}{\'e}nonciation et sa modalit{\'e} dans le discours."
boulaknadel-etal-2008-multi,A Multi-Word Term Extraction Program for {A}rabic Language,2008,15,35,2,0,39465,siham boulaknadel,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Terminology extraction commonly includes two steps: identification of term-like units in the texts, mostly multi-word phrases, and the ranking of the extracted term-like units according to their domain representativity. In this paper, we design a multi-word term extraction program for Arabic language. The linguistic filtering performs a morphosyntactic analysis and takes into account several types of variations. The domain representativity is measure thanks to statistical scores. We evalutate several association measures and show that the results we otained are consitent with those obtained for Romance languages."
goeuriot-etal-2008-characterization,"Characterization of Scientific and Popular Science Discourse in {F}rench, {J}apanese and {R}ussian",2008,1,2,3,1,5695,lorraine goeuriot,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We aim to characterize the comparability of corpora, we address this issue in the trilingual context through the distinction of expert and non expert documents. We work separately with corpora composed of documents from the medical domain in three languages (French, Japanese and Russian) which present an important linguistic distance between them. In our approach, documents are characterized in each language by their topic and by a discursive typology positioned at three levels of document analysis: structural, modal and lexical. The document typology is implemented with two learning algorithms (SVMlight and C4.5). Evaluation of results shows that the proposed discursive typology can be transposed from one language to another, as it indeed allows to distinguish the two aimed discourses (science and popular science). However, we observe that performances vary a lot according to languages, algorithms and types of discursive characteristics."
I08-1013,An Effective Compositional Model for Lexical Alignment,2008,22,8,1,1,5603,beatrice daille,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"The automatic compilation of bilingual dictionaries from comparable corpora has been successful for single-word terms (SWTs), but remains disappointing for multi-word terms (MWTs). One of the main problems is the insufficient coverage of the bilingual dictionary. Using the compositional translation method improved the results, but still shows some limits for MWTs of different syntactic structures. In this paper, we propose to bridge the gap between syntactic structures through morphological links. The results show a significant improvement in the compositional translation of MWTs that demonstrate the efficiency of the morphologically based-method for lexical alignment."
P07-1084,"Bilingual Terminology Mining - Using Brain, not brawn comparable corpora",2007,13,84,2,1,17779,emmanuel morin,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Current research in text mining favours the quantity of texts over their quality. But for bilingual terminology mining, and for many language pairs, large comparable corpora are not available. More importantly, as terms are defined vis-a-vis a specific domain with a restricted register, it is expected that the quality rather than the quantity of the corpus matters more in terminology mining. Our hypothesis, therefore, is that the quality of the corpus is more important than the quantity and ensures the quality of the acquired terminological resources. We show how important the type of discourse is as a characteristic of the comparable corpus."
2007.jeptalnrecital-poster.9,"Caract{\\'e}risation des discours scientifiques et vulgaris{\\'e}s en fran{\\c{c}}ais, japonais et russe",2007,-1,-1,3,1,5695,lorraine goeuriot,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"L{'}objectif principal de notre travail consiste {\`a} {\'e}tudier la notion de comparabilit{\'e} des corpus, et nous abordons cette question dans un contexte monolingue en cherchant {\`a} distinguer les documents scientifiques et vulgaris{\'e}s. Nous travaillons s{\'e}par{\'e}ment sur des corpus compos{\'e}s de documents du domaine m{\'e}dical dans trois langues {\`a} forte distance linguistique (le fran{\c{c}}ais, le japonais et le russe). Dans notre approche, les documents sont caract{\'e}ris{\'e}s dans chaque langue selon leur th{\'e}matique et une typologie discursive qui se situe {\`a} trois niveaux de l{'}analyse des documents : structurel, modal et lexical. Le typage des documents est impl{\'e}ment{\'e} avec deux algorithmes d{'}apprentissage (SVMlight et C4.5). L{'}{\'e}valuation des r{\'e}sultats montre que la typologie discursive propos{\'e}e est portable d{'}une langue {\`a} l{'}autre car elle permet en effet de distinguer les deux discours. Nous constatons n{\'e}anmoins des performances tr{\`e}s vari{\'e}es selon les langues, les algorithmes et les types de caract{\'e}ristiques discursives."
I05-1062,{F}rench-{E}nglish Terminology Extraction from Comparable Corpora,2005,17,44,1,1,5603,beatrice daille,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This article presents a method of extracting bilingual lexica composed of single-word terms (SWTs) and multi-word terms (MWTs) from comparable corpora of a technical domain. First, this method extracts MWTs in each language, and then uses statistical methods to align single words and MWTs by exploiting the term contexts. After explaining the difficulties involved in aligning MWTs and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its significance, particularly in relation to non-compositional MWT alignment."
W04-1814,Construction of Grammar Based Term Extraction Model for {J}apanese,2004,0,0,3,0,17433,koichi takeuchi,Proceedings of {C}ompu{T}erm 2004: 3rd International Workshop on Computational Terminology,0,None
daille-etal-2004-french,{F}rench-{E}nglish Multi-word Term Alignment Based on Lexical Context Analysis,2004,10,5,1,1,5603,beatrice daille,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This article presents a method of extracting bilingual lexica composed of single-word terms (SWTs) and multi-word terms (MWTs) from comparable corpora of a technical domain. First, this method extracts MWTs in each language, and then uses statistical methods to align single words and MWTs by exploiting the term contexts. After explaining the difficulties involved in aligning MWTs and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its significance, particularly in relation to non-compositional MWT alignment."
2004.jeptalnrecital-long.13,Extraction de terminologies bilingues {\\`a} partir de corpus comparables,2004,-1,-1,3,1,17779,emmanuel morin,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente une m{\'e}thode pour extraire, {\`a} partir de corpus comparables d{'}un domaine de sp{\'e}cialit{\'e}, un lexique bilingue comportant des termes simples et complexes. Cette m{\'e}thode extrait d{'}abord les termes complexes dans chaque langue, puis les aligne {\`a} l{'}aide de m{\'e}thodes statistiques exploitant le contexte des termes. Apr{\`e}s avoir rappel{\'e} les difficult{\'e}s que pose l{'}alignement des termes complexes et pr{\'e}cis{\'e} notre approche, nous pr{\'e}sentons le processus d{'}extraction de terminologies bilingues adopt{\'e} et les ressources utilis{\'e}es pour nos exp{\'e}rimentations. Enfin, nous {\'e}valuons notre approche et d{\'e}montrons son int{\'e}r{\^e}t en particulier pour l{'}alignement de termes complexes non compositionnels."
W03-1802,Conceptual Structuring through Term Variations,2003,12,59,1,1,5603,beatrice daille,"Proceedings of the {ACL} 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment",0,"Term extraction systems are now an integral part of the compiling of specialized dictionaries and updating of term banks. In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French. Conceptual relationships are deduced from specific types of term variations, morphological and syntagmatic, and are expressed through lexical functions. The linguistic precision of the conceptual structuring through morphological variations is of 95%."
tsuji-etal-2002-extracting,Extracting {F}rench-{J}apanese Word Pairs from Bilingual Corpora based on Transliteration Rules,2002,3,12,2,0,46035,keita tsuji,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"It has been shown so far that using transliteration rules to extract Japanese Katakana and English word pairs is highly useful and promising. But for Japanese-French pairs, the method is not guaranteed to work, because only a very few Japanese Katakana words are borrowed directly from French. In this paper we will show the possibility of extracting Japanese Katakana and French word pairs based on transliteration from loosely aligned Japanese French bilingual corpora. The method applies all the existing transliteration rules to each mora unit in a Katakana word, and extracts the French word which matches or partially-matches one of these transliteration candidates as translation. For instance, if we have `xe3x82xb0xe3x83xa9xe3x83x95' in the Japanese part of a bilingual corpora, we generate such transliteration candidates as , , ,... and identify similar words from French part of the corpora. The method performed reasonably well, achieving 80% precision at 20% recall. We had also observed that Japanese-English transliteration rules worked well for extracting Katakana-French word pairs."
fourour-etal-2002-incremental,Incremental Recognition and Referential Categorization of {F}rench Proper Names,2002,8,2,3,0,53521,nordine fourour,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents Nemesis, a French proper name (PN) recognizer for Large-scale Information Extraction (IE), whose specifications have been elaborated through corpus investigation both in terms of referential categories and graphical structures. The graphical criteria are used to identify proper names and the referential classification to categorize them. The system is a classical one: it is rule-based and uses specialized lexicons without any linguistic preprocessing. Its originality consists on a modular architecture which includes a learning process. The system up to now recognizes anthroponyms and toponyms with performance achieving 95 % of precision and 90 % of recall."
2001.jeptalnrecital-tutoriel.1,Extraction de collocations {\\`a} partir de textes,2001,-1,-1,1,1,5603,beatrice daille,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Tutoriels,0,"Les collocations sont int{\'e}ressantes dans de nombreuses applications du TALN comme la l{'}analyse ou la g{\'e}n{\'e}ration de textes ou encore la lexicographie monolingue ou bilingue. Les premi{\`e}res tentatives d{'}extraction automatique de collocations {\`a} partir de textes ou de dictionnaires ont vu le jour dans les ann{\'e}es 1970. Il s{'}agissait principalement de m{\'e}thodes {\`a} base de statistiques lexicales. Aujourd{'}hui, les m{\'e}thodes d{'}identification automatique font toujours appel {\`a} des statistiques mais qu{'}elles combinent avec des analyses linguistiques. Nous examinons quelques m{\'e}thodes d{'}identification des collocations en corpus en soulignant pour chaque m{\'e}thode les propri{\'e}t{\'e}s linguistiques des collocations qui ont {\'e}t{\'e} prises en compte."
C00-1032,Morphological Rule Induction for Terminology Acquisition,2000,13,19,1,1,5603,beatrice daille,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"We present the identification in corpora of French relational adjectives (RAdj) such as gazeux (gascous) which is derived from the noun gaz (gas). RAdj appearing in nominal phrases are interesting for terminology acquisition because they hold a naming function. The derivational rules employed to compute the noun from which has been derived the RAdj are acquired semi-automatically from a tagged and a lemmatized corpora. These rules are then integrated into a termer which identifies RAdj thanks to their property of being paraphrasable by a prepositional phrase. RAdj and compound nouns which include a RAdj are then quantified, their linguistic precision is measured and their informative status is evaluated thanks to a thesaurus of the domain."
W94-0104,Study and Implementation of Combined Techniques for Automatic Extraction of Terminology,1994,-1,-1,1,1,5603,beatrice daille,The Balancing Act: Combining Symbolic and Statistical Approaches to Language,0,None
C94-1084,Towards Automatic Extraction of Monolingual and Bilingual Terminology,1994,8,157,1,1,5603,beatrice daille,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, we make use of linguistic knowledge to identify certain noun phrases, both in English and French, which are likely to be terms. We then test and compare different statistical scores to select the good ones among the candidate terms, and finally propose a statistical method to build correspondences of multi-words units across languages."
