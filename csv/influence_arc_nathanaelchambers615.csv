2020.acl-main.426,N19-1423,0,0.0159097,"T Label Embeddings LEAM w/ GloVe LEAM w/ BERT Features BERT + Labels as Input Label Correlation Learned Correlations Semi-supervision Precision Recall F1 25.31 24.47 25.30 24.33 59.66 65.63 33.44 38.87 37.30 40.10 51.33 56.91 28.81 30.04 30.15 30.29 55.18 60.96 59.81 67.29 63.05 54.46 54.48 61.70 57.03 60.22 62.36 56.50 57.94 71.47 76.35 63.11 65.88 Table 1: Comparison Results on ROCStories with Plutchik emotion labels 5 Experimental Setup We compare our proposed models with the models presented in Rashkin et al. (2018), the LEAM architecture of Wang et al. (2018), and fine-tuned BERT models (Devlin et al., 2019) for multi-label classification without label semantics. For all the models we report the micro-averaged Precision, Recall and F1 score of the emotion prediction task. Rashkin et al. (2018) modeled character context and pre-trained on free response data to predict the mental states of characters using different encoderdecoder setups, including BiLSTMs, CNNs, the recurrent entity network (REN) (Henaff et al., 2016), and neural process networks (NPN) (Bosselut et al., 2017). Additionally, we compare with the selfattention architecture proposed in (Paul and Frank, 2019), without the knowledge fro"
2020.acl-main.426,N16-1063,0,0.0600595,"Missing"
2020.acl-main.426,N16-1098,1,0.792867,"a label embeddings, and add mechanisms that track label-label correlations both during training and inference. We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data. Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-theart on an emotion inference task. 1 Introduction Understanding how events in a story affect the characters involved is an integral part of narrative understanding. Rashkin et al. (2018) introduced an emotion inference task on a subset of the ROCStories dataset (Mostafazadeh et al., 2016), labeling entities with the emotions they experience from the short story contexts. Previous work on this and related tasks typically frame them as multi-label classification problems. The standard approach uses an encoder that produces a representation of the target event along with the surrounding story events, and then pushes it through a classification layer to predict the possible emotion labels (Rashkin et al., 2018; Wang et al., 2018). This classification framework ignores the semantics of the emotions themselves. Each emotion label (e.g., joy) is just a binary prediction. However, con"
2020.acl-main.426,speer-havasi-2012-representing,0,0.0227841,"el classification without label semantics. For all the models we report the micro-averaged Precision, Recall and F1 score of the emotion prediction task. Rashkin et al. (2018) modeled character context and pre-trained on free response data to predict the mental states of characters using different encoderdecoder setups, including BiLSTMs, CNNs, the recurrent entity network (REN) (Henaff et al., 2016), and neural process networks (NPN) (Bosselut et al., 2017). Additionally, we compare with the selfattention architecture proposed in (Paul and Frank, 2019), without the knowledge from ConceptNet (Speer and Havasi, 2012) and ELMo embeddings (Peters et al., 2018). To compare against LEAM, we compare it against our proposal of the LEAM+BERT model, where our label attention is computed from BERT representations of each of the label sentences, and words in the input sentence. We also encode the sentence and context separately in a BiLSTM layer as done in Rashkin et al. (2018). We also fine-tuned a BERT-base-uncased model for emotion classification, using xs , xc and Ls as inputs. This beats the other baselines by a significant margin, and is thus a strong new baseline. All our models are evaluated on the emotion"
2020.acl-main.426,P18-1216,0,0.377624,"is an integral part of narrative understanding. Rashkin et al. (2018) introduced an emotion inference task on a subset of the ROCStories dataset (Mostafazadeh et al., 2016), labeling entities with the emotions they experience from the short story contexts. Previous work on this and related tasks typically frame them as multi-label classification problems. The standard approach uses an encoder that produces a representation of the target event along with the surrounding story events, and then pushes it through a classification layer to predict the possible emotion labels (Rashkin et al., 2018; Wang et al., 2018). This classification framework ignores the semantics of the emotions themselves. Each emotion label (e.g., joy) is just a binary prediction. However, consider the sentence, “Danielle was really In this work, we show that explicitly modeling label semantics improves emotion inference. We describe three main contributions1 . First, we show how to use embeddings as the label semantics representation. We then propose a label attention network that produces label-informed representations of the event and the story context to improve prediction accuracy. Second, we add mechanisms that can make use"
2020.acl-main.426,N19-1296,0,0.141105,"etween the emotion labels in the ground truth. For instance, there is a high negative correlation (ρ = −0.9) between JOY and SAD labels and a high positive correlation between JOY and TRUST (ρ = 0.9). We propose two ways to incorporate these label correlations to improve prediction. 4.1 Correlations on Labeled Data In a multi-label setting, a good model should respect the label correlations. If it is confident about a particular label, then it should also be confident about other positively correlated labels, and conversely less confident about labels that are negatively correlated. Following Zhao et al. (2019), we add (i) a loss function that penalizes the model for making incongruous predictions, i.e. those that are not compatible with the label correlations, and (ii) a component that multiplies the classification logit vector z with the learned label relations encoded as a learned correlation matrix G. This component transforms the raw prediction score of each label to a weighted sum of the prediction scores of the other labels. For each label, these weights are given by its learned correlation with all the other labels. Therefore, the prediction score of each label is affected by the prediction"
2020.acl-main.426,N19-1368,0,0.327564,"tive understanding introduced ROCStories, a dataset for evaluating story understanding (Mostafazadeh et al., 2016). On a subset of these stories (Rashkin et al., 2018) added annotations for causal links between events in stories and mental states of characters. They model entity state to predict emotional reactions and motivations for causing events occurring in ROCStories. Additionally, they also introduce a new dataset annotation that tracks emotional reactions and motivations of characters in stories. Other work looked at encoding external knowledge sources to augment motivation inference (Paul and Frank, 2019) on the same dataset. Both treat labels as anonymous classes, whereas this work explores modeling the semantics of the emotion labels explicitly. Recent work in multi-label emotion classification has shown that using the relation information between labels can improve performance. (Kurata et al., 2016) use the label co-occurrence information in the final layer of the neural network to improve multi-label classification. Correlationbased label representations have also been used for music classification styles (Zhao et al., 2019). Our work builds on these and adds a similar result showing that"
2020.acl-main.426,N18-1202,0,0.0548181,"r all the models we report the micro-averaged Precision, Recall and F1 score of the emotion prediction task. Rashkin et al. (2018) modeled character context and pre-trained on free response data to predict the mental states of characters using different encoderdecoder setups, including BiLSTMs, CNNs, the recurrent entity network (REN) (Henaff et al., 2016), and neural process networks (NPN) (Bosselut et al., 2017). Additionally, we compare with the selfattention architecture proposed in (Paul and Frank, 2019), without the knowledge from ConceptNet (Speer and Havasi, 2012) and ELMo embeddings (Peters et al., 2018). To compare against LEAM, we compare it against our proposal of the LEAM+BERT model, where our label attention is computed from BERT representations of each of the label sentences, and words in the input sentence. We also encode the sentence and context separately in a BiLSTM layer as done in Rashkin et al. (2018). We also fine-tuned a BERT-base-uncased model for emotion classification, using xs , xc and Ls as inputs. This beats the other baselines by a significant margin, and is thus a strong new baseline. All our models are evaluated on the emotion reaction prediction task over the eight em"
2020.acl-main.426,P18-1213,0,0.084299,"t evokes joy is unlikely to also evoke sadness. In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference. We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data. Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-theart on an emotion inference task. 1 Introduction Understanding how events in a story affect the characters involved is an integral part of narrative understanding. Rashkin et al. (2018) introduced an emotion inference task on a subset of the ROCStories dataset (Mostafazadeh et al., 2016), labeling entities with the emotions they experience from the short story contexts. Previous work on this and related tasks typically frame them as multi-label classification problems. The standard approach uses an encoder that produces a representation of the target event along with the surrounding story events, and then pushes it through a classification layer to predict the possible emotion labels (Rashkin et al., 2018; Wang et al., 2018). This classification framework ignores the semanti"
2020.conll-1.42,W05-0909,0,0.0351598,"Missing"
2020.conll-1.42,2020.acl-main.225,0,0.0324182,"Missing"
2020.conll-1.42,P19-1254,0,0.0561411,"Missing"
2020.conll-1.42,W04-1013,0,0.0289135,"Missing"
2020.conll-1.42,P18-2045,0,0.0156512,"or character action taken throughout a story (Riedl and Young, 2010b). In our work, we use the sentiment trajectory of the narrative as the scaffold. That is, each Si for a sentence indicates the overall coarse sentiment of the sentence (Positive, Negative, or Neutral). Though simple, the overall sentiment trajectory of a narrative is important in defining the high level ‘shape’ of a narrative often shared among different narratives (Vonnegut, 1981; Reagan et al., 2016). Furthermore, sentiment trajectory has been shown to be fairly useful in story understanding tasks (Chaturvedi et al., 2017; Liu et al., 2018). We discuss in the conclusion future directions for using different types of scaffolds. 2.4 in Figure 1. The model consists of three sets of variables: (1) Switching variables S1 , ..., SN , (2) Latent state variables Z1 , ..., ZN capturing the details of the narrative at sentence i, (3) The sentences themselves X1 , ...XN , where each sentence Xi has ni words, xi1 , ...xini . The joint over all variables factorizes as below into the following components (X:i stands for all sentences before Xi ): N Y P (S, Z, X) = ( P (Xi |Zi , X:i )) | {z } i N Y ( i discrete states Si Si+1 Si+2 Zi Zi+1 Zi+2"
2020.conll-1.42,W19-2304,0,0.0386649,"Missing"
2020.conll-1.42,D18-1356,0,0.0321635,"nglish text data 521 3 In our case, we take each sentence in the narrative to be a different timestep. Different levels of granularity for a timestep may be more befitting for other domains. 4 Note that a bias term may also be added here (we do this in our implementation). We leave the bias off here for clarity 5 Other ways to formulate this transformation are also possible (Barber, 2006; Linderman et al., 2016). The Markov assumption is a common one and we use it here for simplicity. scaffolds include event sequences (Martin et al., 2018), keywords (Yao et al., 2019), or latent template ids (Wiseman et al., 2018). More complex but potentially more informative scaffolds may be created using concepts such as story grammar nonterminals (Lakoff, 1972; Thorndyke, 1977), or character action taken throughout a story (Riedl and Young, 2010b). In our work, we use the sentiment trajectory of the narrative as the scaffold. That is, each Si for a sentence indicates the overall coarse sentiment of the sentence (Positive, Negative, or Neutral). Though simple, the overall sentiment trajectory of a narrative is important in defining the high level ‘shape’ of a narrative often shared among different narratives (Vonneg"
2020.conll-1.42,D18-1462,0,0.16109,"2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 the narrative scaffolds can be specified in a limited ontology and the dynamics operations can be written by hand (such as e.g. the action schemata of Riedl and Young (2010a)). Neural generation has since helped scale to open domains (Roemmele and Gordon, 2015; Khalifa et al., 2017) but not with the same level of control over the narrative. Several recent works have looked at adding the narrative scaffolding component back into neural text generating systems (Fan et al.; Martin et al., 2018; Yao et al., 2019; Xu et al., 2018; Fan et al., 2019). These systems however still do not utilize an explicit model of narrative dynamics, and are thus restricted in the controllability aspect. In this work, we show how the insight of modeling the structure of a narrative along with general purpose dynamics can be combined with modern neural network based language models. We do this by explicitly modeling the narrative state with a latent vector, and modeling how this state transforms over time as a Switching Linear Dynamical System (SLDS). We show how this formulation captures the concepts of narrative dynamics and scaffolds"
2020.conll-1.42,N16-1098,1,\N,Missing
2020.conll-1.42,D17-1168,0,\N,Missing
2020.conll-1.42,N19-1423,0,\N,Missing
2020.emnlp-main.50,D13-1185,1,0.75481,"Missing"
2020.emnlp-main.50,P08-1090,1,0.872582,"Missing"
2020.emnlp-main.50,P09-1068,1,0.7437,"tory provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among event"
2020.emnlp-main.50,chambers-jurafsky-2010-database,1,0.801355,"Missing"
2020.emnlp-main.50,N13-1104,0,0.133398,"Missing"
2020.emnlp-main.50,N19-1423,0,0.0311453,"Missing"
2020.emnlp-main.50,N15-1165,0,0.026451,"Missing"
2020.emnlp-main.50,W16-1701,1,0.907272,"Missing"
2020.emnlp-main.50,P16-1025,1,0.902933,"Missing"
2020.emnlp-main.50,E12-1034,0,0.126036,"Missing"
2020.emnlp-main.50,W19-3311,0,0.0837118,"Missing"
2020.emnlp-main.50,Q18-1023,0,0.0569661,"Missing"
2020.emnlp-main.50,2020.acl-main.713,1,0.645499,"Missing"
2020.emnlp-main.50,N16-1098,1,0.812451,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,W16-1007,1,0.880849,"ream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.1 1 Introduction Existing approaches to automated event extraction retain the overly simplistic assumption that events are atomic occurrences. Understanding events requires knowledge in the form of a repository of abstracted event schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations."
2020.emnlp-main.50,P15-1019,0,0.145444,"Missing"
2020.emnlp-main.50,S19-1012,0,0.0429448,"ent schemas (complex event templates). Scripts (Schank and Abelson, 1977) encode frequently recurring event sequences, where events are ordered by temporal relation (Chambers and Jurafsky, 2009), causal relation (Mostafazadeh et al., 2016b), or narrative order (Jans et al., 2012). Event schemas have become increasingly important for natural language understanding tasks such as story 1 Our code and data are publicly available for research purpose at http://blender.cs.illinois.edu/software/ pathlm. ending prediction (Mostafazadeh et al., 2016a) and reading comprehension (Koˇcisk´y et al., 2018; Ostermann et al., 2019). Previous schema induction methods mostly ignore uncertainty, re-occurring events and multiple hypotheses, with limited attention to capture complex relations among events, other than temporal or causal relations. Temporal relations exist between almost all events, even those that are not semantically related; while research in identifying causal relations has been hobbled by low inter-annotator agreement (Hong et al., 2016). In this paper, we hypothesize that two events are connected when their entity arguments are coreferential or semantically related. For example, in Figure 1, (a) and (b)"
2020.emnlp-main.50,P17-1178,1,0.846469,"ument roles. We follow our recent work on ACE IE (Lin et al., 2020) to split the data. We consider the training set as historical data to train the LM, and the test set as our target data to induce schema for target scenarios. The instance graphs of the target data set are constructed from manual annotations. For historical data, we construct event instance graphs from both manual annotations (Historicalann ) and system extraction results (Historicalsys ) from the state-ofthe-art IE model (Lin et al., 2020). We perform cross-document entity coreference resolution by applying an entity linker (Pan et al., 2017) for both annotated and system generated instance graphs. Table 2 shows the data statistics. Split Historicalann Historicalsys Validation Target The cardinality for an instance graph and a schema will be the number of substructures in each, i.e., X |g|I = count(hvm , emn , vn i), hvm ,emn ,vn i∈g |s|S = 47,525 48,664 3,422 3,673 7,152 7,018 728 802 4,419 4,426 468 424 By extension, each path of length l=5 in a graph schema [φi , ψij , φj , ψjk , φk ] contains two consecutive triples hφi , ψij ,φj i, hφj , ψjk , φk i∈s, and a matched instance path contains two consecutive instance triples hvm ,"
2020.emnlp-main.50,K19-1051,0,0.111184,"Missing"
2020.emnlp-main.50,N18-1202,0,0.129853,"event but fails to extract the I NVESTIGATE C RIME triggered by “discovered” and its D EFENDANT argument “Mohammed A. Salameh”. Event graph scehmas can inform the model that a person who is arrested was usually investigated, our IE system can fix this missing error. Therefore we also conduct extrinsic evaluations and show the effectiveness of the induced schema repository in enhancing downstream end-to-end IE tasks. PART- WHOLE PLACE−1 −−−−−−−→ GPE −−−−−→ ATTACK. We train the path language model on two tasks: learning an auto-regressive language model (Ponte and Croft, 1998; Dai and Le, 2015; Peters et al., 2018; Radford et al.; Yang et al., 2019) to predict an edge or a node, given previous edges and nodes in a path, and a neighboring path classification task to predict how likely two paths co-occur. The path language model is trained from all the paths between two event instances from the same document, based on the assumption that events from the same document (especially news document) tell a coherent story. We propose two intrinsic evaluation metrics, instance coverage and instance coherence, to assess when event instance graphs are covered by each 685 In summary, we make the following novel con"
2020.emnlp-main.50,E14-1024,0,0.100189,"Missing"
2020.emnlp-main.50,D15-1195,0,0.203725,"Missing"
2020.emnlp-main.50,N16-1049,0,0.151836,"Missing"
2020.emnlp-main.50,W17-0901,0,0.0336757,"Missing"
2020.emnlp-main.50,W19-3404,0,0.0428587,"Missing"
2020.findings-emnlp.340,J15-2003,0,0.0195062,", Nathanael Chambers2 and Niranjan Balasubramanian1 1 1 Stony Brook University, Stony Brook, New York 2 US Naval Academy, Annapolis, MD {heekwon, mkoupaee, niranjan}@cs.stonybrook.edu nchamber@usna.edu Abstract events occur together in the world. This is not easily deduced from other event-event relations. Temporal ordering systems can sequence the order in which events occurred (Bethard, 2013; Chambers et al., 2014; Han et al., 2019) but can’t explain why they occurred at all. Which events in a sequence were by chance, and which were required? Textual entailment identifies event paraphrases (Berant et al., 2015) and some causation (Girju, 2003a), but their view misses the broader look at enabling events like preconditions. Let the following serve as an example: Preconditions provide a form of logical connection between events that explains why some events occur together and information that is complementary to the more widely studied relations such as causation, temporal ordering, entailment, and discourse relations. Modeling preconditions in text has been hampered in part due to the lack of large scale labeled data grounded in text. This paper introduces PeKo, a crowd-sourced annotation of precondit"
2020.findings-emnlp.340,S13-2002,0,0.0662327,"Missing"
2020.findings-emnlp.340,P12-1014,0,0.0249626,"ditions comes from the STRIPS program (Fikes and Nilsson, 1971). Preconditions were defined as a set of conditions that MUST be met in order for the action (event) to be allowed to take place. Later work focused on aggregating precondition knowledge for a small class of action words, leveraging FrameNet and a text corpus to generate candidate precondition words using a PMI-based heuristic (Sil et al., 2010; Sil and Yates, 2011). Using small amounts of labeled data, they use handcrafted PMI and wordnet based features to learn a SVM-based classifier that scores preconditions for a given action. Branavan et al. (2012) learned domain-specific preconditions from written instructions for the game of Minecraft. The instructions are procedural and well suited for identification. These mostly target preconditions that are eventstate relations as opposed to our goals of textual event-event identification. ATOMIC (Sap et al., 2019) is a related crowdsourced dataset of event-event relations, where given a simple target event (verb phrase and its arguments), crowd workers provided various types of common-sense knowledge. This included ‘NEED’ events analogous to our precondition events for a target. The main differen"
2020.findings-emnlp.340,W17-2711,0,0.0203134,"for a target. The main difference is our work grounds both target and precondition events in news text, whereas ATOMIC elicits general world knowledge, a complementary approach with different tradeoffs. Interestingly, we find that the precondition relations learnt from textually grounded news events generalize to story events in ATOMIC for our generation task. Annotated Text Corpora Three existing datasets capture some form of precondition knowledge in their annotations: the Rich Event Description (RED) dataset (O’Gorman et al., 2016), CaTeRS (Mostafazadeh et al., 2016), and Event StoryLine (Caselli and Vossen, 2017). These are generally too small for learning text classifiers as we briefly describe now. RED is the most directly related, created to model a broad set of event-event relations in news. Preconditions are not their sole focus, though, so this dataset only contains ~1000 precondition instances. CaTeRs shares a similar problem to RED. It has an enables relation similar to precondition, but since the domain is 5-sentence short stories and preconditions aren’t the main focus, it only has ~400 instances. The Event StoryLine dataset is small in size too, but also doesn’t have a precise precondition"
2020.findings-emnlp.340,Q14-1022,1,0.810003,"ing-lock and turning-key are preconditions in their own story contexts. Strict logicians might take issue, but language understanding requires a looser definition that uses likelihood of occurrence when interpreting real-world scenarios. 4 Preconditions Dataset This section describes our methodology to annotate news articles with the previous section’s definitions. One problem with annotating preconditions in text is the large number of event mentions in each article, which means annotation of all possible event pairs is infeasible. The temporal community has struggled with this same dilemma (Chambers et al., 2014; Vashishtha et al., 2019). We address the question of which pairs to annotate with two approaches. First, instead of attempting a dense annotation of few articles, we sub-sample candidate pairs of events across many articles. Second, we use an automatic temporal relation classifier to filter pairs by identifying possible candidates. We then ask crowd-workers to annotate the resulting pairs for preconditions. 4.1 Candidate Event Pair Extraction Sub-sampling event pairs at random from a document can result in a large number of pairs that are not preconditions. Because precondition event pairs o"
2020.findings-emnlp.340,W14-4012,0,0.0559893,"Missing"
2020.findings-emnlp.340,N19-1423,0,0.0255975,"k → find take → get ask → take work → make tell → take use → find know → get agree → pay born → die use → help touch → miss go → find get → help move → take lose → help leave → take #Instances 4,969 2,715 12,423 28,948 PeKo Task 1: Precondition Identification Given a text snippet with a target and candidate event pair, the task is to classify if the candidate event is a precondition for the target in the context described by the text snippet. This is a standard sentence-level classification task. We evaluate two strong and widely-used large transformer-based language models – fine-tuned BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) base models. For each model, we take the final representation of each event trigger, concatenate together, and then feed into a linear classification layer. We also evaluate a 1-layer GRU sequence model (Cho et al., 2014) with GloVe embeddings (Pennington et al., 2014) to calibrate against a much simpler baseline. See the Appendix for more details on parameters, layer sizes, and training time. Precondition identification is a difficult task. Table 4 shows the results. The GRU-based sequence model trained from scratch on PeKo is better than a prior-based random ba"
2020.findings-emnlp.340,W18-2501,0,0.014131,"evels of context. Using event triggers alone achieves moderate performance. This suggests that the verb trigger does carry a lot of the precondition knowledge regardless of event arguments (e.g., canceling requires scheduling first, but in most cases it doesn’t matter what is canceled). However, if we use event tuples4 , which also captures the main entities of the event, then we see a significant improvement in performance (+6.9 points). In addition to the tuples of the event pair, adding tuple representa4 We used OpenIE(Stanovsky et al., 2018) to extract event tuples implemented in AllenNLP(Gardner et al., 2018) 3823 tions of neighboring events provides an additional gain (+1.5 points). Further inspection of the tuplebased representation shows that automatic tuple extraction sometimes introduces errors and misses critical context and other important discourse cues. The best results come from using the sentence(s) that contain the event pair in its entirety – adding further sentences leads to worse performance. When is it difficult to identify preconditions? The first plot in Figure 4 shows that F1 score is highest where the target event is in the same sentence as the precondition event, higher where"
2020.findings-emnlp.340,W03-1210,0,0.608451,"bramanian1 1 1 Stony Brook University, Stony Brook, New York 2 US Naval Academy, Annapolis, MD {heekwon, mkoupaee, niranjan}@cs.stonybrook.edu nchamber@usna.edu Abstract events occur together in the world. This is not easily deduced from other event-event relations. Temporal ordering systems can sequence the order in which events occurred (Bethard, 2013; Chambers et al., 2014; Han et al., 2019) but can’t explain why they occurred at all. Which events in a sequence were by chance, and which were required? Textual entailment identifies event paraphrases (Berant et al., 2015) and some causation (Girju, 2003a), but their view misses the broader look at enabling events like preconditions. Let the following serve as an example: Preconditions provide a form of logical connection between events that explains why some events occur together and information that is complementary to the more widely studied relations such as causation, temporal ordering, entailment, and discourse relations. Modeling preconditions in text has been hampered in part due to the lack of large scale labeled data grounded in text. This paper introduces PeKo, a crowd-sourced annotation of preconditions between event pairs in news"
2020.findings-emnlp.340,N19-1225,0,0.0513459,"Missing"
2020.findings-emnlp.340,W14-0702,0,0.0266406,"knowledge is not easily accessible in LM-derived representations alone. Our generation results show that fine-tuning an LM on PeKo yields better conditional relations than when trained on raw text or temporally-ordered corpora. 1 I heard a bird sing above as I turned the key in the door. It opened with a push. Introduction Recognizing logical connections between events in text is important for comprehensive document understanding and to improve global coherence in language generation systems. There is a rich body of work in identifying relations between textual events which covers causation (Mirza et al., 2014), temporal relations (Pustejovsky et al., 2003), textual entailment (Dagan et al., 2005), and discourse relations (Blair-Goldensohn and McKeown, 2006). In this work, we focus on the precondition relation, which offers a general view of why certain You can sequence these four events in order, but an ordering does not understand the why of the situation. One of these events (sing) is clearly not relevant to the door opening. How do we know that turning the key is a precondition to opened and not push? Turning the key usually doesn’t cause the door to open (perhaps on some doors, but here a push"
2020.findings-emnlp.340,W16-1007,1,0.878958,"NEED’ events analogous to our precondition events for a target. The main difference is our work grounds both target and precondition events in news text, whereas ATOMIC elicits general world knowledge, a complementary approach with different tradeoffs. Interestingly, we find that the precondition relations learnt from textually grounded news events generalize to story events in ATOMIC for our generation task. Annotated Text Corpora Three existing datasets capture some form of precondition knowledge in their annotations: the Rich Event Description (RED) dataset (O’Gorman et al., 2016), CaTeRS (Mostafazadeh et al., 2016), and Event StoryLine (Caselli and Vossen, 2017). These are generally too small for learning text classifiers as we briefly describe now. RED is the most directly related, created to model a broad set of event-event relations in news. Preconditions are not their sole focus, though, so this dataset only contains ~1000 precondition instances. CaTeRs shares a similar problem to RED. It has an enables relation similar to precondition, but since the domain is 5-sentence short stories and preconditions aren’t the main focus, it only has ~400 instances. The Event StoryLine dataset is small in size to"
2020.findings-emnlp.340,N18-1077,0,0.0567474,"Missing"
2020.findings-emnlp.340,R11-1001,0,0.0322203,"rju, 2003b), and paraphrasal relationships (Lin and Pantel, 2001), but relatively less research into precondition relationships. One of the early definitions and computational use of preconditions comes from the STRIPS program (Fikes and Nilsson, 1971). Preconditions were defined as a set of conditions that MUST be met in order for the action (event) to be allowed to take place. Later work focused on aggregating precondition knowledge for a small class of action words, leveraging FrameNet and a text corpus to generate candidate precondition words using a PMI-based heuristic (Sil et al., 2010; Sil and Yates, 2011). Using small amounts of labeled data, they use handcrafted PMI and wordnet based features to learn a SVM-based classifier that scores preconditions for a given action. Branavan et al. (2012) learned domain-specific preconditions from written instructions for the game of Minecraft. The instructions are procedural and well suited for identification. These mostly target preconditions that are eventstate relations as opposed to our goals of textual event-event identification. ATOMIC (Sap et al., 2019) is a related crowdsourced dataset of event-event relations, where given a simple target event (v"
2020.findings-emnlp.340,N18-1081,0,0.0236845,"is not readily accessible in BERT. mance of BERT when using different levels of context. Using event triggers alone achieves moderate performance. This suggests that the verb trigger does carry a lot of the precondition knowledge regardless of event arguments (e.g., canceling requires scheduling first, but in most cases it doesn’t matter what is canceled). However, if we use event tuples4 , which also captures the main entities of the event, then we see a significant improvement in performance (+6.9 points). In addition to the tuples of the event pair, adding tuple representa4 We used OpenIE(Stanovsky et al., 2018) to extract event tuples implemented in AllenNLP(Gardner et al., 2018) 3823 tions of neighboring events provides an additional gain (+1.5 points). Further inspection of the tuplebased representation shows that automatic tuple extraction sometimes introduces errors and misses critical context and other important discourse cues. The best results come from using the sentence(s) that contain the event pair in its entirety – adding further sentences leads to worse performance. When is it difficult to identify preconditions? The first plot in Figure 4 shows that F1 score is highest where the target"
2020.findings-emnlp.340,P19-1280,0,0.0427177,"Missing"
2020.findings-emnlp.340,W16-5706,0,0.0556905,"Missing"
2020.findings-emnlp.340,D14-1162,0,0.0821535,"Missing"
2021.acl-long.555,P19-1470,0,0.0290959,"Computational Linguistics Prior work has not combined traditional event cooccurrence with event temporality as we do. We propose a conditional generation model to tackle temporal event ordering and event infilling, and train it as a denoising autoencoder over outof-context temporal event sequences. As shown in Figure 1, the encoder of our TemporalBART model reads a temporally scrambled sequence of a subset of input events, obtained by corrupting a temporally-ordered sequence of events from a corpus. The decoder, which can be viewed as a conditional event language model (Kiyomaru et al., 2019; Bosselut et al., 2019; Madaan et al., 2020), then reconstructs the complete, temporally-ordered event sequence. Such denoising training has been successful exploited in many applications (Vincent et al., 2010; Lu et al., 2013; Lample et al., 2018; Lewis et al., 2020), and using seq2seq models to reorder and smooth inputs has been explored before (Goyal and Durrett, 2020), but to our knowledge we are the first to apply this in this temporal modeling setting. The conditional generation architecture of our model is flexible enough to address a variety of tasks, including our temporal ordering and event infilling task"
2021.acl-long.555,P08-1090,1,0.830954,"infilling goals. 2.2 Schema Induction Schema learning systems are often evaluated on their ability to predict unseen events. Initial work 7143 e1 e2 e3 e4 e5 Encoder Decoder e4 e2 e1 Autoencoder (reconstruct original sequence) e4 e5 e2 e1 e3 Event Deletion e1 e2 e3 e4 e5 Event Shuﬄing e1 e2 e3 e4 e5 Input events Figure 2: Our event-based denoising autoencoding training scheme used to encourage our model to learn temporal event knowledge. The input is corrupted by shuffling and deletion. attempted to use statistical methods to derive a library of schematic information (Mooney and DeJong, 1985; Chambers and Jurafsky, 2008; Jans et al., 2012). Another thread exploits event language modeling to learn the distributions over events (Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or focuses on learning event representations (Modi, 2016; Weber et al., 2018a) rather than writing down discrete schemas. However, most of this work only models the cooccurrence between events instead of directly considering temporal information, and only represent events as a small tuple of S-V-O headwords. Another line of work instead directly focuses on extracting coherent narratives from “story salads” (Wang et a"
2021.acl-long.555,P07-2044,1,0.650114,"d Work Learning temporal knowledge to order events and generate new events as part of schemas or stories are two problems that have received significant attention, but in contrast to our work, previous work typically focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang, 2020) treats this task"
2021.acl-long.555,W16-1007,1,0.926305,"e training data we need. In these documents, discourse order is loosely assumed to reflect temporal order, so events extracted from this text can directly provide training data for our models. This use of automatic annotation allows us to use broad-domain data, giving us a strong domain-independent temporal model (Zhao et al., 2021). To evaluate how well our proposed models capture temporal knowledge and solve the two targeted tasks, we apply them on out-of domain test sets in a zero-shot manner. Specifically, for event ordering, we first extract test temporal event sequences from the CaTeRS (Mostafazadeh et al., 2016b) and MCTaco (Zhou et al., 2019) datasets, which include the annotations on temporal relations between events. We then compare the performance of our models with two baselines: a BERT-based pairwise model and a BERT-based pointer network. For event infilling, we use the test event sequences from CaTeRS and examine the ability of our models to order unseen events and generate infilled events in comparison with GPT-2 baselines from story generation. Our BART-based models significantly outperform the baseline models on the ordering settings we consider, and human evaluation verifies that our mod"
2021.acl-long.555,D17-1108,0,0.0202837,"focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang, 2020) treats this task as a graph generation problem, and so is able to predict more complex structures, but it focuses solely on ordering and is not suitable for our event infilling goals. 2.2 Schema Induction Schema learning systems"
2021.acl-long.555,2020.emnlp-main.88,0,0.117498,"s about events without explicitly materializing a discrete schema library. The target tasks in this work are directly motivated by downstream applications of schema learning. Text generation tasks like story completion rely on understanding what makes narratives plausible and what events might be likely to happen before, after, and between other events (Jain et al., 2017; Yao et al., 2019), motivating our event infilling task. Answering questions about causes, effects, or what might happen next in a scenario requires knowing typical temporal orders of event sequences (Zhou et al., 2019, 2020; Ning et al., 2020), motivating our temporal ordering task. 7142 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7142–7157 August 1–6, 2021. ©2021 Association for Computational Linguistics Prior work has not combined traditional event cooccurrence with event temporality as we do. We propose a conditional generation model to tackle temporal event ordering and event infilling, and train it as a denoising autoencoder over outof-context temporal event sequences. As shown in Figure 1, the encoder"
2021.acl-long.555,N18-1077,0,0.255309,"RT captures both temporal ordering and event cooccurrence to make various event-related inferences. ture temporal event knowledge broadly and support a wide range of inferences. We thus need a suitably general modeling framework to capture temporal knowledge about events, which in our case will be a BART-based (Lewis et al., 2020) model we call TemporalBART. Note that classic temporal relation extraction models, which model temporal ordering in context for a particular document, may chiefly learn how to use local discourse cues rather than generalizable event knowledge (Chambers et al., 2014; Ning et al., 2018b). Introduction This paper proposes a single model of events to support inferences in two seemingly different tasks: (1) temporal event ordering and (2) event infilling, or inferring unseen or unmentioned events occurring as part of a larger scenario. Figure 1 shows an example illustrating these two goals. Unlike prior approaches, we aim to address both with the same model architecture, rather than having to annotate data and build ad-hoc models for each task separately; our goal is to work towards models that capThe goals in this work relate to past work on learning narrative schemas (Mooney"
2021.acl-long.555,P18-1122,0,0.234314,"RT captures both temporal ordering and event cooccurrence to make various event-related inferences. ture temporal event knowledge broadly and support a wide range of inferences. We thus need a suitably general modeling framework to capture temporal knowledge about events, which in our case will be a BART-based (Lewis et al., 2020) model we call TemporalBART. Note that classic temporal relation extraction models, which model temporal ordering in context for a particular document, may chiefly learn how to use local discourse cues rather than generalizable event knowledge (Chambers et al., 2014; Ning et al., 2018b). Introduction This paper proposes a single model of events to support inferences in two seemingly different tasks: (1) temporal event ordering and (2) event infilling, or inferring unseen or unmentioned events occurring as part of a larger scenario. Figure 1 shows an example illustrating these two goals. Unlike prior approaches, we aim to address both with the same model architecture, rather than having to annotate data and build ad-hoc models for each task separately; our goal is to work towards models that capThe goals in this work relate to past work on learning narrative schemas (Mooney"
2021.acl-long.555,J05-1004,0,0.142026,"the scenario y: a partial set of unordered events. Our model should learn distributions over a true underlying order of events, without obvious gaps in the event sequence, given this incomplete information. By taking events out of context rather than in the context of a document, we are encouraging the model to encode temporal knowledge between events rather than superficial cues like surface textual order or discourse connectives that might determine their order. For the definition of events, we follow Chambers and Jurafsky (2008) where an event e is a predicate ve along with its arguments (Palmer et al., 2005). Our model can be formulated as a denoising autoencoder if x is created as a noised version of y. Specifically, given a temporal event sequence y as defined above, we first corrupt it to get the required input x by performing two transformation functions consecutively (see Figure 2): Event Shuffling We first perform a random shuffling of the events in y to produce x. To perfectly reconstruct the original sequence y, the model must capture the temporal relations between events. Event Deletion We randomly delete each event in y with probability p to produce x. This denoising scheme is similar t"
2021.acl-long.555,P16-1028,0,0.143115,"s a single model of events to support inferences in two seemingly different tasks: (1) temporal event ordering and (2) event infilling, or inferring unseen or unmentioned events occurring as part of a larger scenario. Figure 1 shows an example illustrating these two goals. Unlike prior approaches, we aim to address both with the same model architecture, rather than having to annotate data and build ad-hoc models for each task separately; our goal is to work towards models that capThe goals in this work relate to past work on learning narrative schemas (Mooney and DeJong, 1985; Chambers, 2013; Peng and Roth, 2016; Peng et al., 2017). Our approach particularly follows a recent line of work using distributed representations of schemas (Pichotta and Mooney, 2016; Weber et al., 2018b), which support inferences about events without explicitly materializing a discrete schema library. The target tasks in this work are directly motivated by downstream applications of schema learning. Text generation tasks like story completion rely on understanding what makes narratives plausible and what events might be likely to happen before, after, and between other events (Jain et al., 2017; Yao et al., 2019), motivating"
2021.acl-long.555,2020.emnlp-main.58,0,0.0778867,"Missing"
2021.acl-long.555,2020.emnlp-main.349,0,0.0593192,"Missing"
2021.acl-long.555,P17-2035,0,0.02266,"part of schemas or stories are two problems that have received significant attention, but in contrast to our work, previous work typically focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang, 2020) treats this task as a graph generation problem, and so is able to predict more complex structu"
2021.acl-long.555,P19-1280,0,0.047847,"Missing"
2021.acl-long.555,S07-1014,0,0.0288069,"2 Background and Related Work Learning temporal knowledge to order events and generate new events as part of schemas or stories are two problems that have received significant attention, but in contrast to our work, previous work typically focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang,"
2021.acl-long.555,C08-3012,0,0.0523325,"l knowledge to order events and generate new events as part of schemas or stories are two problems that have received significant attention, but in contrast to our work, previous work typically focuses on each in isolation. 2.1 Temporal Event Ordering Closely related to the temporal ordering aspect of this paper is temporal relation extraction, which orders pairs of events in text in document context (Pustejovsky et al., 2003b; Cassidy et al., 2014; Ning et al., 2018b). This problem has been addressed as pairwise classification (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008; Cheng and Miyao, 2017; Tourille et al., 2017; Goyal and Durrett, 2019) or as a structured learning problem to enforce constraints on the output (Do et al., 2012; Ning et al., 2017, 2018a; Leeuwenberg and Moens, 2017; Han et al., 2019a,b). However, even in these latter works, the models focus on pairwise relations. In contrast, our work here views temporal event ordering as a sequence generation problem, which provides models a stronger inductive bias to capture global temporal relations between events. One recent effort (Madaan and Yang, 2020) treats this task as a graph generation problem,"
2021.acl-long.555,D19-1273,1,0.85049,"rn the distributions over events (Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or focuses on learning event representations (Modi, 2016; Weber et al., 2018a) rather than writing down discrete schemas. However, most of this work only models the cooccurrence between events instead of directly considering temporal information, and only represent events as a small tuple of S-V-O headwords. Another line of work instead directly focuses on extracting coherent narratives from “story salads” (Wang et al., 2018) or more broadly generating narratives given predefined scenarios (Wang et al., 2019; Qin et al., 2020). However, without considering temporal ordering, these systems are prone to learn discourse ordering of events instead of a strong representation of temporal knowledge. 3 3.1 Method Task Formulation and Model Our framework involves modeling a conditional distribution P (y |x) over temporal event sequences y = {e1 , · · · , el }, which are sequences of events taken out of context (i.e., not represented as spans in a document) which are part of the same scenario, involve shared actors, and are temporally ordered. The input of the model is a (not necessarily temporal) sequence"
2021.acl-long.555,D18-1175,1,0.821086,"sky, 2008; Jans et al., 2012). Another thread exploits event language modeling to learn the distributions over events (Pichotta and Mooney, 2016; Peng and Roth, 2016; Weber et al., 2018b), or focuses on learning event representations (Modi, 2016; Weber et al., 2018a) rather than writing down discrete schemas. However, most of this work only models the cooccurrence between events instead of directly considering temporal information, and only represent events as a small tuple of S-V-O headwords. Another line of work instead directly focuses on extracting coherent narratives from “story salads” (Wang et al., 2018) or more broadly generating narratives given predefined scenarios (Wang et al., 2019; Qin et al., 2020). However, without considering temporal ordering, these systems are prone to learn discourse ordering of events instead of a strong representation of temporal knowledge. 3 3.1 Method Task Formulation and Model Our framework involves modeling a conditional distribution P (y |x) over temporal event sequences y = {e1 , · · · , el }, which are sequences of events taken out of context (i.e., not represented as spans in a document) which are part of the same scenario, involve shared actors, and are"
2021.acl-long.555,D18-1413,1,0.810746,"events occurring as part of a larger scenario. Figure 1 shows an example illustrating these two goals. Unlike prior approaches, we aim to address both with the same model architecture, rather than having to annotate data and build ad-hoc models for each task separately; our goal is to work towards models that capThe goals in this work relate to past work on learning narrative schemas (Mooney and DeJong, 1985; Chambers, 2013; Peng and Roth, 2016; Peng et al., 2017). Our approach particularly follows a recent line of work using distributed representations of schemas (Pichotta and Mooney, 2016; Weber et al., 2018b), which support inferences about events without explicitly materializing a discrete schema library. The target tasks in this work are directly motivated by downstream applications of schema learning. Text generation tasks like story completion rely on understanding what makes narratives plausible and what events might be likely to happen before, after, and between other events (Jain et al., 2017; Yao et al., 2019), motivating our event infilling task. Answering questions about causes, effects, or what might happen next in a scenario requires knowing typical temporal orders of event sequences"
2021.acl-long.555,P18-1050,0,0.14193,"ampling from the model or using it to score sequences. Capitalizing on the success of recent pre-trained encoder-decoder transformers (Lewis et al., 2020; Raffel et al., 2020), our model itself is based on BART, consuming and producing predicate-argument structures rendered in surface order. Gathering large-scale high-quality labeled data with temporal annotations is often expensive and requires specially designed annotation schemes (Pustejovsky et al., 2003a; Cassidy et al., 2014; Ning et al., 2018b; Zhao et al., 2021). Here, we instead turn to a narrative documents corpus, EventsNarratives (Yao and Huang, 2018) and design an automatic method to extract the training data we need. In these documents, discourse order is loosely assumed to reflect temporal order, so events extracted from this text can directly provide training data for our models. This use of automatic annotation allows us to use broad-domain data, giving us a strong domain-independent temporal model (Zhao et al., 2021). To evaluate how well our proposed models capture temporal knowledge and solve the two targeted tasks, we apply them on out-of domain test sets in a zero-shot manner. Specifically, for event ordering, we first extract te"
2021.acl-long.555,2021.adaptnlp-1.20,1,0.773965,"ss a variety of tasks, including our temporal ordering and event infilling tasks, by either sampling from the model or using it to score sequences. Capitalizing on the success of recent pre-trained encoder-decoder transformers (Lewis et al., 2020; Raffel et al., 2020), our model itself is based on BART, consuming and producing predicate-argument structures rendered in surface order. Gathering large-scale high-quality labeled data with temporal annotations is often expensive and requires specially designed annotation schemes (Pustejovsky et al., 2003a; Cassidy et al., 2014; Ning et al., 2018b; Zhao et al., 2021). Here, we instead turn to a narrative documents corpus, EventsNarratives (Yao and Huang, 2018) and design an automatic method to extract the training data we need. In these documents, discourse order is loosely assumed to reflect temporal order, so events extracted from this text can directly provide training data for our models. This use of automatic annotation allows us to use broad-domain data, giving us a strong domain-independent temporal model (Zhao et al., 2021). To evaluate how well our proposed models capture temporal knowledge and solve the two targeted tasks, we apply them on out-o"
2021.acl-long.555,D19-1332,0,0.0911155,", which support inferences about events without explicitly materializing a discrete schema library. The target tasks in this work are directly motivated by downstream applications of schema learning. Text generation tasks like story completion rely on understanding what makes narratives plausible and what events might be likely to happen before, after, and between other events (Jain et al., 2017; Yao et al., 2019), motivating our event infilling task. Answering questions about causes, effects, or what might happen next in a scenario requires knowing typical temporal orders of event sequences (Zhou et al., 2019, 2020; Ning et al., 2020), motivating our temporal ordering task. 7142 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7142–7157 August 1–6, 2021. ©2021 Association for Computational Linguistics Prior work has not combined traditional event cooccurrence with event temporality as we do. We propose a conditional generation model to tackle temporal event ordering and event infilling, and train it as a denoising autoencoder over outof-context temporal event sequences. As shown"
2021.acl-short.76,D13-1178,1,0.803289,"ly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and outof-domain events data. 1 Figure 1: Example of an event schema for which the discourse order is different from the temporal order. Introduction Event-level language models (LMs) provide a way to reason about events, and to approximate schematic and script-like knowledge (Schank and Abelson, 1977; Balasubramanian et al., 2013; Nguyen et al., 2015) about them (Modi and Titov, 2014; Pichotta and Mooney, 2016; Weber et al., 2018). These models aim to learn high-level representations of complex events (e.g., an arrest) and possibly their entity roles from raw text (e.g., a suspect). However, a major limitation is their reliance on the discourse order of event mentions when training the LM. Although powerful, these event LMs capture information we don’t want in true world knowledge. For instance, a script of events may be weakly ordered in real life, but the system instead learns to strongly rely on the text order in w"
2021.acl-short.76,P09-1068,1,0.665532,"events appeared in text (Manshadi et al., 2008). However, relying on discourse order may not be necessary and can potentially limit generalization of event LMs. For some event related tasks such as schema learning (Weber et al., 2018), the discourse order is not directly relevant. For other tasks such as event ordering (Pustejovsky et al., 2003; Chambers et al., 2014; Wang et al., 2018), temporal or logical order of events is most critical – discourse order, at best, is a noisy proxy. In fact, the first systems for schema learning were noticeably not language models (Mooney and DeJong, 1985; Chambers and Jurafsky, 2009, 2011). We introduce three simple perturbation techniques shown in Figure 2 that relax the reliance on discourse sequences. 2.1 Event Permutation One way to reduce reliance on discourse order is to expose the model to random permutations of the input sequences, as shown in Figure 2. Using all possible permutations of a sequence is impractical, so we introduce three specific shuffles that force the model to pay attention to long-term dependencies and avoid the over-reliance on local dependencies/order: • Reversed order: given a set of events as ABCD, the reverse of the sequence is created as D"
2021.acl-short.76,P11-1098,1,0.822527,"Missing"
2021.acl-short.76,K16-1008,0,0.0202393,"tions of the reverse order of the original sequence. The new sequence is: CADB Figure 2: Sequence perturbations strategies. 2 These shuffle patterns were selected to minimize the chance of repetition across permutations. Perturbing Discourse Sequences Event language modeling tasks are typically defined over sequences of events as they appear in text. The events can be represented either as a sequence of words annotated with predicateargument structure (e.g., semantic roles (Pichotta and Mooney, 2016), Open IE tuples (Weber et al., 2018; Rudinger et al., 2015) or with compositional embeddings (Modi, 2016). Generative models are trained to predict subsequent events in a sequence conditioning on previously observed events. Naturally, these models learn the order in which events appeared in text (Manshadi et al., 2008). However, relying on discourse order may not be necessary and can potentially limit generalization of event LMs. For some event related tasks such as schema learning (Weber et al., 2018), the discourse order is not directly relevant. For other tasks such as event ordering (Pustejovsky et al., 2003; Chambers et al., 2014; Wang et al., 2018), temporal or logical order of events is mo"
2021.acl-short.76,W14-1606,0,0.0146415,"rturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and outof-domain events data. 1 Figure 1: Example of an event schema for which the discourse order is different from the temporal order. Introduction Event-level language models (LMs) provide a way to reason about events, and to approximate schematic and script-like knowledge (Schank and Abelson, 1977; Balasubramanian et al., 2013; Nguyen et al., 2015) about them (Modi and Titov, 2014; Pichotta and Mooney, 2016; Weber et al., 2018). These models aim to learn high-level representations of complex events (e.g., an arrest) and possibly their entity roles from raw text (e.g., a suspect). However, a major limitation is their reliance on the discourse order of event mentions when training the LM. Although powerful, these event LMs capture information we don’t want in true world knowledge. For instance, a script of events may be weakly ordered in real life, but the system instead learns to strongly rely on the text order in which the events were described. Figure 1 shows an examp"
2021.acl-short.76,1985.tmi-1.17,0,0.119368,"learn the order in which events appeared in text (Manshadi et al., 2008). However, relying on discourse order may not be necessary and can potentially limit generalization of event LMs. For some event related tasks such as schema learning (Weber et al., 2018), the discourse order is not directly relevant. For other tasks such as event ordering (Pustejovsky et al., 2003; Chambers et al., 2014; Wang et al., 2018), temporal or logical order of events is most critical – discourse order, at best, is a noisy proxy. In fact, the first systems for schema learning were noticeably not language models (Mooney and DeJong, 1985; Chambers and Jurafsky, 2009, 2011). We introduce three simple perturbation techniques shown in Figure 2 that relax the reliance on discourse sequences. 2.1 Event Permutation One way to reduce reliance on discourse order is to expose the model to random permutations of the input sequences, as shown in Figure 2. Using all possible permutations of a sequence is impractical, so we introduce three specific shuffles that force the model to pay attention to long-term dependencies and avoid the over-reliance on local dependencies/order: • Reversed order: given a set of events as ABCD, the reverse of"
2021.acl-short.76,P15-1019,0,0.0481132,"Missing"
2021.acl-short.76,D14-1162,0,0.0870909,"Missing"
2021.acl-short.76,D15-1195,0,0.0543341,"Missing"
2021.acl-short.76,D12-1048,0,0.0118459,"ut procedures. 2.3 Event Masking When dropping events, we can provide additional information to the model about where events were dropped. This forces the model to capture longerterm dependencies among events in the sequence. We randomly select a number of event tuples and replace their tokens with a &lt;mask&gt; token (Masking in Figure 2). For each sequence in the training set, we generate its masked sequences with each having a fixed proportion of its events masked. 3 Experimental Setup Data We train event language models on the Annotated NYT corpus using Open IE event tuples extracted by Ollie (Schmitz et al., 2012). The dataset contains a total of around 1.8 million articles. After preprocessing steps, 1,467,366 articles are used as the training set, 6k articles as test set and 4k articles as the dev set. Each event is a 4-tuple (v,s,o,p) containing the verb, subject, object and preposition. We follow the same preprocessing steps outlined in Weber et al. (2018) to create event sequences. The components of the events (the verb, subject, etc.) are all individual tokens, and are treated like normal text. For example, the events (truck packed with explosives), (police arrested suspect), would be given to th"
2021.acl-short.76,D18-1175,1,0.841428,"r et al., 2015) or with compositional embeddings (Modi, 2016). Generative models are trained to predict subsequent events in a sequence conditioning on previously observed events. Naturally, these models learn the order in which events appeared in text (Manshadi et al., 2008). However, relying on discourse order may not be necessary and can potentially limit generalization of event LMs. For some event related tasks such as schema learning (Weber et al., 2018), the discourse order is not directly relevant. For other tasks such as event ordering (Pustejovsky et al., 2003; Chambers et al., 2014; Wang et al., 2018), temporal or logical order of events is most critical – discourse order, at best, is a noisy proxy. In fact, the first systems for schema learning were noticeably not language models (Mooney and DeJong, 1985; Chambers and Jurafsky, 2009, 2011). We introduce three simple perturbation techniques shown in Figure 2 that relax the reliance on discourse sequences. 2.1 Event Permutation One way to reduce reliance on discourse order is to expose the model to random permutations of the input sequences, as shown in Figure 2. Using all possible permutations of a sequence is impractical, so we introduce"
2021.acl-short.76,D18-1413,1,0.693357,"pendence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and outof-domain events data. 1 Figure 1: Example of an event schema for which the discourse order is different from the temporal order. Introduction Event-level language models (LMs) provide a way to reason about events, and to approximate schematic and script-like knowledge (Schank and Abelson, 1977; Balasubramanian et al., 2013; Nguyen et al., 2015) about them (Modi and Titov, 2014; Pichotta and Mooney, 2016; Weber et al., 2018). These models aim to learn high-level representations of complex events (e.g., an arrest) and possibly their entity roles from raw text (e.g., a suspect). However, a major limitation is their reliance on the discourse order of event mentions when training the LM. Although powerful, these event LMs capture information we don’t want in true world knowledge. For instance, a script of events may be weakly ordered in real life, but the system instead learns to strongly rely on the text order in which the events were described. Figure 1 shows an example where discourse and actual temporal order are"
2021.acl-short.76,2020.emnlp-demos.6,0,0.0499494,"Missing"
2021.acl-short.76,P18-1050,0,0.137033,"equences in the training data to relax the model’s dependence on discourse order. By considering the next event based on shuffled sequences of events, we encourage the model to treat the input more as a set of events rather than strictly as a discourse sequence. Surprisingly, despite our disruption of discourse order, experiments show how perturbations can improve event language modeling of text, particularly when evaluating the model on other domains which present events in different orders (e.g., novels or blogs present data in more of a “narrative” fashion than news datasets common in NLP (Yao and Huang, 2018)). Our experiments evaluate accuracy on the Inverse Narrative Cloze task on in-domain newswire, as well as out-domain novels and blogs1 . 1 The code and data is available at https://github. com/StonyBrookNLP/elm-perturbations 599 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 599–604 August 1–6, 2021. ©2021 Association for Computational Linguistics • Concatenation of events in the odd positions followed by the even positions of the sequence: the permuted seq"
2021.findings-acl.53,W19-5301,0,0.0427087,"Missing"
2021.findings-acl.53,2020.coling-main.52,1,0.738743,"Missing"
2021.findings-acl.53,2020.coling-main.210,0,0.0609817,"Missing"
2021.findings-acl.53,P09-1068,1,0.771513,"Missing"
2021.findings-acl.53,D19-5817,0,0.0571439,"Missing"
2021.findings-acl.53,L18-1438,0,0.0462471,"Missing"
2021.findings-acl.53,W13-2305,0,0.0125685,"t all. StrategyQA (Geva et al., 2021) is a new dataset focusing on performing better implicit reasoning for multi-hop question answering tasks. We summarize the different why-questions corpora in Table 1. None of them represent a large dataset focused on answering why-questions about actions in a narrative. 2.2 Human evaluation for NLG tasks Among language generation tasks, machine translation has received the most attention in terms of human evaluation. Qualified crowd workers score output translations given the source or reference text to calibrate MT systems (Sakaguchi and Van Durme, 2018; Graham et al., 2013, 2014). WMT conducts annual evaluation of outputs of systems submitted to the shared task and uses it as one of the primary metrics (along with BLEU) to rank systems (Bojar et al., 2016, 2017, 2018; Barrault et al., 2019, 2020). ChatEval (Sedoc et al., 2019) is an evaluation platform for chatbots. Zellers et al. (2020) present a leaderboard for their advice generation task. These platforms incorporate some manual analysis, but focus on very different tasks. None of their Mechanical Turk interfaces can be used for our task. We were unable to find a consistent interface for human evaluation of"
2021.findings-acl.53,E14-1047,0,0.0437022,"Missing"
2021.findings-acl.53,I08-1055,0,0.0948131,"Missing"
2021.findings-acl.53,2020.inlg-1.23,0,0.0332554,"Missing"
2021.findings-acl.53,2020.findings-emnlp.171,0,0.020028,"s can be found in Appendix A. We finetuned a pretrained T5-base model from HuggingFace (Wolf et al., 2020) on TellMeWhy. Since it is a natural language generation task related to a story, we use the SQuAD format specified in Appendix D.15 of Raffel et al. (2020) to format our inputs. Our narrative serves as the ‘context’ and the why-question is used as the ‘question’ in the selected input format. We train the model with batch size 16, learning rate 5e-5, maximum source length 75 and maximum answer length 30. The model is trained until the dev loss fails to improve for 3 iterations. UnifiedQA (Khashabi et al., 2020) is a single pretrained model that performs well across 20 different question answering datasets. It is built on top of a T5 model and simplifies finetuning by unifying the various formats used by T5. Its ability to perform both extractive and abstractive QA tasks makes it a suitable candidate for calibrating this task. A pretrained version of this model is available via HuggingFace (Wolf et al., 2020) under the name “allenai/unifiedqa-t5-base&quot;. The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol. We train this model usin"
2021.findings-acl.53,Q18-1023,0,0.0495884,"Missing"
2021.findings-acl.53,D17-1082,0,0.0222405,"Missing"
2021.findings-acl.53,W04-1013,0,0.0948406,"d version of this model is available via HuggingFace (Wolf et al., 2020) under the name “allenai/unifiedqa-t5-base&quot;. The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol. We train this model using learning rate 1e-5 (same as the original paper) and retain other hyperparameters from finetuning T5 as described above. 5.2 Automatic Evaluation We evaluate all of the above models on both the test set and the hidden test set (questions from CATERS data). For automatic evaluation, we report BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), BLEURT (Sellam et al., 2020) scores using the bluert-base-128 checkpoint, and BertScore (Zhang* et al., 2020) using the default roberta-large checkpoint. These numbers are presented in Table 4. Evaluated on Model BLEU RG-L F1 BLEURT BertScore Full Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.96 9.89 24.53 21.97 0.07 0.13 0.13 0.24 0.25 -1.23 -0.75 -0.963 -0.28 -0.30 -0.55 0.18 0.23 0.48 0.43 Implicit-Answer Qs in Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.89 8.14 16.31 14.6 0.06 0.12 0.11 0.17 0.18 -1.22 -0.805 -0.99 -0.51 -0.50 -0.54 0.17 0.22 0.38 0.34 Table 4: Perfor"
2021.findings-acl.53,W19-5302,0,0.0309291,"Missing"
2021.findings-acl.53,W16-1007,1,0.940516,"re external to the narrative, thus providing a challenge for future QA and narrative understanding research. 1 Introduction The actions people perform are steps of plans to achieve their desired goals. When interpreting language, humans naturally understand the reasons behind described actions, even when the reasons are left unstated (Schank and Abelson, 1975). For NLP systems, answering questions about why people perform actions in a narrative can test this ability. Answering such questions often requires filling the implicit gaps in the story itself. Consider this narrative from ROCStories (Mostafazadeh et al., 2016b): Rudy was convinced that bottled waters all tasted the same. He went to the store and bought several popular brands. He went back home and set them all on a table. He spent several hours tasting them one by one. He came to the conclusion that they actually did taste different. Now try to answer the question, “Why did he go to the store and buy several popular brands?” The answer “he wanted to taste test” is not explicit in the narrative and requires us to read between the lines to fill in the gaps (Norvig, 1987). While humans can visualise and process the events in a story to hypothesize wh"
2021.findings-acl.53,2020.emnlp-main.370,0,0.0743723,"Missing"
2021.findings-acl.53,P08-1051,0,0.0615989,"Missing"
2021.findings-acl.53,W19-4113,0,0.0603227,"Missing"
2021.findings-acl.53,N18-2012,0,0.0209232,"Missing"
2021.findings-acl.53,P19-1414,0,0.0541863,"Missing"
2021.findings-acl.53,N16-1098,1,0.828283,"re external to the narrative, thus providing a challenge for future QA and narrative understanding research. 1 Introduction The actions people perform are steps of plans to achieve their desired goals. When interpreting language, humans naturally understand the reasons behind described actions, even when the reasons are left unstated (Schank and Abelson, 1975). For NLP systems, answering questions about why people perform actions in a narrative can test this ability. Answering such questions often requires filling the implicit gaps in the story itself. Consider this narrative from ROCStories (Mostafazadeh et al., 2016b): Rudy was convinced that bottled waters all tasted the same. He went to the store and bought several popular brands. He went back home and set them all on a table. He spent several hours tasting them one by one. He came to the conclusion that they actually did taste different. Now try to answer the question, “Why did he go to the store and buy several popular brands?” The answer “he wanted to taste test” is not explicit in the narrative and requires us to read between the lines to fill in the gaps (Norvig, 1987). While humans can visualise and process the events in a story to hypothesize wh"
2021.findings-acl.53,P02-1040,0,0.114084,"alibrating this task. A pretrained version of this model is available via HuggingFace (Wolf et al., 2020) under the name “allenai/unifiedqa-t5-base&quot;. The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol. We train this model using learning rate 1e-5 (same as the original paper) and retain other hyperparameters from finetuning T5 as described above. 5.2 Automatic Evaluation We evaluate all of the above models on both the test set and the hidden test set (questions from CATERS data). For automatic evaluation, we report BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), BLEURT (Sellam et al., 2020) scores using the bluert-base-128 checkpoint, and BertScore (Zhang* et al., 2020) using the default roberta-large checkpoint. These numbers are presented in Table 4. Evaluated on Model BLEU RG-L F1 BLEURT BertScore Full Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.96 9.89 24.53 21.97 0.07 0.13 0.13 0.24 0.25 -1.23 -0.75 -0.963 -0.28 -0.30 -0.55 0.18 0.23 0.48 0.43 Implicit-Answer Qs in Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.89 8.14 16.31 14.6 0.06 0.12 0.11 0.17 0.18 -1.22 -0.805 -0.99 -0.51 -0.50 -0.54 0.17 0.22 0.38"
2021.findings-acl.53,E14-1024,1,0.870934,"Missing"
2021.findings-acl.53,W18-6319,0,0.0116462,".45 3.96 9.89 24.53 21.97 0.07 0.13 0.13 0.24 0.25 -1.23 -0.75 -0.963 -0.28 -0.30 -0.55 0.18 0.23 0.48 0.43 Implicit-Answer Qs in Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.89 8.14 16.31 14.6 0.06 0.12 0.11 0.17 0.18 -1.22 -0.805 -0.99 -0.51 -0.50 -0.54 0.17 0.22 0.38 0.34 Table 4: Performance of models on the full test set and on implicit-answer questions in the test set using automated metrics. RG-L denotes ROUGE-L. The OO suffix denotes the vanilla version of the model while the FT version denotes the finetuned version. We select one human answer at a time and (using SacreBLEU (Post, 2018)) calculate the BLEU scores for model output with all three references, and select the maximum. Since BLEURT is a sentence level metric, to calculate the reported BLEURT, we average all the (output, reference) scores to obtain a corpus score for each reference. We then select the maximum BLEURT corpus score over all 3 human references. It is important to note that BLEURT was proposed as a metric for relative comparison, not absolute calibration. We also report BertScore F13 (Zhang* et al., 2020) as another semantic automatic evaluation metric. We report a max BertScore in the same way as BLEUR"
2021.findings-acl.53,P18-1020,0,0.0423673,"Missing"
2021.findings-acl.53,W19-8610,0,0.026832,"Missing"
2021.findings-acl.53,N19-4011,0,0.0164576,"ering why-questions about actions in a narrative. 2.2 Human evaluation for NLG tasks Among language generation tasks, machine translation has received the most attention in terms of human evaluation. Qualified crowd workers score output translations given the source or reference text to calibrate MT systems (Sakaguchi and Van Durme, 2018; Graham et al., 2013, 2014). WMT conducts annual evaluation of outputs of systems submitted to the shared task and uses it as one of the primary metrics (along with BLEU) to rank systems (Bojar et al., 2016, 2017, 2018; Barrault et al., 2019, 2020). ChatEval (Sedoc et al., 2019) is an evaluation platform for chatbots. Zellers et al. (2020) present a leaderboard for their advice generation task. These platforms incorporate some manual analysis, but focus on very different tasks. None of their Mechanical Turk interfaces can be used for our task. We were unable to find a consistent interface for human evaluation of an open-ended question answering task. To address this flaw, we propose a standard human intelligence task (HIT) evaluation scheme for our dataset. 3 Dataset Creation We want to test the abilities of models to understand the reasoning behind actions in a stor"
2021.findings-acl.53,2020.acl-main.704,0,0.0178184,"del is available via HuggingFace (Wolf et al., 2020) under the name “allenai/unifiedqa-t5-base&quot;. The input format for this model is simple, just requiring the question and the narrative to be separated by a newline symbol. We train this model using learning rate 1e-5 (same as the original paper) and retain other hyperparameters from finetuning T5 as described above. 5.2 Automatic Evaluation We evaluate all of the above models on both the test set and the hidden test set (questions from CATERS data). For automatic evaluation, we report BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), BLEURT (Sellam et al., 2020) scores using the bluert-base-128 checkpoint, and BertScore (Zhang* et al., 2020) using the default roberta-large checkpoint. These numbers are presented in Table 4. Evaluated on Model BLEU RG-L F1 BLEURT BertScore Full Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.96 9.89 24.53 21.97 0.07 0.13 0.13 0.24 0.25 -1.23 -0.75 -0.963 -0.28 -0.30 -0.55 0.18 0.23 0.48 0.43 Implicit-Answer Qs in Test Set GPT-2-OO GPT2-FT T5-OO T5-FT UnifiedQA 4.45 3.89 8.14 16.31 14.6 0.06 0.12 0.11 0.17 0.18 -1.22 -0.805 -0.99 -0.51 -0.50 -0.54 0.17 0.22 0.38 0.34 Table 4: Performance of models on the full te"
2021.findings-acl.53,verberne-etal-2006-data,0,0.170228,"Missing"
2021.starsem-1.15,2020.acl-main.9,0,0.035791,"Then the model learns to generate text conditioned on a given code. The model requires manually predefined control codes and a corresponding training corpus for each code. Other diverse generation works learn latent representations or codes from input text, and then generate text conditioned on those codes. Shu et al. (2019) applied a sentence embedding to generate syntactically diverse translations. They find that syntax-based encoding with TreeLSTM (Socher et al., 2011) yields better diversity than a contextual encoding using BERT (Devlin et al., 2019) or FastText (Bojanowski et al., 2017). Bao et al. (2020) used K categorical latent variables to control the generation context of dialogue responses and pick the highest probability response from the responses generated using the latent variables. COD3S (Weir et al., 2020) is designed to generate diverse causal relations. It uses locality-sensitive hashing (LSH) (Indyk and Motwani, 1998) on representations from Sentence-BERT (Reimers and Gurevych, 2019). Conditioning on these 16-bit LSH signatures, it generates cause/effect sentences using a Transformer architecture (Vaswani et al., 2017) but with a limited vocabulary size of 10K. These previous ap"
2021.starsem-1.15,Q17-1010,0,0.0066056,"domain, style, or topics. Then the model learns to generate text conditioned on a given code. The model requires manually predefined control codes and a corresponding training corpus for each code. Other diverse generation works learn latent representations or codes from input text, and then generate text conditioned on those codes. Shu et al. (2019) applied a sentence embedding to generate syntactically diverse translations. They find that syntax-based encoding with TreeLSTM (Socher et al., 2011) yields better diversity than a contextual encoding using BERT (Devlin et al., 2019) or FastText (Bojanowski et al., 2017). Bao et al. (2020) used K categorical latent variables to control the generation context of dialogue responses and pick the highest probability response from the responses generated using the latent variables. COD3S (Weir et al., 2020) is designed to generate diverse causal relations. It uses locality-sensitive hashing (LSH) (Indyk and Motwani, 1998) on representations from Sentence-BERT (Reimers and Gurevych, 2019). Conditioning on these 16-bit LSH signatures, it generates cause/effect sentences using a Transformer architecture (Vaswani et al., 2017) but with a limited vocabulary size of 10K"
2021.starsem-1.15,N19-1423,0,0.00600824,"ble control codes, which describe domain, style, or topics. Then the model learns to generate text conditioned on a given code. The model requires manually predefined control codes and a corresponding training corpus for each code. Other diverse generation works learn latent representations or codes from input text, and then generate text conditioned on those codes. Shu et al. (2019) applied a sentence embedding to generate syntactically diverse translations. They find that syntax-based encoding with TreeLSTM (Socher et al., 2011) yields better diversity than a contextual encoding using BERT (Devlin et al., 2019) or FastText (Bojanowski et al., 2017). Bao et al. (2020) used K categorical latent variables to control the generation context of dialogue responses and pick the highest probability response from the responses generated using the latent variables. COD3S (Weir et al., 2020) is designed to generate diverse causal relations. It uses locality-sensitive hashing (LSH) (Indyk and Motwani, 1998) on representations from Sentence-BERT (Reimers and Gurevych, 2019). Conditioning on these 16-bit LSH signatures, it generates cause/effect sentences using a Transformer architecture (Vaswani et al., 2017) but"
2021.starsem-1.15,2020.acl-main.225,0,0.0797603,"a model is asked to reconstruct the sentence including its precondition. For masking, the syntactic subtree of a precondition is replaced with [BLANK]. In order to indicate the events of interest – target and precondition – we use special tokens <event> .. </event> and <pre> .. </pre>. For our new task, instead of generating the entire sentence, we only generate a precondition clause that would fit into the input’s [BLANK]. Since a precondition could be stated in either preceding or succeeding position of its target event, we modeled this as a text infilling task. This approach is inspired by Donahue et al. (2020) and this modification allows the model to focus solely on generating preconditions because the model doesn’t need to copy over its input text. Thus, the model can learn faster and more efficiently. to provide explicit guidance to the model to explore diverse candidates. How can we get such diverse guidance? A main strength of large generative language models is that they learn to generate text that fits with the input context. If we can get the input context to be less specific then we can aim to get more general outputs. We can exploit this behavior by training a separate event sampler that"
2021.starsem-1.15,2020.acl-main.740,0,0.0201043,"Missing"
2021.starsem-1.15,N18-2008,0,0.0201983,"or example, Sil et al. (2010) identified preconditions using a SVM-based score function with hand-crafted PMI and WordNet based features. Branavan et al. (2012) extracted domain-specific precondition relations from instructions for the game of Minecraft. This paper is instead focused on generating novel preconditions. To the best of our knowledge, only the prior PeKo work (Kwon et al., 2020) has attempted this. We are building on those initial ideas. There has been research for diverse generation using control codes or latent variables. Some works use explicit cues to control text generation. Huang et al. (2018) used emotion embeddings to generate dialogue responses in a specific mood. Keskar 2 We will release the source code upon acceptance. et al. (2019) trained a LM with human readable control codes, which describe domain, style, or topics. Then the model learns to generate text conditioned on a given code. The model requires manually predefined control codes and a corresponding training corpus for each code. Other diverse generation works learn latent representations or codes from input text, and then generate text conditioned on those codes. Shu et al. (2019) applied a sentence embedding to gene"
2021.starsem-1.15,2020.findings-emnlp.340,1,0.740926,"iversity of preconditions significantly while also generating more preconditions. 1 Table 1: Top 5 preconditions generated from GPT-2 with beam search decoding. Key problem: the top 4 preconditions are almost identical. Introduction Preconditions are an important part of language understanding with numerous applications, ranging from event understanding to story generation. They provide the semantic glue to understand (or generate) the chains of events common in narrative text. How can we build intelligent systems to fill in these chains, or to identify semantically related events in context? Kwon et al. (2020) took a first step by introducing a precondition generation task, where given a target event mention the goal is to generate text that describes a precondition for the target. They released the ‘PeKo’ dataset for training, and showed that a GPT-2 model can be fine-tuned on input/output sequence pairs. While PeKo is useful, it is constrained by annotating a single relation for each target event. This is contrast to the real-world where most events have many preconditions. For example, “opening a door” has several preconditions like approaching the door, turning a key in the door, and pushing th"
2021.starsem-1.15,D19-1410,0,0.0128944,"ranslations. They find that syntax-based encoding with TreeLSTM (Socher et al., 2011) yields better diversity than a contextual encoding using BERT (Devlin et al., 2019) or FastText (Bojanowski et al., 2017). Bao et al. (2020) used K categorical latent variables to control the generation context of dialogue responses and pick the highest probability response from the responses generated using the latent variables. COD3S (Weir et al., 2020) is designed to generate diverse causal relations. It uses locality-sensitive hashing (LSH) (Indyk and Motwani, 1998) on representations from Sentence-BERT (Reimers and Gurevych, 2019). Conditioning on these 16-bit LSH signatures, it generates cause/effect sentences using a Transformer architecture (Vaswani et al., 2017) but with a limited vocabulary size of 10K. These previous approaches have some drawbacks – they either require explicit control codes and training examples, or they have low interpretability of their codes. Our approach addresses these two limitations: control codes are learned from non-diverse input text and the codes are human-readable events. And these approaches are not directly comparable to our method without proper modification, which would not be fa"
2021.starsem-1.15,2020.acl-main.704,0,0.0399484,"Missing"
2021.starsem-1.15,2020.emnlp-main.421,0,0.0617667,"Missing"
2021.starsem-1.15,2020.emnlp-demos.6,0,0.0342716,"Missing"
chambers-jurafsky-2010-database,W04-1017,0,\N,Missing
chambers-jurafsky-2010-database,W99-0211,0,\N,Missing
chambers-jurafsky-2010-database,S07-1014,0,\N,Missing
chambers-jurafsky-2010-database,W09-1111,0,\N,Missing
chambers-jurafsky-2010-database,N04-1038,0,\N,Missing
chambers-jurafsky-2010-database,P98-1013,0,\N,Missing
chambers-jurafsky-2010-database,C98-1013,0,\N,Missing
chambers-jurafsky-2010-database,P06-1095,0,\N,Missing
chambers-jurafsky-2010-database,P07-2044,1,\N,Missing
chambers-jurafsky-2010-database,P08-1090,1,\N,Missing
chambers-jurafsky-2010-database,P09-1068,1,\N,Missing
D08-1073,P07-2044,1,0.35018,"e individual events are further tagged for temporal information such as tense, modality and grammatical aspect. Time expressions use the TimeML (Ingria and Pustejovsky, 2002) markup language. There are 6 main relations and their inverses in Timebank: before, ibefore, includes, begins, ends and simultaneous. This paper describes work that classifies the relations between events, making use of relations between events and times, and between the times themselves to help inform the decisions. 4 dence scores is important for the global maximization step that is described in the next section. As in Chambers et al. (2007), we build support vector machine (SVM) classifiers and use the probabilities from pairwise SVM decisions as our confidence scores. These scores are then used to choose an optimal global ordering. Following our previous work, we use the set of features summarized in figure 1. They vary from POS tags and lexical features surrounding the event, to syntactic dominance, to whether or not the events share the same tense, grammatical aspect, or aspectual class. These features are the highest performing set on the basic 6-way classification of Timebank. Feature Word* Lemma* Synset* POS* POS bigram* P"
D08-1073,P06-1095,0,0.316594,"ot the events share the same tense, grammatical aspect, or aspectual class. These features are the highest performing set on the basic 6-way classification of Timebank. Feature Word* Lemma* Synset* POS* POS bigram* Prep* Tense* Aspect* Modal* Polarity* Class* Tense Pair Aspect Pair Class Pair POS Pair Tense Match Aspect Match Class Match Dominates The Global Model Text Order Our initial model has two components: (1) a pairwise classifier between events, and (2) a global constraint satisfaction layer that maximizes the confidence scores from the classifier. The first is based on previous work (Mani et al., 2006; Chambers et al., 2007) and the second is a novel contribution to event-event classification. 4.1 Pairwise Classification Classifying the relation between two events is the basis of our model. A soft classification with confi699 Entity Match Same Sent Description The text of the event The lemmatized head word The WordNet synset of head word 4 POS tags, 3 before, and 1 event The POS bigram of the event and its preceding tag Preposition lexeme, if in a prepositional phrase The event’s tense The event’s grammatical aspect The modality of the event Positive or negative The aspecual class of the e"
D08-1073,D08-1027,1,0.0281711,"Missing"
D08-1073,S07-1014,0,0.44595,"Work Recent work on classifying temporal relations within the Timebank Corpus built 6-way relation classifiers over 6 of the corpus’ 13 relations (Mani et al., 2006; Mani et al., 2007; Chambers et al., 2007). A wide range of features are used, ranging from surface indicators to semantic classes. Classifiers make 698 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 698–706, c Honolulu, October 2008. 2008 Association for Computational Linguistics local pairwise decisions and do not consider global implications between the relations. The TempEval-07 (Verhagen et al., 2007) contest recently used two relations, before and after, in a semi-complete textual classification task with a new third relation to distinguish relations that can be labeled with high confidence from those that are uncertain, called vague. The task was a simplified classification task from Timebank in that only one verb, the main verb, of each sentence was used. Thus, the task can be viewed as ordering the main events in pairwise sentences rather than the entire document. This paper uses the core relations of TempEval (before,after,vague) and applies them to a full document ordering task that"
D08-1073,W06-1623,0,\N,Missing
D10-1048,D08-1031,0,0.846856,"s. 1 The second attack occurred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps."
D10-1048,P06-1005,0,0.370589,"ttributes. These are crucial factors for pronominal coreference. Like previous work, we implement pronominal coreference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009)."
D10-1048,J93-2003,0,0.0155393,"ls entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. 3 Description of the Task Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To f"
D10-1048,W99-0613,0,0.0599343,"Missing"
D10-1048,N07-1011,0,0.106871,"in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and 493 Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inf"
D10-1048,P10-2007,0,0.0457833,"Missing"
D10-1048,P05-1045,1,0.02431,"onference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005"
D10-1048,P08-2012,1,0.850737,"rk This work builds upon the recent observation that strong features outweigh complex models for coreference resolution, in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and 493 Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the"
D10-1048,D09-1120,0,0.142423,"curred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps. We propose an unsupervised"
D10-1048,N10-1061,0,0.468981,"Missing"
D10-1048,Y09-1024,0,0.0248102,"reference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009). NER label – from the Stanford NER. If we cannot detect a value, we set attributes to unknown and treat th"
D10-1048,P03-1054,1,0.0100249,"a et al., 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009). It consists of 107 documents and 5,469 mentions. • ACE2004-NWIRE – the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. It contains 128 documents and 11,413 mentions. • MUC6-TEST – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many pr"
D10-1048,H05-1004,0,0.787257,"l., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. 4 Description of the Multi-Pass Sieve Our sieve framework is implemented as a succession of independent coreference models. We first describe how each model selects candidate mentions, and then describe the models themselves. 494 4.1 Mention Processing Given a mention mi , each model may either decline to propose a solution (in the hope that one of the subsequent models will solve it) or deterministically select a single best antecedent from a list of previous mentions m1 , . . . , mi−1 . We sort candidate antecedents using syntacti"
D10-1048,D08-1068,0,0.227799,"• ACE2004-ROTH-DEV2 – development split of Bengston and Roth (2008), from the corpus used in the 2004 Automatic Content Extraction (ACE) evaluation. It contains 68 documents and 4,536 mentions. 2 We use the same corpus names as (Haghighi and Klein, 2009) to facilitate comparison with previous work. • ACE2004-CULOTTA-TEST – partition of ACE 2004 corpus reserved for testing by several previous works (Culotta et al., 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009). It consists of 107 documents and 5,469 mentions. • ACE2004-NWIRE – the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. It contains 128 documents and 11,413 mentions. • MUC6-TEST – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with pr"
D10-1048,N10-1116,0,0.0131313,"er information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. 3 Description of the Task Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To facilitate comparison with most of the recent previous work, we report results using gold mention boundaries. However, our approach does not make any assumptions about the underlying mentions,"
D10-1048,M95-1005,0,0.961704,"ing the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. 4 Description of the Multi-Pass Sieve Our sieve framework is implemented as a succession of independent coreference models. We first describe h"
D10-1048,P09-1074,0,\N,Missing
D13-1185,P04-1056,0,0.0124028,"is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known"
D13-1185,P11-1098,1,0.583128,"knowledge? We present a new generative model for event schemas, Traditionally, template extractors assume foreknowledge of the event schemas. They know a Winner exists, and research focuses on supervised learning to extract winners from text. This paper focuses on the other side of the supervision spectrum. The learner receives no human input, and it first induces a schema before extracting instances of it. Our proposed model contributes to a growing line of research in schema induction. The majority of previous work relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). Chambers and Jurafsky is a pipelined approach, learning events first, and later learning syntactic patterns as fillers. It requires 1797 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics several ad-hoc metrics and parameters, and it lacks the benefits of a formal model. However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event schema. We adapt this entity-driven approach to"
D13-1185,P11-1054,0,0.0181884,"s (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same"
D13-1185,N13-1104,0,0.144469,"97–1807, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics several ad-hoc metrics and parameters, and it lacks the benefits of a formal model. However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event schema. We adapt this entity-driven approach to a single model that requires fewer parameters and far less training data. Further, experiments show stateof-the-art performance. Other research conducted at the time of this paper also proposes a generative model for schema induction (Cheung et al., 2013). Theirs is not entitybased, but instead uses a sequence model (HMMbased) of verb clauses. These two papers thus provide a unique opportunity to compare two very different views of document structure. One is entitydriven, modeling an entity’s role by its coreference chain. The other is clause-driven, classifying individual clauses based on text sequence. Each model makes unique assumptions, providing an interesting contrast. Our entity model outperforms by 7 F1 points on a common extraction task. The rest of the paper describes in detail our main contributions: (1) the first entity-based gener"
D13-1185,P03-1028,0,0.0612281,"elations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping wi"
D13-1185,J93-3001,0,0.138297,"information extraction usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005;"
D13-1185,P06-2027,0,0.0550852,"induced from raw text without prior knowledge? We present a new generative model for event schemas, Traditionally, template extractors assume foreknowledge of the event schemas. They know a Winner exists, and research focuses on supervised learning to extract winners from text. This paper focuses on the other side of the supervision spectrum. The learner receives no human input, and it first induces a schema before extracting instances of it. Our proposed model contributes to a growing line of research in schema induction. The majority of previous work relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). Chambers and Jurafsky is a pipelined approach, learning events first, and later learning syntactic patterns as fillers. It requires 1797 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics several ad-hoc metrics and parameters, and it lacks the benefits of a formal model. However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event sche"
D13-1185,P98-1067,0,0.063336,"els can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 201"
D13-1185,P10-2054,0,0.0165217,"The corpus is particularly challenging because template schemas are inter-mixed and entities can play multiple roles across instances. The training corpus contains 1300 documents, 733 of which are labeled with at least one schema. 567 documents are not labeled with any schemas. 1799 These unlabeled documents are articles that report on non-specific political events and speeches. They make the corpus particularly challenging. The development and test sets each contain 200 documents. 4 A Generative Model for Event Schemas This paper’s model is an entity-based approach, similar in motivation to Haghighi and Klein (2010) and the pipelined induction of Chambers and Jurafsky (2011). Coreference resolution guides the learning by providing a set of pre-resolved entities. Each entity receives a schema role label, so it allows all mentions of the entity to inform that role choice. This important constraint links coreferring mentions to the same schema role, and distinguishes our approach from others (Cheung et al., 2013). 4.1 Illustration The model represents a document as a set of entities. An entity is a set of entity mentions clustered by coreference resolution. We will use the following two sentences for illust"
D13-1185,P10-1029,0,0.0179526,"Missing"
D13-1185,P11-1114,0,0.478492,"., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber"
D13-1185,D07-1075,0,0.0773159,"modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don"
D13-1185,D09-1016,0,0.609307,"out labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown t"
D13-1185,M92-1008,0,0.379492,"usually learns binary relations and atomic facts. Models can learn relations like Person is married to Person without labeled data (Banko et al., 2007b), or rely on seed examples for ontology induction (dog is a mammal) and attribute extraction (dogs have tails) (Carlson et al., 2010b; Carlson et al., 2010a; Huang and Riloff, 2010; Durme and Pasca, 2008). These do not typically capture the deeper connections modeled by event schemas. Algorithms that do focus on event schema extraction typically require both the schemas and labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 20"
D13-1185,W98-1106,0,0.127757,"nd labeled corpora, such as rule-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeate"
D13-1185,P06-2094,0,0.133982,"without prior knowledge? We present a new generative model for event schemas, Traditionally, template extractors assume foreknowledge of the event schemas. They know a Winner exists, and research focuses on supervised learning to extract winners from text. This paper focuses on the other side of the supervision spectrum. The learner receives no human input, and it first induces a schema before extracting instances of it. Our proposed model contributes to a growing line of research in schema induction. The majority of previous work relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). Chambers and Jurafsky is a pipelined approach, learning events first, and later learning syntactic patterns as fillers. It requires 1797 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics several ad-hoc metrics and parameters, and it lacks the benefits of a formal model. However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event schema. We adapt t"
D13-1185,N06-1039,0,0.0161259,"examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. Chen et al. (2011) perform relation extraction with no supervision on earthquake and finance domains. Theirs is"
D13-1185,P03-1029,0,0.00959556,"-based approaches (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Huang and Riloff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. O"
D13-1185,H91-1059,0,0.630107,"Missing"
D13-1185,W06-2207,0,0.0711963,"ely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. Chen et al. (2011) perform relation extraction with no supervision on earthquake and"
D13-1185,C00-2136,0,0.0315315,"ff, 2011). Classifiers rely on 1798 the labeled examples’ surrounding context for features (Maslennikov and Chua, 2007). Weakly supervised learning removes some of the need for labeled data, but most still require the event schemas. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Filatova et al., 2006; Patwardhan and Riloff, 2007; Chen et al., 2011). Bootstrapping with seed examples of known slot fillers has been shown to be effective (Yangarber et al., 2000; Surdeanu et al., 2006). Shinyama and Sekine (2006) presented unrestricted relation discovery to discover relations in unlabeled documents. Their algorithm used redundant documents (e.g., all describe Hurricane Ivan) to observe repeated proper nouns. The approach requires many documents about the exact same event instance, and relations are binary (not schemas) over repeated named entities. Our model instead learns schemas from documents with mixed topics that don’t describe the same event, so repeated proper nouns are less helpful. Chen et al. (2011) perform relation extraction with no super"
D13-1185,C98-1064,0,\N,Missing
D15-1007,S13-2053,0,0.043074,"65 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 65–75, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. reviews sought the sentiment toward particular product features. These systems used rule based models based on parts of speech and surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Most notably, recent Semeval competitions addressed contextual polarity (Nakov et al., 2013; Rosenthal et al., 2014). The top performing systems learned their own lexicons custom to the domain (Mohammad et al., 2013; Zhu et al., 2014). Our proposed system includes many of their features, but several fail to help on nation-nation sentiment. recent competitions included contextual classification tasks, and this paper builds on the best of those algorithms for a unique nation-nation sentiment classifier. We describe a multi-classifier model that aggregates tweets into counts of positive and negative sentiment from one country toward another. Several unique filters are required to resolve textual references toward country names. We first present standard NLP sentiment experiments that show the classifiers ac"
D15-1007,C10-2028,0,0.0263393,"er focuses on identifying tweets with nation mentions, and identifying the sentiment toward the mention, not the overall sentiment of the text. Previous Work Sentiment analysis is a large field applicable to many genres. This paper focuses on social media and contextual polarity, so we only address the closest work in those areas. For a broader perspective, several survey papers are available (Pang and Lee, 2008; Tang et al., 2009; Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012). Several sources for microblogs have been used to measure a large population’s mood and opinion. O’Connor et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are early general sentiment algorithms for social media. Other microblog research focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity ("
D15-1007,S13-2052,0,0.0667348,"Missing"
D15-1007,P13-1108,0,0.110679,"Missing"
D15-1007,pak-paroubek-2010-twitter,0,0.0900954,"d negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are early general sentiment algorithms for social media. Other microblog research focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Our bootstrap learner is similar in its selection of seed tokens. Supervised learning for contextual polarity has received more attention recently. Jiang et al. (2011) is an early approach. Work on product Event detection on Twitter is also relevant (Sakaki et al., 2010; Becker et al., 2011). In fact, O’Connor et al. (2013) modeled events to detect international relations, but our goal is to model long term relation trends, not isolated events"
D15-1007,P05-2008,0,0.0170623,"used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are early general sentiment algorithms for social media. Other microblog research focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Our bootstrap learner is similar in its selection of seed tokens. Supervised learning for contextual polarity has received more attention recently. Jiang et al. (2011) is an early approach. Work on product Event detection on Twitter is also relevant (Sakaki et al., 2010; Becker et al., 2011). In fact, O’Connor et al. (2013) modeled events to detect international relations, but our"
D15-1007,P11-1016,0,0.0182087,"focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Our bootstrap learner is similar in its selection of seed tokens. Supervised learning for contextual polarity has received more attention recently. Jiang et al. (2011) is an early approach. Work on product Event detection on Twitter is also relevant (Sakaki et al., 2010; Becker et al., 2011). In fact, O’Connor et al. (2013) modeled events to detect international relations, but our goal is to model long term relation trends, not isolated events. Large-scale computational studies of social media are relatively new to the international relations community. Barbera and Rivero (2014) is a notable example for election analysis. Some studied online discussion about Palestine (Lynch, 2014) and the role of Twitter in the Arab Spring (Howard et al., 2011; Howard, 201"
D15-1007,S14-2077,0,0.0614766,"2015 Conference on Empirical Methods in Natural Language Processing, pages 65–75, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. reviews sought the sentiment toward particular product features. These systems used rule based models based on parts of speech and surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Most notably, recent Semeval competitions addressed contextual polarity (Nakov et al., 2013; Rosenthal et al., 2014). The top performing systems learned their own lexicons custom to the domain (Mohammad et al., 2013; Zhu et al., 2014). Our proposed system includes many of their features, but several fail to help on nation-nation sentiment. recent competitions included contextual classification tasks, and this paper builds on the best of those algorithms for a unique nation-nation sentiment classifier. We describe a multi-classifier model that aggregates tweets into counts of positive and negative sentiment from one country toward another. Several unique filters are required to resolve textual references toward country names. We first present standard NLP sentiment experiments that show the classifiers achieve good performa"
D15-1007,S14-2009,0,\N,Missing
D18-1413,D13-1178,1,0.857421,"ting a model of scripts based on self organizing maps (Kohonen, 1982). Interestingly, self organizing maps also utilize vector quantization during learning (albeit in a different way than done here). Recent work starting from Chambers and Jurafsky (2008) has focused on learning scripts as 3790 prototypical sequences of events using event cooccurrence. Further work has framed this task as a language modeling problem (Pichotta and Mooney, 2016; Rudinger et al., 2015; Peng and Roth, 2016). Other work has looked at learning more structured forms of script knowledge called schemas (Chambers, 2013; Balasubramanian et al., 2013; Nguyen et al., 2015) which focuses on additionally inducing script specific roles to be filled by entities. In this work we treat event components as separate tokens, though work has also looked into methods for composing this components into a single distributed event representation (Modi and Titov, 2014; Modi, 2016; Weber et al., 2018). We leave this as possible future work. The hierarchical structure of our proposed model is similar to structure of the latent space in other VAE variants (Sonderby et al., 2016; Zhao et al., 2017), with the discrete variables and attentions in our model bei"
D18-1413,K16-1002,0,0.602959,"l for large differences like restaurant vs crime, while the bottom selects between fancy and casual dining. The overall model, which we describe below, takes the form of an autoencoder, with an encoder network inferring values of the latents and a decoder conditioned on the latents generating scripts. We show the usefulness of these latent representations against a prior RNN language model based system (Pichotta and Mooney, 2016) on several tasks. We additionally evaluate the perplexity of the system against the RNN language model, a task that autoencoder models have typically struggled with (Bowman et al., 2016). We find that the latent tree reduces model perplexity by a significant amount, possibly indicating the usefulness of the model in a more general sense. 2 Background 2.1 Variational Autoencoders Variational Autoencoders (VAEs, Kingma and Welling (2014)) are generative models which learn latent codes z for the data x by maximizing a lower bound on the data likelihood: log(p(x)) ≥ Eq(z|x) [p(x|z)] − KL[q(z|x)||p(z)] VAEs consist of two components: an encoder which parameterizes the latent posterior q(z|x) and a decoder which parameterizes p(x|z). The objective function can be made completely di"
D18-1413,D13-1185,1,0.856371,"of scripts, creating a model of scripts based on self organizing maps (Kohonen, 1982). Interestingly, self organizing maps also utilize vector quantization during learning (albeit in a different way than done here). Recent work starting from Chambers and Jurafsky (2008) has focused on learning scripts as 3790 prototypical sequences of events using event cooccurrence. Further work has framed this task as a language modeling problem (Pichotta and Mooney, 2016; Rudinger et al., 2015; Peng and Roth, 2016). Other work has looked at learning more structured forms of script knowledge called schemas (Chambers, 2013; Balasubramanian et al., 2013; Nguyen et al., 2015) which focuses on additionally inducing script specific roles to be filled by entities. In this work we treat event components as separate tokens, though work has also looked into methods for composing this components into a single distributed event representation (Modi and Titov, 2014; Modi, 2016; Weber et al., 2018). We leave this as possible future work. The hierarchical structure of our proposed model is similar to structure of the latent space in other VAE variants (Sonderby et al., 2016; Zhao et al., 2017), with the discrete variables a"
D18-1413,W17-0905,1,0.82504,"Missing"
D18-1413,P08-1090,1,0.883931,"ion of hierarchies in scripts has been studied in the works of Abbott et al. (1985) and Bower et al. (1979). Mooney and DeJong (1985) present an early non probabilistic system for extracting scripts from text. A highly related work by Miikkulainen (1990) provides an early example of a system explicitly designed to take advantage of the hierarchical nature of scripts, creating a model of scripts based on self organizing maps (Kohonen, 1982). Interestingly, self organizing maps also utilize vector quantization during learning (albeit in a different way than done here). Recent work starting from Chambers and Jurafsky (2008) has focused on learning scripts as 3790 prototypical sequences of events using event cooccurrence. Further work has framed this task as a language modeling problem (Pichotta and Mooney, 2016; Rudinger et al., 2015; Peng and Roth, 2016). Other work has looked at learning more structured forms of script knowledge called schemas (Chambers, 2013; Balasubramanian et al., 2013; Nguyen et al., 2015) which focuses on additionally inducing script specific roles to be filled by entities. In this work we treat event components as separate tokens, though work has also looked into methods for composing th"
D18-1413,D14-1179,0,0.0224596,"Missing"
D18-1413,P16-1028,0,0.55306,"ssed case, and the right is a convicted suspect. Our model generated both tracks through a latent hierarchy. Introduction Scripts were originally proposed by Schank and Abelson (1977) as “structures that describe the appropriate sequence of events in a particular context”. These event sequences define expectations for how common scenarios (such as going to a restaurant) should unfold, thus enabling better language understanding. Although scripts represented many other factors (roles, entry conditions, outcomes) recent work in script induction (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016) has focused on language modeling (LM) approaches where the “appropriate sequence of events” is the textual order of events (tuples of event predicates and their arguments). Modeling a distribution of text sequences gives the intuitive interpretation of appropriate event sequences being roughly equivalent to probable textual sequences. We continue with an LM approach, but we tackle two very important LM problems that have not yet been addressed with regards to event sequence modeling. The first problem to address is that language models tend towards local coherency. Count based models are rest"
D18-1413,D14-1162,0,0.0904169,"atent variables, structured in the form of a linear chain (thus no variable has more than one child or parent). Each variable can initially take on K = 512 values, with all latents having an embeddings dimension of 256. The encoder RNN that performs the initial encoding of the event sequence is a bidirectional, single layer RNN with GRU cell (Cho et al., 2014) with a hidden dimension of 512. The inputs to this encoder are word embeddings derived from the onehot encodings of the tokens in the event sequence. The embeddings size is 300. We find initializing the embeddings with pretrained GloVe (Pennington et al., 2014) vectors to be useful. The decoder RNN is also a single layer RNN with GRU cells with a hidden dimension of 512 and 300 dimensional (initialized) word embeddings as inputs. For all experiments we use a vocabulary size of 50k. We train the model using Adam (Kingma and Ba, 2014) with a learning rate of 0.0005, and gradient clipping at 5.0. We find that the training converges around 1.5 epochs on our dataset. Further details can be found in our implementation3 4.3 Baselines We compare the performance of our proposed model against three previous baselines and a modification of our HAQAE model that"
D18-1413,D15-1195,0,0.414353,"Missing"
D18-1413,D15-1166,0,0.00997374,"ents. This is accomplished by parameterizing the encoding function for latent zi as an attention over the input x, with the parent of zi (more specifically, the embedding for the parent’s current value) acting as the ‘query’ vector. As is standard when using attention, the input sequence of events, x = (x1 , ...xn ), is first encoded into a sequence of hidden states hx = (h1 , ..., hn ) via a RNN encoder. The full encoding function for latent zi can thus be written as: fi (x, pr(zi )) = attn(hx , pr(zi )) Though any attention formulation is possible, we use the bi-linear attention proposed in Luong et al. (2015) in our implementations. For the root of the latent tree (z0 ), which has no parents, we use the averaged value of the encoder vectors hx as the query vector for its attention. We can define the decoder in a similar fashion. As is usually done, the distribution p(x|ze ) can be defined in an autoregressive manner using a RNN decoder network. Like the encoding process, different parts of the hierarchy may affect the generation of different parts of the input. We thus also allow the decoder network g(ze ) to be a RNN with a standard attention mechanism over the latent value embeddings, ze . Since"
D18-1413,D12-1048,0,0.0299499,"taining the verb, subject, object and preposition. Events without prepositions are given a null token in their preposition slot. The components of the events (the verb, subject, etc.) are all taken to be individual tokens, and can thus be treated more or 2 less like normal text. For example, the events (he played harp), (he touched moon), would be tokenized and given to the model as: played he harp null tup touched he moon null, where null is the null preposition token and tup is a special separation token between events. We extract event tuples using Open Information Extraction system Ollie (Mausam et al., 2012). We then group together event tuples for 4 subsequent sentences to create a single event sequence. We also ignore tuples with common (is, are, be, ...) and repeating predicates. Finally we have 7123097, 19425, and 28667 event sequences for training, dev, and test dataset respectively. For all the experiments we fix the minimum and maximum sequence lengths to be 8 and 50 respectively. 4.2 HAQAE Model Details The HAQAE model we use across all evaluations uses 5 discrete latent variables, structured in the form of a linear chain (thus no variable has more than one child or parent). Each variable"
D18-1413,K16-1008,0,0.0553915,"t cooccurrence. Further work has framed this task as a language modeling problem (Pichotta and Mooney, 2016; Rudinger et al., 2015; Peng and Roth, 2016). Other work has looked at learning more structured forms of script knowledge called schemas (Chambers, 2013; Balasubramanian et al., 2013; Nguyen et al., 2015) which focuses on additionally inducing script specific roles to be filled by entities. In this work we treat event components as separate tokens, though work has also looked into methods for composing this components into a single distributed event representation (Modi and Titov, 2014; Modi, 2016; Weber et al., 2018). We leave this as possible future work. The hierarchical structure of our proposed model is similar to structure of the latent space in other VAE variants (Sonderby et al., 2016; Zhao et al., 2017), with the discrete variables and attentions in our model being the major differences. Hu et al. (2017) present a VAE based model for controllable text generation, with different latents controlling different aspects of the generated text, but requiring labels for semi-supervision. Other methods using discrete variables for VAEs have also been proposed (Rolfe, 2017), as have var"
D18-1413,W14-1606,0,0.107602,"s of events using event cooccurrence. Further work has framed this task as a language modeling problem (Pichotta and Mooney, 2016; Rudinger et al., 2015; Peng and Roth, 2016). Other work has looked at learning more structured forms of script knowledge called schemas (Chambers, 2013; Balasubramanian et al., 2013; Nguyen et al., 2015) which focuses on additionally inducing script specific roles to be filled by entities. In this work we treat event components as separate tokens, though work has also looked into methods for composing this components into a single distributed event representation (Modi and Titov, 2014; Modi, 2016; Weber et al., 2018). We leave this as possible future work. The hierarchical structure of our proposed model is similar to structure of the latent space in other VAE variants (Sonderby et al., 2016; Zhao et al., 2017), with the discrete variables and attentions in our model being the major differences. Hu et al. (2017) present a VAE based model for controllable text generation, with different latents controlling different aspects of the generated text, but requiring labels for semi-supervision. Other methods using discrete variables for VAEs have also been proposed (Rolfe, 2017),"
D18-1413,1985.tmi-1.17,0,0.28701,"ads to the ending/beginning of the sequence changing or the entities of the sequence changing (but still remaining in the same topical domain). We additionally find that changing the top level latent may often preserve the overall form of the event sequence, and only transform the topic. We provide examples of these output by our system in Table 5. 6 Related Work Scripts were originally proposed by Schank and Abelson (1975) and further expanded upon in Schank and Abelson (1977). The notion of hierarchies in scripts has been studied in the works of Abbott et al. (1985) and Bower et al. (1979). Mooney and DeJong (1985) present an early non probabilistic system for extracting scripts from text. A highly related work by Miikkulainen (1990) provides an early example of a system explicitly designed to take advantage of the hierarchical nature of scripts, creating a model of scripts based on self organizing maps (Kohonen, 1982). Interestingly, self organizing maps also utilize vector quantization during learning (albeit in a different way than done here). Recent work starting from Chambers and Jurafsky (2008) has focused on learning scripts as 3790 prototypical sequences of events using event cooccurrence. Furth"
D18-1413,P15-1019,0,0.0833248,"Missing"
D19-5507,P17-1188,0,0.024051,"numbers. 4. Visual Deception and Unicode. This is a variant of ‘Letters as Digits’ above, but goes beyond ASCII substitution to use Unicode characters. Unicode presents a huge challenge to extraction as these rely entirely on visual similarities in the character images. Below are just some unicode options that resemble the ASCII character ‘8’: Most language models use words or characters as their base inputs. One of our contributions is a visual model of characters. We use an image database of 65k unicode characters developed by BBVA Next Security Lab1 for phishing prevention. Most similar is Liu et al. (2017) who use CNNs for Asian-language classification. They aren’t addressing noise like our paper, but rather the semantics inherent to their visual characters. 8 ! ! ! ! ? Ȣ ȣ ? ? ? ? A rule-based approach would have to manually map all possible characters to their digits, an impossible task for 138k current unicode characters (with future room for 1mil). This would also fail on the larger problem of capturing visually ambiguous close-matches. For instance, an emoticon smiley face can be used for the ASCII letter ‘o’: Finally, we employ data augmentation (Ding et al., 2016; Xu et al., 2016) during"
D19-5507,P16-1101,0,0.0255259,"yers. The network is remarkably adept at this, but we hypothesized that a better model should make a prediction on each and every input character rather than merging all into the same hidden state. Conditional Random Fields (Lafferty et al., 2001) are a natural way of modeling the above. A CRF tags each character as it goes, and performs both training and inference, using viterbi search to find the most likely output prediction sequence. Figure 4 shows this model. We used the CRF implementation in Keras inspired by (Huang et al., 2015) to overlay a CRF on top of the RNN-based models (see also Ma and Hovy (2016)). The output of a CRF is different since it must output a label for every character (rather than just 10 phone digits). We use the standard CRF labels to mark the beginning (B) and included (I) characters. This means that instead of a single label for each possible phone digit (e.g., 8), we now have two labels which represent a character that begins a digit (B8) and a character in the middle or end of a digit (I8). We additionally use an Other label ‘O’ to label the noisy separator characters that aren’t part of any digit’s substring. The following is an example: 41093four 2830 4109threeefour"
D19-5507,W17-4411,0,0.114139,"g Kong, Nov 4, 2019. 2019 Association for Computational Linguistics 2 Previous Work 3 Data and Attributes 3.1 A number of papers have looked into the sex trafficking domain. Some focus on classifying entire ads as trafficking or not (Alvari et al., 2016, 2017), while others build knowledge graphs of mentioned entities (Szekely et al., 2015) or focus on normalizing attributes like geolocations (Kapoor et al., 2017; Kejriwal and Szekely, 2017; Kejriwal et al., 2017). Most of these use phone numbers as features, and several found them to be among the most important input (Dubrawski et al., 2015; Nagpal et al., 2017; Li et al., 2018). In fact, phone numbers are used as gold truth to connect similar ads or link traffickers (Rabbany et al., 2018; Li et al., 2018). Phone numbers have also been shown to be some of the most stable links to entities (Costin et al., 2013), so are important for entity linking tasks. Almost all of these threads assume correct phone extraction and ignore the difficulty of ads with obscured numbers. Although sometimes unspecified, they all appear to use rulebased extractors. We begin by highlighting the main methods people use for adversarial noise in written text. This is not an e"
D19-5507,C16-1138,0,0.0232831,"ar is Liu et al. (2017) who use CNNs for Asian-language classification. They aren’t addressing noise like our paper, but rather the semantics inherent to their visual characters. 8 ! ! ! ! ? Ȣ ȣ ? ? ? ? A rule-based approach would have to manually map all possible characters to their digits, an impossible task for 138k current unicode characters (with future room for 1mil). This would also fail on the larger problem of capturing visually ambiguous close-matches. For instance, an emoticon smiley face can be used for the ASCII letter ‘o’: Finally, we employ data augmentation (Ding et al., 2016; Xu et al., 2016) during training of our visual character model. This is commonly used in the visual community (Salamon and Bello, 2017; Zhong et al., 2017) and we adopt their overall idea to randomly perturb our character images to learn a robust character recognizer. 1 Noisy and Obscured Data (4 !? 2) 456 9412 We are the first to our knowledge to model visual noise with a language model architecture. https://github.com/next-security-lab 49 Ad for Phone 555-584-4630 5. Confounding Separators. Another common noise tactic is to insert arbitrary characters as separators. For example: –270**1tree&&822==31–. The n"
E12-1062,C10-2005,0,0.0119511,"opularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received much less attention than sentiment analysis. The majority of approaches simply select a single keyword (e.g., “Obama”) to represent their topic (e.g., “US President”) and retrieve all tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011). The underlying assumption is that the keyword is precise, and due to the vast number of tweets, the search will return a large enough dataset to measure sentiment"
E12-1062,P11-2099,0,0.0124134,"training classifiers with a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ratings. In this paper we present a method for coupling two distantly supervised algorithms for topic identification and sentiment classification on Twitter. In Section 4, we describe our approach to topic identification and present a new annotated corpus of political tweets for future study. In Section 5, we apply distant supervision to sentiment analysis. Finally, Section 6"
E12-1062,C10-2028,0,0.275798,"m Twitter data. 2 Previous Work The past several years have seen sentiment analysis grow into a diverse research area. The idea of sentiment applied to microblogging domains is relatively new, but there are numerous recent publications on the subject. Since this paper focuses on the microblog setting, we concentrate on these contributions here. The most straightforward approach to sentiment analysis is using a sentiment lexicon to label tweets based on how many sentiment words appear. This approach tends to be used by applications that measure the general mood of a population. O’Connor et al. (2010) use a ratio of positive and negative word counts on Twitter, Kramer (2010) counts lexicon words on Facebook, and Thelwall (2011) uses the publicly available SentiStrength algorithm to make weighted counts of keywords based on predefined polarity strengths. In contrast to lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic"
E12-1062,P11-1016,0,0.0155328,"s appear. This approach tends to be used by applications that measure the general mood of a population. O’Connor et al. (2010) use a ratio of positive and negative word counts on Twitter, Kramer (2010) counts lexicon words on Facebook, and Thelwall (2011) uses the publicly available SentiStrength algorithm to make weighted counts of keywords based on predefined polarity strengths. In contrast to lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple mod"
E12-1062,P09-1113,0,0.0191467,"sidential Job Approval polls than previous work. Finally, we discover a surprising baseline that outperforms previous work without a Topic Identification stage. 1 Introduction Social networks and blogs contain a wealth of data about how the general public views products, campaigns, events, and people. Automated algorithms can use this data to provide instant feedback on what people are saying about a topic. Two challenges in building such algorithms are (1) identifying topic-relevant posts, and (2) identifying the attitude of each post toward the topic. This paper studies distant supervision (Mintz et al., 2009) as a solution to both challenges. We apply our approach to the problem of predicting Presidential Job Approval polls from Twitter data, and we present results that improve on previous work in this area. We also present a novel baseline that performs remarkably well without using topic identification. Topic identification is the task of identifying text that discusses a topic of interest. Most previous work on microblogs uses simple keyword searches to find topic-relevant tweets on the assumption that short tweets do not need more sophisticated processing. For instance, searches for the name “"
E12-1062,pak-paroubek-2010-twitter,0,0.266369,"t most microblog work focuses on two moods: positive and negative sentiment. 603 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 603–612, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics Algorithms to identify these moods range from matching words in a sentiment lexicon to training classifiers with a hand-labeled corpus. Since labeling corpora is expensive, recent work on Twitter uses emoticons (i.e., ASCII smiley faces such as :-( and :-)) as noisy labels in tweets for distant supervision (Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). This paper presents new analysis of the downstream effects of topic identification on sentiment classifiers and their application to political forecasting. Interest in measuring the political mood of a country has recently grown (O’Connor et al., 2010; Tumasjan et al., 2010; Gonzalez-Bailon et al., 2010; Carvalho et al., 2011; Tan et al., 2011). Here we compare our sentiment results to Presidential Job Approval polls and show that the sentiment scores produced by our system are positively correlated with both the Approval and Disapproval job ra"
E12-1062,P05-2008,0,0.453738,"and Thelwall (2011) uses the publicly available SentiStrength algorithm to make weighted counts of keywords based on predefined polarity strengths. In contrast to lexicons, many approaches instead focus on ways to train supervised classifiers. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al., 2011). Instead, distant supervision has grown in popularity. These algorithms use emoticons to serve as semantic indicators for sentiment. For instance, a sad face (e.g., :-() serves as a noisy label for a negative mood. Read (2005) was the first to suggest emoticons for UseNet data, followed by Go et al. (Go et al., 2009) on Twitter, and many others since (Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). Hashtags (e.g., #cool and #happy) have also been used as noisy sentiment labels (Davidov et al., 2010; Kouloumpis et al., 2011). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). Here, we adopt the emoticon algorithm for sentiment analysis, and evaluate it on a specific domain (politics). Topic identification in Twitter has received m"
E12-1062,H05-1044,0,0.0306471,"Missing"
I17-1085,S07-1025,0,0.0940754,"Missing"
I17-1085,W09-1206,0,0.0862807,"Missing"
I17-1085,W06-1623,0,0.236226,"hat describe a series of events rarely do so in order. Basic rules of journalism dictate that important information leads a news report, and accordingly, algorithms that re-order events chronologically need to combine a wealth of contextual, rhetorical, and commonsense information. Most research on event ordering aims to produce only partial orderings of event mentions 843 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 843–853, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009). CAEVO followed these and other hybrid rule-based approaches (D’Souza and Ng, 2013), but with the transitivity constraints yielding larger gains in performance for the more complete temporal graph constructed on the TimeBankDense corpus (Cassidy et al., 2014; Chambers et al., 2014). The TimeBank-Dense corpus provides a significantly more dense and complete set of annotations compared to previous corpora.1 TimeBank-Dense extends a subset of the original TimeBank corpus with annotations for (almost) all event-time, t"
I17-1085,P14-2082,1,0.898688,"t in spite of the density of this corpus, there is still a danger of overfitting. While this paper focuses on temporal ordering, its results are applicable to other areas that use sievebased architectures. 1 Richer datasets are becoming available that provide more complete event orderings which include logically implied relations that are less evident from local text features. In particular, the TimeBank-Dense corpus provides a significantly more dense and complete set of annotations, allowing for the evaluation of methods that make use of broad contextual information across many event pairs (Cassidy et al., 2014). One method that has been developed to leverage such information is CAEVO—a sieve-based architecture that made the first effort toward dense event ordering (Chambers et al., 2014). This method maintains transitivity constraints across independent predictions from several specialized classifiers. More specifically, the architecture runs a series of “sieve” classifiers with their predictions ranked in order by precision using a held-out dataset. The higher precision classifiers are ranked more highly in the series, and predictions are expanded by transitivity rules (e.g. if event e1 is before e"
I17-1085,Q14-1022,1,0.296845,"sievebased architectures. 1 Richer datasets are becoming available that provide more complete event orderings which include logically implied relations that are less evident from local text features. In particular, the TimeBank-Dense corpus provides a significantly more dense and complete set of annotations, allowing for the evaluation of methods that make use of broad contextual information across many event pairs (Cassidy et al., 2014). One method that has been developed to leverage such information is CAEVO—a sieve-based architecture that made the first effort toward dense event ordering (Chambers et al., 2014). This method maintains transitivity constraints across independent predictions from several specialized classifiers. More specifically, the architecture runs a series of “sieve” classifiers with their predictions ranked in order by precision using a held-out dataset. The higher precision classifiers are ranked more highly in the series, and predictions are expanded by transitivity rules (e.g. if event e1 is before e2 , and e2 is before e3 , then e1 is before e3 ) after each individual classifier generates its predictions. The high denIntroduction Narratives that describe a series of events ra"
I17-1085,D08-1073,1,0.861225,"Missing"
I17-1085,S13-2002,0,0.0521076,"s, we describe alternative evaluations on other splits of the data in order to analyze the effect of the common small sizes of temporal corpora like TimeBank-Dense. This analysis is critical for future work in temporal ordering, and sheds further light on previous work’s results. 2 Related Work Early work on event ordering focused on developing machine-learned classifiers that label the temporal relations between small subsets of pairs of events within documents using lexical and syntactical features (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). Later work leveraged information across pairwise predictions by imposing transitivity constraints using techniques 1 The new corpus is the result of several TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2012) which prompted efforts to develop more complete event ordering annotations (Bramsen et al., 2006; Kolomiyets et al., 2012; Do et al., 2012). 844 “sieve” classifiers generate predictions, and predictions from the more reliable sieves earlier in the series inform the predictions of the less reliable sieves later in the series. Generally, the predictions from a highly"
I17-1085,S07-1052,0,0.0847338,"Missing"
I17-1085,D12-1062,0,0.021297,"iers that label the temporal relations between small subsets of pairs of events within documents using lexical and syntactical features (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). Later work leveraged information across pairwise predictions by imposing transitivity constraints using techniques 1 The new corpus is the result of several TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2012) which prompted efforts to develop more complete event ordering annotations (Bramsen et al., 2006; Kolomiyets et al., 2012; Do et al., 2012). 844 “sieve” classifiers generate predictions, and predictions from the more reliable sieves earlier in the series inform the predictions of the less reliable sieves later in the series. Generally, the predictions from a highly-ranked sieve can inform a lowranked sieve in several ways, but within CAEVO, predictions from early classifiers are coupled via transitive inference rules to generate an expanding set of predictions that override output from less reliable classifiers later on in the series. In the next section, we describe a more generic view of this architecture which will motivate al"
I17-1085,N13-1112,0,0.0365264,"Missing"
I17-1085,C16-1265,0,0.0741521,"predictions from a given sieve fˆ. Intuitively, if we want to produce a higher accuracy architecture, then we should adjust the scoring function s to score all correct predictions more highly than all incorrect predictions6 . CAEVO’s use of fˆ precision in computing s is a coarse-grained heuristic in line with this goal, but there are better choices. Given that recent work has shown improvements using word embeddings (using log-linear neural language models such as the Skip-Gram architecture), we extend feature vectors with the word vectors representing events and their similarity. Following Mirza and Tonelli (2016), we use the three million 300-dimensional word2vec vectors 5 pre-trained on part of the Google News dataset (Mikolov et al., 2013). For each token span corresponding to either an event mention or time expression in a given pair datum, we extend the feature vector with normalized sums of word vectors computed from tokens of the span. In addition, we include the cosine similarity between the vectors for the events in a pair, as well as a vector representing the normalized difference between the pair’s vectors. Micro-averaged F1 scores on the TimeBank-Dense test set with these word embedIdeal Sc"
I17-1085,D15-1087,0,0.0490209,"Missing"
I17-1085,J02-3001,0,0.0177649,"Missing"
I17-1085,P12-1010,0,0.0269182,"g machine-learned classifiers that label the temporal relations between small subsets of pairs of events within documents using lexical and syntactical features (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). Later work leveraged information across pairwise predictions by imposing transitivity constraints using techniques 1 The new corpus is the result of several TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2012) which prompted efforts to develop more complete event ordering annotations (Bramsen et al., 2006; Kolomiyets et al., 2012; Do et al., 2012). 844 “sieve” classifiers generate predictions, and predictions from the more reliable sieves earlier in the series inform the predictions of the less reliable sieves later in the series. Generally, the predictions from a highly-ranked sieve can inform a lowranked sieve in several ways, but within CAEVO, predictions from early classifiers are coupled via transitive inference rules to generate an expanding set of predictions that override output from less reliable classifiers later on in the series. In the next section, we describe a more generic view of this architecture whic"
I17-1085,C08-1108,0,0.0755747,"Missing"
I17-1085,S10-1062,0,0.0466588,"Missing"
I17-1085,J13-4004,1,0.874,"Missing"
I17-1085,S07-1014,0,0.0835144,"uture work in temporal ordering, and sheds further light on previous work’s results. 2 Related Work Early work on event ordering focused on developing machine-learned classifiers that label the temporal relations between small subsets of pairs of events within documents using lexical and syntactical features (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). Later work leveraged information across pairwise predictions by imposing transitivity constraints using techniques 1 The new corpus is the result of several TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2012) which prompted efforts to develop more complete event ordering annotations (Bramsen et al., 2006; Kolomiyets et al., 2012; Do et al., 2012). 844 “sieve” classifiers generate predictions, and predictions from the more reliable sieves earlier in the series inform the predictions of the less reliable sieves later in the series. Generally, the predictions from a highly-ranked sieve can inform a lowranked sieve in several ways, but within CAEVO, predictions from early classifiers are coupled via transitive inference rules to generate an expanding set of predictions tha"
I17-1085,D12-1045,0,0.084937,"Missing"
I17-1085,P09-1046,0,0.0607048,"Missing"
I17-1085,S10-1063,0,0.0274938,"increase). Beyond this, we describe alternative evaluations on other splits of the data in order to analyze the effect of the common small sizes of temporal corpora like TimeBank-Dense. This analysis is critical for future work in temporal ordering, and sheds further light on previous work’s results. 2 Related Work Early work on event ordering focused on developing machine-learned classifiers that label the temporal relations between small subsets of pairs of events within documents using lexical and syntactical features (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). Later work leveraged information across pairwise predictions by imposing transitivity constraints using techniques 1 The new corpus is the result of several TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2012) which prompted efforts to develop more complete event ordering annotations (Bramsen et al., 2006; Kolomiyets et al., 2012; Do et al., 2012). 844 “sieve” classifiers generate predictions, and predictions from the more reliable sieves earlier in the series inform the predictions of the less reliable sieves later in the series. Generally, the predictio"
I17-1085,S10-1010,0,\N,Missing
I17-1085,P15-2059,0,\N,Missing
J13-4004,W97-1306,0,0.430065,"le-based systems relied on hand-tuned weights and were not capable of global inference, two factors that led to poor performance and replacement by machine learning. We propose a new approach that brings together the insights of these modern supervised and unsupervised models with the advantages of deterministic, rule-based systems. We introduce a model that performs entity-centric coreference, where all mentions that point to the same real-world entity are jointly modeled, in a rich feature space using solely simple, deterministic rules. Our work is inspired both by the seminal early work of Baldwin (1997), who first proposed that a series of high-precision rules could be used to build a high-precision, low-recall system for anaphora resolution, and by more recent work that has suggested that deterministic rules can outperform machine learning models for coreference (Zhou and Su 2004; Haghighi and Klein 2009) and for named entity recognition (Chiticariu et al. 2010). Figure 1 illustrates the two main stages of our new deterministic model: mention detection and coreference resolution, as well as a smaller post-processing step. In the mention detection stage, nominal and pronominal mentions are i"
J13-4004,P12-1041,0,0.0506069,"Missing"
J13-4004,D08-1031,0,0.592969,"Missing"
J13-4004,P06-1005,0,0.313052,"rs for pronominal coreference. We implement pronominal coreference resolution using an approach standard for many decades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: r r r r r r Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular and plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from Bergsma and Lin (2006). Gender – we assign gender attributes from static lexicons from Bergsma and Lin (2006), and Ji and Lin (2009). Person – we assign person attributes only to pronouns. We do not enforce this constraint when linking two pronouns, however, if one appears within quotes. This is a simple heuristic for speaker detection (e.g., I and she point to the same person in “[I] voted my conscience,” [she] said). Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a dictionary bootstrapped from the Web (Ji and Lin"
J13-4004,P08-1002,0,0.0325892,"Missing"
J13-4004,C82-1006,0,0.137371,"coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solutions combines the intuitions of “shaping” or “successive approximations” first proposed for learning by Skinner (1938), and widely used in NLP (e.g., the successively trained IBM MT models of Brown et al. [1993]) and the “islands of reliability” approaches to parsing and speech recognition [Borghesi and Favareto 1982; Corazza et al. 1991]). The idea of beginning with a high-recall list of candidates that are followed by a series of high-precision filters dates back to one of the earliest architectures in natural language processing, the part of speech tagging algorithm of the Computational Grammar Coder (Klein and Simmons 887 Computational Linguistics Volume 39, Number 4 1963) and the TAGGIT tagger (Greene and Rubin 1971), which begin with a high-recall list of all possible tags for words, and then used high-precision rules to filter likely tags based on context. In the next section we walk through an exa"
J13-4004,W05-0406,0,0.105348,"Missing"
J13-4004,P87-1022,0,0.348698,"Missing"
J13-4004,J93-2003,0,0.0334508,"Missing"
J13-4004,W11-1907,0,0.0406925,"Missing"
J13-4004,W99-0611,0,0.559667,"Missing"
J13-4004,W11-1904,0,0.0068184,"tter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and methodology, our system generally outperforms the previous state of the art. In the CoNLL shared task, 900 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules our system scores 1.8 CoNLL F1 points higher than the next system in the closed track and 2.6 points higher than the second-ranked system in the open track. The Chang et al. (2011) system has marginally higher B3 and BLANC F1 scores, but does not outperform our model on the other two metrics and the average F1 score. Table 5 shows that our model has higher B3 F1 scores than all the other models in the two ACE corpora. The model of Haghighi and Klein (2009) minimally outperforms ours by 0.6 B3 F1 points in the MUC corpus. All in all, these results prove that our approach compares favorably with a wide range of models, which include most aspects deemed important for coreference resolution, among other things, supervised learning using ´ and Turmo 2011; Chang et al. 2011),"
J13-4004,W12-4504,0,0.0360779,"Missing"
J13-4004,D10-1098,0,0.062478,"Missing"
J13-4004,W99-0613,0,0.44242,"Missing"
J13-4004,1991.iwpt-1.24,0,0.0410389,"Missing"
J13-4004,N07-1011,0,0.284716,"or example, even if we start with a perfect set of gold mentions, if we miss all coreference relations in a text, every mention will remain as a singleton and will be removed by the OntoNotes post Table 5 Comparison of our system with the other reported results on the ACE and MUC corpora. All these systems use gold mention boundaries. System B3 MUC R P F1 R P F1 74.5 78.5 73.2 74.5 88.7 79.6 86.7 88.3 81.0 79.0 79.3 80.8 74.1 74.5 – 65.2 87.3 79.4 – 86.8 80.2 76.9 – 74.5 63.1 67.3 – 49.7 90.6 84.7 – 90.9 74.4 75.0 – 64.3 ACE2004-Culotta-Test This paper Haghighi and Klein (2009) Culotta et al. (2007) Bengston and Roth (2008) 70.2 77.7 – 69.9 82.7 74.8 – 82.7 75.9 79.6 – 75.8 ACE2004-nwire This paper Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) 75.1 75.9 70.5 58.5 84.6 77.0 71.3 78.7 79.6 76.5 70.9 67.1 MUC6-Test This paper Haghighi and Klein (2009) Poon and Domingos (2008) Finkel and Manning (2008) 69.1 77.3 75.8 55.1 90.6 87.2 83.0 89.7 78.4 81.9 79.2 68.3 901 Computational Linguistics Volume 39, Number 4 processing, resulting in zero mentions in the final output. Therefore, we included the score using gold mention boundaries in the last part of Table 4 (“"
J13-4004,H05-1013,0,0.0270679,"Missing"
J13-4004,W08-1301,0,0.0240857,"Missing"
J13-4004,N07-1030,0,0.190945,"Missing"
J13-4004,doddington-etal-2004-automatic,0,0.0156715,"ible that . . . , It seems that . . . , It turns out . . . ). The complete set of patterns, using the tregex2 notation, is shown in Appendix B. 5. We discard adjectival forms of nations or nationality acronyms (e.g., American, U.S., U.K.), following the OntoNotes annotation guidelines. 6. We remove stop words from the following list determined by error analysis on mention detection: there, ltd., etc, ’s, hmm. Note that some rules change depending on the corpus we use for evaluation. In particular, adjectival forms of nations are valid mentions in the Automated Content Extraction (ACE) corpus (Doddington et al. 2004), thus they would not be removed when processing this corpus. 3.2 Resolution Architecture Traditionally, coreference resolution is implemented as a quadratic problem, where potential coreference links between any two mentions in a document are considered. This is not ideal, however, as it increases both the likelihood of errors and the processing time. In this article, we argue that it is better to cautiously construct high-quality mention clusters,3 and use an entity-centric model that allows the sharing of information across these incrementally constructed clusters. We achieve these goals by"
J13-4004,P10-2007,0,0.0175971,"Notes corpus, this sieve does not enhance recall significantly, mainly because appositions and predicate nominatives are not annotated in this corpus (they are annotated in ACE). Regardless of annotation standard, however, this sieve is important because it grows entities with high quality elements, which has a significant impact on the entity’s features (as discussed in Section 3.2.3). 3.3.5 Pass 5 – Strict Head Match. Linking a mention to an antecedent based on the naive matching of their head words generates many spurious links because it completely ignores possibly incompatible modifiers (Elsner and Charniak 2010). For example, Yale University and Harvard University have similar head words, but they are obviously different entities. To address this issue, this pass implements several constraints that must all be matched in order to yield a link: r Entity head match – the mention head word matches any head word of mentions in the antecedent entity. Note that this feature is actually more relaxed than naive head matching in a pair of mentions because here it is satisfied when the mention’s head matches the head of any mention in the candidate entity. We constrain this feature by enforcing a conjunction w"
J13-4004,W12-4502,0,0.381614,"Missing"
J13-4004,P05-1045,0,0.191986,"Missing"
J13-4004,P08-2012,0,0.146569,"Missing"
J13-4004,P07-2027,1,0.571167,"Missing"
J13-4004,P07-1107,0,0.299269,"Missing"
J13-4004,D09-1120,0,0.188162,"ng knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution."
J13-4004,N10-1061,0,0.481142,"Missing"
J13-4004,Y09-1024,0,0.646111,"ecades: enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: r r r r r r Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular and plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from Bergsma and Lin (2006). Gender – we assign gender attributes from static lexicons from Bergsma and Lin (2006), and Ji and Lin (2009). Person – we assign person attributes only to pronouns. We do not enforce this constraint when linking two pronouns, however, if one appears within quotes. This is a simple heuristic for speaker detection (e.g., I and she point to the same person in “[I] voted my conscience,” [she] said). Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels (e.g., PERSON is animate whereas LOCATION is not); and (c) a dictionary bootstrapped from the Web (Ji and Lin 2009). NER label – from the Stanford NER. Pronoun distance - sentence distance between a pronoun and its ante"
J13-4004,W97-0319,0,0.282039,"Missing"
J13-4004,P03-1054,0,0.0128291,"ition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development and all others for the formal evaluation. We parsed all documents in the ACE and MUC corpora using the Stanford parser (Klein and Manning 2003) and the Stanford NER (Finkel, Grenager, and Manning 2005). We used the provided parse Table 3 Corpora statistics. Corpora OntoNotes-Dev OntoNotes-Test ACE2004-Culotta-Test ACE2004-nwire MUC6-Test 898 # Documents # Sentences # Words # Entities # Mentions 303 322 107 128 30 6,894 8,262 1,993 3,594 576 136K 142K 33K 74K 13K 3,752 3,926 2,576 4,762 496 14,291 16,291 5,455 11,398 2,136 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the comparison with other systems. 4.2"
J13-4004,P11-1079,0,0.0279236,"Missing"
J13-4004,J94-4002,0,0.268022,"guistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and globa"
J13-4004,W11-1902,1,0.676764,"Missing"
J13-4004,H05-1004,0,0.725061,"Missing"
J13-4004,P04-1018,0,0.0538826,"understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making"
J13-4004,P00-1023,0,0.102077,"ral language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more"
J13-4004,D08-1067,0,0.32593,"Missing"
J13-4004,N09-1065,0,0.134029,"shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule-based models like Lappin and Leass (1994) wer"
J13-4004,P10-1142,0,0.165739,"Missing"
J13-4004,C02-1139,0,0.652854,"ics Computational Linguistics Volume 39, Number 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully des"
J13-4004,P02-1014,0,0.574409,"Missing"
J13-4004,P04-1019,0,0.0115911,"ber 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric"
J13-4004,W04-0707,0,0.046836,"ber 4 Arabic. Our system thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric"
J13-4004,D08-1068,0,0.551661,"n extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems and genres as well. Rule"
J13-4004,W12-4501,0,0.631873,"Missing"
J13-4004,W11-1901,0,0.834001,"ang Zhou and a consultant. . . are removed in this stage. 4. Experimental Results We start this section with overall results on three corpora widely used for the evaluation of coreference resolution systems. We continue with a series of ablative experiments that analyze the contribution of each aspect of our approach and conclude with error analysis, which highlights cases currently not solved by our approach. 4.1 Corpora We used the following corpora for development and formal evaluation: r r r r r OntoNotes-Dev – development partition of OntoNotes v4.0 provided in the CoNLL2011 shared task (Pradhan et al. 2011). OntoNotes-Test – test partition of OntoNotes v4.0 provided in the CoNLL-2011 shared task. ACE2004-Culotta-Test – partition of the ACE 2004 corpus reserved for testing by several previous studies (Culotta et al. 2007; Bengtson and Roth 2008; Haghighi and Klein 2009). ACE2004-nwire – newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. MUC6-Test – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. The corpora statistics are shown in Table 3. We used the first corpus (OntoNotes-Dev) for development a"
J13-4004,D10-1048,1,0.78311,"s of i and j but also any information (head word, named entity type, gender, or number) about the other mentions already linked to i and j in previous steps. Finally, the architecture is highly modular, which means that additional coreference resolution models can be easily integrated. The two stage architecture offers a powerful way to balance both high recall and precision in the system and make use of entity-level information with rule-based architecture. The mention detection stage heavily favors recall, and the following sieves favor precision. Our results here and in our earlier papers (Raghunathan et al. 2010; Lee et al. 2011) show that this design leads to state-of-the-art performance despite the simplicity of the individual components, and that the lack of language-specific lexical features makes the system easy to port to other languages. The intuition is not new; in addition to the prior coreference work mentioned earlier and discussed in Section 6, we draw on classic ideas that have proved to be important again and again in the history of natural language processing. The idea of beginning with the most accurate models or starting with smaller subproblems that allow for high-precision solution"
J13-4004,D09-1101,0,0.0519169,"Missing"
J13-4004,P10-1144,0,0.0676267,"Missing"
J13-4004,N13-1110,0,0.0170762,"Missing"
J13-4004,W11-1903,0,0.0811866,"Missing"
J13-4004,W12-4514,0,0.0215518,"Missing"
J13-4004,J01-4004,0,0.898996,"Missing"
J13-4004,N10-1116,0,0.0385068,"Missing"
J13-4004,C12-1154,0,0.107033,"Missing"
J13-4004,J00-4003,0,0.143805,"Labor Party wants credit controls. • Parser or NER error: Um alright uh Mister Zalisko do you know anything from your personal experience of having been on the cruise as to what happened? – Mister Zalisko is not recognized as a PERSON • Enumerations: This year, the economies of the five large special economic zones, namely, Shenzhen, Zhuhai, Shantou, Xiamen and Hainan, have maintained strong growth momentum. . . . A three dimensional traffic frame in Zhuhai has preliminarily taken shape and the investment environment improves daily. add more sophisticated anaphoricity detection to our system (Vieira and Poesio 2000; Ng and Cardie 2002a; Poesio et al. 2004b; Boyd, Gegg-Harrison, and Byron 2005; Gupta, Purver, and Jurafsky 2007; Bergsma, Lin, and Goebel 2008; Ng 2009). Event mentions. Our system was tailored for the resolution of entity coreference and does not have any event-specific features, such as, for example, matching event participants. Furthermore, our model considers only noun phrases as antecedent candidates, thus missing all mentions that are verbal phrases. Therefore, our system misses most coreference links between event mentions. For example, in Table 12 the pronoun That 907 Computational L"
J13-4004,M95-1005,0,0.945762,"entions 303 322 107 128 30 6,894 8,262 1,993 3,594 576 136K 142K 33K 74K 13K 3,752 3,926 2,576 4,762 496 14,291 16,291 5,455 11,398 2,136 Lee et al. Deterministic Coreference Resolution Based on Entity-Centric, Precision-Ranked Rules trees and named entity labels (not gold) in the OntoNotes corpora to facilitate the comparison with other systems. 4.2 Evaluation Metrics We use five evaluation metrics widely used in the literature. B3 and CEAF have implementation variations in how to take system mentions into account. We followed the same implementation as used in CoNLL-2011 shared task. r MUC (Vilain et al. 1995) – link-based metric which measures how many predicted and gold mention clusters need to be merged to cover the gold and predicted clusters, respectively.  (|Gi |−|p(Gi ) |)  (Gi : a gold mention cluster, p(Gi ): partitions of Gi ) R= (|G |−1)  r r r r P= F1 = i (|Si |−|p(Si ) |)  (|Si |−1) 2PR P+ R (Si : a system mention cluster, p(Si ): partitions of Si ) B3 (Bagga and Baldwin 1998) – mention-based metric which measures the proportion of overlap between predicted and gold mention clusters for a given mention. When Gmi is the gold cluster of mention mi and Smi is the system cluster of men"
J13-4004,W12-3204,1,0.511811,"62.1 60.1 65.5 61.4 56.8 55.0 58.8 68.4 37.2 63.9 62.1 59.8 59.5 59.6 61.5 51.6 55.5 35.2 69.5 68.3 62.2 64.5 73.2 77.1 53.9 54.4 55.5 70.6 65.2 76.7 70.3 62.2 52.5 73.4 70.2 68.2 70.0 66.7 68.7 67.3 67.3 62.5 62.2 61.3 61.2 resources. For the closed track, the organizers provided dictionaries for gender and number information, in addition to parse trees and named entity labels (Pradhan et al. 2011). For the open track, we used the following additional resources: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources (Vogel and Jurafsky 2012) (b) an animacy list (Ji and Lin 2009), (c) a country and state gazetteer, and (d) a demonym list. These resources were also used for the results reported in Table 5. A significant difference between Tables 4 and 5 is that in the former (other than its last block) we used predicted mentions (detected with the algorithm described in Section 3.1), whereas in the latter we used gold mentions. The only reason for this distinction is to facilitate comparison with previous work (all systems listed in Table 5 used gold mention boundaries). The two tables show that, regardless of evaluation corpus and"
J13-4004,W12-4506,0,0.0271675,"Missing"
J13-4004,P07-1067,0,0.0314736,"Missing"
J13-4004,P08-1096,0,0.147619,"ng, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune and difficult to apply to new problems"
J13-4004,C04-1033,0,0.0520591,"ks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with machine learning approaches to coreference resolution. But machine learning, although powerful, has limitations. Supervised machine learning systems rely on expensive hand-labeled data sets and generalize poorly to new words or domains. Unsupervised systems are increasingly more complex, making them hard to tune"
J13-4004,W12-4507,0,0.0515344,"Missing"
J13-4004,W12-4510,0,0.0210587,"Missing"
J13-4004,C04-1075,0,0.276472,"tem thus offers a new paradigm for combining knowledge in rule-based systems that has implications throughout computational linguistics. 1. Introduction Coreference resolution, the task of finding all expressions that refer to the same entity in a discourse, is important for natural language understanding tasks like summarization, question answering, and information extraction. The long history of coreference resolution has shown that the use of highly precise lexical and syntactic features is crucial to high quality resolution (Ng and Cardie 2002b; Lappin and Leass 1994; Poesio et al. 2004a; Zhou and Su 2004; Bengtson and Roth 2008; Haghighi and Klein 2009). Recent work has also shown the importance of global inference—performing coreference resolution jointly for several or all mentions in a document—rather than greedily disambiguating individual pairs of mentions (Morton 2000; Luo et al. 2004; Yang et al. 2004; Culotta et al. 2007; Yang et al. 2008; Poon and Domingos 2008; Denis and Baldridge 2009; Rahman and Ng 2009; Haghighi and Klein 2010; Cai, Mujdricza-Maydt, and Strube 2011). Modern systems have met this need for carefully designed features and global or entity-centric inference with mach"
J13-4004,D12-1045,1,\N,Missing
J13-4004,J14-4004,0,\N,Missing
N16-1098,W08-2227,1,0.249101,"Missing"
N16-1098,D13-1178,0,0.046906,"ured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008), but teasing out useful info"
N16-1098,P13-1035,0,0.0363528,"esentations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques per"
N16-1098,D15-1075,0,0.0394456,"ressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a r"
N16-1098,P08-1090,1,0.58165,"used on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computation"
N16-1098,P09-1068,1,0.635097,"77). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008"
N16-1098,D13-1185,1,0.266302,"and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Picho"
N16-1098,N13-1104,1,0.900888,"ical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008), but teasing out useful information from noisy bl"
N16-1098,W07-1401,0,0.0338919,"Test’. 4.1 Story Cloze Test The cloze task (Taylor, 1953) is used to evaluate a human (or a system) for language understanding by deleting a random word from a sentence and having a human fill in the blank. We introduce ‘Story Cloze Test’, in which a system is given a four-sentence ‘context’ and two alternative endings to the story, called ‘right ending’ and ‘wrong ending’. Hence, in this test the fifth sentence is blank. Then the system’s task is to choose the right ending. The ‘right ending’ can be viewed as ‘entailing’ hypothesis in a classic Recognizing Textual Entailment (RTE) framework (Giampiccolo et al., 2007), and ‘wrong’ ending can be seen as the ’contradicting’ hypothesis. Table 4 shows three example Story Cloze Test cases. Story Cloze Test will serve as a generic story understanding evaluation framework, also applicable to evaluation of story generation models (for instance by computing the log-likelihoods assigned to the two ending alternatives by the story generation model), which does not necessarily imply requirement for explicit narrative knowledge learning. However, it is safe to say that any model that performs well on Story Cloze Test is demonstrating some level of deeper story understa"
N16-1098,E12-1034,0,0.0756634,"Missing"
N16-1098,P04-1077,0,0.0329659,"orkers participated Average # cases written by one worker Max # cases written by one worker Average payment per test case (cents) Size of the final set (verified by human) 13,500 282 47.8 1461 10 3,744 Table 5: Statistics for crowd-sourcing Story Cloze Test instances. search engine8 hits of the main event (verb) together with its semantic roles (e.g., ‘I*poison*flowers’ vs ‘I*nourish*flowers’). We extract the main verb and its corresponding roles using TRIPS semantic parser. 2. N-gram Overlap: Simply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothe"
N16-1098,P14-5010,0,0.00272135,"imply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothesis that matches the sentiment of the last context sentence. 8 https://developers.google.com/ custom-search/ 846 6. Skip-thoughts Model: This model uses Skipthoughts’ Sentence2Vec embedding (Kiros et al., 2015) which models the semantic space of novels. This model is trained on the ‘BookCorpus’ (Zhu et al., 2015) (containing 16 different genres) of over 11,000 books. We use the skip-thoughts embedding of the alternatives and contexts for making decision the same way as with GenSim model. 7. Narrati"
N16-1098,P09-1025,0,0.00927628,"ing the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, there is a rich body of work on story plot generation and creative or artistic story telling (M´endez et al., 2014; Riedl and Le´on, 2008). This paper is unique to these in its corpus of short, simple stories with a wide variety of commonsense events. We show these to be useful for learning, but also for enabling a rich evaluation framework for narrative understanding. 3 A Corpus of Short Commonsense Stories We aimed to build a corpus with two goals in mind: 1. The corpus contains a variety of commonsense causal and temporal relations between everyday events. This enables learning n"
N16-1098,W16-1007,1,0.12213,"Missing"
N16-1098,P15-1019,0,0.0707931,"Missing"
N16-1098,E14-1024,0,0.538376,"2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward child"
N16-1098,P10-1100,0,0.0173048,"g can help direct the field to a new direction of deeper language understanding. 2 Related Work Several lines of research have recently focused on learning narrative/event representations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a"
N16-1098,D13-1020,0,0.0780145,"vents, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, the"
N16-1098,D15-1195,0,0.170589,"Missing"
N16-1098,H89-1033,0,0.710176,"onsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding. 1 Introduction Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced"
N18-1147,P11-1040,0,0.0185214,"an attack as seen through its users, and (5) we make available the largest list of historical DDoS attacks to date. 2 Previous Work The most relevant line of research to this paper is event extraction from social media. Space prohibits describing all work; the major approaches vary in levels of supervision. Ritter et al. (2012) used a Latent Dirichlet Allocation model to identify events in text without labeled data. They showed you can cluster and extract events like concerts, movies, and performances into a calendar. General event detection from social media has continued in several threads (Benson et al., 2011; Popescu et al., 2011; Anantharam et al., 2015; Wei, 2016; Zhou et al., 2017). Guo et al. (2013) link tweets to news stories using an annotated dataset. Sakaki et al. (2010) detect earthquake events by monitoring tweets with keywords like ‘earthquake’. This is similar in goal to our paper, but different in approach and brittle in its application. We crucially do not assume that users use known keywords and phrases. We take inspiration from the thread of work on flu detection (Lamb et al., 2013; Broniatowski et al., 2013). Their work leverages mentions of an event (‘caught’, ‘sick’, ‘flu’), an"
N18-1147,N16-1045,0,0.361134,"experimented with whether outages could be detected from its counts. They use a trend detection formula to notice increases of this one phrase to trigger an alert. We compare against this strong baseline later. The work by Kergl et al. (2016) uses social media to identify users who discuss zero-day exploits. While not directly related to the work in this paper, its success reinforces the hypothesis that social media contains useful data for computer security monitoring. The main thread in this area is the learning model from Ritter et al. (Ritter et al., 2015) and follow-on work (Kergl, 2015; Chang et al., 2016). They proposed a weakly supervised learner to identify cybersecurity events from Twitter. They 1627 Attacked Services (dd-mm-yy) Ancestry.com 16-06-14 Lib. Congress BBC Website 14-03-15 Newsweek Call of Duty 20-09-14 Planned Parent. DNS 21-10-16 Reddit Github 27-03-15 Spamhaus 18-07-16 29-09-16 29-07-15 19-04-13 18-03-13 Table 1: A sample of 10 DDoS events in our dataset. A 20 day span is collected around each attack date. collected tweets that contain the word ‘DDoS’, and then collected a set of known network attack days. The known days provided a training set from which they trained this we"
N18-1147,P13-1024,0,0.0152741,"attacks to date. 2 Previous Work The most relevant line of research to this paper is event extraction from social media. Space prohibits describing all work; the major approaches vary in levels of supervision. Ritter et al. (2012) used a Latent Dirichlet Allocation model to identify events in text without labeled data. They showed you can cluster and extract events like concerts, movies, and performances into a calendar. General event detection from social media has continued in several threads (Benson et al., 2011; Popescu et al., 2011; Anantharam et al., 2015; Wei, 2016; Zhou et al., 2017). Guo et al. (2013) link tweets to news stories using an annotated dataset. Sakaki et al. (2010) detect earthquake events by monitoring tweets with keywords like ‘earthquake’. This is similar in goal to our paper, but different in approach and brittle in its application. We crucially do not assume that users use known keywords and phrases. We take inspiration from the thread of work on flu detection (Lamb et al., 2013; Broniatowski et al., 2013). Their work leverages mentions of an event (‘caught’, ‘sick’, ‘flu’), and then uses human annotators to label these mentions as relevant to the desired event (flu). We a"
N18-1147,N13-1097,0,0.0364132,"ances into a calendar. General event detection from social media has continued in several threads (Benson et al., 2011; Popescu et al., 2011; Anantharam et al., 2015; Wei, 2016; Zhou et al., 2017). Guo et al. (2013) link tweets to news stories using an annotated dataset. Sakaki et al. (2010) detect earthquake events by monitoring tweets with keywords like ‘earthquake’. This is similar in goal to our paper, but different in approach and brittle in its application. We crucially do not assume that users use known keywords and phrases. We take inspiration from the thread of work on flu detection (Lamb et al., 2013; Broniatowski et al., 2013). Their work leverages mentions of an event (‘caught’, ‘sick’, ‘flu’), and then uses human annotators to label these mentions as relevant to the desired event (flu). We also identify mentions of an event, but we crucially differ by not knowing event words a priori. We believe a typical user does not know what a DDoS attack is, so we cannot assume certain language will be used. A major contribution of this work is the first analysis of how the (perhaps uninformed) public perceives DDoS attacks as they occur. The first work (to our knowledge) on attack detection from"
N18-1147,I11-1108,0,0.0264939,"This paper is the first to learn models of language without ‘attack’ dictionaries and seed words. The goal is the real-time detection of attacks without network data. Our secondary goal is to illustrate NLP applications to computer security topics. Research on information extraction from social media has shown that many types of events in the world can be reliably detected from the language that users post. Several approaches have been shown effective in identifying events like earthquakes (Sakaki et al., 2010), concerts and product releases (Ritter et al., 2012), and other natural disasters (Neubig et al., 2011). Detecting DDoS attacks is not too dissimilar from these goals. An attack is a real event in the world, and it takes a community by surprise. This paper thus adopts ideas from NLP, but applies them to the unique application of DDoS detection. Social media is obviously not the only way (nor the most direct) to monitor network services and attacks. There are several commercial services that directly measure outages, such as norsecorp1 . These perform direct monitoring of network response. We do not propose social media as a better alternative, but rather as an alternative that enhances direct m"
N18-1147,D09-1026,0,0.0438242,"unique applica1629 tion at hand. For readers unfamiliar with LDA, the model can be thought of as a clustering algorithm, and an overview of LDA and its variants can be found in Blei’s survey (Blei, 2012). 4.4.1 LDA for Security Events A traditional LDA model can learn general topics on our dataset with the hope that attack topics bubble up. Our initial experiments found this to be insufficient and the non-attack days were full of distracting topics. For the goal of analyzing attack discussion, we need to encourage the LDA model to learn attackspecific topics. We draw heavily from Labeled LDA (Ramage et al., 2009). Each word is assigned a topic as in standard LDA, but topics can have a known label from the document. This is relevant to this paper because we know which days are attacks (in training). Thus, when a tweet is on an attack day, we assign the tweet a label ATTACK, and bias the Labeled LDA learner to assign its words to an attack-related topic. What labels do we have in our data? ATTACK and N OTATTACK labels are first, but we also know which entities are mentioned in tweets, providing labels to learn entity-specific topics. We can label a tweet about reddit as R EDDIT, and bias the Labeled LDA"
N18-1147,E17-1076,0,0.0122252,"of historical DDoS attacks to date. 2 Previous Work The most relevant line of research to this paper is event extraction from social media. Space prohibits describing all work; the major approaches vary in levels of supervision. Ritter et al. (2012) used a Latent Dirichlet Allocation model to identify events in text without labeled data. They showed you can cluster and extract events like concerts, movies, and performances into a calendar. General event detection from social media has continued in several threads (Benson et al., 2011; Popescu et al., 2011; Anantharam et al., 2015; Wei, 2016; Zhou et al., 2017). Guo et al. (2013) link tweets to news stories using an annotated dataset. Sakaki et al. (2010) detect earthquake events by monitoring tweets with keywords like ‘earthquake’. This is similar in goal to our paper, but different in approach and brittle in its application. We crucially do not assume that users use known keywords and phrases. We take inspiration from the thread of work on flu detection (Lamb et al., 2013; Broniatowski et al., 2013). Their work leverages mentions of an event (‘caught’, ‘sick’, ‘flu’), and then uses human annotators to label these mentions as relevant to the desire"
P07-2044,P06-1095,0,0.36598,"built a MaxEnt classifier that assigns each pair of events one of 6 relations from an augmented Timebank corpus. Their classifier relies on perfect features that were hand-tagged in the corpus, including tense, aspect, modality, polarity and event class. Pairwise agreement on tense and aspect are also included. In a second study, they applied rules of temporal transitivity to greatly expand the corpus, providing different results on this enlarged dataset. We could not duplicate their reported performance on this enlarged data, and instead focus on performing well on the Timebank data itself. Lapata and Lascarides (2006) trained an event classifier for inter-sentential events. They built a corpus by saving sentences that contained two events, one of which is triggered by a key time word (e.g. after and before). Their learner was based on syntax and clausal ordering features. Boguraev and Ando (2005) evaluated machine learning on related tasks, but not relevant to event-event classification. Our work is most similar to Mani’s in that we are Proceedings of the ACL 2007 Demo and Poster Sessions, pages 173–176, c Prague, June 2007. 2007 Association for Computational Linguistics learning relations given event pair"
P08-1090,P98-1013,0,0.262015,"eframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events 1 We analyzed FrameNet (Baker et al., 1998) for insight, but found that very few of the frames are event sequences of the type characterizing narratives and scripts. 790 rather than just pairs of related frames) and apply it to a different task (finding a coherent structured narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the re"
P08-1090,P05-1018,0,0.00605106,"rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 3.1 The Narrative Chain Model Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. We propose to use this same intuition to induce narrative chains. We assume that although a narrative has several participants, there is a central actor who characterizes a narrative chain: the protagonist. Narrative chains are thus structured by the protagonist’s grammatical roles in the events. In addition, narrative events are ordered by some theory of time. This paper describes a partial ordering with the before (no overlap) relation. Our task, therefore, is to learn events that constitute narrative chains. Formally, a narrat"
P08-1090,N04-1038,0,0.0112226,"nts. Finally, the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains. 2 Previous Work While previous work hasn’t focused specifically on learning narratives1 , our work draws from two lines of research in summarization and anaphora resolution. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios. These terms can capture some narrative relations, but the model requires topic-sorted training data. Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events"
P08-1090,P07-1057,0,0.00731615,"nt&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events 1 We analyzed FrameNet (Baker et al., 1998) for insight, but found that very few of the frames are event sequences of the type characterizing narratives and scripts. 790 rather than just pairs of related frames) and apply it to a different task (finding a coherent structured narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overl"
P08-1090,P07-2044,1,0.741104,"Missing"
P08-1090,W04-3205,0,0.0171432,"s in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, u"
P08-1090,de-marneffe-etal-2006-generating,0,0.0116931,"Missing"
P08-1090,E03-1061,0,0.0333632,"guments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 3.1 The Narrative Chain Model Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequen"
P08-1090,J95-2003,0,0.065801,"ncies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 3.1 The Narrative Chain Model Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. We propose to use this same intuition to induce narrative chains. We assume that although a narrative has several participants, there is a central actor who characterizes a narrative chain: the protagonist. Narrative chains are thus structured by the protagonist’s grammatical roles in the events. In addition, narrative events are ordered by some theory of time. This paper describes a partial ordering with the before (no overlap) relation. Our task, therefore,"
P08-1090,P06-1095,0,0.0121413,"Missing"
P08-1090,C00-1072,0,0.00547159,"te partial orders of our learned events. We show, using a coherence-based evaluation of temporal ordering, that our partial orders lead to better coherence judgements of real narrative instances extracted from documents. Finally, the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains. 2 Previous Work While previous work hasn’t focused specifically on learning narratives1 , our work draws from two lines of research in summarization and anaphora resolution. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios. These terms can capture some narrative relations, but the model requires topic-sorted training data. Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Ril"
P08-1090,1985.tmi-1.17,0,0.0542763,"s can be filled in and instantiated in a particular text situation to draw inferences. Chains focus on a single actor to faciliIt would be useful for question answering or textual entailment to know that ‘X denied ’ is also a likely event in the left chain, while ‘ replaces W’ temporally follows the right. Narrative chains (such as Firing of Employee or Executive Resigns) offer the structure and power to directly infer these new subevents by providing critical background knowledge. In part due to its complexity, automatic induction has not been addressed since the early nonstatistical work of Mooney and DeJong (1985). The first step to narrative induction uses an entitybased model for learning narrative relations by fol789 Proceedings of ACL-08: HLT, pages 789–797, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics lowing a protagonist. As a narrative progresses through a series of events, each event is characterized by the grammatical role played by the protagonist, and by the protagonist’s shared connection to surrounding events. Our algorithm is an unsupervised distributional learning approach that uses coreferring arguments as evidence of a narrative relation. We show, us"
P08-1090,J91-1002,0,0.0323474,"ed narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting"
P08-1090,N04-1041,0,0.00835113,"ence). Given a list of observed verb/dependency counts, we approximate the pointwise mutual information (PMI) by: pmi(e(w, d), e(v, g)) = log P (e(w, d), e(v, g)) (1) P (e(w, d))P (e(v, g)) where e(w, d) is the verb/dependency pair w and d (e.g. e(push,subject)). The numerator is defined by: C(e(w, d), e(v, g)) P x,y d,f C(e(x, d), e(y, f )) (2) P (e(w, d), e(v, g)) = P where C(e(x, d), e(y, f )) is the number of times the two events e(x, d) and e(y, f ) had a coreferring entity filling the values of the dependencies d and f . We also adopt the ‘discount score’ to penalize low occuring words (Pantel and Ravichandran, 2004). Given the debate over appropriate metrics for distributional learning, we also experimented with the t-test. Our experiments found that PMI outperforms the t-test on this task by itself and when interpolated together using various mixture weights. Once pairwise relation scores are calculated, a global narrative score can then be built such that all events provide feedback on the event in question. For instance, given all narrative events in a document, we can find the next most likely event to occur by maximizing: max j:0&lt;j&lt;m n X pmi(ei , fj ) (3) i=0 where n is the number of events in our c"
P08-1090,C98-1013,0,\N,Missing
P08-1090,J08-1001,0,\N,Missing
P09-1068,P98-1013,0,0.820001,"ive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to Events Roles A search B A = Police B = Suspect C = Plea D = Jury A arrest B B plead C D acquit B D convict B D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a c"
P09-1068,N04-1038,0,0.00846275,"t the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles. 602 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602–610, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Background Our previous work, however, has two major limitations. First, the model did not express any information about the protagonist, such as its type or role. Role information (su"
P09-1068,P08-1090,1,0.409958,"PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to Events Roles A search B A = Police B = Suspect C = Plea D = Jury A arrest B B plead C D acquit B D convict B D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004), and FrameNet frames (Baker et al., 1998). This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, whil"
P09-1068,de-marneffe-etal-2006-generating,0,0.0120698,"Missing"
P09-1068,W05-1007,0,0.0167345,"meNet (Baker et al., 1998) or VerbNet (Kipper et al., 2000) as gold standard roles and training data. More recent learning work has applied bootstrapping approaches (Swier and Stevenson, 2004; He and Gildea, 2006), but these still rely on a hand labeled seed corpus as well as a pre-defined set of roles. Grenegar and Manning (2006) use the EM algorithm to learn PropBank roles from unlabeled data, and unlike bootstrapping, they don’t need a labeled corpus from which to start. However, they do require a predefined set of roles (arg0, arg1, etc.) to define the domain of their probabilistic model. Green and Dorr (2005) use WordNet’s graph structure to cluster its verbs into FrameNet frames, using glosses to name potential slots. We differ in that we attempt to learn frame-like narrative structure from untagged newspaper text. Most similar to us, Alishahi and Stevenson (2007) learn verb specific semantic profiles of arguments using WordNet classes to define the roles. We learn situation-specific classes of roles shared by multiple verbs. Thus, two open goals in role learning include (1) unsupervised learning and (2) learning the roles themselves rather than relying on pre-defined role classes. As just descri"
P09-1068,W06-1601,0,0.0646395,"Missing"
P09-1068,J05-1004,0,0.334704,"heir participants) and frames to drive interpretation of syntax and word use. Knowledge structures such as these provided the interpreter rich information about many aspects of meaning. The problem with these rich knowledge structures is that the need for hand construction, specificity, and domain dependence prevents robust and flexible language understanding. Instead, modern work on understanding has focused on shallower representations like semantic roles, which express at least one aspect of the semantics of events and have proved amenable to supervised learning from corpora like PropBank (Palmer et al., 2005) and Framenet (Baker et al., 1998). Unfortunately, creating these supervised corpora is an expensive and difficult multi-year effort, requiring complex decisions about the exact set of roles to Events Roles A search B A = Police B = Suspect C = Plea D = Jury A arrest B B plead C D acquit B D convict B D sentence B Being able to robustly learn sets of related events (left) and frame-specific role information about the argument types that fill them (right) could assist a variety of NLP applications, from question answering to machine translation. Our previous work (Chambers and Jurafsky, 2008) r"
P09-1068,W04-3213,0,\N,Missing
P09-1068,C98-1013,0,\N,Missing
P10-1046,P99-1014,0,0.25135,"Missing"
P10-1046,P98-1013,0,0.00545487,"ficult to classify than random choices. Nakov and Hearst (2003) further illustrated how random confounders are easier to identify than those selected from semantically ambiguous, yet related concepts. Our approach evaluates selectional preferences, not WSD, but our results complement these findings. We identified three methods of confounder selection based on varying levels of corpus fre4 Recent work does include some seen data. Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al., 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). Neither evaluates all of the seen data, however. 448 C(vd , n) as the number of times v and n (ignoring d) appear in the same n-gram. We propose a conditional probability baseline: ( P (n|vd ) = C(vd ,n) C(vd ,∗) if C(vd , n) > 0 0 otherwise 5.3 We implemented the current state-of-the-art smoothing model of Erk (2007). The model is based on the idea that the arguments of a particular verb slot tend to be similar to each other. Given two potential arguments for a verb, the correct one sh"
P10-1046,D08-1007,0,0.410392,"WSD has shown that confounder choice can make the pseudo-disambiguation task significantly easier. Gaustad (2001) showed that human-generated pseudo-words are more difficult to classify than random choices. Nakov and Hearst (2003) further illustrated how random confounders are easier to identify than those selected from semantically ambiguous, yet related concepts. Our approach evaluates selectional preferences, not WSD, but our results complement these findings. We identified three methods of confounder selection based on varying levels of corpus fre4 Recent work does include some seen data. Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al., 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). Neither evaluates all of the seen data, however. 448 C(vd , n) as the number of times v and n (ignoring d) appear in the same n-gram. We propose a conditional probability baseline: ( P (n|vd ) = C(vd ,n) C(vd ,∗) if C(vd , n) > 0 0 otherwise 5.3 We implemented the current state-of-the-art smoothing model of Erk (2007). The m"
P10-1046,C00-2137,0,0.186311,"Missing"
P10-1046,P09-2019,0,0.0979729,"Missing"
P10-1046,P07-1028,0,0.564095,"wed that human-generated pseudo-words are more difficult to classify than random choices. Nakov and Hearst (2003) further illustrated how random confounders are easier to identify than those selected from semantically ambiguous, yet related concepts. Our approach evaluates selectional preferences, not WSD, but our results complement these findings. We identified three methods of confounder selection based on varying levels of corpus fre4 Recent work does include some seen data. Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al., 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). Neither evaluates all of the seen data, however. 448 C(vd , n) as the number of times v and n (ignoring d) appear in the same n-gram. We propose a conditional probability baseline: ( P (n|vd ) = C(vd ,n) C(vd ,∗) if C(vd , n) > 0 0 otherwise 5.3 We implemented the current state-of-the-art smoothing model of Erk (2007). The model is based on the idea that the arguments of a particular verb slot tend to be similar to each other. G"
P10-1046,J03-3005,0,0.469918,"he first to test only on unseen pairs. Several papers followed with differing methods of choosing a test pair (v, n) and its confounder v 0 . Dagan et al. (1999) tested all unseen (v, n) occurrences of the most frequent 1000 verbs in his corpus. They then sorted verbs by corpus frequency and chose the neighboring verb v 0 of v as the confounder to ensure the closest frequency match possible. Rooth et al. (1999) tested 3000 random (v, n) pairs, but required the verbs and nouns to appear between 30 and 3000 times in training. They also chose confounders randomly so that the new pair was unseen. Keller and Lapata (2003) specifically addressed the impact of unseen data by using the web to first ‘see’ the data. They evaluated unseen pseudowords by attempting to first observe them in a larger corpus (the Web). One modeling difference was to disambiguate the nouns as selectional preferences instead of the verbs. Given a test pair (v, n) and its confounder (v, n0 ), they used web searches such as “v Det n” to make the decision. Results beat or matched current results at the time. We present a similarly motivated, but new webbased approach later. Very recent work with pseudo-words (Erk, 2007; Bergsma et al., 2008)"
P10-1046,E99-1005,0,0.0446049,"erb slot tend to be similar to each other. Given two potential arguments for a verb, the correct one should correlate higher with the arguments observed with the verb during training. Formally, given a verb v and a grammatical dependency d, the score for a noun n is defined: where C(vd , n) is the number of times the head word n was seen as an argument to the predicate v, and C(vd , ∗) is the number of times vd was seen with any argument. Given a test (vd , n) and its confounder (vd , n0 ), choose n if P (n|vd ) > P (n0 |vd ), and n0 otherwise. If P (n|vd ) = P (n0 |vd ), randomly choose one. Lapata et al. (1999) showed that corpus frequency and conditional probability correlate with human decisions of adjective-noun plausibility, and Dagan et al. (1999) appear to propose a very similar baseline for verb-noun selectional preferences, but the paper evaluates unseen data, and so the conditional probability model is not studied. We later analyze this baseline against a more complicated smoothing approach. 5.2 Smoothing Model Svd (n) = X sim(n, w) ∗ C(vd , w) (1) w∈Seen(vd ) where sim(n, w) is a noun-noun similarity score, Seen(vd ) is the set of seen head words filling the slot vd during training, and C("
P10-1046,N03-2023,0,0.405902,"Missing"
P10-1046,P93-1024,0,0.048268,"utze (1992) simply called the words, ‘artificial ambiguous words’, but Gale et al. (1992) proposed the succinct name, pseudo-word. Both papers cited the sparsity and difficulty of creating large labeled datasets as the motivation behind pseudo-words. Gale et al. selected unambiguous words from the corpus and paired them with random words from different thesaurus categories. Sch¨utze paired his words with confounders that were ‘comparable in frequency’ and ‘distinct semantically’. Gale et al.’s pseudo-word term continues today, as does Sch¨utze’s frequency approach to selecting the confounder. Pereira et al. (1993) soon followed with a selectional preference proposal that focused on a language model’s effectiveness on unseen data. The work studied clustering approaches to assist in similarity decisions, predicting which of two verbs 3 How Frequent is Unseen Data? Most NLP tasks evaluate their entire datasets, but as described above, most selectional preference evaluations have focused only on unseen data. This section investigates the extent of unseen examples in a typical training/testing environment 446 of newspaper articles. The results show that even with a small training size, seen examples dominat"
P10-1046,C98-1013,0,\N,Missing
P11-1098,P98-1013,0,0.048966,"discovery that focuses on learning atomic facts (Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure 976 itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic function"
P11-1098,P04-1056,0,0.0983946,"his paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenba"
P11-1098,P08-1090,1,0.888564,"a of using unlabeled documents to discover relations in text, and of defining semantic roles by sets of entities. However, the limitations to their approach are that (1) redundant documents about specific events are required, (2) relations are binary, and (3) only slots with named entities are learned. We will extend their work by showing how to learn without these assumptions, obviating the need for redundant documents, and learning templates with any type and any number of slots. Large-scale learning of scripts and narrative schemas also captures template-like knowledge from unlabeled text (Chambers and Jurafsky, 2008; Kasch and Oates, 2010). Scripts are sets of related event words and semantic roles learned by linking syntactic functions with coreferring arguments. While they learn interesting event structure, the structures are limited to frequent topics in a large corpus. We borrow ideas from this work as well, but our goal is to instead characterize a specific domain with limited data. Further, we are the first to apply this knowledge to the IE task of filling in template mentions in documents. In summary, our work extends previous work on unsupervised IE in a number of ways. We are the first to learn"
P11-1098,P09-1068,1,0.897926,"can extract a richer representation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure 976 itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template’s event words (e.g., the objects of detonate and explode). Our goal is to characterize a dom"
P11-1098,P03-1028,0,0.228003,"stems. The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extracto"
P11-1098,J93-3001,0,0.72418,". We evaluate on the MUC-4 terrorism corpus with results approaching those of supervised systems. The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabel"
P11-1098,D08-1094,0,0.0073989,"In the sentence, he ran and then he fell, the subjects of run and fall corefer, and so they likely belong to the same scenario-specific semantic role. We applied this idea to a new vector similarity framework. We represent a relation as a vector of all relations with which their arguments coreferred. For instance, arguments of the relation go off:s were seen coreferring with mentions in plant:o, set off:o and injure:s. We represent go off:s as a vector of these relation counts, calling this its coref vector representation. Selectional preferences (SPs) are also useful in measuring similarity (Erk and Pado, 2008). A relation can be represented as a vector of its observed arguments during training. The SPs for go off:s in our data include {bomb, device, charge, explosion}. We measure similarity using cosine similarity between the vectors in both approaches. However, 2 coreference and SPs measure different types of similarity. Coreference is a looser narrative similarity (bombings cause injuries), while SPs capture synonymy (plant and place have similar arguments). We observed that many narrative relations are not synonymous, and vice versa. We thus take the maximum of either cosine score as our final s"
P11-1098,P06-2027,0,0.403729,"nding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled documents, and extract th"
P11-1098,P98-1067,0,0.221888,"f supervised systems. The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word"
P11-1098,W06-1601,0,0.028342,"and we did not experiment with other quantities. A search for optimum parameter values may lead to better results. 4.3 Inducing Semantic Roles (Slots) Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off"
P11-1098,P08-1030,0,0.204695,"lgorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been s"
P11-1098,W10-0905,0,0.124693,"ntation of a particular domain. However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates. Very little work addresses how to learn the template structure 976 itself. Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus. There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010). Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: Bombing Template {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template’s event words (e.g., the objects of detonate and explode). Our goal is to characterize a domain by learning this tem"
P11-1098,N10-1137,0,0.0246136,"ith other quantities. A search for optimum parameter values may lead to better results. 4.3 Inducing Semantic Roles (Slots) Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles. Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy. Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998). Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions. This paper extends this intuition by introducing a new vectorbased approach to coreference similarity. 4.3.1 Syntactic Relations as Roles We learn the roles of cluster C by clustering the syntactic relations RC of its words. Consider the following example: C = {go off, explode, set off, damage, destroy} RC ="
P11-1098,D07-1075,0,0.885082,"n the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled d"
P11-1098,D09-1016,0,0.798865,"o characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; R"
P11-1098,M92-1008,0,0.539077,"C-4 terrorism corpus with results approaching those of supervised systems. The core of this paper focuses on how to characterize a domain-specific corpus by learning rich template structure. We describe how to first expand the small corpus’ size, how to cluster its events, and finally how to induce semantic roles. Section 5 then describes the extraction algorithm, followed by evaluations against previous work in section 6 and 7. 2 Previous Work Many template extraction algorithms require full knowledge of the templates and labeled corpora, such as in rule-based systems (Chinchor et al., 1993; Rau et al., 1992) and modern supervised classifiers (Freitag, 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered e"
P11-1098,W98-1106,0,0.165524,"nescu and Mooney, 2004; Patwardhan and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted"
P11-1098,N06-1039,0,0.397191,"t clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled documents, and extract their fillers. Central to the algorithm is collecting multiple documents describ977 ing the same exact event (e.g. Hurricane Ivan), and observing repeated word patterns across documents connecting the same proper nouns. Learned patterns represent binary relations, and they show how to construct tables of extracted entities for these relations. Our approach draws on this idea of using unlabeled documents to discover"
P11-1098,P03-1029,0,0.0623833,"an and Riloff, 2009). Classifiers rely on the labeled examples’ surrounding context for features such as nearby tokens, document position, syntax, named entities, semantic classes, and discourse relations (Maslennikov and Chua, 2007). Ji and Grishman (2008) also supplemented labeled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery"
P11-1098,H91-1059,0,0.864192,"ling in template mentions in documents. In summary, our work extends previous work on unsupervised IE in a number of ways. We are the first to learn MUC-4 templates, and we are the first to extract entities without knowing how many templates exist, without examples of slot fillers, and without event-clustered documents. 3 The Domain and its Templates Our goal is to learn the general event structure of a domain, and then extract the instances of each learned event. In order to measure performance in both tasks (learning structure and extracting instances), we use the terrorism corpus of MUC-4 (Sundheim, 1991) as our target domain. This corpus was chosen because it is annotated with templates that describe all of the entities involved in each event. An example snippet from a bombing document is given here: The terrorists used explosives against the town hall. El Comercio reported that alleged Shining Path members also attacked public facilities in huarpacha, Ambo, tomayquichua, and kichki. Municipal official Sergio Horna was seriously wounded in an explosion in Ambo. The entities from this document fill the following slots in a MUC-4 bombing template. Perp: Shining Path members Victim: Sergio Horna"
P11-1098,W06-2207,0,0.137821,"eled with unlabeled data. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled documents, and extract their fillers. Central to the algorithm is collecting multiple documents describ977 ing the same exact event (e.g. Hurricane Ivan), and observing repeated word patterns across documents connecting the same prop"
P11-1098,C04-1078,0,0.0550395,"orpus is to report the F1 score for slot type accuracy, ignoring the template type. For instance, a perpetrator of a bombing and a perpetrator of an attack are treated the same. This allows supervised classifiers to train on all perpetrators at once, rather than template-specific learners. Although not ideal for our learning goals, we report it for comparison against previous work. Several supervised approaches have presented results on MUC-4, but unfortunately we cannot compare against them. Maslennikov and Chua (2006; 2007) evaluated a random subset of test (they report .60 and .63 F1), and Xiao et al. (2004) did not evaluate all slot types (they report .57 F1). Figure 5 thus shows our results with previous work that is comparable: the fully supervised and 983 Patwardhan & Riloff-09 : Supervised Patwardhan & Riloff-07 : Weak-Sup Our Results (1 attack) Our Results (5 attack) P 48 42 48 44 R 59 48 25 36 F1 53 44 33 40 Figure 5: MUC-4 extraction, ignoring template type. F1 Score Results Kidnap .53 Bomb .43 Arson .42 Attack .16 / .25 Figure 6: Performance of individual templates. Attack compares our 1 vs 5 best templates. weakly supervised approaches of Patwardhan and Riloff (2009; 2007). We give two"
P11-1098,C00-2136,0,0.288082,"a. Weakly supervised approaches remove some of the need for fully labeled data. Most still require the templates and their slots. One common approach is to begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007). Filatova et al. (2006) integrate named entities into pattern learning (PERSON won) to approximate unknown semantic roles. Bootstrapping with seed examples of known slot fillers has been shown to be effective (Surdeanu et al., 2006; Yangarber et al., 2000). In contrast, this paper removes these data assumptions, learning instead from a corpus of unknown events and unclustered documents, without seed examples. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. They present unrestricted relation discovery as a means of discovering relations in unlabeled documents, and extract their fillers. Central to the algorithm is collecting multiple documents describ977 ing the same exact event (e.g. Hurricane Ivan), and observing repeated word patterns across documents connecting the same proper nouns. Learned pattern"
P11-1098,W04-3213,0,\N,Missing
P11-1098,C98-1013,0,\N,Missing
P11-1098,C98-1064,0,\N,Missing
P12-1011,D08-1073,1,0.867265,"new discriminative classifier that greatly improves previous work. The temporal reasoning community has long depended on document timestamps to ground relaReconstructing the timeline of events from this document requires extensive temporal knowledge, most notably, the document’s creation date to ground its relative expressions (e.g., this year = 2012). Not only did the latest TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2009) include tasks to link events to the (known) document creation time, but state-of-the-art event-event ordering algorithms also rely on these timestamps (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This knowledge is assumed to be available, but unfortunately this is not often the case, particularly on the Web. Document timestamps are growing in importance to the information retrieval (IR) and management communities as well. Several IR applications depend on knowledge of when documents were posted, such as computing document relevance (Li and Croft, 2003; Dakka et al., 2008) and labeling search queries with temporal profiles (Diaz and Jones, 2004; Zhang et al., 2009). Dating documents is similarly important to processing historical and heritage collections of te"
P12-1011,W06-0903,0,0.43939,"and tf-idf scores. They also integrated search engine results as features, but did not see an improvement. Both works evaluated on the news genre. Recent work by Kumar et al. (2011) focused on dating Gutenberg short stories. As above, they learned unigram LMs, but instead measured the KLdivergence between a document and a time period’s LM. Our proposed models differ from this work by applying rich linguistic features, discriminative models, and by focusing on how time expressions improve accuracy. We also study the news genre. The only work we are aware of within the NLP community is that of Dalli and Wilks (2006). They computed probability distributions over different time periods (e.g., months and years) for each observed token. The work is similar to the above IR work in its bag of words approach to classification. 99 They focused on finding words that show periodic spikes (defined by the word’s standard deviation in its distribution over time), weighted with inverse document frequency scores. They evaluated on a subset of the Gigaword Corpus (Graff, 2002). The experimental setup in the above work (except Kumar et al. who focus on fiction) all train on news articles from a particular time period, an"
P12-1011,P00-1010,0,0.369882,"Missing"
P12-1011,J93-2004,0,0.0391835,"Missing"
P12-1011,S07-1014,0,0.0240689,"of its text indicate the year in which the document was written? This paper proposes a learning approach that builds constraints from a document’s use of time expressions, and combines them with a new discriminative classifier that greatly improves previous work. The temporal reasoning community has long depended on document timestamps to ground relaReconstructing the timeline of events from this document requires extensive temporal knowledge, most notably, the document’s creation date to ground its relative expressions (e.g., this year = 2012). Not only did the latest TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2009) include tasks to link events to the (known) document creation time, but state-of-the-art event-event ordering algorithms also rely on these timestamps (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This knowledge is assumed to be available, but unfortunately this is not often the case, particularly on the Web. Document timestamps are growing in importance to the information retrieval (IR) and management communities as well. Several IR applications depend on knowledge of when documents were posted, such as computing document relevance (Li and Croft, 2003; Dakka"
P12-1011,P09-1046,0,0.0619561,"that greatly improves previous work. The temporal reasoning community has long depended on document timestamps to ground relaReconstructing the timeline of events from this document requires extensive temporal knowledge, most notably, the document’s creation date to ground its relative expressions (e.g., this year = 2012). Not only did the latest TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2009) include tasks to link events to the (known) document creation time, but state-of-the-art event-event ordering algorithms also rely on these timestamps (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This knowledge is assumed to be available, but unfortunately this is not often the case, particularly on the Web. Document timestamps are growing in importance to the information retrieval (IR) and management communities as well. Several IR applications depend on knowledge of when documents were posted, such as computing document relevance (Li and Croft, 2003; Dakka et al., 2008) and labeling search queries with temporal profiles (Diaz and Jones, 2004; Zhang et al., 2009). Dating documents is similarly important to processing historical and heritage collections of text. Some of the early wor"
P14-2082,bethard-etal-2008-building,1,0.363636,"Missing"
P14-2082,S13-2002,1,0.796379,"did not appear that anyone else was injured. The other customers fled, and the police said it did not appear that anyone else was injured. Figure 1: A TimeBank annotated document is on the left, and this paper’s TimeBank-Dense annotation is on the right. Solid arrows indicate BEFORE relations and dotted arrows indicate INCLUDED IN relations. Events Times Rels R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval-07 6832 1249 5790 0.7 TempEval-10 5688 2117 4907 0.6 TempEval-13 11145 2078 11098 0.8 Kolomiyets-12 1233 – 1139 0.9 Do 20122 324 232 3132 5.6 This work 1729 289 12715 6.3 (Bethard, 2013). We describe the first annotation framework that forces annotators to annotate all pairs1 . With this new process, we created a dense ordering of document events that can properly evaluate both relation identification and relation annotation. Figure 1 illustrates one document before and after our new annotations. 2 Previous Annotation Work The majority of corpora and competitions for event ordering contain sparse annotations. Annotators for the original TimeBank (Pustejovsky et al., 2003) only annotated relations judged to be salient by the annotator. Subsequent TempEval competitions (Verhage"
P14-2082,P09-1046,0,0.229847,"ion is to assume that they are vague or ambiguous. However, all 6 edges have clear well-defined ordering relations: belonged BEFORE confirmed belonged BEFORE found found BEFORE confirmed belonged BEFORE Friday confirmed IS INCLUDED IN Friday found IS INCLUDED IN Friday3 Learning algorithms handle these unlabeled edges by making incorrect assumptions, or by ignoring large parts of the temporal graph. Several models with rich temporal reasoners have been published, but since they require more connected graphs, improvement over pairwise classifiers have been minimal (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This paper thus proposes an annotation process that builds denser graphs with formal properties that learners can rely on, such as locally complete subgraphs. 3.1 edges is prohibitive. We approximate completeness by creating locally complete graphs over neighboring sentences. The resulting event graph for a document is strongly connected, but not complete. Specifically, the following edge types are included: 1. Event-Event, Event-Time, and Time-Time pairs in the same sentence 2. Event-Event, Event-Time, and Time-Time pairs between the current and next sentence 3. Event-DCT pairs for every ev"
P14-2082,W06-1623,0,0.656964,"notator did not look at the pair of events, so a relation may or may not exist. 3. The annotator failed to look at the pair of events, so a single relation may exist. Training and evaluation of temporal reasoners is hampered by this ambiguity. To combat this, our 1 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be stro"
P14-2082,D08-1073,1,0.849299,"he 3 unlabeled edges. One option is to assume that they are vague or ambiguous. However, all 6 edges have clear well-defined ordering relations: belonged BEFORE confirmed belonged BEFORE found found BEFORE confirmed belonged BEFORE Friday confirmed IS INCLUDED IN Friday found IS INCLUDED IN Friday3 Learning algorithms handle these unlabeled edges by making incorrect assumptions, or by ignoring large parts of the temporal graph. Several models with rich temporal reasoners have been published, but since they require more connected graphs, improvement over pairwise classifiers have been minimal (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009). This paper thus proposes an annotation process that builds denser graphs with formal properties that learners can rely on, such as locally complete subgraphs. 3.1 edges is prohibitive. We approximate completeness by creating locally complete graphs over neighboring sentences. The resulting event graph for a document is strongly connected, but not complete. Specifically, the following edge types are included: 1. Event-Event, Event-Time, and Time-Time pairs in the same sentence 2. Event-Event, Event-Time, and Time-Time pairs between the current and next sentence 3. Eve"
P14-2082,D12-1062,0,0.602665,"his ambiguity. To combat this, our 1 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be strongly connected. Table 1 compares the size and density of our corpus to others. Ours is the densest and it contains the largest number of temporal relations. 2 Do et al. (2012) reports 6264 relations, but this includes both t"
P14-2082,P12-1010,1,0.580125,"failed to look at the pair of events, so a single relation may exist. Training and evaluation of temporal reasoners is hampered by this ambiguity. To combat this, our 1 Table 1: Events, times, relations and the ratio of relations to events + times (R) in various corpora. annotation adopts the VAGUE relation introduced by TempEval 2007, and our approach forces annotators to use it. This is the only work that includes such a mechanism. This paper is not the first to look into more dense annotations. Bramsen et al. (2006) annotated multisentence segments of text to build directed acyclic graphs. Kolomiyets et al. (2012) annotated “temporal dependency structures”, though they only focused on relations between pairs of events. Do et al. (2012) produced the densest annotation, but “the annotator was not required to annotate all pairs of event mentions, but as many as possible”. The current paper takes a different tack to annotation by requiring annotators to label every possible pair of events/times in a given window. Thus this work is the first annotation effort that can guarantee its event/time graph to be strongly connected. Table 1 compares the size and density of our corpus to others. Ours is the densest a"
P14-2082,S13-2001,0,0.0782729,"syntactic positions: events and times in the same noun phrase, main events in consecutive sentences, etc. We now aim for a shift in the community wherein all pairs are considered candidates for temporal ordering, allowing researchers to ask questions such as: how must algorithms adapt to label the complete graph of pairs, and if the more difficult and ambiguous event pairs are included, how must feature-based learners change? We are not the first to propose these questions, but this paper is the first to directly propose the means by which they can be addressed. The stated goal of TempEval-3 (UzZaman et al., 2013) was to focus on relation identification instead of classification, but the training and evaluation data followed the TimeBank approach where only a subset of event pairs were labeled. As a result, many systems focused on classification, with the top system classifying pairs in only three syntactic constructions 501 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 501–506, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Current Systems & Evaluations This Proposal There were four or five peo"
P14-2082,S07-1014,0,0.080368,"s on the TimeBank setup. This paper addresses one of its shortcomings: sparse annotation. We describe a new annotation framework (and a TimeBank-Dense corpus) that we believe is needed to fulfill the data needs of deeper reasoners. The TimeBank includes a small subset of all possible relations in its documents. The annotators were instructed to label relations critical to the document’s understanding. The result is a sparse labeling that leaves much of the document unlabeled. The TempEval contests have largely followed suit and focused on specific types of event pairs. For instance, TempEval (Verhagen et al., 2007) only labeled relations between events that syntactically dominated each other. This paper is the first attempt to annotate a document’s entire temporal graph. A consequence of focusing on all relations is a shift from the traditional classification task, where the system is given a pair of events and asked only to label the type of relation, to an identification task, where the system must determine for itself which events in the document to pair up. For example, in TempEval-1 and 2 (Verhagen et al., 2007; Verhagen et al., 2010), systems were given event pairs in specific syntactic positions:"
P14-2082,S10-1010,0,\N,Missing
Q14-1022,S07-1025,1,0.264648,"part because of the sparsity of temporal relations in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006;"
Q14-1022,S13-2002,1,0.839357,"evaluate both identification and classification. We are not the first to approach relation identification with classification, but this paper is the first to directly and comprehensively address it. Most recently, TempEval 3 (UzZaman et al., 2013a) proposed a labeling of raw text without prior relation identification, but the challenge ultimately relied on the TimeBank. Systems were only evaluated on its subset of labeled event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away from the idea of monolithic learners, and propose smaller specialized classifiers. Inspiration comes from the recent success in named entity coreference with sieve-based learning (Lee et al., 2013). CAEVO contains a host of classi273 Transactions of the Association for Computational Linguistics, 2 (2014) 273–284. Action Editor: Ellen Riloff. c Submitted 11/2013; Rev"
Q14-1022,C82-1006,0,0.602682,"e been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliability” approaches to parsing and speech (Borghesi and Favareto, 1982; Corazza et al., 1991). D’Souza and Ng (2013) recently combined a rule-based model with a machine learned model, but lacked the fine-grained formality of a cascade of sieves. This paper is inspired by the above and is the first to apply it to temporal ordering as an extensible, formal architecture. 275 TimeBank-Dense There were four or five people inside, and they just started firing There were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. Ms. Sanders was hit several times and was pronounced dead at the scene."
Q14-1022,J93-2003,0,0.0437312,"; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliability” approaches to parsing and speech (Borghesi and Favareto, 1982; Corazza et al., 1991). D’Souza and Ng (2013) recently combined a rule-based model with a machine learned model, but lacked the fine-grained formality of a cascade of sieves. This paper is inspired by the above and is the first to apply it to temporal ordering as an extensible, formal architecture. 275 TimeBank-Dense There were four or five people inside, and they just started firing There were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pron"
Q14-1022,P14-2082,1,0.609047,"and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. Ms. Sanders was hit several times and was pronounced dead at the scene. The other customers fled, and the police said it did not appear that anyone else was injured. The other customers fled, and the police said it did not appear that anyone else was injured. Figure 1: TimeBank document on the left with TimeBankDense on the right. Solid arrows indicate BEFORE and dotted INCLUDED IN. Relations with the DCT not shown. 3 TimeBank-Dense: A Dense Ordering We use a new corpus, called TimeBank-Dense (Cassidy et al., 2014), to motivate and evaluate our architecture. This section highlights its main features. The TimeBank-Dense corpus was created to address the sparsity in current corpora. It is unique in that the annotators were forced to label all local edges, even in ambiguous cases. The corpus is not a complete graph over events and time expressions, but it approximates completeness by labeling locally complete graphs over neighboring sentences. All pairs of events and time expressions in the same sentence and all pairs of events and time expressions in the immediately following sentence were labeled. It als"
Q14-1022,D08-1073,1,0.754947,"; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models ("
Q14-1022,S13-2012,1,0.504215,"events share the same lemma, or are each a member of the same synset are thus labeled VAGUE. Time-time edges with the same lemma/synset are labeled SIMULTANEOUS. 4.1.8 All Vague The majority class baseline for this task is to label all edges as VAGUE. This sieve is added to the end of the gauntlet, labeling any remaining unlabeled edges. 4.2 Machine Learned Sieves Current state-of-the-art models for temporal ordering are machine learned classifiers. The top systems in the latest TempEval-3 contest used supervised classifiers for the different types of edges in the event graph (Bethard, 2013; Chambers, 2013). In the spirit of the sieve architecture, rather than training one large classifier for all types of edges, we create targeted classifiers for each type of edge. The resulting models are again ranked by precision and mutually 279 Event-Time Features Token, lemma, POS tag of event Tense, aspect, class of event Bigram of event and time expression words Token path from event to time Syntactic parse tree path between the event and time Typed dependency edge path between the events Boolean: syntactically dominates or is-dominated Boolean: time expression concludes sentence? Boolean: time expressio"
Q14-1022,S07-1052,0,0.0270224,"ity of temporal relations in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsk"
Q14-1022,1991.iwpt-1.24,0,0.115917,"Missing"
Q14-1022,W13-0107,0,0.0408812,"ed to by the verb occurs. • Point of reference (R) A single time with respect to which S is ordered by one dimension of tense, and to which E is ordered by another. Reichenbach’s account maps the set of possible orderings of S, E, and R, where each pair of elements can be ordered with &lt; (before), = (simultaneous), or &gt; (after), onto a set of tense names given by: {anterior, simple, posterior} × {past, present, f uture}. The relative ordering of E and R is indicated by the first dimension, while that of S and R is given by the second. Figure 2 depicts examples of the ordering R &lt; S. Similar to Derczynski and Gaizauskas (2013) we map a subset of Reichenbach’s tenses onto pairs of TimeML tense and aspect attribute values, which are given by {simple, perf ect, progressive} × {past, present, f uture}, where the first dimension is grammatical aspect and the second is tense. We refer to an event’s tense/aspect combination as its tense-aspect profile. Consider two events e1 and e2 associated with S1 /E1 /R1 and S2 /E2 /R2 . Intuitively, given R1 = R2 and the time point orderings for each event (derived from its tense-aspect profile) we can enumerate the possible interval relations that might hold between E1 and E2 , whic"
Q14-1022,D12-1062,0,0.83008,"o 2012 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relations to events + times (R) in various corpora. To avoid such sparse annotations, researchers have explored schemes that encourage annotators to connect all events. Bramsen et al. (2006) annotated timelines as directed acyclic graphs, though they annotated multi-sentence segments of text rather than individual events. Kolomiyets et al. (2012) annotated “temporal dependency structures” (i.e. dependency trees of temporal relations), though they only focused on pairs of events. In Do et al. (2012), “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, and then more relations were automatically inferred after the annotation was complete. In contrast, in our work we required annotators to label every possible pair of events/times in a given window, and our event graphs are guaranteed to be strongly connected, with every edge verified by the annotators. Table 1 compares the density of relation annotation across various corpora. A major dilemma from this prior work is that unlabeled event/time pairs are inherently ambiguous. The unlabeled pair h"
Q14-1022,N13-1112,0,0.357589,"Missing"
Q14-1022,W13-1203,0,0.0183503,"Missing"
Q14-1022,P12-1010,1,0.467407,"Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4907 0.6 TempEval 2013 11145 2078 11098 0.8 Kolomiyets 2012 1233 – 1139 0.9 1 Do 2012 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relations to events + times (R) in various corpora. To avoid such sparse annotations, researchers have explored schemes that encourage annotators to connect all events. Bramsen et al. (2006) annotated timelines as directed acyclic graphs, though they annotated multi-sentence segments of text rather than individual events. Kolomiyets et al. (2012) annotated “temporal dependency structures” (i.e. dependency trees of temporal relations), though they only focused on pairs of events. In Do et al. (2012), “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, and then more relations were automatically inferred after the annotation was complete. In contrast, in our work we required annotators to label every possible pair of events/times in a given window, and our event graphs are guaranteed to be strongly connected, with every edge verified by the annotators. Table 1 compares the density of relatio"
Q14-1022,J13-4004,1,0.881597,"event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away from the idea of monolithic learners, and propose smaller specialized classifiers. Inspiration comes from the recent success in named entity coreference with sieve-based learning (Lee et al., 2013). CAEVO contains a host of classi273 Transactions of the Association for Computational Linguistics, 2 (2014) 273–284. Action Editor: Ellen Riloff. c Submitted 11/2013; Revised 5/2014; Published 10/2014. 2014 Association for Computational Linguistics. fiers that each specialize on different types of edges. The classifiers are ordered by their individual precision, and run in order starting with the most precise. Each sieve informs those below it by passing on its decisions as graph constraints. These more precise constraints are then used by later sieves to assist their less precise decisions."
Q14-1022,S10-1063,0,0.0144695,"aining corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et"
Q14-1022,C08-1108,0,0.0340181,"and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and i"
Q14-1022,S10-1062,0,0.0171222,"tions in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikant"
Q14-1022,S13-2001,0,0.16096,"d of a complete graph labeling algorithm is that of relation identification. Research on the TimeBank and in the TempEval contests has largely focused on relation classification. The event pairs are given, and the task is to classify them. We are now forced to first determine which events should be paired up before classification. Our experiments here fully integrate and evaluate both identification and classification. We are not the first to approach relation identification with classification, but this paper is the first to directly and comprehensively address it. Most recently, TempEval 3 (UzZaman et al., 2013a) proposed a labeling of raw text without prior relation identification, but the challenge ultimately relied on the TimeBank. Systems were only evaluated on its subset of labeled event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away f"
Q14-1022,S07-1014,0,0.0464592,"Missing"
Q14-1022,P09-1046,0,0.148581,"et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliab"
Q14-1022,S10-1010,0,\N,Missing
Q14-1022,W06-1623,0,\N,Missing
Q18-1048,P11-1062,0,0.135964,"t and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity constraints to be effective in learning entailment graphs, the Integer Linear Programming (ILP) solution of Berant et al. is not scalable beyond a few hundred nodes. In fact, the problem of finding a maximally weighted transitive subgraph of a graph with arbitrary edge weights is NP-hard (Berant et al., 2011). This paper instead proposes a scalable solution that does not rely on transitivity closure, but 703 Transactions of the Association fo"
Q18-1048,D15-1075,0,0.125403,"Missing"
Q18-1048,D17-1070,0,0.0422865,"Missing"
Q18-1048,P14-1061,1,0.833691,"better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions fro"
Q18-1048,P15-1034,0,0.0372871,"n C1 =3 unique predicates; (2) remove any predicate that is observed with fewer than C2 =3 unique argument-pairs. This leaves us with |P |=101K unique predicates in 346 entailment graphs. The maximum graph size is 53K nodes,8 and the total number of non-zero local scores in all graphs is 66M. In the future, we plan to test our method on an even larger corpus, but preliminary experiments suggest that data sparsity will persist regardless of the corpus size, because of the power law distribution of the terms. We compared our extractions qualitatively with Stanford Open IE (Etzioni et al., 2011; Angeli et al., 2015). Our CCG-based extraction generated noticeably 7 In our experiments, the total number of edges is ≈ .01|V |2 and most of predicate pairs are seen in less than 20 subgraphs, rather than |T |2 . 8 There are 4 graphs with more than 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of B"
Q18-1048,J15-2003,0,0.30255,"ing the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a combinatory categorial gramma"
Q18-1048,D17-1091,1,0.841356,"only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) con"
Q18-1048,P12-1013,0,0.0150022,"ith PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) connections. on assumptions concerning the graph structure. Berant et al. (2012, 2015) propose Tree-Node-Fix (TNF), an approximation method that scales better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensiv"
Q18-1048,P16-2041,0,0.530878,"han 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of Berant’s entailment data set. The types of this data set do not match with FIGER types, but we perform a simple handmapping between their types and FIGER types.10 Evaluation Entailment Data Sets Levy/Holt’s Entailment Data Set Levy and Dagan (2016) proposed a new annotation method (and a new data set) for collecting relational inference data in context. Their method removes a major bias in other inference data sets such as Zeichner’s (Zeichner et al., 2012), where candidate entailments were selected using a directional similarity measure. Levy and Dagan form questions of the type which city (qtype ), is located near (qrel ), mountains (qarg )? and provide possible answers of the form Kyoto (aanswer ), is surrounded by (arel ), mountains (aarg ). Annotators are shown a question with multiple possible answers, where aanswer is masked by q"
Q18-1048,S17-1026,0,0.22754,"Missing"
Q18-1048,N13-1092,0,0.0938356,"Missing"
Q18-1048,D13-1064,1,0.942149,"third argument by concatenating the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a comb"
Q18-1048,W14-2406,1,0.894326,"Missing"
Q18-1048,P05-1014,0,0.315327,"o arguments, where the type of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions signif"
Q18-1048,P98-2127,0,0.422662,"es as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies"
Q18-1048,P13-2078,0,0.0219818,"ype of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imp"
Q18-1048,C16-1268,0,0.106009,"ed by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on"
Q18-1048,P18-1188,1,0.78834,"Missing"
Q18-1048,W04-3250,0,0.0561007,"Missing"
Q18-1048,W17-2623,0,0.121698,"ign workers . . . . . . Barnes & Noble CEO William Lynch said as he unveiled his company’s Nook Tablet on Monday. The report said opium has accounted for more than half of Afghanistan’s gross domestic product in 2007. Who praised Mitt Romney’s credentials? Which gene did the ALS association discover ? How many Americans suffer from food allergies? What law might the deal break? Who launched the Nook Tablet? What makes up half of Afghanistans GDP ? Table 3: Examples where explicit entailment relations improve the rankings. The related words are boldfaced. contains questions about CNN articles (Trischler et al., 2017). Machine reading comprehension is usually evaluated by posing questions about a text passage and then assessing the answers of a system (Trischler et al., 2017). The data sets that are used for this task are often in the form of (document,question,answer) triples, where answer is a short span of the document. Answer selection is an important task, where the goal is to select the sentence(s) that contain the answer. We show improvements by adding knowledge from our learned entailments without changing the graphs or tuning them to this task in any way. Inverse sentence frequency (ISF) is a stro"
Q18-1048,P15-2070,0,0.0778647,"Missing"
Q18-1048,D14-1162,0,0.0911904,"Missing"
Q18-1048,P15-1129,0,0.0166683,"arallelizable and takes only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within"
Q18-1048,Q14-1030,1,0.832844,"(if any). We thus type all entities that can be grounded in Wikipedia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relati"
Q18-1048,W03-1011,0,0.834164,"and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although"
Q18-1048,N13-1008,0,0.245356,"dge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions from text, facts in knowledge bases, or both. Unlike our work, which directly learns entailment relations between predicates, these methods aim at predicting the source data—that is, whether two entities have a particular relationship. The common Related Work Our work is closely related to Berant et al. (2011), where entailment graphs are learned by imposing transitivity constraints on the entailment relations. However, the exact solution to the problem is not scalabl"
Q18-1048,D10-1106,0,0.090144,"Missing"
Q18-1048,P12-2031,0,0.170474,"Missing"
Q18-1048,D13-1183,0,0.540702,"edia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relations for the given sentence: visit1,2 with arguments (Obama, Hawaii),"
Q18-1048,C08-1107,0,0.828734,"as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity"
S13-2012,chang-manning-2012-sutime,0,0.441243,"sk was dependent on the other. This paper presents NavyTime, a system inspired partly by this previous breakup of the tasks. We focus on breaking up the event/time ordering task further, and show that 5 classifiers yield better performance than the traditional 3 (or even 1). The first required steps to annotate a document are to extract its events and time expressions. This paper describes a new event extractor with a rich set of contextual features that is a top performer for event attributes at Tempeval-3. We then explore additions to SUTime, a top rule-based extractor for time expressions (Chang and Manning, 2012). However, the core challenge is to link these extracted events and times together. We describe new models for these difficult tasks: (1) identifying ordered pairs, and (2) labeling the ordering relations. Relation identification is rarely addressed in the literature. Given a set of events, which pairs of events are temporally related? Almost all previous work assumes we are given the pairs, and the task is to label the relation (before, after, etc.). Raw text presents a new challenge: extract the relevant pairs before labeling them. We present some of the first results that compare rule-based"
S13-2012,S07-1014,0,0.167253,"ion points. Experiments show that more specialized classifiers perform better than few joint classifiers. The NavyTime system ranked second both overall and in most subtasks like event extraction and relation labeling. 1 Introduction The SemEval-2013 Task 1 (TempEval-3) contest is the third instantiation of an event ordering challenge. However, it is the first to start from raw text with the challenge to create an end-to-end algorithm for event ordering. Previous challenges included the individual aspects of such a system, including event extraction, timex extraction, and event/time ordering (Verhagen et al., 2007; Verhagen et al., 2010). However, neither task was dependent on the other. This paper presents NavyTime, a system inspired partly by this previous breakup of the tasks. We focus on breaking up the event/time ordering task further, and show that 5 classifiers yield better performance than the traditional 3 (or even 1). The first required steps to annotate a document are to extract its events and time expressions. This paper describes a new event extractor with a rich set of contextual features that is a top performer for event attributes at Tempeval-3. We then explore additions to SUTime, a to"
S13-2012,S10-1010,0,\N,Missing
S13-2064,C10-2028,0,0.0457123,"013), pages 390–394, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics 2 Previous Work Sentiment analysis is a large field applicable to many genres. This paper focuses on social media (microblogs) and contextual polarity, so we only address the closest work in those areas. For a broader perspective, several survey papers are available (Pang and Lee, 2008; Tang et al., 2009; Liu and Zhang, 2012; Tsytsarau and Palpanas, 2012). Microblogs serve as a quick way to measure a large population’s mood and opinion. Many different sources have been used. O’Connor et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go e"
S13-2064,P11-1016,0,0.032324,"face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approaches are somewhat simple, selecting single keywords (e.g., “Obama”) to represent the topic (e.g., “US President”), and retrieve tweets that contain the word (O’Connor et al., 2010; Tumasjan et al., 2010; Tan et al., 2011"
S13-2064,E12-1062,1,0.784457,"entiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought the sentiment toward particular product features. These systems used rule based approaches based on parts of speech and other surface features (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). Finally, topic identification in microblogs is also related. The first approach"
S13-2064,pak-paroubek-2010-twitter,0,0.022244,"o of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches with labeled training data. Earlier work on product reviews sought"
S13-2064,P05-2008,0,0.040628,"or et al. (2010) used Twitter data to compute a ratio of positive and negative words to measure consumer confidence and presidential approval. Kramer (2010) counted lexicon words on Facebook for a general ’happiness’ measure, and Thelwall (2011) built a general sentiment model on MySpace user comments. These are general sentiment algorithms. Specific work on microblogs has focused on finding noisy training data with distant supervision. Many of these algorithms use emoticons as semantic indicators of polarity. For instance, a tweet that contains a sad face likely contains a negative polarity (Read, 2005; Go et al., 2009; Bifet and Frank, 2010; Pak and Paroubek, 2010; Davidov et al., 2010; Kouloumpis et al., 2011). In a similar vein, hashtags can also serve as noisy labels (Davidov et al., 2010; Kouloumpis et al., 2011). Most work on distant supervision relies on a variety of syntactic and word-based features (Marchetti-Bowick and Chambers, 2012). We adopt many of these features. Supervised learning for contextual sentiment analysis has not been thoroughly investigated. Labeled data for specific words or queries is expensive to generate, so Jiang et al. (2011) is one of the few approaches wit"
S13-2064,S13-2052,0,0.0806469,"Missing"
S15-2134,S13-2002,0,0.110539,"arated event detection and classification, without event coreference • hlt-fbk-ev1-trel2. SVM, separated event detection and classification, with event coref • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-shelf system: trained and tested by organizers 5 7 Time Expression Reasoner (TREFL) As an extra evaluation, task organizers added a new run for each system augmented wit"
S15-2134,Q14-1022,1,0.272596,"lar participants, optimized for task: • HITSZ-ICRC4 . rule-based timex module, SVM (liblinear) for event and relation detection and classification • hlt-fbk-ev1-trel1. SVM, separated event detection and classification, without event coreference • hlt-fbk-ev1-trel2. SVM, separated event detection and classification, with event coref • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-s"
S15-2134,S10-1063,1,0.913075,"• hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4 Annotations Submitted 1-day after the deadline Off-the-shelf system: the author was co-organizer 6 Off-the-shelf system: trained and tested by organizers 5 7 Time Expression Reasoner (TREFL) As an extra evaluation, task organizers added a new run for each system augmented with a postprocessing step. The goal is to analyze how a general time expression reasoner could improve results. The TREFL component is straightforward: resolve all ti"
S15-2134,S13-2001,1,0.797947,"Missing"
S15-2134,S07-1014,1,0.893706,"Missing"
W04-2302,C00-1007,0,0.160911,"d surface generators for the air travel domain. Their input semantic form is a set of attribute-value pairs that are specific to the airline reservation task. The language models were standard n-gram approaches that depended on a tagged air travel corpus for the attribute types. Both groups ran human evaluations; Ratnaparkhi studied a 2 subject evaluation (with marks of OK,Good,Bad) and Oh and Rudnicky studied 12 subjects that compared the output between a template generator and the corpus-based approach. The latter showed no significant difference. Most recently, Chen et al. utilized FERGUS (Bangalore and Rambow, 2000) and attempted to make it more domain independent in (Chen et al., 2002). There are two stochastic processes in FERGUS; a tree chooser that maps an input syntactic tree to a TAG tree, and a trigram language model that chooses the best sentence in the lattice. They found that a domain-specific corpus performs better than a Wall Street Journal (WSJ) corpus for the trigram LM. Work was done to try and use an independent LM, but (Rambow et al., 2001) found interrogatives to be unrepresented by a WSJ model and fell back on air travel models. This problem was not discussed in (Chen et al., 2002). Pe"
W04-2302,W00-1401,0,0.0453486,"eration is particularly difficult due to the diverse amount of correct output that can be generated. There are many ways to present a given semantic representation in English and what determines quality of content and form are often subjective measures. There are two general approaches to a surface generation evaluation. The first uses human evaluators to score the output with some pre-defined ranking measure. The second uses a quantitative automatic approach usually based on n-gram presence and word ordering. Bangalore et al. describe some of the quantitative measures that have been used in (Bangalore et al., 2000). Callaway recently used quantitative measures in an evaluation between symbolic and stochastic surface generators in (Callaway, 2003). The most common quantitative measure is Simple String Accuracy. This metric uses an ideal output string and compares it to a generated string using a metric that combines three word error counts; insertion, deletion, and substitution. One variation on this approach is tree-based metrics. These attempt to better represent how bad a bad result is. The tree-based accuracy metrics do not compare two strings directly, but instead build a dependency tree for the ide"
W04-2302,C02-1138,0,0.278187,"t of attribute-value pairs that are specific to the airline reservation task. The language models were standard n-gram approaches that depended on a tagged air travel corpus for the attribute types. Both groups ran human evaluations; Ratnaparkhi studied a 2 subject evaluation (with marks of OK,Good,Bad) and Oh and Rudnicky studied 12 subjects that compared the output between a template generator and the corpus-based approach. The latter showed no significant difference. Most recently, Chen et al. utilized FERGUS (Bangalore and Rambow, 2000) and attempted to make it more domain independent in (Chen et al., 2002). There are two stochastic processes in FERGUS; a tree chooser that maps an input syntactic tree to a TAG tree, and a trigram language model that chooses the best sentence in the lattice. They found that a domain-specific corpus performs better than a Wall Street Journal (WSJ) corpus for the trigram LM. Work was done to try and use an independent LM, but (Rambow et al., 2001) found interrogatives to be unrepresented by a WSJ model and fell back on air travel models. This problem was not discussed in (Chen et al., 2002). Perhaps automatically extracted trees from the corpora are able to create"
W04-2302,P98-1116,0,0.144333,"within the air travel domain. The final generation system cannot be ported to a new domain without further effort. By creating grammar rules that convert a semantic form, some of these restrictions can be removed. The next section describes our stochastic approach and how it was modified from machine translation to spoken dialogue. 3 Stochastic Generation (HALogen) We used the HALogen framework (Langkilde-Geary, 2002) for our surface generation. HALogen was originally created for a domain within MT and is a sentence planner and a surface realizer. Analysis and MT applications can be found in (Langkilde and Knight, 1998; Knight and Langkilde, 2000). HALogen accepts a feature-value structure ranging from high-level semantics to shallow syntax. Figure 1 shows a mixture of both as an example. Given this input, generation is a two step process. First, the input form is converted into a word forest (a more efficient representation of a word lattice) as described in (LangkildeGeary, 2002). Second, the language model chooses the most probable path through the forest as the output sentence. (V68753 / move :TENSE past :AGENT (V68837 / person :QUANT three :NUMBER plural ) :THEME (V68846 / ambulance ) ) Figure 1: HALog"
W04-2302,W02-2103,0,0.729695,"re intentionally using two out-of-domain language models. Most of the work on FERGUS and the previous surface generation evaluations in dialogue systems are dependent on English syntax and word choice within the air travel domain. The final generation system cannot be ported to a new domain without further effort. By creating grammar rules that convert a semantic form, some of these restrictions can be removed. The next section describes our stochastic approach and how it was modified from machine translation to spoken dialogue. 3 Stochastic Generation (HALogen) We used the HALogen framework (Langkilde-Geary, 2002) for our surface generation. HALogen was originally created for a domain within MT and is a sentence planner and a surface realizer. Analysis and MT applications can be found in (Langkilde and Knight, 1998; Knight and Langkilde, 2000). HALogen accepts a feature-value structure ranging from high-level semantics to shallow syntax. Figure 1 shows a mixture of both as an example. Given this input, generation is a two step process. First, the input form is converted into a word forest (a more efficient representation of a word lattice) as described in (LangkildeGeary, 2002). Second, the language mo"
W04-2302,W00-0306,0,0.0627258,"based approach to surface generation does not use large linguistic databases but rather depends on language modeling of corpora to predict correct and natural utterances. The approach is attractive in comparison to templates and rule-based approaches because the language models implicitly encode the natural ordering of English. Recently, the results from corpus-based surface generation in dialogue systems have been within specific domains, the vast majority of which have used the Air Travel Domain with Air Travel corpora. Ratnaparkhi (Ratnaparkhi, 2000; Ratnaparkhi, 2002) and Oh and Rudnicky (Oh and Rudnicky, 2000) both studied surface generators for the air travel domain. Their input semantic form is a set of attribute-value pairs that are specific to the airline reservation task. The language models were standard n-gram approaches that depended on a tagged air travel corpus for the attribute types. Both groups ran human evaluations; Ratnaparkhi studied a 2 subject evaluation (with marks of OK,Good,Bad) and Oh and Rudnicky studied 12 subjects that compared the output between a template generator and the corpus-based approach. The latter showed no significant difference. Most recently, Chen et al. utili"
W04-2302,2001.mtsummit-papers.68,0,0.0182222,"compares it to a generated string using a metric that combines three word error counts; insertion, deletion, and substitution. One variation on this approach is tree-based metrics. These attempt to better represent how bad a bad result is. The tree-based accuracy metrics do not compare two strings directly, but instead build a dependency tree for the ideal string and attempt to create the same dependency tree from the generated string. The score is dependent not only on word choice, but on positioning at the phrasal level. Finally, the most recent evaluation metric is the Bleu Metric from IBM(Papineni et al., 2001). Designed for Machine Translation, it scores generated sentences based on the n-gram appearance from multiple ideal sentences. This approach provides more than one possible realization of an LF and compares the generated sentence to all possibilities. Unfortunately, the above automatic metrics are very limited in mimicking human scores. The Bleu metric can give reasonable scores, but the results are not as good when only one human translation is available. These automatic metrics all compare the desired output with the actual output. We decided to ignore this evaluation because it is too depe"
W04-2302,H01-1055,0,0.21776,"Missing"
W04-2302,A00-2026,0,0.0523912,"d approach is difficult to port to new domains. The corpus-based approach to surface generation does not use large linguistic databases but rather depends on language modeling of corpora to predict correct and natural utterances. The approach is attractive in comparison to templates and rule-based approaches because the language models implicitly encode the natural ordering of English. Recently, the results from corpus-based surface generation in dialogue systems have been within specific domains, the vast majority of which have used the Air Travel Domain with Air Travel corpora. Ratnaparkhi (Ratnaparkhi, 2000; Ratnaparkhi, 2002) and Oh and Rudnicky (Oh and Rudnicky, 2000) both studied surface generators for the air travel domain. Their input semantic form is a set of attribute-value pairs that are specific to the airline reservation task. The language models were standard n-gram approaches that depended on a tagged air travel corpus for the attribute types. Both groups ran human evaluations; Ratnaparkhi studied a 2 subject evaluation (with marks of OK,Good,Bad) and Oh and Rudnicky studied 12 subjects that compared the output between a template generator and the corpus-based approach. The latter sh"
W04-2302,P02-1040,0,\N,Missing
W04-2302,C98-1112,0,\N,Missing
W05-1604,W00-1401,0,0.0500591,"Missing"
W05-1604,W04-2302,1,0.796743,"Missing"
W05-1604,C02-1138,0,0.0573562,"Missing"
W05-1604,W96-0501,0,0.0325518,"Missing"
W05-1604,habash-2000-oxygen,0,0.0452579,"Missing"
W05-1604,W02-2103,0,0.0309965,"Missing"
W05-1604,A00-2023,0,0.0882814,"Missing"
W05-1604,2001.mtsummit-papers.68,0,0.0482007,"Missing"
W05-1604,P02-1040,0,\N,Missing
W07-1427,W02-1001,0,0.0111527,"Missing"
W07-1427,de-marneffe-etal-2006-generating,1,0.0621763,"Missing"
W07-1427,levy-andrew-2006-tregex,0,0.0313803,"age A core part of an entailment system is the ability to find semantically equivalent patterns in text. Previously, we wrote tedious graph traversal code by hand for each desired pattern. As a remedy, we wrote Semgrex, a pattern language for dependency graphs. We use Semgrex atop the typed dependencies from the Stanford Parser (de Marneffe et al., 2006b), as aligned in the alignment phase, to identify both semantic patterns in a single text and over two aligned pieces of text. The syntax of the language was modeled after tgrep/Tregex, query languages used to find syntactic patterns in trees (Levy and Andrew, 2006). This speeds up the process of graph search and reduces errors that occur in complicated traversal code. 5.1 Semgrex Features Rather than providing regular expression matching of atomic tree labels, as in most tree pattern languages, Semgrex represents nodes as a (nonrecursive) attribute-value matrix. It then uses regular expressions for subsets of attribute values. For example, {word:run;tag:/ˆNN/} refers to any node that has a value run for the attribute word and a tag that starts with NN, while {} refers to any node in the graph. However, the most important part of Semgrex is that it allow"
W07-1427,W07-1431,1,0.380995,"Missing"
W07-1427,N06-1006,1,\N,Missing
W11-0116,W09-3208,0,0.0257341,"of events. Since an event’s duration is highly dependent on context, our algorithm models this aspectual property as a distribution over durations rather than a single mean duration. For example, a “war” typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon wit"
W11-0116,W04-3205,0,0.104792,"pired by the standard use of web patterns for the acquisition of relational lexical knowledge. Hearst (1998) first observed that a phrase like “. . . algae, such as Gelidium. . . ” indicates that “Gelidium” is a type of “algae”, and so hypernym-hyponym relations can be identified by querying a text collection with patterns like “such <noun> as <noun>” and “<noun> , including <noun>”. A wide variety of pattern-based work followed, including the application of the idea in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like “to <verb> and then <verb>” (Chklovski and Pantel, 2004). More recent work has learned nominal gender and animacy by matching patterns like “<noun> * himself” and “<noun> and her” to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like “John Joseph”, which were observed often with masculine pronouns and never with feminine or neuter pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can predict person names as well as a fully supervised named entity recognition system. Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to the task o"
W11-0116,D09-1120,0,0.00678247,"ration using a corpus collected through Amazon’s Mechanical Turk. We make available a new database of events and their duration distributions for use in research involving the temporal and aspectual properties of events. 1 Introduction Bridging the gap between lexical knowledge and world knowledge is crucial for achieving natural language understanding. For example, knowing whether a nominal is a person or organization and whether a person is male or female substantially improves coreference resolution, even when such knowledge is gathered through noisy unsupervised approaches (Bergsma, 2005; Haghighi and Klein, 2009). However, existing algorithms and resources for such semantic knowledge have focused primarily on static properties of nominals (e.g. gender or entity type), not dynamic properties of verbs and events. This paper shows how to learn one such property: the typical duration of events. Since an event’s duration is highly dependent on context, our algorithm models this aspectual property as a distribution over durations rather than a single mean duration. For example, a “war” typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rar"
W11-0116,Y09-1024,0,0.0116617,"t “Gelidium” is a type of “algae”, and so hypernym-hyponym relations can be identified by querying a text collection with patterns like “such <noun> as <noun>” and “<noun> , including <noun>”. A wide variety of pattern-based work followed, including the application of the idea in VerbOcean to acquire aspects and temporal structure such as happens-before, using patterns like “to <verb> and then <verb>” (Chklovski and Pantel, 2004). More recent work has learned nominal gender and animacy by matching patterns like “<noun> * himself” and “<noun> and her” to a corpus of Web n-grams (Bergsma, 2005; Ji and Lin, 2009). Phrases like “John Joseph”, which were observed often with masculine pronouns and never with feminine or neuter pronouns, can thus have their gender identified as masculine. Ji and Lin found that such web-counts can predict person names as well as a fully supervised named entity recognition system. Our goal, then, is to integrate these two strands in the literature, applying pattern/web approaches to the task of estimating event durations. One difference from previous work is the distributional nature of the extracted knowledge. In the time domain, unlike in most previous relation-extraction"
W11-0116,P03-1054,0,0.00293524,"ion, maximum entropy and support vector machine classifiers, but as discussed in Section 8, the maximum entropy model performed best in cross-validations on the training data. 5 Unsupervised Approach While supervised learning is effective for many NLP tasks, it is sensitive to the amount of available training data. Unfortunately, the training data for event durations is very small, consisting of only 58 news articles (Pan et al., 2006), and labeling further data is quite expensive. This motivates our desire to find an 1 We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). 147 approach that does not rely on labeled data, but instead utilizes the large amounts of text available on the Web to search for duration-specific patterns. This section describes our web-based approach to learning event durations. 5.1 Web Query Patterns Temporal properties of events are often described explicitly in language-specific constructions which can help us infer an event’s duration. Consider the following two sentences from our corpus: • Many spend hours surfing the Internet. • The answer is coming up in a few minutes. These sentences explicitly describe the duration of the event"
W11-0116,W10-0719,1,0.809058,"Missing"
W11-0116,P06-1050,0,0.150641,"g event distributions based on web counts. We then evaluate both of these models on an existing annotated corpus of event durations and make comparisons to durations we collected using Amazon’s Mechanical Turk. Finally, we present a generated database of event durations. 145 2 Previous Work Early work on extracting event properties focused on linguistic aspect, for example, automatically distinguishing culminated events that have an end point from non-culminated events that do not (Siegel and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed by Pan et al. (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al., 2003) with duration lower and upper bounds. They then trained support vector machines on their annotated corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes, hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and is also a good baseline. We replicate their work and also add new features as described below. Our approac"
W11-0116,W09-2418,0,0.0138796,"sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon with duration distributions for common English events. We first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations. Next, we present our approach to learning"
W11-0116,J00-4004,0,0.0166472,"entation of the latest supervised system for predicting event durations. Next, we present our approach to learning event distributions based on web counts. We then evaluate both of these models on an existing annotated corpus of event durations and make comparisons to durations we collected using Amazon’s Mechanical Turk. Finally, we present a generated database of event durations. 145 2 Previous Work Early work on extracting event properties focused on linguistic aspect, for example, automatically distinguishing culminated events that have an end point from non-culminated events that do not (Siegel and McKeown, 2000). The more fine-grained task of predicting the duration of events was first proposed by Pan et al. (2006), who annotated each event in a small section of the TimeBank (Pustejovsky et al., 2003) with duration lower and upper bounds. They then trained support vector machines on their annotated corpus for two prediction tasks: less-than-a-day vs. more-than-a-day, and bins like seconds, minutes, hours, etc. Their models used features like bags of words, heads of syntactic subjects and objects, and WordNet hypernyms of the events. This work provides a valuable resource in its annotated corpus and i"
W11-0116,D08-1027,0,0.0272319,"Missing"
W11-0116,S07-1014,0,0.011816,"typically lasts years, sometimes months, but almost never seconds, while “look” typically lasts seconds or minutes, but rarely years or decades. Our approach uses web queries to model an event’s typical distribution in the real world. Learning such rich aspectual properties of events is an important area for computational semantics, and should enrich applications like event coreference (e.g., Chen and Ji, 2009) in much the same way that gender has benefited nominal coreference systems. Event durations are also key to building event timelines and other deeper temporal understandings of a text (Verhagen et al., 2007; Pustejovsky and Verhagen, 2009). The contributions of this work are: • Demonstrating how to acquire event duration distributions by querying the web with patterns. • Showing that a system that predicts event durations based only on our web count distributions can outperform a supervised system that requires manually annotated training data. • Making available an event duration lexicon with duration distributions for common English events. We first review previous work and describe our re-implementation and augmentation of the latest supervised system for predicting event durations. Next, we"
W11-0116,P87-1001,0,\N,Missing
W11-1902,W97-1306,0,0.302132,"rmance despite the simplicity of the individual components. This strategy has been successfully used before for information extraction, e.g., in the BioNLP 2009 event extraction shared task (Kim et al., 2009), several of the top systems had a first high-recall component to identify event anchors, followed by high-precision classifiers, which identified event arguments and removed unlikely event candidates (Bj¨orne et al., 2009). In the coreference resolution space, several works have shown that applying a list of rules from highest to lowest precision is beneficial for coreference resolution (Baldwin, 1997; Raghunathan el al., 2010). However, we believe we are the first to show that this high-recall/high-precision strategy yields competitive results for the complete task of coreference resolution, i.e., including mention detection and both nominal and pronominal coreference. 2.1 Mention Detection Sieve In our particular setup, the recall of the mention detection component is more important than its precision, because any missed mentions are guaranteed to affect the final score, but spurious mentions may not impact the overall score if they are left as singletons, which are discarded by our post"
W11-1902,D08-1031,0,0.585246,"ld-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example, the mention selected f"
W11-1902,W09-1402,0,0.0144679,"Missing"
W11-1902,H05-1013,0,0.0128588,"Missing"
W11-1902,N10-1061,0,0.158402,"68.3 68.9 70.0 70.8 R 43.4 43.3 46.3 46.3 CEAFE P 47.8 46.8 50.5 49.6 F1 45.5 45.0 48.3 47.9 R 70.6 71.9 72.0 73.4 BLANC P 76.2 76.6 78.6 79.0 F1 73.0 74.0 74.8 75.8 avg F1 57.8 58.3 60.7 61.4 Table 4: Results on the official test set. as well, whereas in development (lines 6 and 7 in Table 3), gold mentions included only mentions part of an actual coreference chain. This explains the large difference between, say, line 6 in Table 3 and line 4 in Table 4. Our scores are comparable to previously reported state-of-the-art results for coreference resolution with predicted mentions. For example, Haghighi and Klein (2010) compare four state-of-the-art systems on three different corpora and report B3 scores between 63 and 77 points. While the corpora used in (Haghighi and Klein, 2010) are different from the one in this shared task, our result of 68 B3 suggests that our system’s performance is competitive. In this task, our submissions in both the open and the closed track obtained the highest scores. 4 Conclusion In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves). Our approach starts with a high-recall mention detection compo"
W11-1902,N06-2015,0,0.132313,"i Surdeanu, Dan Jurafsky Stanford NLP Group Stanford University, Stanford, CA 94305 {heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.edu Abstract standing, e.g., linking speakers to compatible pronouns. Second, we incorporated a mention detection sieve at the beginning of the processing flow. This sieve filters our syntactic constituents unlikely to be mentions using a simple set of rules on top of the syntactic analysis of text. And lastly, we added a post-processing step, which guarantees that the output of our system is compatible with the shared task and OntoNotes specifications (Hovy et al., 2006; Pradhan et al., 2007). This paper details the coreference resolution system submitted by Stanford at the CoNLL2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of"
W11-1902,D09-1128,0,0.0144125,"e 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example, the mention selected from the cluster {Pre"
W11-1902,Y09-1024,0,0.0223833,"for the gold mention the working meeting of the 32 ”863 Program”. Due to this boundary mismatch, all mentions found to be coreferent with this predicted mention are counted as precision errors, and all mentions in the same coreference cluster with the gold mention are counted as recall errors. Table 3 lists the results of our end-to-end system on the development partition. “External Resources”, which were used only in the open track, includes: (a) a hand-built list of genders of first names that we created, incorporating frequent names from census lists and other sources, (b) an animacy list (Ji and Lin, 2009), (c) a country and state gazetteer, and (d) a demonym list. “Discourse” stands for the sieve introduced in Section 2.3.3. “Semantics” stands for the sieves presented in Section 2.3.2. The table shows that the discourse sieve yields an improvement of almost 2 points to the overall score (row 1 versus 3), and external resources contribute 0.5 points. On the other hand, the semantic sieves do not help (row 3 versus 4). The latter result contradicts our initial experiments, where we measured a minor improvement when these sieves were enabled and gold mentions were used. Our hypothesis is that, wh"
W11-1902,P07-1068,0,0.0061436,"on their precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer str"
W11-1902,P02-1014,0,0.574048,"wo new sieves that address nominal mentions and are inserted based on their precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to p"
W11-1902,W11-1901,0,0.492284,"c coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. 1 Introduction This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011). Our system extends the multi-pass sieve system of Raghunathan et al. (2010), which applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones. Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. We made three considerable extensions to the Raghunathan et al. (2010) model. First, we added five additional sieves,"
W11-1902,D10-1048,1,0.816894,"ntic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track. 1 Introduction This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011). Our system extends the multi-pass sieve system of Raghunathan et al. (2010), which applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones. Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. We made three considerable extensions to the Raghunathan et al. (2010) model. First, we added five additional sieves, the majority of which address the semantic similarity between mentions, e.g.,"
W11-1902,P07-1067,0,0.0116669,"precision in a held-out corpus (see Table 1 for the complete list of sieves deployed in our system). Since these two sieves use 3 We initialize the clusters as singletons and grow them progressively in each sieve. 30 Bare plurals - bare plurals are generic and cannot have a coreferent antecedent. 2.3.2 Semantic-Similarity Sieves We first extend the above system with two new sieves that exploit semantics from WordNet, Wikipedia infoboxes, and Freebase records, drawing on previous coreference work using these databases (Ng & Cardie, 2002; Daum´e & Marcu, 2005; Ponzetto & Strube, 2006; Ng, 2007; Yang & Su, 2007; Bengston & Roth, 2008; Huang et al., 2009; inter alia). Since the input to a sieve is a collection of mention clusters built by the previous (more precise) sieves, we need to link mention clusters (rather than individual mentions) to records in these three knowledge bases. The following steps generate a query for these resources from a mention cluster. First, we select the most representative mention in a cluster by preferring mentions headed by proper nouns to mentions headed by common nouns, and nominal mentions to pronominal ones. In case of ties, we select the longer string. For example,"
W11-1902,W09-1401,0,\N,Missing
W11-1902,N06-1025,0,\N,Missing
W16-1007,W08-2227,1,0.373098,"Missing"
W16-1007,J08-4004,0,0.0549757,"ory for coders who have not captured this relation. The agreement according to Fleiss’s Kappa κ = 0.49 without applying basic closure and κ = 0.51 with closure10 , which shows moderate agreement. For reference, the agreement on semantic link annotation in the most recent clinical TempEval was 0.44 without closure and 0.47 with closure. 6 Related Work Given that there are no prefixed set of event entity spans for straight-forward computation of interannotator agreement, we do the following: among all the annotators, we aggregate the spans of the annotated event entity as the annotation object (Artstein and Poesio, 2008). Then, if there exists a span which is not annotated by one of the coders (annotators) it will be labeled as ‘NONE’ for its category. The agreement according to Fleiss’s Kappa κ = 0.91, which shows substantial agreement on event entity annotation. Although direct comparison of κ values is not possible, as a point of reference, the event One of the most recent temporal annotation schemas is Temporal Histories of Your Medical Event (THYME) (Styler et al., 2014). This annotation guideline was devised for the purpose of establishing timelines in clinical narratives, i.e. the free text portions co"
W16-1007,D13-1178,0,0.0209354,"s. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality"
W16-1007,W13-2322,0,0.0370696,"and auxiliary verbs. Whenever the semantic contribution of the verb is minimal and the nonverb element of the construction is an event in the 4 These verbs are the same as aspectual events characterized by TimeML, which include ‘INITIATES’, ‘CULMINATES’, ‘TERMINATES’, ‘CONTINUES’ and ‘REINITIATES’. TRIPS ontology, we annotate the non-verb element as the event. Thus, we annotate the noun predicate ’offer’ in the sentence ‘Yesterday, John made an offer to buy the house for 350,000’, similarly to the way Abstract Meaning Representation (AMR) drops the light verb and promotes the noun predicate (Banarescu et al., 2013). This annotation is also close to the PropBank annotation of copulas and light verbs (Bonial et al., 2014), where they annotate the noun predicate and predicate adjective as the event; however, PropBank includes an explicit marking of the verb as either a light verb or a copula verb. 3 The Semantic Relations Between Event Entities A more challenging problem than event entity detection is the identification of the semantic relation that holds between events. Events take place in time, hence temporal relations between events are crucial to study. Furthermore, causality plays a crucial role in e"
W16-1007,S15-2136,0,0.059425,"Missing"
W16-1007,S13-2002,0,0.0561543,"medical disorders and conditions, e.g., cancer, heart attack, stroke, etc. – Occurring: e.g., happen, occur. Our semantic framework captures the set of event entities and their pairwise semantic relations, which together form an inter-connected network of events. In this Section we define event entities and discuss their annotation process. 2.1 of interest in various NLP applications. However, there is still no consensus regarding the span of events and how they should be annotated. There has been some good progress in domain-specific annotation of events, e.g., recent Clinical TempEval task (Bethard, 2013) and THYME annotation scheme (Styler et al., 2014), however, the detection of events in broad-coverage natural language has been an ongoing endeavor in the field. One of the existing definitions for event is provided in the TimeML annotation schema (Pustejovsky et al., 2003): Definition Event is mainly used as a term referring to any situation that can happen, occur, or hold. The definition and detection of events has been a topic to the core story. 52 – Natural-phenomenon: e.g., earthquake, tsunami. This ontology has one of the richest event hierarchies, which perfectly serves our purpose of"
W16-1007,bittar-etal-2012-temporal,0,0.0283798,"t pair. Figure 2: Frequency of semantic links in our dataset. Figure 1: Semantic annotation of a sample story. – 9 causal relations: Including ‘cause (before/overlaps)’, ‘enable (before/overlaps)’, ‘prevent (before/overlaps)’, ‘cause-to-end (before/overlaps/during)’ – 4 temporal relations: Including ‘Before’, ‘Overlaps’, ‘Contains’, ‘Identity’. The semantic relation annotation between two events should start with deciding about any causal relations and then, if there was not any causal relation, proceed to choosing any existing temporal relation. 4 Annotating at Story level It has been shown (Bittar et al., 2012) that temporal annotation can be most properly carried out by taking into account the full context for sentences, as opposed to TimeML, which is a surface-based annotation. The scope and goal of this paper very well aligns with this observation. We carry out the annotation at the story level, meaning that we annotate inter-event relations across the five sentences of a story. It suffices to do the event-event relation specification minimally given the transitivity of temporal relations. For example for three consecutive events e1 e2 e3 one should only annotate the ‘before’ relation between e1"
W16-1007,bonial-etal-2014-propbank,0,0.0623713,"construction is an event in the 4 These verbs are the same as aspectual events characterized by TimeML, which include ‘INITIATES’, ‘CULMINATES’, ‘TERMINATES’, ‘CONTINUES’ and ‘REINITIATES’. TRIPS ontology, we annotate the non-verb element as the event. Thus, we annotate the noun predicate ’offer’ in the sentence ‘Yesterday, John made an offer to buy the house for 350,000’, similarly to the way Abstract Meaning Representation (AMR) drops the light verb and promotes the noun predicate (Banarescu et al., 2013). This annotation is also close to the PropBank annotation of copulas and light verbs (Bonial et al., 2014), where they annotate the noun predicate and predicate adjective as the event; however, PropBank includes an explicit marking of the verb as either a light verb or a copula verb. 3 The Semantic Relations Between Event Entities A more challenging problem than event entity detection is the identification of the semantic relation that holds between events. Events take place in time, hence temporal relations between events are crucial to study. Furthermore, causality plays a crucial role in establishing semantic relation between events, specifically in stories. In this Section, we provide details"
W16-1007,P08-1090,1,0.737058,"is commonsense knowledge can be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commons"
W16-1007,P09-1068,1,0.706816,"be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . Thi"
W16-1007,N13-1104,1,0.852947,"knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality2 five-sentence stori"
W16-1007,W14-2903,0,0.0947084,"Missing"
W16-1007,C14-1198,0,0.162142,"annotates temporal and causal relations in parallel (Steven Bethard and Martin, 2008). Bethard et al. annotated a dataset of 1,000 conjoined-event temporal-causal relations, collected from Wall Street Journal corpus. Each event pair was annotated manually with both temporal (BEFORE, AFTER, NO-REL) and causal relations (CAUSE, NO-REL). For example, sentence 12 is an entry in their dataset. This dataset makes no distinction between various types of causal relation. (12) Fuel tanks had leaked and contaminated the soil. - (leaked BEFORE contaminated) - (leaked CAUSED contaminated). A recent work (Mirza and Tonelli, 2014) has proposed a TimeML-style annotation standard for capturing causal relations between events. They mainly introduce ‘CLINK’, analogous to ‘TLINK’ in TimeML, to be added to the existing TimeML link tags. Under this framework, Mirza et al (Mirza and Tonelli, 2014) annotates 318 CLINKs in TempEval3 TimeBank. They only annotate explicit causal relations signaled by linguistic markers, such as {because of, as a result of, due to, so, therefore, 59 thus}. Another relevant work is Richer Event Descriptions (RED) (Ikuta et al., 2014), which combines event coreference and THYME annotations, and also"
W16-1007,N16-1098,1,0.0733092,"s need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality2 five-sentence stories 1 These stories can be found here: http://cs. rochester.edu/nlp/rocstories 2 Each of these stories have the following major characteristics: is realistic, has a clear beginning and ending where something happens in between, does not include anything irrelevant 51 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 51–61, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics that are fu"
W16-1007,P15-1019,0,0.0541783,"Missing"
W16-1007,E14-1024,0,0.0242643,"g happens in between, does not include anything irrelevant 51 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 51–61, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics that are full of stereotypical causal and temporal relations between events, making them a perfect resource for learning narrative schemas. One of the prerequisites for learning scripts from these stories is to extract events and find inter-event semantic relations. Earlier work (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015) defines verbs as events and uses TimeML-based (Pustejovsky et al., 2003) learning for temporal ordering of events. This clearly has many shortcomings, including, but not limited to (1) not capturing a wide range of non-verbal events such as ‘earthquake’, (2) not capturing a more comprehensive set of semantic relations between events such as causality, which is a core relation in stories. In this paper we formally define a new comprehensive semantic framework for capturing stereotypical event-event temporal and causal relations in commonsense stories, the details of whi"
W16-1007,W11-0419,0,0.0270517,"κ values is not possible, as a point of reference, the event One of the most recent temporal annotation schemas is Temporal Histories of Your Medical Event (THYME) (Styler et al., 2014). This annotation guideline was devised for the purpose of establishing timelines in clinical narratives, i.e. the free text portions contained in electronic health records. In their work, they combine the TimeML annotation schema with Allen Interval Algebra, identifying the five temporal relations BEFORE, OVERLAP, BEGINS-ON, ENDS-ON, and CONTAINS. Of note is that they adopt the notion of narrative containers (Pustejovsky and Stubbs, 2011), which are time slices in which events can take place, such as DOCTIME (time of the report) and before DOCTIME. As such, the THYME guideline focuses on ordering events with respect to specific time intervals, while in our work, we are only focused on the relation between two events, without concern for ordering. Their simplification of temporal links is similar to ours, however, our reasoning for simplification takes 9 This is based on the fact that any relation such as ‘A enablebefore B’ or ‘A overlaps B’ can be naively approximated to ‘A before B’. 10 Temporal closure (Gerevini et al., 1995"
W16-1007,D15-1195,0,0.0555407,"Missing"
W16-1007,bethard-etal-2008-building,0,0.134989,"Missing"
W16-1007,Q14-1012,0,0.0456985,"Missing"
W16-1007,H89-1033,0,0.739052,"ons between events. By annotating a total of 1,600 sentences in the context of 320 five-sentence short stories sampled from ROCStories corpus, we demonstrate that these stories are indeed full of causal and temporal relations. Furthermore, we show that the CaTeRS annotation scheme enables high inter-annotator agreement for broad-coverage event entity annotation and moderate agreement on semantic link annotation. 1 Introduction Understanding events and their relations in natural language has become increasingly important for various NLP tasks. Most notably, story understanding (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000) which is an extremely challenging task in natural language understanding, is highly dependent on understanding events and their relations. Recently, we have witnessed a renewed interest in story and narrative understanding based on the progress made in core NLP tasks. Perhaps the biggest challenge of story understanding (and story generation) is having commonsense knowledge for the interpretation of narrative events. This commonsense knowledge can be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences t"
W16-1007,miltsakaki-etal-2004-penn,0,\N,Missing
W16-1007,prasad-etal-2008-penn,0,\N,Missing
W17-0905,P08-1090,1,0.845144,"slowly/unknowingly been altered to accommodate LMs. Most notably, tests are auto-generated rather than by hand, and no effort is taken to include core script events. Recent work is not clear on evaluation goals and contains contradictory results. We implement several models, and show that the test’s bias to high-frequency events explains the inconsistencies. We conclude with recommendations on how to return to the test’s original intent, and offer brief suggestions on a path forward. 1 2 Previous Work 2.1 The Original Narrative Event Cloze The narrative event cloze task was first proposed in Chambers and Jurafsky (2008). These papers introduced the first models that automatically induced event structures like Schankian scripts from unlabeled text. They learned chains of events that form common-sense structures. An event was defined as a verb/dependency tuple where the main entity in a story (the protagonist) filled the typed dependency of the verb. The following is an example with its corresponding event chain: Text The police arrested Jon but he escaped. Jon fled the country. Chain (arrested, object), (escaped, subj), (fled, subj) Introduction A small but growing body of work is looking at learning real-wor"
W17-0905,E12-1034,0,0.206394,"Missing"
W17-0905,L16-1555,0,0.207648,"Missing"
W17-0905,K16-1008,0,0.561557,"Missing"
W17-0905,N16-1098,1,0.852328,"her, the test must not include events brought in through parser and coreference errors. By evaluating on parser output as gold data, we evaluate how well our models match our flawed text pre-processing tools. We acknowledge that human involvement is expensive, but the current trend to automate evaluations does not appear to be evaluating commonsense knowledge. Finally, although this paper focuses on the narrative event cloze, we recognize that different evaluations are also possible. However, the traits of human-annotation and core-events seem to be required. One interesting task this year is Mostafazadeh et al. (2016) and the Story Cloze (manually created). Different from event chains, it still meets the requirements and provides a very large common corpus with 100k short stories. Another recent proposal is the InScript Corpus from Modi et al. (2016). They used Amazon Turk to create 1000 stories covering 10 predefined scenarios. While not as large and diverse as the Story Cloze, the entire corpus was annotated for gold events, coreference, and entities. This is an interesting new resource that avoids many of the problems discussed above, although issues of an event’s coreness to a narrative may still need"
W17-0905,E14-1024,0,0.614321,"ordering information because document order does not imply real-world order, and scripts focused on real-world structure. This change naturally benefits text language models. 3 Data Processing To be consistent with recent work, we use the English Gigaword Corpus for training and test. We parse into typed dependencies with the CoreNLP toolkit, run coreference, and extract event chains connected by a single entity’s mentions. Each coreference entity then extracts its event chain, made up of the predicates in which it is a subject, object, or preposition argument. An event is a tuple similar to Pichotta and Mooney (2014): (s, o, p, event) where s/o/p are the subject, object, and preposition unique entity IDs. Entity singletons All Chains: Instead of selecting the central entity in a document and testing that scenario’s chain, they included all entity chains. Different papers vary on this detail, but all appear to auto-extract multiple chains per document. Some include minimum chain length requirements. All Events: Fourth, the LM cloze includes all repeated events in a chain. If an event chain contains 5 ’said’ events, 5 cloze tests with the same answer ‘said’ are in the evaluation. Critically, variants of 42"
W17-0905,D15-1195,0,0.338762,"Missing"
W17-0905,N16-1067,0,\N,Missing
W17-0906,W17-0908,0,0.124042,"Missing"
W17-0906,W17-0909,0,0.387095,"d. Submissions The Shared Task was conducted through CodaLab competitions4 . We received a total of 18 registrations, out of which eight teams participated: four teams from the US, three teams from Germany and one team from India. In the following, we provide short paragraphs summarizing our baseline and approaches of the submissions. More details can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they lexically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as wo"
W17-0906,W17-0912,0,0.164607,"ls can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they lexically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as word and character n-gram in each candidate ending (independent of story). These style features have been shown useful in other tasks such as age, gender, or native language detection. ROCNLP (baseline) Two feed-forward neural networks trained jointly on ROCStories to project the four-sentences context and the right fifth sentence into the"
W17-0906,H05-1044,0,0.0509919,"Missing"
W17-0906,H89-1033,0,0.679051,"Missing"
W17-0906,N16-1098,1,0.310191,"esident Marco was excited to be a registered voter. He thought long and hard about who to vote for. Finally he had decided on his favorite candidate. He placed his vote for that candidate. Marco was proud that he had finally voted. Spaghetti Sauce Tina made spaghetti for her boyfriend. It took a lot of work, but she was very proud. Her boyfriend ate the whole plate and said it was good. Tina tried it herself, and realized it was disgusting. She was touched that he pretended it was good to spare her feelings. Table 2: Example ROCStories instances from the Winter 2017 release. 3 As described in Mostafazadeh et al. (2016), the SCT cases are collected through Amazon Mechanical Turk (Mturk) on the basis of the ROCStories corpus, a collection of five-sentence everyday life stories which are full of stereotypical sequence of events. To construct SCT cases, they randomly sampled complete five-sentence stories from the ROCStories corpus and presented only the first four sentences of each story to the Mturk workers. Then, for each story, a worker was asked to write a ‘right ending’ and a ‘wrong ending’. This resulting set was further filtered by human verification: they compile each SCT case into two independent five"
W17-0906,W17-0911,0,0.0613484,"2017). Linguistic features include aspects of sentiment, negation, pronominalization and n-gram overlap between the story and possible endings. 98,159 1,871 1,871 Table 3: The size of the provided shared task datasets. uate the systems in terms of accuracy, which we #correct measure as #test Any other details recases . garding our shared task can be accessed via our shared task page http://cs.rochester. edu/nlp/rocstories/LSDSem17/. 4 roemmele (University of Southern California). Binary classifier based on a recurrent neural network that operates over (sentence-level) Skipthought embeddings (Roemmele et al., 2017). For training, different data augmentation methods are explored. Submissions The Shared Task was conducted through CodaLab competitions4 . We received a total of 18 registrations, out of which eight teams participated: four teams from the US, three teams from Germany and one team from India. In the following, we provide short paragraphs summarizing our baseline and approaches of the submissions. More details can be found in the respective system description papers. mflor (Educational Testing Service). Rulebased combination of two systems that score possible endings in terms of how well they l"
W17-0906,W17-0910,0,0.073945,"Missing"
W17-0906,W17-0907,0,0.273047,"exically cohere with and fit the sentiment of the given story (Flor and Somasundaran, 2017). Sentiment is given priority, and the model backs off to lexical coherence based on pointwise mutual information scores. Pranav Goel (IIT Varanasi). Ensemble model that takes into account scores from two systems that measure overlap in sentiment and sentence similarity between the story and the two possible endings (Goel and Singh, 2017). msap (University of Washington). Linear classifier based on language modeling probabilities of the entire story, and linguistic features of only the ending sentences (Schwartz et al., 2017). These ending “style” features include sentence length as well as word and character n-gram in each candidate ending (independent of story). These style features have been shown useful in other tasks such as age, gender, or native language detection. ROCNLP (baseline) Two feed-forward neural networks trained jointly on ROCStories to project the four-sentences context and the right fifth sentence into the same vector space. This model is called Deep Structured Semantic Model (DSSM) (Huang et al., 2013) and had outperformed all the other baselines reported in Mostafazadeh et al. (2016). cogcomp"
W17-1104,N13-1121,0,0.060247,"Missing"
W17-1104,E06-1002,0,0.0423653,"of Facts John Kielbowicz and Ostendorf, 2015). This paper is similar in challenge in that we also start with only the user’s name, but the methods are very different. This paper is also a new form of entity linking. Entity linking is an active research field that aims to resolve an entity mention (e.g., ”michael jordan”) to an entry in a knowledge base (e.g., michael jordan’s wikipedia page). Most work in this field relies on the text context around the mention to measure similarity with the text description of the entity in the KB, such as a wikipedia entry’s text (Adafre and de Rijke, 2005; Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Dredze et al., 2010; Ratinov et al., 2011; Han et al., 2011; Demartini et al., 2012; Moro et al., 2014; Moro and Navigli, 2015). All of these papers assume they have knowledge base entries with text to assist in resolving entity mentions. This paper is different in that we don’t have a knowledge base, but just an online alias. We start from the assumption that we don’t have text from that alias, and must rely solely on observed mentions and properties of the name/alias itself. Finally, user linking across website communities is an active res"
W17-1104,D11-1120,0,0.0347893,"2013; Volkova et al., 2014). Pla et al. (2014) even uses sentiment analysis. This paper is related to attribute extraction in the desire to learn about an online user. However, the task at hand is to resolve mentions of a user’s online alias (i.e., twitter handle) and mentions of a named entity (i.e., a business or a person’s real name). Unlike the body of work on attribute extraction, we assume we do not have an entity’s body of published text, but instead just observed their name in text. More related to our goal of name understanding is research on gender identification (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012; Filippova, 2012; Ciot et al., 2013). In many of these, the name of a user is informative. Most work thus uses the name as an indicator, but then also uses the user’s text posts to assist. The name itself offers insight into this answer, and some models rely first on dictionaries of names before backing off to a machine learner trained on user tweets (Liu and Ruths, 2013; Volkova and Yarowsky, 2014). Chen et al. (Chen et al., 2015b) pursued a novel line of investigation which uses only a user’s name, and infers visual attributes by using clickthrough data on name searches and"
W17-1104,D13-1114,0,0.0166431,"es sentiment analysis. This paper is related to attribute extraction in the desire to learn about an online user. However, the task at hand is to resolve mentions of a user’s online alias (i.e., twitter handle) and mentions of a named entity (i.e., a business or a person’s real name). Unlike the body of work on attribute extraction, we assume we do not have an entity’s body of published text, but instead just observed their name in text. More related to our goal of name understanding is research on gender identification (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012; Filippova, 2012; Ciot et al., 2013). In many of these, the name of a user is informative. Most work thus uses the name as an indicator, but then also uses the user’s text posts to assist. The name itself offers insight into this answer, and some models rely first on dictionaries of names before backing off to a machine learner trained on user tweets (Liu and Ruths, 2013; Volkova and Yarowsky, 2014). Chen et al. (Chen et al., 2015b) pursued a novel line of investigation which uses only a user’s name, and infers visual attributes by using clickthrough data on name searches and web images. Although very different in the type of pr"
W17-1104,D15-1240,0,0.0376906,"Missing"
W17-1104,C10-1032,0,0.0286674,"lenge in that we also start with only the user’s name, but the methods are very different. This paper is also a new form of entity linking. Entity linking is an active research field that aims to resolve an entity mention (e.g., ”michael jordan”) to an entry in a knowledge base (e.g., michael jordan’s wikipedia page). Most work in this field relies on the text context around the mention to measure similarity with the text description of the entity in the KB, such as a wikipedia entry’s text (Adafre and de Rijke, 2005; Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Dredze et al., 2010; Ratinov et al., 2011; Han et al., 2011; Demartini et al., 2012; Moro et al., 2014; Moro and Navigli, 2015). All of these papers assume they have knowledge base entries with text to assist in resolving entity mentions. This paper is different in that we don’t have a knowledge base, but just an online alias. We start from the assumption that we don’t have text from that alias, and must rely solely on observed mentions and properties of the name/alias itself. Finally, user linking across website communities is an active research area. Research typically focuses on finding similarities in the so"
W17-1104,D12-1135,0,0.0249588,"l. (2014) even uses sentiment analysis. This paper is related to attribute extraction in the desire to learn about an online user. However, the task at hand is to resolve mentions of a user’s online alias (i.e., twitter handle) and mentions of a named entity (i.e., a business or a person’s real name). Unlike the body of work on attribute extraction, we assume we do not have an entity’s body of published text, but instead just observed their name in text. More related to our goal of name understanding is research on gender identification (Rao et al., 2010; Burger et al., 2011; Van Durme, 2012; Filippova, 2012; Ciot et al., 2013). In many of these, the name of a user is informative. Most work thus uses the name as an indicator, but then also uses the user’s text posts to assist. The name itself offers insight into this answer, and some models rely first on dictionaries of names before backing off to a machine learner trained on user tweets (Liu and Ruths, 2013; Volkova and Yarowsky, 2014). Chen et al. (Chen et al., 2015b) pursued a novel line of investigation which uses only a user’s name, and infers visual attributes by using clickthrough data on name searches and web images. Although very differe"
W17-1104,P14-1018,0,0.0334624,"Missing"
W17-1104,S15-2049,0,0.0265008,"also a new form of entity linking. Entity linking is an active research field that aims to resolve an entity mention (e.g., ”michael jordan”) to an entry in a knowledge base (e.g., michael jordan’s wikipedia page). Most work in this field relies on the text context around the mention to measure similarity with the text description of the entity in the KB, such as a wikipedia entry’s text (Adafre and de Rijke, 2005; Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Dredze et al., 2010; Ratinov et al., 2011; Han et al., 2011; Demartini et al., 2012; Moro et al., 2014; Moro and Navigli, 2015). All of these papers assume they have knowledge base entries with text to assist in resolving entity mentions. This paper is different in that we don’t have a knowledge base, but just an online alias. We start from the assumption that we don’t have text from that alias, and must rely solely on observed mentions and properties of the name/alias itself. Finally, user linking across website communities is an active research area. Research typically focuses on finding similarities in the social network structure (Backstrom et al., 2007; Narayanan and Shmatikov, 2009; Tan et al., 2014; Liu et al.,"
W17-1104,Q14-1019,0,0.0357172,"rent. This paper is also a new form of entity linking. Entity linking is an active research field that aims to resolve an entity mention (e.g., ”michael jordan”) to an entry in a knowledge base (e.g., michael jordan’s wikipedia page). Most work in this field relies on the text context around the mention to measure similarity with the text description of the entity in the KB, such as a wikipedia entry’s text (Adafre and de Rijke, 2005; Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Dredze et al., 2010; Ratinov et al., 2011; Han et al., 2011; Demartini et al., 2012; Moro et al., 2014; Moro and Navigli, 2015). All of these papers assume they have knowledge base entries with text to assist in resolving entity mentions. This paper is different in that we don’t have a knowledge base, but just an online alias. We start from the assumption that we don’t have text from that alias, and must rely solely on observed mentions and properties of the name/alias itself. Finally, user linking across website communities is an active research area. Research typically focuses on finding similarities in the social network structure (Backstrom et al., 2007; Narayanan and Shmatikov, 2009; Tan"
W17-1104,C14-1019,0,0.0712623,"Missing"
W17-1104,D12-1005,0,0.0513913,"Missing"
W17-1104,P11-1138,0,\N,Missing
