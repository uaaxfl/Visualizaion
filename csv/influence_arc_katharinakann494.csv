2020.aacl-main.56,Q16-1022,0,0.0273759,"Missing"
2020.aacl-main.56,2020.emnlp-main.618,0,0.176607,"18) is a questionanswering dataset consisting of passages extracted from Wikipedia articles and crowd-sourced questions and answers. In SQuAD version 1.1, each example consists of a context passage and a question, and the answer is a text span from the context. SQuAD version 2.0 includes additional questions with no answers, written adversarially by crowdworkers. We use both versions in our experiments. 2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering (XQuAD; Artetxe et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigenbaum et al., 2017, 2018), which requires identifying parallel sentences from corpora of different languages; and Tatoeba (Artetxe and Schwenk, 2019), which involves aligning pairs of sentences wi"
2020.aacl-main.56,D19-1252,0,0.034384,"Missing"
2020.aacl-main.56,D19-1243,0,0.0512798,"Missing"
2020.aacl-main.56,Q17-1024,0,0.0927489,"Missing"
2020.aacl-main.56,P18-4020,0,0.0202348,"Missing"
2020.aacl-main.56,2020.acl-main.653,0,0.118049,"cted from Wikipedia articles and crowd-sourced questions and answers. In SQuAD version 1.1, each example consists of a context passage and a question, and the answer is a text span from the context. SQuAD version 2.0 includes additional questions with no answers, written adversarially by crowdworkers. We use both versions in our experiments. 2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering (XQuAD; Artetxe et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigenbaum et al., 2017, 2018), which requires identifying parallel sentences from corpora of different languages; and Tatoeba (Artetxe and Schwenk, 2019), which involves aligning pairs of sentences with the same meaning. Among the 9 tasks, BUCC and Tatoeba are s"
2020.aacl-main.56,P19-1441,0,0.15449,"an additional task for intermediate training, using the same multi-task sampling strategy as above. XLM-R + Translated Intermediate Task We translate intermediate-task training and validation data for three tasks and fine-tune XLM-R on translated intermediate-task data before we train and evaluate on the target tasks. 3.2 Software Experiments were carried out using the jiant (Phang et al., 2020) library (2.0 alpha), based on PyTorch (Paszke et al., 2019) and Transformers (Wolf et al., 2019). 7 XLM-R Large (Conneau et al., 2020) is a 550m-parameter variant of the RoBERTa masked language model (Liu et al., 2019b) trained on a cleaned version of CommonCrawl on 100 languages. Notably, Yoruba is used in the POS and NER XTREME tasks but not is not in the set of 100 languages. Results We train three versions of each intermediate-task model with different random seeds. For each run, we compute the average target-task performance across languages, and report the median performance across the three random seeds. Intermediate-Task Training As shown in Table 2, no single intermediate task yields positive transfer across all target tasks. The target tasks TyDiQA, BUCC and Tatoeba see consistent gains from most"
2020.aacl-main.56,2021.ccl-1.108,0,0.128091,"Missing"
2020.aacl-main.56,D17-1269,0,0.049625,"Missing"
2020.aacl-main.56,D11-1006,0,0.110474,"Missing"
2020.aacl-main.56,2020.acl-main.441,0,0.167403,"r from literature. Pruksachatkun et al. (2020) shows that MNLI (of which ANLI+ is a superset), CommonsenseQA, Cosmos QA and HellaSwag yield positive transfer to a range of downstream English-language tasks in intermediate training. CCG involves token-wise prediction and is similar to the POS and NER target tasks. Both versions of SQuAD are widely-used questionanswering tasks, while QQP is semantically similar to sentence retrieval target tasks (BUCC and Tatoeba) as well as PAWS-X, another paraphrasedetection task. ANLI + MNLI + SNLI (ANLI+ ) The Adversarial Natural Language Inference dataset (Nie et al., 2020) is collected using model-in-the-loop crowdsourcing as an extension of the Stanford Natural Language Inference (SNLI; Bowman et al., 2015) and Multi-Genre Natural Language Inference (MNLI; Williams et al., 2018) corpora. We follow Nie et al. (2020) and use the concatenated ANLI, MNLI and SNLI training sets, which we refer to as ANLI+ . For all three natural language inference tasks, examples consist of premise and hypothesis sentence pairs, and the task is to classify the relationship between the premise and hypothesis as entailment, contradiction, or neutral. CommonsenseQA CommonsenseQA (Talm"
2020.aacl-main.56,P17-1178,0,0.0173271,"experiments. 2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering (XQuAD; Artetxe et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigenbaum et al., 2017, 2018), which requires identifying parallel sentences from corpora of different languages; and Tatoeba (Artetxe and Schwenk, 2019), which involves aligning pairs of sentences with the same meaning. Among the 9 tasks, BUCC and Tatoeba are sentence retrieval tasks that do not include training sets, and are scored based on the similarity of learned representations (see Appendix A). XQuAD, TyDiQA and Tatoeba do not include development sets separate from the test sets.4 For all XTREME tasks, we follow the training and evaluation protocol described in the benchmark p"
2020.aacl-main.56,2020.emnlp-main.617,0,0.103534,"Missing"
2020.aacl-main.56,2020.acl-demos.15,1,0.91429,"ta before using it on a non-English target task, which can lead to the catastrophic forgetting of other languages acquired during pretraining. We investigate whether continuing to train on the multilingual MLM pretraining objective while fine-tuning on an English intermediate task can prevent catastrophic forgetting of the target languages and improve downstream transfer performance. We construct a multilingual corpus across the 40 languages covered by the XTREME benchmark using Wikipedia dumps from April 14, 2020 for each language and the MLM data creation scripts from the jiant 1.3 library (Phang et al., 2020). In total, we use 2 million sentences sampled across all 40 languages using the sampling ratio from Conneau and Lample (2019) with ↵ = 0.3. 2.4 Translated Intermediate-Task Training Large-scale labeled datasets are rarely available in languages other than English for most languageunderstanding benchmark tasks. Given the availability of increasingly performant machine translation models, we investigate if using machinetranslated intermediate-task data can improve samelanguage transfer performance, compared to using English intermediate task data. We translate training and validation data of th"
2020.aacl-main.56,2020.eamt-1.61,0,0.0114263,"sks. Given the availability of increasingly performant machine translation models, we investigate if using machinetranslated intermediate-task data can improve samelanguage transfer performance, compared to using English intermediate task data. We translate training and validation data of three intermediate tasks: QQP, HellaSwag, and MNLI. We choose these tasks based on the size of the training sets and because their examplelevel (rather than word-level) labels can be easily mapped onto translated data. To translate QQP and HellaSwag, we use pretrained machine translation models from OPUS-MT (Tiedemann and Thottingal, 2020). These models are trained with Marian-NMT (Junczys-Dowmunt et al., 2018) on OPUS data (Tiedemann, 2012), which integrates several resources depending on the available corpora for the language pair. For MNLI, we use the publicly available machine-translated training data of XNLI provided by the XNLI authors.6 We use German, Russian, and Swahili translations of 3 http://data.quora.com/ First-Quora-DatasetRelease-Question-Pairs 4 UDPOS also does not include development sets for Kazakh, Thai, Tagalog or Yoruba. 560 5 https://github.com/google-research/ xtreme 6 According to Conneau et al. (2018),"
2020.aacl-main.56,P19-1439,1,0.784162,"Missing"
2020.aacl-main.56,N18-1101,1,0.808471,"in intermediate training. CCG involves token-wise prediction and is similar to the POS and NER target tasks. Both versions of SQuAD are widely-used questionanswering tasks, while QQP is semantically similar to sentence retrieval target tasks (BUCC and Tatoeba) as well as PAWS-X, another paraphrasedetection task. ANLI + MNLI + SNLI (ANLI+ ) The Adversarial Natural Language Inference dataset (Nie et al., 2020) is collected using model-in-the-loop crowdsourcing as an extension of the Stanford Natural Language Inference (SNLI; Bowman et al., 2015) and Multi-Genre Natural Language Inference (MNLI; Williams et al., 2018) corpora. We follow Nie et al. (2020) and use the concatenated ANLI, MNLI and SNLI training sets, which we refer to as ANLI+ . For all three natural language inference tasks, examples consist of premise and hypothesis sentence pairs, and the task is to classify the relationship between the premise and hypothesis as entailment, contradiction, or neutral. CommonsenseQA CommonsenseQA (Talmor et al., 2019) is a multiple-choice QA dataset generated by crowdworkers based on clusters of concepts from ConceptNet (Speer et al., 2017). Cosmos QA Cosmos QA is multiple-choice commonsense-based reading com"
2020.aacl-main.56,D19-1382,0,0.024611,"l questions with no answers, written adversarially by crowdworkers. We use both versions in our experiments. 2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering (XQuAD; Artetxe et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigenbaum et al., 2017, 2018), which requires identifying parallel sentences from corpora of different languages; and Tatoeba (Artetxe and Schwenk, 2019), which involves aligning pairs of sentences with the same meaning. Among the 9 tasks, BUCC and Tatoeba are sentence retrieval tasks that do not include training sets, and are scored based on the similarity of learned representations (see Appendix A). XQuAD, TyDiQA and Tatoeba do not include development sets separate from the test sets.4 For"
2020.aacl-main.56,D18-1009,0,0.0247261,"-choice QA dataset generated by crowdworkers based on clusters of concepts from ConceptNet (Speer et al., 2017). Cosmos QA Cosmos QA is multiple-choice commonsense-based reading comprehension dataset (Huang et al., 2019b) generated by crowdworkers, with a focus on the causes and effects of events. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning dataset framed as a fourway multiple choice task, where examples consist of an incomplete paragraph and four choices of spans, only one of which is a plausible continuation of the scenario. It is built using adversarial filtering (Zellers et al., 2018; Le Bras et al., 2020) with BERT. 2 If a word is tokenized into sub-word tokens, we use the representation of the first token for the tag prediction for that word as in Devlin et al. (2019a). 559 and their sample implementation.5 Intermediateand target-task statistics are shown in Table 1. MNLI In additional to the full ANLI+ , we also consider the MNLI task as a standalone intermediate task because of its already large and diverse training set. QQP Quora Question Pairs3 is a paraphrase detection dataset. Examples in the dataset consist of two questions, labeled for whether they are semantica"
2020.aacl-srw.13,P19-1157,0,0.028483,"ork Sequence-to-sequence models. Popular neural architectures for sequence-to-sequence tasks include those based on LSTMs or GRUs in combination with attention (Bahdanau et al., 2015), transformer models, which use attention instead of recurrence (Vaswani et al., 2017), or pointergenerator models based on LSTMs (See et al., 2017). Sequence-to-sequence models have been applied to a large set of NLP tasks, including translation (Bahdanau et al., 2015; Vaswani et al., 2017), summarization (Raffel et al., 2019), morphological generation (Kann and Sch¨utze, 2016), or historical text normalization (Flachs et al., 2019). To the best of our knowledge, pointer-generators have so far only been applied to tasks with overlappings source and target vocabularies (See et al., 2017; Sharma et al., 2018; Deaton et al., 2019). Here, we propose a pointer-generator transformer for tasks with disjoint vocabularies. G2P. Early algorithms for G2P relied on handwritten parser-based rules in the format of Chomsky-Halle rewrite – or LTS – rules (Chomsky and Halle, 1968). Subsequently, other techniques have been developed, including rule-based systems (Black et al., 1998), maximum entropy models (Chen, 2003), LSTMs (Rao et al.,"
2020.aacl-srw.13,K18-3001,1,0.896528,"Missing"
2020.acl-main.467,N19-1300,0,0.428418,"2019b). We aim to answer the following specific questions: • What kind of tasks tend to make good intermediate tasks across a wide variety of target tasks? Introduction Unsupervised pretraining—e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b)—has recently pushed the state of the art on many natural language understanding tasks. One method of further improving pretrained models that has been shown to be broadly helpful is to first finetune a pretrained model on an intermediate task, before fine-tuning again on the target task of interest (Phang et al., 2018; Wang et al., 2019a; Clark et al., 2019a; Sap et al., 2019), also referred to as ∗ Equal contribution. • Which linguistic skills does a model learn from intermediate-task training? • Which skills learned from intermediate tasks help the model succeed on which target tasks? The first question is the most straightforward: it can be answered by a sufficiently exhaustive search over possible intermediate–target task pairs. The second and third questions address the why rather than the when, and differ in a crucial detail: A 5231 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231–5247 c J"
2020.acl-main.467,W19-4828,0,0.39453,"2019b). We aim to answer the following specific questions: • What kind of tasks tend to make good intermediate tasks across a wide variety of target tasks? Introduction Unsupervised pretraining—e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b)—has recently pushed the state of the art on many natural language understanding tasks. One method of further improving pretrained models that has been shown to be broadly helpful is to first finetune a pretrained model on an intermediate task, before fine-tuning again on the target task of interest (Phang et al., 2018; Wang et al., 2019a; Clark et al., 2019a; Sap et al., 2019), also referred to as ∗ Equal contribution. • Which linguistic skills does a model learn from intermediate-task training? • Which skills learned from intermediate tasks help the model succeed on which target tasks? The first question is the most straightforward: it can be answered by a sufficiently exhaustive search over possible intermediate–target task pairs. The second and third questions address the why rather than the when, and differ in a crucial detail: A 5231 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231–5247 c J"
2020.acl-main.467,P18-1198,0,0.363494,"raining, and fine-tuning on a target or probing task. on each target and probing task individually. Target tasks are tasks of interest to the general community, spanning various facets of natural language, domains, and sources. Probing tasks, while potentially similar in data source to target tasks such as with CoLA, are designed to isolate the presence of particular linguistic capabilities or skills. For instance, solving the target task BoolQ (Clark et al., 2019a) may require various skills including coreference and commonsense reasoning, while probing tasks like the SentEval probing suite (Conneau et al., 2018) target specific syntactic and metadatalevel phenomena such as subject-verb agreement and sentence length detection. 2.2 Tasks Table 1 presents an overview of the intermediate and target tasks. 2.2.1 Intermediate Tasks We curate a diverse set of tasks that either represent an especially large annotation effort or that have been shown to yield positive transfer in prior work. The resulting set of tasks cover question answering, commonsense reasoning, and natural language inference. QAMR The Question–Answer Meaning Representations dataset (Michael et al., 2018) is a crowdsourced QA task consisti"
2020.acl-main.467,N19-1423,0,0.358065,"en question. We perform a broad survey of intermediate and target task pairs, following an experimental pipeline similar to Phang et al. (2018) and Wang et al. (2019a). This differs from previous work in that we use a larger and more diverse set of intermediate and target tasks, introduce additional analysis-oriented probing tasks, and use a better-performing base model RoBERTa (Liu et al., 2019b). We aim to answer the following specific questions: • What kind of tasks tend to make good intermediate tasks across a wide variety of target tasks? Introduction Unsupervised pretraining—e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b)—has recently pushed the state of the art on many natural language understanding tasks. One method of further improving pretrained models that has been shown to be broadly helpful is to first finetune a pretrained model on an intermediate task, before fine-tuning again on the target task of interest (Phang et al., 2018; Wang et al., 2019a; Clark et al., 2019a; Sap et al., 2019), also referred to as ∗ Equal contribution. • Which linguistic skills does a model learn from intermediate-task training? • Which skills learned from intermediate tasks help the model succe"
2020.acl-main.467,D15-1076,0,0.0227138,"at is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. We use the CCG supertagging task, which is the task of assigning tags to individual word tokens that jointly determine the parse of the sentence. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning task that tests a model’s ability to choose the most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling dataset (QA-SRL; He et al., 2015) for a QA task that is derived from a semantic role labeling task. Each example, which consists of a set of questions and answers, corresponds to a predicate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL is specifically focused on verbs. SST-2 The Stanford sentiment treebank (Socher et al., 2013) is a sentiment classification task based on movie reviews. We use the binary sentence classification version of the task. QQP The Quora Question Pairs dataset1 is constructed based on questions posted on the community question"
2020.acl-main.467,W09-2415,0,0.124413,"Missing"
2020.acl-main.467,N19-1419,0,0.0393455,"ntermediate task, three-step procedure. Similar work (Lin et al., 2019b) has been done on cross-lingual transfer—the analogous challenge of transferring learned knowledge from a highresource to a low-resource language. Many recent works have attempted to understand the knowledge and linguistic skills BERT learns, for instance by analyzing the language model surprisal for subject–verb agreements (Goldberg, 2018), identifying specific knowledge or phenomena encapsulated in the representations learned by BERT using probing tasks (Tenney et al., 2019b,a; Warstadt et al., 2019a; Lin et al., 2019a; Hewitt and Manning, 2019; Jawahar et al., 2019), analyzing the attention heads of BERT (Clark et al., 2019b; 5238 Coenen et al., 2019; Lin et al., 2019a; Htut et al., 2019), and testing the linguistic generalizations of BERT across runs (McCoy et al., 2019). However, relatively little work has been done to analyze fine-tuned BERT-style models (Wang et al., 2019a; Warstadt et al., 2019a). 6 Conclusion and Future Work This paper presents a large-scale study on when and why intermediate-task training works with pretrained models. We perform experiments on RoBERTa with a total of 110 pairs of intermediate and target task"
2020.acl-main.467,J07-3004,0,0.00896312,"et al., 2019). The questions concern the causes or effects of events that require reasoning not only based on the exact text spans in the context, but also wide-range abstractive commonsense reasoning. It differs from CommonsenseQA in that it focuses on causal and deductive commensense reasoning and that it requires reading comprehension over an auxiliary passage, rather than simply answering a freestanding question. SocialIQA SocialIQA (Sap et al., 2019) is a task for multiple choice QA. It tests for reasoning surrounding emotional and social intelligence in everyday situations. CCG CCGbank (Hockenmaier and Steedman, 2007) is a task that is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. We use the CCG supertagging task, which is the task of assigning tags to individual word tokens that jointly determine the parse of the sentence. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning task that tests a model’s ability to choose the most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling data"
2020.acl-main.467,N06-2015,0,0.0334048,"ich tests a model’s ability to understand how ideas in the various clauses relate to each other. AJ-EOS is a task that tests a model’s ability to identify grammatical sentences without indicators such as punctuation marks and capitalization, and consists of grammatical text that are removed of punctuation. Edge-Probing Tasks The edge probing (EP) tasks are a set of core NLP labeling tasks, collected by Tenney et al. (2019b) and cast into Boolean classification. These tasks focus on the syntactic and semantic relations between spans in a sentence. The first five tasks use the OntoNotes corpus (Hovy et al., 2006): Part-of-Speech tagging (EP-POS) is a task that tests a model’s ability to predict the syntactic category (noun, verb, adjective, etc.) for each word in the sentence. Named entity recognition (EP-NER) is task that tests a model’s abil5234 ity to predict the category of an entity in a given span. Semantic Role Labeling (EP-SRL) is a task that tests a model’s ability to assign a label to a given span of words that indicates its semantic role (agent, goal, etc.) in the sentence. Coreference (EP-Coref) is a task that tests a model’s ability to classify if two spans of tokens refer to the same ent"
2020.acl-main.467,D19-1243,0,0.0571475,"Missing"
2020.acl-main.467,P19-1356,0,0.0177692,"ep procedure. Similar work (Lin et al., 2019b) has been done on cross-lingual transfer—the analogous challenge of transferring learned knowledge from a highresource to a low-resource language. Many recent works have attempted to understand the knowledge and linguistic skills BERT learns, for instance by analyzing the language model surprisal for subject–verb agreements (Goldberg, 2018), identifying specific knowledge or phenomena encapsulated in the representations learned by BERT using probing tasks (Tenney et al., 2019b,a; Warstadt et al., 2019a; Lin et al., 2019a; Hewitt and Manning, 2019; Jawahar et al., 2019), analyzing the attention heads of BERT (Clark et al., 2019b; 5238 Coenen et al., 2019; Lin et al., 2019a; Htut et al., 2019), and testing the linguistic generalizations of BERT across runs (McCoy et al., 2019). However, relatively little work has been done to analyze fine-tuned BERT-style models (Wang et al., 2019a; Warstadt et al., 2019a). 6 Conclusion and Future Work This paper presents a large-scale study on when and why intermediate-task training works with pretrained models. We perform experiments on RoBERTa with a total of 110 pairs of intermediate and target tasks, and perform an analy"
2020.acl-main.467,N18-1023,0,0.0451025,"of texts, a pronoun from each text, and a list of possible noun phrases from each text. The dataset has been designed such that world knowledge is required to determine which of the possible noun phrases is the correct referent to the pronoun. We use the SuperGLUE binary classification cast of the task, where each example consists of a text, a pronoun, and a noun phrase from the text, which models must classify as being coreferent to the pronoun or not. Recognizing Textual Entailment (RTE; Dagan et al., 2005, et seq) is a textual entailment task. Multi-Sentence Reading Comprehension (MultiRC; Khashabi et al., 2018) is a multi-hop QA task that consists of paragraphs, a question on each paragraph, and a list of possible answers, in which models must distinguish which of the possible answers are true and which are false. Word-in-Context (WiC; Pilehvar and Camacho-Collados, 2019) is a binary classification word sense disambiguation task. Examples consist of two text snippets, with a polysemous word that appears in both. Models must determine whether the same sense of the word is used in both contexts. BoolQ (Clark et al., 2019a) is a QA task that consists of passages and a yes/no question associated with ea"
2020.acl-main.467,S19-1026,1,0.807373,"Missing"
2020.acl-main.467,W19-4825,0,0.0643425,"Missing"
2020.acl-main.467,P19-1301,0,0.0622512,"Missing"
2020.acl-main.467,N19-1128,0,0.0351443,"Missing"
2020.acl-main.467,P19-1441,0,0.385592,"robing tasks. STILTs. However, this approach does not always improve target task performance, and it is unclear under what conditions it does. This paper offers a large-scale empirical study aimed at addressing this open question. We perform a broad survey of intermediate and target task pairs, following an experimental pipeline similar to Phang et al. (2018) and Wang et al. (2019a). This differs from previous work in that we use a larger and more diverse set of intermediate and target tasks, introduce additional analysis-oriented probing tasks, and use a better-performing base model RoBERTa (Liu et al., 2019b). We aim to answer the following specific questions: • What kind of tasks tend to make good intermediate tasks across a wide variety of target tasks? Introduction Unsupervised pretraining—e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b)—has recently pushed the state of the art on many natural language understanding tasks. One method of further improving pretrained models that has been shown to be broadly helpful is to first finetune a pretrained model on an intermediate task, before fine-tuning again on the target task of interest (Phang et al., 2018; Wang et al., 2019a; Clark"
2020.acl-main.467,N18-2089,0,0.0254086,"s like the SentEval probing suite (Conneau et al., 2018) target specific syntactic and metadatalevel phenomena such as subject-verb agreement and sentence length detection. 2.2 Tasks Table 1 presents an overview of the intermediate and target tasks. 2.2.1 Intermediate Tasks We curate a diverse set of tasks that either represent an especially large annotation effort or that have been shown to yield positive transfer in prior work. The resulting set of tasks cover question answering, commonsense reasoning, and natural language inference. QAMR The Question–Answer Meaning Representations dataset (Michael et al., 2018) is a crowdsourced QA task consisting of question–answer pairs that correspond to predicate–argument relationships. It is derived from Wikinews and Wikipedia sentences. For example, if the sentence is “Ada Lovelace was a computer scientist.”, a potential question is “What is Ada’s last name?”, with the answer being “Lovelace.” CommonsenseQA CommonsenseQA (Talmor et al., 2019) is a multiple-choice QA task derived from ConceptNet (Speer et al., 2017) with the help of crowdworkers, that is designed to test a range of commonsense knowledge. Intermediate Task Training We fine-tune RoBERTa on each i"
2020.acl-main.467,D12-1071,0,0.0515117,"ict the fine-grained non-exclusive semantic attributes of a given span. Edge probing uses two datasets for SPR: SPR1 (EP-SPR1) (Teichert et al., 2017), derived from the Penn Treebank, and SPR2 (EP-SPR2) (Rudinger et al., 2018), derived from the English Web Treebank. Relation classification (EP-Rel) is a task that tests a model’s ability to predict the relation between two entities. We use the SemEval 2010 Task 8 dataset (Hendrickx et al., 2009) for this task. For example, the relation between “Yeri” and “Korea” in “Yeri is from Korea” is ENTITY-ORIGIN. The Definite Pronoun Resolution dataset (Rahman and Ng, 2012) (EPDPR) is a task that tests a model’s ability to handle coreference, and differs from OntoNotes in that it focuses on difficult cases of definite pronouns. SentEval Tasks The SentEval probing tasks (SE) (Conneau et al., 2018) are cast in the form of single-sentence classification. Sentence Length (SE-SentLen) is a task that tests a model’s ability to classify the length of a sentence. Word Content (SE-WC) is a task that tests a model’s ability to identify which of a set of 1,000 potential words appear in a given sentence. Tree Depth (SETreeDepth) is a task that tests a model’s ability to est"
2020.acl-main.467,D18-1114,0,0.0844869,"Missing"
2020.acl-main.467,D19-1454,0,0.053626,"Missing"
2020.acl-main.467,silveira-etal-2014-gold,1,0.895585,"Missing"
2020.acl-main.467,D13-1170,0,0.00698795,"he most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling dataset (QA-SRL; He et al., 2015) for a QA task that is derived from a semantic role labeling task. Each example, which consists of a set of questions and answers, corresponds to a predicate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL is specifically focused on verbs. SST-2 The Stanford sentiment treebank (Socher et al., 2013) is a sentiment classification task based on movie reviews. We use the binary sentence classification version of the task. QQP The Quora Question Pairs dataset1 is constructed based on questions posted on the community question-answering website Quora. The task is to determine if two questions are semantically equivalent. MNLI The Multi-Genre Natural Language Inference dataset (Williams et al., 2018) is a crowdsourced collection of sentence pairs with textual entailment annotations across a variety of genres. 2.2.2 Target Tasks We use ten target tasks, eight of which are drawn from the SuperGL"
2020.acl-main.467,P19-1485,0,0.0248439,"ate strongly with each other and have similar patterns of probing-task performance. 5 Related Work Within the paradigm of training large pretrained Transformer language representations via intermediate-stage training before fine-tuning on a target task, positive transfer has been shown in both sequential task-to-task (Phang et al., 2018) and multi-task-to-task (Liu et al., 2019a; Raffel et al., 2019) formats. Wang et al. (2019a) perform an extensive study on transfer with BERT, finding language modeling and NLI tasks to be among the most beneficial tasks for improving target-task performance. Talmor and Berant (2019) perform a similar cross-task transfer study on reading comprehension datasets, finding similar positive transfer in most cases, with the biggest gains stemming from a combination of multiple QA datasets. Our work consists of a larger, more diverse, set of intermediate task–target task pairs. We also use probing tasks to shed light on the skills learned by the intermediate tasks. Among the prior work on predicting transfer performance, Bingel and Søgaard (2017) is the most similar to ours. They do a regression analysis that predicts target-task performance on the basis of various features of t"
2020.acl-main.467,N19-1421,0,0.0687043,"Missing"
2020.acl-main.467,D17-3004,0,0.107746,"Missing"
2020.acl-main.467,P19-1452,0,0.174188,"e-choice QA task that consists of news articles. For each article, models are given a question about each article with one entity masked out and a list of possible entities from the article, and the goal is to correctly identify the masked entity out of the list. Additionally, we use CommonsenseQA and Cosmos QA as target tasks, due to their unique combination of small dataset size and high level of difficulty for high-performing models like BERT from our set of intermediate tasks. 2.2.3 Probing Tasks We use well-established datasets for our probing tasks, including the edge-probing suite from Tenney et al. (2019b), function word oriented tasks from Kim et al. (2019), and sentence-level probing datasets (SentEval; Conneau et al., 2018). Acceptability Judgment Tasks This set of binary classifications tasks was designed to investigate if a model can judge the grammatical acceptability of a sentence. We use the following five datasets: AJ-CoLA is a task that tests for a model’s understanding of general grammaticality using the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019b), which is drawn from 22 theoretical linguistics publications. The other tasks concern the behaviors of specific c"
2020.acl-main.467,P19-1439,1,0.81674,"Missing"
2020.acl-main.467,D19-1286,1,0.93361,"ks. 2.2.3 Probing Tasks We use well-established datasets for our probing tasks, including the edge-probing suite from Tenney et al. (2019b), function word oriented tasks from Kim et al. (2019), and sentence-level probing datasets (SentEval; Conneau et al., 2018). Acceptability Judgment Tasks This set of binary classifications tasks was designed to investigate if a model can judge the grammatical acceptability of a sentence. We use the following five datasets: AJ-CoLA is a task that tests for a model’s understanding of general grammaticality using the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019b), which is drawn from 22 theoretical linguistics publications. The other tasks concern the behaviors of specific classes of function words, using the dataset by Kim et al. (2019): AJ-WH is a task that tests a model’s ability to detect if a wh-word in a sentence has been swapped with another wh-word, which tests a model’s ability to identify the antecedent associated with the wh-word. AJ-Def is a task that tests a model’s ability to detect if the definite/indefinite articles in a given sentence have been swapped. AJCoord is a task that tests a model’s ability to detect if a coordinating conju"
2020.acl-main.467,Q19-1040,1,0.936652,"ks. 2.2.3 Probing Tasks We use well-established datasets for our probing tasks, including the edge-probing suite from Tenney et al. (2019b), function word oriented tasks from Kim et al. (2019), and sentence-level probing datasets (SentEval; Conneau et al., 2018). Acceptability Judgment Tasks This set of binary classifications tasks was designed to investigate if a model can judge the grammatical acceptability of a sentence. We use the following five datasets: AJ-CoLA is a task that tests for a model’s understanding of general grammaticality using the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019b), which is drawn from 22 theoretical linguistics publications. The other tasks concern the behaviors of specific classes of function words, using the dataset by Kim et al. (2019): AJ-WH is a task that tests a model’s ability to detect if a wh-word in a sentence has been swapped with another wh-word, which tests a model’s ability to identify the antecedent associated with the wh-word. AJ-Def is a task that tests a model’s ability to detect if the definite/indefinite articles in a given sentence have been swapped. AJCoord is a task that tests a model’s ability to detect if a coordinating conju"
2020.acl-main.467,W17-4413,0,0.0426709,"mmonsense knowledge. Intermediate Task Training We fine-tune RoBERTa on each intermediate task. The training procedure follows the standard procedure of fine-tuning a pretrained model on a target task, as described in Devlin et al. (2019). We opt for single intermediate-task training as opposed to multi-task training (cf. Liu et al., 2019a) to isolate the effect of skills learned from individual intermediate tasks. SciTail SciTail (Khot et al., 2018) is a textual entailment task built from multiple-choice science questions from 4th grade and 8th grade exams, as well as crowdsourced questions (Welbl et al., 2017). The task is to determine whether a hypothesis, which is constructed from a science question and its corresponding answer, is entailed or not (neutral) by the premise. Target and Probing Task Fine-Tuning After intermediate-task training, we fine-tune our models Cosmos QA Cosmos QA is a task for a commonsense-based reading comprehension task 5232 Target Tasks Intermediate Tasks Name CommonsenseQA SciTail Cosmos QA SocialIQA CCG HellaSwag QA-SRL SST-2 QAMR QQP MNLI |Train ||Dev |task 9,741 23,596 25,588 33,410 38,015 39,905 44,837 67,349 73,561 363,846 392,702 metrics genre/source 1,221 1,304 3"
2020.acl-main.467,N18-1101,1,0.826025,"ate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL is specifically focused on verbs. SST-2 The Stanford sentiment treebank (Socher et al., 2013) is a sentiment classification task based on movie reviews. We use the binary sentence classification version of the task. QQP The Quora Question Pairs dataset1 is constructed based on questions posted on the community question-answering website Quora. The task is to determine if two questions are semantically equivalent. MNLI The Multi-Genre Natural Language Inference dataset (Williams et al., 2018) is a crowdsourced collection of sentence pairs with textual entailment annotations across a variety of genres. 2.2.2 Target Tasks We use ten target tasks, eight of which are drawn from the SuperGLUE benchmark (Wang et al., 2019b). The tasks in the SuperGLUE benchmark 1 http://data.quora.com/First-Quora-DatasetReleaseQuestion-Pairs 5233 cover question answering, entailment, word sense disambiguation, and coreference resolution and have been shown to be easy for humans but difficult for models like BERT. Although we offer a brief description of the tasks below, we refer readers to the SuperGLUE"
2020.acl-main.467,D18-1009,0,0.0398946,"tests for reasoning surrounding emotional and social intelligence in everyday situations. CCG CCGbank (Hockenmaier and Steedman, 2007) is a task that is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. We use the CCG supertagging task, which is the task of assigning tags to individual word tokens that jointly determine the parse of the sentence. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning task that tests a model’s ability to choose the most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling dataset (QA-SRL; He et al., 2015) for a QA task that is derived from a semantic role labeling task. Each example, which consists of a set of questions and answers, corresponds to a predicate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL is specifically focused on verbs. SST-2 The Stanford sentiment treebank (Socher et al., 2013) is a sentiment classification task based on movie reviews. We use the binary sentenc"
2020.acl-main.467,P19-1472,0,0.0255593,"eading comprehension over an auxiliary passage, rather than simply answering a freestanding question. SocialIQA SocialIQA (Sap et al., 2019) is a task for multiple choice QA. It tests for reasoning surrounding emotional and social intelligence in everyday situations. CCG CCGbank (Hockenmaier and Steedman, 2007) is a task that is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. We use the CCG supertagging task, which is the task of assigning tags to individual word tokens that jointly determine the parse of the sentence. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning task that tests a model’s ability to choose the most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling dataset (QA-SRL; He et al., 2015) for a QA task that is derived from a semantic role labeling task. Each example, which consists of a set of questions and answers, corresponds to a predicate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL"
2020.acl-main.467,W18-5446,1,\N,Missing
2020.acl-main.467,E17-2026,0,\N,Missing
2020.acl-main.598,P17-1183,0,0.0240582,"l. (2014) combine information about paradigms and word frequency from corpora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and Sch¨utze, 2016; Aharoni and Goldberg, 2017; Bergmanis et al., 2017; Makarov et al., 2017; Zhou and Neubig, 2017; Kann and Sch¨utze, 2018). We integrate two systems (Cotterell et al., 2017; Makarov and Clematide, 2018b) produced for SIGMORPHON shared tasks into our framework for unsupervised morphological paradigm completion. Morphological Analysis Most research on unsupervised systems for morphology aims at developing approaches to segment words into their smallest meaning-bearing units, called morphemes (Goldsmith, 2001; Creutz, 2003; Creutz and Lagus, 2007; Snyder and Barzilay, 2008). Unsupervised morphological paradigm completion d"
2020.acl-main.598,N15-1107,0,0.119412,"es, even obtains higher accuracy than a minimally supervised system. 2 Related Work Morphological Generation Versions of our task with varying degrees of supervision—though never totally unsupervised—have been explored in the past. Yarowsky and Wicentowski (2000) is the previous work most similar to ours. They also assume raw text and a word list as input, but additionally require knowledge of a language’s consonants and vowels, as well as canonical suffixes for each part of speech. Dreyer and Eisner (2011) assume access to seed paradigms to discover paradigms in an empirical Bayes framework. Ahlberg et al. (2015) and Hulden et al. (2014) combine information about paradigms and word frequency from corpora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and"
2020.acl-main.598,D19-1091,0,0.0998832,"Missing"
2020.acl-main.598,K17-2002,1,0.927203,"Missing"
2020.acl-main.598,K18-3001,1,0.715869,"Missing"
2020.acl-main.598,K17-2001,0,0.250519,"Missing"
2020.acl-main.598,P03-1036,0,0.908221,"tion, Guzm´an et al. (2019) encounter difficulties when translating into the morphologically rich languages Nepalese and Sinhalese. Children acquire morphological knowledge from raw utterances and, in particular, without access to explicit morphological information (Berko, 1958). Do they have an innate capacity that enables them to learn a language’s morphology? Or can morphology be learned in an unsupervised fashion? This question—in addition to practical considerations like benefits for the aforementioned NLP tasks— has motivated work on unsupervised morphological analyses (Goldsmith, 2001; Creutz, 2003). To the best of our knowledge, no previous work has considered unsupervised morphological generation.2 However, over the last few years, there has 2 Kann et al. (2017), which performs a zero-shot inflection experiment, uses prior information about paradigm size and related languages and, thus, cannot draw the same conclusions. 6696 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6696–6707 c July 5 - 10, 2020. 2020 Association for Computational Linguistics been a lot of progress on morphological generation tasks with limited amounts of supervision"
2020.acl-main.598,N07-1048,0,0.0810653,"Missing"
2020.acl-main.598,D11-1057,0,0.805391,"f all steps in our pipeline. We further show that our system outperforms trivial baselines and, for some languages, even obtains higher accuracy than a minimally supervised system. 2 Related Work Morphological Generation Versions of our task with varying degrees of supervision—though never totally unsupervised—have been explored in the past. Yarowsky and Wicentowski (2000) is the previous work most similar to ours. They also assume raw text and a word list as input, but additionally require knowledge of a language’s consonants and vowels, as well as canonical suffixes for each part of speech. Dreyer and Eisner (2011) assume access to seed paradigms to discover paradigms in an empirical Bayes framework. Ahlberg et al. (2015) and Hulden et al. (2014) combine information about paradigms and word frequency from corpora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological in"
2020.acl-main.598,N13-1138,0,0.29226,"centowski (2000) is the previous work most similar to ours. They also assume raw text and a word list as input, but additionally require knowledge of a language’s consonants and vowels, as well as canonical suffixes for each part of speech. Dreyer and Eisner (2011) assume access to seed paradigms to discover paradigms in an empirical Bayes framework. Ahlberg et al. (2015) and Hulden et al. (2014) combine information about paradigms and word frequency from corpora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and Sch¨utze, 2016; Aharoni and Goldberg, 2017; Bergmanis et al., 2017; Makarov et al., 2017; Zhou and Neubig, 2017; Kann and Sch¨utze, 2018). We integrate two systems (Cotterell et al., 2017; Makarov and Clematide, 2018b) produced for SIGMORPHON shared t"
2020.acl-main.598,2020.acl-main.695,0,0.247281,"nsupervised approaches to problems in morphology, we refer the reader to Hammarstr¨om and Borin (2011). SIGMORPHON 2020: Unsupervised Morphological Paradigm Completion After multiple shared tasks on morphological inflection starting with Cotterell et al. (2016), in 2020, SIGMORPHON (the ACL special interest group on computational morphology and phonology) is organizing 6697 its first shared task on unsupervised morphological paradigm completion.3 The system presented here is the official shared task baseline system. The first other approach applicable to this shared task has been developed by Erdmann et al. (2020). Their pipeline system is similar in spirit to ours, but the individual components are different, e.g., a transformer model (Vaswani et al., 2017) is used for inflection generation. 3 Formal Task Description Given a corpus D = w1 , . . . , w|D |with a vocabulary V of word types {wi } and a lexicon L = {`j } with |L |lemmas belonging to the same part of speech, the task of unsupervised morphological paradigm completion consists of generating the paradigms {π(`)}`∈L of the entries in the lexicon. Following Matthews and Matthews (1972) and Aronoff (1976), we treat a paradigm as a vector of infle"
2020.acl-main.598,N16-1077,0,0.047045,"ours. They also assume raw text and a word list as input, but additionally require knowledge of a language’s consonants and vowels, as well as canonical suffixes for each part of speech. Dreyer and Eisner (2011) assume access to seed paradigms to discover paradigms in an empirical Bayes framework. Ahlberg et al. (2015) and Hulden et al. (2014) combine information about paradigms and word frequency from corpora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and Sch¨utze, 2016; Aharoni and Goldberg, 2017; Bergmanis et al., 2017; Makarov et al., 2017; Zhou and Neubig, 2017; Kann and Sch¨utze, 2018). We integrate two systems (Cotterell et al., 2017; Makarov and Clematide, 2018b) produced for SIGMORPHON shared tasks into our framework for unsupervised morpholog"
2020.acl-main.598,J01-2001,0,0.776676,"d machine translation, Guzm´an et al. (2019) encounter difficulties when translating into the morphologically rich languages Nepalese and Sinhalese. Children acquire morphological knowledge from raw utterances and, in particular, without access to explicit morphological information (Berko, 1958). Do they have an innate capacity that enables them to learn a language’s morphology? Or can morphology be learned in an unsupervised fashion? This question—in addition to practical considerations like benefits for the aforementioned NLP tasks— has motivated work on unsupervised morphological analyses (Goldsmith, 2001; Creutz, 2003). To the best of our knowledge, no previous work has considered unsupervised morphological generation.2 However, over the last few years, there has 2 Kann et al. (2017), which performs a zero-shot inflection experiment, uses prior information about paradigm size and related languages and, thus, cannot draw the same conclusions. 6696 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6696–6707 c July 5 - 10, 2020. 2020 Association for Computational Linguistics been a lot of progress on morphological generation tasks with limited amounts"
2020.acl-main.598,D19-1632,0,0.0530468,"Missing"
2020.acl-main.598,J11-2002,0,0.0735095,"Missing"
2020.acl-main.598,E14-1060,0,0.0395851,"curacy than a minimally supervised system. 2 Related Work Morphological Generation Versions of our task with varying degrees of supervision—though never totally unsupervised—have been explored in the past. Yarowsky and Wicentowski (2000) is the previous work most similar to ours. They also assume raw text and a word list as input, but additionally require knowledge of a language’s consonants and vowels, as well as canonical suffixes for each part of speech. Dreyer and Eisner (2011) assume access to seed paradigms to discover paradigms in an empirical Bayes framework. Ahlberg et al. (2015) and Hulden et al. (2014) combine information about paradigms and word frequency from corpora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and Sch¨utze, 2016; Aharoni a"
2020.acl-main.598,W17-4110,1,0.893542,"Missing"
2020.acl-main.598,P17-1182,1,0.936915,"Missing"
2020.acl-main.598,P16-2090,1,0.933515,"Missing"
2020.acl-main.598,D18-1363,1,0.918557,"Missing"
2020.acl-main.598,D18-1314,0,0.337762,"sume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and Sch¨utze, 2016; Aharoni and Goldberg, 2017; Bergmanis et al., 2017; Makarov et al., 2017; Zhou and Neubig, 2017; Kann and Sch¨utze, 2018). We integrate two systems (Cotterell et al., 2017; Makarov and Clematide, 2018b) produced for SIGMORPHON shared tasks into our framework for unsupervised morphological paradigm completion. Morphological Analysis Most research on unsupervised systems for morphology aims at developing approaches to segment words into their smallest meaning-bearing units, called morphemes (Goldsmith, 2001; Creutz, 2003; Creutz and Lagus, 2007; Snyder and Barzilay, 2008). Unsupervised morphological paradigm completion differs from segmentation in that, besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots as"
2020.acl-main.598,C18-1008,0,0.132959,"sume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and Sch¨utze, 2016; Aharoni and Goldberg, 2017; Bergmanis et al., 2017; Makarov et al., 2017; Zhou and Neubig, 2017; Kann and Sch¨utze, 2018). We integrate two systems (Cotterell et al., 2017; Makarov and Clematide, 2018b) produced for SIGMORPHON shared tasks into our framework for unsupervised morphological paradigm completion. Morphological Analysis Most research on unsupervised systems for morphology aims at developing approaches to segment words into their smallest meaning-bearing units, called morphemes (Goldsmith, 2001; Creutz, 2003; Creutz and Lagus, 2007; Snyder and Barzilay, 2008). Unsupervised morphological paradigm completion differs from segmentation in that, besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots as"
2020.acl-main.598,W19-4226,1,0.670368,"Missing"
2020.acl-main.598,2020.lrec-1.352,1,0.352951,"Unlike widely used soft-attention sequence-to-sequence models (Bahdanau et al., 2015), which predict the target tokens directly, it predicts edit action sequences to transform the input sequence into outputs, and it disposes of a hard attention mechanism. We denote the surface form change grouping in combination with this system as PCS-III-H. 5 5.1 Experiments Data To evaluate our approach in a real-world setting, we restrict our data to resources typically available to a field linguist: a small written corpus (≤ 100k tokens) and a small lexicon. For our corpora, we use the JHU Bible Corpus (McCarthy et al., 2020), which allows future work to build systems in 1600 languages. The Bible is frequently available even in low-resource languages: Ethnologue identifies 3,995 written languages, and the New Testament has been translated into 2,246. The Bible is also highly representative of a language’s core vocabulary: Resnik et al. (1999) find high overlap with both the Longman Dictionary of Contemporary English (Summers and Gadsby, 1995) and the Brown Corpus (Francis and Kucera, 1964). Furthermore, the Bible is multiparallel and, thus, allows for a fair comparison across languages without confounds like domai"
2020.acl-main.598,Q15-1012,0,0.13639,"om segmentation in that, besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots as well as generating unobserved forms. The model by Xu et al. (2018) recovers something akin to morphological paradigms. However, those paradigms are a means to a segmentation end, and Xu et al. (2018) do not explicitly model information about the paradigm size as required for our task. Other unsupervised approaches to learning morphological analysis and generation rely on projections between word embeddings (Soricut and Och, 2015; Narasimhan et al., 2015); however, these approaches rely on billions of words to train embeddings; at a minimum, Narasimhan et al. (2015) use 129 million word tokens of English Wikipedia. As we will describe later on (§5.1), we, in contrast, are concerned with the setting with mere thousands of sentences. For a detailed survey of unsupervised approaches to problems in morphology, we refer the reader to Hammarstr¨om and Borin (2011). SIGMORPHON 2020: Unsupervised Morphological Paradigm Completion After multiple shared tasks on morphological inflection starting with Cotterell et al. (2016), in 2020, SIGMORPHON (the ACL"
2020.acl-main.598,D14-1095,0,0.0226475,"2012), and Archi paradigms, an extreme example, can have over 1.5 million slots (Kibrik, 1977). Morphologically rich languages constitute a challenge for natural language processing (NLP) systems: because each lemma can take on a variety of surface forms, the frequency of each individual inflected word decreases drastically. This yields problems for speech recognition (Creutz 1 (2) NEW LEMMAS morir - mueres estar tener empezar pasar ... Our implementation is available under https://github. com/cai-lw/morpho-baseline. et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), and keyword spotting (Narasimhan et al., 2014), inter alia. For unsupervised machine translation, Guzm´an et al. (2019) encounter difficulties when translating into the morphologically rich languages Nepalese and Sinhalese. Children acquire morphological knowledge from raw utterances and, in particular, without access to explicit morphological information (Berko, 1958). Do they have an innate capacity that enables them to learn a language’s morphology? Or can morphology be learned in an unsupervised fashion? This question—in addition to practical considerations like benefits for the aforementioned NLP tasks— has motivated work on unsuperv"
2020.acl-main.598,N15-1093,0,0.0297106,"vious work most similar to ours. They also assume raw text and a word list as input, but additionally require knowledge of a language’s consonants and vowels, as well as canonical suffixes for each part of speech. Dreyer and Eisner (2011) assume access to seed paradigms to discover paradigms in an empirical Bayes framework. Ahlberg et al. (2015) and Hulden et al. (2014) combine information about paradigms and word frequency from corpora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and Sch¨utze, 2016; Aharoni and Goldberg, 2017; Bergmanis et al., 2017; Makarov et al., 2017; Zhou and Neubig, 2017; Kann and Sch¨utze, 2018). We integrate two systems (Cotterell et al., 2017; Makarov and Clematide, 2018b) produced for SIGMORPHON shared tasks into our framework"
2020.acl-main.598,C18-1005,0,0.071939,"amework for unsupervised morphological paradigm completion. Morphological Analysis Most research on unsupervised systems for morphology aims at developing approaches to segment words into their smallest meaning-bearing units, called morphemes (Goldsmith, 2001; Creutz, 2003; Creutz and Lagus, 2007; Snyder and Barzilay, 2008). Unsupervised morphological paradigm completion differs from segmentation in that, besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots as well as generating unobserved forms. The model by Xu et al. (2018) recovers something akin to morphological paradigms. However, those paradigms are a means to a segmentation end, and Xu et al. (2018) do not explicitly model information about the paradigm size as required for our task. Other unsupervised approaches to learning morphological analysis and generation rely on projections between word embeddings (Soricut and Och, 2015; Narasimhan et al., 2015); however, these approaches rely on billions of words to train embeddings; at a minimum, Narasimhan et al. (2015) use 129 million word tokens of English Wikipedia. As we will describe later on (§5.1), we, in"
2020.acl-main.598,P00-1027,0,0.588032,"or unsupervised paradigm completion, best-match accuracy (§5.4), and experiment on 14 languages from 7 families. As we are tackling a novel task with no baselines in the NLP literature, we perform an extensive ablation study to demonstrate the importance of all steps in our pipeline. We further show that our system outperforms trivial baselines and, for some languages, even obtains higher accuracy than a minimally supervised system. 2 Related Work Morphological Generation Versions of our task with varying degrees of supervision—though never totally unsupervised—have been explored in the past. Yarowsky and Wicentowski (2000) is the previous work most similar to ours. They also assume raw text and a word list as input, but additionally require knowledge of a language’s consonants and vowels, as well as canonical suffixes for each part of speech. Dreyer and Eisner (2011) assume access to seed paradigms to discover paradigms in an empirical Bayes framework. Ahlberg et al. (2015) and Hulden et al. (2014) combine information about paradigms and word frequency from corpora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNe"
2020.acl-main.598,P17-1029,0,0.0130746,"ora to perform semi-supervised paradigm completion. Our work differs from them in that we do not assume any gold paradigms to be given. Durrett and DeNero (2013), Nicolai et al. (2015), and Faruqui et al. (2016) explore a fully supervised approach, learning morphological paradigms from large annotated inflection tables. This framework has evolved into the SIGMORPHON shared tasks on morphological inflection (Cotterell et al., 2016), which have sparked further interest in morphological generation (Kann and Sch¨utze, 2016; Aharoni and Goldberg, 2017; Bergmanis et al., 2017; Makarov et al., 2017; Zhou and Neubig, 2017; Kann and Sch¨utze, 2018). We integrate two systems (Cotterell et al., 2017; Makarov and Clematide, 2018b) produced for SIGMORPHON shared tasks into our framework for unsupervised morphological paradigm completion. Morphological Analysis Most research on unsupervised systems for morphology aims at developing approaches to segment words into their smallest meaning-bearing units, called morphemes (Goldsmith, 2001; Creutz, 2003; Creutz and Lagus, 2007; Snyder and Barzilay, 2008). Unsupervised morphological paradigm completion differs from segmentation in that, besides capturing how morphology is"
2020.acl-main.598,Q15-1026,0,0.140892,"Missing"
2020.acl-main.598,P08-1084,0,0.113269,"orphological generation (Kann and Sch¨utze, 2016; Aharoni and Goldberg, 2017; Bergmanis et al., 2017; Makarov et al., 2017; Zhou and Neubig, 2017; Kann and Sch¨utze, 2018). We integrate two systems (Cotterell et al., 2017; Makarov and Clematide, 2018b) produced for SIGMORPHON shared tasks into our framework for unsupervised morphological paradigm completion. Morphological Analysis Most research on unsupervised systems for morphology aims at developing approaches to segment words into their smallest meaning-bearing units, called morphemes (Goldsmith, 2001; Creutz, 2003; Creutz and Lagus, 2007; Snyder and Barzilay, 2008). Unsupervised morphological paradigm completion differs from segmentation in that, besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots as well as generating unobserved forms. The model by Xu et al. (2018) recovers something akin to morphological paradigms. However, those paradigms are a means to a segmentation end, and Xu et al. (2018) do not explicitly model information about the paradigm size as required for our task. Other unsupervised approaches to learning morphological analysis and generation rely on p"
2020.acl-main.598,N15-1186,0,0.195419,"m completion differs from segmentation in that, besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots as well as generating unobserved forms. The model by Xu et al. (2018) recovers something akin to morphological paradigms. However, those paradigms are a means to a segmentation end, and Xu et al. (2018) do not explicitly model information about the paradigm size as required for our task. Other unsupervised approaches to learning morphological analysis and generation rely on projections between word embeddings (Soricut and Och, 2015; Narasimhan et al., 2015); however, these approaches rely on billions of words to train embeddings; at a minimum, Narasimhan et al. (2015) use 129 million word tokens of English Wikipedia. As we will describe later on (§5.1), we, in contrast, are concerned with the setting with mere thousands of sentences. For a detailed survey of unsupervised approaches to problems in morphology, we refer the reader to Hammarstr¨om and Borin (2011). SIGMORPHON 2020: Unsupervised Morphological Paradigm Completion After multiple shared tasks on morphological inflection starting with Cotterell et al. (2016), in"
2020.acl-main.598,Q16-1018,0,0.0186997,"h lemma, at most one inflected form per paradigm slot can be found in the corpus. Formally, for a multi-to-one mapping from EDIT TREES to paradigm slots z : Ψ → Γ, we define the EDIT TREE set Ψγ of a potential paradigm slot γ as S Ψγ = {ψ |z(ψ) = γ}, with γ Ψγ = Ψ. Then, for any lemma ` ∈ L and proposed paradigm slot γ ∈ Γ, we have: !r[ 1 , , ] Figure 4: An example of the distributionally informed feature function with window size 3 for the past tense slot (PST). stop ∈ L and fPST (stop) = stopped. When the sliding window arrives to this instance of stopped, PST ~r[N,V,V] is increased by 1.8 (Stratos et al., 2016) to extract tags for each word in the corpus D. This tagger assigns an anchor word, i.e., a pseudo POS tag, to each word wi in the corpus by using an anchor hidden Markov model (HMM) with 8 hidden states. Our feature function counts the tag tuples within a sliding window centered on each instance of inflected forms of a potential paradigm slot. Formally, we denote the set of lemmas that surface form change ψ is applied to as Lψ , the set of lemmas that express a potential paradigm slot γ as Lγ , and the corresponding inflected forms as Vγ : Lψ = {` ∈ L|ψ(`) = w ∧ w ∈ V } [ Lγ = Lψ (10) (11) ψ∈"
2020.acl-main.598,W16-2002,0,\N,Missing
2020.emnlp-main.423,W19-0302,0,0.036601,"Missing"
2020.emnlp-main.423,C14-1111,0,0.033726,"Missing"
2020.emnlp-main.423,E09-2008,0,0.0673325,"Missing"
2020.emnlp-main.423,D16-1097,1,0.897832,"Missing"
2020.emnlp-main.423,C18-1006,1,0.84345,"ring, e.g., the English word collision, its surface segmentation is collis+ion, while its canonical segmentation is collide+ion. Figure 1 provides examples for all five languages we experiment on. Neural models have shown to perform well on this task when large amounts of training data are available (Kann et al., 2016; Ruzsics and Samardzic, 2017). Nevertheless, datasets with morphological annotations are difficult to obtain, since they require expert annotators. Furthermore, many languages with complex morphology are spoken by a limited number of people or are listed as endangered languages (Mager et al., 2018), which reduces the possible annotator pool even more. However, morphological segmentation is important for downstream tasks like machine translation (Conforti et al., 2018; Vania and Lopez, 2017), dependency parsing (Seeker and C¸etino˘glu, 2015; Vania et al., 2018), or semantic role labeling (Sahin and Steedman, 2018). Moreover, high performance on these tasks can yield more language independent NLP models (Gerz et al., 2018). Here, we focus on low-resource canonical segmentation. We propose two new models for the task, which have recently been successfully applied to a related morphological"
2020.emnlp-main.423,D18-1314,0,0.385794,"dependent NLP models (Gerz et al., 2018). Here, we focus on low-resource canonical segmentation. We propose two new models for the task, which have recently been successfully applied to a related morphological generation task called morphological inflection. The approaches we in5237 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5237–5250, c November 16–20, 2020. 2020 Association for Computational Linguistics vestigate are (i) an LSTM pointer-generator model (Sharma et al., 2018a), and (ii) a neural transducer trained with imitation learning (IL; Makarov and Clematide, 2018a). Since both canonical segmentation and morphological inflection are characterlevel string transduction tasks, we hypothesize that models which can learn one from limited data, will also be able to do so for the other. We experiment on three benchmark datasets in German, English, and Indonesian, but simulate a low-resource scenario by reducing the number of training examples. We further evaluate our models on datasets for two truly low-resource languages: Popoluca and Tepehua. We find that our new models indeed outperform previous approaches on all languages. For additional insight, we also"
2020.emnlp-main.423,C18-1008,0,0.133637,"dependent NLP models (Gerz et al., 2018). Here, we focus on low-resource canonical segmentation. We propose two new models for the task, which have recently been successfully applied to a related morphological generation task called morphological inflection. The approaches we in5237 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5237–5250, c November 16–20, 2020. 2020 Association for Computational Linguistics vestigate are (i) an LSTM pointer-generator model (Sharma et al., 2018a), and (ii) a neural transducer trained with imitation learning (IL; Makarov and Clematide, 2018a). Since both canonical segmentation and morphological inflection are characterlevel string transduction tasks, we hypothesize that models which can learn one from limited data, will also be able to do so for the other. We experiment on three benchmark datasets in German, English, and Indonesian, but simulate a low-resource scenario by reducing the number of training examples. We further evaluate our models on datasets for two truly low-resource languages: Popoluca and Tepehua. We find that our new models indeed outperform previous approaches on all languages. For additional insight, we also"
2020.emnlp-main.423,K17-2004,0,0.0223018,"wledge. Our aim is to explore data-driven approaches for the low-resource setting in order to overcome this limitation. 2 In recent years, the area of morphological generation has experienced substantial progress, with a variety of methods that can be used for the canonical segmentation task. Kann et al. (2016) used a sequence-to-sequence model to inflect a word given a set of morphological tags. Sharma et al. (2018a) proposed a pointer-generator model, which was more suitable for the low-resource setting. Aharoni and Goldberg (2017) proposed a neural transducer with hard monotonic attention. Makarov et al. (2017) extended this approach and added a copy operation, and Makarov and Clematide (2018a) proposed imitation learning (Daum´e et al., 2009) for training it. Here, we explore the applicability of the models by Sharma et al. (2018a) and Makarov Related Work The task of morphological segmentation was introduced by Harris (1951). Most work has considered the surface segmentation task, for which unsupervised methods like LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002, 2007; Poon et al., 2009) played an important role. The latter was further extended to a semisupervised version (Koh"
2020.emnlp-main.423,N09-1024,0,0.0198196,"Aharoni and Goldberg (2017) proposed a neural transducer with hard monotonic attention. Makarov et al. (2017) extended this approach and added a copy operation, and Makarov and Clematide (2018a) proposed imitation learning (Daum´e et al., 2009) for training it. Here, we explore the applicability of the models by Sharma et al. (2018a) and Makarov Related Work The task of morphological segmentation was introduced by Harris (1951). Most work has considered the surface segmentation task, for which unsupervised methods like LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002, 2007; Poon et al., 2009) played an important role. The latter was further extended to a semisupervised version (Kohonen et al., 2010; Gr¨onroos et al., 2014). Over the last years, supervised methods have attracted more attention: Ruokolainen et al. (2013) cast the task as a sequence labeling problem using conditional random fields (CRFs; Lafferty et al., For fusional languages, surface segmentation is not very effective. Therefore, restoring morphemes to their canonical form was previously discussed in linguistics (Kay, 1977) as well as in the NLP literature. Previous approaches include unsupervised (Naradowsky and G"
2020.emnlp-main.423,W13-3504,0,0.0249483,"Daum´e et al., 2009) for training it. Here, we explore the applicability of the models by Sharma et al. (2018a) and Makarov Related Work The task of morphological segmentation was introduced by Harris (1951). Most work has considered the surface segmentation task, for which unsupervised methods like LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002, 2007; Poon et al., 2009) played an important role. The latter was further extended to a semisupervised version (Kohonen et al., 2010; Gr¨onroos et al., 2014). Over the last years, supervised methods have attracted more attention: Ruokolainen et al. (2013) cast the task as a sequence labeling problem using conditional random fields (CRFs; Lafferty et al., For fusional languages, surface segmentation is not very effective. Therefore, restoring morphemes to their canonical form was previously discussed in linguistics (Kay, 1977) as well as in the NLP literature. Previous approaches include unsupervised (Naradowsky and Goldwater, 2009), as well as joint models for segmentation and transduction (Cotterell et al., 2016b) and neural encoder-decoder models (Kann et al., 2016; Ruzsics and Samardzic, 2017). However, up to now, supervised models have onl"
2020.emnlp-main.423,K17-1020,0,0.0156436,"ges, morphemes are merged during word formation and, thereby, change their surface forms. Thus, in this paper, we tackle the task of canonical segmentation (Cotterell et al., 2016b), which consists of segmenting a word while restoring the original forms of its morphemes. Considering, e.g., the English word collision, its surface segmentation is collis+ion, while its canonical segmentation is collide+ion. Figure 1 provides examples for all five languages we experiment on. Neural models have shown to perform well on this task when large amounts of training data are available (Kann et al., 2016; Ruzsics and Samardzic, 2017). Nevertheless, datasets with morphological annotations are difficult to obtain, since they require expert annotators. Furthermore, many languages with complex morphology are spoken by a limited number of people or are listed as endangered languages (Mager et al., 2018), which reduces the possible annotator pool even more. However, morphological segmentation is important for downstream tasks like machine translation (Conforti et al., 2018; Vania and Lopez, 2017), dependency parsing (Seeker and C¸etino˘glu, 2015; Vania et al., 2018), or semantic role labeling (Sahin and Steedman, 2018). Moreove"
2020.emnlp-main.423,P18-1036,0,0.0227469,"2016; Ruzsics and Samardzic, 2017). Nevertheless, datasets with morphological annotations are difficult to obtain, since they require expert annotators. Furthermore, many languages with complex morphology are spoken by a limited number of people or are listed as endangered languages (Mager et al., 2018), which reduces the possible annotator pool even more. However, morphological segmentation is important for downstream tasks like machine translation (Conforti et al., 2018; Vania and Lopez, 2017), dependency parsing (Seeker and C¸etino˘glu, 2015; Vania et al., 2018), or semantic role labeling (Sahin and Steedman, 2018). Moreover, high performance on these tasks can yield more language independent NLP models (Gerz et al., 2018). Here, we focus on low-resource canonical segmentation. We propose two new models for the task, which have recently been successfully applied to a related morphological generation task called morphological inflection. The approaches we in5237 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5237–5250, c November 16–20, 2020. 2020 Association for Computational Linguistics vestigate are (i) an LSTM pointer-generator model (Sharma et al., 2018"
2020.emnlp-main.423,P17-1051,0,0.0396519,"Missing"
2020.emnlp-main.423,P17-1099,0,0.0934225,"Missing"
2020.emnlp-main.423,Q15-1026,1,0.890343,"Missing"
2020.emnlp-main.423,K18-3001,1,0.823809,"Missing"
2020.emnlp-main.423,D18-1278,0,0.054371,"Missing"
2020.emnlp-main.423,P17-1184,0,0.0287816,"on. Neural models have shown to perform well on this task when large amounts of training data are available (Kann et al., 2016; Ruzsics and Samardzic, 2017). Nevertheless, datasets with morphological annotations are difficult to obtain, since they require expert annotators. Furthermore, many languages with complex morphology are spoken by a limited number of people or are listed as endangered languages (Mager et al., 2018), which reduces the possible annotator pool even more. However, morphological segmentation is important for downstream tasks like machine translation (Conforti et al., 2018; Vania and Lopez, 2017), dependency parsing (Seeker and C¸etino˘glu, 2015; Vania et al., 2018), or semantic role labeling (Sahin and Steedman, 2018). Moreover, high performance on these tasks can yield more language independent NLP models (Gerz et al., 2018). Here, we focus on low-resource canonical segmentation. We propose two new models for the task, which have recently been successfully applied to a related morphological generation task called morphological inflection. The approaches we in5237 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5237–5250, c November 16–20"
2020.emnlp-main.424,P17-1183,0,0.0127464,"ncerned with generation (as opposed to analysis) has focused on morpholog5253 Text Segmented Glossed Translation Vecherom ya pobejala vecher-om ya pobeja-la evening-INS 1.SG.NOM run-PFV.PST.SG.FEM ‘In the evening I ran to the store.’ v v in magazin. magazin store.ACC Table 2: An example of typical interlinear glossed text (IGT) with a transliterated Russian sentence, including translation. This paper leverages the original text and gloss lines. ical inflection or reinflection. Approaches include Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016); Kann and Sch¨utze (2016); Aharoni and Goldberg (2017). Partially building on these, other research has developed models which are more suitable for low-resource languages and perform well with limited data (Kann et al., 2017b; Sharma et al., 2018; Makarov and Clematide, 2018; Wu and Cotterell, 2019; Kann et al., 2020a; Wu et al., 2020). These are the most relevant approaches for our work, since we expect IGT2P to aid documentation of low-resource languages. Accordingly, we use the systems by Wu and Cotterell (2019) and Wu et al. (2020) in our experiments. Work on paradigm completion – or the paradigm cell filling problem (PCFP; Ackerman et al.,"
2020.emnlp-main.424,N13-1138,0,0.0527418,"sks described in Section 2.1. Most recent work in the area of computational morphology which was concerned with generation (as opposed to analysis) has focused on morpholog5253 Text Segmented Glossed Translation Vecherom ya pobejala vecher-om ya pobeja-la evening-INS 1.SG.NOM run-PFV.PST.SG.FEM ‘In the evening I ran to the store.’ v v in magazin. magazin store.ACC Table 2: An example of typical interlinear glossed text (IGT) with a transliterated Russian sentence, including translation. This paper leverages the original text and gloss lines. ical inflection or reinflection. Approaches include Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016); Kann and Sch¨utze (2016); Aharoni and Goldberg (2017). Partially building on these, other research has developed models which are more suitable for low-resource languages and perform well with limited data (Kann et al., 2017b; Sharma et al., 2018; Makarov and Clematide, 2018; Wu and Cotterell, 2019; Kann et al., 2020a; Wu et al., 2020). These are the most relevant approaches for our work, since we expect IGT2P to aid documentation of low-resource languages. Accordingly, we use the systems by Wu and Cotterell (2019) and Wu et al. (2020) in our exp"
2020.emnlp-main.424,D19-1091,0,0.340374,"Missing"
2020.emnlp-main.424,N16-1077,0,0.0139444,"the area of computational morphology which was concerned with generation (as opposed to analysis) has focused on morpholog5253 Text Segmented Glossed Translation Vecherom ya pobejala vecher-om ya pobeja-la evening-INS 1.SG.NOM run-PFV.PST.SG.FEM ‘In the evening I ran to the store.’ v v in magazin. magazin store.ACC Table 2: An example of typical interlinear glossed text (IGT) with a transliterated Russian sentence, including translation. This paper leverages the original text and gloss lines. ical inflection or reinflection. Approaches include Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016); Kann and Sch¨utze (2016); Aharoni and Goldberg (2017). Partially building on these, other research has developed models which are more suitable for low-resource languages and perform well with limited data (Kann et al., 2017b; Sharma et al., 2018; Makarov and Clematide, 2018; Wu and Cotterell, 2019; Kann et al., 2020a; Wu et al., 2020). These are the most relevant approaches for our work, since we expect IGT2P to aid documentation of low-resource languages. Accordingly, we use the systems by Wu and Cotterell (2019) and Wu et al. (2020) in our experiments. Work on paradigm completion – or the"
2020.emnlp-main.424,bender-2014-language,0,0.0126653,"lemmas, an accompanying step in fieldwork is that of elicitation of inflectional paradigms for selected lemmas. Presenting candidate words to a native speaker for acceptance or rejection is often easier than asking the speaker to grasp the abstract concept of a paradigm and to generate the missing cells in a table. With the help of IGT2P, linguists could use the machine-generated word forms to support this elicitation process. IGT2P then becomes a tool for the discovery of morphological patterns in under-described and endangered languages. 3 Related Work IGT for NLP. The AGGREGATION project (Bender, 2014) has used IGT to automatically construct grammars for multiple languages. This includes inferring and visualizing systems of morphosyntax (Lepp et al., 2019; Wax, 2014). Much of their data comes from the Online Database of INterlinear Text (Lewis and Xia, 2010, ODIN) which is a collection of IGTs extracted from published linguistic documents on the web. Published IGT excerpts, such as those in ODIN, differ from IGTs produced by field linguists such as those used in our experiments. First, noise is generally removed from the published examples. Second, the amount of glossed information in publi"
2020.emnlp-main.424,K18-3001,1,0.86364,"Missing"
2020.emnlp-main.424,K17-2001,1,0.92412,"Missing"
2020.emnlp-main.424,E17-1049,1,0.901192,"Missing"
2020.emnlp-main.424,P17-1182,1,0.917577,"Missing"
2020.emnlp-main.424,E17-2120,0,0.0177591,"u and Cotterell (2019) and Wu et al. (2020) in our experiments. Work on paradigm completion – or the paradigm cell filling problem (PCFP; Ackerman et al., 2009) – includes Malouf (2016), who trained recurrent neural networks for it, and applied them successfully to Irish, Maltese, and Khaling, among other languages. Silfverberg and Hulden (2018) also trained neural networks for the task. Kann et al. (2017a) differed from other approaches in that they encoded multiple inflected forms of a lemma to provide complementary information for the generation of unknown forms of the same lemma. Finally, Cotterell et al. (2017b) introduced neural graphical models which completed paradigms based on principal parts. The unsupervised version of the paradigm completion task (Jin et al., 2020) has been the subject of a recent shared task (Kann et al., 2020b), with the conclusion that it is exremely challenging for current state-of-the-art systems. Here, we propose to, instead of generating paradigms from raw text, generate them from IGT, a resource available for many under-studied languages. 4 To POS Tag or Not to POS Tag In addition to the lemma and the morphological features of the target form, part-of-speech (POS) ta"
2020.emnlp-main.424,P16-2090,1,0.910705,"Missing"
2020.emnlp-main.424,N19-4022,0,0.0210044,"speaker for acceptance or rejection is often easier than asking the speaker to grasp the abstract concept of a paradigm and to generate the missing cells in a table. With the help of IGT2P, linguists could use the machine-generated word forms to support this elicitation process. IGT2P then becomes a tool for the discovery of morphological patterns in under-described and endangered languages. 3 Related Work IGT for NLP. The AGGREGATION project (Bender, 2014) has used IGT to automatically construct grammars for multiple languages. This includes inferring and visualizing systems of morphosyntax (Lepp et al., 2019; Wax, 2014). Much of their data comes from the Online Database of INterlinear Text (Lewis and Xia, 2010, ODIN) which is a collection of IGTs extracted from published linguistic documents on the web. Published IGT excerpts, such as those in ODIN, differ from IGTs produced by field linguists such as those used in our experiments. First, noise is generally removed from the published examples. Second, the amount of glossed information in published IGT snippets can vary widely depending on the phenomenon that is the main focus of the publication. Computational morphology. Our work is further relat"
2020.emnlp-main.424,D18-1314,0,0.0119307,"the store.’ v v in magazin. magazin store.ACC Table 2: An example of typical interlinear glossed text (IGT) with a transliterated Russian sentence, including translation. This paper leverages the original text and gloss lines. ical inflection or reinflection. Approaches include Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016); Kann and Sch¨utze (2016); Aharoni and Goldberg (2017). Partially building on these, other research has developed models which are more suitable for low-resource languages and perform well with limited data (Kann et al., 2017b; Sharma et al., 2018; Makarov and Clematide, 2018; Wu and Cotterell, 2019; Kann et al., 2020a; Wu et al., 2020). These are the most relevant approaches for our work, since we expect IGT2P to aid documentation of low-resource languages. Accordingly, we use the systems by Wu and Cotterell (2019) and Wu et al. (2020) in our experiments. Work on paradigm completion – or the paradigm cell filling problem (PCFP; Ackerman et al., 2009) – includes Malouf (2016), who trained recurrent neural networks for it, and applied them successfully to Irish, Maltese, and Khaling, among other languages. Silfverberg and Hulden (2018) also trained neural networks"
2020.emnlp-main.424,W19-4226,1,0.816571,"ng paradigm cells. Noisy paradigms are automatically constructed from IGT and a language expert creates “cleaned” paradigms. Both sets are tested on the same missing word forms and the results are compared. Introduction Over the last few years, multiple shared tasks have encouraged the development of systems for learning morphology, including generating inflected forms of the canonical form—the lemma—of a word. NLP systems that account for morphology can reduce data sparsity caused by an abundance of individual word forms in morphologically rich languages (Cotterell et al., 2016, 2017a, 2018; McCarthy et al., 2019; Vylomova et al., 2020) and help mitigate bias in training data for natural language processing (NLP) systems (Zmigrod et al., 2019). However, such systems have often been limited to languages with publicly available structured data, i.e. languages for which tables containing inflectional patterns can be found, for example, in online dictionaries like Wiktionary.1 This limits the development of NLP systems for morphology to languages for which morphological information can be easily extracted. Here, we propose to instead make use of a resource which is much more common, especially for low-res"
2020.emnlp-main.424,N15-1093,0,0.0695259,"1. Most recent work in the area of computational morphology which was concerned with generation (as opposed to analysis) has focused on morpholog5253 Text Segmented Glossed Translation Vecherom ya pobejala vecher-om ya pobeja-la evening-INS 1.SG.NOM run-PFV.PST.SG.FEM ‘In the evening I ran to the store.’ v v in magazin. magazin store.ACC Table 2: An example of typical interlinear glossed text (IGT) with a transliterated Russian sentence, including translation. This paper leverages the original text and gloss lines. ical inflection or reinflection. Approaches include Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016); Kann and Sch¨utze (2016); Aharoni and Goldberg (2017). Partially building on these, other research has developed models which are more suitable for low-resource languages and perform well with limited data (Kann et al., 2017b; Sharma et al., 2018; Makarov and Clematide, 2018; Wu and Cotterell, 2019; Kann et al., 2020a; Wu et al., 2020). These are the most relevant approaches for our work, since we expect IGT2P to aid documentation of low-resource languages. Accordingly, we use the systems by Wu and Cotterell (2019) and Wu et al. (2020) in our experiments. Work on parad"
2020.emnlp-main.424,D18-1315,1,0.806599,"l., 2017b; Sharma et al., 2018; Makarov and Clematide, 2018; Wu and Cotterell, 2019; Kann et al., 2020a; Wu et al., 2020). These are the most relevant approaches for our work, since we expect IGT2P to aid documentation of low-resource languages. Accordingly, we use the systems by Wu and Cotterell (2019) and Wu et al. (2020) in our experiments. Work on paradigm completion – or the paradigm cell filling problem (PCFP; Ackerman et al., 2009) – includes Malouf (2016), who trained recurrent neural networks for it, and applied them successfully to Irish, Maltese, and Khaling, among other languages. Silfverberg and Hulden (2018) also trained neural networks for the task. Kann et al. (2017a) differed from other approaches in that they encoded multiple inflected forms of a lemma to provide complementary information for the generation of unknown forms of the same lemma. Finally, Cotterell et al. (2017b) introduced neural graphical models which completed paradigms based on principal parts. The unsupervised version of the paradigm completion task (Jin et al., 2020) has been the subject of a recent shared task (Kann et al., 2020b), with the conclusion that it is exremely challenging for current state-of-the-art systems. He"
2020.emnlp-main.424,P19-1148,0,0.185939,"agazin store.ACC Table 2: An example of typical interlinear glossed text (IGT) with a transliterated Russian sentence, including translation. This paper leverages the original text and gloss lines. ical inflection or reinflection. Approaches include Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016); Kann and Sch¨utze (2016); Aharoni and Goldberg (2017). Partially building on these, other research has developed models which are more suitable for low-resource languages and perform well with limited data (Kann et al., 2017b; Sharma et al., 2018; Makarov and Clematide, 2018; Wu and Cotterell, 2019; Kann et al., 2020a; Wu et al., 2020). These are the most relevant approaches for our work, since we expect IGT2P to aid documentation of low-resource languages. Accordingly, we use the systems by Wu and Cotterell (2019) and Wu et al. (2020) in our experiments. Work on paradigm completion – or the paradigm cell filling problem (PCFP; Ackerman et al., 2009) – includes Malouf (2016), who trained recurrent neural networks for it, and applied them successfully to Irish, Maltese, and Khaling, among other languages. Silfverberg and Hulden (2018) also trained neural networks for the task. Kann et al"
2020.emnlp-main.424,2021.eacl-main.163,1,0.931506,"Missing"
2020.emnlp-main.424,P00-1027,0,0.519567,"Missing"
2020.emnlp-main.424,P19-1161,0,0.0284426,"s are tested on the same missing word forms and the results are compared. Introduction Over the last few years, multiple shared tasks have encouraged the development of systems for learning morphology, including generating inflected forms of the canonical form—the lemma—of a word. NLP systems that account for morphology can reduce data sparsity caused by an abundance of individual word forms in morphologically rich languages (Cotterell et al., 2016, 2017a, 2018; McCarthy et al., 2019; Vylomova et al., 2020) and help mitigate bias in training data for natural language processing (NLP) systems (Zmigrod et al., 2019). However, such systems have often been limited to languages with publicly available structured data, i.e. languages for which tables containing inflectional patterns can be found, for example, in online dictionaries like Wiktionary.1 This limits the development of NLP systems for morphology to languages for which morphological information can be easily extracted. Here, we propose to instead make use of a resource which is much more common, especially for low-resource languages: we explore how to leverage interlinear glossed text (IGT)—a common artifact of linguistic field research—to generate"
2020.emnlp-main.94,D14-1162,0,0.0830111,"Missing"
2020.emnlp-main.94,D16-1126,0,0.0164152,". They also generated acrostic poems, but used character-level modelling to achieve this – which was simpler than in our case, since they worked with Chinese text where characters often correspond to entire words. Our preliminary experiments on English showed that character-level models learn easily to generate acrostics by themselves, however do not follow the topic as coherently as word-level models. Zhang and Lapata (2014); Zhang et al. (2017); Yang et al. (2017); Yi et al. (2018a,c); Yang et al. (2019) are other examples of work on generating Chinese poems, but did not focus on acrostics. Ghazvininejad et al. (2016, 2017) built a model to generate poems based on topics in a similar fashion to ours. However, they chose words related to a given topic to be the last words in each line – and to rhyme. For this, they built rhyming classes for an input topic first, from which rhyming pairs were chosen. The most obvious differences to our work are, however, that they produced poems following predefined stress patterns, while we are interested in free verse poems and that we generate acrostic poems, while they did not. Finally, some additional work on poem generation includes Yi et al. (2018b), who applied rein"
2020.emnlp-main.94,P17-4008,0,0.431215,"Missing"
2020.emnlp-main.94,W17-3502,0,0.0125458,"d by our proposed baseline model for the word poet. Introduction Poetry, derived from the Greek word poiesis (”making”), is the art of combining rhythmic and aesthetic properties of a language to convey a specific message. Its creation is a manifestation of creativity, and, as such, hard to automate. However, since the development of creative machines is a crucial step towards real artificial intelligence, automatic poem generation is an important task at the intersection of computational creativity and natural language generation, and earliest attempts date back several decades; see Gonc¸alo Oliveira (2017) for an overview. With this paper, we add a new task to this research area: acrostic poem generation in English. Acrostic poems, or simply acrostics, are a special type of poetry, in which typically the first letter of each line spells out a word or message, as in the example in Figure 1. While this is the only formal characteristic of an acrostic, we here define the task of acrostic poem generation as generating poems such that, additionally, poems should also both rhyme and relate to the topic of their hidden word, e.g., the content of the poem in Figure 1 should be related to the word ”poet"
2020.emnlp-main.94,P18-1181,0,0.284826,"icitly state the topics of individual poems. Thus, we automatically predict a topic for each poem with the help of our topic prediction model, which will be described in Subsection 3.3. The poems in UnknownTopicPoems are again broken down into poems with 4 to 8 lines, and, for our main experiments, this dataset is combined with KnownTopicPoems, increasing the number of samples to 119,252 poems for 144 topics. Table 1 shows detailed statistics for both datasets. All poems are tokenized with the help of NLTK. Sonnets. The last poem dataset we make use of is the sonnet poem dataset introduced by Lau et al. (2018). This dataset consists of Shakespearean sonnets. Since those differ significantly in style from the poems in KnownTopicPoems and UnknownTopicPoems, we do not train our language model directly on them. However, we make use of the fact that sonnets follow a known rhyming scheme, and leverage them to train a neural model to produce rhymes, which will be explained in detail in Subsection 3.2. As for the previous two datasets, poems are tokenized using NLTK. 3 https://github.com/researchmm/ img2poem/blob/master/data/unim_poem.json 1231 Wikipedia. Finally, we utilize a large English Wikipedia corpu"
2020.emnlp-main.94,K18-1024,0,0.0136833,"ion of a convolutional neural network (CNN) and a gated recurrent unit (GRU) to generate poems which related to the target image. They also generated acrostic poems, but used character-level modelling to achieve this – which was simpler than in our case, since they worked with Chinese text where characters often correspond to entire words. Our preliminary experiments on English showed that character-level models learn easily to generate acrostics by themselves, however do not follow the topic as coherently as word-level models. Zhang and Lapata (2014); Zhang et al. (2017); Yang et al. (2017); Yi et al. (2018a,c); Yang et al. (2019) are other examples of work on generating Chinese poems, but did not focus on acrostics. Ghazvininejad et al. (2016, 2017) built a model to generate poems based on topics in a similar fashion to ours. However, they chose words related to a given topic to be the last words in each line – and to rhyme. For this, they built rhyming classes for an input topic first, from which rhyming pairs were chosen. The most obvious differences to our work are, however, that they produced poems following predefined stress patterns, while we are interested in free verse poems and that we"
2020.emnlp-main.94,D18-1353,0,0.0111068,"ion of a convolutional neural network (CNN) and a gated recurrent unit (GRU) to generate poems which related to the target image. They also generated acrostic poems, but used character-level modelling to achieve this – which was simpler than in our case, since they worked with Chinese text where characters often correspond to entire words. Our preliminary experiments on English showed that character-level models learn easily to generate acrostics by themselves, however do not follow the topic as coherently as word-level models. Zhang and Lapata (2014); Zhang et al. (2017); Yang et al. (2017); Yi et al. (2018a,c); Yang et al. (2019) are other examples of work on generating Chinese poems, but did not focus on acrostics. Ghazvininejad et al. (2016, 2017) built a model to generate poems based on topics in a similar fashion to ours. However, they chose words related to a given topic to be the last words in each line – and to rhyme. For this, they built rhyming classes for an input topic first, from which rhyming pairs were chosen. The most obvious differences to our work are, however, that they produced poems following predefined stress patterns, while we are interested in free verse poems and that we"
2020.emnlp-main.94,P17-1125,0,0.0369952,"Missing"
2020.emnlp-main.94,D14-1074,0,0.5612,"ined on gold poems, i.e., our KnownTopicPoems dataset. ”+” and ”-” indicate if topics are fed into the model (+) or substituted by zero vectors (-). • PRED/GOLD+ and PRED/GOLD-. We further train language models on both the KnownTopicPoems dataset and the UnknownTopicPoem dataset. ”+” and ”-” indicate if topics are fed into the model (+) or substituted by zero vectors (-). 4.2 Human Evaluation Of Poems Experimental setup. In order to get an idea of the difficulty of our proposed task, we need to assess the quality of the poems generated by our baseline. Following previous work (Manurung, 2003; Zhang and Lapata, 2014; Loller-Andersen and Gamb¨ack, 2018), we ask human annotators to evaluate 40 poems generated for the words in Table 4 for readability (lexical and syntactic coherence), meaningfulness (if the poem can be interpreted as conveying a message to its reader), and poeticness (if the poem rhymes and looks like a poem) on a scale from 1 (worst) to 5 (best). Additionally, we also ask for an overall score. We collect a minimum of 2 and a maximum of 5 ratings for each aspect of each poem. All annotators are fluent in English: they either are or have in the past been working or studying at an English-spe"
2020.iwpt-1.11,P06-1109,0,0.0774761,"hitecture is to enable self-training, we further hypothesize that, since language modeling and parsing annotations seem to provide complementary information, UP should aid low-resource supervised parsing. As a proof of concept, we employ SS-PRPN for semisupervised training. In extremely-low-data regimes with no more than 250 labeled parses, SS-PRPN outperforms supervised and unsupervised baselines in most settings on unlabeled parsing, and in all settings on labeled constituency parsing. Related Work Following the line of research on non-neural UP models (Clark, 2001; Klein and Manning, 2002; Bod, 2006), early approaches to neural UP (Yogatama et al., 2017; Choi et al., 2018) obtain improved performance on downstream tasks, yet show highly inconsistent behavior in parsing (Williams et al., 2018). Recently, Shen et al. (2018a) introduce the first high performing neural UP model (Htut et al., 2018). Dyer et al. (2019) raise concerns that PRPN’s parsing methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen et al. (2019)"
2020.iwpt-1.11,W01-0713,0,0.171502,"ics velopment of a semi-supervised architecture is to enable self-training, we further hypothesize that, since language modeling and parsing annotations seem to provide complementary information, UP should aid low-resource supervised parsing. As a proof of concept, we employ SS-PRPN for semisupervised training. In extremely-low-data regimes with no more than 250 labeled parses, SS-PRPN outperforms supervised and unsupervised baselines in most settings on unlabeled parsing, and in all settings on labeled constituency parsing. Related Work Following the line of research on non-neural UP models (Clark, 2001; Klein and Manning, 2002; Bod, 2006), early approaches to neural UP (Yogatama et al., 2017; Choi et al., 2018) obtain improved performance on downstream tasks, yet show highly inconsistent behavior in parsing (Williams et al., 2018). Recently, Shen et al. (2018a) introduce the first high performing neural UP model (Htut et al., 2018). Dyer et al. (2019) raise concerns that PRPN’s parsing methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been"
2020.iwpt-1.11,N19-1116,0,0.0159894,"e concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen et al. (2019) propose an architecture consisting of an LSTM (Hochreiter and Schmidhuber, 1997) with a modified update function for the LSTM cell state, Kim et al. (2019a)—the current state-of-the-art—introduce a model based on a mixture of probabilistic context-free grammars, Kim et al. (2019b) present unsupervised learning of recurrent neural networks grammars, Li et al. (2019) combine PRPN with imitation learning, and Drozdov et al. (2019) employ a recursive autoencoder. Kim et al. (2020) examine tree induction from pretrained models. 2 Model Syntactic Distances In order to parse a sentence, a computational model needs to output some kind of variables representing a unique tree structure. The variables we use are syntactic distances as introduced by Shen et al. (2018a). They represent the syntactic relationships between all successive pairs of words in a sentence. If the distance between two neighboring words is large, they belong to different subtrees, and, thus, their traversal distance in the tree is large. A parse tree can"
2020.iwpt-1.11,P19-1228,0,0.611094,"ituency trees without the need for annotated treebanks. Self-training (Yarowsky, 1995; Riloff et al., 2003) consists of training a model, using it to label new examples and, based on a confidence metric, adding a subset to the training set, before repeating training. For supervised parsing, results with self-training have been mixed (Charniak, 1997; Steedman et al., 2003; McClosky D, 2006). For unsupervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly on language modeling and supervised parsing. This enables our model to leverage silver-standard annotations obtained via selftraining for supervision. Our approach"
2020.iwpt-1.11,N19-1114,0,0.219698,"ituency trees without the need for annotated treebanks. Self-training (Yarowsky, 1995; Riloff et al., 2003) consists of training a model, using it to label new examples and, based on a confidence metric, adding a subset to the training set, before repeating training. For supervised parsing, results with self-training have been mixed (Charniak, 1997; Steedman et al., 2003; McClosky D, 2006). For unsupervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly on language modeling and supervised parsing. This enables our model to leverage silver-standard annotations obtained via selftraining for supervision. Our approach"
2020.iwpt-1.11,P02-1017,0,0.23577,"of a semi-supervised architecture is to enable self-training, we further hypothesize that, since language modeling and parsing annotations seem to provide complementary information, UP should aid low-resource supervised parsing. As a proof of concept, we employ SS-PRPN for semisupervised training. In extremely-low-data regimes with no more than 250 labeled parses, SS-PRPN outperforms supervised and unsupervised baselines in most settings on unlabeled parsing, and in all settings on labeled constituency parsing. Related Work Following the line of research on non-neural UP models (Clark, 2001; Klein and Manning, 2002; Bod, 2006), early approaches to neural UP (Yogatama et al., 2017; Choi et al., 2018) obtain improved performance on downstream tasks, yet show highly inconsistent behavior in parsing (Williams et al., 2018). Recently, Shen et al. (2018a) introduce the first high performing neural UP model (Htut et al., 2018). Dyer et al. (2019) raise concerns that PRPN’s parsing methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen e"
2020.iwpt-1.11,N15-1067,0,0.271032,"d Dl can be supervised, but Dl can also be learned in a latent manner. Introduction Unsupervised parsing (UP) models learn to parse sentences into unlabeled constituency trees without the need for annotated treebanks. Self-training (Yarowsky, 1995; Riloff et al., 2003) consists of training a model, using it to label new examples and, based on a confidence metric, adding a subset to the training set, before repeating training. For supervised parsing, results with self-training have been mixed (Charniak, 1997; Steedman et al., 2003; McClosky D, 2006). For unsupervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly"
2020.iwpt-1.11,P19-1338,0,0.0174027,"ng methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen et al. (2019) propose an architecture consisting of an LSTM (Hochreiter and Schmidhuber, 1997) with a modified update function for the LSTM cell state, Kim et al. (2019a)—the current state-of-the-art—introduce a model based on a mixture of probabilistic context-free grammars, Kim et al. (2019b) present unsupervised learning of recurrent neural networks grammars, Li et al. (2019) combine PRPN with imitation learning, and Drozdov et al. (2019) employ a recursive autoencoder. Kim et al. (2020) examine tree induction from pretrained models. 2 Model Syntactic Distances In order to parse a sentence, a computational model needs to output some kind of variables representing a unique tree structure. The variables we use are syntactic distances as introduced by Shen et al. (2018a). They represent the syntactic relationships between all successive pairs of words in a sentence. If the distance between two neighboring words is large, they belong to different subtrees, and, thus,"
2020.iwpt-1.11,N06-1020,0,0.25874,"Missing"
2020.iwpt-1.11,W03-0404,0,0.131515,"Missing"
2020.iwpt-1.11,N16-1024,0,0.0239796,"PRPN 35 50 100 150 200 250 Figure 3: Low-resource parsing on the PTB. The first and second plots show unlabeled and labeled F1 respectively, plotted against the training data size. Low-Resource Parsing Performance We further investigate how SS-PRPN performs when limited gold parses are available in addition to unlabeled data. To predict constituency labels, we add and train an additional linear output layer after the first convolutional layer. We find that, on the development set, converting Dg into parse trees works better for low-resource parsing than Dl . As supervised baselines, we employ Dyer et al. (2016)’s recurrent neural network grammar (RNNG) and a supervised parser (SP) based on syntactic distances (Shen et al., 2018b). Figure 3 shows results for 50 to 250 annotated examples. The upper part shows the unlabeled parsing performance in comparison to the UP baselines. We outperform all baselines for 50 to 150 examples, while SP performs slightly better with more annotations. When looking at labeled F1 in the lower part of Figure 3, SS-PRPN clearly outperforms SP, which indicates that unlabeled data can be leveraged in the low-resource setting. 5 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Number of"
2020.iwpt-1.11,P18-1108,0,0.164263,"ervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly on language modeling and supervised parsing. This enables our model to leverage silver-standard annotations obtained via selftraining for supervision. Our approach draws on the idea of syntactic distances, which can be learned both as latent variables (Shen et al., 2018a) and as explicit supervision targets (Shen et al., 2018b). We use both of these, leveraging annotations obtained via UP to supervise the two different outputs of the parser, in addition to standard UP training. SS-PRPN, in combination with self-training, improves over its original version by 8."
2020.iwpt-1.11,W18-5452,1,0.906162,"-data regimes with no more than 250 labeled parses, SS-PRPN outperforms supervised and unsupervised baselines in most settings on unlabeled parsing, and in all settings on labeled constituency parsing. Related Work Following the line of research on non-neural UP models (Clark, 2001; Klein and Manning, 2002; Bod, 2006), early approaches to neural UP (Yogatama et al., 2017; Choi et al., 2018) obtain improved performance on downstream tasks, yet show highly inconsistent behavior in parsing (Williams et al., 2018). Recently, Shen et al. (2018a) introduce the first high performing neural UP model (Htut et al., 2018). Dyer et al. (2019) raise concerns that PRPN’s parsing methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen et al. (2019) propose an architecture consisting of an LSTM (Hochreiter and Schmidhuber, 1997) with a modified update function for the LSTM cell state, Kim et al. (2019a)—the current state-of-the-art—introduce a model based on a mixture of probabilistic context-free grammars, Kim et al. (2019b) present unsuperv"
2020.iwpt-1.11,E03-1008,0,0.171205,"ser, represented by the dotted box, outputs syntactic distances Dg and Dl . Both Dg and Dl can be supervised, but Dl can also be learned in a latent manner. Introduction Unsupervised parsing (UP) models learn to parse sentences into unlabeled constituency trees without the need for annotated treebanks. Self-training (Yarowsky, 1995; Riloff et al., 2003) consists of training a model, using it to label new examples and, based on a confidence metric, adding a subset to the training set, before repeating training. For supervised parsing, results with self-training have been mixed (Charniak, 1997; Steedman et al., 2003; McClosky D, 2006). For unsupervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension"
2020.iwpt-1.11,P95-1026,0,0.883065,"Missing"
2020.scil-1.19,W17-5401,0,0.0391357,"Missing"
2020.scil-1.19,K18-3001,1,0.93987,"Missing"
2020.scil-1.19,N19-1423,0,0.0280978,"or other details of the training regime, e.g., dropout, might differ. Pretraining can be seen as finding a suitable initialization of model parameters, before training on limited amounts of task- or language-specific examples. In the context of morphological generation, pretraining in combination with fine-tuning has been used by Kann and Sch¨utze (2018), which proposes to pretrain a model on general inflection data and fine-tune on examples from a specific paradigm whose remaining forms should be automatically generated. Famous examples for pretraining in the wider area of NLP include BERT (Devlin et al., 2019) or GPT-2 (Radford et al., 2019): there, general properties of language are learned using large unlabeled corpora. Here, we are interested in pretraining as a simulation of familiarity with a native language. By investigating a fine-tuned model we ask the question: How does extensive knowledge of one language influence the acquisition of another? 4 4.1 Experimental Design Target Languages We choose three target languages. English (ENG) is a morphologically impoverished language, as far as inflectional morphology is concerned. Its verbal paradigm only consists of up to 5 different forms and its"
2020.scil-1.19,N13-1138,0,0.0219544,"ections of a lemma is called its paradigm. Each inflection within a paradigm can be associated with a tag, i.e., 3rdSgPres is the morphological tag associated with the inflection dances of the English lemma dance. We display the paradigms of dance and eat in Table 1. The presence of rich inflectional morphology is problematic for NLP systems as it increases word form sparsity. For instance, while English verbs can have up to 5 inflected forms, Archi verbs have thousands (Kibrik, 1998), even by a conservative count. Thus, an important task in the area of morphology is morphological inflection (Durrett and DeNero, 2013; Cotterell et al., 2018), which consists of mapping a lemma to an indicated inflected form. An (irregular) English example would be (eat, PAST) ! ate with PAST being the target tag, denoting the past tense form. Additionally, a rich inflectional morphology is also challenging for L2 language learners, since both rules and their exceptions need to be memorized. In NLP, morphological inflection has recently frequently been cast as a sequence-to-sequence problem, where the sequence of target (sub-)tags together with the sequence of input characters constitute the input sequence, and the characte"
2020.scil-1.19,W16-2524,0,0.0137967,"enced by a learner’s native language (L1) (Dulay and Burt, 1974; Kellerman, 1979). A language’s morphosyntax seems to be no exception to this rule (Bliss, 2006), but the exact nature of this influence remains unknown. For instance, it is unclear whether it is constraints imposed by the phonological or by the morphosyntactic attributes of the L1 that are more important during the process of learning an L2’s morphosyntax. Within the area of natural language processing (NLP) research, experimenting on neural network models just as if they were human subjects has recently been gaining popularity (Ettinger et al., 2016, 2017; Kim et al., 2019). Often, so-called probing tasks are used, which require a specific subset of linguistic knowledge and can, thus, be Inf 3rdSgPres PresPart Past PastPart walk eat dance dances dancing danced danced eat eats eating ate eaten Table 1: Paradigms of the English lemmas dance and eat. dance has 4 distinct inflected forms; eat has 5. leveraged for qualitative evaluation. The goal is to answer the question: What do neural networks learn that helps them to succeed in a given task? Neural network models, and specifically sequence-to-sequence models, have pushed the state of the"
2020.scil-1.19,P16-1154,0,0.0189357,"morphological inflection has recently frequently been cast as a sequence-to-sequence problem, where the sequence of target (sub-)tags together with the sequence of input characters constitute the input sequence, and the characters of the inflected word form the output. Neural models Formal definition. Let M be the paradigm slots which are being expressed in a language, and w a lemma in that language. We then define the paradigm ⇡ of w as: ⇡(w) = 3 3.1 n fk [w], tk o k2M(w) (1) Model Pointer–Generator Network The models we experiment with are based on a pointer–generator network architecture (Gu et al., 2016; See et al., 2017), i.e., a recurrent neural network (RNN)-based sequence-to-sequence network with attention and a copy mechanism. A standard sequence-to-sequence model (Bahdanau et al., 2015) has been shown to perform well for morphological inflection (Kann and Sch¨utze, 2016) and has, thus, been subject to cognitively motivated experiments (Kirov and Cotterell, 2018) before. Here, however, we choose the pointer– generator variant of Sharma et al. (2018), since it performs better in low-resource settings, which we will assume for our target languages. We will explain the model shortly in the"
2020.scil-1.19,W17-4110,1,0.854209,"imilarly to us, Gorman et al. (2019) performed a manual error analysis of morphological inflection systems for multiple languages. However, they did not investigate transfer learning, but focused on monolingual models. Outside the scope of the shared tasks, Kann et al. (2017) investigated cross-lingual transfer for morphological inflection, but was limited to a quantitative analysis. Furthermore, that work experimented with a standard sequence-to-sequence model (Bahdanau et al., 2015) in a multi-task training fashion (Caruana, 1997), while we pretrain and fine-tune pointer–generator networks. Jin and Kann (2017) also investigated crosslingual transfer in neural sequence-to-sequence models for morphological inflection. However, their experimental setup mimicked Kann et al. (2017), and the main research questions were different: While Jin and Kann (2017) asked how cross-lingual knowledge transfer works during multi-task training of neural sequence-tosequence models on two languages, we investigate if neural inflection models demonstrate interesting differences in production errors depending on the pretraining language. Besides that, we differ in the artificial neural network architecture and language p"
2020.scil-1.19,P17-1182,1,0.920223,"Missing"
2020.scil-1.19,P16-2090,1,0.857378,"Missing"
2020.scil-1.19,D18-1363,1,0.8724,"Missing"
2020.scil-1.19,S19-1026,0,0.0311351,"Missing"
2020.scil-1.19,W19-4226,0,0.0385434,"Missing"
2020.scil-1.19,P17-1099,0,0.0333904,"flection has recently frequently been cast as a sequence-to-sequence problem, where the sequence of target (sub-)tags together with the sequence of input characters constitute the input sequence, and the characters of the inflected word form the output. Neural models Formal definition. Let M be the paradigm slots which are being expressed in a language, and w a lemma in that language. We then define the paradigm ⇡ of w as: ⇡(w) = 3 3.1 n fk [w], tk o k2M(w) (1) Model Pointer–Generator Network The models we experiment with are based on a pointer–generator network architecture (Gu et al., 2016; See et al., 2017), i.e., a recurrent neural network (RNN)-based sequence-to-sequence network with attention and a copy mechanism. A standard sequence-to-sequence model (Bahdanau et al., 2015) has been shown to perform well for morphological inflection (Kann and Sch¨utze, 2016) and has, thus, been subject to cognitively motivated experiments (Kirov and Cotterell, 2018) before. Here, however, we choose the pointer– generator variant of Sharma et al. (2018), since it performs better in low-resource settings, which we will assume for our target languages. We will explain the model shortly in the following and refe"
2020.scil-1.19,P11-2120,0,0.012019,"ask training of neural sequence-tosequence models on two languages, we investigate if neural inflection models demonstrate interesting differences in production errors depending on the pretraining language. Besides that, we differ in the artificial neural network architecture and language pairs we investigate. Cross-lingual transfer in NLP. Cross-lingual transfer learning has been used for a large variety NLP of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Machine translation has been no exception (Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017). Recent research asked how to automatically select a suitable source language for a given target language (Lin et al., 2019). This is similar to our work in that our findings could potentially be leveraged to find good source languages. Acquisition of morphological inflection. Finally, a lot of research has focused on human L1 and L2 acquisition of inflectional morphology (Salaberry, 2000; Herschensohn, 2001; Housen, 2002; Ionin and Wexler, 2002; Weerman et al., 2006;"
2020.scil-1.19,N16-1161,0,0.0214394,"ow cross-lingual knowledge transfer works during multi-task training of neural sequence-tosequence models on two languages, we investigate if neural inflection models demonstrate interesting differences in production errors depending on the pretraining language. Besides that, we differ in the artificial neural network architecture and language pairs we investigate. Cross-lingual transfer in NLP. Cross-lingual transfer learning has been used for a large variety NLP of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Machine translation has been no exception (Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017). Recent research asked how to automatically select a suitable source language for a given target language (Lin et al., 2019). This is similar to our work in that our findings could potentially be leveraged to find good source languages. Acquisition of morphological inflection. Finally, a lot of research has focused on human L1 and L2 acquisition of inflectional morphology (Salaberry, 2000; Herschensohn, 2001; Housen, 2002"
2020.scil-1.19,Q14-1005,0,0.0273457,"different: While Jin and Kann (2017) asked how cross-lingual knowledge transfer works during multi-task training of neural sequence-tosequence models on two languages, we investigate if neural inflection models demonstrate interesting differences in production errors depending on the pretraining language. Besides that, we differ in the artificial neural network architecture and language pairs we investigate. Cross-lingual transfer in NLP. Cross-lingual transfer learning has been used for a large variety NLP of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Machine translation has been no exception (Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017). Recent research asked how to automatically select a suitable source language for a given target language (Lin et al., 2019). This is similar to our work in that our findings could potentially be leveraged to find good source languages. Acquisition of morphological inflection. Finally, a lot of research has focused on human L1 and L2 acquisition of inflectional morphology (Salabe"
2020.scil-1.19,N16-1004,0,0.0216194,"ate if neural inflection models demonstrate interesting differences in production errors depending on the pretraining language. Besides that, we differ in the artificial neural network architecture and language pairs we investigate. Cross-lingual transfer in NLP. Cross-lingual transfer learning has been used for a large variety NLP of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Machine translation has been no exception (Zoph and Knight, 2016; Ha et al., 2016; Johnson et al., 2017). Recent research asked how to automatically select a suitable source language for a given target language (Lin et al., 2019). This is similar to our work in that our findings could potentially be leveraged to find good source languages. Acquisition of morphological inflection. Finally, a lot of research has focused on human L1 and L2 acquisition of inflectional morphology (Salaberry, 2000; Herschensohn, 2001; Housen, 2002; Ionin and Wexler, 2002; Weerman et al., 2006; Zhang and Widyastuti, 2010). To name some specific examples, Marqu´esPascual (2011) in"
2020.sigmorphon-1.13,2020.sigmorphon-1.2,0,0.0555792,"Missing"
2020.sigmorphon-1.13,2020.lrec-1.521,0,0.329667,"Missing"
2020.sigmorphon-1.13,2021.eacl-main.163,0,0.777578,"Missing"
2020.sigmorphon-1.3,P03-1036,0,0.427615,"Missing"
2020.sigmorphon-1.3,2020.sigmorphon-1.11,0,0.0613575,"Missing"
2020.sigmorphon-1.3,N15-1107,1,0.866875,"riori (MAP) estimations to determine segmentation points, or minimum description length (MDL)-based approaches. However, they tended to make assumptions regarding how morphemes are combined, and worked best for purely concatenative morphology. Furthermore, these methods had no productive method of handling allomorphy—morphemic variance was simply treated as separate morphemes. tently word embeddings change between related word forms, with the goal of providing useful word embeddings for unseen words. Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011; Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; Nicolai et al., 2020) learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013). The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is reflected in the word form,"
2020.sigmorphon-1.3,D11-1057,0,0.0701913,"(MLE) or maximum a posteriori (MAP) estimations to determine segmentation points, or minimum description length (MDL)-based approaches. However, they tended to make assumptions regarding how morphemes are combined, and worked best for purely concatenative morphology. Furthermore, these methods had no productive method of handling allomorphy—morphemic variance was simply treated as separate morphemes. tently word embeddings change between related word forms, with the goal of providing useful word embeddings for unseen words. Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011; Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; Nicolai et al., 2020) learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013). The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is refle"
2020.sigmorphon-1.3,J01-2001,0,0.785887,"Missing"
2020.sigmorphon-1.3,P82-1020,0,0.795212,"Missing"
2020.sigmorphon-1.3,2020.acl-main.598,1,0.850896,"ne of the submitted systems was able to improve over the baseline on average over all 9 test languages. Only on 3 languages did a submitted system obtain the best results. This shows that unsupervised morphological paradigm completion is still largely unsolved. We present an analysis here, so that this shared task will ground further research on the topic. 1 guess1 guess2 lemma2 lemma2 guess guess guess lemma 1 1 3 guess 2 4 guess guess 1 guess 5 guess 2 guess 3 guess 4 6 lemman guess guess guess 3 5 guess 4 6 guess5 guess6 Figure 1: The task of unsupervised morphological paradigm completion (Jin et al., 2020) consists of generating complete inflectional paradigms for given lemmas, with the only additional available information being a corpus without annotations. ing have not yet been developed. We anticipate that such systems will be extremely useful, as they will open the possibility of rapid development of first-pass inflectional paradigms in a large set of languages. These can be utilized both in se for generation and as a starting point for elicitation (Sylak-Glassman et al., 2016), thus aiding the development of low-resource human language technologies (Christianson et al., 2018). In this pap"
2020.sigmorphon-1.3,K18-3001,1,0.0792627,"Missing"
2020.sigmorphon-1.3,P16-2090,1,0.901708,"Missing"
2020.sigmorphon-1.3,K17-2001,1,0.925404,"Missing"
2020.sigmorphon-1.3,D18-1363,1,0.921705,"Missing"
2020.sigmorphon-1.3,L18-1293,1,0.871732,"Missing"
2020.sigmorphon-1.3,D18-2012,0,0.0402026,"Missing"
2020.sigmorphon-1.3,P19-1172,1,0.84237,"native morphology. Furthermore, these methods had no productive method of handling allomorphy—morphemic variance was simply treated as separate morphemes. tently word embeddings change between related word forms, with the goal of providing useful word embeddings for unseen words. Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011; Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; Nicolai et al., 2020) learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013). The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots and, finally, generation of unobserved forms. Past shared tasks. This task extends a tradition of SIGMORPHON shared tasks concentrating on inflectional morp"
2020.sigmorphon-1.3,W10-2211,0,0.0931197,"Missing"
2020.sigmorphon-1.3,P17-1099,0,0.0243433,"stom inflection system. The IMS–CUBoulder team relied on LSTM (Hochreiter and Schmidhuber, 1997) sequence-tosequence models for inflection. In IMS-CUB-1, the generation component is based on the architecture by Bahdanau et al. (2015), but with fewer parameters, as suggested by Kann and Sch¨utze (2016). This model – as well as all other inflection components used for systems in this category – receives the sequence of the lemma’s characters and the paradigm slot number as input and produces a sequence of output characters. Their second system, IMS-CUB-2, uses an LSTM pointer-generator network (See et al., 2017) instead. This architecture has originally been proposed for low-resource morphological inflection by Sharma et al. (2018). The NYU–CUBoulder team also substituted the baseline’s generation component. Their morphological inflection models are ensembles of dif5 5.1 Results and Analysis Results on Development Languages To encourage reproducibility, we first report the performance of all systems on the development languages in the upper part of Table 4. Although participants were not evaluated on these languages, the results provide insight and enable future researchers to benchmark their progres"
2020.sigmorphon-1.3,2020.sigmorphon-1.9,1,0.755428,"Missing"
2020.sigmorphon-1.3,D18-1314,0,0.0742252,"ieval component is a list of inflected forms with their lemmas, annotated with a paradigm slot number. The generation component receives this output and prepares the data to train an inflectional generator. First, identified inflections are divided into a training and development split, and missing paradigm slots are identified. The generator is trained on the discovered inflections, and new forms are predicted for each missing slot. We used two morphological inflection systems for the two variants of our baseline: the non-neural baseline from Cotterell et al. (2017) and the model proposed by Makarov and Clematide (2018). Both are highly suitable for the low-resource setting. 4.2 ferent combinations of transformer sequence-tosequence models (Vaswani et al., 2017) and pointergenerator transformers, a model they introduced for the task. NYU-CUB-1 is an ensemble of 6 pointergenerator transformers, while NYU-CUB-2 is an ensemble of 6 vanilla transformers. Their last system, NYU-CUB-3, is an ensemble of all 12 models. 4.3 Submitted Systems: Segment+Conquer The KU–CST team did not modify the baseline directly, but, nevertheless, was heavily inspired by it. Their system first employs a charactersegmentation algorith"
2020.sigmorphon-1.3,W19-4226,1,0.657183,"tions, requiring participants to generate an inflection solely utilizing a provided lemma and sentential cues. This task further imitated language learners, but extended beyond morphological learning to morphosyntactic incorporation. Furthermore, removing the requirement of an inflectional feature vector more closely approximated the generation step in our task. However, it was still supervised in that participants were provided with lemma–inflection pairs in context during training. We, in contrast, made no assumption of the existence of such pairs. Finally, the fourth iteration of the task (McCarthy et al., 2019) again concentrated on lesssupervised inflection. Cross-lingual training allowed low-resource inflectors to leverage information from high-resource languages, while a contextual analysis task flipped the previous year’s contextual task on its head—tagging a sentence with inflectional information. This process is very similar to the retrieval portion of our task. We extended this effort to not only identify the paradigm slot of particular word, but to combine learned information from each class to extend and complete existing paradigms. Furthermore, we lifted the requirement of named inflection"
2020.sigmorphon-1.3,P08-1084,0,0.108806,"Missing"
2020.sigmorphon-1.3,N15-1186,0,0.0841628,"previously unencountered word forms, after having studied thousands of other types. The second task (Cotterell et al., 2017) extended While Xu et al. (2018) did discover something similar to paradigms, those paradigms were a means to a segmentation end and the shape or size of the paradigms was not a subject of their research. Moon et al. (2009) similarly uses segmentation and clustering of affixes to group words into conflation sets, groups of morphologically related words, in an unsupervised way. Their work assumes prefixing and suffixing morphology. In a more task-driven line of research, Soricut and Och (2015) develop an approach to learn morphological transformation rules from observing how consis58 6.2 the first task from 10 to 52 languages and started to encourage the development of tools for the lowresource setting. While the first shared task approximated an adult learner with experience with thousands of word forms, low-resource inflection was closer to the language learner that has only studied a small number of inflections—however, it was closer to L2 learning than L1, as it still required training sets with lemma–inflection–slot triplets. The 2017 edition of the shared task also introduced"
2020.sigmorphon-1.3,2020.lrec-1.352,1,0.595113,"rms for all lemmas in both the predictions and the ground truth before evaluating. Provided Resources We provided data for 5 development and 9 test languages. The development languages were available for system development and hyperparameter tuning, while the test languages were released shortly before the shared task deadline. For the test languages, no ground truth data was available before system submission. This setup emulated a realworld scenario with the goal to create a system for languages about which we have no information. For the raw text corpora, we leveraged the JHU Bible Corpus (McCarthy et al., 2020). This resource covers 1600 languages, which will enable future work to quickly produce systems for a large set of languages. Additionally, using the Bible allowed for a fair comparison of models across languages without potential confounds such as domain mismatch. 7 of the languages have only the New Testament available (approximately 8k sentences), and 7 have both the New and Old Testaments (approximately 31k sentences). All morphological information was taken from UniMorph (Sylak-Glassman et al., 2015; Kirov et al., 2018), a resource which contains paradigms for more than 100 languages. How"
2020.sigmorphon-1.3,L16-1497,0,0.0222648,"6 lemman guess guess guess 3 5 guess 4 6 guess5 guess6 Figure 1: The task of unsupervised morphological paradigm completion (Jin et al., 2020) consists of generating complete inflectional paradigms for given lemmas, with the only additional available information being a corpus without annotations. ing have not yet been developed. We anticipate that such systems will be extremely useful, as they will open the possibility of rapid development of first-pass inflectional paradigms in a large set of languages. These can be utilized both in se for generation and as a starting point for elicitation (Sylak-Glassman et al., 2016), thus aiding the development of low-resource human language technologies (Christianson et al., 2018). In this paper, we present the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2). We asked participants to produce systems that can learn to inflect in an unsupervised fashion: given a small corpus (the Bible) together with a list of lemmas for each language, systems for the shared task should output all corresponding inflected forms. In their output, systems had to mark which forms expressed the same morphosyntactic features, e.g., demonstr"
2020.sigmorphon-1.3,D09-1070,0,0.0920434,"Missing"
2020.sigmorphon-1.3,P15-2111,0,0.0214204,"bout which we have no information. For the raw text corpora, we leveraged the JHU Bible Corpus (McCarthy et al., 2020). This resource covers 1600 languages, which will enable future work to quickly produce systems for a large set of languages. Additionally, using the Bible allowed for a fair comparison of models across languages without potential confounds such as domain mismatch. 7 of the languages have only the New Testament available (approximately 8k sentences), and 7 have both the New and Old Testaments (approximately 31k sentences). All morphological information was taken from UniMorph (Sylak-Glassman et al., 2015; Kirov et al., 2018), a resource which contains paradigms for more than 100 languages. However, this information was only accessible to the participants for the development languages. UniMorph paradigms were further used internally for evaluation on the test languages—this data was then released after the conclusion of the shared task. Example. Assume our gold standard is (1) (the complete, 5-slot English paradigms for the verbs walk and listen) and a system outputs the following, including an error in the fourth row: 3.2 Languages During the development phase of the shared task, we released"
2020.sigmorphon-1.3,Q13-1001,0,0.0712336,"Missing"
2020.sigmorphon-1.3,C18-1005,0,0.0169383,"rms. Past shared tasks. This task extends a tradition of SIGMORPHON shared tasks concentrating on inflectional morphology. The first such task (Cotterell et al., 2016) encouraged participants to create inflectional tools in a typologically diverse group of 10 languages. The task was fully-supervised, requiring systems to learn inflectional morphology from a large annotated database. This task is similar to human learners needing to generate inflections of previously unencountered word forms, after having studied thousands of other types. The second task (Cotterell et al., 2017) extended While Xu et al. (2018) did discover something similar to paradigms, those paradigms were a means to a segmentation end and the shape or size of the paradigms was not a subject of their research. Moon et al. (2009) similarly uses segmentation and clustering of affixes to group words into conflation sets, groups of morphologically related words, in an unsupervised way. Their work assumes prefixing and suffixing morphology. In a more task-driven line of research, Soricut and Och (2015) develop an approach to learn morphological transformation rules from observing how consis58 6.2 the first task from 10 to 52 languages"
2020.sigmorphon-1.3,H01-1035,0,0.0415472,"between related word forms, with the goal of providing useful word embeddings for unseen words. Our task further differs from traditional paradigm completion (e.g., Dreyer and Eisner, 2011; Ahlberg et al., 2015) in that no seed paradigms are observed. Thus, no information is being provided regarding the paradigm size, inflectional features, or relationships between lemmas and inflected forms. Other recent work (Nicolai and Yarowsky, 2019; Nicolai et al., 2020) learned fine-grained morphosyntactic tools from the Bible, though they leveraged supervision projected from higher-resource languages (Yarowsky et al., 2001; T¨ackstr¨om et al., 2013). The task of unsupervised morphological paradigm completion concerns more than just segmentation: besides capturing how morphology is reflected in the word form, it also requires correctly clustering transformations into paradigm slots and, finally, generation of unobserved forms. Past shared tasks. This task extends a tradition of SIGMORPHON shared tasks concentrating on inflectional morphology. The first such task (Cotterell et al., 2016) encouraged participants to create inflectional tools in a typologically diverse group of 10 languages. The task was fully-super"
2020.sigmorphon-1.8,K17-2001,0,0.113463,"eudo training files for morphological inflection. Our inflection model then learns from these and, subsequently, generates all missing forms. 2 ing (Devlin et al., 2019). There has been very little work on transformers for morphological inflection, with, to the best of our knowledge, Erdmann et al. (2020) being the only published paper. However, the widespread success of transformers in NLP leads us to believe that a transformer model could perform well on morphological inflection. Pointer-generators. In addition to the transformer, the architecture of our model is also inspired by See et al. (2017), who used a pointergenerator network for abstractive summarization. Their model could choose between generating a new element and copying an element from the input directly to the output. This copying of words from the source text via pointing (Vinyals et al., 2015), improved the handling of out-of-vocabulary words. Copy mechanisms have also been used for other tasks, including morphological inflection (Sharma et al., 2018). Transformers with copy mechanisms have been used for word-level tasks (Zhao et al., 2019), but, as far as we know, never before on the character level. Related Work SIGMO"
2020.sigmorphon-1.8,W16-2002,0,0.0617818,"llow easy copying of input characters. Our best performing system for Task 0 is placed 6th out of 23 systems. We further use our inflection systems as subcomponents of approaches for Task 2. Our best performing system for Task 2 is the 2nd best out of 7 submissions. 1 Features V;PST V;3;SG;PRS Inflected form hugged seels Figure 1: Morphological inflection examples in English. A lemma and features are mapped to an inflected form. erating the indicated inflected form, cf. Figure 1. Morphological inflection is a useful tool for many natural language processing tasks (Seeker and C¸etinoglu, 2015; Cotterell et al., 2016b), especially in morphologically rich languages where handling inflected forms can reduce data sparsity (Minkov et al., 2007). The SIGMORPHON 2020 Shared Task consists of three separate tasks. We participate in Task 0 on typologically diverse morphological inflection (Vylomova et al., 2020) and Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). Task 0 consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. For this task, we implement a pointergenerator transformer model, based on the vanilla tra"
2020.sigmorphon-1.8,P16-1156,0,0.0499543,"Missing"
2020.sigmorphon-1.8,N19-1423,0,0.176741,"prefixation rules collected from the original dataset. For Task 2, participants are given raw text and a source file with lemmas. The objective is to generate the complete paradigms for all lemmas. Our systems for this task consist of a combination of the official baseline system (Jin et al., 2020) and our systems for Task 0. The baseline system finds inflected forms in the text, decides on the number of inflected forms per lemma, and produces pseudo training files for morphological inflection. Our inflection model then learns from these and, subsequently, generates all missing forms. 2 ing (Devlin et al., 2019). There has been very little work on transformers for morphological inflection, with, to the best of our knowledge, Erdmann et al. (2020) being the only published paper. However, the widespread success of transformers in NLP leads us to believe that a transformer model could perform well on morphological inflection. Pointer-generators. In addition to the transformer, the architecture of our model is also inspired by See et al. (2017), who used a pointergenerator network for abstractive summarization. Their model could choose between generating a new element and copying an element from the inpu"
2020.sigmorphon-1.8,D19-1091,0,0.0327319,"Missing"
2020.sigmorphon-1.8,2020.acl-main.695,0,0.0193137,"bjective is to generate the complete paradigms for all lemmas. Our systems for this task consist of a combination of the official baseline system (Jin et al., 2020) and our systems for Task 0. The baseline system finds inflected forms in the text, decides on the number of inflected forms per lemma, and produces pseudo training files for morphological inflection. Our inflection model then learns from these and, subsequently, generates all missing forms. 2 ing (Devlin et al., 2019). There has been very little work on transformers for morphological inflection, with, to the best of our knowledge, Erdmann et al. (2020) being the only published paper. However, the widespread success of transformers in NLP leads us to believe that a transformer model could perform well on morphological inflection. Pointer-generators. In addition to the transformer, the architecture of our model is also inspired by See et al. (2017), who used a pointergenerator network for abstractive summarization. Their model could choose between generating a new element and copying an element from the input directly to the output. This copying of words from the source text via pointing (Vinyals et al., 2015), improved the handling of out-of"
2020.sigmorphon-1.8,2020.sigmorphon-1.2,0,0.0894477,"Missing"
2020.sigmorphon-1.8,K18-3001,1,0.777885,"Missing"
2020.sigmorphon-1.8,2020.acl-main.598,1,0.915377,"ion and morphological reinflection, i.e., the task of generating inflected forms from forms different from the lemma. For languages with small training sets, we also perform hallucination pretraining (Anastasopoulos and Neubig, 2019), where we generate pseudo training instances for the task, based on suffixation and prefixation rules collected from the original dataset. For Task 2, participants are given raw text and a source file with lemmas. The objective is to generate the complete paradigms for all lemmas. Our systems for this task consist of a combination of the official baseline system (Jin et al., 2020) and our systems for Task 0. The baseline system finds inflected forms in the text, decides on the number of inflected forms per lemma, and produces pseudo training files for morphological inflection. Our inflection model then learns from these and, subsequently, generates all missing forms. 2 ing (Devlin et al., 2019). There has been very little work on transformers for morphological inflection, with, to the best of our knowledge, Erdmann et al. (2020) being the only published paper. However, the widespread success of transformers in NLP leads us to believe that a transformer model could perf"
2020.sigmorphon-1.8,P17-1182,1,0.856249,"Missing"
2020.sigmorphon-1.8,K18-3013,0,0.0338101,"Missing"
2020.sigmorphon-1.8,2020.sigmorphon-1.3,1,0.893864,". A lemma and features are mapped to an inflected form. erating the indicated inflected form, cf. Figure 1. Morphological inflection is a useful tool for many natural language processing tasks (Seeker and C¸etinoglu, 2015; Cotterell et al., 2016b), especially in morphologically rich languages where handling inflected forms can reduce data sparsity (Minkov et al., 2007). The SIGMORPHON 2020 Shared Task consists of three separate tasks. We participate in Task 0 on typologically diverse morphological inflection (Vylomova et al., 2020) and Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). Task 0 consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. For this task, we implement a pointergenerator transformer model, based on the vanilla transformer model (Vaswani et al., 2017) and the pointer-generator model (See et al., 2017). After adding a copy mechanism to the transformer, it produces a final probability distribution as a combination of generating elements from its output vocabulary and copying elements – characters in our case – from the input. As most inflected forms derive their characters from the"
2020.sigmorphon-1.8,W16-2010,1,0.848494,"Missing"
2020.sigmorphon-1.8,P16-2090,1,0.730034,"Missing"
2020.sigmorphon-1.8,K18-3008,0,0.257908,"(Bahdanau et al., 2015) and achieved the best result in the SIGMORPHON 2016 shared task (Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). Due to the often monotonic alignment between input and output, Aharoni and Goldberg (2017) proposed a model with hard monotonic attention. Based on this, Makarov et al. (2017) implemented a neural state-transition system which also used hard monotonic attention and achieved the best results for Task 1 of the SIGMORPHON 2017 shared task. In 2018, the best results were achieved by a revised version of the neural transducer, trained with imitation learning (Makarov and Clematide, 2018). That model learned an alignment instead of maximizing the likelihood of gold action sequences given by a separate aligner. 3 SIGMORPHON 2020 Shared Task The SIGMORPHON 2020 Shared Task is composed of three tasks: Task 0 on typologically diverse morphological inflection (Vylomova et al., 2020), Task 1 on multilingual grapheme-tophoneme conversion (Gorman et al., 2020), and Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). We submit systems to Tasks 0 and 2. 3.1 Task 0: Typologically Diverse Morphological Inflection SIGMORPHON 2020 Task 0 focuses on morphological in"
2020.sigmorphon-1.8,P19-1148,0,0.0371178,"Missing"
2020.sigmorphon-1.8,P07-1017,0,0.0724814,"inflection systems as subcomponents of approaches for Task 2. Our best performing system for Task 2 is the 2nd best out of 7 submissions. 1 Features V;PST V;3;SG;PRS Inflected form hugged seels Figure 1: Morphological inflection examples in English. A lemma and features are mapped to an inflected form. erating the indicated inflected form, cf. Figure 1. Morphological inflection is a useful tool for many natural language processing tasks (Seeker and C¸etinoglu, 2015; Cotterell et al., 2016b), especially in morphologically rich languages where handling inflected forms can reduce data sparsity (Minkov et al., 2007). The SIGMORPHON 2020 Shared Task consists of three separate tasks. We participate in Task 0 on typologically diverse morphological inflection (Vylomova et al., 2020) and Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). Task 0 consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. For this task, we implement a pointergenerator transformer model, based on the vanilla transformer model (Vaswani et al., 2017) and the pointer-generator model (See et al., 2017). After adding a copy mechanism to the"
2020.sigmorphon-1.8,N19-1014,0,0.0241094,"In addition to the transformer, the architecture of our model is also inspired by See et al. (2017), who used a pointergenerator network for abstractive summarization. Their model could choose between generating a new element and copying an element from the input directly to the output. This copying of words from the source text via pointing (Vinyals et al., 2015), improved the handling of out-of-vocabulary words. Copy mechanisms have also been used for other tasks, including morphological inflection (Sharma et al., 2018). Transformers with copy mechanisms have been used for word-level tasks (Zhao et al., 2019), but, as far as we know, never before on the character level. Related Work SIGMORPHON and CoNLL–SIGMORPHON shared tasks. In recent years, the SIGMORPHON and CoNLL–SIGMORPHON shared tasks have promoted research on computational morphology, with a strong focus on morphological inflection. Research related to those shared tasks includes Kann and Sch¨utze (2016b), who used an LSTM (Hochreiter and Schmidhuber, 1997) sequence-to-sequence model with soft attention (Bahdanau et al., 2015) and achieved the best result in the SIGMORPHON 2016 shared task (Kann and Sch¨utze, 2016a; Cotterell et al., 2016"
2020.sigmorphon-1.8,P17-1099,0,0.248739,"ed forms can reduce data sparsity (Minkov et al., 2007). The SIGMORPHON 2020 Shared Task consists of three separate tasks. We participate in Task 0 on typologically diverse morphological inflection (Vylomova et al., 2020) and Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). Task 0 consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. For this task, we implement a pointergenerator transformer model, based on the vanilla transformer model (Vaswani et al., 2017) and the pointer-generator model (See et al., 2017). After adding a copy mechanism to the transformer, it produces a final probability distribution as a combination of generating elements from its output vocabulary and copying elements – characters in our case – from the input. As most inflected forms derive their characters from the source lemma, the use of a mechanism for copying characters directly from the lemma has proven to be effective for morphological inflection generation, especially in the low resource setting (Aharoni and Goldberg, 2017; Makarov et al., 2017). For our submissions, we further increase the size of all training sets b"
2020.sigmorphon-1.8,Q15-1026,0,0.0250191,"ormer model to allow easy copying of input characters. Our best performing system for Task 0 is placed 6th out of 23 systems. We further use our inflection systems as subcomponents of approaches for Task 2. Our best performing system for Task 2 is the 2nd best out of 7 submissions. 1 Features V;PST V;3;SG;PRS Inflected form hugged seels Figure 1: Morphological inflection examples in English. A lemma and features are mapped to an inflected form. erating the indicated inflected form, cf. Figure 1. Morphological inflection is a useful tool for many natural language processing tasks (Seeker and C¸etinoglu, 2015; Cotterell et al., 2016b), especially in morphologically rich languages where handling inflected forms can reduce data sparsity (Minkov et al., 2007). The SIGMORPHON 2020 Shared Task consists of three separate tasks. We participate in Task 0 on typologically diverse morphological inflection (Vylomova et al., 2020) and Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). Task 0 consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. For this task, we implement a pointergenerator transformer model,"
2020.sigmorphon-1.9,N15-1107,0,0.184209,"sed morphological paradigm completion. Related Work Unsupervised methods have shown to be effective for morphological surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentowski (2000) worked on a task which was similar to unsupervised morphological paradigm completion, but required additional knowledge (e.g., a list of morphemes). Dreyer and Eisner (2011) used a set of seed paradigms to train a paradigm completion model. Ahlberg et al. (2015) and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar to this shared task. Learning to generate morphological paradigms 100 3 System Description In this section, we introduce our pipeline system for unsupervised morphological paradigm completion. First, we describe the baseline system, since we rely on some of its components. Then, we describe our morphological inflection models. 3.1 The Shared Task Baseline For the initial steps of our pipeline, we employ the first three components of the baseline ("
2020.sigmorphon-1.9,K18-3001,1,0.0858144,"Missing"
2020.sigmorphon-1.9,K17-2001,0,0.108439,"Missing"
2020.sigmorphon-1.9,P03-1036,0,0.681285,"has further gained popularity through previous SIGMORPHON and CoNLL–SIGMORPHON shared tasks on the topic (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019). The systems proposed for these shared tasks have a special relevance for our work, as we investigate the performance of morphological inflection components based on Kann and Sch¨utze (2016a,b) and Sharma et al. (2018) within a pipeline for unsupervised morphological paradigm completion. Related Work Unsupervised methods have shown to be effective for morphological surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentowski (2000) worked on a task which was similar to unsupervised morphological paradigm completion, but required additional knowledge (e.g., a list of morphemes). Dreyer and Eisner (2011) used a set of seed paradigms to train a paradigm completion model. Ahlberg et al. (2015) and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar to this shared task. Learni"
2020.sigmorphon-1.9,D11-1057,0,0.348802,"ased on Kann and Sch¨utze (2016a,b) and Sharma et al. (2018) within a pipeline for unsupervised morphological paradigm completion. Related Work Unsupervised methods have shown to be effective for morphological surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentowski (2000) worked on a task which was similar to unsupervised morphological paradigm completion, but required additional knowledge (e.g., a list of morphemes). Dreyer and Eisner (2011) used a set of seed paradigms to train a paradigm completion model. Ahlberg et al. (2015) and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar to this shared task. Learning to generate morphological paradigms 100 3 System Description In this section, we introduce our pipeline system for unsupervised morphological paradigm completion. First, we describe the baseline system, since we rely on some of its components. Then, we describe our morphological inflection models. 3.1 The Shared Task Baseline For"
2020.sigmorphon-1.9,N13-1138,0,0.020169,"challenging task: no submitted system outperforms the baselines. 2 Language Training Development Test Basque 85 16 499 Bulgarian 1609 441 2874 English 343 83 302 Finnish 2306 522 1789 German 3940 999 667 Kannada 832 211 2854 Navajo 17 4 279 Spanish 1940 494 2506 Turkish 3095 787 8502 Table 1: Number of instances retrieved by steps 1 to 3 in our pipeline, which are used for training and development of our inflection generation components. The test set contains the lemma and paradigm slot for forms that need to be generated. in a fully supervised way is the more common approach. Methods include Durrett and DeNero (2013), Nicolai et al. (2015), and Kann and Sch¨utze (2018). Supervised morphological inflection has further gained popularity through previous SIGMORPHON and CoNLL–SIGMORPHON shared tasks on the topic (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019). The systems proposed for these shared tasks have a special relevance for our work, as we investigate the performance of morphological inflection components based on Kann and Sch¨utze (2016a,b) and Sharma et al. (2018) within a pipeline for unsupervised morphological paradigm completion. Related Work Unsupervised methods have shown to be effe"
2020.sigmorphon-1.9,2020.acl-main.695,0,0.0167846,"ogical surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentowski (2000) worked on a task which was similar to unsupervised morphological paradigm completion, but required additional knowledge (e.g., a list of morphemes). Dreyer and Eisner (2011) used a set of seed paradigms to train a paradigm completion model. Ahlberg et al. (2015) and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar to this shared task. Learning to generate morphological paradigms 100 3 System Description In this section, we introduce our pipeline system for unsupervised morphological paradigm completion. First, we describe the baseline system, since we rely on some of its components. Then, we describe our morphological inflection models. 3.1 The Shared Task Baseline For the initial steps of our pipeline, we employ the first three components of the baseline (Jin et al., 2020), cf. Figure 2, which we describe in this subsection. We use the official implementation.2 Ret"
2020.sigmorphon-1.9,J01-2001,0,0.733191,"rvised morphological inflection has further gained popularity through previous SIGMORPHON and CoNLL–SIGMORPHON shared tasks on the topic (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019). The systems proposed for these shared tasks have a special relevance for our work, as we investigate the performance of morphological inflection components based on Kann and Sch¨utze (2016a,b) and Sharma et al. (2018) within a pipeline for unsupervised morphological paradigm completion. Related Work Unsupervised methods have shown to be effective for morphological surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentowski (2000) worked on a task which was similar to unsupervised morphological paradigm completion, but required additional knowledge (e.g., a list of morphemes). Dreyer and Eisner (2011) used a set of seed paradigms to train a paradigm completion model. Ahlberg et al. (2015) and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar"
2020.sigmorphon-1.9,E14-1060,0,0.0194278,"completion. Related Work Unsupervised methods have shown to be effective for morphological surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentowski (2000) worked on a task which was similar to unsupervised morphological paradigm completion, but required additional knowledge (e.g., a list of morphemes). Dreyer and Eisner (2011) used a set of seed paradigms to train a paradigm completion model. Ahlberg et al. (2015) and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar to this shared task. Learning to generate morphological paradigms 100 3 System Description In this section, we introduce our pipeline system for unsupervised morphological paradigm completion. First, we describe the baseline system, since we rely on some of its components. Then, we describe our morphological inflection models. 3.1 The Shared Task Baseline For the initial steps of our pipeline, we employ the first three components of the baseline (Jin et al., 2020), cf. Fi"
2020.sigmorphon-1.9,2020.acl-main.598,1,0.858366,"and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar to this shared task. Learning to generate morphological paradigms 100 3 System Description In this section, we introduce our pipeline system for unsupervised morphological paradigm completion. First, we describe the baseline system, since we rely on some of its components. Then, we describe our morphological inflection models. 3.1 The Shared Task Baseline For the initial steps of our pipeline, we employ the first three components of the baseline (Jin et al., 2020), cf. Figure 2, which we describe in this subsection. We use the official implementation.2 Retrieval of relevant edit trees. This component (cf. Figure 2.1) identifies words in the monolingual corpus that could belong to a given lemma’s paradigm by computing the longest common substring between the lemma and all words. Then, the 2 https://github.com/cai-lw/ morpho-baseline Figure 2: The baseline system. This paper experiments with modifying the generation module. All components are described in §3.1. transformation from a lemma to each word potentially from its paradigm is represented by edit"
2020.sigmorphon-1.9,2020.sigmorphon-1.3,1,0.755428,"Missing"
2020.sigmorphon-1.9,P16-2090,1,0.942338,"Missing"
2020.sigmorphon-1.9,D18-1363,1,0.845969,"Missing"
2020.sigmorphon-1.9,L18-1293,0,0.118028,"Missing"
2020.sigmorphon-1.9,D18-1314,0,0.174495,"se elements, similar potential slots are merged until the final paradigm size for a language is being determined. Generation. Now, that the system has a set of lemmas and corresponding potential inflected forms, the baseline employs a morphological inflection component, which learns to generate inflections from lemmas and a slot indicator, and generates missing forms (cf. Figure 2.4). We experiment with substituting this final component. In the remainder of this paper, we will refer to the original baselines with the non-neural system from Cotterell et al. (2017) and the inflection model from Makarov and Clematide (2018) as BL-1 and BL-2, respectively. LSTM Encoder-Decoder Pointer-Generator Network For IMS-CUB2, we use a pointer-generator network (See et al., 2017).3 We expect this system to perform better than IMS-CUB1, given the pointergenerator’s better performance on morphological inflection in the low-resource setting (Sharma et al., 2018). A pointer-generator network is a hybrid between an attention-based sequence-to-sequence model (Bahdanau et al., 2015) and a pointer network (Vinyals et al., 2015). The standard pointer-generator network consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 199"
2020.sigmorphon-1.9,W19-4226,0,0.0449885,"2506 Turkish 3095 787 8502 Table 1: Number of instances retrieved by steps 1 to 3 in our pipeline, which are used for training and development of our inflection generation components. The test set contains the lemma and paradigm slot for forms that need to be generated. in a fully supervised way is the more common approach. Methods include Durrett and DeNero (2013), Nicolai et al. (2015), and Kann and Sch¨utze (2018). Supervised morphological inflection has further gained popularity through previous SIGMORPHON and CoNLL–SIGMORPHON shared tasks on the topic (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019). The systems proposed for these shared tasks have a special relevance for our work, as we investigate the performance of morphological inflection components based on Kann and Sch¨utze (2016a,b) and Sharma et al. (2018) within a pipeline for unsupervised morphological paradigm completion. Related Work Unsupervised methods have shown to be effective for morphological surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentows"
2020.sigmorphon-1.9,2020.lrec-1.352,0,0.161014,"Missing"
2020.sigmorphon-1.9,N15-1093,0,0.0166533,"ted system outperforms the baselines. 2 Language Training Development Test Basque 85 16 499 Bulgarian 1609 441 2874 English 343 83 302 Finnish 2306 522 1789 German 3940 999 667 Kannada 832 211 2854 Navajo 17 4 279 Spanish 1940 494 2506 Turkish 3095 787 8502 Table 1: Number of instances retrieved by steps 1 to 3 in our pipeline, which are used for training and development of our inflection generation components. The test set contains the lemma and paradigm slot for forms that need to be generated. in a fully supervised way is the more common approach. Methods include Durrett and DeNero (2013), Nicolai et al. (2015), and Kann and Sch¨utze (2018). Supervised morphological inflection has further gained popularity through previous SIGMORPHON and CoNLL–SIGMORPHON shared tasks on the topic (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019). The systems proposed for these shared tasks have a special relevance for our work, as we investigate the performance of morphological inflection components based on Kann and Sch¨utze (2016a,b) and Sharma et al. (2018) within a pipeline for unsupervised morphological paradigm completion. Related Work Unsupervised methods have shown to be effective for morphological"
2020.sigmorphon-1.9,N09-1024,0,0.0301412,"previous SIGMORPHON and CoNLL–SIGMORPHON shared tasks on the topic (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019). The systems proposed for these shared tasks have a special relevance for our work, as we investigate the performance of morphological inflection components based on Kann and Sch¨utze (2016a,b) and Sharma et al. (2018) within a pipeline for unsupervised morphological paradigm completion. Related Work Unsupervised methods have shown to be effective for morphological surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentowski (2000) worked on a task which was similar to unsupervised morphological paradigm completion, but required additional knowledge (e.g., a list of morphemes). Dreyer and Eisner (2011) used a set of seed paradigms to train a paradigm completion model. Ahlberg et al. (2015) and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar to this shared task. Learning to generate morphological paradigms 100 3"
2020.sigmorphon-1.9,P17-1099,0,0.0287296,"f lemmas and corresponding potential inflected forms, the baseline employs a morphological inflection component, which learns to generate inflections from lemmas and a slot indicator, and generates missing forms (cf. Figure 2.4). We experiment with substituting this final component. In the remainder of this paper, we will refer to the original baselines with the non-neural system from Cotterell et al. (2017) and the inflection model from Makarov and Clematide (2018) as BL-1 and BL-2, respectively. LSTM Encoder-Decoder Pointer-Generator Network For IMS-CUB2, we use a pointer-generator network (See et al., 2017).3 We expect this system to perform better than IMS-CUB1, given the pointergenerator’s better performance on morphological inflection in the low-resource setting (Sharma et al., 2018). A pointer-generator network is a hybrid between an attention-based sequence-to-sequence model (Bahdanau et al., 2015) and a pointer network (Vinyals et al., 2015). The standard pointer-generator network consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder and a unidirectional LSTM decoder with a copy mechanism. Here, we follow (Sharma et al., 2018) and use two separate encoders: one for th"
2020.sigmorphon-1.9,P00-1027,0,0.337626,"McCarthy et al., 2019). The systems proposed for these shared tasks have a special relevance for our work, as we investigate the performance of morphological inflection components based on Kann and Sch¨utze (2016a,b) and Sharma et al. (2018) within a pipeline for unsupervised morphological paradigm completion. Related Work Unsupervised methods have shown to be effective for morphological surface segmentation. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz, 2003; Creutz and Lagus, 2007; Poon et al., 2009) are two unsupervised systems for the task. In the realm of morphological generation, Yarowsky and Wicentowski (2000) worked on a task which was similar to unsupervised morphological paradigm completion, but required additional knowledge (e.g., a list of morphemes). Dreyer and Eisner (2011) used a set of seed paradigms to train a paradigm completion model. Ahlberg et al. (2015) and Hulden et al. (2014) also relied on information about the paradigms in the language. Erdmann et al. (2020) proposed a system for a task similar to this shared task. Learning to generate morphological paradigms 100 3 System Description In this section, we introduce our pipeline system for unsupervised morphological paradigm complet"
2021.acl-long.351,2020.emnlp-main.363,0,0.037632,"Missing"
2021.acl-long.351,D17-1269,0,0.0605367,"Missing"
2021.acl-long.351,2020.lrec-1.352,0,0.0255054,"Missing"
2021.acl-long.351,2020.lrec-1.497,0,0.0422045,"Missing"
2021.acl-long.351,P17-1178,0,0.0678707,"Missing"
2021.acl-long.351,2020.emnlp-demos.7,0,0.0708109,"Missing"
2021.acl-long.351,2020.emnlp-main.617,0,0.0568223,"Missing"
2021.acl-long.351,P19-1493,0,0.0267647,"es are concatenated such that the model can rely on subwords in both languages for prediction. Finally, XLM-R is an improved version of XLM. Notable differences include the larger vocabulary of 250k subwords created using SentencePiece tokenization (Kudo and Richardson, 2018) and the training data, which is taken from CommonCrawl and is considerably more than for mBERT and XLM. XLM-R relies solely on MLM for pretraining and achieves stateof-the-art results on multiple benchmarks (Conneau et al., 2020). We therefore focus solely on XLM-R in our experiments. Downstream Performance of PMMs While Pires et al. (2019) and Wu and Dredze (2019) show the strong zero-shot performance of mBERT, Wu and Dredze (2020) shine light on the difference in performance between well and poorly represented languages after finetuning on target-task data. Muller et al. (2020) observe varying zero-shot performance of mBERT on different languages not present in its pretraining data. They group them into ‘easy’ languages, on which mBERT performs well without any modification, ‘medium’ languages, on which mBERT performs well after additional pretraining on monolingual data, and ‘hard’ languages, on which mBERT’s performs poorly"
2021.acl-long.351,2020.acl-demos.14,0,0.0222376,"gging, in a similar setting to our work, Eskander et al. (2020) achieve strong zero-shot results by using unsupervised projection (Yarowsky et al., 2001) with aligned Bibles. Recent work for cross-lingual NER includes Mayhew et al. (2017) who use dictionary translations to create target-language training data, as well as Xie et al. (2018) who use a bilingual dictionary in addition to self-attention. Bharadwaj et al. (2016) use phoneme conversion to aid cross-lingual NER in a zero-shot setting. More recently, Bari et al. (2020) propose a model only using monolingual data for each language, and Qi et al. (2020) propose a language-agnostic toolkit supporting NER for 66 languages. In contrast to these works, we focus on the improvements offered 4556 by adaptation methods for pretrained models for general tasks. 2.2 Pretrained Multilingual Models PMMs can be seen as the natural extension of multilingual embeddings to pretrained transformerbased models. mBERT was the first PMM, covering the 104 languages with the largest Wikipedias. It uses a 110k byte-pair encoding (BPE) vocabulary (Sennrich et al., 2016) and is pretrained on both a next sentence prediction and a masked language modeling (MLM) objectiv"
2021.acl-long.351,P19-1015,0,0.0383438,"Missing"
2021.acl-long.351,H01-1035,0,0.140765,"r method for cross-lingual transfer involves multilingual embeddings, where languages are jointly learned as opposed to being aligned (Ammar et al., 2016; Artetxe and Schwenk, 2019). For a more in-depth look at cross-lingual word embeddings, we refer the reader to Ruder et al. (2019). While the above works deal with generally improving cross-lingual representations, task-specific cross-lingual systems often show strong performance in a zero-shot setting. For POS tagging, in a similar setting to our work, Eskander et al. (2020) achieve strong zero-shot results by using unsupervised projection (Yarowsky et al., 2001) with aligned Bibles. Recent work for cross-lingual NER includes Mayhew et al. (2017) who use dictionary translations to create target-language training data, as well as Xie et al. (2018) who use a bilingual dictionary in addition to self-attention. Bharadwaj et al. (2016) use phoneme conversion to aid cross-lingual NER in a zero-shot setting. More recently, Bari et al. (2020) propose a model only using monolingual data for each language, and Qi et al. (2020) propose a language-agnostic toolkit supporting NER for 66 languages. In contrast to these works, we focus on the improvements offered 45"
2021.acl-long.351,P16-1162,0,0.0120328,"ng. More recently, Bari et al. (2020) propose a model only using monolingual data for each language, and Qi et al. (2020) propose a language-agnostic toolkit supporting NER for 66 languages. In contrast to these works, we focus on the improvements offered 4556 by adaptation methods for pretrained models for general tasks. 2.2 Pretrained Multilingual Models PMMs can be seen as the natural extension of multilingual embeddings to pretrained transformerbased models. mBERT was the first PMM, covering the 104 languages with the largest Wikipedias. It uses a 110k byte-pair encoding (BPE) vocabulary (Sennrich et al., 2016) and is pretrained on both a next sentence prediction and a masked language modeling (MLM) objective. Languages with smaller Wikipedias are upsampled and highly represented languages are downsampled. XLM is a PMM trained on 15 languages. XLM similarly trains on Wikipedia data, using a BPE vocabulary with 95k subwords and up- and downsamples languages similarly to mBERT. XLM also introduces translation language modeling (TLM), a supervised pretraining objective, where tokens are masked as for MLM, but parallel sentences are concatenated such that the model can rely on subwords in both languages"
2021.acl-long.351,P19-1355,0,0.0405557,"Missing"
2021.acl-long.351,2020.findings-emnlp.240,0,0.242353,"hich mBERT’s performs poorly even after modification. They additionally note the importance of script, finding that transliterating into Latin offers improvements for some languages. As transliteration involves language specific tools, we consider it out of scope for this work, and leave further investigation in how to best utilize transliteration for future work. Lauscher et al. (2020) focus on PMM finetuning, and find that for unseen languages, gathering labeled data for few-shot learning may be more effective than gathering large amounts of unlabeled data. Additionally, Chau et al. (2020), Wang et al. (2020), and Pfeiffer et al. (2020b) present the adaptation methods whose performance we investigate here in a setting where only the Bible is available. We give a general overview of these methods in the remainder of this section, before describing their application in our experiments in Section 3. 2.3 Adaptation Methods Continued Pretraining In a monolingual setting, continued pretraining of a language representation model on an MLM objective has shown to help downstream performance on tasks involving text from a domain distant from the pretraining corpora (Gururangan et al., 2020). In a multilingu"
2021.acl-long.351,2020.emnlp-demos.6,0,0.087514,"Missing"
2021.acl-long.351,D19-1077,0,0.0208412,"that the model can rely on subwords in both languages for prediction. Finally, XLM-R is an improved version of XLM. Notable differences include the larger vocabulary of 250k subwords created using SentencePiece tokenization (Kudo and Richardson, 2018) and the training data, which is taken from CommonCrawl and is considerably more than for mBERT and XLM. XLM-R relies solely on MLM for pretraining and achieves stateof-the-art results on multiple benchmarks (Conneau et al., 2020). We therefore focus solely on XLM-R in our experiments. Downstream Performance of PMMs While Pires et al. (2019) and Wu and Dredze (2019) show the strong zero-shot performance of mBERT, Wu and Dredze (2020) shine light on the difference in performance between well and poorly represented languages after finetuning on target-task data. Muller et al. (2020) observe varying zero-shot performance of mBERT on different languages not present in its pretraining data. They group them into ‘easy’ languages, on which mBERT performs well without any modification, ‘medium’ languages, on which mBERT performs well after additional pretraining on monolingual data, and ‘hard’ languages, on which mBERT’s performs poorly even after modification."
2021.acl-long.351,2020.repl4nlp-1.16,0,0.0192268,". Finally, XLM-R is an improved version of XLM. Notable differences include the larger vocabulary of 250k subwords created using SentencePiece tokenization (Kudo and Richardson, 2018) and the training data, which is taken from CommonCrawl and is considerably more than for mBERT and XLM. XLM-R relies solely on MLM for pretraining and achieves stateof-the-art results on multiple benchmarks (Conneau et al., 2020). We therefore focus solely on XLM-R in our experiments. Downstream Performance of PMMs While Pires et al. (2019) and Wu and Dredze (2019) show the strong zero-shot performance of mBERT, Wu and Dredze (2020) shine light on the difference in performance between well and poorly represented languages after finetuning on target-task data. Muller et al. (2020) observe varying zero-shot performance of mBERT on different languages not present in its pretraining data. They group them into ‘easy’ languages, on which mBERT performs well without any modification, ‘medium’ languages, on which mBERT performs well after additional pretraining on monolingual data, and ‘hard’ languages, on which mBERT’s performs poorly even after modification. They additionally note the importance of script, finding that transli"
2021.acl-long.351,D18-1034,0,0.0172939,"e in-depth look at cross-lingual word embeddings, we refer the reader to Ruder et al. (2019). While the above works deal with generally improving cross-lingual representations, task-specific cross-lingual systems often show strong performance in a zero-shot setting. For POS tagging, in a similar setting to our work, Eskander et al. (2020) achieve strong zero-shot results by using unsupervised projection (Yarowsky et al., 2001) with aligned Bibles. Recent work for cross-lingual NER includes Mayhew et al. (2017) who use dictionary translations to create target-language training data, as well as Xie et al. (2018) who use a bilingual dictionary in addition to self-attention. Bharadwaj et al. (2016) use phoneme conversion to aid cross-lingual NER in a zero-shot setting. More recently, Bari et al. (2020) propose a model only using monolingual data for each language, and Qi et al. (2020) propose a language-agnostic toolkit supporting NER for 66 languages. In contrast to these works, we focus on the improvements offered 4556 by adaptation methods for pretrained models for general tasks. 2.2 Pretrained Multilingual Models PMMs can be seen as the natural extension of multilingual embeddings to pretrained tra"
2021.acl-short.139,2020.emnlp-main.207,0,0.0191581,"). However, supervised methods still outperform their unsupervised and semi-supervised counterparts, which makes collecting training data for MT important. Prior work scrapes data from the web (Lai et al., 2020; Resnik and Smith, 2003), or uses movie subtitles (Zhang et al., 2014), religious texts (Resnik et al., 1999), or multilingual parliament proceedings (Koehn, 2005). However, those and similar resources are only available for a limited set of languages. A large amount of data for a diverse set of low-resource languages cannot be collected using these methods. For low-resource languages, Hasan et al. (2020) propose a method to convert noisy parallel documents into parallel sentences. Zhang et al. (2020) filter noisy sentence pairs from MT training data. The closest work to ours is Madaan et al. (2020). The authors collect (pseudo-)parallel sentences with images from the Flickr8k dataset (Hodosh et al., 2013) as a pivot, filtering to obtain images which are simplistic and do not contain culture-specific references. Since Flickr8k already 1 All data collected for our experiments is available at https://nala-cub.github.io/resources. contains 5 English captions per image, they select images whose ca"
2021.acl-short.139,2005.mtsummit-papers.11,0,0.263747,"n NLP (Devlin et al., 2019), several techniques for improving neural MT for low-resource languages have been proposed (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019; Lample et al., 2017; Lewis et al., 2019; Liu et al., 2020). However, supervised methods still outperform their unsupervised and semi-supervised counterparts, which makes collecting training data for MT important. Prior work scrapes data from the web (Lai et al., 2020; Resnik and Smith, 2003), or uses movie subtitles (Zhang et al., 2014), religious texts (Resnik et al., 1999), or multilingual parliament proceedings (Koehn, 2005). However, those and similar resources are only available for a limited set of languages. A large amount of data for a diverse set of low-resource languages cannot be collected using these methods. For low-resource languages, Hasan et al. (2020) propose a method to convert noisy parallel documents into parallel sentences. Zhang et al. (2020) filter noisy sentence pairs from MT training data. The closest work to ours is Madaan et al. (2020). The authors collect (pseudo-)parallel sentences with images from the Flickr8k dataset (Hodosh et al., 2013) as a pivot, filtering to obtain images which ar"
2021.acl-short.139,2020.acl-main.703,0,0.0219054,"Missing"
2021.acl-short.139,2020.tacl-1.47,0,0.0412983,"nd and expensive. Here, we present a data collection strategy for MT which, in contrast, is cheap and simple, as it does not require bilingual speakers. Based on the insight that humans pay specific attention to movements, we use graphics interchange formats (GIFs) as a pivot to collect parallel sentences from monolingual annotators. We use our strategy to collect data in Hindi, Tamil and English. As a baseline, we also collect data using images as a pivot. We perform an intrinsic evaluation by manually evaluating a subset of the sentence pairs and an extrinsic evaluation by finetuning mBART (Liu et al., 2020) on the collected data. We find that sentences collected via GIFs are indeed of higher quality. 1 Figure 1: Sentences written by English and Hindi annotators using GIFs or images as a pivot. and it is impossible to collect large MT corpora for a diverse set of languages using these methods. Professional translators, which are a straightforward alternative, are often rare or expensive. Introduction Machine translation (MT) – automatic translation of text from one natural language into another – provides access to information written in foreign languages and enables communication between speaker"
2021.acl-short.139,J03-3002,0,0.106522,"tion Machine translation (MT) – automatic translation of text from one natural language into another – provides access to information written in foreign languages and enables communication between speakers of different languages. However, developing high performing MT systems requires large amounts of training data in the form of parallel sentences – a resource which is often difficult and expensive to obtain, especially for languages less frequently studied in natural language processing (NLP), endangered languages, or dialects. For some languages, it is possible to scrape data from the web (Resnik and Smith, 2003), or to leverage existing translations, e.g., of movie subtitles (Zhang et al., 2014) or religious texts (Resnik et al., 1999). However, such sources of data are only available for a limited number of languages, In this paper, we propose a new data collection strategy which is cheap, simple, effective and, importantly, does not require professional translators or even bilingual speakers. It is based on two assumptions: (1) non-textual modalities can serve as a pivot for the annotation process (Madaan et al., 2020); and (2) annotators subconsciously pay increased attention to moving objects, si"
2021.acl-short.139,P16-1009,0,0.0230426,"l baseline, we compare to data collected in previous work (Madaan et al., 2020). We perform both intrinsic and extrinsic evaluations – by manually evaluating the collected sentences and by training MT systems on the collected data, respectively – and find that leveraging GIFs indeed results in parallel sentences of higher quality as compared to our baselines.1 2 Related Work In recent years, especially with the success of transfer learning (Wang et al., 2018) and pretraining in NLP (Devlin et al., 2019), several techniques for improving neural MT for low-resource languages have been proposed (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019; Lample et al., 2017; Lewis et al., 2019; Liu et al., 2020). However, supervised methods still outperform their unsupervised and semi-supervised counterparts, which makes collecting training data for MT important. Prior work scrapes data from the web (Lai et al., 2020; Resnik and Smith, 2003), or uses movie subtitles (Zhang et al., 2014), religious texts (Resnik et al., 1999), or multilingual parliament proceedings (Koehn, 2005). However, those and similar resources are only available for a limited set of languages. A large amount of data for a diverse s"
2021.acl-short.139,W18-5446,0,0.0316802,"Missing"
2021.acl-short.139,P19-1579,0,0.0172009,"previous work (Madaan et al., 2020). We perform both intrinsic and extrinsic evaluations – by manually evaluating the collected sentences and by training MT systems on the collected data, respectively – and find that leveraging GIFs indeed results in parallel sentences of higher quality as compared to our baselines.1 2 Related Work In recent years, especially with the success of transfer learning (Wang et al., 2018) and pretraining in NLP (Devlin et al., 2019), several techniques for improving neural MT for low-resource languages have been proposed (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019; Lample et al., 2017; Lewis et al., 2019; Liu et al., 2020). However, supervised methods still outperform their unsupervised and semi-supervised counterparts, which makes collecting training data for MT important. Prior work scrapes data from the web (Lai et al., 2020; Resnik and Smith, 2003), or uses movie subtitles (Zhang et al., 2014), religious texts (Resnik et al., 1999), or multilingual parliament proceedings (Koehn, 2005). However, those and similar resources are only available for a limited set of languages. A large amount of data for a diverse set of low-resource languages cannot be"
2021.acl-short.139,2020.acl-main.756,0,0.0210256,"s, which makes collecting training data for MT important. Prior work scrapes data from the web (Lai et al., 2020; Resnik and Smith, 2003), or uses movie subtitles (Zhang et al., 2014), religious texts (Resnik et al., 1999), or multilingual parliament proceedings (Koehn, 2005). However, those and similar resources are only available for a limited set of languages. A large amount of data for a diverse set of low-resource languages cannot be collected using these methods. For low-resource languages, Hasan et al. (2020) propose a method to convert noisy parallel documents into parallel sentences. Zhang et al. (2020) filter noisy sentence pairs from MT training data. The closest work to ours is Madaan et al. (2020). The authors collect (pseudo-)parallel sentences with images from the Flickr8k dataset (Hodosh et al., 2013) as a pivot, filtering to obtain images which are simplistic and do not contain culture-specific references. Since Flickr8k already 1 All data collected for our experiments is available at https://nala-cub.github.io/resources. contains 5 English captions per image, they select images whose captions are short and of high similarity to each other. Culture-specific images are manually discar"
2021.acl-short.139,zhang-etal-2014-dual,0,0.148395,"nto another – provides access to information written in foreign languages and enables communication between speakers of different languages. However, developing high performing MT systems requires large amounts of training data in the form of parallel sentences – a resource which is often difficult and expensive to obtain, especially for languages less frequently studied in natural language processing (NLP), endangered languages, or dialects. For some languages, it is possible to scrape data from the web (Resnik and Smith, 2003), or to leverage existing translations, e.g., of movie subtitles (Zhang et al., 2014) or religious texts (Resnik et al., 1999). However, such sources of data are only available for a limited number of languages, In this paper, we propose a new data collection strategy which is cheap, simple, effective and, importantly, does not require professional translators or even bilingual speakers. It is based on two assumptions: (1) non-textual modalities can serve as a pivot for the annotation process (Madaan et al., 2020); and (2) annotators subconsciously pay increased attention to moving objects, since humans are extremely good at detecting motion, a crucial skill for survival (Albr"
2021.acl-short.139,P02-1040,0,0.119512,"o focus on similar content. Images Finding images which are comparable to our GIFs is non-trivial. While we could compare our GIFs’ descriptions to image captions, we hypothesize that the similarity between the images obtained thereby and the GIFs would be too low for a clean comparison. Thus, we consider two alternatives: (1) using the first frame of all GIFs, and (2) using the middle frame of all GIFs. In a preliminary study, we obtain two Hindi one-sentence descriptions from two different annotators for both the first and the middle frame for a subset of 100 GIFs. We then compare the BLEU (Papineni et al., 2002) scores of all sentence pairs. We find that, on average, sentences for the middle frame have a BLEU score of 7.66 as compared to 4.58 for the first frame. Since a higher BLEU score indicates higher similarity and, thus, higher potential suitability as MT training data, we use the middle frames for the image-as-pivot condition in our final experiments. 1100 Rating Sentences from the GIF-as-Pivot Setting 1 A child flips on a trampoline. A girl enjoyed while playing. 3 A man in a hat is walking up the stairs holding a bottle of water. A man is walking with a plastic bottle. 5 A man is laughing wh"
2021.americasnlp-1.23,2021.americasnlp-1.30,0,0.035558,"ill now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 2017). We employed the hyperparameters proposed by Guzmán et al. (2019) for a low-resource scenario. We implemented the model using Fairseq (Ott et al., 2019). The implementation of the baseline can be found in the official shared task repository.4 4.2 University of British Columbia The team of the University of British Columbia (UBC-NLP; Billah-Nagoudi et al., 2021) participated for all ten language pairs and in both tracks. They used an encoder-decoder transformer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages have more available data than others, this dataset is unbalanced in favor of languages like Nahuatl, Guarani, and Quechua. The team also proposed a two-stage fine-tuning method: first fine-tuning on the entire da"
2021.americasnlp-1.23,2021.americasnlp-1.28,0,0.0389782,"Missing"
2021.americasnlp-1.23,2020.lrec-1.320,1,0.692133,"test datasets were translated to modern Nahuatl. In particular, the translations belong to Nahuatl Central/Nahuatl de la Huasteca (Hidalgo y San Luis Potosí) dialects. In order to be closer to the training corpus, an orthographic normalization was applied. A simple rule based approach was used, which was based on the most predictable orthographic changes between modern varieties and Classical Nahuatl. Spanish—Guarani Guarani is mostly spoken in Paraguay, Bolivia, Argentina and Brazil. It belongs to the Tupian language family (ISO gnw, gun, gug, gui, grn, nhd). The training corpus for Guarani (Chiruzzo et al., 2020) was collected from web sources (blogs and news articles) that contained a mix of dialects, from pure Guarani to more mixed Jopara which combines Guarani with Spanish neologisms. The development and test corpora, on the other hand, are in standard Paraguayan Guarani. Spanish—Bribri Bribri is a Chibchan language spoken in southern Costa Rica (ISO code bzd). The training set for Bribri was extracted from six sources (Feldman and Coto-Solano, 2020; Margery, 2005; Jara Murillo, 2018a; Constenla et al., 2004; Jara Murillo and García Segura, 2013; Jara Murillo, 2018b; Flores Solórzano, 2017), includ"
2021.americasnlp-1.23,D18-1269,0,0.0884352,"uage families: Aymaran, Arawak, Chibchan, Tupi-Guarani, UtoAztecan, Oto-Manguean, Quechuan, and Panoan. The ten language pairs included in the shared task are: Quechua–Spanish, Wixarika–Spanish, Shipibo-Konibo–Spanish, Asháninka–Spanish, Raramuri–Spanish, Nahuatl–Spanish, Otomí– Spanish, Aymara–Spanish, Guarani–Spanish, and Bribri–Spanish. For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018). For a complete description of this dataset we refer the reader to Ebrahimi et al. (2021). Together with the data, we also provided: a simple baseline based on the small transformer architecture (Vaswani et al., 2017) proposed together with the FLORES dataset (Guzmán et al., 2019); and a description of challenges and particular characteristics for all provided resources1 . We established two tracks: one where training models on the development set after hyperparameter tuning is 1 https://github.com/AmericasNLP/americasnlp2021/ blob/main/data/information_datasets.pdf 202 Proceedings of the Fir"
2021.americasnlp-1.23,P18-1128,0,0.020486,"8.4 8.3 ChrF 39.4 38.3 35.8 33.2 32.8 31.8 26.9 10.3 9.8 9.0 6.6 ChrF 39.9 38.0 29.7 28.6 16.3 15.5 12.4 ChrF 25.8 24.8 24.7 23.9 21.6 16.5 15.9 12.2 10.5 10.5 8.4 Table 3: Results of Track 1 (development set used for training) for all systems and language pairs. The results are ranked by the official metric of the shared task: ChrF. One team decided to send a anonymous submission (Anonym). Best results are shown in bold, and they are significantly better than the second place team (in each language-pair) according to the Wilcoxon signed-ranked test and Pitman’s permutation test with p&lt;0.05 (Dror et al., 2018). 208 ious high-resource languages, and then finetuned for each target language using the official provided data. 4.7 NRC-CNRC The team of the National Research Council Canada (NRC-CNRC; Knowles et al., 2021) submitted systems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used"
2021.americasnlp-1.23,2020.coling-main.351,1,0.754898,"oken in Paraguay, Bolivia, Argentina and Brazil. It belongs to the Tupian language family (ISO gnw, gun, gug, gui, grn, nhd). The training corpus for Guarani (Chiruzzo et al., 2020) was collected from web sources (blogs and news articles) that contained a mix of dialects, from pure Guarani to more mixed Jopara which combines Guarani with Spanish neologisms. The development and test corpora, on the other hand, are in standard Paraguayan Guarani. Spanish—Bribri Bribri is a Chibchan language spoken in southern Costa Rica (ISO code bzd). The training set for Bribri was extracted from six sources (Feldman and Coto-Solano, 2020; Margery, 2005; Jara Murillo, 2018a; Constenla et al., 2004; Jara Murillo and García Segura, 2013; Jara Murillo, 2018b; Flores Solórzano, 2017), including a dictionary, a grammar, two language learning textbooks, one storybook and the transcribed sentences from Spanish–Wixarika Wixarika (also known as 2 Huichol) with ISO code hch is spoken in Mexico ISO 639-3 for the Nahutal languages: and belongs to the Yuto-Aztecan linguistic family. nch, ncx, naz, nln, nhe, ngu, nhk, nhx, nhp, ncl, nhm, nhy, The training, development and test sets all belong nlv, ppl, nhz, npl, nhc, nhv, to the same dialec"
2021.americasnlp-1.23,galarreta-etal-2017-corpus,1,0.842998,"training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a bilingual teacher, and follows the most recent guidelines of the Minister of Education in Peru, however, the third source is an extraction of parallel sentences from an old dictionary. The development and test sets were created following the official convention as in the translated training sets. Spanish—Quechua Quechua is a family of languages spoken in Argentina, Bolivia, Colombia, Spanish—Asháninka Asháninka is an Ecuador, Peru, and Chile with many ISO codes for Arawakan language (ISO: cni) spoken"
2021.americasnlp-1.23,W19-6804,1,0.768784,"az jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a bilingual teacher, and follows the most recent guidelines of the Minister of Education in Peru, however, the third source is an extraction of parallel sentences from an old dictionary. The development and test sets were created following the official convention as in the translated training sets. Spanish—Quechua Quechua is a family of languages spoken in Argentina, Bolivia, Colombia, Spanish—Asháninka Asháninka is an Ecu"
2021.americasnlp-1.23,N15-2021,1,0.735842,"eceive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available parallel corpora (§3). Additionally, all participants were allowed to use other existing datasets or create their own resources for training in order to improve their systems. E"
2021.americasnlp-1.23,L16-1666,1,0.700987,"ng similarities and differences between training sets on the one hand and development and test sets on the other. The training data (Mager et al., 2018a) is a translation of the fairy tales of Hans Christian Andersen and contains word acquisitions and code-switching. Spanish–Nahuatl Nahuatl is a Yuto-Aztecan language spoken in Mexico and El Salvador, with a wide dialectal variation (around 30 variants). For each main dialect a specific ISO 639-3 code is available.2 There is a lack of consensus regarding the orthographic standard. This is very noticeable in the training data: the train corpus (Gutierrez-Vasques et al., 2016) has dialectal, domain, orthographic and diachronic variation (Nahuatl side). However, the majority of entries are closer to a Classical Nahuatl orthographic “standard”. The development and test datasets were translated to modern Nahuatl. In particular, the translations belong to Nahuatl Central/Nahuatl de la Huasteca (Hidalgo y San Luis Potosí) dialects. In order to be closer to the training corpus, an orthographic normalization was applied. A simple rule based approach was used, which was based on the most predictable orthographic changes between modern varieties and Classical Nahuatl. Spani"
2021.americasnlp-1.23,D19-1632,1,0.923833,"nish, Aymara–Spanish, Guarani–Spanish, and Bribri–Spanish. For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018). For a complete description of this dataset we refer the reader to Ebrahimi et al. (2021). Together with the data, we also provided: a simple baseline based on the small transformer architecture (Vaswani et al., 2017) proposed together with the FLORES dataset (Guzmán et al., 2019); and a description of challenges and particular characteristics for all provided resources1 . We established two tracks: one where training models on the development set after hyperparameter tuning is 1 https://github.com/AmericasNLP/americasnlp2021/ blob/main/data/information_datasets.pdf 202 Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas, pages 202–217 June 11, 2021. ©2021 Association for Computational Linguistics allowed (Track 1), and one where models cannot be trained directly on the development set (Track 2). Machine translation"
2021.americasnlp-1.23,2021.americasnlp-1.25,0,0.125338,"dded to the existing dataset. Then, a normalization process was done using existing tools, and the aligned data was further filtered. The quality of the data was also considered, and each dataset was assigned a weight depending on a noisiness estimation. The team used a transformer sequenceto-sequence model trained via two steps. For their main submission they first trained on data which 4 Otomí online corpus: https://tsunkua.elotl.mx/about/ 206 https://github.com/AmericasNLP/americasnlp2021 Team Langs. Sub. CoAStaL (Bollmann et al., 2021) 10 20 Helsinki (Vázquez et al., 2021) 10 50 NRC-CNRC (Knowles et al., 2021) 4 17 REPUcs (Moreno, 2021) Tamalli (Parida et al., 2021) 1 2 10 UBC-NLP (BillahNagoudi et al., 2021) UTokyo (Zheng et al., 2021) Anonymous Data Models Multilingual Pretrained PB-SMT, Constrained Random Strings Transformer NMT No No 42 Bible, JW300, OPUS, Wikipedia, New collected data Bible, OPUS, Constitutions, Normalization, Filtering, BackTranslation No external data, preoricessing, BPE Dropout. JW300, New dataset, Europarl - 8 29 Bible, Wikipedia 10 40 8 14 Monolingual from other languages. Data - Yes, all ST No languages + Spanish-English Transofrmer NMT Yes, languages 4- No Transformer N"
2021.americasnlp-1.23,P07-2045,0,0.0229964,"lcoxon signed-ranked test and Pitman’s permutation test with p&lt;0.05 (Dror et al., 2018). 208 ious high-resource languages, and then finetuned for each target language using the official provided data. 4.7 NRC-CNRC The team of the National Research Council Canada (NRC-CNRC; Knowles et al., 2021) submitted systems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used BPE for tokenization. The team experimented with multilingual models pretrained on either 3 or 4 languages, finding that the 4 language model achieved higher performance. Additionally the team trained a Translation Memory (Simard and Fujita, 2012) using half of the examples of the development set. Surprisingly, even given its small amount of training data, this system outperformed the team’s Track 2 submission for Rarámuri. 4.8 Tamalli The team Tamalli7 (Parida et al., 2021) participated in Track 1 for all 10 language pairs. T"
2021.americasnlp-1.23,D18-2012,0,0.0268314,"encoders using UD annotations could not get any meaningful results. 4.5 REPUcs the the Spanish–Quechua language pair in both tracks. The team collected external data from 3 different sources and analyzed the domain disparity between this training data and the development set. To solve the problem of domain mismatch, they decided to collect additional data that could be a better match for the target domain. The used data from a handbook (Iter and Ortiz-Cárdenas, 2019), a lexicon,5 and poems on the web (Duran, 2010).6 Their model is a transformer encoder-decoder architecture with SentencePiece (Kudo and Richardson, 2018) tokenization. Together with the existing parallel corpora, the new paired data was used for finetuning on top of a pretrained Spanish–English translation model. The team submitted two versions of their system: the first was only finetuned on JW300+ data, while the second one additionally leveraged the newly collected dataset. 4.6 UTokyo The team of the University of Tokyo (UTokyo; Zheng et al., 2021) submitted systems for all languages and both tracks. A multilingual pretrained encoder-decoder model (mBART; Liu et al., 2020) was used, implemented with the Fairseq toolkit (Ott et al., 2019). T"
2021.americasnlp-1.23,C18-1006,1,0.909443,"ms achieved 12.97 ChrF higher than baseline, when averaged across languages. 1 Introduction Many of the world’s languages, including languages native to the Americas, receive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available para"
2021.americasnlp-1.23,mayer-cysouw-2014-creating,0,0.0200209,"to the Valle del Mezquital dialect (ote). This was specially challenging for the translation task, since the development and test sets are from the Ñûhmû de Ixtenco, Tlaxcala, variant (otz), which also has its own orthographic system. This variant is especially endangered as less than 100 elders still speak it. 3.3 External Data Used by Participants In addition to the provided datasets, participants also used additional publicly available parallel data, monolingual corpora or newly collected data sets. The most common datasets were JW300 (Agi´c and Vuli´c, 2019) and the Bible’s New Testament (Mayer and Cysouw, 2014; Christodouloupoulos and Steedman, 2015; McCarthy et al., 2020). Besides those, GlobalVoices (Prokopidis et al., 2016) and datasets available at OPUS (Tiedemann, 2012) were added. New datasets were extracted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a"
2021.americasnlp-1.23,2020.lrec-1.352,0,0.0111319,"llenging for the translation task, since the development and test sets are from the Ñûhmû de Ixtenco, Tlaxcala, variant (otz), which also has its own orthographic system. This variant is especially endangered as less than 100 elders still speak it. 3.3 External Data Used by Participants In addition to the provided datasets, participants also used additional publicly available parallel data, monolingual corpora or newly collected data sets. The most common datasets were JW300 (Agi´c and Vuli´c, 2019) and the Bible’s New Testament (Mayer and Cysouw, 2014; Christodouloupoulos and Steedman, 2015; McCarthy et al., 2020). Besides those, GlobalVoices (Prokopidis et al., 2016) and datasets available at OPUS (Tiedemann, 2012) were added. New datasets were extracted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 20"
2021.americasnlp-1.23,nordhoff-hammarstrom-2012-glottolog,0,0.0139496,"manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages. 1 Introduction Many of the world’s languages, including languages native to the Americas, receive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; B"
2021.americasnlp-1.23,2020.loresmt-1.1,1,0.864016,"earchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available parallel corpora (§3). Additionally, all participants were allowed to use other existing datasets or create their own resources for training in order to improve their systems. Each language pair used in the shared task c"
2021.americasnlp-1.23,N19-4009,1,0.919181,"acted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 2017). We employed the hyperparameters proposed by Guzmán et al. (2019) for a low-resource scenario. We implemented the model using Fairseq (Ott et al., 2019). The implementation of the baseline can be found in the official shared task repository.4 4.2 University of British Columbia The team of the University of British Columbia (UBC-NLP; Billah-Nagoudi et al., 2021) participated for all ten language pairs and in both tracks. They used an encoder-decoder transformer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages h"
2021.americasnlp-1.23,P02-1040,0,0.110098,"ee to use any resources they could find. Possible resources could, for instance, include existing or newly created parallel data, dictionaries, tools, or pretrained models. We invited submissions to two different tracks: Systems in Track 1 were allowed to use the development set as part of the training data, since this is a common practice in the machine translation community. Systems in Track 2 were not allowed to be trained directly on the development set, mimicking a more realistic low-resource setting. 2.2 resulting in a small number of words per sentence. We further reported BLEU scores (Papineni et al., 2002) for all systems and languages. Adequacy The output sentence expresses the meaning of the reference. 1. Extremely bad: The original meaning is not contained at all. 2. Bad: Some words or phrases allow to guess the content. 3. Neutral. 4. Sufficiently good: The original meaning is understandable, but some parts are unclear or incorrect. 5. Excellent: The meaning of the output is the same as that of the reference. Fluency The output sentence is easily readable and looks like a human-produced text. Primary Evaluation In order to be able to evaluate a large number of systems on all 10 languages, w"
2021.americasnlp-1.23,2021.americasnlp-1.24,0,0.0516404,"Missing"
2021.americasnlp-1.23,W15-3049,0,0.0738763,"Missing"
2021.americasnlp-1.23,L16-1144,0,0.161871,"2019), which consists of Jehovah’s Witness texts, sentences extracted from the official dictionary of the Minister of Education (MINEDU), and miscellaneous dictionary entries and samples which have been collected and reviewed by Huarcaya Taquiri (2020). Spanish–Aymara Aymara is a Aymaran language spoken in Bolivia, Peru, and Chile (ISO codes aym, ayr, ayc). The development and test sets are translated into the Central Aymara variant (ayr), specifically Aymara La Paz jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al.,"
2021.americasnlp-1.23,2012.amta-papers.26,0,0.0148085,"ems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used BPE for tokenization. The team experimented with multilingual models pretrained on either 3 or 4 languages, finding that the 4 language model achieved higher performance. Additionally the team trained a Translation Memory (Simard and Fujita, 2012) using half of the examples of the development set. Surprisingly, even given its small amount of training data, this system outperformed the team’s Track 2 submission for Rarámuri. 4.8 Tamalli The team Tamalli7 (Parida et al., 2021) participated in Track 1 for all 10 language pairs. The team used an IBM Model 2 for SMT, and a transformer model for NMT. The team’s NMT models were trained in two settings: one-to-one, with one model being trained per target language, and one-to-many, where decoder weights were shared across languages and a language embedding layer was added to the decoder. They s"
2021.americasnlp-1.23,tiedemann-2012-parallel,0,0.274715,"sentences extracted from the official dictionary of the Minister of Education (MINEDU), and miscellaneous dictionary entries and samples which have been collected and reviewed by Huarcaya Taquiri (2020). Spanish–Aymara Aymara is a Aymaran language spoken in Bolivia, Peru, and Chile (ISO codes aym, ayr, ayc). The development and test sets are translated into the Central Aymara variant (ayr), specifically Aymara La Paz jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a"
2021.americasnlp-1.23,2021.americasnlp-1.29,0,0.0275308,"rmer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages have more available data than others, this dataset is unbalanced in favor of languages like Nahuatl, Guarani, and Quechua. The team also proposed a two-stage fine-tuning method: first fine-tuning on the entire dataset, and then only on the target languages. 4.3 Helsinki The University of Helsinki (Helsinki; Vázquez et al., 2021) participated for all ten language pairs in both tracks. This team did an extensive exploration of the existing datasets, and collected additional resources both from commonly used sources such as the Bible and Wikipedia, as well as other minor sources such as constitutions. Monolingual data was used to generate paired sentences through back-translation, and these parallel examples were added to the existing dataset. Then, a normalization process was done using existing tools, and the aligned data was further filtered. The quality of the data was also considered, and each dataset was assigned"
2021.americasnlp-1.23,2020.emnlp-main.43,0,0.0550595,"Missing"
2021.americasnlp-1.23,2021.americasnlp-1.26,0,0.0686019,"d. The quality of the data was also considered, and each dataset was assigned a weight depending on a noisiness estimation. The team used a transformer sequenceto-sequence model trained via two steps. For their main submission they first trained on data which 4 Otomí online corpus: https://tsunkua.elotl.mx/about/ 206 https://github.com/AmericasNLP/americasnlp2021 Team Langs. Sub. CoAStaL (Bollmann et al., 2021) 10 20 Helsinki (Vázquez et al., 2021) 10 50 NRC-CNRC (Knowles et al., 2021) 4 17 REPUcs (Moreno, 2021) Tamalli (Parida et al., 2021) 1 2 10 UBC-NLP (BillahNagoudi et al., 2021) UTokyo (Zheng et al., 2021) Anonymous Data Models Multilingual Pretrained PB-SMT, Constrained Random Strings Transformer NMT No No 42 Bible, JW300, OPUS, Wikipedia, New collected data Bible, OPUS, Constitutions, Normalization, Filtering, BackTranslation No external data, preoricessing, BPE Dropout. JW300, New dataset, Europarl - 8 29 Bible, Wikipedia 10 40 8 14 Monolingual from other languages. Data - Yes, all ST No languages + Spanish-English Transofrmer NMT Yes, languages 4- No Transformer NMT. WB-SMT. Transformer NMT, Transformer T5 Yes, with Spanish-English 10-languages Spanish-English pretraining No 10-Languages Ne"
2021.eacl-main.230,K17-2001,0,0.0579581,"Missing"
2021.eacl-main.230,N19-1357,0,0.022187,"ct mismatches in subject–verb agreement in the presence of distractor nouns. Warstadt et al. (2019) created a benchmark called BLiMP to assess the ability of language models to handle specific syntactic phenomena in English. Mueller et al. (2020) introduced a similar suite of test sets in English, French, German, Hebrew and Russian, also focusing on syntactic phenomena. Similarly, Xiang et al. (2021) presented CLiMP, a benchmark for the evaluation of Chinese language models. Besides that, attention mechanisms (Bahdanau et al., 2015) in neural models have been common subjects of investigation. Jain and Wallace (2019) claimed that ”attention is not explanation”, to be later on challenged by Wiegreffe and Pinter (2019), who argued that ”attention is not not explanation”. However, the relationship between inputs, attention weights, and outputs is still poorly understood. Furthermore, our work is related to research on which information is learned and how information is encoded by so-called language representation models, e.g., BERT (Devlin et al., 2019) or 2679 Inﬂection LSTM LM LSTM Inﬂection Transformer G2P LSTM ELMo LM Transformer G2P Transformer ELMo Small ELMo Medium ELMo Large Figure 5: Distance matric"
2021.eacl-main.230,P16-2090,1,0.859314,"Missing"
2021.eacl-main.230,W19-0129,1,0.731027,"Rogers et al. (2020). Different word similarity datasets have been used for word embedding evaluation, for instance RG-65 (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al., 2002), or SimLex999 (Hill et al., 2015). In contrast to the work in this paragraph, which was concerned with word or sentence embeddings, we aim at understanding character embeddings. Embedding analysis. The research which is closest to our work investigates which information is captured by different types of embeddings, often by training a classifier to predict certain features of interest. For instance, Kann et al. (2019) used a classifier-based approach to examine whether word and sentence embeddings encode information about the frame-selectional properties of verbs. Ettinger et al. (2016) investigated the grammatical information contained in sentence embeddings regarding multiple linguistic phenomena. Qian et al. (2016) mapped a dense embedding to a sparse linguistic property space to explore the contained information. Bjerva and Augenstein (2018) studied language embeddings. In this paper, we performed an in-depth analysis of character embeddings extracted from various character-level models for NLP. We lev"
2021.eacl-main.230,Q16-1037,0,0.0222002,"In particular vowels 7 Related Work Neural network analysis. A lot of ink has been spilled on what neural network models learn and how. For instance, Zhang and Bowman (2018) investigated different pretraining objectives on their ability to induce syntactic and part-of-speech information. Pruksachatkun et al. (2020) studied model performance on probing tasks to investigate what models learn from intermediate-task training. Belinkov et al. (2017) explored what neural machine translation models learn about morphology. Other work created test sets to evaluate specific linguistic model abilities. Linzen et al. (2016) made a dataset to investigate the ability of neural networks to detect mismatches in subject–verb agreement in the presence of distractor nouns. Warstadt et al. (2019) created a benchmark called BLiMP to assess the ability of language models to handle specific syntactic phenomena in English. Mueller et al. (2020) introduced a similar suite of test sets in English, French, German, Hebrew and Russian, also focusing on syntactic phenomena. Similarly, Xiang et al. (2021) presented CLiMP, a benchmark for the evaluation of Chinese language models. Besides that, attention mechanisms (Bahdanau et al."
2021.eacl-main.230,2021.ccl-1.108,0,0.0223256,"Missing"
2021.eacl-main.230,P16-2067,0,0.0258266,"stitutes a barrier for model improvement. Therefore, a lot of research has been dedicated to investigating the information encoded by neural networks. Especially word embeddings, contextualized word representations, and language representation models like BERT (Devlin et al., 2019) have been exhaustively studied (Rogers et al., 2020). ∗ Equal contribution. This fact has led to the establishment of a workshop with the same name: https://blackboxnlp.github.io 1 Character embeddings are used for a large set of tasks, either as a supplement to word-level input, e.g., for part-of-speech tagging by Plank et al. (2016), or on their own, e.g., for character-level sequence-to-sequence (seq2seq) tasks by Kann and Sch¨utze (2016). Despite this, they have not yet been explicitly analysed. One reason for this might be that identifying relevant properties to study is more challenging than for their word-level counterparts. However, we argue that, in order to truly shine light into black-box NLP models, it is necessary to understand each and every component of them. In this paper, we perform a detailed study of English character embeddings. Our first contribution is a character similarity task (§3) in analogy to th"
2021.eacl-main.230,2020.acl-main.467,1,0.834169,"re between 0.64 for ELMos and 0.55 for ELMol . Betweenness centrality. The local values of betweenness centrality (cf. Figure 4) show the rich structure of the similarity matrices for most embeddings. For all but the ELMo models, the majority of letters have either extremely high or low levels of betweenness. In particular vowels 7 Related Work Neural network analysis. A lot of ink has been spilled on what neural network models learn and how. For instance, Zhang and Bowman (2018) investigated different pretraining objectives on their ability to induce syntactic and part-of-speech information. Pruksachatkun et al. (2020) studied model performance on probing tasks to investigate what models learn from intermediate-task training. Belinkov et al. (2017) explored what neural machine translation models learn about morphology. Other work created test sets to evaluate specific linguistic model abilities. Linzen et al. (2016) made a dataset to investigate the ability of neural networks to detect mismatches in subject–verb agreement in the presence of distractor nouns. Warstadt et al. (2019) created a benchmark called BLiMP to assess the ability of language models to handle specific syntactic phenomena in English. Mue"
2021.eacl-main.230,P16-1140,0,0.0574468,"Missing"
2021.eacl-main.230,2020.tacl-1.54,0,0.0428086,"Missing"
2021.eacl-main.230,2020.acl-main.490,0,0.0353066,"Missing"
2021.eacl-main.230,N18-1202,0,0.365546,"Computational Linguistics, pages 2673–2685 April 19 - 23, 2021. ©2021 Association for Computational Linguistics of methods to characterize the structure of character embedding matrices (§4). The methods we propose are (i) a clustering analysis, (ii) computing the clustering coefficient, (iii) measuring betweenness centrality, and (iv) computing cut-distances. Our final contribution is a detailed character embedding analysis (§6). We explore 6 types of embeddings, obtained from 4 different architectures trained on 3 different tasks, as well as 4 embedding matrices from pretrained ELMo models (Peters et al., 2018). Comparing across models trained on the same task, character embeddings from LSTMs, which, similar to humans, process input sequentially, correlate more with human similarity scores than embeddings from transformers. Comparing across tasks, character embeddings from language modeling show a surprisingly low correlation with human judgments. In contrast, the correlation is highest for grapheme-to-phoneme conversion (G2P). This is in line with findings that colors perceived by synesthetes are influenced by the sound of each letter (Asano and Yokosawa, 2013). 2 Synesthesia Synesthesia is the per"
2021.eacl-main.230,W18-5446,0,0.0389513,"Missing"
2021.eacl-main.230,P13-2145,0,0.01801,"Missing"
2021.eacl-main.230,D19-1002,0,0.0126387,") created a benchmark called BLiMP to assess the ability of language models to handle specific syntactic phenomena in English. Mueller et al. (2020) introduced a similar suite of test sets in English, French, German, Hebrew and Russian, also focusing on syntactic phenomena. Similarly, Xiang et al. (2021) presented CLiMP, a benchmark for the evaluation of Chinese language models. Besides that, attention mechanisms (Bahdanau et al., 2015) in neural models have been common subjects of investigation. Jain and Wallace (2019) claimed that ”attention is not explanation”, to be later on challenged by Wiegreffe and Pinter (2019), who argued that ”attention is not not explanation”. However, the relationship between inputs, attention weights, and outputs is still poorly understood. Furthermore, our work is related to research on which information is learned and how information is encoded by so-called language representation models, e.g., BERT (Devlin et al., 2019) or 2679 Inﬂection LSTM LM LSTM Inﬂection Transformer G2P LSTM ELMo LM Transformer G2P Transformer ELMo Small ELMo Medium ELMo Large Figure 5: Distance matrices and corresponding dendrograms reveal the cluster structure of character embeddings. Darker colors d"
2021.eacl-main.230,2021.eacl-main.242,1,0.724734,"chine translation models learn about morphology. Other work created test sets to evaluate specific linguistic model abilities. Linzen et al. (2016) made a dataset to investigate the ability of neural networks to detect mismatches in subject–verb agreement in the presence of distractor nouns. Warstadt et al. (2019) created a benchmark called BLiMP to assess the ability of language models to handle specific syntactic phenomena in English. Mueller et al. (2020) introduced a similar suite of test sets in English, French, German, Hebrew and Russian, also focusing on syntactic phenomena. Similarly, Xiang et al. (2021) presented CLiMP, a benchmark for the evaluation of Chinese language models. Besides that, attention mechanisms (Bahdanau et al., 2015) in neural models have been common subjects of investigation. Jain and Wallace (2019) claimed that ”attention is not explanation”, to be later on challenged by Wiegreffe and Pinter (2019), who argued that ”attention is not not explanation”. However, the relationship between inputs, attention weights, and outputs is still poorly understood. Furthermore, our work is related to research on which information is learned and how information is encoded by so-called la"
2021.eacl-main.230,W18-5448,0,0.0152377,"efficients between 0.72 and 0.88 and, thus, close to the human average of 0.81. In contrast, the cluster coefficients for ELMo embeddings are between 0.64 for ELMos and 0.55 for ELMol . Betweenness centrality. The local values of betweenness centrality (cf. Figure 4) show the rich structure of the similarity matrices for most embeddings. For all but the ELMo models, the majority of letters have either extremely high or low levels of betweenness. In particular vowels 7 Related Work Neural network analysis. A lot of ink has been spilled on what neural network models learn and how. For instance, Zhang and Bowman (2018) investigated different pretraining objectives on their ability to induce syntactic and part-of-speech information. Pruksachatkun et al. (2020) studied model performance on probing tasks to investigate what models learn from intermediate-task training. Belinkov et al. (2017) explored what neural machine translation models learn about morphology. Other work created test sets to evaluate specific linguistic model abilities. Linzen et al. (2016) made a dataset to investigate the ability of neural networks to detect mismatches in subject–verb agreement in the presence of distractor nouns. Warstadt"
2021.eacl-main.242,W19-4828,0,0.0265874,"lso possible (Warstadt et al., 2019), but can be less informative on LMs’ linguistic knowledge acquisition due to the bias introduced by training on acceptability judgment labels. Some prior work evaluates the linguistic knowledge of different non-English models (Ravfogel et al., 2018; Gulordava et al., 2018; Mueller et al., 2020). However, these efforts focus mainly on subject-verb agreement, which is absent in Chinese, and the knowledge of Chinese LMs has not yet been explicitly studied. Finally, the linguistic abilities of English BERT have been investigated in a a lot of prior work, e.g., Clark et al. (2019); Vig (2019); Hewitt and Manning (2019). We refer the reader to Rogers et al. (2021) for an overview. Vocabulary We translate Warstadt et al.’s (2020) English vocabulary, containing 3,000 English words with morphological, syntactical, and semantic annotations. We add words and features specific to Chinese linguistic phenomena to our vocabulary, including classifiers, verb complements, action verbs, and 王鑫 被 自行车 扔 了。 W´angx¯ın b`ei z`ıx´ıngch¯e r¯eng le SUBJ. PASS. OBJ. V. PST. “Xin Wang was thrown away by a bike.” It is possible to model acceptability in a totally unsupervised way using LMs. T"
2021.eacl-main.242,2020.acl-main.747,0,0.0714743,"Missing"
2021.eacl-main.242,N19-1423,0,0.0389879,"representation models which have been trained on a masked language modeling objective. 2 https://github.com/googleresearch/bert/blob/master/multilingual.md 2784 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2784–2790 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Language Models LMs assign probabilities to sequences of words (Jurafsky and Martin, 2009). Recently, they have become commonly used as pretrained models, which can be fine-tuned for downstream NLP tasks (Peters et al., 2018; Devlin et al., 2019; Conneau et al., 2020). Strictly speaking, LMs compute the probabilities of words based only on past context. BERT (Devlin et al., 2019), however, is trained using a masked language modeling objective: it predicts words based on past and future tokens. Wang and Cho (2019) show that BERT is a Markov random field language model that can assign sentences a pseudo-log-likelihood score, which is computed by summing the conditional log probabilities of all tokens in the sentence, as well as generate text. Shin et al. (2019) and Salazar et al. (2020) apply pseudo-log-likelihood scores to sentence ra"
2021.eacl-main.242,C18-1152,0,0.0250955,"they can judge the grammatical acceptability of sentences. One common version of this task uses MPs to evaluate LMs’ linguistic knowledge (Linzen et al., 2016; Marvin and Linzen, 2018; Warstadt et al., 2020; Wilcox et al., 2018). A MP is a pair of sentences that only differ in acceptability due to a single edit, as in (1) and (2). Native speakers can be asked to choose which sentence in each pair sounds more grammatical. Semiautomatically generating MPs can yield a larger set of controlled sentences, providing sufficient data for model evaluation (Linzen et al., 2016; Marvin and Linzen, 2018; Ettinger et al., 2018). (1) 王鑫 把 自行车 扔 了。 W´angx¯ın bˇa z`ıx´ıngch¯e r¯eng le 3 CLiMP Our main contribution is CLiMP, a corpus of Chinese MPs designed to evaluate Chinese LMs. CLiMP consists of 1,000 MPs for each of 16 grammatical contrasts, covering 9 major Chinese linguistic phenomena. Example MPs for each phenomenon are shown in Table 1. 3.1 Data Generation We generate data from grammar templates for every paradigm we incorporate. Our templates set lexical, syntactic, and semantic constraints for each paradigm, aiming at building robust contrasts and keeping the sentence length the same within each MP. We then b"
2021.eacl-main.242,N18-1108,0,0.0162743,".2 SUBJ. BA. OBJ. V. PST. “Xin Wang threw away a bike.” (2) assigns a probability to each sentence in a MP, and the one with the higher score is predicted as correct, and the model’s predictions can be evaluated against human judgments (Marvin and Linzen, 2018; Warstadt et al., 2020). Supervised approaches are also possible (Warstadt et al., 2019), but can be less informative on LMs’ linguistic knowledge acquisition due to the bias introduced by training on acceptability judgment labels. Some prior work evaluates the linguistic knowledge of different non-English models (Ravfogel et al., 2018; Gulordava et al., 2018; Mueller et al., 2020). However, these efforts focus mainly on subject-verb agreement, which is absent in Chinese, and the knowledge of Chinese LMs has not yet been explicitly studied. Finally, the linguistic abilities of English BERT have been investigated in a a lot of prior work, e.g., Clark et al. (2019); Vig (2019); Hewitt and Manning (2019). We refer the reader to Rogers et al. (2021) for an overview. Vocabulary We translate Warstadt et al.’s (2020) English vocabulary, containing 3,000 English words with morphological, syntactical, and semantic annotations. We add words and features spe"
2021.eacl-main.242,N19-1419,0,0.147467,"ed using a masked language modeling objective: it predicts words based on past and future tokens. Wang and Cho (2019) show that BERT is a Markov random field language model that can assign sentences a pseudo-log-likelihood score, which is computed by summing the conditional log probabilities of all tokens in the sentence, as well as generate text. Shin et al. (2019) and Salazar et al. (2020) apply pseudo-log-likelihood scores to sentence ranking and LM evaluation. 2.2 Evaluation of Linguistic Knowledge Numerous methods exist for probing syntactic knowledge of neural network models in English (Hewitt and Manning, 2019; Tenney et al., 2019), and a growing body of work evaluates the syntactic knowledge of neural models by testing whether they can judge the grammatical acceptability of sentences. One common version of this task uses MPs to evaluate LMs’ linguistic knowledge (Linzen et al., 2016; Marvin and Linzen, 2018; Warstadt et al., 2020; Wilcox et al., 2018). A MP is a pair of sentences that only differ in acceptability due to a single edit, as in (1) and (2). Native speakers can be asked to choose which sentence in each pair sounds more grammatical. Semiautomatically generating MPs can yield a larger se"
2021.eacl-main.242,2020.acl-main.158,0,0.0337792,"Missing"
2021.eacl-main.242,D18-1151,0,0.08517,"s in the sentence, as well as generate text. Shin et al. (2019) and Salazar et al. (2020) apply pseudo-log-likelihood scores to sentence ranking and LM evaluation. 2.2 Evaluation of Linguistic Knowledge Numerous methods exist for probing syntactic knowledge of neural network models in English (Hewitt and Manning, 2019; Tenney et al., 2019), and a growing body of work evaluates the syntactic knowledge of neural models by testing whether they can judge the grammatical acceptability of sentences. One common version of this task uses MPs to evaluate LMs’ linguistic knowledge (Linzen et al., 2016; Marvin and Linzen, 2018; Warstadt et al., 2020; Wilcox et al., 2018). A MP is a pair of sentences that only differ in acceptability due to a single edit, as in (1) and (2). Native speakers can be asked to choose which sentence in each pair sounds more grammatical. Semiautomatically generating MPs can yield a larger set of controlled sentences, providing sufficient data for model evaluation (Linzen et al., 2016; Marvin and Linzen, 2018; Ettinger et al., 2018). (1) 王鑫 把 自行车 扔 了。 W´angx¯ın bˇa z`ıx´ıngch¯e r¯eng le 3 CLiMP Our main contribution is CLiMP, a corpus of Chinese MPs designed to evaluate Chinese LMs. CLiMP c"
2021.eacl-main.242,2020.acl-main.490,0,0.114765,". “Xin Wang threw away a bike.” (2) assigns a probability to each sentence in a MP, and the one with the higher score is predicted as correct, and the model’s predictions can be evaluated against human judgments (Marvin and Linzen, 2018; Warstadt et al., 2020). Supervised approaches are also possible (Warstadt et al., 2019), but can be less informative on LMs’ linguistic knowledge acquisition due to the bias introduced by training on acceptability judgment labels. Some prior work evaluates the linguistic knowledge of different non-English models (Ravfogel et al., 2018; Gulordava et al., 2018; Mueller et al., 2020). However, these efforts focus mainly on subject-verb agreement, which is absent in Chinese, and the knowledge of Chinese LMs has not yet been explicitly studied. Finally, the linguistic abilities of English BERT have been investigated in a a lot of prior work, e.g., Clark et al. (2019); Vig (2019); Hewitt and Manning (2019). We refer the reader to Rogers et al. (2021) for an overview. Vocabulary We translate Warstadt et al.’s (2020) English vocabulary, containing 3,000 English words with morphological, syntactical, and semantic annotations. We add words and features specific to Chinese lingui"
2021.eacl-main.242,N18-1202,0,0.0301467,"ch includes language representation models which have been trained on a masked language modeling objective. 2 https://github.com/googleresearch/bert/blob/master/multilingual.md 2784 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2784–2790 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Language Models LMs assign probabilities to sequences of words (Jurafsky and Martin, 2009). Recently, they have become commonly used as pretrained models, which can be fine-tuned for downstream NLP tasks (Peters et al., 2018; Devlin et al., 2019; Conneau et al., 2020). Strictly speaking, LMs compute the probabilities of words based only on past context. BERT (Devlin et al., 2019), however, is trained using a masked language modeling objective: it predicts words based on past and future tokens. Wang and Cho (2019) show that BERT is a Markov random field language model that can assign sentences a pseudo-log-likelihood score, which is computed by summing the conditional log probabilities of all tokens in the sentence, as well as generate text. Shin et al. (2019) and Salazar et al. (2020) apply pseudo-log-likelihood"
2021.eacl-main.242,W18-5412,0,0.0748128,"e3 used to create it. 3.2 SUBJ. BA. OBJ. V. PST. “Xin Wang threw away a bike.” (2) assigns a probability to each sentence in a MP, and the one with the higher score is predicted as correct, and the model’s predictions can be evaluated against human judgments (Marvin and Linzen, 2018; Warstadt et al., 2020). Supervised approaches are also possible (Warstadt et al., 2019), but can be less informative on LMs’ linguistic knowledge acquisition due to the bias introduced by training on acceptability judgment labels. Some prior work evaluates the linguistic knowledge of different non-English models (Ravfogel et al., 2018; Gulordava et al., 2018; Mueller et al., 2020). However, these efforts focus mainly on subject-verb agreement, which is absent in Chinese, and the knowledge of Chinese LMs has not yet been explicitly studied. Finally, the linguistic abilities of English BERT have been investigated in a a lot of prior work, e.g., Clark et al. (2019); Vig (2019); Hewitt and Manning (2019). We refer the reader to Rogers et al. (2021) for an overview. Vocabulary We translate Warstadt et al.’s (2020) English vocabulary, containing 3,000 English words with morphological, syntactical, and semantic annotations. We ad"
2021.eacl-main.242,2020.acl-main.240,0,0.067306,"uned for downstream NLP tasks (Peters et al., 2018; Devlin et al., 2019; Conneau et al., 2020). Strictly speaking, LMs compute the probabilities of words based only on past context. BERT (Devlin et al., 2019), however, is trained using a masked language modeling objective: it predicts words based on past and future tokens. Wang and Cho (2019) show that BERT is a Markov random field language model that can assign sentences a pseudo-log-likelihood score, which is computed by summing the conditional log probabilities of all tokens in the sentence, as well as generate text. Shin et al. (2019) and Salazar et al. (2020) apply pseudo-log-likelihood scores to sentence ranking and LM evaluation. 2.2 Evaluation of Linguistic Knowledge Numerous methods exist for probing syntactic knowledge of neural network models in English (Hewitt and Manning, 2019; Tenney et al., 2019), and a growing body of work evaluates the syntactic knowledge of neural models by testing whether they can judge the grammatical acceptability of sentences. One common version of this task uses MPs to evaluate LMs’ linguistic knowledge (Linzen et al., 2016; Marvin and Linzen, 2018; Warstadt et al., 2020; Wilcox et al., 2018). A MP is a pair of s"
2021.eacl-main.242,P19-3007,0,0.0195799,"t et al., 2019), but can be less informative on LMs’ linguistic knowledge acquisition due to the bias introduced by training on acceptability judgment labels. Some prior work evaluates the linguistic knowledge of different non-English models (Ravfogel et al., 2018; Gulordava et al., 2018; Mueller et al., 2020). However, these efforts focus mainly on subject-verb agreement, which is absent in Chinese, and the knowledge of Chinese LMs has not yet been explicitly studied. Finally, the linguistic abilities of English BERT have been investigated in a a lot of prior work, e.g., Clark et al. (2019); Vig (2019); Hewitt and Manning (2019). We refer the reader to Rogers et al. (2021) for an overview. Vocabulary We translate Warstadt et al.’s (2020) English vocabulary, containing 3,000 English words with morphological, syntactical, and semantic annotations. We add words and features specific to Chinese linguistic phenomena to our vocabulary, including classifiers, verb complements, action verbs, and 王鑫 被 自行车 扔 了。 W´angx¯ın b`ei z`ıx´ıngch¯e r¯eng le SUBJ. PASS. OBJ. V. PST. “Xin Wang was thrown away by a bike.” It is possible to model acceptability in a totally unsupervised way using LMs. The model 3 T"
2021.eacl-main.242,W19-2304,0,0.159249,"es 2784–2790 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Language Models LMs assign probabilities to sequences of words (Jurafsky and Martin, 2009). Recently, they have become commonly used as pretrained models, which can be fine-tuned for downstream NLP tasks (Peters et al., 2018; Devlin et al., 2019; Conneau et al., 2020). Strictly speaking, LMs compute the probabilities of words based only on past context. BERT (Devlin et al., 2019), however, is trained using a masked language modeling objective: it predicts words based on past and future tokens. Wang and Cho (2019) show that BERT is a Markov random field language model that can assign sentences a pseudo-log-likelihood score, which is computed by summing the conditional log probabilities of all tokens in the sentence, as well as generate text. Shin et al. (2019) and Salazar et al. (2020) apply pseudo-log-likelihood scores to sentence ranking and LM evaluation. 2.2 Evaluation of Linguistic Knowledge Numerous methods exist for probing syntactic knowledge of neural network models in English (Hewitt and Manning, 2019; Tenney et al., 2019), and a growing body of work evaluates the syntactic knowledge of neura"
2021.eacl-main.242,2020.tacl-1.25,1,0.955493,"n Language models (LMs) are crucial parts of natural language processing (NLP) systems for a large variety of tasks, including summarization, machine translation, and dialog generation. More recently, they have become popular in the form of pretrained models,1 which are then fine-tuned on downstream tasks and often obtain state-of-the-art performance (Peters et al., 2018; Devlin et al., 2019; Conneau et al., 2020). However, which linguistic phenomena language models can or cannot learn is still poorly understood for many languages. Resources for the syntactic evaluation of LMs, such as BLiMP (Warstadt et al., 2020) have focused mainly on English, and non-English resources currently only cover a small set of phenomena (Mueller et al., 2020; Gulordava et al., 2018; Ravfogel et al., 2018). In order to spur the analysis and subsequent improvement of LMs in Chinese, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP), which can be used to evaluate LMs’ knowledge of Chinese grammar. CLiMP consists of 16 individual datasets that are semi-automatically generated from grammar templates. Each set—or paradigm—contains 1,000 minimal pairs (MPs). Together, they cover 9 core linguistic phenomena in Ch"
2021.eacl-main.242,Q19-1040,1,0.84751,"for each paradigm, aiming at building robust contrasts and keeping the sentence length the same within each MP. We then build an annotated vocabulary, and generate sentences by sampling words from it. (1) and (2) show an MP together with the template3 used to create it. 3.2 SUBJ. BA. OBJ. V. PST. “Xin Wang threw away a bike.” (2) assigns a probability to each sentence in a MP, and the one with the higher score is predicted as correct, and the model’s predictions can be evaluated against human judgments (Marvin and Linzen, 2018; Warstadt et al., 2020). Supervised approaches are also possible (Warstadt et al., 2019), but can be less informative on LMs’ linguistic knowledge acquisition due to the bias introduced by training on acceptability judgment labels. Some prior work evaluates the linguistic knowledge of different non-English models (Ravfogel et al., 2018; Gulordava et al., 2018; Mueller et al., 2020). However, these efforts focus mainly on subject-verb agreement, which is absent in Chinese, and the knowledge of Chinese LMs has not yet been explicitly studied. Finally, the linguistic abilities of English BERT have been investigated in a a lot of prior work, e.g., Clark et al. (2019); Vig (2019); Hew"
2021.eacl-main.242,W18-5423,0,0.0192793,"n et al. (2019) and Salazar et al. (2020) apply pseudo-log-likelihood scores to sentence ranking and LM evaluation. 2.2 Evaluation of Linguistic Knowledge Numerous methods exist for probing syntactic knowledge of neural network models in English (Hewitt and Manning, 2019; Tenney et al., 2019), and a growing body of work evaluates the syntactic knowledge of neural models by testing whether they can judge the grammatical acceptability of sentences. One common version of this task uses MPs to evaluate LMs’ linguistic knowledge (Linzen et al., 2016; Marvin and Linzen, 2018; Warstadt et al., 2020; Wilcox et al., 2018). A MP is a pair of sentences that only differ in acceptability due to a single edit, as in (1) and (2). Native speakers can be asked to choose which sentence in each pair sounds more grammatical. Semiautomatically generating MPs can yield a larger set of controlled sentences, providing sufficient data for model evaluation (Linzen et al., 2016; Marvin and Linzen, 2018; Ettinger et al., 2018). (1) 王鑫 把 自行车 扔 了。 W´angx¯ın bˇa z`ıx´ıngch¯e r¯eng le 3 CLiMP Our main contribution is CLiMP, a corpus of Chinese MPs designed to evaluate Chinese LMs. CLiMP consists of 1,000 MPs for each of 16 grammatic"
2021.emnlp-main.63,2020.emnlp-main.703,0,0.0609879,"Missing"
2021.emnlp-main.63,2020.emnlp-main.557,0,0.0158903,"and Merlo, 2020; Shwartz and Choi, 2020). Previous work has shown the importance of color in visual perception and object recognition (Rosenthal et al., 2018; Gegenfurtner and Rieger, 2000). More recently Teichmann et al. (2020) use time resolved neural imaging data to demonstrate how the typicality of object-color relationships influences object representations in visual processing. Probing LMs A wide range of papers have probed LMs in a zero-shot fashion by looking at how they fill in a [MASK] token in handcrafted (Weir et al., 2020; Petroni et al., 2019; Jiang et al., 2020; Ettinger, 2020; Lin et al., 2020) or automatically generated (Bouraoui et al., 2020; Jiang et al., 2020) template sentences. Others, such as Warstadt et al. (2020) compare perplexities between minimal pairs of sentences. A different approach is to analyze the representation quality of LMs for linguistic tasks by training a simple MLP on pretrained model representations (Da and Kasai, 2019; Lin et al., 2019). However, Zhang and Bowman (2018) demonstrate that the procedure of training an additional classifier may distort the results. An alternative approach introduced by Voita and Titov (2020) is information-theoretic probing w"
2021.emnlp-main.63,W19-4825,0,0.0419042,"Missing"
2021.emnlp-main.63,P12-3029,0,0.0699159,"Missing"
2021.emnlp-main.63,2021.ccl-1.108,0,0.0612651,"Missing"
2021.emnlp-main.63,D19-1250,0,0.145619,"rongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research. 1 Everyone knows that most bananas are [MASK]. Figure 1: An example prompt from CoDa. Introduction Given sufficient scale, language models (LMs)1 are able to function as knowledge bases, yielding factoids and relational knowledge across a wide range of topics (Petroni et al., 2019; Bouraoui et al., 2020). However, subsequent work (Bender and Koller, 2020; Bisk et al., 2020; Aroca-Ouellette et al., 2021) has raised concerns about the inherent limitations of text-only pretraining. Motivated by these concerns and limitations, we identify and investigate how reporting bias, a concrete and measurable signal, correlates with these limitations and how multimodal training can mitigate these issues. Grice’s conversational maxim of quantity (Grice, 1975) asserts that utterances only contain the required amount of information. This leads to explicit reporting of self-evident know"
2021.emnlp-main.63,2020.coling-main.605,0,0.181611,"st splits, with 311, 104, and 106 objects respectively. There is no object overlap between the different sets. 3 3.1 Reporting Bias Background As previously stated, Grice’s conversational maxim of quantity manifests as reporting bias – i.e., people not usually stating obvious facts or properties –, and impacts nearly all datasets that contain text. Reporting bias has been studied in the context of both NLP and image captioning. Gordon and Van Durme (2013) perform a quantitative analysis using n-gram frequencies from text, finding this phenomenon particularly relevant to internet text corpora. Shwartz and Choi (2020) extend these experiments to pretrained models such as Bert (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). Similar to our work, they analyze color attribution of the form “The banana is tasty.” However, their ground truth is extracted from Wikipedia bi-grams and, thus, suffers from reporting bias itself. In contrast, we circumvent this problem by collecting the ground truth in CoDa directly from humans. 3.2 Reporting Bias in Text Our hypothesis is that pretrained LMs inherit reporting bias with respect to colors from their training data. Thus, prior to our main experiments, we investiga"
2021.emnlp-main.63,2020.emnlp-main.162,0,0.0345084,"number of training objects. Note that with 14 objects, all models surpass zero-shot performance in terms of Jensen-Shannon divergence. With enough training objects, we observe similar ranking patterns observed in the zero-shot setting for text-only models. However, the advantage of this approach is that we can additionally include multimodal architectures. The results from these experiments demonstrate that multimodal models outperform text-only models at recovering color distributions. They manage to do so even though the performance of multimodal models is often lower on classic NLP tasks (Tan and Bansal, 2020) and many multimodal datasets are even more prone to reporting bias in text (Misra et al., 2016; van Miltenburg, 2016; 5.3 Representation Probes Burns et al., 2018). This further support the arFig. 4 shows the average correlation and Jensen- guments in Bisk et al. (2020) that understanding Shannon divergence for unseen objects as a func- concepts requires experiencing them in their natu11 That is, where LMs directly reflect n-gram frequencies. ral form. 829 0.4 Jensen Shannon Divergence Representation Average Correlation 0.6 0.4 0.2 1 10 100 Training Objects 1,000 GPT-2 RoBERTa ALBERT CLIP Ran"
2021.findings-acl.404,2020.acl-main.463,0,0.011077,"world experience. By highlighting these limitations, we hope to motivate the development of models with a human-like understanding of the physical world. 1 A person drops a glass, a pillow, a coin, and a pen from a balcony. The [MASK] is most likely to break. A) glass B) pillow C) coin D) pen Figure 1: An example question from PROST. the reader can also use their knowledge of gravity and the properties of a pint glass to reason about potential outcomes: the pint glass will fall toward the ground and will likely break on impact. Introduction In the context of natural language processing (NLP), Bender and Koller (2020) provides a working definition of “understanding” as the ability to recover the communicative intent from an utterance. To achieve this, one must be able to query a set of concepts that is aligned with the speaker’s own understanding. An example of such alignment is our interaction with the physical world. This experience, shared by all humans, provides a common set of concepts to rely on in communication. For example, the reader can map the phrase I dropped my pint glass to a set of relevant experiences and generate a mental depiction of the scene. Further, ∗ *Email has no accent, but include"
2021.findings-acl.404,2020.emnlp-main.703,0,0.022322,"Missing"
2021.findings-acl.404,N19-1300,0,0.0348158,"Missing"
2021.findings-acl.404,P19-1388,0,0.017698,"trieval than a general understanding and reasoning about the world. Another set of reasoning-based benchmarks focuses on common sense reasoning. SWAG and its extension hellaSWAG evaluate commonsense natural language inference (Zellers et al., 2018, 2019). Sap et al. (2019) tests commonsense reasoning about social situations. However, commonsense reasoning is often subjective and requires understanding of complex human–human interactions involving social and societal norms. In contrast, physical reasoning is based on objective and well defined constructs. Other datasets (Forbes and Choi, 2017; Elazar et al., 2019; Goel et al., 2019) focus on object– attribute comparison. However, they compare concepts at a word level rather than sentence level and use a large training set to create an engineered object–attribute comparison model. It is difficult to see how these models could generalize to other forms of reasoning. Moreover, all the above datasets follow a pretraining-agnostic identically distributed (PAID) paradigm (Linzen, 2020), making them susceptible to models that can leverage unintended correlations between the training and test sets. Zero-Shot LM Probes Similar to PROST, several recent benchmar"
2021.findings-acl.404,2020.tacl-1.3,0,0.0149224,"identically distributed (PAID) paradigm (Linzen, 2020), making them susceptible to models that can leverage unintended correlations between the training and test sets. Zero-Shot LM Probes Similar to PROST, several recent benchmarks have circumvented the concern of identically distributed training and test sets by probing models in a zero-shot manner. Petroni et al. (2019) queries masked LMs (MLMs) for factual knowledge using templates in the format of Dante was born in [MASK]. Talmor et al. (2020) use a similar format to probe six concepts ranging from age comparison to taxonomy conjunction. Ettinger (2020) uses this format to show that BERT robustly retrieves hypernyms, but fails to understand negation. Lin et al. (2020) probe numerical commensense in both MLMs and traditional LMs. Warstadt et al. (2020) measures traditional LMs’ sense of grammatical acceptability by comparing sentence probabilities. Grounded Language Environments PROST investigates if pretrained models show a lack of understanding of the physical world which could result from learning language without grounding. While not used for pretraining, a number of multi-modal environments have been developed to ground language. Shridha"
2021.findings-acl.404,P17-1025,0,0.0228953,"more on information retrieval than a general understanding and reasoning about the world. Another set of reasoning-based benchmarks focuses on common sense reasoning. SWAG and its extension hellaSWAG evaluate commonsense natural language inference (Zellers et al., 2018, 2019). Sap et al. (2019) tests commonsense reasoning about social situations. However, commonsense reasoning is often subjective and requires understanding of complex human–human interactions involving social and societal norms. In contrast, physical reasoning is based on objective and well defined constructs. Other datasets (Forbes and Choi, 2017; Elazar et al., 2019; Goel et al., 2019) focus on object– attribute comparison. However, they compare concepts at a word level rather than sentence level and use a large training set to create an engineered object–attribute comparison model. It is difficult to see how these models could generalize to other forms of reasoning. Moreover, all the above datasets follow a pretraining-agnostic identically distributed (PAID) paradigm (Linzen, 2020), making them susceptible to models that can leverage unintended correlations between the training and test sets. Zero-Shot LM Probes Similar to PROST, se"
2021.findings-acl.404,D19-6016,0,0.0201916,"l understanding and reasoning about the world. Another set of reasoning-based benchmarks focuses on common sense reasoning. SWAG and its extension hellaSWAG evaluate commonsense natural language inference (Zellers et al., 2018, 2019). Sap et al. (2019) tests commonsense reasoning about social situations. However, commonsense reasoning is often subjective and requires understanding of complex human–human interactions involving social and societal norms. In contrast, physical reasoning is based on objective and well defined constructs. Other datasets (Forbes and Choi, 2017; Elazar et al., 2019; Goel et al., 2019) focus on object– attribute comparison. However, they compare concepts at a word level rather than sentence level and use a large training set to create an engineered object–attribute comparison model. It is difficult to see how these models could generalize to other forms of reasoning. Moreover, all the above datasets follow a pretraining-agnostic identically distributed (PAID) paradigm (Linzen, 2020), making them susceptible to models that can leverage unintended correlations between the training and test sets. Zero-Shot LM Probes Similar to PROST, several recent benchmarks have circumvented"
2021.findings-acl.404,2020.findings-emnlp.171,0,0.0105869,"ction and uses static masking, RoBERTa is trained on MLM with dynamic masking, and ALBERT uses whole-word n-gram masking. For probing, we filter out all but the four answer choices from the output vocabulary and select the token with the highest probability as the model’s decision. Encoder-decoder Models We also include results for T5 (Raffel et al., 2020). T5 is trained using a span corruption objective, in which spans of the input sequence are randomly replaced with 4601 Finetuned Conditional LMs To better understand the limitations of text-only training, we additionally evaluate UnifiedQA (Khashabi et al., 2020). UnifiedQA is a pretrained QA model, built off T5, and finetuned on SQuad 1.1, SQuaD 2.0, NarrativeQA, RACE, ARC, OpenBookQA, MCTest, and BoolQ (Rajpurkar et al., 2016, 2018; Kočiský et al., 2018; Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Richardson et al., 2013; Clark et al., 2019). We format all of our templates to fit their multiple-choice question answering format and use their provided scoring metrics to select the models’ answers.3 5 Results The per model and per concept results are shown in Table 5. For concepts with more than one template—direction, mass, height, an"
2021.findings-acl.404,Q18-1023,0,0.0413847,"Missing"
2021.findings-acl.404,D17-1082,0,0.0553096,"Missing"
2021.findings-acl.404,2020.emnlp-main.557,0,0.0117756,"nded correlations between the training and test sets. Zero-Shot LM Probes Similar to PROST, several recent benchmarks have circumvented the concern of identically distributed training and test sets by probing models in a zero-shot manner. Petroni et al. (2019) queries masked LMs (MLMs) for factual knowledge using templates in the format of Dante was born in [MASK]. Talmor et al. (2020) use a similar format to probe six concepts ranging from age comparison to taxonomy conjunction. Ettinger (2020) uses this format to show that BERT robustly retrieves hypernyms, but fails to understand negation. Lin et al. (2020) probe numerical commensense in both MLMs and traditional LMs. Warstadt et al. (2020) measures traditional LMs’ sense of grammatical acceptability by comparing sentence probabilities. Grounded Language Environments PROST investigates if pretrained models show a lack of understanding of the physical world which could result from learning language without grounding. While not used for pretraining, a number of multi-modal environments have been developed to ground language. Shridhar et al. (2020)’s ALFRED builds on other vision-and-language navigation environments (Gordon et al., 2018; Reg4598 ne"
2021.findings-acl.404,2020.acl-main.465,0,0.0163799,"eractions involving social and societal norms. In contrast, physical reasoning is based on objective and well defined constructs. Other datasets (Forbes and Choi, 2017; Elazar et al., 2019; Goel et al., 2019) focus on object– attribute comparison. However, they compare concepts at a word level rather than sentence level and use a large training set to create an engineered object–attribute comparison model. It is difficult to see how these models could generalize to other forms of reasoning. Moreover, all the above datasets follow a pretraining-agnostic identically distributed (PAID) paradigm (Linzen, 2020), making them susceptible to models that can leverage unintended correlations between the training and test sets. Zero-Shot LM Probes Similar to PROST, several recent benchmarks have circumvented the concern of identically distributed training and test sets by probing models in a zero-shot manner. Petroni et al. (2019) queries masked LMs (MLMs) for factual knowledge using templates in the format of Dante was born in [MASK]. Talmor et al. (2020) use a similar format to probe six concepts ranging from age comparison to taxonomy conjunction. Ettinger (2020) uses this format to show that BERT robu"
2021.findings-acl.404,2021.ccl-1.108,0,0.0213623,"Missing"
2021.findings-acl.404,D19-1250,0,0.0120539,"nce level and use a large training set to create an engineered object–attribute comparison model. It is difficult to see how these models could generalize to other forms of reasoning. Moreover, all the above datasets follow a pretraining-agnostic identically distributed (PAID) paradigm (Linzen, 2020), making them susceptible to models that can leverage unintended correlations between the training and test sets. Zero-Shot LM Probes Similar to PROST, several recent benchmarks have circumvented the concern of identically distributed training and test sets by probing models in a zero-shot manner. Petroni et al. (2019) queries masked LMs (MLMs) for factual knowledge using templates in the format of Dante was born in [MASK]. Talmor et al. (2020) use a similar format to probe six concepts ranging from age comparison to taxonomy conjunction. Ettinger (2020) uses this format to show that BERT robustly retrieves hypernyms, but fails to understand negation. Lin et al. (2020) probe numerical commensense in both MLMs and traditional LMs. Warstadt et al. (2020) measures traditional LMs’ sense of grammatical acceptability by comparing sentence probabilities. Grounded Language Environments PROST investigates if pretra"
2021.findings-acl.404,P18-2124,0,0.0246377,"Missing"
2021.findings-acl.404,D16-1264,0,0.066179,"Missing"
2021.findings-acl.404,Q13-1003,0,0.0665723,"Missing"
2021.findings-acl.404,2020.acl-main.442,0,0.0121669,"y interacting with the world, and are useful concepts for any embodied agent. The questions are constructed from 14 manually written templates. Each template follows one of three different formats: the first format is specific to the set of questions pertaining to directions; the second format is used to gauge the relative attributes—specifically mass, height, and circumference—of objects; and the third format targets the affordances of objects—specifically whether an object is stackable, rollable, graspable, or breakable, and whether a surfaces is slideable or bounceable2 . We use CheckList (Ribeiro et al., 2020) to obtain the questions from our templates. We show all templates in Table 1 and explain them in detail below. We end this section by describing the objects featured in PROST. Direction Templates We use two templates to generate questions which probe understanding of direction. The first focuses on cardinal directions. The second uses a set of four manually crafted ques1 An exception is Lynch and Sermanet (2020), which incorporates modern LMs and provides impressive generalizability. However, they too only use language as an input and do not analyze how language understanding evolves. 2 Bounc"
2021.findings-acl.404,D18-1260,0,0.0234315,"Missing"
2021.findings-acl.404,D13-1020,0,0.0397128,"Missing"
2021.findings-acl.404,D19-1454,0,0.0132474,"so focuses on simple physics, however there is no language component. Clark et al. (2018) and Kembhavi et al. (2017) both provide a large set of grade school multiple-choice questions, including some that could be solved with reasoning. However both provide corresponding material where the solution can be found, relying more on information retrieval than a general understanding and reasoning about the world. Another set of reasoning-based benchmarks focuses on common sense reasoning. SWAG and its extension hellaSWAG evaluate commonsense natural language inference (Zellers et al., 2018, 2019). Sap et al. (2019) tests commonsense reasoning about social situations. However, commonsense reasoning is often subjective and requires understanding of complex human–human interactions involving social and societal norms. In contrast, physical reasoning is based on objective and well defined constructs. Other datasets (Forbes and Choi, 2017; Elazar et al., 2019; Goel et al., 2019) focus on object– attribute comparison. However, they compare concepts at a word level rather than sentence level and use a large training set to create an engineered object–attribute comparison model. It is difficult to see how these"
2021.findings-acl.404,2020.tacl-1.48,0,0.0344615,"e models could generalize to other forms of reasoning. Moreover, all the above datasets follow a pretraining-agnostic identically distributed (PAID) paradigm (Linzen, 2020), making them susceptible to models that can leverage unintended correlations between the training and test sets. Zero-Shot LM Probes Similar to PROST, several recent benchmarks have circumvented the concern of identically distributed training and test sets by probing models in a zero-shot manner. Petroni et al. (2019) queries masked LMs (MLMs) for factual knowledge using templates in the format of Dante was born in [MASK]. Talmor et al. (2020) use a similar format to probe six concepts ranging from age comparison to taxonomy conjunction. Ettinger (2020) uses this format to show that BERT robustly retrieves hypernyms, but fails to understand negation. Lin et al. (2020) probe numerical commensense in both MLMs and traditional LMs. Warstadt et al. (2020) measures traditional LMs’ sense of grammatical acceptability by comparing sentence probabilities. Grounded Language Environments PROST investigates if pretrained models show a lack of understanding of the physical world which could result from learning language without grounding. Whil"
2021.findings-acl.404,W18-5446,0,0.0305683,"Missing"
2021.findings-acl.404,2020.tacl-1.25,0,0.0656225,"r to PROST, several recent benchmarks have circumvented the concern of identically distributed training and test sets by probing models in a zero-shot manner. Petroni et al. (2019) queries masked LMs (MLMs) for factual knowledge using templates in the format of Dante was born in [MASK]. Talmor et al. (2020) use a similar format to probe six concepts ranging from age comparison to taxonomy conjunction. Ettinger (2020) uses this format to show that BERT robustly retrieves hypernyms, but fails to understand negation. Lin et al. (2020) probe numerical commensense in both MLMs and traditional LMs. Warstadt et al. (2020) measures traditional LMs’ sense of grammatical acceptability by comparing sentence probabilities. Grounded Language Environments PROST investigates if pretrained models show a lack of understanding of the physical world which could result from learning language without grounding. While not used for pretraining, a number of multi-modal environments have been developed to ground language. Shridhar et al. (2020)’s ALFRED builds on other vision-and-language navigation environments (Gordon et al., 2018; Reg4598 neri et al., 2013; Zhu et al., 2017; Anderson et al., 2018), and enables grounding of l"
2021.findings-acl.404,D18-1009,0,0.0118338,"lish a goal. This research also focuses on simple physics, however there is no language component. Clark et al. (2018) and Kembhavi et al. (2017) both provide a large set of grade school multiple-choice questions, including some that could be solved with reasoning. However both provide corresponding material where the solution can be found, relying more on information retrieval than a general understanding and reasoning about the world. Another set of reasoning-based benchmarks focuses on common sense reasoning. SWAG and its extension hellaSWAG evaluate commonsense natural language inference (Zellers et al., 2018, 2019). Sap et al. (2019) tests commonsense reasoning about social situations. However, commonsense reasoning is often subjective and requires understanding of complex human–human interactions involving social and societal norms. In contrast, physical reasoning is based on objective and well defined constructs. Other datasets (Forbes and Choi, 2017; Elazar et al., 2019; Goel et al., 2019) focus on object– attribute comparison. However, they compare concepts at a word level rather than sentence level and use a large training set to create an engineered object–attribute comparison model. It is"
2021.findings-acl.404,P19-1472,0,0.0298911,"Missing"
2021.findings-acl.418,2020.acl-main.697,0,0.0242902,"earning and engagement. Prior work in understanding classroom discourse using NLP includes Suresh et al. (2019) and Donnelly et al. (2016). They propose an application where feedback can be provided to teachers by automatically classifying their utterances into talk moves. Other applications of NLP to education include language learning assistance (Beatty, 2013; Carlini et al., 2014; Tetreault et al., 2014), writing assistance (Dahlmeier and Ng, 2011; Chollampatt and Ng, 2018; Chukharev-Hudilainen and Saricaoglu, 2016), and automated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 2020). Analogous to FTMP, future dialogue act prediction is"
2021.findings-acl.418,2020.bea-1.0,0,0.168825,"Missing"
2021.findings-acl.418,P98-1032,0,0.366987,"lp of an NLP system and, thus, to improve student learning and engagement. Prior work in understanding classroom discourse using NLP includes Suresh et al. (2019) and Donnelly et al. (2016). They propose an application where feedback can be provided to teachers by automatically classifying their utterances into talk moves. Other applications of NLP to education include language learning assistance (Beatty, 2013; Carlini et al., 2014; Tetreault et al., 2014), writing assistance (Dahlmeier and Ng, 2011; Chollampatt and Ng, 2018; Chukharev-Hudilainen and Saricaoglu, 2016), and automated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 20"
2021.findings-acl.418,W14-3501,0,0.0170187,"lassroom discourse with annotations for talk moves used by the teacher. 3.2 NLP for Educational Applications Our work is a first step towards improving in-class discussions with the help of an NLP system and, thus, to improve student learning and engagement. Prior work in understanding classroom discourse using NLP includes Suresh et al. (2019) and Donnelly et al. (2016). They propose an application where feedback can be provided to teachers by automatically classifying their utterances into talk moves. Other applications of NLP to education include language learning assistance (Beatty, 2013; Carlini et al., 2014; Tetreault et al., 2014), writing assistance (Dahlmeier and Ng, 2011; Chollampatt and Ng, 2018; Chukharev-Hudilainen and Saricaoglu, 2016), and automated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act t"
2021.findings-acl.418,D14-1179,0,0.0394396,"Missing"
2021.findings-acl.418,D11-1010,0,0.0147882,"her. 3.2 NLP for Educational Applications Our work is a first step towards improving in-class discussions with the help of an NLP system and, thus, to improve student learning and engagement. Prior work in understanding classroom discourse using NLP includes Suresh et al. (2019) and Donnelly et al. (2016). They propose an application where feedback can be provided to teachers by automatically classifying their utterances into talk moves. Other applications of NLP to education include language learning assistance (Beatty, 2013; Carlini et al., 2014; Tetreault et al., 2014), writing assistance (Dahlmeier and Ng, 2011; Chollampatt and Ng, 2018; Chukharev-Hudilainen and Saricaoglu, 2016), and automated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the tas"
2021.findings-acl.418,C16-1189,0,0.0120003,"mated scoring (Burstein et al., 1998; Farag et al., 2018; Beigman Klebanov and Madnani, 2020). 3.3 Dialogue Systems Our work is further related to research on dialogue systems. Similar to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 2020). Analogous to FTMP, future dialogue act prediction is the task of predicting what the next dialogue act should be, given a conversation history (Tanaka et al., 2019). Pretrained models have been successfully adapted to the task of dialogue generation (Zhang et al., 2020; Wu et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). However, if directly used in the classroom, these models could potentially produce harmful or unsuitable dialogue as they are trained on large datasets comprising conversations from the internet (Bender et al., 2021). Additionally, we want a sys"
2021.findings-acl.418,W02-0109,0,0.664718,"Missing"
2021.findings-acl.418,N19-4009,0,0.0505929,"Missing"
2021.findings-acl.418,P16-1162,0,0.0107246,"sing two special tokens, [USR] and [SYS]. Correspondingly, we use the [USR] token to indicate student utterances and the [SYS] token to indicate teacher utterances. We then concatenate a context of w utterances, marked by speaker tokens when there is a change in speaker, to obtain ctod . Finally, we encode ctod using the pretrained TOD-BERT model and concatenate it with the output of the dialogue encoder and talk move encoder. Equation 5 then becomes: rt = cat(bt , dt , TOD-BERT(ctod )) (7) We call this model 3-E-TOD-BERT. When pretrained sentence encoders are used, we use the respective BPE (Sennrich et al., 2016) tokenizer for each model. 4.2 Computing the Loss We train all models using a cross-entropy loss. However, we observe a strong class imbalance in our training data, cf. Figure 2. Thus, we compute label weights inversely proportional to the frequency of a label’s occurrence in the data and use them to weight the loss for each training example. 4743 5 Experiments 30000 Dataset 25000 5.2 Baselines We compare our model to three baselines. Random Baseline (RB) This baseline randomly selects one of the 8 talk moves for each input. Talk Move Bigram Model (TMBM) For this baseline, we compute the condi"
2021.findings-acl.418,J00-3003,0,0.798124,"Missing"
2021.findings-acl.418,P19-2027,0,0.0198107,"to talk moves, dialogue acts provide a categorization for utterances, but, in contrast to talk moves, they apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 2020). Analogous to FTMP, future dialogue act prediction is the task of predicting what the next dialogue act should be, given a conversation history (Tanaka et al., 2019). Pretrained models have been successfully adapted to the task of dialogue generation (Zhang et al., 2020; Wu et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). However, if directly used in the classroom, these models could potentially produce harmful or unsuitable dialogue as they are trained on large datasets comprising conversations from the internet (Bender et al., 2021). Additionally, we want a system to facilitate structured conversations, and not cause further diversions – this is in contrast to many task-oriented or open-domain dialogue systems whose purpose is to entertain a"
2021.findings-acl.418,2020.emnlp-main.66,0,0.33312,"the next talk move. We further propose a model for FTMP, which we call 3-E.1 It consists of three recurrent neural network (RNN) encoders: one for individual utterances, one for utterance sequences, and one for talk move sequences. The model is trained on transcripts of classroom discussions where teacher utterances have been annotated for the talk moves they represent. We consider the actions of the teacher to be our gold standard data for FTMP. We show that our model strongly outperforms multiple baselines and that adding sentence representations from RoBERTa (Liu et al., 2019) or TOD-BERT (Wu et al., 2020) – a model trained on task-oriented dialogue – does not increase performance further. Finally, we investigate the performance of human annotators on FTMP. Unlike the teacher, they do not have access to multi-modal signals, subject matter information, or knowledge of student behavior. This setting, which mimics the information available to our model, is significantly different from the teachers who generate the gold standard utterances captured in our data. We present a detailed analysis of their performance on a diagnostic test set, and highlight similarities to our model’s performance. Our fi"
2021.findings-acl.418,2020.acl-demos.30,0,0.0192988,"apply to generalpurpose conversations (Stolcke et al., 2000; Calhoun et al., 2010). Examples include Statement, Question, Greeting, and Apology. Dialogue act tagging, which is sometimes called dialogue act prediction, is the task of classifying an utterance into the category it belongs to (Yu and Yu, 2019; Khanpour et al., 2016; Wu et al., 2020). Analogous to FTMP, future dialogue act prediction is the task of predicting what the next dialogue act should be, given a conversation history (Tanaka et al., 2019). Pretrained models have been successfully adapted to the task of dialogue generation (Zhang et al., 2020; Wu et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). However, if directly used in the classroom, these models could potentially produce harmful or unsuitable dialogue as they are trained on large datasets comprising conversations from the internet (Bender et al., 2021). Additionally, we want a system to facilitate structured conversations, and not cause further diversions – this is in contrast to many task-oriented or open-domain dialogue systems whose purpose is to entertain and appear personable to the user. Hence, we propose FTMP as a crucial first step towards an NLP system ca"
2021.mtsummit-loresmt.11,2020.nlpcovid19-2.5,0,0.0582442,"Missing"
2021.mtsummit-loresmt.11,2021.mtsummit-loresmt.13,0,0.117853,"pated for English↔Marathi (see Table 3). All the teams who submitted their systems were invited to submit system description papers describing their experiments. Table 3 identifies the participating teams and their language choices. Team IIITT oneNLP-IIITH A3108 CFILT-IITBombay UCF adapt dcu Total English–Irish en2ga & ga2en — — — en2ga & ga2en en2ga 3 English–Marathi en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en — 5 TSign–TChinese — — — — — — 0 System Description Paper (Puranik et al., 2021) (Mujadia and Sharma, 2021) (Yadav and Shrivastava, 2021) (Jain et al., 2021) (Chen and Fazio, 2021) (Lankford et al., 2021) 6 Table 3: Details of the teams and submitted systems for the LoResMT 2021 Shared Task. Next, we give a short description of the approaches used by each team to build their systems. More details about the approaches can be found in the papers by respective teams in the accompanying proceeding. 16 https://github.com/loresmt/loresmt-2021 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 118 • IIITT (Puranik et al., 2021) used a fairseq pre-trained model Indictr"
2021.mtsummit-loresmt.11,2021.mtsummit-loresmt.17,0,0.144913,"three teams participated for English↔Marathi (see Table 3). All the teams who submitted their systems were invited to submit system description papers describing their experiments. Table 3 identifies the participating teams and their language choices. Team IIITT oneNLP-IIITH A3108 CFILT-IITBombay UCF adapt dcu Total English–Irish en2ga & ga2en — — — en2ga & ga2en en2ga 3 English–Marathi en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en — 5 TSign–TChinese — — — — — — 0 System Description Paper (Puranik et al., 2021) (Mujadia and Sharma, 2021) (Yadav and Shrivastava, 2021) (Jain et al., 2021) (Chen and Fazio, 2021) (Lankford et al., 2021) 6 Table 3: Details of the teams and submitted systems for the LoResMT 2021 Shared Task. Next, we give a short description of the approaches used by each team to build their systems. More details about the approaches can be found in the papers by respective teams in the accompanying proceeding. 16 https://github.com/loresmt/loresmt-2021 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 118 • IIITT (Puranik et al., 2021) used a fairseq pr"
2021.mtsummit-loresmt.11,W19-6800,0,0.0890573,"arathi. Unfortunately, there were no systems submissions for the Taiwanese Sign language↔Traditional Chinese task. Maximum system performance was computed using BLEU and follow as 36.0 for English–Irish, 34.6 for Irish–English, 24.2 for English–Marathi, and 31.3 for Marathi–English. 1 Introduction The workshop on technologies for machine translation of low resource languages (LoResMT)1 is a yearly workshop which focuses on scientific research topics and technological resources for machine translation (MT) using low-resource languages. Based on the success of its three predecessors (Liu, 2018; Karakanta et al., 2019, 2020), the fourth LoResMT workshop intoduces a shared task section based on COVID-19 and sign language data as part of its research objectives. The hope is to provide assistance with translation for low-resource languages where it could be needed most during the COVID-19 pandemic. 1 https://sites.google.com/view/loresmt/ Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 114 To provide a trajectory of the LoResMT shared task success, a summary of the previous tasks follows. The firs"
2021.mtsummit-loresmt.11,2021.mtsummit-loresmt.15,0,0.142417,"hi (see Table 3). All the teams who submitted their systems were invited to submit system description papers describing their experiments. Table 3 identifies the participating teams and their language choices. Team IIITT oneNLP-IIITH A3108 CFILT-IITBombay UCF adapt dcu Total English–Irish en2ga & ga2en — — — en2ga & ga2en en2ga 3 English–Marathi en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en — 5 TSign–TChinese — — — — — — 0 System Description Paper (Puranik et al., 2021) (Mujadia and Sharma, 2021) (Yadav and Shrivastava, 2021) (Jain et al., 2021) (Chen and Fazio, 2021) (Lankford et al., 2021) 6 Table 3: Details of the teams and submitted systems for the LoResMT 2021 Shared Task. Next, we give a short description of the approaches used by each team to build their systems. More details about the approaches can be found in the papers by respective teams in the accompanying proceeding. 16 https://github.com/loresmt/loresmt-2021 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 118 • IIITT (Puranik et al., 2021) used a fairseq pre-trained model Indictrans for EnglishMarathi."
2021.mtsummit-loresmt.11,2021.mtsummit-loresmt.16,0,0.130902,"Marathi tasks, one team participated for EnglishIrish and three teams participated for English↔Marathi (see Table 3). All the teams who submitted their systems were invited to submit system description papers describing their experiments. Table 3 identifies the participating teams and their language choices. Team IIITT oneNLP-IIITH A3108 CFILT-IITBombay UCF adapt dcu Total English–Irish en2ga & ga2en — — — en2ga & ga2en en2ga 3 English–Marathi en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en — 5 TSign–TChinese — — — — — — 0 System Description Paper (Puranik et al., 2021) (Mujadia and Sharma, 2021) (Yadav and Shrivastava, 2021) (Jain et al., 2021) (Chen and Fazio, 2021) (Lankford et al., 2021) 6 Table 3: Details of the teams and submitted systems for the LoResMT 2021 Shared Task. Next, we give a short description of the approaches used by each team to build their systems. More details about the approaches can be found in the papers by respective teams in the accompanying proceeding. 16 https://github.com/loresmt/loresmt-2021 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 11"
2021.mtsummit-loresmt.11,2020.loresmt-1.4,1,0.617027,"w/loresmt/ Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 114 To provide a trajectory of the LoResMT shared task success, a summary of the previous tasks follows. The first LoResMT shared task (Karakanta et al., 2019) took place in 2019. There, monolingual and parallel corpora for Bhojpuri, Magahi, Sindhi, and Latvian were provided as training data for two types of machine translation systems: neural and statistical. As an extension to the first shared task, a second shared task (Ojha et al., 2020) was presented in 2020 which focused on zero-shot approaches for MT systems. This year, the shared task introduces a new objective focused on MT systems for COVIDrelated texts and sign language. Participants for this shared task were asked to submit novel MT systems for the following language pairs: • English↔Irish • English↔Marathi • Taiwanese Sign Language↔Traditional Chinese The low-resource languages presented in this shared task were found to be sufficient data for baseline systems to perform translation on the latest COVID-related texts and sign language. Irish, Marathi, and Taiwanese Si"
2021.mtsummit-loresmt.11,P02-1040,0,0.109199,"ven the following markers for denotation: • “-a” - Only provided development, training and monolingual corpora. • “-b”- Any provided corpora, plus publicly available language’s corpora and pretrained/linguistic model (e.g. systems used pre-trained word2vec, UDPipe, etc. model). • “-c” - Any provided corpora, plus any publicly external monolingual corpora. Each team was allowed to submit any number of systems for evaluation and their best 3 systems were included in the final ranking presented in this report. Each submitted system was evaluated on standard automatic MT evaluation metrics; BLEU (Papineni et al., 2002), CHRF (Popovi´c, 2015) and TER (Post, 2018). The schedule for deliver of training data and release of test data along with notification and submission can be found in Table 1. Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 115 Date May 10, 2021 July 01, 2021 July 13, 2021 July 20, 2021 July 27, 2021 August 01, 2021 Event Release of training data Release of test data Submission of the systems Notification of results Submission of shared task papers Camera-ready Table 1: LoResMT 20"
2021.mtsummit-loresmt.11,W15-3049,0,0.0603677,"Missing"
2021.mtsummit-loresmt.11,W18-6319,0,0.0149118,"provided development, training and monolingual corpora. • “-b”- Any provided corpora, plus publicly available language’s corpora and pretrained/linguistic model (e.g. systems used pre-trained word2vec, UDPipe, etc. model). • “-c” - Any provided corpora, plus any publicly external monolingual corpora. Each team was allowed to submit any number of systems for evaluation and their best 3 systems were included in the final ranking presented in this report. Each submitted system was evaluated on standard automatic MT evaluation metrics; BLEU (Papineni et al., 2002), CHRF (Popovi´c, 2015) and TER (Post, 2018). The schedule for deliver of training data and release of test data along with notification and submission can be found in Table 1. Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 115 Date May 10, 2021 July 01, 2021 July 13, 2021 July 20, 2021 July 27, 2021 August 01, 2021 Event Release of training data Release of test data Submission of the systems Notification of results Submission of shared task papers Camera-ready Table 1: LoResMT 2021 Shared Task programming 3 Languages and d"
2021.mtsummit-loresmt.11,2021.mtsummit-loresmt.14,0,0.0289736,"lish↔Irish and English↔Marathi tasks, one team participated for EnglishIrish and three teams participated for English↔Marathi (see Table 3). All the teams who submitted their systems were invited to submit system description papers describing their experiments. Table 3 identifies the participating teams and their language choices. Team IIITT oneNLP-IIITH A3108 CFILT-IITBombay UCF adapt dcu Total English–Irish en2ga & ga2en — — — en2ga & ga2en en2ga 3 English–Marathi en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en en2mr & mr2en — 5 TSign–TChinese — — — — — — 0 System Description Paper (Puranik et al., 2021) (Mujadia and Sharma, 2021) (Yadav and Shrivastava, 2021) (Jain et al., 2021) (Chen and Fazio, 2021) (Lankford et al., 2021) 6 Table 3: Details of the teams and submitted systems for the LoResMT 2021 Shared Task. Next, we give a short description of the approaches used by each team to build their systems. More details about the approaches can be found in the papers by respective teams in the accompanying proceeding. 16 https://github.com/loresmt/loresmt-2021 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low"
2021.mtsummit-loresmt.11,P16-1009,0,0.0794747,"Missing"
2021.sigmorphon-1.12,2020.acl-main.598,1,0.801349,"with a character-level language model for weighing substring differences between words. Due to time constraints we are only able to cluster over a subset of each languages’ vocabulary. Despite of this, our system’s performance is comparable to the baseline. 2 Related Work Unsupervised morphology has attracted a great deal of interest historically, including a large body of work focused on segmentation (Xu et al., 2018; Creutz and Lagus, 2007; Poon et al., 2009; Narasimhan et al., 2015). Recently, the task of unsupervised morphological paradigm completion has been proposed (Kann et al., 2020; Jin et al., 2020; Erdmann et al., 2020), wherein the goal is to induce full paradigms from raw text corpora. In this year’s SIGMORPHON shared task, we are asked to only address part of the unsupervised paradigm completion task: paradigm clustering. Intuitively, the task of segmentation is related to paradigm clustering, but the outputs are different. Goldsmith (2001) produces morphological signatures, which are similar to approximate paradigms, based on an algorithm that uses minimum description length. However, this type of algorithm relies heavily on purely orthographic features of the vocabulary. Schone an"
2021.sigmorphon-1.12,2020.sigmorphon-1.3,1,0.840019,"text. We experiment with a character-level language model for weighing substring differences between words. Due to time constraints we are only able to cluster over a subset of each languages’ vocabulary. Despite of this, our system’s performance is comparable to the baseline. 2 Related Work Unsupervised morphology has attracted a great deal of interest historically, including a large body of work focused on segmentation (Xu et al., 2018; Creutz and Lagus, 2007; Poon et al., 2009; Narasimhan et al., 2015). Recently, the task of unsupervised morphological paradigm completion has been proposed (Kann et al., 2020; Jin et al., 2020; Erdmann et al., 2020), wherein the goal is to induce full paradigms from raw text corpora. In this year’s SIGMORPHON shared task, we are asked to only address part of the unsupervised paradigm completion task: paradigm clustering. Intuitively, the task of segmentation is related to paradigm clustering, but the outputs are different. Goldsmith (2001) produces morphological signatures, which are similar to approximate paradigms, based on an algorithm that uses minimum description length. However, this type of algorithm relies heavily on purely orthographic features of the voc"
2021.sigmorphon-1.12,Q17-1010,0,0.0863235,"dings (Mikolov et al., 2013) to find meaningful morphemes based on analogies: regularities exhibited by embedding spaces allow for inferences of certain types (e.g., king is to man what queen is to woman). Hypothesizing that these regularities also hold for morphological relations, they represent morphemes by vector differences between semantically similar forms, e.g., the vec− tor for the suffix → s may be represented by the −−→ −→ difference between cats and cat. Drawing upon these intuitions, we follow Rosa and Zabokrtsk´y (2019), which combines semantic distance using fastText embeddings (Bojanowski et al., 2017) with an orthographic distance between word pairs. Words are then clustered into paradigms using agglomerative clustering. 108 3 Task Description Given a raw text corpus, the task is to sort words into clusters that correspond to paradigms. More formally, for the vocabulary Σ of all types attested in the corpus and the set of morphological paradigms Π for which at least one word is in Σ, the goal T is to output clusters corresponding to πk Σ for all πk ∈ Π. Data As the raw text data for this task, JHU Bible corpora (McCarthy et al., 2020b) are provided by the organizers. This is the only data"
2021.sigmorphon-1.12,2020.acl-main.695,0,0.237912,"level language model for weighing substring differences between words. Due to time constraints we are only able to cluster over a subset of each languages’ vocabulary. Despite of this, our system’s performance is comparable to the baseline. 2 Related Work Unsupervised morphology has attracted a great deal of interest historically, including a large body of work focused on segmentation (Xu et al., 2018; Creutz and Lagus, 2007; Poon et al., 2009; Narasimhan et al., 2015). Recently, the task of unsupervised morphological paradigm completion has been proposed (Kann et al., 2020; Jin et al., 2020; Erdmann et al., 2020), wherein the goal is to induce full paradigms from raw text corpora. In this year’s SIGMORPHON shared task, we are asked to only address part of the unsupervised paradigm completion task: paradigm clustering. Intuitively, the task of segmentation is related to paradigm clustering, but the outputs are different. Goldsmith (2001) produces morphological signatures, which are similar to approximate paradigms, based on an algorithm that uses minimum description length. However, this type of algorithm relies heavily on purely orthographic features of the vocabulary. Schone and Jurafsky (2001) hypot"
2021.sigmorphon-1.12,W18-5806,0,0.0205131,"ir ends, which is where characters belonging to the stem are likely to be in suffixing languages. The JW distance is averaged with the JW distance of a simplified variant of the string. The simplified variant is a string that has been lower cased, transliterated to ASCII, and had the non-initial vowels deleted. This is done to soften the impact of characters that are likely to correspond with affixes. Crucially, we believe that this biases the system towards languages that express inflection via suffixation. Semantic Distance We represent words in the corpus by fastText embeddings, similar to Erdmann and Habash (2018), who cluster fastText embeddings for the same task in various Arabic dialects. We expect fastText embeddings to provide better representations than, e.g., Word2Vec (Mikolov et al., 2013), due to the limited size of the Bible corpora. Unfortunately, using fastText may also inadvertently result in higher similarity between words belonging to different lemmas that contain overlapping subwords corresponding to affixes. Overall Distance We compute a pairwise distance matrix for all words in the corpus. The distance between two words w1 and w2 is computed as: cos(wˆ1 , wˆ2 ) + 1 , 2 (1) where wˆ1 a"
2021.sigmorphon-1.12,J01-2001,0,0.0428556,"cluding a large body of work focused on segmentation (Xu et al., 2018; Creutz and Lagus, 2007; Poon et al., 2009; Narasimhan et al., 2015). Recently, the task of unsupervised morphological paradigm completion has been proposed (Kann et al., 2020; Jin et al., 2020; Erdmann et al., 2020), wherein the goal is to induce full paradigms from raw text corpora. In this year’s SIGMORPHON shared task, we are asked to only address part of the unsupervised paradigm completion task: paradigm clustering. Intuitively, the task of segmentation is related to paradigm clustering, but the outputs are different. Goldsmith (2001) produces morphological signatures, which are similar to approximate paradigms, based on an algorithm that uses minimum description length. However, this type of algorithm relies heavily on purely orthographic features of the vocabulary. Schone and Jurafsky (2001) hypothesize that approximating semantic information can help differentiate between hypothesized morphemes, revealing those that are productive. They propose an algorithm that combines orthography, semantics, and syntactic distributions to induce morphological relationships. They used semantic relatedness, quantified by latent semanti"
2021.sigmorphon-1.12,W19-4226,0,0.0522141,"observed in a given corpus. Lastly, unsupervised systems have the advantage of not needing annotated data, which can be costly in terms of time and money, or, in the case of extinct or endangered languages, entirely impossible. Since 2016, the Association for Computational Linguistics’ Special Interest Group on Computational Morphology and Phonology (SIGMORPHON) has created shared tasks to help spur the development of state-of-the-art systems to explicitly handle morphological processes in a language. These tasks have involved morphological inflection (Cotterell et al., 2016), lemmatization (McCarthy et al., 2019), as well as other, related tasks. SIGMORPHON has increased the level of difficulty of the shared tasks, largely along two dimensions. The first dimension is the amount of data available for models to learn, reflecting the difficulties of analyzing low-resource languages. The second dimension is the amount of structure provided in the input data. Initially, SIGMORPHON shared tasks provided predefined tables of lemmas, morphological tags, and inflected forms. For the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering, only raw text is provided as input. We propose a s"
2021.sigmorphon-1.12,2020.lrec-1.352,0,0.192739,"combines semantic distance using fastText embeddings (Bojanowski et al., 2017) with an orthographic distance between word pairs. Words are then clustered into paradigms using agglomerative clustering. 108 3 Task Description Given a raw text corpus, the task is to sort words into clusters that correspond to paradigms. More formally, for the vocabulary Σ of all types attested in the corpus and the set of morphological paradigms Π for which at least one word is in Σ, the goal T is to output clusters corresponding to πk Σ for all πk ∈ Π. Data As the raw text data for this task, JHU Bible corpora (McCarthy et al., 2020b) are provided by the organizers. This is the only data that systems can use. The organizers further provide development and test sets consisting of gold clusters for a subset of words in the Bible corpora. Each T cluster is a list of words representing πk Σ for πk ∈ Πdev or πk ∈ Πtest , respectively, and Πdev , Πtest ( Π. The partial morphological paradigms in Πdev and Πtest are taken from the UniMorph database (McCarthy et al., 2020a). Development sets are only available for the development languages, while test sets are only provided for the test languages. All test sets are hidden from th"
2021.sigmorphon-1.12,Q15-1012,0,0.0197996,"ropose a system that combines orthographic and semantic similarity measures to cluster surface forms found in raw text. We experiment with a character-level language model for weighing substring differences between words. Due to time constraints we are only able to cluster over a subset of each languages’ vocabulary. Despite of this, our system’s performance is comparable to the baseline. 2 Related Work Unsupervised morphology has attracted a great deal of interest historically, including a large body of work focused on segmentation (Xu et al., 2018; Creutz and Lagus, 2007; Poon et al., 2009; Narasimhan et al., 2015). Recently, the task of unsupervised morphological paradigm completion has been proposed (Kann et al., 2020; Jin et al., 2020; Erdmann et al., 2020), wherein the goal is to induce full paradigms from raw text corpora. In this year’s SIGMORPHON shared task, we are asked to only address part of the unsupervised paradigm completion task: paradigm clustering. Intuitively, the task of segmentation is related to paradigm clustering, but the outputs are different. Goldsmith (2001) produces morphological signatures, which are similar to approximate paradigms, based on an algorithm that uses minimum de"
2021.sigmorphon-1.12,N09-1024,0,0.0521104,"ided as input. We propose a system that combines orthographic and semantic similarity measures to cluster surface forms found in raw text. We experiment with a character-level language model for weighing substring differences between words. Due to time constraints we are only able to cluster over a subset of each languages’ vocabulary. Despite of this, our system’s performance is comparable to the baseline. 2 Related Work Unsupervised morphology has attracted a great deal of interest historically, including a large body of work focused on segmentation (Xu et al., 2018; Creutz and Lagus, 2007; Poon et al., 2009; Narasimhan et al., 2015). Recently, the task of unsupervised morphological paradigm completion has been proposed (Kann et al., 2020; Jin et al., 2020; Erdmann et al., 2020), wherein the goal is to induce full paradigms from raw text corpora. In this year’s SIGMORPHON shared task, we are asked to only address part of the unsupervised paradigm completion task: paradigm clustering. Intuitively, the task of segmentation is related to paradigm clustering, but the outputs are different. Goldsmith (2001) produces morphological signatures, which are similar to approximate paradigms, based on an algo"
2021.sigmorphon-1.12,W00-0712,0,0.306801,"ms, based on an algorithm that uses minimum description length. However, this type of algorithm relies heavily on purely orthographic features of the vocabulary. Schone and Jurafsky (2001) hypothesize that approximating semantic information can help differentiate between hypothesized morphemes, revealing those that are productive. They propose an algorithm that combines orthography, semantics, and syntactic distributions to induce morphological relationships. They used semantic relatedness, quantified by latent semantic analysis, combined with the frequencies of affixes and syntactic context (Schone and Jurafsky, 2000). More recently, Soricut and Och (2015) have used SkipGram word embeddings (Mikolov et al., 2013) to find meaningful morphemes based on analogies: regularities exhibited by embedding spaces allow for inferences of certain types (e.g., king is to man what queen is to woman). Hypothesizing that these regularities also hold for morphological relations, they represent morphemes by vector differences between semantically similar forms, e.g., the vec− tor for the suffix → s may be represented by the −−→ −→ difference between cats and cat. Drawing upon these intuitions, we follow Rosa and Zabokrtsk´y"
2021.sigmorphon-1.12,N01-1024,0,0.031031,"al., 2020; Erdmann et al., 2020), wherein the goal is to induce full paradigms from raw text corpora. In this year’s SIGMORPHON shared task, we are asked to only address part of the unsupervised paradigm completion task: paradigm clustering. Intuitively, the task of segmentation is related to paradigm clustering, but the outputs are different. Goldsmith (2001) produces morphological signatures, which are similar to approximate paradigms, based on an algorithm that uses minimum description length. However, this type of algorithm relies heavily on purely orthographic features of the vocabulary. Schone and Jurafsky (2001) hypothesize that approximating semantic information can help differentiate between hypothesized morphemes, revealing those that are productive. They propose an algorithm that combines orthography, semantics, and syntactic distributions to induce morphological relationships. They used semantic relatedness, quantified by latent semantic analysis, combined with the frequencies of affixes and syntactic context (Schone and Jurafsky, 2000). More recently, Soricut and Och (2015) have used SkipGram word embeddings (Mikolov et al., 2013) to find meaningful morphemes based on analogies: regularities ex"
2021.sigmorphon-1.12,D18-1315,0,0.0221025,"via concatenation. We thus experiment with removing the edit distance heuristics and, instead, utilizing probabilities from a character-level language model (LM) to distinguish between stems and affixes. In doing so, we hope to achieve better results for templatic languages, such as Maltese. We hypothesize that the LM will have a higher confidence for characters that are part of an affix than for those that are part of the stem. We then draw upon this hypothesis and weigh edit operations between two strings based on these confidences. LM-weighted Edit Distance Similar to the intuition behind Silfverberg and Hulden (2018), we train a character-level LM on the entire vocabulary for each Bible corpus. Unlike their work, we do not have inflectional tags for each word. Despite this, we hypothesize that the highly regular and frequent nature of inflectional affixes will lead to higher likelihoods for characters that occur in affixes than for those in stems. We train a two-layer LSTM (Hochreiter and Schmidhuber, 1997) with an embedding size of 128 and a hidden layer size of 128. We train the model until the training loss stops decreasing, for up to 100 epochs, using Adam (Kingma and Ba, 2014) with a learning rate of"
2021.sigmorphon-1.12,N15-1186,0,0.0303311,"description length. However, this type of algorithm relies heavily on purely orthographic features of the vocabulary. Schone and Jurafsky (2001) hypothesize that approximating semantic information can help differentiate between hypothesized morphemes, revealing those that are productive. They propose an algorithm that combines orthography, semantics, and syntactic distributions to induce morphological relationships. They used semantic relatedness, quantified by latent semantic analysis, combined with the frequencies of affixes and syntactic context (Schone and Jurafsky, 2000). More recently, Soricut and Och (2015) have used SkipGram word embeddings (Mikolov et al., 2013) to find meaningful morphemes based on analogies: regularities exhibited by embedding spaces allow for inferences of certain types (e.g., king is to man what queen is to woman). Hypothesizing that these regularities also hold for morphological relations, they represent morphemes by vector differences between semantically similar forms, e.g., the vec− tor for the suffix → s may be represented by the −−→ −→ difference between cats and cat. Drawing upon these intuitions, we follow Rosa and Zabokrtsk´y (2019), which combines semantic distan"
2021.sigmorphon-1.12,D18-1268,0,0.0249249,"aradigm Clustering, only raw text is provided as input. We propose a system that combines orthographic and semantic similarity measures to cluster surface forms found in raw text. We experiment with a character-level language model for weighing substring differences between words. Due to time constraints we are only able to cluster over a subset of each languages’ vocabulary. Despite of this, our system’s performance is comparable to the baseline. 2 Related Work Unsupervised morphology has attracted a great deal of interest historically, including a large body of work focused on segmentation (Xu et al., 2018; Creutz and Lagus, 2007; Poon et al., 2009; Narasimhan et al., 2015). Recently, the task of unsupervised morphological paradigm completion has been proposed (Kann et al., 2020; Jin et al., 2020; Erdmann et al., 2020), wherein the goal is to induce full paradigms from raw text corpora. In this year’s SIGMORPHON shared task, we are asked to only address part of the unsupervised paradigm completion task: paradigm clustering. Intuitively, the task of segmentation is related to paradigm clustering, but the outputs are different. Goldsmith (2001) produces morphological signatures, which are similar"
2021.sigmorphon-1.8,K17-2001,1,0.900551,"Missing"
2021.sigmorphon-1.8,W16-2002,1,0.895884,"Missing"
2021.sigmorphon-1.8,N13-1138,0,0.0713896,"Missing"
2021.sigmorphon-1.8,2020.acl-main.695,1,0.883705,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.12,1,0.8311,"Missing"
2021.sigmorphon-1.8,Q17-1010,0,0.106213,"Missing"
2021.sigmorphon-1.8,2020.acl-main.598,1,0.881267,"Missing"
2021.sigmorphon-1.8,K18-3001,1,0.886365,"Missing"
2021.sigmorphon-1.8,2020.lrec-1.497,0,0.0616438,"Missing"
2021.sigmorphon-1.8,2020.sigmorphon-1.3,1,0.815511,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.10,0,0.061232,"Missing"
2021.sigmorphon-1.8,2020.acl-demos.14,0,0.0404485,"Missing"
2021.sigmorphon-1.8,W19-4226,1,0.902221,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.11,1,0.696729,"Missing"
2021.sigmorphon-1.8,2020.lrec-1.352,1,0.814695,"Missing"
2021.sigmorphon-1.8,2021.sigmorphon-1.9,0,0.0885046,"Missing"
D16-1097,W14-4012,0,0.188779,"Missing"
D16-1097,D14-1179,0,0.0413197,"Missing"
D16-1097,P11-1004,0,0.1291,"Missing"
D16-1097,K15-1017,1,0.890809,"Missing"
D16-1097,N16-1080,1,0.852389,"Missing"
D16-1097,W02-0603,0,0.445604,"el’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved"
D16-1097,D08-1113,0,0.059286,"tation. Baselines. Our first baseline is the joint transduction and segmentation model (JOINT) of Cotterell et al. (2016). It is the current state of the art on the datasets we use and the task of canonical segmentation in general. This model uses a jointly trained, separate transduction and segmentation component. Importantly, the joint model of Cotterell et al. (2016) already contains segment-level features. Thus, reranking this baseline would not provide a similar boost. Our second baseline is a weighted finite-state transducer (WFST) (Mohri et al., 2002) with a loglinear parameterization (Dreyer et al., 2008), again, taken from Cotterell et al. (2016). The WFST baseline is particularly relevant because, like our encoder-decoder, it formulates the problem directly as a string-to-string transduction. Evaluation Metrics. We follow Cotterell et al. (2016) and use the following evaluation measures: error rate, edit distance and morpheme F1 . Error rate is defined as 1 minus the proportion of guesses that are completely correct. Edit distance is the Levenshtein distance between guess and gold standard. For this, guess and gold are each represented as one string with a distinguished character denoting th"
D16-1097,J01-2001,0,0.0390897,"raves et al., 2013). 4 Experiments k∈Sw The reranking model’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance wit"
D16-1097,P16-2090,1,0.887592,"Missing"
D16-1097,W10-2210,0,0.569431,"e reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morphological segmentation using a window LSTM. Even though Wang et al. (2016) also employ"
D16-1097,N16-3005,0,0.0128112,"3). 4 Experiments k∈Sw The reranking model’s ability to embed morphemes is important for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More"
D16-1097,D14-1095,0,0.268843,"Missing"
D16-1097,N09-1024,0,0.209303,"for morphological segmentation since we often have strong corpus-level signals. The reranker also takes into account the characterlevel information through the score of the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morpho"
D16-1097,W13-3504,0,0.319601,"the encoderdecoder model. Due to this combination we expect stronger performance. 3 Related Work Various approaches to morphological segmentation have been proposed in the literature. In the unsupervised realm, most work has been based on the principle of minimum description length (Cover and Thomas, 2012), e.g., L INGUISTICA (Goldsmith, 2001; Lee and Goldsmith, 2016) or M ORFESSOR (Creutz and Lagus, 2002; Creutz et al., 2007; Poon et al., 2009). M ORFESSOR was later extended to a semi-supervised version by Kohonen et al. (2010). Supervised approaches have also been considered. Most notably, Ruokolainen et al. (2013) developed a supervised approach for morphological segmentation based on conditional random fields (CRFs) which they later extended to work also in a semisupervised way (Ruokolainen et al., 2014) using letter successor variety features (Hafer and Weiss, 1974). Similarly, Cotterell et al. (2015) improved performance with a semi-Markov CRF. More recently, Wang et al. (2016) achieved stateof-the-art results on surface morphological segmentation using a window LSTM. Even though Wang et al. (2016) also employ a recurrent neural network, we distinguish our approach, in that we focus on canonical mor"
D16-1097,Q15-1026,0,0.180686,"Missing"
D16-1097,P13-1118,0,0.0302509,"Missing"
D18-1363,W16-2007,0,0.0158399,"easonably accurate if no better alternative is available; consider the following SBJV;PST;2;PL ! IND;PST;1;SG;PFV generations: “parlassiez” 7! “parlai”, “finissiez” 7! “finis”, “missiez” 7! “mis”, “prissiez” 7! “pris”. We thus conclude that SHIP indeed learns to select appropriate source forms. 6 Related Work Morphological generation. In the last two years, most work on paradigm completion has been done in the context of the SIGMORPHON 2016 and the CoNLL–SIGMORPHON 2017 shared tasks (Cotterell et al., 2016, 2017a). Due to the success of neural seq2seq models in 2016 (Kann and Sch¨utze, 2016b; Aharoni et al., 2016), systems developed for the 2017 edition were mostly neural (Makarov et al., 2017; Bergmanis et al., 2017; Zhou and Neubig, 2017). Besides the shared task systems, Kann and Sch¨utze (2017) presented a paradigm completion model for a multi-source setting that made use of an attention mechanism to decide which input form to attend to at each time step. They used randomly chosen, independent pairs of source and target forms for training. This differs crucially from the setting we consider in that no complete paradigms were available in their training sets. Only Cotterell et al. (2017b) addressed"
D18-1363,E14-1060,0,0.135897,"ed, e.g., semi-supervised training (Zhou and Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is sim"
D18-1363,E17-1049,1,0.875012,"Missing"
D18-1363,K17-2002,1,0.915766,"Missing"
D18-1363,P17-1182,1,0.853174,"Missing"
D18-1363,P17-1031,0,0.0223035,"eural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptation, e.g., in machine translation (Luong and Manning, 2015). One difference is that training set and test set can hardly be called different domains in paradigm completion. Another difference is that explicit structured labels (the morphological tags of the forms in the input subset) are available at test time in paradigm completion. 7 Conclusion We presented two new methods for minimalresource paradigm completion: paradigm transduction and SHIP. Paradigm transduction learns general inflection ru"
D18-1363,W14-4012,0,0.15631,"Missing"
D18-1363,P16-2090,1,0.87624,"Missing"
D18-1363,chrupala-etal-2008-learning,0,0.065546,"Missing"
D18-1363,K17-2001,0,0.678061,",st 1 ,ct ) (5) t=1 st denotes the state of the decoder at step t, and ct is the sum of the hidden representations of the encoder, weighted by an attention mechanism. Additional background on the general model architecture is given in Bahdanau et al. (2015); details on MED can be found in Kann and Sch¨utze (2016b). 3.2 where A is a set of autoencoding examples, e✓ is the encoder, and D is a labeled training set of tuples of source s, morphological source tag tS , morphological target tag tT , and target w. Semi-supervised MED In order to make use of unlabeled data with MED, Kann and Sch¨utze (2017) defined an auxiliary autoencoding task and proposed a multi-task learning approach. For this extension, an additional symbol is added to the input vocabulary. Each input is then of the form (A |M + ) ⌃+ , with A being a novel tag for autoencoding, ⌃ being the alphabet of the language, and M being the set of morphological subtags of the source and the target. As for the basic MED model, all parts of the input are represented by embeddings. The training objective is to maximize the joint likelihood for the tasks of paradigm completion and autoencoding: P S T L(✓) = (s,tS ,tT ,w)2D logp✓ (w |e✓"
D18-1363,E17-2120,0,0.489715,"the lemma’s paradigm. In this work, we address paradigm completion (PC), the morphological task of, given a partial paradigm of a lemma, generating all of its missing forms. For the partial paradigm represented by the input subset {(“Schneemannes”, GEN;SG), (“Schneem¨annern”, DAT;PL)} of the German noun “Schneemann” shown in Figure 1, the goal of PC is to generate the output subset consisting of the six remaining forms. Neural seq2seq models define the state of the art for morphological generation if training sets are large; however, they have been less successful in the low-resource setting (Cotterell et al., 2017a). In this paper, we address an even more extreme minimalresource setting: for some of our experiments, our training sets only contain k ⇡ 10 paradigms. Each paradigm has multiple cells, so the number of forms (as opposed to the number of paradigms) is not necessarily minimal. However, we will see that generalizing from paradigm to paradigm is a key challenge, making the number of paradigms a good measure of the effective training set size. We propose two PC methods for the minimalresource setting: paradigm transduction and source selection with high precision (SHIP). We define a learning alg"
D18-1363,D08-1113,0,0.023465,"cal generation with limited data have been proposed, e.g., semi-supervised training (Zhou and Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Ni"
D18-1363,N13-1138,0,0.0363807,"mited data have been proposed, e.g., semi-supervised training (Zhou and Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transdu"
D18-1363,N16-1077,0,0.0331872,"Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptation, e.g., in machine tr"
D18-1363,W17-4111,1,0.868442,"Missing"
D18-1363,L18-1293,0,0.0600656,"jections (cf. Figure 4). We then interpret the weight of an edge as a measure of the (un)reliability of the corresponding two source-target relationships. Our intuition is that the fewer different edit trees relate source and target, the more reliable the source is for generating the target. At test time, we find for each target tj a source tk such that nkj  nij 8i 2 J(w). We then use fk [w] to generate fj [w]. Again, Figure 4 shows examples. 4 4.1 Experiments Data We run experiments on the datasets from task 2 of the CoNLL–SIGMORPHON 2017 shared task, which have been created using UniMorph (Kirov et al., 2018). We give a short overview here; see (Cotterell et al., 2017a) for details. The dataset contains, for each of 52 languages, a development set of 50 partial paradigms, a test set of 50 partial paradigms, and three training sets of complete paradigms. Training set sizes are 10 (SET1), 50 (SET2), and 200 (SET3). Recall that we view the number of paradigms (not the number of forms) as the best measure of the amount of training data available. Even for SET3, there are only 200 lemmas per language in the training set, which are additionally distributed over multiple POS tags, compared to &gt;600 lemmas"
D18-1363,W16-2006,0,0.0169763,"y little 3258 BL: COPY BL: MED BL: PT BL: SIG17 SIG17+SHIP MED+PT MED+PT+SHIP SET1 .0810 .0004 .0833 .5012 .5971 .5808 .5793 SET2 .0810 .0432 .0833 .6576 .7355 .7486 .7547 SET3 .0810 .4211 .0775 .7707 .8008 .8454 .8483 Table 1: Accuracy on PC for SIG17+SHIP (the shared task baseline SIG17 with SHIP), MED+PT (MED with paradigm transduction), MED+PT+SHIP (MED with paradigm transduction and SHIP), as well as all baselines (BL). Results are averaged over all languages, and best results are in bold; detailed accuracies for all languages can be found in Appendix A. training data. Its design follows Liu and Mao (2016): SIG17 first aligns each input lemma and output inflected form. Afterwards, it assumes that each aligned pair can be split into a prefix, a stem, and a suffix. Based on this alignment, the system extracts prefix (resp. suffix) rules from the prefix (resp. suffix) pairings. At test time, suitable rules are applied to the input string to generate the target; more details can be found in Cotterell et al. (2017a). 4.4 Results Our results are shown in Table 1. For SET1, SIG17+SHIP obtains the highest accuracy, while, for SET2 and SET3, MED+PT+SHIP performs best. This difference can be easily expla"
D18-1363,2015.iwslt-evaluation.11,0,0.0408532,". (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptation, e.g., in machine translation (Luong and Manning, 2015). One difference is that training set and test set can hardly be called different domains in paradigm completion. Another difference is that explicit structured labels (the morphological tags of the forms in the input subset) are available at test time in paradigm completion. 7 Conclusion We presented two new methods for minimalresource paradigm completion: paradigm transduction and SHIP. Paradigm transduction learns general inflection rules through standard inductive training and idiosyncracies of a test paradigm through transduction. We showed that paradigm transduction effectively mitigates"
D18-1363,N15-1093,0,0.0417257,"sed training (Zhou and Neubig, 2017; Kann and Sch¨utze, 2017) or simultaneous training on multiple languages (Kann et al., 2017b). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set. Other important work on morphological generation—neural and non-neural—includes Dreyer et al. (2008); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptati"
D18-1363,P17-2014,0,0.0293957,"8); Durrett and DeNero (2013); Hulden et al. (2014); Nicolai et al. (2015); Faruqui et al. (2016); Yin et al. (2016). Seq2seq models in NLP. Even though neural seq2seq models were originally designed for machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing (Vinyals et al., 2015), language correction (Xie et al., 2016), normalization of historical texts (Bollmann et al., 2017), or text simplification (Nisioi et al., 2017). Transductive inference is similar to domain adaptation, e.g., in machine translation (Luong and Manning, 2015). One difference is that training set and test set can hardly be called different domains in paradigm completion. Another difference is that explicit structured labels (the morphological tags of the forms in the input subset) are available at test time in paradigm completion. 7 Conclusion We presented two new methods for minimalresource paradigm completion: paradigm transduction and SHIP. Paradigm transduction learns general inflection rules through standard inductive training and id"
D18-1363,L16-1497,0,0.110751,"Completion In this section, we formally define our task, developing the notation for the rest of the paper. Given the set of morphological tags T (w) of a lemma w, we define the paradigm of w as the set of tuples of inflected form fk and tag tk : ⇡(w) = fk [w],tk tk 2T (w) (1) The example in Figure 1 thus corresponds to: ⇡(Schneemann) = “Schneemann”, NOM;SG . . . “Schneem¨anner”, ACC;PL . A training set in our setup consists of complete paradigms, i.e., all inflected forms of each lemma are available. This simulates a setting in which a linguist annotates complete paradigms, as done, e.g., in Sylak-Glassman et al. (2016). In contrast, each element of the test set is a partial paradigm, which we refer to as the input subset. This simulates a setting in which we collect all forms of a lemma occurring in a (manually or automatically) annotated input corpus; this set will generally not be complete. The PC task consists of generating the output subset of the paradigm, i.e., the forms belonging to form-tag pairs which are missing from the collected subset. 3 Method Our approach for PC is based on MED (Morphological Encoder-Decoder), a state-of-the-art model for morphological generation in the high-resource case, wh"
D18-1363,W17-4704,0,0.0338536,"Missing"
D18-1363,Q16-1019,1,0.873013,"Missing"
D18-1363,P17-1029,0,0.0408664,"parlassiez” 7! “parlai”, “finissiez” 7! “finis”, “missiez” 7! “mis”, “prissiez” 7! “pris”. We thus conclude that SHIP indeed learns to select appropriate source forms. 6 Related Work Morphological generation. In the last two years, most work on paradigm completion has been done in the context of the SIGMORPHON 2016 and the CoNLL–SIGMORPHON 2017 shared tasks (Cotterell et al., 2016, 2017a). Due to the success of neural seq2seq models in 2016 (Kann and Sch¨utze, 2016b; Aharoni et al., 2016), systems developed for the 2017 edition were mostly neural (Makarov et al., 2017; Bergmanis et al., 2017; Zhou and Neubig, 2017). Besides the shared task systems, Kann and Sch¨utze (2017) presented a paradigm completion model for a multi-source setting that made use of an attention mechanism to decide which input form to attend to at each time step. They used randomly chosen, independent pairs of source and target forms for training. This differs crucially from the setting we consider in that no complete paradigms were available in their training sets. Only Cotterell et al. (2017b) addressed essentially the same task we do, but they only considered the high-resource setting: their models were trained on hundreds of com"
D19-1329,P17-1183,0,0.0266486,", while staying as close to the original pronunciation as possible. Unlike for translation, focus lies on the sound; the target language meaning is usually ignored. Data. For our transliteration experiments, we follow Upadhyay et al. (2018). We experiment on datasets from the Named Entities Workshop 2015 (Duan et al., 2015) in Hindi, Kannada, Bengali, Tamil, and Hebrew. For this task, all languages are both development and target languages. Model. The last featured model is an LSTM sequence-to-sequence model similar to that by Bahdanau et al. (2015), except for using hard monotonic attention (Aharoni and Goldberg, 2017). It attends to a single character at a time, and attention moves monotonically over the input. We take hyperparameters and code from Upadhyay et al. (2018).5 Early stopping is done by training for 20 epochs and applying the best model regarding development accuracy to the test data. 4.4 Experimental Setup We run all experiments using the implementations from previous work or OpenNMT as described above. Existing code is only modified where necessary. Most importantly, we add storing of the DevLang model during the main training phase. 5 Results Development sets vs. development languages. We ar"
D19-1329,P17-1031,0,0.0565016,"to compare between tasks, we further limit this study to sequence-to-sequence tasks. 4.1 Historical Text Normalization (NORM) Task. The goal of historical text normalization is to convert old texts into a form that conforms with contemporary spelling conventions. Historical text normalization is a specific case of the general task of text normalization, which additionally encompasses, e.g., correction of spelling mistakes or normalization of social media text. Data. We experiment on the ten datasets from Bollmann et al. (2018), which represent eight different languages: German (two datasets; Bollmann et al., 2017; Odebrecht et al., 2017); English, Hungarian, Icelandic, and Swedish (Pettersson, 2016); Slovene (two datasets; Ljubeˇsic et al., 2016); and Spanish and Portuguese (Vaamonde, 2015). We treat the two datasets for German and Slovene as different languages. All languages serve both as development languages for all other languages and as target languages. Model. Our model for this task is an LSTM (Hochreiter and Schmidhuber, 1997) encoderdecoder model with attention (Bahdanau et al., 2015). Both encoder and decoder have a single hidden layer. We use the default model in OpenNMT (Klein et al., 201"
D19-1329,W18-3403,0,0.284664,"nd multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization (Bollmann et al., 2018), morphological segmentation (Kann et al., 2018), morphological inflection (Makarov and Clematide, 2018; Sharma et al., 2018), argument component identification (Schulz et al., 2018), and transliteration (Upadhyay et al., 2018). 3342 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3342–3349, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics However, in a real-world setting with limited resources, it is unlikely that such a development set"
D19-1329,K18-3001,1,0.837587,"ding development accuracy is applied to the test set. 4.2 Morphological Inflection (MORPH) Task. Morphological inflection consists of mapping the canonical form of a word, the lemma, to an indicated inflected form. This task gets very complex for morphologically rich languages, where a single lemma can have hundreds or thousand of inflected forms. Recently, morphological inflection has frequently been cast as a sequenceto-sequence task, mapping the characters of the input word together with the morphological features specifying the target to the characters of the corresponding inflected form (Cotterell et al., 2018). Data. We experiment on the datasets released for a 2018 shared task (Cotterell et al., 2018), which cover 103 languages and feature an explicit low-resource setting. We randomly choose ten development languages: Armenian, Basque, Galician, Georgian, Greenlandic, Icelandic, Karbadian, Kannada, Latin, and Lithuanian. Model. For MORPH, we experiment with a pointer-generator network architecture (Gu et al., 2016; See et al., 2017). This is a sequence-tosequence model similar to that for NORM, but employs separate encoders for characters and features. It is further equipped with a copy mechanism:"
D19-1329,P07-1033,0,0.202739,"Missing"
D19-1329,N19-1423,0,0.0128989,"er, since T is finite, overfitting the training set might lead to poor generalization performance. One way to avoid fitting Equation 1 too closely is early stopping: a separate development or validation set is used to end training as soon as the loss on the development set LD (θ) starts increasing or model performance on the development set D starts decreasing. The best set of parameters θ is used in the final model. This works well when large amounts of data are available to create training, development and test splits. Recently, however, with the success of pretraining (Peters et al., 2018; Devlin et al., 2019) and multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization"
D19-1329,P13-1057,0,0.0892694,"Oliver et al. (2018) investigate how to evaluate semi-supervised training algorithms in a realistic way; they differ from us in that they focus exclusively on semi-supervised learning (SSL) algorithms, and do not consider NLP explicitly. However, in line with our conclusion, they report that recent practices for evaluating SSL techniques do not address the question of the algorithms’ real-word applicability in a satisfying way. In NLP, several earlier works have explicitly investigated real-world low-resource settings as opposed to artificial proxy settings, e.g., for part-of-speech tagging (Garrette et al., 2013) or machine translation (Irvine and Callison-Burch, 2013). While those mostly focus on real data-poor languages, we explicitly investigate the effect of the common practice to assume a relatively large development set for early stopping in the low-resource setting. Low-resource settings in NLP. Research in the area of neural methods for low-resource NLP has gained popularity in recent years, with a dedicated Experimental Design • Main training phase. We train models for all languages keeping both the model resulting from the original early stopping strategy (DevSet) and that from the epoch com"
D19-1329,P16-1154,0,0.02812,"ceto-sequence task, mapping the characters of the input word together with the morphological features specifying the target to the characters of the corresponding inflected form (Cotterell et al., 2018). Data. We experiment on the datasets released for a 2018 shared task (Cotterell et al., 2018), which cover 103 languages and feature an explicit low-resource setting. We randomly choose ten development languages: Armenian, Basque, Galician, Georgian, Greenlandic, Icelandic, Karbadian, Kannada, Latin, and Lithuanian. Model. For MORPH, we experiment with a pointer-generator network architecture (Gu et al., 2016; See et al., 2017). This is a sequence-tosequence model similar to that for NORM, but employs separate encoders for characters and features. It is further equipped with a copy mechanism: using attention to decide on what element from the input sequence to copy, the model computes a probability for either copying or generation while producing an output. The final probability distribution over the target vocabulary is a combination of both. Hyperparameters are taken from Sharma et al. (2018).4 For early stopping, we also follow Sharma et al. (2018): all models are trained 3 github.com/OpenNMT/O"
D19-1329,W18-3400,0,0.187709,"Missing"
D19-1329,W13-2233,0,0.0304037,"te semi-supervised training algorithms in a realistic way; they differ from us in that they focus exclusively on semi-supervised learning (SSL) algorithms, and do not consider NLP explicitly. However, in line with our conclusion, they report that recent practices for evaluating SSL techniques do not address the question of the algorithms’ real-word applicability in a satisfying way. In NLP, several earlier works have explicitly investigated real-world low-resource settings as opposed to artificial proxy settings, e.g., for part-of-speech tagging (Garrette et al., 2013) or machine translation (Irvine and Callison-Burch, 2013). While those mostly focus on real data-poor languages, we explicitly investigate the effect of the common practice to assume a relatively large development set for early stopping in the low-resource setting. Low-resource settings in NLP. Research in the area of neural methods for low-resource NLP has gained popularity in recent years, with a dedicated Experimental Design • Main training phase. We train models for all languages keeping both the model resulting from the original early stopping strategy (DevSet) and that from the epoch computed in the stopping point selection phase (DevLang).2 T"
D19-1329,P17-1182,1,0.882068,"Missing"
D19-1329,N18-1005,1,0.903263,"Missing"
D19-1329,P17-4012,0,0.0215587,"mann et al., 2017; Odebrecht et al., 2017); English, Hungarian, Icelandic, and Swedish (Pettersson, 2016); Slovene (two datasets; Ljubeˇsic et al., 2016); and Spanish and Portuguese (Vaamonde, 2015). We treat the two datasets for German and Slovene as different languages. All languages serve both as development languages for all other languages and as target languages. Model. Our model for this task is an LSTM (Hochreiter and Schmidhuber, 1997) encoderdecoder model with attention (Bahdanau et al., 2015). Both encoder and decoder have a single hidden layer. We use the default model in OpenNMT (Klein et al., 2017)3 as our implementation and employ the hyperparameters from Bollmann et al. (2018). In the original paper, early stopping is done by training for 50 epochs, and the best model regarding development accuracy is applied to the test set. 4.2 Morphological Inflection (MORPH) Task. Morphological inflection consists of mapping the canonical form of a word, the lemma, to an indicated inflected form. This task gets very complex for morphologically rich languages, where a single lemma can have hundreds or thousand of inflected forms. Recently, morphological inflection has frequently been cast as a sequ"
D19-1329,D18-1314,0,0.0237466,"re showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization (Bollmann et al., 2018), morphological segmentation (Kann et al., 2018), morphological inflection (Makarov and Clematide, 2018; Sharma et al., 2018), argument component identification (Schulz et al., 2018), and transliteration (Upadhyay et al., 2018). 3342 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3342–3349, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics However, in a real-world setting with limited resources, it is unlikely that such a development set would be available for early stopping, since it would be more effective to use at least part of it for"
D19-1329,N18-1202,0,0.0117913,"examples in T . However, since T is finite, overfitting the training set might lead to poor generalization performance. One way to avoid fitting Equation 1 too closely is early stopping: a separate development or validation set is used to end training as soon as the loss on the development set LD (θ) starts increasing or model performance on the development set D starts decreasing. The best set of parameters θ is used in the final model. This works well when large amounts of data are available to create training, development and test splits. Recently, however, with the success of pretraining (Peters et al., 2018; Devlin et al., 2019) and multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of histori"
D19-1329,N18-2006,0,0.0573727,"n Cho and Samuel R. Bowman New York University, USA {kann, kyunghyun.cho, bowman}@nyu.edu Abstract # train Table 1: Number of examples used for training and development in recent low-resource NLP experiments; ES=early stopping on the development set. Experiments from papers in bold will be revisited here. Introduction Parametric machine learning models are frequently trained by minimizing the loss on the training set T , LT (θ) = X l (θ, x) , ES Bollmann et al. (2018) 5k 12k-46k Yes Kann et al. (2018) 400-700 100-200 Yes Makarov and Clematide (2018) 100 1k Yes Sharma et al. (2018) 100 100 Yes Schulz et al. (2018) 1k-21k 9k N/A Upadhyay et al. (2018) 500 1k Yes Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions: Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does it l"
D19-6123,W18-3020,0,0.0143063,"be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are in"
D19-6123,D16-1073,0,0.0204782,"rmance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019)"
D19-6123,P17-2021,0,0.0709194,"Missing"
D19-6123,P19-1234,0,0.0156458,"ve been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cross-lingual transfer and difference in hyper parameters. In this work, we focus on adaptSetup Since transfer learning is mostly needed and also particularly effective in the low-data regime, we combine the training sets of 2,500 examples for English and German—the two of our languages which are related—to form a multilingual training set. We then train PRPN models on this combined training set, sharing all para"
D19-6123,N19-1388,0,0.0324253,"al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages"
D19-6123,W18-2704,0,0.0249191,"arsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words"
D19-6123,P06-1109,0,0.0575599,"ervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ"
D19-6123,W01-0713,0,0.232608,"mobile devices. Second, neural models which have been trained simultaneously on multiple languages have been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and s"
D19-6123,P17-1182,1,0.8623,"Missing"
D19-6123,P19-1228,0,0.221934,"Missing"
D19-6123,D18-1269,1,0.821128,"ask in another (usually low-resource) language, is very common when working on resource-poor languages in NLP. There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsi"
D19-6123,N19-1114,0,0.122249,"erarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 209–218 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 tion? From a practical NLP perspective, grammar induction enables us to obtain syntactic information without labeled data, i.e., even in lowresource settings and for resource-poor languages. This information can then be of help for downstream tasks like machine translation"
D19-6123,D16-1001,0,0.0169883,"lliams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investigated the behavior of th"
D19-6123,P02-1017,0,0.435434,"odels, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo),"
D19-6123,N19-1116,0,0.0197397,"nd Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cros"
D19-6123,P04-1061,0,0.16553,"ave been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates"
D19-6123,D18-1543,0,0.0392102,"Missing"
D19-6123,P03-1013,0,0.0582167,"splits: the development and test sets consist of 300 files each, while the training set consists of the remaining 2407. Balanced trees baseline (BTB). Finally, this baseline is similar to LBR/RBR, but considering balanced binary trees, which are created by recursively splitting each span into halves. For odd lengths, the middle word becomes a part of the right subtree. German. We use the NEGRA corpus (Skut et al., 1997), which consists of approximately 350, 000 words of German newspaper text (20,602 sentences). We divide the dataset into training, development, and test splits as suggested by Dubey and Keller (2003). 4 4.1 Monolingual Experiments Language-Dependence of Hyperparameters Best binary tree upper bound (BB). Since our datasets contain n-ary trees, but the PRPN only produces binary trees, obtaining a perfect F1 score is impossible. This upper bound represents the best score which can be obtained with binary trees. It is of practical importance to know whether a set of hyperparameters found for one language transfers to another one without any changes, especially for low-resource language without annotated (development) data. Therefore, we ask the following questions: (i) Do hyperparameters depe"
D19-6123,P19-1227,0,0.0247975,"ces for 12,500 examples and for the entire training sets for all languages. Thus, we conclude that disposing of more than 12,500 examples is generally beneficial. 5 0.186 (.08) 0.326 (.02) Results Results for single-language models as well as the multilingual versions are shown in Table 3. The parsing performance of the multilingual model is 215 ing knowledge gained from one (usually highresource) language for solving a task in another (usually low-resource) language, is very common when working on resource-poor languages in NLP. There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et"
D19-6123,D18-1162,0,0.0249036,"Missing"
D19-6123,D11-1118,0,0.0309377,"ted languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gatin"
D19-6123,P17-1076,0,0.0617362,"ist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investigated the behavior of the PRPN, a neural unsupervised constituency parsing model, for the languages Arabic, Chinese, English, and German. While, overall, our experiment"
D19-6123,Q18-1019,1,0.795528,"suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 201"
D19-6123,W19-4226,0,0.146783,"s needed for a set of languages, less memory is required to store all parameters. This facilitates, for instance, the application on mobile devices. Second, neural models which have been trained simultaneously on multiple languages have been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following t"
D19-6123,J97-3002,0,0.261135,"Missing"
D19-6123,N18-4013,1,0.832279,"serve whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figu"
D19-6123,P12-1066,0,0.0262921,"been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classificati"
D19-6123,H01-1035,0,0.216396,"Missing"
D19-6123,P92-1017,0,0.57111,"r grammar induction—i.e., models, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for L"
D19-6123,N19-1380,0,0.0180437,"There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to o"
D19-6123,P13-1043,0,0.02848,"be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investi"
D19-6123,P18-1108,0,0.207133,",416 18,598 1,000 1,000 see it as the supervised approach which is most comparable to the PRPN. Since our model does not predict labels which are used to recover nary trees in the original work, we compute the F1 score for this approach only with respect to binary gold trees. This is acceptable for our purposes, since we are interested in the supervised parser upper bound only to get an idea of the difficulty of the datasets in our different languages. For the supervised SUB baseline, we use hidden state and embedding dimensions of 100 and 300, respectively, and keep the default settings from Shen et al. (2018b) for all other hyperparameters. Table 1: Dataset statistics. English. We perform English constituency parsing experiments for comparison with the original work by Shen et al. (2018a). We use the Wall Street Journal Section of the Penn Treebank (Marcus et al., 1999). We use parts 00-21 for training, 22 for validation and 23 for testing. Left/right-branching trees baseline (LBR/RBR). Our next baseline consists of purely left- or right-branching trees. LBR refers to the F1 score strictly left-branching binary trees obtain compared to the gold annotations, and RBR denotes the score of strictly r"
D19-6123,P19-1180,0,0.0163381,"s as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cross-lingual transfer and difference in hyper parameters. In this work, we focus on adaptSetup Since transfer learning is mostly needed and also particularly effective in the low-data regime, we combine the training set"
D19-6123,A97-1014,0,0.0531374,"ed for each of test and development, and the remaining 1434 constitute our training set. Chinese. We use the Chinese Penn Treebank v8.0 (Xue et al., 2005). Again, we randomly separate files into splits: the development and test sets consist of 300 files each, while the training set consists of the remaining 2407. Balanced trees baseline (BTB). Finally, this baseline is similar to LBR/RBR, but considering balanced binary trees, which are created by recursively splitting each span into halves. For odd lengths, the middle word becomes a part of the right subtree. German. We use the NEGRA corpus (Skut et al., 1997), which consists of approximately 350, 000 words of German newspaper text (20,602 sentences). We divide the dataset into training, development, and test splits as suggested by Dubey and Keller (2003). 4 4.1 Monolingual Experiments Language-Dependence of Hyperparameters Best binary tree upper bound (BB). Since our datasets contain n-ary trees, but the PRPN only produces binary trees, obtaining a perfect F1 score is impossible. This upper bound represents the best score which can be obtained with binary trees. It is of practical importance to know whether a set of hyperparameters found for one l"
D19-6123,P09-1009,0,0.028359,"l., 2018a,c). While the latter model family has been able to generate parse trees which show a high accordance with expert annotations, its members, with the prominent Parsing Reading Predict-Network (PRPN) being no exception, have mostly been evaluated on English.1 Thus, it is not obvious whether and when obtained results would hold true for other languages, especially if they are unrelated to English or dispose of significantly smaller training corpora. Some nonneural models for grammar induction—i.e., models, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language l"
D19-6123,P11-2120,0,0.0220928,"r methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are"
D19-6128,C16-1333,1,0.894016,"Missing"
D19-6128,P17-1014,0,0.0285848,"his condition, the test set is still relatively small. Thus, the word embeddings do not have much distributional information with which to arrive at good word representations for previously out-of-vocabulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on"
D19-6128,E17-2039,1,0.851467,"Missing"
D19-6128,E17-1005,0,0.0343576,"Missing"
D19-6128,N18-1172,1,0.854624,"f the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples where multi-task learning is combined with other methods is the semi"
D19-6128,D15-1041,0,0.0150694,"cies obtained from our training data. 2.2 We approach sequence labelling by using a variant of a bidirectional recurrent neural network, which uses both preceding and succeeding context when predicting the label of a word. This choice was made as such models at the same time obtain high performance on all three tasks and lend themselves nicely to multi-task training via hard parameter sharing. This system is based on the hierarchical bi-LSTM of Plank et al. (2016) and is implemented using DyNet (Neubig et al., 2017). On the subword-level, the LSTM is bi-directional and operates on characters (Ballesteros et al., 2015; Ling et al., 2015). Second, a context bi-LSTM operates on the word level, from which output is passed on to a classification layer. Multi-task training is approached using hard parameter sharing (Caruana, 1993). We consider T datasets, each containing pairs of input-output set ), w ∈ V , y t ∈ Lt . The input quences (w1:n , y1:n i i vocabulary V is shared across tasks, but the outputs (tagsets) Lt are task dependent. At each step in the training process we choose a random task t, followed by a randomly chosen batch of training instance. Each task is associated with an independent classificat"
D19-6128,L16-1262,0,0.0622977,"Missing"
D19-6128,N18-1202,0,0.0189821,"for self-training. Second, we choose a transductive approach, because we assume that not all auxiliary task examples will lead to equal improvements on the main task. In particular, we expect auxiliary task labels for the test instances to be most useful, since information about those instances is most relevant for the prediction of the main task labels on this data. Similarly to contextualised word representations, this offers an additional signal for the test set instances, as we obtain this through predicted auxiliary labels rather than direct encoding of the context (Devlin et al., 2018; Peters et al., 2018). 3.1 question how it compares to adding additional (expensive) gold-standard annotations for the main and the auxiliary tasks. To ensure that our findings are generalisable, we use a large sample of 56 treebanks, covering 41 languages and several domains. Although this experimental set-up would allow us to run multilingual experiments, we only train monolingual models, and aggregate results across languages and treebanks. We investigate three tasks; two of them being morpho-syntactic (POS tagging and DepRel tagging) and one being semantic (semantic tagging). In all cases, POS is the auxiliary"
D19-6128,P16-2067,0,0.0199108,"multi-task model for DepRel tagging and semantic tagging, respectively. 2 (2016). We use this task as an unsupervised auxiliary baseline, with frequencies obtained from our training data. 2.2 We approach sequence labelling by using a variant of a bidirectional recurrent neural network, which uses both preceding and succeeding context when predicting the label of a word. This choice was made as such models at the same time obtain high performance on all three tasks and lend themselves nicely to multi-task training via hard parameter sharing. This system is based on the hierarchical bi-LSTM of Plank et al. (2016) and is implemented using DyNet (Neubig et al., 2017). On the subword-level, the LSTM is bi-directional and operates on characters (Ballesteros et al., 2015; Ling et al., 2015). Second, a context bi-LSTM operates on the word level, from which output is passed on to a classification layer. Multi-task training is approached using hard parameter sharing (Caruana, 1993). We consider T datasets, each containing pairs of input-output set ), w ∈ V , y t ∈ Lt . The input quences (w1:n , y1:n i i vocabulary V is shared across tasks, but the outputs (tagsets) Lt are task dependent. At each step in the t"
D19-6128,W03-0404,0,0.107928,"ulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For"
D19-6128,D17-1038,0,0.0171906,"can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples where multi-task learning is combined with"
D19-6128,P16-2038,0,0.159736,"05) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples w"
D19-6128,L16-1680,0,0.0458928,"Missing"
D19-6128,P95-1026,0,0.760946,"ary rate is reduced to zero in this condition, the test set is still relatively small. Thus, the word embeddings do not have much distributional information with which to arrive at good word representations for previously out-of-vocabulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-t"
D19-6128,D15-1176,0,\N,Missing
D19-6128,P19-1301,0,\N,Missing
E17-1049,W16-2007,0,0.235502,"Missing"
E17-1049,N15-1107,0,0.0463434,"gically rich languages still constitute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person singular present tense of bring is generated. One generalization of inflection is morphological reinflection (MRI) (Cotterell et al., 2016a), where we must produce an inflected form from a triple of target tag, source form and source tag. The inflection task is the special case where the source form is the lemma. As an ex"
E17-1049,D15-1041,0,0.0289893,"sense, the verbal paradigm is partitioned into subparadigms. To see why multi-source models could help in this case, starting only from the infinitive treffen makes it difficult to predict subjunctive form tr¨afest, but the additional information of the fellow subjunctive form tr¨afe makes the task easier. Introduction Morphologically rich languages still constitute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person"
E17-1049,W14-4012,0,0.178924,"Missing"
E17-1049,K15-1017,1,0.918613,"Missing"
E17-1049,Q15-1031,1,0.856892,"transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has ac"
E17-1049,N16-1080,1,0.875264,"Missing"
E17-1049,W02-0603,0,0.06034,"09) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has achieved widespread adoption. Bayesian methods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contai"
E17-1049,D09-1011,0,0.0711054,"orming weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Cre"
E17-1049,D08-1113,0,0.275655,"Missing"
E17-1049,N13-1138,0,0.107651,"y transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. pe"
E17-1049,D13-1105,0,0.022021,"ods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521"
E17-1049,N16-1077,0,0.179268,"echanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for the reinflection task. Besides generation, computational work on"
E17-1049,N16-1101,0,0.0232653,"nd P that encode that the target form is 3rd person plural. We omit the tags from the diagram to which the model hardly attends. al., 2015), parsing (Vinyals et al., 2014) and automatic speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The first work on multi-source models was presented for machine translation. Zoph and Knight (2016) made simultaneous use of source sentences in multiple languages in order to find the best match possible in the target language. Unlike our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms f"
E17-1049,E14-1060,0,0.17837,"cific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and apply the missing values. performing model made use of an attention mechanism (Kann and Sch¨utze, 2016a), first popularized in machine translation (Bahdanau et al., 2015). We generalize this architecture to the multi-source case in this paper for t"
E17-1049,W16-2010,1,0.877615,"Missing"
E17-1049,P16-2090,1,0.891599,"Missing"
E17-1049,P13-2017,0,0.0266029,"Missing"
E17-1049,D15-1272,1,0.856915,"Missing"
E17-1049,N15-1093,0,0.071106,"ute a challenge for natural language processing (NLP). The increased data sparsity caused by highly inflected word forms in certain languages causes otherwise state-of-the-art systems to perform worse in standard tasks, e.g., parsing (Ballesteros et al., 2015) and machine translation (Bojar et al., 2016). To create systems whose performance is not deterred by complex morphology, the development of NLP tools for the generation and analysis of morphological forms is crucial. Indeed, these considerations have motivated a great deal of recent work on the topic (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In the area of generation, the most natural task is morphological inflection—finding an inflected form for a given target tag and lemma. An example for English is as follows: (trg:3rdSgPres, bring) 7→ brings. In this case, the 3rd person singular present tense of bring is generated. One generalization of inflection is morphological reinflection (MRI) (Cotterell et al., 2016a), where we must produce an inflected form from a triple of target tag, source form and source tag. The inflection task is the special case where the source form is the lemma. As an example, we may again consider generati"
E17-1049,W16-2003,0,0.0239677,"th inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521 6 Conclusion Generation of unknown inflections in morphologically rich languages is an important task that remains unsolved. We provide a new angle on the problem by considering systems that are allowed to have multiple inflected forms as input. To this end, we define the task of multi-source morphological reinflection as a generalization of singlesource MRI (Cotterell et al., 2016a) and present a model that solves the task. We extend an attentionbased RNN encoder-decoder architecture from the sin"
E17-1049,N16-1076,1,0.804979,"ke our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. Dreyer et al. (2008) develop a high-performing weighted finite-state transducer for the task, which was later hybridized with an LSTM (Rastogi et al., 2016). Durrett and DeNero (2013) apply a semi-CRF to heuristically extracted rules to generate inflected forms from lemmata using data scraped from Wiktionary. Improved systems for the Wiktionary data were subsequently developed by Hulden et al. (2014), who used a semi-supervised approach, and Faruqui et al. (2016), who used a character-level LSTM. All of the above work has focused on the single input case. Two important exceptions, however, have considered the multi-input case. Both Dreyer and Eisner (2009) and Cotterell et al. (2015b) define a string-valued graphical model over the paradigm and a"
E17-1049,W13-3504,0,0.0535729,"for the reinflection task. Besides generation, computational work on morphology has also focused on analysis. In this area, a common task—morphological segmentation—is to break up a word into its sequence of constituent morphs. The unsupervised M ORFESSOR model (Creutz and Lagus, 2002) has achieved widespread adoption. Bayesian methods have also proven themselves successful in unsupervised morphological segmentation (Johnson et al., 2006; Goldwater et al., 2009). When labeled training data for segmentation is available, supervised methods significantly outperform the unsupervised techniques (Ruokolainen et al., 2013; Cotterell et al., 2015a; Cotterell et al., 2016b). As we pointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinfl"
E17-1049,P15-2111,0,0.0314828,"ointed out in Section 2, morphologically annotated corpora provide an ideal source of data for the multi-source MRI task: they are annotated on the token level with inflectional features and often contain several different inflected forms of a lemma. Eskander et al. (2013) develop an algorithm for automatic learning of inflectional classes and associated lemmas from morphologically annotated corpora, an approach that could be usefully combined with our multi-source MRI framework. The SIGMORPHON 2016 Shared Task on Morphological Reinflection (Cotterell et al., 2016a), based on the U NI M ORPH (Sylak-Glassman et al., 2015) data, resulted in the development of numerous methods. RNN encoder-decoder models (Aha¨ roni et al., 2016; Kann and Sch¨utze, 2016a; Ostling, 2016) obtained the strongest performance and are the current state of the art on the task. The best521 6 Conclusion Generation of unknown inflections in morphologically rich languages is an important task that remains unsolved. We provide a new angle on the problem by considering systems that are allowed to have multiple inflected forms as input. To this end, we define the task of multi-source morphological reinflection as a generalization of singlesour"
E17-1049,N16-1004,0,0.0178675,"; Bahdanau et Figure 4: Attention heatmap for the multi-source model. The example is for the German verb wiegen ‘to weigh’. The model learns to focus most of its attention on forms that share the irregular subjunctive stem w¨og in addition to the target subtags 3 and P that encode that the target form is 3rd person plural. We omit the tags from the diagram to which the model hardly attends. al., 2015), parsing (Vinyals et al., 2014) and automatic speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The first work on multi-source models was presented for machine translation. Zoph and Knight (2016) made simultaneous use of source sentences in multiple languages in order to find the best match possible in the target language. Unlike our model, they apply transformations to the hidden states of the encoders that are input to the decoder. Firat et al. (2016)’s neural architecture for MT translates from any of N source languages to any of M target languages, using language specific encoders and decoders, but sharing one single attention-mechanism. In contrast to our work, they obtain a single output for each input. Much ink has been spilled on morphological reinflection over recent years. D"
K17-2002,E17-1005,0,0.00549682,"ach prepended with a + X s∈S log pθ (s |e(s)), where D is the labeled training data, with each example consisting of a lemma l, a morphological tag t and an inflected form w, and S is a set of autoencoding examples. The function e represents the encoder, which depends on θ. In the setting with no outside resources we experiment with two variants of the sequence autoencoder. The first of these, AE-TD, uses the 3 Multitask learning for NLP using encoder-decoder networks typically assumes that the separate tasks either have distinct encoders, distinct decoders, or both (e.g., Luong et al., 2016; Alonso and Plank, 2017; Bollmann et al., 2017). Here, we use the same encoder and decoder for both tasks. In preliminary experiments, we tried pre-training the autoencoder instead (see, e.g., Dai and Le, 2015; Kamper et al., 2015), but found that interspersing examples gave a clear advantage. 2 Of course, the morphological variants we find may be noisy, so a better method for identifying these might still improve upon random strings. 32 Word 1⇔Word 2 deceive⇔deception receive⇔reception perceive⇔perception conceive⇔conception lemmas and target forms in the training data as inputs to the autoencoder, yielding up to t"
K17-2002,W02-0606,0,0.108183,"oduce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Fi"
K17-2002,P17-1031,0,0.0164218,"s∈S log pθ (s |e(s)), where D is the labeled training data, with each example consisting of a lemma l, a morphological tag t and an inflected form w, and S is a set of autoencoding examples. The function e represents the encoder, which depends on θ. In the setting with no outside resources we experiment with two variants of the sequence autoencoder. The first of these, AE-TD, uses the 3 Multitask learning for NLP using encoder-decoder networks typically assumes that the separate tasks either have distinct encoders, distinct decoders, or both (e.g., Luong et al., 2016; Alonso and Plank, 2017; Bollmann et al., 2017). Here, we use the same encoder and decoder for both tasks. In preliminary experiments, we tried pre-training the autoencoder instead (see, e.g., Dai and Le, 2015; Kamper et al., 2015), but found that interspersing examples gave a clear advantage. 2 Of course, the morphological variants we find may be noisy, so a better method for identifying these might still improve upon random strings. 32 Word 1⇔Word 2 deceive⇔deception receive⇔reception perceive⇔perception conceive⇔conception lemmas and target forms in the training data as inputs to the autoencoder, yielding up to twice as many autoencoder"
K17-2002,N15-1186,0,0.0377683,"mples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Finally, all word pairs with the same differences"
K17-2002,P17-1029,0,0.0531855,"Missing"
K17-2002,J11-2002,0,0.0734975,"Missing"
K17-2002,P17-1182,1,0.740658,"Missing"
K17-2002,W16-2010,1,0.383881,"Missing"
K17-2002,W17-4111,1,0.800423,"Missing"
K17-2002,W10-2211,0,0.0774086,"ion training pairs (any duplicate lemmas or target forms are included only once). Our second autoencoder variant, AE-RS, uses randomly generated strings as inputs, which means we can produce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities"
K17-2002,W02-0604,0,0.507225,"mber of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Table 2 for examples). Finally, all word pairs wi"
K17-2002,W00-0712,0,0.0402176,"puts, which means we can produce an arbitrary number of autoencoding examples. In this and following systems, we use the postfix XXK (e.g. 1K, 2K, 4K) to indicate the number of additional examples generated. To obtain each example, we first choose its length uniformly at random from the interval [4, 12] and then sample each character uniformly at random from the alphabet of the respective language. phemes, their constituent parts, e.g., (Kurimo et al., 2010; Hammarstr¨om and Borin, 2011), others have focused on finding morphologically related words and the orthographic patterns relating them (Schone and Jurafsky, 2000; Baroni et al., 2002; Neuvel and Fulop, 2002; Soricut and Och, 2015): walk ⇔ walking ε ⇔ ing (3) We adopt the algorithm by Neuvel and Fulop (2002) to learn Word Formation Strategies (WFS)—frequently occurring orthographic patterns that relate whole words to other whole words. The input of this algorithm is a list of N words4 . The algorithm works by comparing each of the N words to all other words. It first finds word similarities as the Longest Common Subsequence (LCS) between the two words. Then it finds word differences as the orthographic differences with respect to similarities (see Tabl"
K17-2003,K17-2001,0,0.0969552,"Missing"
K17-2003,W16-2007,0,0.144695,"Missing"
K17-2003,2015.iwslt-evaluation.11,0,0.0461088,"d to thus make use of all known forms. However, they suppose to be able to compute and use good estimates for the probabilities p(fi (wl )|fj (wl )) for source form fj (wl ) and target form fi (wl ), since they use at least 632 entire paradigms per part of speech and language for training. Using a minimum spanning tree, they approximate a solution to the maximum-a-posteriori 7 Fine-Tuning for Multi-Source Input For sequence-to-sequence models for neural machine translation, it has been shown that specialized models for a certain domain are able to obtain better performances than general ones (Luong and Manning, 2015). One way to perform such a domain adaptation is fine-tuning: a general model, which has been trained on out-of-domain data, is further trained on (newly) available indomain data, cf. Figure 3. This brings the conditional probability p(y1 , ..., ym |x1 , ..., xn ) for an output sequence (y1 , ..., ym ) given an input sequence (x1 , ..., xn ) closer to the target distribution. Here, we propose to improve multi-source morphological reinflection by treating each paradigm as a separate domain and performing “domain adaptation” everytime a new paradigm should be completed by the model. In particula"
K17-2003,W14-4012,0,0.0687042,"Missing"
K17-2003,P17-1029,0,0.0817964,"Missing"
K17-2003,D14-1179,0,0.0574271,"Missing"
K17-2003,P17-2061,0,0.0413652,"Missing"
K17-2003,E17-2120,0,\N,Missing
K18-1031,W17-4755,0,0.0431605,"Missing"
K18-1031,E06-1032,0,0.210955,"Missing"
K18-1031,P14-2074,0,0.0206331,"orm well with a single reference. The need of better metrics for MT has been addressed since 2008 in the WMT metrics shared task (Bojar et al., 2016b, 2017). For unsupervised dialogue generation, Liu et al. (2016) obtained close to no correlation with human judgements for BLEU, ROUGE and METEOR. They contributed this in a large part to the unrestrictedness of dialogue answers, which makes it hard to match given references. They emphasized that the community should move away from these metrics for dialogue generation tasks, and develop metrics that correlate more strongly with human judgments. Elliott and Keller (2014) reported the same for BLEU and image caption generation. Duˇsek et al. (2017) suggested an RNN to evaluate NLG at the utterance level, given only the input meaning representation. has been rare. Heilman et al. (2014) predicted the fluency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. Stent et al. (2005) and Cahill (2009) found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit flu"
K18-1031,C16-1294,0,0.0234514,"ich errors matter to humans. Work on automatic fluency evaluation in NLP Results and Discussion Results are shown in Table 7. First, we can see that using SVR (line 1) to combine ROUGE-Lmult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary. Next, we consider the setting where only references and no annotated examples are available. In 319 reference. This was supported by Isabelle et al. (2017), who proposed a so-called challenge set approach as an alternative. Graham et al. (2016) performed a large-scale evaluation of human-targeted metrics for machine translation, which can be seen as a compromise between human evaluation and fully automatic metrics. They also found fully automatic metrics to correlate only weakly or moderately with human judgments. Bojar et al. (2016a) further confirmed that automatic MT evaluation methods do not perform well with a single reference. The need of better metrics for MT has been addressed since 2008 in the WMT metrics shared task (Bojar et al., 2016b, 2017). For unsupervised dialogue generation, Liu et al. (2016) obtained close to no co"
K18-1031,P14-2029,0,0.162486,"le, even though it is grammatical – a popular example is Chomsky’s phrase “Colorless green ideas sleep furiously.” (Chomsky, 1957) In turn, acceptable sentences can be ungrammatical, e.g., in an informal context or in poems (Newmeyer, 1983). Scientists—linguists, cognitive scientists, psychologists, and NLP researcher alike—disagree about how to represent human linguistic abilities. One subject of debates are acceptability judgments: while, for many, acceptability is a binary condition on membership in a set of wellformed sentences (Chomsky, 1957), others assume that it is gradient in nature (Heilman et al., 2014; Toutanova et al., 2016). Tackling this research question, Lau et al. (2017) aimed at modeling human acceptability judgments automatically, with the goal to gain insight into the nature of human perception of acceptability. In particular, they tried to answer the question: Do humans judge acceptability on a gradient scale? Their experiments showed a strong correlation between human judgments and normalized sentence log-probabilities under a variety of LMs for artificial data they had created by translating and back-translating sentences with neural models. While they tried different types of"
K18-1031,E87-1007,0,0.555575,"formed combination—performs significantly better than both ROUGE-L-mult and WPSLOR on their own, it should be the metric of choice for evaluating fluency with given references. 1 Table 7: Combinations; all differences except for 3 and 4 are statistically significant; refs=number of references used to compute the metric; ROUGE=ROUGE-L-mult; best results in bold. based metric ROUGE-LM. In order to make this second experiment comparable to the SVR-based one, we use the same 1955 test examples. 5.2 6 6.1 Related Work Fluency Evaluation Fluency evaluation is related to grammatical error detection (Atwell, 1987; Wagner et al., 2007; Schmaltz et al., 2016; Liu and Liu, 2017) and grammatical error correction (Islam and Inkpen, 2011; Ng et al., 2013, 2014; Bryant and Ng, 2015; Yuan and Briscoe, 2016). However, it differs from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans. Work on automatic fluency evaluation in NLP Results and Discussion Results are shown in Table 7. First, we can see that using SVR (line 1) to combine ROUGE-Lmult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept:"
K18-1031,W00-1401,0,0.247222,"s, finding only low correlation with the latter. However, they did not propose an alternative evaluation. We aim at closing this gap. 6.2 Compression Evaluation Automatic compression evaluation has mostly had a strong focus on content. Hence, word-overlap metrics like ROUGE (Lin and Och, 2004) have been widely used for compression evaluation. However, they have certain shortcomings, e.g., they correlate best for extractive compression, while we, in contrast, are interested in an approach which generalizes to abstractive systems. Alternatives include success rate (Jing, 2000), simple accuracy (Bangalore et al., 2000), which is based on the edit distance between the generation and the reference, or word accuracy (Hori and Furui, 2004), the equivalent for multiple references. 6.3 7 Future Work The work presented in this paper brings up multiple interesting next steps for future research. First, in Subsection 4.7, we investigated the performances of WordSLOR and WPSLOR in dependence of the domain of the considered text. We concluded that an application was possible even for unrelated domains. However, we did not experiment with alternative LMs, which leaves the following questions unresolved: (i) Would train"
K18-1031,D17-1263,0,0.0183274,"fers from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans. Work on automatic fluency evaluation in NLP Results and Discussion Results are shown in Table 7. First, we can see that using SVR (line 1) to combine ROUGE-Lmult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary. Next, we consider the setting where only references and no annotated examples are available. In 319 reference. This was supported by Isabelle et al. (2017), who proposed a so-called challenge set approach as an alternative. Graham et al. (2016) performed a large-scale evaluation of human-targeted metrics for machine translation, which can be seen as a compromise between human evaluation and fully automatic metrics. They also found fully automatic metrics to correlate only weakly or moderately with human judgments. Bojar et al. (2016a) further confirmed that automatic MT evaluation methods do not perform well with a single reference. The need of better metrics for MT has been addressed since 2008 in the WMT metrics shared task (Bojar et al., 2016"
K18-1031,P12-1101,0,0.0589435,"aticality or readability. Note that the exact definitions of all those terms vary slightly throughout the literature. 313 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 313–323 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics 3. We propose ROUGE-LM, a reference-based metric, which achieves a significantly higher correlation with human fluency judgments than all other metrics in our experiments. guage models (LMs) can be used for modeling human ratings of acceptability. Namely, they found SLOR (Pauls and Klein, 2012)—sentence logprobability which is normalized by unigram logprobability and sentence length—to correlate well with acceptability judgments at the sentence level. However, to the best of our knowledge, these insights have so far gone disregarded by the natural language processing (NLP) community. In this paper, we investigate the practical implications of Lau et al. (2017)’s findings for fluency evaluation of NLG, using the task of automatic compression (Knight and Marcu, 2000; McDonald, 2006) as an example (cf. Table 1). Specifically, we test our hypothesis that SLOR should be a suitable metric"
K18-1031,A00-1043,0,0.130747,"ent and fluency of compressions, finding only low correlation with the latter. However, they did not propose an alternative evaluation. We aim at closing this gap. 6.2 Compression Evaluation Automatic compression evaluation has mostly had a strong focus on content. Hence, word-overlap metrics like ROUGE (Lin and Och, 2004) have been widely used for compression evaluation. However, they have certain shortcomings, e.g., they correlate best for extractive compression, while we, in contrast, are interested in an approach which generalizes to abstractive systems. Alternatives include success rate (Jing, 2000), simple accuracy (Bangalore et al., 2000), which is based on the edit distance between the generation and the reference, or word accuracy (Hori and Furui, 2004), the equivalent for multiple references. 6.3 7 Future Work The work presented in this paper brings up multiple interesting next steps for future research. First, in Subsection 4.7, we investigated the performances of WordSLOR and WPSLOR in dependence of the domain of the considered text. We concluded that an application was possible even for unrelated domains. However, we did not experiment with alternative LMs, which leaves the follo"
K18-1031,Q16-1005,0,0.018094,"and T3 (abstractive); cf. Toutanova et al. (2016) for details) for the test data are provided. Each example has 3 to 5 independent human ratings for content and fluency. We are interested in the latter, which is rated on an ordinal scale from 1 (disfluent) through 3 (fluent). We experiment on the 2955 system outputs for the test split. Average fluency scores per system are shown in Table 2. As can be seen, ILP produces the best output. In contrast, NAMAS is the worst system for fluency. In order to be able to judge the reliability of the human annotations, we follow the procedure suggested by Pavlick and Tetreault (2016) and used by Toutanova et al. (2016), and compute the quadratic weighted κ (Cohen, 1968) for the human fluency scores of the system-generated compressions as 0.337. 4.2 NCE(S) = ln(pM (S)) (3) with pM (S) being the probability assigned to the sentence by a LM. We employ the same LMs as for SLOR, i.e., LMs trained on words (WordNCE) and WordPieces (WPNCE). Perplexity. Our next baseline is perplexity, which corresponds to the exponentiated crossentropy: PPL(S) = exp(−NCE(S)) (4) About BLEU. Due to its popularity, we also performed initial experiments with BLEU (Papineni et al., 2002). Its correl"
K18-1031,W16-0528,0,0.0125409,"cantly better than both ROUGE-L-mult and WPSLOR on their own, it should be the metric of choice for evaluating fluency with given references. 1 Table 7: Combinations; all differences except for 3 and 4 are statistically significant; refs=number of references used to compute the metric; ROUGE=ROUGE-L-mult; best results in bold. based metric ROUGE-LM. In order to make this second experiment comparable to the SVR-based one, we use the same 1955 test examples. 5.2 6 6.1 Related Work Fluency Evaluation Fluency evaluation is related to grammatical error detection (Atwell, 1987; Wagner et al., 2007; Schmaltz et al., 2016; Liu and Liu, 2017) and grammatical error correction (Islam and Inkpen, 2011; Ng et al., 2013, 2014; Bryant and Ng, 2015; Yuan and Briscoe, 2016). However, it differs from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans. Work on automatic fluency evaluation in NLP Results and Discussion Results are shown in Table 7. First, we can see that using SVR (line 1) to combine ROUGE-Lmult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approa"
K18-1031,P04-1077,0,0.623488,"was The HSUS. 1 Table 1: Example compressions from our dataset with their fluency scores; scores in [1, 3], higher is better. for filtering unacceptable generations at application time. However, fluency evaluation of NLG systems constitutes a hard challenge: systems are often not limited to reusing words from the input, but can generate in an abstractive way. Hence, it is not guaranteed that a correct output will match any of a finite number of given references. This results in difficulties for current reference-based evaluation, especially of fluency, causing wordoverlap metrics like ROUGE (Lin and Och, 2004) to correlate only weakly with human judgments (Toutanova et al., 2016). As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Introduction Producing sentences which are perceived as natural by a human addressee—a property which we will denote as fluency1 throughout this paper —is a crucial goal of all natural language generation (NLG) systems: it makes interactions more natural, avoids misunderstandings and, overall, leads to higher user satisfaction and user trust (Martindale and Carpuat, 2018). Thus, fluency evaluation is important, e.g., during"
K18-1031,D16-1230,0,0.0496013,"h as an alternative. Graham et al. (2016) performed a large-scale evaluation of human-targeted metrics for machine translation, which can be seen as a compromise between human evaluation and fully automatic metrics. They also found fully automatic metrics to correlate only weakly or moderately with human judgments. Bojar et al. (2016a) further confirmed that automatic MT evaluation methods do not perform well with a single reference. The need of better metrics for MT has been addressed since 2008 in the WMT metrics shared task (Bojar et al., 2016b, 2017). For unsupervised dialogue generation, Liu et al. (2016) obtained close to no correlation with human judgements for BLEU, ROUGE and METEOR. They contributed this in a large part to the unrestrictedness of dialogue answers, which makes it hard to match given references. They emphasized that the community should move away from these metrics for dialogue generation tasks, and develop metrics that correlate more strongly with human judgments. Elliott and Keller (2014) reported the same for BLEU and image caption generation. Duˇsek et al. (2017) suggested an RNN to evaluate NLG at the utterance level, given only the input meaning representation. has bee"
K18-1031,W18-1803,0,0.0170768,", especially of fluency, causing wordoverlap metrics like ROUGE (Lin and Och, 2004) to correlate only weakly with human judgments (Toutanova et al., 2016). As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Introduction Producing sentences which are perceived as natural by a human addressee—a property which we will denote as fluency1 throughout this paper —is a crucial goal of all natural language generation (NLG) systems: it makes interactions more natural, avoids misunderstandings and, overall, leads to higher user satisfaction and user trust (Martindale and Carpuat, 2018). Thus, fluency evaluation is important, e.g., during system development, or Evaluating sentences on their fluency, on the other hand, is a linguistic ability of humans which has been the subject of a decade-long debate in cognitive science. In particular, the question has been raised whether the grammatical knowledge that underlies this ability is probabilistic or categorical in nature (Chomsky, 1957; Manning, 2003; Sprouse, 2007). Within this context, Lau et al. (2017) have recently shown that neural lan∗ *This research was carried out while the first author was interning at Google. 1 Altern"
K18-1031,E06-1038,0,0.0640259,"e models (LMs) can be used for modeling human ratings of acceptability. Namely, they found SLOR (Pauls and Klein, 2012)—sentence logprobability which is normalized by unigram logprobability and sentence length—to correlate well with acceptability judgments at the sentence level. However, to the best of our knowledge, these insights have so far gone disregarded by the natural language processing (NLP) community. In this paper, we investigate the practical implications of Lau et al. (2017)’s findings for fluency evaluation of NLG, using the task of automatic compression (Knight and Marcu, 2000; McDonald, 2006) as an example (cf. Table 1). Specifically, we test our hypothesis that SLOR should be a suitable metric for evaluation of compression fluency which (i) does not rely on references; (ii) can naturally be applied at the sentence level (in contrast to the system level); and (iii) does not need human fluency annotations of any kind. In particular the first aspect, i.e., SLOR not needing references, makes it a promising candidate for automatic evaluation. Getting rid of human references has practical importance in a variety of settings, e.g., if references are unavailable due to a lack of resource"
K18-1031,D16-1033,0,0.0697518,"th their fluency scores; scores in [1, 3], higher is better. for filtering unacceptable generations at application time. However, fluency evaluation of NLG systems constitutes a hard challenge: systems are often not limited to reusing words from the input, but can generate in an abstractive way. Hence, it is not guaranteed that a correct output will match any of a finite number of given references. This results in difficulties for current reference-based evaluation, especially of fluency, causing wordoverlap metrics like ROUGE (Lin and Och, 2004) to correlate only weakly with human judgments (Toutanova et al., 2016). As a result, fluency evaluation of NLG is often done manually, which is costly and time-consuming. Introduction Producing sentences which are perceived as natural by a human addressee—a property which we will denote as fluency1 throughout this paper —is a crucial goal of all natural language generation (NLG) systems: it makes interactions more natural, avoids misunderstandings and, overall, leads to higher user satisfaction and user trust (Martindale and Carpuat, 2018). Thus, fluency evaluation is important, e.g., during system development, or Evaluating sentences on their fluency, on the ot"
K18-1031,W14-1701,0,0.0727754,"Missing"
K18-1031,N10-3002,0,0.131359,"terance level, given only the input meaning representation. has been rare. Heilman et al. (2014) predicted the fluency (which they called grammaticality) of sentences written by English language learners. In contrast to ours, their approach is supervised. Stent et al. (2005) and Cahill (2009) found only low correlation between automatic metrics and fluency ratings for system-generated English paraphrases and the output of a German surface realiser, respectively. Explicit fluency evaluation of NLG, including compression and the related task of summarization, has mostly been performed manually. Vadlapudi and Katragadda (2010) used LMs for the evaluation of summarization fluency, but their models were based on partof-speech tags, which we do not require, and they were non-neural. Further, they evaluated longer texts, not single sentences like we do. Toutanova et al. (2016) compared 80 word-overlap metrics for evaluating the content and fluency of compressions, finding only low correlation with the latter. However, they did not propose an alternative evaluation. We aim at closing this gap. 6.2 Compression Evaluation Automatic compression evaluation has mostly had a strong focus on content. Hence, word-overlap metric"
K18-1031,W13-3601,0,0.0341013,"evaluating fluency with given references. 1 Table 7: Combinations; all differences except for 3 and 4 are statistically significant; refs=number of references used to compute the metric; ROUGE=ROUGE-L-mult; best results in bold. based metric ROUGE-LM. In order to make this second experiment comparable to the SVR-based one, we use the same 1955 test examples. 5.2 6 6.1 Related Work Fluency Evaluation Fluency evaluation is related to grammatical error detection (Atwell, 1987; Wagner et al., 2007; Schmaltz et al., 2016; Liu and Liu, 2017) and grammatical error correction (Islam and Inkpen, 2011; Ng et al., 2013, 2014; Bryant and Ng, 2015; Yuan and Briscoe, 2016). However, it differs from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans. Work on automatic fluency evaluation in NLP Results and Discussion Results are shown in Table 7. First, we can see that using SVR (line 1) to combine ROUGE-Lmult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary. Next, we consider the setting where only references and no annot"
K18-1031,D07-1012,0,0.0391573,"Missing"
K18-1031,P02-1040,0,0.103903,"ed by Pavlick and Tetreault (2016) and used by Toutanova et al. (2016), and compute the quadratic weighted κ (Cohen, 1968) for the human fluency scores of the system-generated compressions as 0.337. 4.2 NCE(S) = ln(pM (S)) (3) with pM (S) being the probability assigned to the sentence by a LM. We employ the same LMs as for SLOR, i.e., LMs trained on words (WordNCE) and WordPieces (WPNCE). Perplexity. Our next baseline is perplexity, which corresponds to the exponentiated crossentropy: PPL(S) = exp(−NCE(S)) (4) About BLEU. Due to its popularity, we also performed initial experiments with BLEU (Papineni et al., 2002). Its correlation with human scores was so low that we do not consider it in our final experiments. LM Hyperparameters and Training We train our LSTM LMs on the English Gigaword corpus (Parker et al., 2011), which consists of news data. The hyperparameters of all LMs are tuned using perplexity on a held-out part of Gigaword, since we expect LM perplexity and final evaluation performance of WordSLOR and, respectively, WPSLOR to correlate. Our best networks consist of two layers with 512 hidden units each, and are trained for 2, 000, 000 steps with a minibatch size of 128. For optimization, we e"
K18-1031,N16-1042,0,0.0275867,"Table 7: Combinations; all differences except for 3 and 4 are statistically significant; refs=number of references used to compute the metric; ROUGE=ROUGE-L-mult; best results in bold. based metric ROUGE-LM. In order to make this second experiment comparable to the SVR-based one, we use the same 1955 test examples. 5.2 6 6.1 Related Work Fluency Evaluation Fluency evaluation is related to grammatical error detection (Atwell, 1987; Wagner et al., 2007; Schmaltz et al., 2016; Liu and Liu, 2017) and grammatical error correction (Islam and Inkpen, 2011; Ng et al., 2013, 2014; Bryant and Ng, 2015; Yuan and Briscoe, 2016). However, it differs from those in several aspects; most importantly, it is concerned with the degree to which errors matter to humans. Work on automatic fluency evaluation in NLP Results and Discussion Results are shown in Table 7. First, we can see that using SVR (line 1) to combine ROUGE-Lmult and WPSLOR outperforms both individual scores (lines 3-4) by a large margin. This serves as a proof of concept: the information contained in the two approaches is indeed complementary. Next, we consider the setting where only references and no annotated examples are available. In 319 reference. This"
K18-3001,K18-3001,1,0.103672,"Missing"
K18-3001,P16-2090,1,0.838493,"Missing"
K18-3001,K17-2003,1,0.836665,"Missing"
K18-3001,K17-2010,1,0.734971,"Missing"
K18-3001,W18-6011,1,0.913422,"and a target UniMorph sentence is shown in Figure 3. Since the selection of languages in task 2 is small and we do not attempt to correct annotation errors in the UD source materials, conversion between UD and UniMorph morphosyntactic descriptions is generally straightforward.11 However, UD descriptions are more fine-grained than their UniMorph equivalents. For example, UD denotes lexical features such as noun gender which are inherent features of a lexeme possessed by all of its word forms. Such inherent features are missing from UniMorph which exclusively annotates inflectional morphology (McCarthy et al., 2018). Therefore, UD fea$ → sta$ ti$ → dista$ koti$ → kodista$ i$ → ista$ oti$ → odista$ Such rules are then extracted from each example inflection in the training data. At generation time, the longest matching left hand side of a rule is identified and applied to the citation form. For example, if the Finnish noun luoti ‘bullet’ were to be inflected in the elative (N;IN+ABL;SG) using only the extracted rules given above, the transformation oti$ → odista$ would be triggered, producing the output luodista. In case there are multiple candidate rules of equally long left hand sides that all match, tie"
K18-3001,K18-3012,0,0.30177,"Missing"
K18-3001,K18-3015,0,0.276525,"Missing"
K18-3001,P15-2111,1,\N,Missing
K18-3001,K17-2002,1,\N,Missing
K18-3001,K17-3001,0,\N,Missing
K18-3001,W17-4110,1,\N,Missing
K18-3001,N18-2087,1,\N,Missing
K18-3001,P18-1245,1,\N,Missing
K18-3001,L18-1293,1,\N,Missing
K18-3001,K18-3004,0,\N,Missing
K18-3001,K18-3010,0,\N,Missing
K18-3001,K18-3013,0,\N,Missing
K18-3001,K18-3003,0,\N,Missing
K18-3001,K18-3016,0,\N,Missing
K18-3001,K18-3005,0,\N,Missing
K18-3001,W16-2006,0,\N,Missing
K18-3001,K17-2008,1,\N,Missing
K18-3001,K17-2005,0,\N,Missing
K18-3001,K18-3008,0,\N,Missing
K18-3001,K18-3007,0,\N,Missing
K18-3006,E14-1060,0,0.0205784,"icit low-resource settings were first introduced to the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31.18 33.77 26.83 38.62 45.09 Related Work Table 2: Test accuracies when c"
K18-3006,N15-1093,0,0.018475,"o the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31.18 33.77 26.83 38.62 45.09 Related Work Table 2: Test accuracies when counting all plausible forms as correct; BL = BASEL"
K18-3006,N18-1126,0,0.150462,"in the 2017 edition of the shared task (Cotterell et al., 2017a). In 2017, explicit low-resource settings were first introduced to the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31."
K18-3006,K18-3001,1,0.854511,"Missing"
K18-3006,K17-2001,0,0.0946098,"Missing"
K18-3006,E17-2120,0,0.0118469,"CoNLL–SIGMORPHON 2017 shared tasks. The first edition of the shared task in 2016 (Cotterell et al., 2016) resulted in 3 different types of systems: “pipeline approaches” (unsupervised alignment algorithms applied to the source-target pairs, followed by a model which predicts edit operations), “neural approaches”, and “linguistically inspired systems”. The winning system was a neural network, namely a character-based RNN encoder-decoder model with attention, similar to the one we use here (Kann and Sch¨utze, 2016). Hence, neural models gained popularity in the 2017 edition of the shared task (Cotterell et al., 2017a). In 2017, explicit low-resource settings were first introduced to the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inf"
K18-3006,N13-1138,0,0.0309843,"ngs were first introduced to the shared task. These settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31.18 33.77 26.83 38.62 45.09 Related Work Table 2: Test accuracies when counting all plausible forms"
K18-3006,N16-1077,0,0.0191269,"e settings demonstrated the effectiveness of hard attention in neural sequence-to-sequence models if training data are limited (Makarov et al., 2017). Research not immediately done for the shared tasks included papers on multi-source reinflection (Cotterell et al., 2017b; Kann et al., 2017a), cross-lingual transfer for reinflection (Kann et al., 2017b), or first intents of neural inflection systems which make use of context for lemmatization (Bergmanis and Goldwater, 2018). Older work on morphological inflection includes Ahlberg et al. (2014); Durrett and DeNero (2013); Nicolai et al. (2015); Faruqui et al. (2016), inter alia. Table 1: Test accuracies when considering only the gold solution; BL = BASELINE; CPH = COPENHAGEN; CUB = CUBoulder. Best results per language in bold; our results in italic. BL BME-HAS CPH CUB NYU UZH de en es fi ru sv 0.10 2.92 11.08 0.89 2.71 0.96 31.14 62.64 33.52 11.18 21.29 27.34 21.54 66.87 37.31 16.14 24.40 36.38 11.53 66.36 31.42 10.04 22.59 19.04 48.43 72.21 31.98 18.68 23.29 37.13 61.38 74.02 37.17 28.21 30.42 39.36 av. 3.11 31.18 33.77 26.83 38.62 45.09 Related Work Table 2: Test accuracies when counting all plausible forms as correct; BL = BASELINE; CPH = COPENHAGEN;"
K18-3006,E17-1049,1,0.884614,"Missing"
K18-3006,P17-1182,1,0.895074,"Missing"
K18-3006,W16-2010,1,0.8906,"Missing"
K18-3006,P16-1162,0,0.0295864,"ter level, i.e., the input to the system is the character sequence of the input lemma, represented by embeddings. The output is the (predicted) character sequence of the inflected form. Additionally, we include the sentence context as follows: Given a sentence s = [w1 , w2 , . . . , wi−1 , l, wi+1 , . . . , wn ], where l is the lemma of the inflected form of interest, and w1 , . . . , wn with n 6= i are the surrounding context words, we split the past context cprev = [w1 , w2 , . . . , wi−1 ] and the future context cf ut = [wi+1 , . . . , wn ] into subword units using byte pair encoding (BPE, Sennrich et al. (2016)). We then use two additional encoders to encode the sequences of subword units of both contexts. Using bidirectional encoders, the final hidden states produced by each encoder are concatena4 4.1 Official System Evaluation Datasets The data for Task 2, track 2, LOW consists of sentences taken from the Universal Dependencies 59 Decoder (characters) ... Lemma (characters) ... ... Past context (BPE) Future context (BPE) Attention mechanism Attention mechanism Attention mechanism Figure 1: Overview of our employed model architecture. the lemma of the word at the next position in the sentence, (v)"
K18-3006,P15-2111,0,\N,Missing
K18-3006,K17-2003,1,\N,Missing
K18-3006,K17-2002,1,\N,Missing
K18-3006,K17-3001,0,\N,Missing
K18-3006,W17-4110,1,\N,Missing
K18-3006,N18-2087,0,\N,Missing
K18-3006,P18-1245,0,\N,Missing
K18-3006,L18-1293,0,\N,Missing
K18-3006,W18-6011,0,\N,Missing
K18-3006,K18-3004,0,\N,Missing
K18-3006,K18-3010,0,\N,Missing
K18-3006,K18-3013,0,\N,Missing
K18-3006,K18-3003,0,\N,Missing
K18-3006,K18-3016,0,\N,Missing
K18-3006,K18-3005,0,\N,Missing
N18-1005,Q16-1031,0,0.0258793,"character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several languages. They report"
N18-1005,D16-1097,1,0.923198,"Missing"
N18-1005,P17-1182,1,0.891715,"Missing"
N18-1005,W10-2210,0,0.513559,"only on the available annotated data. Differences Between Multi-task Training and Data Augmentation The difference between MTT-U (resp. MTT-R) and DA-U (resp. MTT-U) is a single element in the input sequence (the one representing the task). Semi-supervised MORFESSOR (MORF). We further compare to the semi-supervised version 51 auxiliary task of autoencoding corpus data are m = 4 for Mexicanero, Nahuatl and Wixarika and m = 1 for Yorem Nokki. For multi-task training with autoencoding of random strings we select m = 8 for Mexicanero, Nahuatl and Yorem Nokki and m = 4 for Wixarika. of MORFESSOR (Kohonen et al., 2010), a wellknown morphological segmentation system. During training, we tune the hyperparameters for each language on the respective development set. The best performing model is applied to the test set. FlatCat (FC). Our next baseline is FlatCat (Gr¨onroos et al., 2014), a variant of MORFESSOR. It consists of a hidden Markov model for segmentation. The states of the model correspond either to a word boundary and one of the four morph categories stem, prefix, suffix, and nonmorpheme. It can work in an unsupervised way, but, similar to the previous baseline, can make effective use of small amounts"
N18-1005,D11-1005,0,0.0291798,"end of a morpheme, or as a single-character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentati"
N18-1005,K15-1017,1,0.9191,"Missing"
N18-1005,N16-1080,1,0.908981,"Missing"
N18-1005,W02-0603,0,0.941577,"important for polysynthetic languages with a high morpheme-to-word ratio and a consequently large overall number of words. To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here, we add three new questions to this line of research: (i) Are data-hungry neural network models Introduction Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications ∗ *The first two authors contributed equally. 47 Proceedings of NAACL-HLT 2018, pages 47–57 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Mexicanero frq. m. 136 ni 128 ki 114 ti 105 u 70 s 44 mo 42 ka 39 a 31"
N18-1005,P08-1115,0,0.048669,"ure research. 1 Recovering morphemes provides information about unknown words and is thus especially important for polysynthetic languages with a high morpheme-to-word ratio and a consequently large overall number of words. To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here, we add three new questions to this line of research: (i) Are data-hungry neural network models Introduction Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications ∗ *The first two authors contributed equally. 47 Proceedings of NAACL-HLT 2018, pages 47–57 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Associa"
N18-1005,W13-3504,0,0.166783,"s with a high morpheme-to-word ratio and a consequently large overall number of words. To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here, we add three new questions to this line of research: (i) Are data-hungry neural network models Introduction Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications ∗ *The first two authors contributed equally. 47 Proceedings of NAACL-HLT 2018, pages 47–57 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Mexicanero frq. m. 136 ni 128 ki 114 ti 105 u 70 s 44 mo 42 ka 39 a 31 nich 31 $i 24 ta 24 l 22 tahtanili 21"
N18-1005,J01-2001,0,0.0744251,"(Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several languages. They reported better results than earlier work, including semi-supervised approaches. In the following year, they extended their approach to be able to use unlabeled data as well, further improving performance (Ruokolainen et al., 2014). Cott"
N18-1005,E14-4017,0,0.574739,"large overall number of words. To illustrate how segmentation helps understanding unknown multiplemorpheme words, consider an example in this paper’s language of writing: even if the word unconditionally did not appear in a given training corpus, its meaning could still be derived from a combination of its morphs un, condition, al and ly. Due to its importance for down-stream tasks (Creutz et al., 2007; Dyer et al., 2008), segmentation has been tackled in many different ways, considering unsupervised (Creutz and Lagus, 2002), supervised (Ruokolainen et al., 2013) and semisupervised settings (Ruokolainen et al., 2014). Here, we add three new questions to this line of research: (i) Are data-hungry neural network models Introduction Due to the advent of computing technologies to indigenous communities all over the world, natural language processing (NLP) applications ∗ *The first two authors contributed equally. 47 Proceedings of NAACL-HLT 2018, pages 47–57 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Mexicanero frq. m. 136 ni 128 ki 114 ti 105 u 70 s 44 mo 42 ka 39 a 31 nich 31 $i 24 ta 24 l 22 tahtanili 21 no 17 ya 17 t 17 ke 17 ita 16 piya 15 an applicable to"
N18-1005,K17-1020,0,0.0489552,"q approaches for morphological segmentation we know of focused on canonical segmentation (Cotterell et al., 2016) which differs from the surface segmentation task considered here in that it restores changes to the surface form of morphemes which occurred during word formation. Kann et al. (2016) also used an encoder-decoder RNN and combined it with a neural reranker. While our model architecture was inspired by them, their model was purely supervised. Additionally, they did not investigate the applicability of their neural seq2seq model in low-resource settings or for polysynthetic languages. Ruzsics and Samardzic (2017) extended the standard encoder-decoder architecture for canonical segmentation to contain a language model over segments and improved results. However, a big difference to our work is that they still used more than ten times as much training data as we have available for the indigenous Mexican languages we are working on here. Another neural approach—this time for surface segmentation—was presented by Wang et al. 9 Conclusion and Future Work We first investigated the applicability of neural seq2seq models to morphological surface segmentation for polysynthetic languages in minimalresource sett"
N18-1005,C14-1111,0,0.425303,"Missing"
N18-1005,L16-1666,0,0.0646046,"ent set separately for each language. We explore m times the amount of instances in the original training set, with m ∈ {1, 2, 4, 8}. The reasons why we expect our data augmentation methods to lead to better segmentation models are similar to those for multi-task training. We call models trained on datasets augmented with unlabeled corpus data or random strings DAU or DA-R, respectively. 5.3 Experiments Data We apply our models to the datasets described in §3. For the multi-task training and data augmentation using unlabeled data, we use (unsegmented) words from a parallel corpus collected by Gutierrez-Vasques et al. (2016) for Nahuatl and the closely related Mexicanero. For Wixarika we use data from Mager et al. (2018) and for Yorem Nokki we use text from Maldonado Mart´ınez et al. (2010). 6.2 Baselines Now, we will describe the baselines we use to evaluate the overall performance of our approaches. Supervised seq2seq RNN (S2S). As a first baseline, we employ a fully supervised neural model without data augmentation or multi-task training, i.e., an attention-based encoder-decoder RNN (Bahdanau et al., 2015) which has been trained only on the available annotated data. Differences Between Multi-task Training and"
N18-1005,P11-2120,0,0.0307297,"or as a single-character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several l"
N18-1005,N16-1161,0,0.0252151,"r either as the beginning, middle or end of a morpheme, or as a single-character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied"
N18-1005,Q14-1005,0,0.0319037,"lem and use LSTMs to classify every character either as the beginning, middle or end of a morpheme, or as a single-character morpheme. Cross-lingual knowledge transfer via language tags was proposed for neural seq2seq models before, both for tasks that handle sequences of words (Johnson et al., 2017) and tasks that work on sequences of characters (Kann et al., 2017). However, to the best of our knowledge, we are the first to try such an approach for a morphological segmentation task. In many other areas of NLP, cross-lingual transfer has been applied successfully, e.g., in entity recognition (Wang and Manning, 2014), language modeling (Tsvetkov et al., 2016), or parsing (Cohen et al., 2011; Søgaard, 2011; Ammar et al., 2016). Work on morphological segmentation was started more than 6 decades ago (Harris, 1951). Since then, many approaches have been developed: In the realm of unsupervised methods, two important systems are LINGUISTICS (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002). The latter was later extended to a semi-supervised version (Kohonen et al., 2010) in order to make use of the abundance of unlabeled data which is available for many languages. Ruokolainen et al. (2013) focused explic"
N18-1005,P08-1101,0,0.0603964,"or many languages. Ruokolainen et al. (2013) focused explicitly on low-resource scenarios and applied CRFs to morphological segmentation in several languages. They reported better results than earlier work, including semi-supervised approaches. In the following year, they extended their approach to be able to use unlabeled data as well, further improving performance (Ruokolainen et al., 2014). Cotterell et al. (2015) trained a semi-Markov CRF (semi-CRF) (Sarawagi and Cohen, 2005) jointly on morphological segmentation, stemming and tagging. For the similar problem of Chinese word segmentation, Zhang and Clark (2008) trained a model jointly on part-of-speech tagging. However, we are not aware of any prior work on multi-task training or data augmentation for neural segmentation models. In fact, the two only neural seq2seq approaches for morphological segmentation we know of focused on canonical segmentation (Cotterell et al., 2016) which differs from the surface segmentation task considered here in that it restores changes to the surface form of morphemes which occurred during word formation. Kann et al. (2016) also used an encoder-decoder RNN and combined it with a neural reranker. While our model archite"
N18-1005,Q17-1024,0,\N,Missing
N19-1201,W16-1714,1,0.850283,"f variable length 0 < m ≤ |xi |, where |xi |is the number of characters in xi . The second sequence is such that T s = hts1 , . . . , tsi , . . . , ts|X |i, where |T s |= |X s |= |X |and each tsi ∈ T s is an n-tuple of tags from a given set of LID tags. An input–output example for a DE–TR mixed phrase is shown in Figure 2. Input h ‘Yerim’, ‘seni’, ‘,’, ‘danke’, ‘Schatzym’i Output h (Yerim), (seni), (,), (danke), (Schatzy, m)i h (TR), (TR), (OTHER), (DE), (DE, TR)i Figure 2: Subword-level LID in German–Turkish. 3.2 Datasets German–Turkish The German–Turkish Twitter Corpus (C¸etino˘glu and C¸o¨ ltekin, 2016) consists of 1029 tweets with 17K tokens. They are manually normalized, tokenized, and annotated with language IDs. The language ID tag set consists of TR (Turkish), DE (German), LANG3 (other language), MIXED (intra-word CS), AMBIG (ambiguous language ID in context), and OTHER (punctuation, numbers, emoticns, symbols, etc.). Named entities are tagged with a combination of NE and their language ID: NE.TR, NE.DE, NE.LANG3. In the original corpus, some Turkish and mixed words undergo a morphosyntactic split,2 with splitting points not usually corresponding to language boundaries. For the purpose"
N19-1201,K15-1017,0,0.0348064,"Missing"
N19-1201,W02-0603,0,0.358605,"Missing"
N19-1201,W17-0114,0,0.0647423,"Missing"
N19-1201,W18-4807,0,0.0851664,"Missing"
N19-1201,W16-5805,0,0.564634,"be mixed within one text, or words from different languages can be combined into sentences. CS can also occur on the subword level, when speakers combine morphemes from different languages (intra-word CS). This last phenomenon can mostly be found if at least one of the languages is morphologically rich. An example for intra-word CS between the Romance language Spanish and the Yuto-Aztecan language Wixarika1 is shown in Figure 1. CS language identification (LID) , i.e., predicting the language of each token in a text, has attracted a lot of attention in recent years (cf. Solorio et al. (2014); Molina et al. (2016)). However, intra-word mixing is mostly not handled explicitly: words with morphemes from more than one language are simply tagged with a mixed label. While this works reasonably well for previously studied language pairs, overlooking intra-word CS leads to a major loss of information for highly polsynthetic languages. A mixed word is unknown for NLP systems, yet a single word contains much more information, cf. Figure 1 (b). Furthermore, we find intra-word CS to be much more frequent for Spanish–Wixarika than for previously studied language pairs, such that it is crucial to handle it. Motivat"
N19-1201,D13-1032,0,0.0707609,"Missing"
N19-1201,W16-2013,0,0.0420873,"Missing"
N19-1201,J01-2001,0,0.0792764,"Missing"
N19-1201,P16-2067,0,0.0311623,"a 64-dimensional input layer, 32 dimensions for tags, 16 for segments, and 4 for lengths. For training, we use Adam (Kingma and Ba, 2014). 4.2 Baselines BiLSTM+Seq2Seq/BiLSTM+CRF Our first baselines are pipelines. First, the input text is tagged with language IDs. Language IDs of a mixed word are directly predicted as a combination of all language ID tags of the word (i.e., WIX ES). Second, a subword-level model segments words with composed language ID tags. For word-level tagging, we use a hierarchical bidirectional LSTM (BiLSTM) that incorporates both token- and character-level information (Plank et al., 2016), similar to the winning system (Samih et al., 2016) of the Second Code-Switching Shared Task (Molina et al., 2016). 4 For the subword level, we use two supervised segmentation methods: a CRF segmenter proposed by Ruokolainen et al. (2013), that models segmentation as a labeling problem and a sequence-to-sequence (Seq2Seq) model trained with an auxiliary task as proposed by Kann et al. (2018). CRFTag+Seq2Seq/CRFTag+CRF Since our datasets might be small for training neural networks, we substitute the BiLSTM with a CRF tagger (M¨uller et al., 2013, CRFTag) in the first step. For segmentation, we"
N19-1201,P17-1180,0,0.0522917,"Missing"
N19-1201,C14-1111,0,0.0451731,"Missing"
N19-1201,W13-3504,0,0.0274883,"xt is tagged with language IDs. Language IDs of a mixed word are directly predicted as a combination of all language ID tags of the word (i.e., WIX ES). Second, a subword-level model segments words with composed language ID tags. For word-level tagging, we use a hierarchical bidirectional LSTM (BiLSTM) that incorporates both token- and character-level information (Plank et al., 2016), similar to the winning system (Samih et al., 2016) of the Second Code-Switching Shared Task (Molina et al., 2016). 4 For the subword level, we use two supervised segmentation methods: a CRF segmenter proposed by Ruokolainen et al. (2013), that models segmentation as a labeling problem and a sequence-to-sequence (Seq2Seq) model trained with an auxiliary task as proposed by Kann et al. (2018). CRFTag+Seq2Seq/CRFTag+CRF Since our datasets might be small for training neural networks, we substitute the BiLSTM with a CRF tagger (M¨uller et al., 2013, CRFTag) in the first step. For segmentation, we use the same two approaches as for the previous baselines. CharBiLSTM We further employ a BiLSTM to tag each character with a language ID. For training, each character inherits the language ID of the word or segment it belongs to. At pred"
N19-1201,W16-5806,0,0.0953364,", 16 for segments, and 4 for lengths. For training, we use Adam (Kingma and Ba, 2014). 4.2 Baselines BiLSTM+Seq2Seq/BiLSTM+CRF Our first baselines are pipelines. First, the input text is tagged with language IDs. Language IDs of a mixed word are directly predicted as a combination of all language ID tags of the word (i.e., WIX ES). Second, a subword-level model segments words with composed language ID tags. For word-level tagging, we use a hierarchical bidirectional LSTM (BiLSTM) that incorporates both token- and character-level information (Plank et al., 2016), similar to the winning system (Samih et al., 2016) of the Second Code-Switching Shared Task (Molina et al., 2016). 4 For the subword level, we use two supervised segmentation methods: a CRF segmenter proposed by Ruokolainen et al. (2013), that models segmentation as a labeling problem and a sequence-to-sequence (Seq2Seq) model trained with an auxiliary task as proposed by Kann et al. (2018). CRFTag+Seq2Seq/CRFTag+CRF Since our datasets might be small for training neural networks, we substitute the BiLSTM with a CRF tagger (M¨uller et al., 2013, CRFTag) in the first step. For segmentation, we use the same two approaches as for the previous bas"
N19-1201,J11-2002,0,0.0770037,"Missing"
N19-1201,N18-1005,1,0.887174,"Missing"
N19-1201,W16-5815,0,0.0581078,"Missing"
N19-1201,D18-1030,0,0.0368529,"Missing"
N19-1201,P08-1101,0,0.112749,"Missing"
N19-1201,W14-3902,0,\N,Missing
N19-1201,W14-3907,0,\N,Missing
N19-1201,C16-1115,0,\N,Missing
P16-2090,E14-1060,0,0.155742,"Missing"
P16-2090,N15-1107,0,0.19027,"ss of downstream tasks like machine translation and question answering. Accordingly, learning morphological inflection patterns from labeled data is an important challenge. The task of morphological reinflection (MRI) consists of producing an inflected form for a given source form, source tag and target tag. A special case is morphological inflection (MI), the task of finding an inflected form for a given lemma and target tag. An English example is “tree”+PLURAL → “trees”. Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In this work, we propose the neural encoderdecoder MED – Morphological Encoder-Decoder – a character-level sequence-to-sequence attention model that is a language-independent solution for 1 ryancotterell.github.io/ sigmorphon2016/ 555 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 555–560, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics alizations across tag pairs. 2 with st being an RNN hidden state for time t and ct being the weighted sum of the annotations (h1 , ..., h"
P16-2090,W14-4012,0,0.247307,"Missing"
P16-2090,D15-1272,1,0.873371,"Missing"
P16-2090,N15-1093,0,0.341306,"translation and question answering. Accordingly, learning morphological inflection patterns from labeled data is an important challenge. The task of morphological reinflection (MRI) consists of producing an inflected form for a given source form, source tag and target tag. A special case is morphological inflection (MI), the task of finding an inflected form for a given lemma and target tag. An English example is “tree”+PLURAL → “trees”. Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015). In this work, we propose the neural encoderdecoder MED – Morphological Encoder-Decoder – a character-level sequence-to-sequence attention model that is a language-independent solution for 1 ryancotterell.github.io/ sigmorphon2016/ 555 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 555–560, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics alizations across tag pairs. 2 with st being an RNN hidden state for time t and ct being the weighted sum of the annotations (h1 , ..., hTx ) produced by the encoder, using t"
P16-2090,W98-1239,0,0.140041,"Missing"
P16-2090,D08-1113,0,0.723939,"reinflection in a language. Thus, if the training set is not large enough in this respect, then POET will not be beneficial. Figure 1: Overview of MED 3 Experiments where S(x) and T (x) are source and target tags of x and e(x) is e(σ(x), τ (x)), the edit tree that transforms the source form into the target form. Let ρ be a target form predicted by the MRI system for the source form σ and let s and t be source and target tags. POET does not change ρ if e(σ, ρ) ∈ Es,t . Otherwise it replaces ρ with τ :  0 τ if e(σ, τ 0 ) ∈ Es,t , |ρ, τ 0 |= 1 τ := ρ else We compare MED with the three models of Dreyer et al. (2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al., 2015), the SIGMORPHON16 baseline, and (ii) Faruqui et al. (2015)’s encoder-decoder model.3 We call the latter MODEL*TAG as it requires training as many models as there are target tags. We evaluate MED on two MRI tasks: CELEX and SIGMORPHON16. CELEX. This task is based on complete inflection tables for German extracted from CELEX. For this experiment we follow Dreyer et al. (2008). We use four pairs of morphological tags and corresponding word forms from the German pa"
P16-2090,N13-1138,0,0.345793,"ot be beneficial. Figure 1: Overview of MED 3 Experiments where S(x) and T (x) are source and target tags of x and e(x) is e(σ(x), τ (x)), the edit tree that transforms the source form into the target form. Let ρ be a target form predicted by the MRI system for the source form σ and let s and t be source and target tags. POET does not change ρ if e(σ, ρ) ∈ Es,t . Otherwise it replaces ρ with τ :  0 τ if e(σ, τ 0 ) ∈ Es,t , |ρ, τ 0 |= 1 τ := ρ else We compare MED with the three models of Dreyer et al. (2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al., 2015), the SIGMORPHON16 baseline, and (ii) Faruqui et al. (2015)’s encoder-decoder model.3 We call the latter MODEL*TAG as it requires training as many models as there are target tags. We evaluate MED on two MRI tasks: CELEX and SIGMORPHON16. CELEX. This task is based on complete inflection tables for German extracted from CELEX. For this experiment we follow Dreyer et al. (2008). We use four pairs of morphological tags and corresponding word forms from the German part of the CELEX morphological database. The 4 different transduction tasks are: 13SIA → 13SKE, 2PIE → 13PKE, 2P"
P16-2090,D13-1105,0,0.0358913,"and irregular verbs (e.g., abhing vs. abh¨angte). For SIGMORPHON16, Table 2 shows that MED outperforms the baseline for all eight languages. Absolute performance and variance is probably influenced by type of morphology (e.g., templatic vs. agglutinative), regularity of the language, number of different tag pairs and other factors. MED performs well even for complex and diverse languages like Arabic, Finnish, Navajo and Turkish, suggesting that the type of attentionbased encoder-decoder we use – single-model, using an explicit morphological representation – is a good choice for MRI. 558 2013; Eskander et al., 2013; Nicolai et al., 2015). Chrupała (2008) defined edit trees and Chrupała (2008) and M¨uller et al. (2015) use them for morphological tagging and lemmatization. In the last years, RNN encoder-decoder models and RNNs in general were applied to several NLP tasks. For example, they proved to be useful for machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) and speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). MED bears some resemblance to Faruqui et al. (2015)’s work. However, they train one network for every t"
P17-1182,E14-1060,0,0.185562,"Missing"
P17-1182,Q16-1031,0,0.0196567,"we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a mo"
P17-1182,P16-1184,0,0.0440907,"slation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised"
P17-1182,W14-4012,0,0.19462,"Missing"
P17-1182,P17-2061,0,0.0549325,"Missing"
P17-1182,D11-1005,0,0.0462193,"sfer with transfer from a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, a"
P17-1182,P15-1166,0,0.0464722,"o, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future res"
P17-1182,N13-1138,0,0.107841,"ical taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2016; Kann and Sch¨utze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves an"
P17-1182,P08-1115,0,0.0686385,"nt of tools for computational morphology with limited annotated data. In many languages, individual lexical entries may be realized as distinct inflections of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In morphologically rich languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these"
P17-1182,N16-1077,0,0.272749,"to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox application to paradigm completion in the low-resource scenario. To mitigate this, we consider transfer learning: we train an end-to-end neural system jointly with limited data from a lowresource language and a larger amount of data from a high-resource language. This technique allows 1993 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1993–2003 c Vancouver, Can"
P17-1182,N16-1101,0,0.0242819,"(2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future research for paradigm completion. (iii) Whereas training RNNs in machine translation is hard, we only expe"
P17-1182,Q15-1031,1,0.880055,"Missing"
P17-1182,Q17-1024,0,0.0696793,"Missing"
P17-1182,P16-1156,1,0.900318,"Missing"
P17-1182,D16-1097,1,0.909381,"Missing"
P17-1182,P08-1084,0,0.0879979,"r, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-M"
P17-1182,P16-2090,1,0.823613,"Missing"
P17-1182,P11-2120,0,0.0812648,"rom a ciphered language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative meth"
P17-1182,L16-1498,0,0.017455,"e transfer and use it as a baseline for all target languages. 5 Accuracy 0.8 Exp. 1: Transfer Learning for Paradigm Completion In this experiment, we investigate to what extent our model transfers morphological knowledge from a high-resource source language to a low-resource target language. We experimentally answer three questions. (i) Is transfer learning possible for morphology? (ii) How much annotated data do we need in the low-resource target language? (iii) How closely related must the two languages be to achieve good results? Data. Based on complete inflection tables from unimorph.org (Kirov et al., 2016), we create datasets as follows. Each training set consists of 12,000 samples in the high-resource source 0.6 0.4 0.2 Languages Pt Ca It Fr Ar Es 0.0 0 50·2 50·21 50·22 50·23 50·24 50·25 50·26 50·27 Number of Samples Experiments We run four experiments on 21 distinct pairings of languages to show the feasibility of morphological transfer and analyze our method. We first discuss details common to all experiments. We keep hyperparameters during all experiments (and for all languages) fixed to the following values. Encoder and decoder RNNs each have 100 hidden units and the size of all subtag, ch"
P17-1182,D14-1095,0,0.0808608,"be realized as distinct inflections of a single lemma depending on the syntactic context. For example, the 3SgPresInd of the English verbal lemma to bring is brings. In morphologically rich languages, a lemma can have hundreds of individual forms. Thus, both generation and analysis of such morphological inflections are active areas of research in NLP and morphological processing has been shown to be a boon to several other down-stream applications, e.g., machine translation (Dyer et al., 2008), speech recognition (Creutz et al., 2007), parsing (Seeker and C¸etino˘glu, 2015), keyword spotting (Narasimhan et al., 2014) and word embeddings (Cotterell et al., 2016b), inter alia. In this work, we focus on paradigm completion, a form of morphological generation that maps a given lemma to a target inflection, e.g., (bring, Past) 7→ brought (with Past being the target tag). RNN sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) are the state of the art for paradigm completion (Faruqui et al., 2016; Kann and Sch¨utze, 2016a; Cotterell et al., 2016a). However, these models require a large amount of data to achieve competitive performance; this makes them unsuitable for out-of-thebox applica"
P17-1182,P12-1066,0,0.0488593,"language to Spanish, we see large performance gains, at least for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been propose"
P17-1182,N15-1093,0,0.123208,"of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al., 2016a). Some work first applies an unsupervised alignment model to source and target string pairs and then learns a string-to-string mapping (Durrett and DeNero, 2013; Nicolai et al., 2015), using, e.g., a semi-Markov conditional random field (Sarawagi and Cohen, 2004). Encoderdecoder RNNs (Aharoni et al., 2016; Faruqui et al., 2016; Kann and Sch¨utze, 2016b), a method which our work further develops for the cross-lingual scenario, define the current state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Gr"
P17-1182,H05-1108,0,0.117788,"Missing"
P17-1182,petrov-etal-2012-universal,0,0.0908597,"Missing"
P17-1182,Q15-1026,0,0.0607982,"Missing"
P17-1182,D10-1103,0,0.0291944,"tion (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual abstract morphemes and segmentation simultaneously and reported, similar to us, best results for related languages. Work on paradigm completion has recently been encouraged by the SIGMORPHON 2016 shared task on morphological reinflection (Cotterell et al"
P17-1182,P15-2111,0,0.0361423,"Past Indicative Indicative Sg Pl Sg Pl sue˜no sue˜nas sue˜na so˜namos so˜na´ is sue˜nan so˜ne´ so˜naste so˜no´ so˜namos so˜nasteis so˜naron Table 1: Partial inflection table for the Spanish verb so˜nar. Introduction Low-resource natural language processing (NLP) remains an open problem for many tasks of interest. Furthermore, for most languages in the world, highcost linguistic annotation and resource creation are unlikely to be undertaken in the near future. In the case of morphology, out of the 7000 currently spoken (Lewis, 2009) languages, only about 200 have computer-readable annotations (Sylak-Glassman et al., 2015) – although morphology is easy to annotate compared to syntax and semantics. Transfer learning is one solution to this problem: it exploits annotations in a high-resource language to train a system for a low-resource language. In this work, we present a method for cross-lingual transfer of inflectional morphology using an encoder-decoder recurrent neural network (RNN). This allows for the development of tools for computational morphology with limited annotated data. In many languages, individual lexical entries may be realized as distinct inflections of a single lemma depending on the syntacti"
P17-1182,N16-1161,0,0.0339616,"for the 200 sample case: 0.38 (0→ES) vs. 0.54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the t"
P17-1182,Q14-1005,0,0.342964,"54 (PT→ES) and 0.56 (AR→ES). This is evidence that our conjecture is correct that the baseline Arabic mainly acts as a regularizer that prevents the model from memorizing the training set and therefore improves performance. So performance improves even though no true transfer of morphological knowledge takes place. 6 Related Work Cross-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the re"
P17-1182,D15-1083,1,0.848421,"Missing"
P17-1182,H01-1035,0,0.102341,"-lingual transfer learning has been used for many tasks, e.g., automatic speech recognition (Huang et al., 2013), parsing (Cohen et al., 2011; Søgaard, 2011; Naseem et al., 2012; Ammar et al., 2016), language modeling (Tsvetkov et al., 2016), entity recognition (Wang and Manning, 2014b) and machine translation (Johnson et al., 2016; Ha et al., 2016). One straightforward method is to translate datasets and then train a monolingual model (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005). Also, aligned corpora have been used to project information from annotations in one language to another (Yarowsky et al., 2001; Pad´o and Lapata, 2005). The drawback is that machine translation 2000 errors cause errors in the target. Therefore, alternative methods have been proposed, e.g., to port a model trained on the source language to the target language (Shi et al., 2010). In the realm of morphology, Buys and Botha (2016) recently adapted methods for the training of POS taggers to learn weakly supervised morphological taggers with the help of parallel text. Snyder and Barzilay (2008a, 2008b) developed a non-parametric Bayesian model for morphological segmentation. They performed identification of cross-lingual a"
P17-1182,N16-1004,0,0.0198626,"nt state of the art. Encoder-decoder RNNs were developed in parallel by Cho et al. (2014) and Sutskever et al. (2014) for machine translation and extended by Bahdanau et al. (2015) with an attention mechanism, supporting better generalization. They have been applied to NLP tasks like speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013), parsing (Vinyals et al., 2015) and segmentation (Kann et al., 2016). More recently, a number of papers have used encoder-decoder RNNs in multitask and transfer learning settings; this is mainly work in machine translation: (Dong et al., 2015; Zoph and Knight, 2016; Chu et al., 2017; Johnson et al., 2016; Luong et al., 2016; Firat et al., 2016; Ha et al., 2016), inter alia. Each of these papers has both similarities and differences with our approach. (i) Most train several distinct models whereas we train a single model on input augmented with an explicit encoding of the language (similar to (Johnson et al., 2016)). (ii) Let k and m be the number of different input and output languages. We address the case k ∈ {1, 2, 3} and m = k. Other work has addressed cases with k > 3 or m > 3; this would be an interesting avenue of future research for paradigm comp"
P17-1182,W16-2007,0,\N,Missing
P19-1574,P18-2043,0,0.0135221,"are represented in embeddings. While WSD and EL are important, they conflate (a) the evaluation of the information content of an embedding with (b) a model’s ability to extract that information based on contextual clues. We mostly focus on (a) here. Also, in contrast to WSD datasets, WIKI-PSE is not based on inferred sense tags and not based on artificial ambiguity, i.e., pseudowords (Gale et al., 1992; Sch¨utze, 1992), but on real senses marked by Wikipedia hyperlinks. There has been work in generating dictionary definitions from word embeddings (Noraset et al., 2017; Bosc and Vincent, 2018; Gadetsky et al., 2018). Gadetsky et al. (2018) explicitly adress ambiguity and generate definitions for words conditioned on their embeddings and selected contexts. This also conflates (a) and (b). Some prior work also looks at how ambiguity affects word embeddings. Arora et al. (2018) posit that a word embedding is a linear combination of its sense embeddings and that senses can be extracted via sparse coding. Mu et al. (2017) argue that sense and word vectors are linearly related and show that word embeddings are intersections of sense subspaces. Working with synthetic data, Yaghoobzadeh and Sch¨utze (2016) evalu"
P19-1574,W16-2507,0,0.021121,"chutze 1 Microsoft Research Montr´eal Center for Data Science, New York University 3 IXA NLP Group, University of the Basque Country 4 CIS, LMU Munich yayaghoo@microsoft.com 2 Abstract tools to analyze them. However, the main tool for analyzing their semantic content is still lookWord embeddings typically represent differing at nearest neighbors of embeddings. Nearest ent meanings of a word in a single conflated neighbors are based on full-space similarity nevector. Empirical analysis of embeddings of glecting the multifacetedness property of words ambiguous words is currently limited by the (Gladkova and Drozd, 2016) and making them unsmall size of manually annotated resources stable (Wendlandt et al., 2018). and by the fact that word senses are treated as unrelated individual concepts. We present As an alternative, we propose diagnostic clasa large dataset based on manual Wikipedia ansification of embeddings into semantic classes notations and word senses, where word senses as a probing task to reveal their meaning confrom different words are related by semantic tent. We will refer to semantic classes as Sclasses. This is the basis for novel diagnosclasses. We use S-classes such as food, drug tic tests f"
P19-1574,P12-1092,0,0.0647545,"n transittransit line, transit, finance-currency, disease, chemistry, body part, finance-stock exchange, law, medicine-medical treatment, medicinedrug, broadcast-tv channel, medicine-symptom, biology, visual art-color Table 1: S-classes in WIKI-PSE sorted by frequency. Figure 1: Example of how we build WIKI-PSE. There are three sentences linking “apple” to different entities. There are two mentions (m2 ,m3 ) with the organization sense (S-class) and one mention (m1 ) with the food sense (S-class). nated from embeddings, i.e., that a separate embedding is needed for each sense (Sch¨utze, 1998; Huang et al., 2012; Neelakantan et al., 2014; Li and Jurafsky, 2015; Camacho-Collados and Pilehvar, 2018). This can improve performance on contextual word similarity, but a recent study (Dubossarsky et al., 2018) questions this finding. WIKI-PSE allows us to compute sense embeddings; we will analyze their effect on word embeddings in our diagnostic classifications. 3 WIKI-PSE Resource class in the corpus. There exist sense annotated corpora like SemCor (Miller et al., 1993), but due to the cost of annotation, those corpora are usually limited in size, which can hurt the quality of the trained word embeddings –"
P19-1574,E09-1045,0,0.0468017,"Missing"
P19-1574,H93-1061,0,0.577046,"(2006) and Izquierdo et al. (2009) for word erally represented well in a single-vector emsense disambiguation, but have not been used for bedding – if the sense is frequent. (ii) A clasanalyzing embeddings. sifier can accurately predict whether a word Analysis based on S-classes is only promising if is single-sense or multi-sense, based only on we have high-quality S-class annotations. Existing its embedding. (iii) Although rare senses are datasets are either too small to train embeddings, not well represented in single-vector embeddings, this does not have negative impact on an e.g., SemCor (Miller et al., 1993), or artificially NLP application whose performance depends generated (Yaghoobzadeh and Sch¨utze, 2016). on frequent senses. Therefore, we build WIKI-PSE, a WIKIpediabased resource for Probing Semantics in word Em1 Introduction beddings. We focus on common and proper nouns, and use their S-classes as proxies for senses. Word embeddings learned by methods like For example, “lamb” has the senses food and Word2vec (Mikolov et al., 2013) and Glove (Penliving-thing. nington et al., 2014) have had a big impact on Embeddings do not explicitly address ambigunatural language processing (NLP) and inform"
P19-1574,J17-3004,1,0.894956,"Missing"
P19-1574,D14-1162,0,0.0902657,"Missing"
P19-1574,N18-1202,0,0.0635981,"ns, and use their S-classes as proxies for senses. Word embeddings learned by methods like For example, “lamb” has the senses food and Word2vec (Mikolov et al., 2013) and Glove (Penliving-thing. nington et al., 2014) have had a big impact on Embeddings do not explicitly address ambigunatural language processing (NLP) and informaity; multiple senses of a word are crammed into a tion retrieval (IR). They are effective and effisingle vector. This is not a problem in some applicient for many tasks. More recently, contextualcations (Li and Jurafsky, 2015); one possible exized embeddings like ELMo (Peters et al., 2018) planation is that this is an effect of sparse coding and BERT (Devlin et al., 2018) have further imthat supports the recovery of individual meanings proved performance. To understand both word from a single vector (Arora et al., 2018). But amand contextualized embeddings, which still rely on biguity has an adverse effect in other scenarios, word/subword embeddings at their lowest layer, e.g., Xiao and Guo (2014) see the need of filterwe must peek inside the blackbox embeddings. ing out embeddings of ambiguous words in depenGiven the importance of word embeddings, atdency parsing. tempts have"
P19-1574,J14-4005,0,0.0355501,"Missing"
P19-1574,J98-1004,1,0.561454,"Missing"
P19-1574,E17-1119,0,0.0136715,"n only work if the individual S-classes are recognizable, which is not the case for rare senses in regular word embeddings. NLP Application Experiments Our primary goal is to probe meanings in word embeddings without confounding factors like contextual usage. However, to give insights on how our probing results relate to NLP tasks, we evaluate our embeddings when used to represent word tokens.7 Note that our objective here is not to improve over other baselines, but to perform analysis. We select mention, sentence and sentence-pair classification datasets. For mention classification, we adapt Shimaoka et al. (2017)’s setup:8 training, evaluation (FIGER dataset) and implementation. The task is to predict the contextual fine-grained types of entity mentions. We lowercase the dataset to match the vocabularies of GLOVE(6B), FASTTEXT(Wiki) and our embeddings. For sentence and sentence-pair classifications, we use the SentEval9 (Conneau and Kiela, 2018) setup for four datasets: MR (Pang and Lee, 2005) (positive/negative sentiment prediction for movie reviews) , CR (Hu and Liu, 2004) (positive/negative sentiment prediction for product reviews), SUBJ (Pang and Lee, 2004) (subjectivity/objectivity prediction) an"
P19-1574,J17-4004,0,0.0247803,"ector embeddings has little negative impact – this indicates that for many common NLP benchmarks only frequent senses are needed. 2 Related Work S-classes (semantic classes) are a central concept in semantics and in the analysis of semantic phenomena (Yarowsky, 1992; Ciaramita and Johnson, 2003; Senel et al., 2018). They have been used for analyzing ambiguity by Kohomban and Lee (2005), Ciaramita and Altun (2006), and Izquierdo et al. (2009), inter alia. There are some datasets designed for interpreting word embedding dimensions using S-classes, e.g., SEMCAT (Senel et al., 2018) and HyperLex (Vulic et al., 2017). The main differentiator of our work is our probing approach using supervised classification of word embeddings. Also, we do not use WordNet senses but Wikipedia entity annotations since WordNettagged corpora are small. In this paper, we probe word embeddings with supervised classification. Probing the layers of neural networks has become very popular. Conneau et al. (2018) probe sentence embeddings on how well they predict linguistically motivated classes. Hupkes et al. (2018) apply diagnostic classifiers to test hypotheses about the hidden states of RNNs. Focusing on embeddings, Kann et al."
P19-1574,N18-1190,0,0.0191456,"oup, University of the Basque Country 4 CIS, LMU Munich yayaghoo@microsoft.com 2 Abstract tools to analyze them. However, the main tool for analyzing their semantic content is still lookWord embeddings typically represent differing at nearest neighbors of embeddings. Nearest ent meanings of a word in a single conflated neighbors are based on full-space similarity nevector. Empirical analysis of embeddings of glecting the multifacetedness property of words ambiguous words is currently limited by the (Gladkova and Drozd, 2016) and making them unsmall size of manually annotated resources stable (Wendlandt et al., 2018). and by the fact that word senses are treated as unrelated individual concepts. We present As an alternative, we propose diagnostic clasa large dataset based on manual Wikipedia ansification of embeddings into semantic classes notations and word senses, where word senses as a probing task to reveal their meaning confrom different words are related by semantic tent. We will refer to semantic classes as Sclasses. This is the basis for novel diagnosclasses. We use S-classes such as food, drug tic tests for an embedding’s content: we probe and living-thing to define word senses. Sword embeddings"
P19-1574,W14-1613,0,0.0158333,"ve and effisingle vector. This is not a problem in some applicient for many tasks. More recently, contextualcations (Li and Jurafsky, 2015); one possible exized embeddings like ELMo (Peters et al., 2018) planation is that this is an effect of sparse coding and BERT (Devlin et al., 2018) have further imthat supports the recovery of individual meanings proved performance. To understand both word from a single vector (Arora et al., 2018). But amand contextualized embeddings, which still rely on biguity has an adverse effect in other scenarios, word/subword embeddings at their lowest layer, e.g., Xiao and Guo (2014) see the need of filterwe must peek inside the blackbox embeddings. ing out embeddings of ambiguous words in depenGiven the importance of word embeddings, atdency parsing. tempts have been made to construct diagnostic 5740 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5740–5753 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics We present the first comprehensive empirical analysis of ambiguity in word embeddings. Our resource, WIKI-PSE, enables novel diagnostic tests that help explain how (and how well) e"
P19-1574,D15-1083,1,0.89959,"Missing"
P19-1574,P16-1023,1,0.930111,"Missing"
P19-1574,C92-2070,0,0.622512,"(i) Single-vector embeddings can represent many non-rare senses well. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) In experiments with five common datasets for mention, sentence and sentencepair classification tasks, the lack of representation of rare senses in single-vector embeddings has little negative impact – this indicates that for many common NLP benchmarks only frequent senses are needed. 2 Related Work S-classes (semantic classes) are a central concept in semantics and in the analysis of semantic phenomena (Yarowsky, 1992; Ciaramita and Johnson, 2003; Senel et al., 2018). They have been used for analyzing ambiguity by Kohomban and Lee (2005), Ciaramita and Altun (2006), and Izquierdo et al. (2009), inter alia. There are some datasets designed for interpreting word embedding dimensions using S-classes, e.g., SEMCAT (Senel et al., 2018) and HyperLex (Vulic et al., 2017). The main differentiator of our work is our probing approach using supervised classification of word embeddings. Also, we do not use WordNet senses but Wikipedia entity annotations since WordNettagged corpora are small. In this paper, we probe wo"
P19-1574,D15-1200,0,\N,Missing
P19-1574,C04-1051,0,\N,Missing
P19-1574,W03-1022,0,\N,Missing
P19-1574,W06-1670,0,\N,Missing
P19-1574,P04-1035,0,\N,Missing
P19-1574,P05-1005,0,\N,Missing
P19-1574,D14-1113,0,\N,Missing
P19-1574,N15-1142,0,\N,Missing
P19-1574,K16-1006,0,\N,Missing
P19-1574,P16-1191,0,\N,Missing
P19-1574,L18-1269,0,\N,Missing
P19-1574,D18-1200,0,\N,Missing
P19-1574,D18-1181,0,\N,Missing
P19-1574,N19-1423,0,\N,Missing
P19-1574,Q18-1034,0,\N,Missing
W16-2010,E14-1060,0,0.0258653,"owever, the difference is not big. This might actually be different for languages other than Russian as we did not investigate from a linguistic point of view if the order matters contentwise for any of the languages. 68 6 Related Work were good choices, especially the representation of morphological tags. However, it might be possible to further improve MED’s performance increasing the size of the used embeddings and choosing another initialization. Prior work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D´ejean, 1998), different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). and work on morphological tagging and lemmatization (M¨uller et al., 2015). RNN encoder-decoder models, gated RNNs in general as well as LSTMs were applied to several NLP tasks including some on morphology like morphological segmentation (Wang et al., 2016) during the last years. Other tasks they proved to be useful for are machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) or speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The mos"
W16-2010,N15-1107,0,0.0817768,"y analyze and generate different morphological forms, including previously unseen forms. Two examples are machine translation and question answering, where errors in the understanding of morphological forms can seriously harm performance. Accordingly, learning morphological inflection patterns from labeled data is an important challenge. The task of morphological inflection (MI) consists of generating an inflected form for a given lemma and target tag. Several approaches have been developed for this, including machine learning models and models that exploit the paradigm structure of language (Ahlberg et al., 2015; 62 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 62–70, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics Decoder. It will be described in detail in this Section. for Hungarian and Maltese were only released at evaluation time. Tasks. The Shared Task consists of 3 separate tasks with increasing difficulty: task 1 is supposed to be the easiest and task 3 the hardest. The first task consists of mapping a given lemma and target tag to a target form. Task 2 requires the mapping of a giv"
W16-2010,W14-4012,0,0.0234538,"Missing"
W16-2010,W98-1239,0,0.0618057,"Missing"
W16-2010,N13-1138,0,0.0187163,"is not big. This might actually be different for languages other than Russian as we did not investigate from a linguistic point of view if the order matters contentwise for any of the languages. 68 6 Related Work were good choices, especially the representation of morphological tags. However, it might be possible to further improve MED’s performance increasing the size of the used embeddings and choosing another initialization. Prior work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D´ejean, 1998), different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). and work on morphological tagging and lemmatization (M¨uller et al., 2015). RNN encoder-decoder models, gated RNNs in general as well as LSTMs were applied to several NLP tasks including some on morphology like morphological segmentation (Wang et al., 2016) during the last years. Other tasks they proved to be useful for are machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) or speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The most similar work to ours was"
W16-2010,D13-1105,0,0.0622397,"tually be different for languages other than Russian as we did not investigate from a linguistic point of view if the order matters contentwise for any of the languages. 68 6 Related Work were good choices, especially the representation of morphological tags. However, it might be possible to further improve MED’s performance increasing the size of the used embeddings and choosing another initialization. Prior work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D´ejean, 1998), different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). and work on morphological tagging and lemmatization (M¨uller et al., 2015). RNN encoder-decoder models, gated RNNs in general as well as LSTMs were applied to several NLP tasks including some on morphology like morphological segmentation (Wang et al., 2016) during the last years. Other tasks they proved to be useful for are machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) or speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The most similar work to ours was probably the one by Fa"
W16-2010,P16-2090,1,0.36865,"Missing"
W16-2010,D15-1272,1,0.451281,"Missing"
W16-2010,N15-1093,0,0.0515452,"languages other than Russian as we did not investigate from a linguistic point of view if the order matters contentwise for any of the languages. 68 6 Related Work were good choices, especially the representation of morphological tags. However, it might be possible to further improve MED’s performance increasing the size of the used embeddings and choosing another initialization. Prior work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D´ejean, 1998), different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). and work on morphological tagging and lemmatization (M¨uller et al., 2015). RNN encoder-decoder models, gated RNNs in general as well as LSTMs were applied to several NLP tasks including some on morphology like morphological segmentation (Wang et al., 2016) during the last years. Other tasks they proved to be useful for are machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) or speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013). The most similar work to ours was probably the one by Faruqui et al. (2015). In"
W16-2010,N16-1077,0,\N,Missing
W16-2010,W16-2002,0,\N,Missing
W17-4110,K17-2001,0,0.154544,"learned from the characters or tags and discuss where sequences of letters or tags from a second language contribute to or restrain performance on the paradigm compleIntroduction Neural sequence-to-sequence models define the state of the art for paradigm completion (Cotterell et al., 2016, 2017; Kann and Sch¨utze, 2016), the task of generating inflected forms of a lemma’s paradigm, e.g., filling the empty fields in Table 1 using one of the non-empty fields. However, those models are in general very datahungry, and do not reach good performances in low-resource settings. Therefore, Kann et al. (2017) propose to leverage morphological knowledge from a high-resource language (source language) to improve paradigm completion in a closely related language with insufficient resources (target language). This is achieved by a form of multi-task learning – they train an encoder-decoder model simultaneously on training examples for both languages. While closer related languages seem to help more than distant ones, the mechanisms how this transfer works still 70 Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 70–75, c Copenhagen, Denmark, September 7, 2017. 2017"
W17-4110,P15-1166,0,0.0292854,"th encoder-decoder networks. Encoder-decoder RNNs were introduced by Cho et al. (2014) and Sutskever et al. (2014) and extended by an attention mechanism by Bahdanau et al. (2015). Lately, much work was done on multi-task learning and transfer learning with encoder-decoder RNNs. Luong et al. (2015) investigated multi-task setups for sequence-to-sequence learning, combining multiple encoders and decoders. In contrast, in our experiments, we use only one encoder and one decoder. There exists much work on multi-task learning with encoderdecoder RNNs for machine translation (Johnson et al., 2016; Dong et al., 2015; Firat et al., 2016; 73 Ha et al., 2016). Alonso and Plank (2016) explored multi-task learning empirically, analyzing when it improves performance. Here, we focus on how transfer via multi-task learning works. Paradigm completion. SIGMORPHON hosted two shared tasks on paradigm completion (Cotterell et al., 2016, 2017), in order to encourage the development of systems for the task. One approach is to treat it as a string transduction problem by applying an alignment model with a semi-Markov model (Durrett and DeNero, 2013; Nicolai et al., 2015). Recently, neural sequenceto-sequence models are"
W17-4110,N13-1138,0,0.142318,"Missing"
W17-4110,N16-1077,0,0.086101,"Missing"
W17-4110,N16-1101,0,0.0156705,"networks. Encoder-decoder RNNs were introduced by Cho et al. (2014) and Sutskever et al. (2014) and extended by an attention mechanism by Bahdanau et al. (2015). Lately, much work was done on multi-task learning and transfer learning with encoder-decoder RNNs. Luong et al. (2015) investigated multi-task setups for sequence-to-sequence learning, combining multiple encoders and decoders. In contrast, in our experiments, we use only one encoder and one decoder. There exists much work on multi-task learning with encoderdecoder RNNs for machine translation (Johnson et al., 2016; Dong et al., 2015; Firat et al., 2016; 73 Ha et al., 2016). Alonso and Plank (2016) explored multi-task learning empirically, analyzing when it improves performance. Here, we focus on how transfer via multi-task learning works. Paradigm completion. SIGMORPHON hosted two shared tasks on paradigm completion (Cotterell et al., 2016, 2017), in order to encourage the development of systems for the task. One approach is to treat it as a string transduction problem by applying an alignment model with a semi-Markov model (Durrett and DeNero, 2013; Nicolai et al., 2015). Recently, neural sequenceto-sequence models are also widely used (Fa"
W17-4110,Q17-1024,0,0.0520993,"Missing"
W17-4110,P17-1182,1,0.875972,"Missing"
W17-4110,P16-2090,1,0.912712,"Missing"
W17-4110,E17-1005,0,0.0644449,"Missing"
W17-4110,N15-1093,0,0.0923593,"Missing"
W17-4110,W14-4012,0,0.0253286,"Missing"
W17-4110,P17-1029,0,0.0614087,"Missing"
W17-4111,K17-2001,0,0.0636864,"Missing"
W17-4111,N13-1138,0,0.150344,"Missing"
W17-4111,N16-1077,0,0.0664057,"Missing"
W17-4111,P17-1182,1,0.822714,"Missing"
W17-4111,P16-1185,0,0.0259796,"Missing"
W17-4111,N15-1093,0,0.106512,"Missing"
W17-4111,W14-4012,0,0.0196779,"Missing"
W17-4111,W16-2005,0,0.0338841,"Missing"
W17-4111,D14-1179,0,0.0200405,"Missing"
W17-4111,W16-2003,0,0.0354377,"Missing"
W17-4111,N09-1024,0,0.0487814,"Missing"
W17-4111,P08-1084,0,0.0941985,"Missing"
W17-4111,D11-1014,0,0.107625,"Missing"
W18-3013,W14-1611,0,0.0231012,"in a classification setting and different subspaces of embeddings are analyzed. Extrinsic evaluations are also used (Li and Jurafsky, 2015; K¨ohn, 2015; Lai et al., 2015). In most tasks, embeddings are used in context/sentence representations with composition involved. In this work, we evaluate embeddings in isolation, on their ability to represent multiple senses. Related tasks and datasets. Our proposed task is fine-grained name typing (FNT). A related task is entity set expansion (ESE): given a set of a few seed entities of a particular class, find other entities (Thelen and Riloff, 2002; Gupta and Manning, 2014). We can formulate FNT as ESE, however, there is a difference in the training data assumption. For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach. In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible. Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005). Recently, there has been increased interest in finegrained typing of mentions (Ling and"
W18-3013,D15-1246,0,0.0193011,"Missing"
W18-3013,D15-1200,0,0.148229,"ind all types that a name can refer to based on the name embedding. Given the scale of entities in knowledge bases, we can build datasets for this task that are complementary to the current embedding evaluation datasets in: they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context. 1 Figure 1: Types (ellipses; green) of the entities (rectangles; red), to which the name “Washington” can refer. Ideally, the embedding for “Washington” should represent all these types. Extrinsic evaluations are also used, cf. Li and Jurafsky (2015). In these tasks, embeddings are used in context/sentence representations with composition involved. In this paper, we propose a new evaluation method. In contrast to the prior work on intrinsic evaluation, our method is supervised, largescale, fine-grained, automatically built, and evaluates embeddings in a classification setting where different subspaces of embeddings need to be analyzed. In contrast to the prior work on extrinsic evaluation, we evaluate embeddings in isolation, without confounding factors like sentence contexts or composition functions. Introduction Distributed representati"
W18-3013,N15-1142,0,0.0410841,"Missing"
W18-3013,P14-1023,0,0.138775,"r work on intrinsic evaluation, our method is supervised, largescale, fine-grained, automatically built, and evaluates embeddings in a classification setting where different subspaces of embeddings need to be analyzed. In contrast to the prior work on extrinsic evaluation, we evaluate embeddings in isolation, without confounding factors like sentence contexts or composition functions. Introduction Distributed representation of words, aka word embedding, is an important element of many natural language processing applications. The quality of word embeddings is assessed using different methods. Baroni et al. (2014) evaluate word embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference. Different concept categorization datasets are introduced. These datasets are small (<500) (Baroni et al., 2014; Rubinstein et al., 2015) and therefore measure the goodness of embeddings by the quality of their clustering. Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities. Our evaluation is based on an entity-oriented task in information extraction (IE). Different areas of IE try to predict relevant data about"
W18-3013,D16-1144,0,0.0252587,"er, there is a difference in the training data assumption. For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach. In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible. Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005). Recently, there has been increased interest in finegrained typing of mentions (Ling and Weld, 2012; Yogatama et al., 2015; Ren et al., 2016; Shimaoka et al., 2016). One way of solving our task is to collect every mention of a name, use NER to predict the context-dependent types of mentions, and then take all predictions as the global types of the name. However, our focus in this paper is on how embedding models perform and propose this task as a good evaluation method. We leave the comparison to an NER-based approach for future work. Corpus-level fine-grained entity typing is the 1 Our dataset is available at: https://github.com/ yyaghoobzadeh/name_typing 102 1 holds. For example, for the name “Hamilton”, we should find all of th"
W18-3013,E17-1055,1,0.887038,"Missing"
W18-3013,N13-1008,0,0.0214632,"ategorization datasets are introduced. These datasets are small (<500) (Baroni et al., 2014; Rubinstein et al., 2015) and therefore measure the goodness of embeddings by the quality of their clustering. Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities. Our evaluation is based on an entity-oriented task in information extraction (IE). Different areas of IE try to predict relevant data about entities from text, either locally (i.e., at the context-level), or globally (i.e., at the corpus-level). For example, local (Zeng et al., 2014) and global (Riedel et al., 2013) in relation extraction, or local (Ling and Weld, 2012) and global (Yaghoobzadeh and Sch¨utze, 2015) in entity typing. In most global tasks, each entity is indexed with an identifier (ID) that usually comes from knowledge bases such as 101 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 101–106 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics like sentence context1 . Freebase. Exceptions are tasks in lexicon generation or population like entity set expansion (ESE) (Thelen and Riloff, 2002), which are global but without entity IDs."
W18-3013,P15-2048,0,0.0150152,"ulate FNT as ESE, however, there is a difference in the training data assumption. For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach. In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible. Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005). Recently, there has been increased interest in finegrained typing of mentions (Ling and Weld, 2012; Yogatama et al., 2015; Ren et al., 2016; Shimaoka et al., 2016). One way of solving our task is to collect every mention of a name, use NER to predict the context-dependent types of mentions, and then take all predictions as the global types of the name. However, our focus in this paper is on how embedding models perform and propose this task as a good evaluation method. We leave the comparison to an NER-based approach for future work. Corpus-level fine-grained entity typing is the 1 Our dataset is available at: https://github.com/ yyaghoobzadeh/name_typing 102 1 holds. For example, for the name “Hamilton”, we sho"
W18-3013,P15-2119,0,0.0245732,"n, we evaluate embeddings in isolation, without confounding factors like sentence contexts or composition functions. Introduction Distributed representation of words, aka word embedding, is an important element of many natural language processing applications. The quality of word embeddings is assessed using different methods. Baroni et al. (2014) evaluate word embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference. Different concept categorization datasets are introduced. These datasets are small (<500) (Baroni et al., 2014; Rubinstein et al., 2015) and therefore measure the goodness of embeddings by the quality of their clustering. Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities. Our evaluation is based on an entity-oriented task in information extraction (IE). Different areas of IE try to predict relevant data about entities from text, either locally (i.e., at the context-level), or globally (i.e., at the corpus-level). For example, local (Zeng et al., 2014) and global (Riedel et al., 2013) in relation extraction, or local (Ling and Weld, 2012) and global (Yaghoobzadeh and Sch¨utze, 20"
W18-3013,C14-1220,0,0.0486649,"preference. Different concept categorization datasets are introduced. These datasets are small (<500) (Baroni et al., 2014; Rubinstein et al., 2015) and therefore measure the goodness of embeddings by the quality of their clustering. Usually cosine is used as the similarity metric between embeddings, ignoring subspace similarities. Our evaluation is based on an entity-oriented task in information extraction (IE). Different areas of IE try to predict relevant data about entities from text, either locally (i.e., at the context-level), or globally (i.e., at the corpus-level). For example, local (Zeng et al., 2014) and global (Riedel et al., 2013) in relation extraction, or local (Ling and Weld, 2012) and global (Yaghoobzadeh and Sch¨utze, 2015) in entity typing. In most global tasks, each entity is indexed with an identifier (ID) that usually comes from knowledge bases such as 101 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 101–106 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics like sentence context1 . Freebase. Exceptions are tasks in lexicon generation or population like entity set expansion (ESE) (Thelen and Riloff, 2002), which ar"
W18-3013,D15-1036,0,0.0200014,"subspaces of embeddings. In summary, our contributions are (i) introducing a new evaluation method for word embeddings (ii) publishing a new dataset that is a good resource for evaluating word embeddings and is complementary to prior work: it is very large, contains more different classes than previous word categorization datasets, and allows the direct evaluation of embeddings without confounding factors 2 Related Work Embedding evaluation. Baroni et al. (2014) evaluate embeddings on different intrinsic tests: similarity, analogy, synonym detection, categorization and selectional preference. Schnabel et al. (2015) introduce tasks with more fine-grained datasets. The concept categorization datasets used for embedding evaluation are mostly small (<500) (Baroni et al., 2014) and therefore measure the goodness of embeddings by the quality of their clustering. In contrast, we test embeddings in a classification setting and different subspaces of embeddings are analyzed. Extrinsic evaluations are also used (Li and Jurafsky, 2015; K¨ohn, 2015; Lai et al., 2015). In most tasks, embeddings are used in context/sentence representations with composition involved. In this work, we evaluate embeddings in isolation,"
W18-3013,W16-1313,0,0.0143477,"ference in the training data assumption. For our task, we assume to have enough instances for each type available, and, therefore, to be able to use a supervised learning approach. In contrast, for ESE, mostly only 3-5 seeds are given as training seeds for a set, which makes an evaluation like ours impossible. Named entity recognition (NER) consists of recognizing and classifying mentions of entities locally in a particular context (Finkel et al., 2005). Recently, there has been increased interest in finegrained typing of mentions (Ling and Weld, 2012; Yogatama et al., 2015; Ren et al., 2016; Shimaoka et al., 2016). One way of solving our task is to collect every mention of a name, use NER to predict the context-dependent types of mentions, and then take all predictions as the global types of the name. However, our focus in this paper is on how embedding models perform and propose this task as a good evaluation method. We leave the comparison to an NER-based approach for future work. Corpus-level fine-grained entity typing is the 1 Our dataset is available at: https://github.com/ yyaghoobzadeh/name_typing 102 1 holds. For example, for the name “Hamilton”, we should find all of the following: LOCATION, O"
W18-3013,W02-1028,0,0.202606,"example, local (Zeng et al., 2014) and global (Riedel et al., 2013) in relation extraction, or local (Ling and Weld, 2012) and global (Yaghoobzadeh and Sch¨utze, 2015) in entity typing. In most global tasks, each entity is indexed with an identifier (ID) that usually comes from knowledge bases such as 101 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 101–106 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics like sentence context1 . Freebase. Exceptions are tasks in lexicon generation or population like entity set expansion (ESE) (Thelen and Riloff, 2002), which are global but without entity IDs. ESE usually starts from a few seed entities per set and completes the set using pattern-based methods. Here, we address the task of fine-grained name typing (FNT), a global prediction task, operating on the surface names of entities. FNT and ESE share applications in name lexicon population. FNT is different from ESE because we assume to have sufficient training instances for each type to train supervised models. The challenging goal of FNT is to find the types of all entities a name can refer to. For example, ”Washington” might refer to several entit"
W18-3013,D15-1083,1,0.891522,"Missing"
W18-3013,P16-1023,1,0.878829,"Missing"
W18-3401,D15-1041,0,0.028899,"with a character-based sequence-to-sequence model. Furthermore, we experiment with different choices of external resources and corresponding auxiliary tasks and show that autoencoding can be as efficient as an auxiliary task for low-resource POS tagging as lemmatization. Finally, we evaluate our models on 34 typologically diverse languages. 2 Figure 1: Our multi-task architecture, consisting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We be"
W18-3401,E17-2026,1,0.947527,".6776(.00) .6226( - ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,W17-0225,1,0.850592,"or results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-reso"
W18-3401,C16-1333,1,0.850199,"owever, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevan"
W18-3401,D11-1005,0,0.0784096,"Missing"
W18-3401,K17-2001,0,0.389323,"- ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,P15-2044,1,0.928316,"Missing"
W18-3401,N16-1077,0,0.0302103,"Lemmatization is a task from the area of inflectional morphology. In particular, it is a special case of morphological inflection. Its goal is to map a given inflected word form to its lemma, e.g., sue˜no 7→ so˜nar. (6) Word autoencoding. For the word autoencoding task, we use the inflected forms from the SIGMORPHON 2017 shared task dataset for each respective setting. Due to identical forms for different slot in the morphological paradigm of some lemmas, we might have duplicate examples in those datasets. Sequence-to-sequence models have shown strong performances on morphological inflection (Aharoni et al., 2016; Kann and Sch¨utze, 2016; Makarov et al., 2017). Therefore, when morphological dictionaries are available, we can easily combine a neural model for lemmatization with a POS tagger, using our architecture. Our intuition for this auxiliary task is that it should be possible to include morphological information into our character-based word representations. Formally, the task can be described as follows. Let AL be a discrete alphabet for language L and let TL be a set of morphological tags for L. The morphological paradigm π of a lemma w in L is a set of pairs n o π(w) = fk [w], tk (7) Random s"
W18-3401,P15-1166,0,0.0831235,"Missing"
W18-3401,P07-1094,0,0.133713,"Missing"
W18-3401,P17-2054,1,0.900977,"Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to mention some important work here. Cross-lingual approaches have been used for a large variety of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Ts"
W18-3401,N18-1172,1,0.839691,"nt training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to me"
W18-3401,P16-2067,1,0.917337,"ting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We believe that using subword-level auxiliary tasks to regularize the character-level encoding in hierarchical LSTMs is a flexible and efficient way to get the best of both worlds: such a model is still able to make predictions about unknown words, but the subword-level auxiliary task should prevent it from overfitting. 2.1 vc,i = conc(LSTMc,f (c1:m ), (1) LSTMc,b (cm:1 )) Second, a cont"
W18-3401,P17-1194,0,0.0202508,". Furthermore, their model is also a multi-task model, being trained jointly on predicting the POS and the log-frequency of a word. Their architecture obtained state-of-the-art results for POS tagging in several languages. Hence, in the low-resource setting considered here, we build upon the architecture developed by Plank et al. (2016), and extend it to a multi-task architecture involving sequenceto-sequence learning. Note though that in contrast to our setup, their tasks are both sequence-labeling tasks and using the same input for both tasks. The same holds true for the multi-task model by Rei (2017), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in S"
W18-3401,P17-1182,1,0.881428,"Missing"
W18-3401,P16-2090,1,0.82551,"Missing"
W18-3401,P11-2120,1,0.816951,"Missing"
W18-3401,P16-2038,1,0.707545,"o obtain better performance on a sequence classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lin"
W18-3401,Q16-1023,0,0.0435006,"individual languages. Here, we notice that autoencoders often outperform lemmatization for agglutinative languages. An explanation for this might be that agglutinative morphology is harder to learn, and the chance of overfitting on a small sample is therefore higher. Hyperparameters For all networks, we use 300-dimensional character embeddings, 64-dimensional word embeddings and 100-dimensional LSTM hidden states. Encoder and decoder LSTMs have 1 hidden layer each. For training, we use ADAM (Kingma and Ba, 2014), as well as word dropout and character dropout, each with a coefficient of 0.25 (Kiperwasser and Goldberg, 2016). Gaussian noise is added to the concatenation of the last states of the character LSTMs for POS tagging. All models are trained using early stopping, with a minimum number of 75 (single-task and low), 30 (medium) or 20 (high) epochs and a maximum number of 300 epochs, which is never reached. We stop training if we obtain no improvement for 10 consecutive epochs. The best model on the development set is used for testing. 5 Results The test results for all languages and settings are presented in Table 1. Our first observation is that using 100 words of auxiliary task data seems to be sufficient"
W18-3401,N06-1012,0,0.0581584,"), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in Sutton et al. (2006). Sutton et al. (2006) use a group lasso regularizer to prevent feature swamping. In the same way, we could also detect distributionally similar characters and use a group lasso regularizer to prevent covariate characters to swamp each other. However, this effect can potentially also hurt performance if done in an uninformed way. We intuit that this makes it also impossible for the model to learn useful similarities between characters (random string autoencod7 4.5 4.5 POS, main task POS+AE-Random, main task POS+AE-Random, aux. task POS+Lemmatization, main task POS+Lemmatization, aux. task POS+"
W18-3401,Q13-1001,0,0.0766196,"Missing"
W18-3401,D12-1127,0,0.148688,"Missing"
W18-3401,N16-1161,0,0.0651137,"Missing"
W18-3401,E17-1005,1,0.841116,"ce classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we cons"
W18-3401,Q14-1005,0,0.0607788,"Missing"
W18-3401,D13-1032,0,0.0818875,"Missing"
W18-3401,P12-1066,0,0.0687448,"Missing"
W18-3401,H01-1035,0,0.151424,"Missing"
W18-3401,N16-1004,0,0.0425229,"Missing"
W18-4808,P18-2049,0,0.0123418,"ction Until a few years ago, research on machine translation (MT) between polysynthetic and fusional languages did not get much attention from the natural language processing (NLP) community. Furthermore, with the rise of neural MT (NMT), the common assumption that machine learning approaches for MT were language independent routed the efforts into the direction of general model improvements. But this assumption does not hold completely true, and, recently, efforts have been made to adapt models to individual languages, e.g., in order to improve poor results on morphologically-rich languages (Ataman and Federico, 2018; Al-Mannai et al., 2014; Lee et al., 2016). Koehn (2005) mentioned this problem while he analyzed the Europarl corpus, stating that “translating from an information-rich into an informationpoor language is easier than the other way around”. However, doing so, we unfortunately note a loss of information. This idea that some languages encode more information in one phrase than others given rise to many questions in linguistics and NLP, and motivated this paper. Polysynthetic languages are a special type of information-rich languages, and come with their own set of challenges for translation. St"
W18-4808,P08-1087,0,0.0268174,"s can learn some morphological aspects (Belinkov et al., 2017) even for morphologically rich languages like Arabic. Vania and Lopez (2017) analyzed this proposal, concluding that, although character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approaches. This line of research has pursued both the goal of improving translation from morphologically-rich languages into morphologically-poorer ones like English (Habash and Sadat, 2006), and the other way around (Avramidis and Koehn, 2008; Oflazer, 2008). One important development was the inclusion of linguistic markups into factored translation models (Koehn and Hoang, 2007; Oflazer, 2008; Fraser, 2009). Virpioja et al. (2007) proposed a combined usage of Morfessor (Creutz and Lagus, 2005), an unsupervised segmentation model, and phrase-based SMT systems, in order to make use of segmented input. The translation improvement through initial morphological segmentation was also found for translation of the polysynthetic Wixarika into Spanish (Mager Hois et al., 2016). In each case the main goal of previous work was to increase th"
W18-4808,P17-1080,0,0.0236726,"large number of polysynthetic languages. However, most systems fail to achieve good performances for polysynthetic languages, particularly in a low-resource context; with often better results for SMT than for NMT (Mager and Meza, 2018). In recent years, character-based NMT systems were claimed to handle sub-word phenomena (Sennrich et al., 2016), and others target specifically morphological rich languages (Passban et al., 2018; Peyman et al., 2018), but with the condition of feeding the neural network with vast amounts of data. Character-based NMT systems can learn some morphological aspects (Belinkov et al., 2017) even for morphologically rich languages like Arabic. Vania and Lopez (2017) analyzed this proposal, concluding that, although character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approaches. This line of research has pursued both the goal of improving translation from morphologically-rich languages into morphologically-poorer ones like English (Habash and Sadat, 2006), and the other way around (Avramidis and Koehn, 2008; Oflazer, 2008). One important devel"
W18-4808,J93-2003,0,0.106155,"e MT. However, statistical MT (SMT) and NMT are essential for a broad coverage, due to the vast diversity of polysynthetic languages. In this paper, we introduce the following research questions: (i) Which information is commonly not encoded in the target text when translating to a fusional language from a polysynthetic one? (ii) How can this information loss be explained from a linguistic point of view? (iii) Are some morphemes particularly hard to translate? In an attempt to start answering the before mentioned questions, we present a quantitative study, using morpheme-based SMT alignments (Brown et al., 1993) between the following language pairs: Nahuatl-Spanish, Wixarika-Spanish, and Yorem Nokki-Spanish1 . With the exception of Spanish, all these languages are from the Yuto-Nahua linguistic family and have different levels of polysynthesis. We search for commonly not aligned morphemes and analyze the results. Trying to find answers to our research questions, we also present the qualitative aspects of this information loss. 2 About Polysynthetic Languages Translating from a polysynthetic language to a fusional one faces difficulties; a significant number of morphemes can get lost because polysynth"
W18-4808,N18-2085,0,0.0204272,"lly not directly translated to Spanish. However, as a result, a problem arises when trying to translate such a phrase back to Wixarika, because the information if the action is held in the visual field of the speaker or not is not available. 3 Previous Work Nowadays, the area of NLP is largely dominated by data-driven—often neural—approaches which, as far as the machine learning model is concerned, strive to be language-independent. However, the performance of such systems does still vary in dependence of the typology of each language. In order to shed light on this phenomenon and its causes, Cotterell et al. (2018) studied how the difficulty of language modeling (LM) depends on the language. They found inflectional morphology to be a key factor: even character-based long short-term memory (LSTM) models performed worse for morphologically rich lan75 guages. In this paper, we will study the causes of possible performance loss for polysynthetic languages in MT. MT has made a big step forward with the development of SMT and, later on, NMT. Those approaches make it possible to construct reasonably well performing MT systems using parallel corpora that can be gathered from a wide range of sources, and do not"
W18-4808,W09-0420,0,0.0139888,"lthough character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approaches. This line of research has pursued both the goal of improving translation from morphologically-rich languages into morphologically-poorer ones like English (Habash and Sadat, 2006), and the other way around (Avramidis and Koehn, 2008; Oflazer, 2008). One important development was the inclusion of linguistic markups into factored translation models (Koehn and Hoang, 2007; Oflazer, 2008; Fraser, 2009). Virpioja et al. (2007) proposed a combined usage of Morfessor (Creutz and Lagus, 2005), an unsupervised segmentation model, and phrase-based SMT systems, in order to make use of segmented input. The translation improvement through initial morphological segmentation was also found for translation of the polysynthetic Wixarika into Spanish (Mager Hois et al., 2016). In each case the main goal of previous work was to increase the BLEU score. However, in this paper we aim to improve our understanding of the information which is lost in the translation process; and particularly for polysynthetic"
W18-4808,N06-2013,0,0.0237892,"th vast amounts of data. Character-based NMT systems can learn some morphological aspects (Belinkov et al., 2017) even for morphologically rich languages like Arabic. Vania and Lopez (2017) analyzed this proposal, concluding that, although character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approaches. This line of research has pursued both the goal of improving translation from morphologically-rich languages into morphologically-poorer ones like English (Habash and Sadat, 2006), and the other way around (Avramidis and Koehn, 2008; Oflazer, 2008). One important development was the inclusion of linguistic markups into factored translation models (Koehn and Hoang, 2007; Oflazer, 2008; Fraser, 2009). Virpioja et al. (2007) proposed a combined usage of Morfessor (Creutz and Lagus, 2005), an unsupervised segmentation model, and phrase-based SMT systems, in order to make use of segmented input. The translation improvement through initial morphological segmentation was also found for translation of the polysynthetic Wixarika into Spanish (Mager Hois et al., 2016). In each c"
W18-4808,N18-1005,1,0.529146,"Missing"
W18-4808,D07-1091,0,0.0316224,"yzed this proposal, concluding that, although character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approaches. This line of research has pursued both the goal of improving translation from morphologically-rich languages into morphologically-poorer ones like English (Habash and Sadat, 2006), and the other way around (Avramidis and Koehn, 2008; Oflazer, 2008). One important development was the inclusion of linguistic markups into factored translation models (Koehn and Hoang, 2007; Oflazer, 2008; Fraser, 2009). Virpioja et al. (2007) proposed a combined usage of Morfessor (Creutz and Lagus, 2005), an unsupervised segmentation model, and phrase-based SMT systems, in order to make use of segmented input. The translation improvement through initial morphological segmentation was also found for translation of the polysynthetic Wixarika into Spanish (Mager Hois et al., 2016). In each case the main goal of previous work was to increase the BLEU score. However, in this paper we aim to improve our understanding of the information which is lost in the translation process; and p"
W18-4808,2005.mtsummit-papers.11,0,0.0626673,"een polysynthetic and fusional languages did not get much attention from the natural language processing (NLP) community. Furthermore, with the rise of neural MT (NMT), the common assumption that machine learning approaches for MT were language independent routed the efforts into the direction of general model improvements. But this assumption does not hold completely true, and, recently, efforts have been made to adapt models to individual languages, e.g., in order to improve poor results on morphologically-rich languages (Ataman and Federico, 2018; Al-Mannai et al., 2014; Lee et al., 2016). Koehn (2005) mentioned this problem while he analyzed the Europarl corpus, stating that “translating from an information-rich into an informationpoor language is easier than the other way around”. However, doing so, we unfortunately note a loss of information. This idea that some languages encode more information in one phrase than others given rise to many questions in linguistics and NLP, and motivated this paper. Polysynthetic languages are a special type of information-rich languages, and come with their own set of challenges for translation. Studying their particularities is an important prerequisite"
W18-4808,P16-1160,0,0.0138429,"anslation (MT) between polysynthetic and fusional languages did not get much attention from the natural language processing (NLP) community. Furthermore, with the rise of neural MT (NMT), the common assumption that machine learning approaches for MT were language independent routed the efforts into the direction of general model improvements. But this assumption does not hold completely true, and, recently, efforts have been made to adapt models to individual languages, e.g., in order to improve poor results on morphologically-rich languages (Ataman and Federico, 2018; Al-Mannai et al., 2014; Lee et al., 2016). Koehn (2005) mentioned this problem while he analyzed the Europarl corpus, stating that “translating from an information-rich into an informationpoor language is easier than the other way around”. However, doing so, we unfortunately note a loss of information. This idea that some languages encode more information in one phrase than others given rise to many questions in linguistics and NLP, and motivated this paper. Polysynthetic languages are a special type of information-rich languages, and come with their own set of challenges for translation. Studying their particularities is an importan"
W18-4808,J03-1002,0,0.00801265,"| {z } (2) a After computing the alignments for each sentence pair in our dataset, we count all morphemes which are not aligned, in order to find information that is not expressed in the target language. As SMT is a probabilistic method the resulting alienations are not exact and should be taken as an approximation. Adding this fact, also the amount of data used to train the system influences the resulting inference. 76 5 Experiments In order to get the alignments of morphemes in Nahuatl, Wixarika and Yorem Nokki with their Spanish counterparts, we train a word-based SMT system using GIZA++ (Och and Ney, 2003) on parallel datasets in the respective languages, which we will describe below. We parsed the resulting alignment files to extract non-aligned morphemes in each translation direction. As we are working with three language pairs and are interested in both translation directions, we trained six models (Spanish-Wixarika, WixarikaSpanish, Spanish-Nahuatl, Nahuatl-Spanish, Spanish-Yorem Nokki and Yorem Nokki-Spanish). 5.1 Languages Nahuatl is a language of the Yuto-Nahua language family and the most spoken native language in Mexico. The variants of this language are diverse, and in some cases can"
W18-4808,N18-1006,0,0.0128399,"ably well performing MT systems using parallel corpora that can be gathered from a wide range of sources, and do not need handwritten rules generated by experts. This is crucial, due to the large number of polysynthetic languages. However, most systems fail to achieve good performances for polysynthetic languages, particularly in a low-resource context; with often better results for SMT than for NMT (Mager and Meza, 2018). In recent years, character-based NMT systems were claimed to handle sub-word phenomena (Sennrich et al., 2016), and others target specifically morphological rich languages (Passban et al., 2018; Peyman et al., 2018), but with the condition of feeding the neural network with vast amounts of data. Character-based NMT systems can learn some morphological aspects (Belinkov et al., 2017) even for morphologically rich languages like Arabic. Vania and Lopez (2017) analyzed this proposal, concluding that, although character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approaches. This line of research has pursued both the goal of improving translation from"
W18-4808,C18-1265,0,0.0117541,"T systems using parallel corpora that can be gathered from a wide range of sources, and do not need handwritten rules generated by experts. This is crucial, due to the large number of polysynthetic languages. However, most systems fail to achieve good performances for polysynthetic languages, particularly in a low-resource context; with often better results for SMT than for NMT (Mager and Meza, 2018). In recent years, character-based NMT systems were claimed to handle sub-word phenomena (Sennrich et al., 2016), and others target specifically morphological rich languages (Passban et al., 2018; Peyman et al., 2018), but with the condition of feeding the neural network with vast amounts of data. Character-based NMT systems can learn some morphological aspects (Belinkov et al., 2017) even for morphologically rich languages like Arabic. Vania and Lopez (2017) analyzed this proposal, concluding that, although character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approaches. This line of research has pursued both the goal of improving translation from morphologically-rich"
W18-4808,P16-1162,0,0.0173318,"ment of SMT and, later on, NMT. Those approaches make it possible to construct reasonably well performing MT systems using parallel corpora that can be gathered from a wide range of sources, and do not need handwritten rules generated by experts. This is crucial, due to the large number of polysynthetic languages. However, most systems fail to achieve good performances for polysynthetic languages, particularly in a low-resource context; with often better results for SMT than for NMT (Mager and Meza, 2018). In recent years, character-based NMT systems were claimed to handle sub-word phenomena (Sennrich et al., 2016), and others target specifically morphological rich languages (Passban et al., 2018; Peyman et al., 2018), but with the condition of feeding the neural network with vast amounts of data. Character-based NMT systems can learn some morphological aspects (Belinkov et al., 2017) even for morphologically rich languages like Arabic. Vania and Lopez (2017) analyzed this proposal, concluding that, although character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approa"
W18-4808,P11-3003,0,0.0271921,"e most difficult morphemes to align. This can be a consequence of the low-resource setting we have. The last language pair (Nahuatl-Spanish) has less non-aligned morphemes than the previews ones. Nahuatl is also the only language for which the most frequently unaligned token is a word: the token “in” is an article. However, its translation is not trivial. As most Yuto-Nahua languages, Nahuatl does not mark grammatical gender (Mager Hois, 2017). The lack of gender information can hurt SMT performance. In practice, for such cases post-processing can be used to correct some of the system errors (Stymne, 2011). However, as in the previous cases, the object (“k” and “ki”) a noun agreement morphemes (“ni” and “ti”) are the most frequently unaligned morphemes. Table 2 shows the amount of non-aligned tokens, words, and morphemes for each language pair and each translation direction. Our first observation is that the rate of non-aligned tokens for the direction Spanish-polysynthetic language is far lower than the other way around. The highest rates of non-aligned tokens are found for Wixarika and Yorem Nokki with 0.617 and 0.616, respectively. For Nahuatl and Spanish, this rate is with 0.448 notably low"
W18-4808,P17-1184,0,0.0128823,"ve good performances for polysynthetic languages, particularly in a low-resource context; with often better results for SMT than for NMT (Mager and Meza, 2018). In recent years, character-based NMT systems were claimed to handle sub-word phenomena (Sennrich et al., 2016), and others target specifically morphological rich languages (Passban et al., 2018; Peyman et al., 2018), but with the condition of feeding the neural network with vast amounts of data. Character-based NMT systems can learn some morphological aspects (Belinkov et al., 2017) even for morphologically rich languages like Arabic. Vania and Lopez (2017) analyzed this proposal, concluding that, although character-based models improve translation, best results are achieved with accurate morphological segmentation. The problem exposed in this paper has mainly been studied in the context of SMT approaches. This line of research has pursued both the goal of improving translation from morphologically-rich languages into morphologically-poorer ones like English (Habash and Sadat, 2006), and the other way around (Avramidis and Koehn, 2008; Oflazer, 2008). One important development was the inclusion of linguistic markups into factored translation mod"
W18-4808,2007.mtsummit-papers.65,0,0.0818047,"Missing"
W18-4808,W14-3628,0,\N,Missing
W18-4808,Q17-1026,0,\N,Missing
W19-0129,P98-1013,0,0.635257,". *Ada exercised herself. Ada exercised. Table 1: Examples from each verb frame in the dataset. Bolded verbs evoke both verb frames; other verbs evoke only one. Transitive verb frames include: Causative, S PRAY–L OAD with, S PRAY–L OAD locative, U NDERSTOOD O BJECT reflexive. Intransitive verb frames include: Inchoative, no-there (with locative adjunct), there (with locative adjunct), and U NDERSTOOD -O BJECT no-reflexive. 2-obj. class includes a ditransitive frame and a prepositional dative frame. can appear in) has been described and classified in several verb lexica (Grishman et al., 1994; Baker et al., 1998; Fillmore et al., 2003; Kipper-Schuler, 2005; Kipper-Schuler et al., 2006). Knowledge about verb frames and their alternations is part of a human speaker’s linguistic competence, and as such, should potentially be learned by ANNs. We present two datasets and two experiments that compare ANNs’ knowledge of verb frame alternations at the word level and the sentence level, respectively. First, we ask if a verb’s word embedding can be used to predict which frames that verb can licitly appear in. We construct a dataset of verbs, the Lexical Verb–frame Alternations dataset (LaVA), based on Levin (1"
W19-0129,P04-3031,0,0.0663762,"rained on 6B tokens (Pennington et al., 2014).3 GloVe embeddings are used frequently in natural language processing (NLP), so evaluating them for knowledge of verb frames will be relevant for their application to and future research on tasks requiring rich syntactic features. Second, we use embeddings trained on the smaller 100M token British National Corpus4 (BNC), optimizing a language modeling objective. The language model (LM) is a (single-directional) LSTM trained by Warstadt et al. (2018) using PyTorch and optimized using Adam (Kingma and Ba, 2015). The BNC data is tokenized using NLTK (Bird and Loper, 2004) and words outside the 100k most frequent words in the BNC are replaced with &lt;unk>. Our peripheral interest in how humans learn lexical frame-selectional properties motivates us to investigate these LM-trained word embeddings. We reduce the potential differences between human learners and our models by considering embeddings that are trained on an amount of data similar to what humans are exposed to during language acquisition. For this reason, most publicly available, pre-trained word vectors are a rather unnatural fit, since these embeddings are usually trained on several orders of magnitude"
W19-0129,N18-1083,0,0.0165497,"ndividual alternation for our models to learn. 7 Related Work This investigation is part of a growing body of work which seeks to investigate the linguistic competence of ANNs. For instance, a study by Linzen et al. (2016) tested the ability of ANNs to identify mismatches in subject–verb agreement, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence embeddings contain grammatical information, e.g., about the syntactic scope of negation. Further previous studies on which types of information are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings which were trained as part of an ANN for machine trans¨ lation. Finally, Ostling and Tiedemann (2017) learned language embeddings via multilingual language modeling and used them to reconstruct genealogical trees. However, we are interested in word or sentence embeddings. Extracting information from word embeddings is a common task in natural language processing. While most NLP research is application-o"
W19-0129,W11-0110,0,0.0339384,"about the task at hand as possible (e.g., by varying the training corpus or embedding method), we are interested in the question how much information is trivially contained in selected popular embeddings. Also worth mentioning here is a lexical resource named VerbNet (Kipper-Schuler, 2005; KipperSchuler et al., 2006). This database contains verbs which were classified according to their seman295 tic and syntactic properties, including their Levin classes.7 VerbNet has been used in various NLP applications, e.g., semantic role labeling (Giuglea and Moschitti, 2006), word sense disambiguation (Brown et al., 2011), information extraction (Mausam et al., 2012), or investigation of human language acquisition (Korhonen, 2010). While this resource is very extensive, it only provides a few example sentences (generally only one or two per frame) for each verb. Since we want to investigate if argument structure information is present in sentence embeddings, we create a larger corpus. 8 Conclusions We present complementary word-level and sentence-level datasets, LaVA and FAVA, covering five verb-alternations. We train classifiers on verb embeddings to distinguish which syntactic frames a verb can evoke and whi"
W19-0129,W16-2524,0,0.245899,"example, the lexical set in (2) is used to generate 18 minimal pairs of sentences as in (3) (one pair for each combination of verb, patient, location, and preposition). (2) verbs = {hung, draped} patients = {the blanket, the towel, the cloth} locations = {the bed, the armchair, the couch} prepositions = {over} (3) a. Betty draped the blanket over the couch. b. *Betty draped the couch with the blanket. A similar, semi-automatic sentence creation method focusing only on the passive alternation (and non-argument structure syntactic reorderings using negation and relative clauses) was employed by Ettinger et al. (2016) and Warstadt et al. (2018). Using this method, we construct five sentence-level datasets highlighting different verb alternations (C AUSATIVE –I NCHOATIVE,2 DATIVE, S PRAY–L OAD, there-I NSERTION, U NDERSTOOD -O BJECT) that are chosen so that sentences could be generated with the maximum of variability in the choice of verbs. We split our data into training, development, and test sets by binning lexical sets into training and evaluation bins randomly, in equal proportions. The evaluation set is then split 80/20 into test and development set. Splitting by lexical bin rather than by sentence pr"
W19-0129,P06-1117,0,0.0601984,"s on obtaining embeddings which contain as much knowledge about the task at hand as possible (e.g., by varying the training corpus or embedding method), we are interested in the question how much information is trivially contained in selected popular embeddings. Also worth mentioning here is a lexical resource named VerbNet (Kipper-Schuler, 2005; KipperSchuler et al., 2006). This database contains verbs which were classified according to their seman295 tic and syntactic properties, including their Levin classes.7 VerbNet has been used in various NLP applications, e.g., semantic role labeling (Giuglea and Moschitti, 2006), word sense disambiguation (Brown et al., 2011), information extraction (Mausam et al., 2012), or investigation of human language acquisition (Korhonen, 2010). While this resource is very extensive, it only provides a few example sentences (generally only one or two per frame) for each verb. Since we want to investigate if argument structure information is present in sentence embeddings, we create a larger corpus. 8 Conclusions We present complementary word-level and sentence-level datasets, LaVA and FAVA, covering five verb-alternations. We train classifiers on verb embeddings to distinguish"
W19-0129,C94-1042,0,0.0527846,".-Refl. U.-Obj.-No-Refl. *Ada exercised herself. Ada exercised. Table 1: Examples from each verb frame in the dataset. Bolded verbs evoke both verb frames; other verbs evoke only one. Transitive verb frames include: Causative, S PRAY–L OAD with, S PRAY–L OAD locative, U NDERSTOOD O BJECT reflexive. Intransitive verb frames include: Inchoative, no-there (with locative adjunct), there (with locative adjunct), and U NDERSTOOD -O BJECT no-reflexive. 2-obj. class includes a ditransitive frame and a prepositional dative frame. can appear in) has been described and classified in several verb lexica (Grishman et al., 1994; Baker et al., 1998; Fillmore et al., 2003; Kipper-Schuler, 2005; Kipper-Schuler et al., 2006). Knowledge about verb frames and their alternations is part of a human speaker’s linguistic competence, and as such, should potentially be learned by ANNs. We present two datasets and two experiments that compare ANNs’ knowledge of verb frame alternations at the word level and the sentence level, respectively. First, we ask if a verb’s word embedding can be used to predict which frames that verb can licitly appear in. We construct a dataset of verbs, the Lexical Verb–frame Alternations dataset (LaVA"
W19-0129,kipper-etal-2006-extending,0,0.0433898,"ach verb frame in the dataset. Bolded verbs evoke both verb frames; other verbs evoke only one. Transitive verb frames include: Causative, S PRAY–L OAD with, S PRAY–L OAD locative, U NDERSTOOD O BJECT reflexive. Intransitive verb frames include: Inchoative, no-there (with locative adjunct), there (with locative adjunct), and U NDERSTOOD -O BJECT no-reflexive. 2-obj. class includes a ditransitive frame and a prepositional dative frame. can appear in) has been described and classified in several verb lexica (Grishman et al., 1994; Baker et al., 1998; Fillmore et al., 2003; Kipper-Schuler, 2005; Kipper-Schuler et al., 2006). Knowledge about verb frames and their alternations is part of a human speaker’s linguistic competence, and as such, should potentially be learned by ANNs. We present two datasets and two experiments that compare ANNs’ knowledge of verb frame alternations at the word level and the sentence level, respectively. First, we ask if a verb’s word embedding can be used to predict which frames that verb can licitly appear in. We construct a dataset of verbs, the Lexical Verb–frame Alternations dataset (LaVA), based on Levin (1993), and train a multi-class classifier to identify the licit syntactic fr"
W19-0129,Q16-1037,0,0.0438598,"81.8 Majority BL MCC 0.0 Acc. 66.6 0.0 77.6 0.0 82.1 0.0 60.3 0.0 77.5 0.0 53.7 Table 5: Results from Experiment 2. “w/o CoLA” are models trained on datasets not augmented with CoLA; “w/ CoLA” are models trained on augmented datasets; “Comb.” refers to an aggregate dataset. Bolded MCC values represent moderate correlations (above 0.45). of all the generated data, yet it was by far the hardest individual alternation for our models to learn. 7 Related Work This investigation is part of a growing body of work which seeks to investigate the linguistic competence of ANNs. For instance, a study by Linzen et al. (2016) tested the ability of ANNs to identify mismatches in subject–verb agreement, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence embeddings contain grammatical information, e.g., about the syntactic scope of negation. Further previous studies on which types of information are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings"
W19-0129,D17-1268,0,0.0147146,"c competence of ANNs. For instance, a study by Linzen et al. (2016) tested the ability of ANNs to identify mismatches in subject–verb agreement, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence embeddings contain grammatical information, e.g., about the syntactic scope of negation. Further previous studies on which types of information are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings which were trained as part of an ANN for machine trans¨ lation. Finally, Ostling and Tiedemann (2017) learned language embeddings via multilingual language modeling and used them to reconstruct genealogical trees. However, we are interested in word or sentence embeddings. Extracting information from word embeddings is a common task in natural language processing. While most NLP research is application-oriented and directly or indirectly focuses on obtaining embeddings which contain as much knowledge about the task at hand as possible (e.g., by varyi"
W19-0129,D12-1048,0,0.0147924,"varying the training corpus or embedding method), we are interested in the question how much information is trivially contained in selected popular embeddings. Also worth mentioning here is a lexical resource named VerbNet (Kipper-Schuler, 2005; KipperSchuler et al., 2006). This database contains verbs which were classified according to their seman295 tic and syntactic properties, including their Levin classes.7 VerbNet has been used in various NLP applications, e.g., semantic role labeling (Giuglea and Moschitti, 2006), word sense disambiguation (Brown et al., 2011), information extraction (Mausam et al., 2012), or investigation of human language acquisition (Korhonen, 2010). While this resource is very extensive, it only provides a few example sentences (generally only one or two per frame) for each verb. Since we want to investigate if argument structure information is present in sentence embeddings, we create a larger corpus. 8 Conclusions We present complementary word-level and sentence-level datasets, LaVA and FAVA, covering five verb-alternations. We train classifiers on verb embeddings to distinguish which syntactic frames a verb can evoke and which it cannot. We further train acceptability c"
W19-0129,N13-1090,0,0.0752757,"lopment, and test sets by binning lexical sets into training and evaluation bins randomly, in equal proportions. The evaluation set is then split 80/20 into test and development set. Splitting by lexical bin rather than by sentence prevents models from finding a trivial solution to classification by learning to 2 The C AUSATIVE –I NCHOATIVE dataset presented here is an expanded version of an analysis dataset in Warstadt et al. (2018). 291 Embeddings, i.e., vector representations of linguistic objects like characters, words, or sentences, encode helpful information for downstream applications (Mikolov et al., 2013). In particular, they can be used to leverage knowledge from one task for another and have been shown to improve performance on a diverse set of tasks. Embeddings are usually low-dimensional; common sizes differ between 100 and 300. Our experiments make use of three types of word and sentence embeddings, which we will describe in the following. Word Embeddings For our word-level experiments, we use two different embeddings which differ in the way of their creation. First, we use 300-dimensional GloVe embeddings trained on 6B tokens (Pennington et al., 2014).3 GloVe embeddings are used frequent"
W19-0129,E17-2102,0,0.0252246,"ent, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence embeddings contain grammatical information, e.g., about the syntactic scope of negation. Further previous studies on which types of information are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings which were trained as part of an ANN for machine trans¨ lation. Finally, Ostling and Tiedemann (2017) learned language embeddings via multilingual language modeling and used them to reconstruct genealogical trees. However, we are interested in word or sentence embeddings. Extracting information from word embeddings is a common task in natural language processing. While most NLP research is application-oriented and directly or indirectly focuses on obtaining embeddings which contain as much knowledge about the task at hand as possible (e.g., by varying the training corpus or embedding method), we are interested in the question how much information is trivially contained in selected popular emb"
W19-0129,D14-1162,0,0.103224,"nformation for downstream applications (Mikolov et al., 2013). In particular, they can be used to leverage knowledge from one task for another and have been shown to improve performance on a diverse set of tasks. Embeddings are usually low-dimensional; common sizes differ between 100 and 300. Our experiments make use of three types of word and sentence embeddings, which we will describe in the following. Word Embeddings For our word-level experiments, we use two different embeddings which differ in the way of their creation. First, we use 300-dimensional GloVe embeddings trained on 6B tokens (Pennington et al., 2014).3 GloVe embeddings are used frequently in natural language processing (NLP), so evaluating them for knowledge of verb frames will be relevant for their application to and future research on tasks requiring rich syntactic features. Second, we use embeddings trained on the smaller 100M token British National Corpus4 (BNC), optimizing a language modeling objective. The language model (LM) is a (single-directional) LSTM trained by Warstadt et al. (2018) using PyTorch and optimized using Adam (Kingma and Ba, 2015). The BNC data is tokenized using NLTK (Bird and Loper, 2004) and words outside the 1"
W19-0129,N18-1202,0,0.0621809,"ntil the completion of 4 training epochs without improvement in Matthews correlation coefficient on the development set. The architecture of the real/fake encoder is shown in Figure 1. A bidirectional long-short term memory network (LSTM, Hochreiter and Schmidhuber, 1997) reads the words of a sentence. A fixed-length sentence embedding is then produced by a max-pooling operation over the concatenations of the forward and backward hidden states at each time-step. This encoding serves as input to a sigmoid output layer, which outputs a binary prediction. The input to the encoder are ELMo-style (Peters et al., 2018) contextualized word embeddings from a trained LM. As in ELMo, the representation for a word wi is a linear combination of the hidden states hji for each layer j in an LSTM LM, though we depart from that paper by using only a forward LM. As argued in Warstadt et al. (2018), this sentence encoder is a reasonable model for a human learner because it is not exposed to any knowledge of language that could not plausibly be part of the input to a human learner. Its training data consists of the same 100 million tokens used to train the word embeddings, augmented with another 100 million generated to"
